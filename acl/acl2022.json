[
    {
        "id": "2022.acl-short.96",
        "title": "(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models\u2019 Performance",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In the domain of Morphology, Inflection is a fundamental and important task that gained a lot of traction in recent years, mostly via SIGMORPHON\u2019s shared-tasks. With average accuracy above 0.9 over the scores of all languages, the task is considered mostly solved using relatively generic neural seq2seq models, even with little data provided. In this work, we propose to re-evaluate morphological inflection models by employing harder train-test splits that will challenge the generalization capacity of the models. In particular, as opposed to the na\u00efve split-by-form, we propose a split-by-lemma method to challenge the performance on existing benchmarks. Our experiments with the three top-ranked systems on the SIGMORPHON\u2019s 2020 shared-task show that the lemma-split presents an average drop of 30 percentage points in macro-average for the 90 languages included. The effect is most significant for low-resourced languages with a drop as high as 95 points, but even high-resourced languages lose about 10 points on average. Our results clearly show that generalizing inflection to unseen lemmas is far from being solved, presenting a simple yet effective means to promote more sophisticated models.",
        "author": "Omer Goldman; David Guriel; Reut Tsarfaty",
        "authorids": "/o/omer-goldman/; /d/david-guriel/; /r/reut-tsarfaty/",
        "bibtex": "@inproceedings{goldman-etal-2022-un,\n    title = \"(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance\",\n    author = \"Goldman, Omer  and\n      Guriel, David  and\n      Tsarfaty, Reut\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.96/\",\n    doi = \"10.18653/v1/2022.acl-short.96\",\n    pages = \"864--870\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.96.pdf",
        "site": "https://aclanthology.org/2022.acl-short.96/",
        "pdf_size": 183297,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15388423733946890751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Bar-Ilan University; Bar-Ilan University; Bar-Ilan University",
        "aff_domain": "gmail.com;gmail.com;biu.ac.il",
        "email": "gmail.com;gmail.com;biu.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Bar-Ilan University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.biu.ac.il",
        "aff_unique_abbr": "BIU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.acl-long.75",
        "title": "A Closer Look at How Fine-tuning Changes BERT",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Given the prevalence of pre-trained contextualized representations in today\u2019s NLP, there have been many efforts to understand what information they contain, and why they seem to be universally successful. The most common approach to use these representations involves fine-tuning them for an end task. Yet, how fine-tuning changes the underlying embedding space is less studied. In this work, we study the English BERT family and use two probing techniques to analyze how fine-tuning changes the space. We hypothesize that fine-tuning affects classification performance by increasing the distances between examples associated with different labels. We confirm this hypothesis with carefully designed experiments on five different NLP tasks. Via these experiments, we also discover an exception to the prevailing wisdom that \u201cfine-tuning always improves performance\u201d. Finally, by comparing the representations before and after fine-tuning, we discover that fine-tuning does not introduce arbitrary changes to representations; instead, it adjusts the representations to downstream tasks while largely preserving the original spatial structure of the data points.",
        "author": "Yichu Zhou; Vivek Srikumar",
        "authorids": "/y/yichu-zhou/; /v/vivek-srikumar/",
        "bibtex": "@inproceedings{zhou-srikumar-2022-closer,\n    title = \"A Closer Look at How Fine-tuning Changes {BERT}\",\n    author = \"Zhou, Yichu  and\n      Srikumar, Vivek\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.75/\",\n    doi = \"10.18653/v1/2022.acl-long.75\",\n    pages = \"1046--1061\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.75.pdf",
        "site": "https://aclanthology.org/2022.acl-long.75/",
        "pdf_size": 3010464,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5763242010984558412&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computing, University of Utah; School of Computing, University of Utah",
        "aff_domain": "cs.utah.edu;cs.utah.edu",
        "email": "cs.utah.edu;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Utah",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.utah.edu",
        "aff_unique_abbr": "U of U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Utah",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.345",
        "title": "A Comparative Study of Faithfulness Metrics for Model Interpretability Methods",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Interpretable methods to reveal the internal reasoning processes behind machine learning models have attracted increasing attention in recent years. To quantify the extent to which the identified interpretations truly reflect the intrinsic decision-making mechanisms, various faithfulness evaluation metrics have been proposed. However, we find that different faithfulness metrics show conflicting preferences when comparing different interpretations. Motivated by this observation, we aim to conduct a comprehensive and comparative study of the widely adopted faithfulness metrics. In particular, we introduce two assessment dimensions, namely diagnosticity and complexity. Diagnosticity refers to the degree to which the faithfulness metric favors relatively faithful interpretations over randomly generated ones, and complexity is measured by the average number of model forward passes. According to the experimental results, we find that sufficiency and comprehensiveness metrics have higher diagnosticity and lower complexity than the other faithfulness metrics.",
        "author": "Chun Sik Chan; Huanqi Kong; Liang Guanqing",
        "authorids": "/c/chun-sik-chan/; /h/huanqi-kong/; /l/liang-guanqing/",
        "bibtex": "@inproceedings{chan-etal-2022-comparative,\n    title = \"A Comparative Study of Faithfulness Metrics for Model Interpretability Methods\",\n    author = \"Chan, Chun Sik  and\n      Kong, Huanqi  and\n      Guanqing, Liang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.345/\",\n    doi = \"10.18653/v1/2022.acl-long.345\",\n    pages = \"5029--5038\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.345.pdf",
        "site": "https://aclanthology.org/2022.acl-long.345/",
        "pdf_size": 337757,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9760165439572848511&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Wisers AI Lab, Wisers Information Limited; Wisers AI Lab, Wisers Information Limited; Wisers AI Lab, Wisers Information Limited",
        "aff_domain": "wisers.com;wisers.com;wisers.com",
        "email": "wisers.com;wisers.com;wisers.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Wisers Information Limited",
        "aff_unique_dep": "Wisers AI Lab",
        "aff_unique_url": "https://www.wisers.net",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.572",
        "title": "A Comparison of Strategies for Source-Free Domain Adaptation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Data sharing restrictions are common in NLP, especially in the clinical domain, but there is limited research on adapting models to new domains without access to the original training data, a setting known as source-free domain adaptation. We take algorithms that traditionally assume access to the source-domain training data\u2014active learning, self-training, and data augmentation\u2014and adapt them for source free domain adaptation. Then we systematically compare these different strategies across multiple tasks and domains. We find that active learning yields consistent gains across all SemEval 2021 Task 10 tasks and domains, but though the shared task saw successful self-trained and data augmented models, our systematic comparison finds these strategies to be unreliable for source-free domain adaptation.",
        "author": "Xin Su; Yiyun Zhao; Steven Bethard",
        "authorids": "/x/xin-su/; /y/yiyun-zhao/; /s/steven-bethard/",
        "bibtex": "@inproceedings{su-etal-2022-comparison,\n    title = \"A Comparison of Strategies for Source-Free Domain Adaptation\",\n    author = \"Su, Xin  and\n      Zhao, Yiyun  and\n      Bethard, Steven\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.572/\",\n    doi = \"10.18653/v1/2022.acl-long.572\",\n    pages = \"8352--8367\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.572.pdf",
        "site": "https://aclanthology.org/2022.acl-long.572/",
        "pdf_size": 382755,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4321860393076444396&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Arizona; University of Arizona; University of Arizona",
        "aff_domain": "email.arizona.edu;email.arizona.edu;email.arizona.edu",
        "email": "email.arizona.edu;email.arizona.edu;email.arizona.edu",
        "github": "github.com/xinsu626/SourceFreeDomainAdaptation8352",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Arizona",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.arizona.edu",
        "aff_unique_abbr": "UA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.336",
        "title": "A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Learning high-quality sentence representations is a fundamental problem of natural language processing which could benefit a wide range of downstream tasks. Though the BERT-like pre-trained language models have achieved great success, using their sentence representations directly often results in poor performance on the semantic textual similarity task. Recently, several contrastive learning methods have been proposed for learning sentence representations and have shown promising results. However, most of them focus on the constitution of positive and negative representation pairs and pay little attention to the training objective like NT-Xent, which is not sufficient enough to acquire the discriminating power and is unable to model the partial order of semantics between sentences. So in this paper, we propose a new method ArcCSE, with training objectives designed to enhance the pairwise discriminative power and model the entailment relation of triplet sentences. We conduct extensive experiments which demonstrate that our approach outperforms the previous state-of-the-art on diverse sentence related tasks, including STS and SentEval.",
        "author": "Yuhao Zhang; Hongji Zhu; Yongliang Wang; Nan Xu; Xiaobo Li; Binqiang Zhao",
        "authorids": "/y/yuhao-zhang/; /h/hongji-zhu/; /y/yongliang-wang/; /n/nan-xu/; /x/xiaobo-li/; /b/binqiang-zhao/",
        "bibtex": "@inproceedings{zhang-etal-2022-contrastive,\n    title = \"A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space\",\n    author = \"Zhang, Yuhao  and\n      Zhu, Hongji  and\n      Wang, Yongliang  and\n      Xu, Nan  and\n      Li, Xiaobo  and\n      Zhao, Binqiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.336/\",\n    doi = \"10.18653/v1/2022.acl-long.336\",\n    pages = \"4892--4903\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.336.pdf",
        "site": "https://aclanthology.org/2022.acl-long.336/",
        "pdf_size": 1012197,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14071487508853059548&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Alibaba Group; Alibaba Group; Alibaba Group; Institute of Automation, Chinese Academy of Sciences; Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;ia.ac.cn;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;ia.ac.cn;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "Alibaba Group;Chinese Academy of Sciences",
        "aff_unique_dep": ";Institute of Automation",
        "aff_unique_url": "https://www.alibaba.com;http://www.ia.cas.cn",
        "aff_unique_abbr": "Alibaba;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.47",
        "title": "A Copy-Augmented Generative Model for Open-Domain Question Answering",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Open-domain question answering is a challenging task with a wide variety of practical applications. Existing modern approaches mostly follow a standard two-stage paradigm: retriever then reader. In this article, we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both extractive and generative readers. In particular, our model is built upon the powerful generative model FiD (CITATION). We enhance the original generative reader by incorporating a pointer network to encourage the model to directly copy words from the retrieved passages. We conduct experiments on the two benchmark datasets, Natural Questions and TriviaQA, and the empirical results demonstrate the performance gains of our proposed approach.",
        "author": "Shuang Liu; Dong Wang; Xiaoguang Li; Minghui Huang; Meizhen Ding",
        "authorids": "/s/shuang-liu/; /d/dong-wang/; /x/xiaoguang-li/; /m/minghui-huang/; /m/meizhen-ding/",
        "bibtex": "@inproceedings{liu-etal-2022-copy,\n    title = \"A Copy-Augmented Generative Model for Open-Domain Question Answering\",\n    author = \"Liu, Shuang  and\n      Wang, Dong  and\n      Li, Xiaoguang  and\n      Huang, Minghui  and\n      Ding, Meizhen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.47/\",\n    doi = \"10.18653/v1/2022.acl-short.47\",\n    pages = \"435--441\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.47.pdf",
        "site": "https://aclanthology.org/2022.acl-short.47/",
        "pdf_size": 303904,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15654706869997161646&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "Huawei Noah\u2019s Ark Lab; AI Application Research Center (AARC); Huawei Noah\u2019s Ark Lab; AI Application Research Center (AARC); AI Application Research Center (AARC)",
        "aff_domain": "huawei.com;huawei.com; ; ; ",
        "email": "huawei.com;huawei.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;1;1",
        "aff_unique_norm": "Huawei;AI Application Research Center",
        "aff_unique_dep": "Noah\u2019s Ark Lab;Application Research",
        "aff_unique_url": "https://www.huawei.com;",
        "aff_unique_abbr": "Huawei;AARC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2022.findings-acl.151",
        "title": "A Feasibility Study of Answer-Agnostic Question Generation for Education",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We conduct a feasibility study into the applicability of answer-agnostic question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% \u2192 83%) as determined by expert annotators. We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.",
        "author": "Liam Dugan; Eleni Miltsakaki; Shriyash Upadhyay; Etan Ginsberg; Hannah Gonzalez; DaHyeon Choi; Chuning Yuan; Chris Callison-Burch",
        "authorids": "/l/liam-dugan/; /e/eleni-miltsakaki/; /s/shriyash-upadhyay/; /e/etan-ginsberg/; /h/hannah-gonzalez/; /d/dahyeon-choi/; /c/chuning-yuan/; /c/chris-callison-burch/",
        "bibtex": "@inproceedings{dugan-etal-2022-feasibility,\n    title = \"A Feasibility Study of Answer-Agnostic Question Generation for Education\",\n    author = \"Dugan, Liam  and\n      Miltsakaki, Eleni  and\n      Upadhyay, Shriyash  and\n      Ginsberg, Etan  and\n      Gonzalez, Hannah  and\n      Choi, DaHyeon  and\n      Yuan, Chuning  and\n      Callison-Burch, Chris\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.151/\",\n    doi = \"10.18653/v1/2022.findings-acl.151\",\n    pages = \"1919--1926\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.151.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.151/",
        "pdf_size": 260616,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1143516754293630738&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 13,
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "github": "https://github.com/liamdugan/summary-qg",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.317",
        "title": "A Few-Shot Semantic Parser for Wizard-of-Oz Dialogues with the Precise ThingTalk Representation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Previous attempts to build effective semantic parsers for Wizard-of-Oz (WOZ) conversations suffer from the difficulty in acquiring a high-quality, manually annotated training set. Approaches based only on dialogue synthesis are insufficient, as dialogues generated from state-machine based models are poor approximations of real-life conversations. Furthermore, previously proposed dialogue state representations are ambiguous and lack the precision necessary for building an effective agent. This paper proposes a new dialogue representation and a sample-efficient methodology that can predict precise dialogue states in WOZ conversations. We extended the ThingTalk representation to capture all information an agent needs to respond properly. Our training strategy is sample-efficient: we combine (1) few-shot data sparsely sampling the full dialogue space and (2) synthesized data covering a subset space of dialogues generated by a succinct state-based dialogue model. The completeness of the extended ThingTalk language is demonstrated with a fully operational agent, which is also used in training data synthesis. We demonstrate the effectiveness of our methodology on MultiWOZ 3.0, a reannotation of the MultiWOZ 2.1 dataset in ThingTalk. ThingTalk can represent 98% of the test turns, while the simulator can emulate 85% of the validation set. We train a contextual semantic parser using our strategy, and obtain 79% turn-by-turn exact match accuracy on the reannotated test set.",
        "author": "Giovanni Campagna; Sina Semnani; Ryan Kearns; Lucas Jun Koba Sato; Silei Xu; Monica Lam",
        "authorids": "/g/giovanni-campagna/; /s/sina-semnani/; /r/ryan-kearns/; /l/lucas-jun-koba-sato/; /s/silei-xu/; /m/monica-lam/",
        "bibtex": "@inproceedings{campagna-etal-2022-shot,\n    title = \"A Few-Shot Semantic Parser for {W}izard-of-{O}z Dialogues with the Precise {T}hing{T}alk Representation\",\n    author = \"Campagna, Giovanni  and\n      Semnani, Sina  and\n      Kearns, Ryan  and\n      Koba Sato, Lucas Jun  and\n      Xu, Silei  and\n      Lam, Monica\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.317/\",\n    doi = \"10.18653/v1/2022.findings-acl.317\",\n    pages = \"4021--4034\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.317.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.317/",
        "pdf_size": 335964,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2252376655882221998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://oval.cs.stanford.edu/releases/",
        "author_num": 6
    },
    {
        "id": "2022.acl-short.89",
        "title": "A Flexible Multi-Task Model for BERT Serving",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We present an efficient BERT-based multi-task (MT) framework that is particularly suitable for iterative and incremental development of the tasks. The proposed framework is based on the idea of partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the other layers frozen. For each task, we train independently a single-task (ST) model using partial fine-tuning. Then we compress the task-specific layers in each ST model using knowledge distillation. Those compressed ST models are finally merged into one MT model so that the frozen layers of the former are shared across the tasks. We exemplify our approach on eight GLUE tasks, demonstrating that it is able to achieve 99.6% of the performance of the full fine-tuning method, while reducing up to two thirds of its overhead.",
        "author": "Tianwen Wei; Jianwei Qi; Shenghuan He",
        "authorids": "/t/tianwen-wei/; /j/jianwei-qi/; /s/shenghuan-he/",
        "bibtex": "@inproceedings{wei-etal-2022-flexible,\n    title = \"A Flexible Multi-Task Model for {BERT} Serving\",\n    author = \"Wei, Tianwen  and\n      Qi, Jianwei  and\n      He, Shenghuan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.89/\",\n    doi = \"10.18653/v1/2022.acl-short.89\",\n    pages = \"785--796\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.89.pdf",
        "site": "https://aclanthology.org/2022.acl-short.89/",
        "pdf_size": 2396268,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=647884370861257382&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Xiaomi, XiaoAI Team; Xiaomi, XiaoAI Team; Xiaomi, XiaoAI Team",
        "aff_domain": "xiaomi.com;xiaomi.com;xiaomi.com",
        "email": "xiaomi.com;xiaomi.com;xiaomi.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Xiaomi Corporation",
        "aff_unique_dep": "XiaoAI Team",
        "aff_unique_url": "https://www.xiaomi.com",
        "aff_unique_abbr": "Xiaomi",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.197",
        "title": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners. For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks. Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at https://github.com/woojeongjin/FewVLM",
        "author": "Woojeong Jin; Yu Cheng; Yelong Shen; Weizhu Chen; Xiang Ren",
        "authorids": "/w/woojeong-jin/; /y/yu-cheng/; /y/yelong-shen/; /w/weizhu-chen/; /x/xiang-ren/",
        "bibtex": "@inproceedings{jin-etal-2022-good,\n    title = \"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models\",\n    author = \"Jin, Woojeong  and\n      Cheng, Yu  and\n      Shen, Yelong  and\n      Chen, Weizhu  and\n      Ren, Xiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.197/\",\n    doi = \"10.18653/v1/2022.acl-long.197\",\n    pages = \"2763--2775\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.197.pdf",
        "site": "https://aclanthology.org/2022.acl-long.197/",
        "pdf_size": 1002340,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1342661209364328261&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "University of Southern California; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; University of Southern California",
        "aff_domain": "usc.edu;microsoft.com;microsoft.com;microsoft.com;usc.edu",
        "email": "usc.edu;microsoft.com;microsoft.com;microsoft.com;usc.edu",
        "github": "https://github.com/woojeongjin/FewVLM",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Southern California;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://www.usc.edu;https://www.microsoft.com",
        "aff_unique_abbr": "USC;Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.206",
        "title": "A Graph Enhanced BERT Model for Event Prediction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Predicting the subsequent event for an existing event context is an important but challenging task, as it requires understanding the underlying relationship between events. Previous methods propose to retrieve relational features from event graph to enhance the modeling of event correlation. However, the sparsity of event graph may restrict the acquisition of relevant graph information, and hence influence the model performance. To address this issue, we consider automatically building of event graph using a BERT model. To this end, we incorporate an additional structured variable into BERT to learn to predict the event connections in the training process. Hence, in the test process, the connection relationship for unseen events can be predicted by the structured variable. Results on two event prediction tasks: script event prediction and story ending prediction, show that our approach can outperform state-of-the-art baseline methods.",
        "author": "Li Du; Xiao Ding; Yue Zhang; Ting Liu; Bing Qin",
        "authorids": "/l/li-du/; /x/xiao-ding/; /y/yue-zhang/; /t/ting-liu/; /b/bing-qin/",
        "bibtex": "@inproceedings{du-etal-2022-graph,\n    title = \"A Graph Enhanced {BERT} Model for Event Prediction\",\n    author = \"Du, Li  and\n      Ding, Xiao  and\n      Zhang, Yue  and\n      Liu, Ting  and\n      Qin, Bing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.206/\",\n    doi = \"10.18653/v1/2022.findings-acl.206\",\n    pages = \"2628--2638\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.206.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.206/",
        "pdf_size": 799534,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3131838231249045422&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2022.acl-long.195",
        "title": "A Meta-framework for Spatiotemporal Quantity Extraction from Text",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events. This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it. This meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models. We demonstrate the meta-framework in three domains\u2014the COVID-19 pandemic, Black Lives Matter protests, and 2020 California wildfires\u2014to show that the formalism is general and extensible, the crowdsourcing pipeline facilitates fast and high-quality data annotation, and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful. We release all resources for future research on this topic at https://github.com/steqe.",
        "author": "Qiang Ning; Ben Zhou; Hao Wu; Haoruo Peng; Chuchu Fan; Matt Gardner",
        "authorids": "/q/qiang-ning/; /b/ben-zhou/; /h/hao-wu/; /h/haoruo-peng/; /c/chuchu-fan/; /m/matt-gardner/",
        "bibtex": "@inproceedings{ning-etal-2022-meta,\n    title = \"A Meta-framework for Spatiotemporal Quantity Extraction from Text\",\n    author = \"Ning, Qiang  and\n      Zhou, Ben  and\n      Wu, Hao  and\n      Peng, Haoruo  and\n      Fan, Chuchu  and\n      Gardner, Matt\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.195/\",\n    doi = \"10.18653/v1/2022.acl-long.195\",\n    pages = \"2736--2749\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.195.pdf",
        "site": "https://aclanthology.org/2022.acl-long.195/",
        "pdf_size": 328529,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14818060280335896395&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Amazon; University of Pennsylvania; Hooray Data; Newsbreak; MIT; Microsoft Semantic Machines",
        "aff_domain": "amazon.com;seas.upenn.edu;hooray.ai;newsbreak.com;mit.edu;microsoft.com",
        "email": "amazon.com;seas.upenn.edu;hooray.ai;newsbreak.com;mit.edu;microsoft.com",
        "github": "https://github.com/steqe",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;4;5",
        "aff_unique_norm": "Amazon;University of Pennsylvania;Hooray Data;NewsBreak;Massachusetts Institute of Technology;Microsoft",
        "aff_unique_dep": "Amazon.com, Inc.;;;;;Semantic Machines",
        "aff_unique_url": "https://www.amazon.com;https://www.upenn.edu;;;https://web.mit.edu;https://www.microsoft.com",
        "aff_unique_abbr": "Amazon;UPenn;;;MIT;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2022.acl-long.550",
        "title": "A Model-agnostic Data Manipulation Method for Persona-based Dialogue Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Towards building intelligent dialogue agents, there has been a growing interest in introducing explicit personas in generation models. However, with limited persona-based dialogue data at hand, it may be difficult to train a dialogue generation model well. We point out that the data challenges of this generation task lie in two aspects: first, it is expensive to scale up current persona-based dialogue datasets; second, each data sample in this task is more complex to learn with than conventional dialogue data. To alleviate the above data issues, we propose a data manipulation method, which is model-agnostic to be packed with any persona-based dialogue generation model to improve their performance. The original training samples will first be distilled and thus expected to be fitted more easily. Next, we show various effective ways that can diversify such easier distilled data. A given base model will then be trained via the constructed data curricula, i.e. first on augmented distilled samples and then on original ones. Experiments illustrate the superiority of our method with two strong base dialogue models (Transformer encoder-decoder and GPT2).",
        "author": "Yu Cao; Wei Bi; Meng Fang; Shuming Shi; Dacheng Tao",
        "authorids": "/y/yu-cao/; /w/wei-bi/; /m/meng-fang/; /s/shuming-shi/; /d/dacheng-tao/",
        "bibtex": "@inproceedings{cao-etal-2022-model,\n    title = \"A Model-agnostic Data Manipulation Method for Persona-based Dialogue Generation\",\n    author = \"Cao, Yu  and\n      Bi, Wei  and\n      Fang, Meng  and\n      Shi, Shuming  and\n      Tao, Dacheng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.550/\",\n    doi = \"10.18653/v1/2022.acl-long.550\",\n    pages = \"7984--8002\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.550.pdf",
        "site": "https://aclanthology.org/2022.acl-long.550/",
        "pdf_size": 865210,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10895592377235336360&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, The University of Sydney, Australia; Tencent AI Lab, Shenzhen, China; Eindhoven University of Technology (TU/e), Eindhoven, The Netherlands; Tencent AI Lab, Shenzhen, China; School of Computer Science, The University of Sydney, Australia + JD Explore Academy, Beijing, China",
        "aff_domain": "sydney.edu.au;tencent.com;tue.nl;tencent.com;gmail.com",
        "email": "sydney.edu.au;tencent.com;tue.nl;tencent.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;1;0+3",
        "aff_unique_norm": "University of Sydney;Tencent;Eindhoven University of Technology;JD",
        "aff_unique_dep": "School of Computer Science;AI Lab;;JD Explore Academy",
        "aff_unique_url": "https://www.sydney.edu.au;https://ai.tencent.com;https://www.tue.nl;",
        "aff_unique_abbr": "USYD;Tencent AI Lab;TU/e;",
        "aff_campus_unique_index": "0;1;2;1;0+3",
        "aff_campus_unique": "Sydney;Shenzhen;Eindhoven;Beijing",
        "aff_country_unique_index": "0;1;2;1;0+1",
        "aff_country_unique": "Australia;China;Netherlands"
    },
    {
        "id": "2022.acl-long.351",
        "title": "A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-document summarization (MDS) has made significant progress in recent years, in part facilitated by the availability of new, dedicated datasets and capacious language models. However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives. As for many other generative tasks, reinforcement learning (RL) offers the potential to improve the training of MDS models; yet, it requires a carefully-designed reward that can ensure appropriate leverage of both the reference summaries and the input documents. For this reason, in this paper we propose fine-tuning an MDS baseline with a reward that balances a reference-based metric such as ROUGE with coverage of the input documents. To implement the approach, we utilize RELAX (Grathwohl et al., 2018), a contemporary gradient estimator which is both low-variance and unbiased, and we fine-tune the baseline in a few-shot style for both stability and computational efficiency. Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to +0.95 pp average ROUGE score and +3.17 pp METEOR score over the baseline, and competitive results with the literature. In addition, they show that the coverage of the input documents is increased, and evenly across all documents.",
        "author": "Jacob Parnell; Inigo Jauregi Unanue; Massimo Piccardi",
        "authorids": "/j/jacob-parnell/; /i/inigo-jauregi-unanue/; /m/massimo-piccardi/",
        "bibtex": "@inproceedings{parnell-etal-2022-multi,\n    title = \"A Multi-Document Coverage Reward for {RELAX}ed Multi-Document Summarization\",\n    author = \"Parnell, Jacob  and\n      Jauregi Unanue, Inigo  and\n      Piccardi, Massimo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.351/\",\n    doi = \"10.18653/v1/2022.acl-long.351\",\n    pages = \"5112--5128\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.351.pdf",
        "site": "https://aclanthology.org/2022.acl-long.351/",
        "pdf_size": 997857,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9259741381936188165&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 6,
        "aff": "University of Technology Sydney, NSW, Australia+RoZetta Technology, NSW, Australia; University of Technology Sydney, NSW, Australia+RoZetta Technology, NSW, Australia; University of Technology Sydney, NSW, Australia",
        "aff_domain": "rozettatechnology.com;rozettatechnology.com;uts.edu.au",
        "email": "rozettatechnology.com;rozettatechnology.com;uts.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0",
        "aff_unique_norm": "University of Technology Sydney;RoZetta Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uts.edu.au;",
        "aff_unique_abbr": "UTS;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2022.findings-acl.263",
        "title": "A Natural Diet: Towards Improving Naturalness of Machine Translation Output",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Machine translation (MT) evaluation often focuses on accuracy and fluency, without paying much attention to translation style. This means that, even when considered accurate and fluent, MT output can still sound less natural than high quality human translations or text originally written in the target language. Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence. In this work we propose a method for training MT systems to achieve a more natural style, i.e. mirroring the style of text originally written in the target language. Our method tags parallel training data according to the naturalness of the target side by contrasting language models trained on natural and translated data. Tagging data allows us to put greater emphasis on target sentences originally written in the target language. Automatic metrics show that the resulting models achieve lexical richness on par with human translations, mimicking a style much closer to sentences originally written in the target language. Furthermore, we find that their output is preferred by human experts when compared to the baseline translations.",
        "author": "Markus Freitag; David Vilar; David Grangier; Colin Cherry; George Foster",
        "authorids": "/m/markus-freitag/; /d/david-vilar/; /d/david-grangier/; /c/colin-cherry/; /g/george-foster/",
        "bibtex": "@inproceedings{freitag-etal-2022-natural,\n    title = \"A Natural Diet: Towards Improving Naturalness of Machine Translation Output\",\n    author = \"Freitag, Markus  and\n      Vilar, David  and\n      Grangier, David  and\n      Cherry, Colin  and\n      Foster, George\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.263/\",\n    doi = \"10.18653/v1/2022.findings-acl.263\",\n    pages = \"3340--3353\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.263.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.263/",
        "pdf_size": 218546,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6171673866168603765&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 4,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.353",
        "title": "A Neural Network Architecture for Program Understanding Inspired by Human Behaviors",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Program understanding is a fundamental task in program language processing. Despite the success, existing works fail to take human behaviors as reference in understanding programs. In this paper, we consider human behaviors and propose the PGNN-EK model that consists of two main components. On the one hand, inspired by the \u201cdivide-and-conquer\u201d reading behaviors of humans, we present a partitioning-based graph neural network model PGNN on the upgraded AST of codes. On the other hand, to characterize human behaviors of resorting to other resources to help code comprehension, we transform raw codes with external knowledge and apply pre-training techniques for information extraction. Finally, we combine the two embeddings generated from the two components to output code embeddings. We conduct extensive experiments to show the superior performance of PGNN-EK on the code summarization and code clone detection tasks. In particular, to show the generalization ability of our model, we release a new dataset that is more challenging for code clone detection and could advance the development of the community. Our codes and data are publicly available at https://github.com/RecklessRonan/PGNN-EK.",
        "author": "Renyu Zhu; Lei Yuan; Xiang Li; Ming Gao; Wenyuan Cai",
        "authorids": "/r/renyu-zhu/; /l/lei-yuan/; /x/xiang-li/; /m/ming-gao/; /w/wenyuan-cai/",
        "bibtex": "@inproceedings{zhu-etal-2022-neural,\n    title = \"A Neural Network Architecture for Program Understanding Inspired by Human Behaviors\",\n    author = \"Zhu, Renyu  and\n      Yuan, Lei  and\n      Li, Xiang  and\n      Gao, Ming  and\n      Cai, Wenyuan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.353/\",\n    doi = \"10.18653/v1/2022.acl-long.353\",\n    pages = \"5142--5153\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.353.pdf",
        "site": "https://aclanthology.org/2022.acl-long.353/",
        "pdf_size": 477906,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11353629236316165821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; Shanghai Hypers Data Technology Inc., Shanghai, China",
        "aff_domain": "stu.ecnu.edu.cn;stu.ecnu.edu.cn;dase.ecnu.edu.cn;dase.ecnu.edu.cn;hypers.com",
        "email": "stu.ecnu.edu.cn;stu.ecnu.edu.cn;dase.ecnu.edu.cn;dase.ecnu.edu.cn;hypers.com",
        "github": "https://github.com/RecklessRonan/PGNN-EK",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "East China Normal University;Shanghai Hypers Data Technology Inc.",
        "aff_unique_dep": "School of Data Science and Engineering;",
        "aff_unique_url": "http://www.ecnu.edu.cn;",
        "aff_unique_abbr": "ECNU;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.300",
        "title": "A Neural Pairwise Ranking Model for Readability Assessment",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Automatic Readability Assessment (ARA), the task of assigning a reading level to a text, is traditionally treated as a classification problem in NLP research. In this paper, we propose the first neural, pairwise ranking approach to ARA and compare it with existing classification, regression, and (non-neural) ranking methods. We establish the performance of our approach by conducting experiments with three English, one French and one Spanish datasets. We demonstrate that our approach performs well in monolingual single/cross corpus testing scenarios and achieves a zero-shot cross-lingual ranking accuracy of over 80% for both French and Spanish when trained on English data. Additionally, we also release a new parallel bilingual readability dataset, that could be useful for future research. To our knowledge, this paper proposes the first neural pairwise ranking model for ARA, and shows the first results of cross-lingual, zero-shot evaluation of ARA with neural models.",
        "author": "Justin Lee; Sowmya Vajjala",
        "authorids": "/j/justin-lee/; /s/sowmya-vajjala/",
        "bibtex": "@inproceedings{lee-vajjala-2022-neural,\n    title = \"A Neural Pairwise Ranking Model for Readability Assessment\",\n    author = \"Lee, Justin  and\n      Vajjala, Sowmya\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.300/\",\n    doi = \"10.18653/v1/2022.findings-acl.300\",\n    pages = \"3802--3813\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.300.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.300/",
        "pdf_size": 258592,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7343521369692958754&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Toronto + CSA Group; National Research Council",
        "aff_domain": "mail.utoronto.ca;nrc-cnrc.gc.ca",
        "email": "mail.utoronto.ca;nrc-cnrc.gc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "University of Toronto;CSA Group;National Research Council",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.csa-group.org;https://www.nrc-cnrc.gc.ca",
        "aff_unique_abbr": "U of T;CSA Group;NRC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.findings-acl.110",
        "title": "A Novel Framework Based on Medical Concept Driven Attention for Explainable Medical Code Prediction via External Knowledge",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Medical code prediction from clinical notes aims at automatically associating medical codes with the clinical notes. Rare code problem, the medical codes with low occurrences, is prominent in medical code prediction. Recent studies employ deep neural networks and the external knowledge to tackle it. However, such approaches lack interpretability which is a vital issue in medical application. Moreover, due to the lengthy and noisy clinical notes, such approaches fail to achieve satisfactory results. Therefore, in this paper, we propose a novel framework based on medical concept driven attention to incorporate external knowledge for explainable medical code prediction. In specific, both the clinical notes and Wikipedia documents are aligned into topic space to extract medical concepts using topic modeling. Then, the medical concept-driven attention mechanism is applied to uncover the medical code related concepts which provide explanations for medical code prediction. Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baselines.",
        "author": "Tao Wang; Linhai Zhang; Chenchen Ye; Junxi Liu; Deyu Zhou",
        "authorids": "/t/tao-wang/; /l/linhai-zhang/; /c/chenchen-ye/; /j/junxi-liu/; /d/deyu-zhou/",
        "bibtex": "@inproceedings{wang-etal-2022-novel,\n    title = \"A Novel Framework Based on Medical Concept Driven Attention for Explainable Medical Code Prediction via External Knowledge\",\n    author = \"Wang, Tao  and\n      Zhang, Linhai  and\n      Ye, Chenchen  and\n      Liu, Junxi  and\n      Zhou, Deyu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.110/\",\n    doi = \"10.18653/v1/2022.findings-acl.110\",\n    pages = \"1407--1416\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.110.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.110/",
        "pdf_size": 1598408,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1584994096602570394&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.178",
        "title": "A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP. However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models\u2019 complexity, thus leading to challenges in model explainability. To address this challenge, we propose a novel practical framework by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process. We apply it in the context of a news article classification task. The experiments on two large-scaled news corpora demonstrate that the proposed model can achieve competitive performance with many state-of-the-art alternatives and illustrate its appropriateness from an explainability perspective. We release the source code here.",
        "author": "Dairui Liu; Derek Greene; Ruihai Dong",
        "authorids": "/d/dairui-liu/; /d/derek-greene/; /r/ruihai-dong/",
        "bibtex": "@inproceedings{liu-etal-2022-novel,\n    title = \"A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification\",\n    author = \"Liu, Dairui  and\n      Greene, Derek  and\n      Dong, Ruihai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.178/\",\n    doi = \"10.18653/v1/2022.findings-acl.178\",\n    pages = \"2280--2290\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.178.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.178/",
        "pdf_size": 822190,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10264738945122375408&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Insight Centre for Data Analytics, Dublin + School of Computer Science, University College Dublin, Ireland; Insight Centre for Data Analytics, Dublin + School of Computer Science, University College Dublin, Ireland; Insight Centre for Data Analytics, Dublin + School of Computer Science, University College Dublin, Ireland",
        "aff_domain": "ucdconnect.ie;ucd.ie;ucd.ie",
        "email": "ucdconnect.ie;ucd.ie;ucd.ie",
        "github": "https://github.com/Ruixinhua/BATM",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Insight Centre for Data Analytics;University College Dublin",
        "aff_unique_dep": ";School of Computer Science",
        "aff_unique_url": "https://insight-centre.org;https://www.ucd.ie",
        "aff_unique_abbr": ";UCD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Dublin;",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Ireland"
    },
    {
        "id": "2022.acl-long.481",
        "title": "A Rationale-Centric Framework for Human-in-the-loop Machine Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a novel rational-centric framework with human-in-the-loop \u2013 Rationales-centric Double-robustness Learning (RDL) \u2013 to boost model out-of-distribution performance in few-shot learning scenarios. By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible \u201cinductive bias\u201d, exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation. Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests, especially for few-shot learning scenarios, compared to many state-of-the-art benchmarks. We also perform extensive ablation studies to support in-depth analyses of each component in our framework.",
        "author": "Jinghui Lu; Linyi Yang; Brian Namee; Yue Zhang",
        "authorids": "/j/jinghui-lu/; /l/linyi-yang/; /b/brian-namee/; /y/yue-zhang/",
        "bibtex": "@inproceedings{lu-etal-2022-rationale,\n    title = \"A Rationale-Centric Framework for Human-in-the-loop Machine Learning\",\n    author = \"Lu, Jinghui  and\n      Yang, Linyi  and\n      Namee, Brian  and\n      Zhang, Yue\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.481/\",\n    doi = \"10.18653/v1/2022.acl-long.481\",\n    pages = \"6986--6996\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.481.pdf",
        "site": "https://aclanthology.org/2022.acl-long.481/",
        "pdf_size": 391926,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12213284284880285532&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "The Insight Centre for Data Analytics, University College Dublin + School of Computer Science, University College Dublin + SenseTime Research; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; The Insight Centre for Data Analytics, University College Dublin + School of Computer Science, University College Dublin; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "ucd.ie;westlake.edu.cn;ucd.ie;westlake.edu.cn",
        "email": "ucd.ie;westlake.edu.cn;ucd.ie;westlake.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+1;2+3;0+0;2+3",
        "aff_unique_norm": "University College Dublin;SenseTime;Westlake University;Westlake Institute for Advanced Study",
        "aff_unique_dep": "The Insight Centre for Data Analytics;SenseTime Research;School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "https://www.ucd.ie;https://www.sensetime.com;https://www.westlake.edu.cn;http://www.wias.org.cn/",
        "aff_unique_abbr": "UCD;SenseTime;;WIAS",
        "aff_campus_unique_index": "1;;1;",
        "aff_campus_unique": ";Dublin",
        "aff_country_unique_index": "0+0+1;1+1;0+0;1+1",
        "aff_country_unique": "Ireland;China"
    },
    {
        "id": "2022.acl-short.94",
        "title": "A Recipe for Arbitrary Text Style Transfer with Large Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as \u2018make this melodramatic\u2019 or \u2018insert a metaphor.\u2019",
        "author": "Emily Reif; Daphne Ippolito; Ann Yuan; Andy Coenen; Chris Callison-Burch; Jason Wei",
        "authorids": "/e/emily-reif/; /d/daphne-ippolito/; /a/ann-yuan/; /a/andy-coenen/; /c/chris-callison-burch/; /j/jason-wei/",
        "bibtex": "@inproceedings{reif-etal-2022-recipe,\n    title = \"A Recipe for Arbitrary Text Style Transfer with Large Language Models\",\n    author = \"Reif, Emily  and\n      Ippolito, Daphne  and\n      Yuan, Ann  and\n      Coenen, Andy  and\n      Callison-Burch, Chris  and\n      Wei, Jason\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.94/\",\n    doi = \"10.18653/v1/2022.acl-short.94\",\n    pages = \"837--848\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.94.pdf",
        "site": "https://aclanthology.org/2022.acl-short.94/",
        "pdf_size": 610998,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4729037043546212755&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 6,
        "aff": "Google Research; Google Research + University of Pennsylvania; Google Research; Google Research; University of Pennsylvania; Google Research",
        "aff_domain": "google.com;seas.upenn.edu;google.com;google.com;seas.upenn.edu;google.com",
        "email": "google.com;seas.upenn.edu;google.com;google.com;seas.upenn.edu;google.com",
        "github": "",
        "project": "https://bit.ly/3fLDuci",
        "author_num": 6,
        "aff_unique_index": "0;0+1;0;0;1;0",
        "aff_unique_norm": "Google;University of Pennsylvania",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.upenn.edu",
        "aff_unique_abbr": "Google Research;UPenn",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.70",
        "title": "A Risk-Averse Mechanism for Suicidality Assessment on Social Media",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe consequences. We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale (C-SSRS). We propose SASI, a risk-averse and self-aware transformer-based hierarchical attention classifier, augmented to refrain from making uncertain predictions. We show that SASI is able to refrain from 83% of incorrect predictions on real-world Reddit data. Furthermore, we discuss the qualitative, practical, and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework.",
        "author": "Ramit Sawhney; Atula Neerkaje; Manas Gaur",
        "authorids": "/r/ramit-sawhney/; /a/atula-neerkaje/; /m/manas-gaur/",
        "bibtex": "@inproceedings{sawhney-etal-2022-risk,\n    title = \"A Risk-Averse Mechanism for Suicidality Assessment on Social Media\",\n    author = \"Sawhney, Ramit  and\n      Neerkaje, Atula  and\n      Gaur, Manas\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.70/\",\n    doi = \"10.18653/v1/2022.acl-short.70\",\n    pages = \"628--635\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.70.pdf",
        "site": "https://aclanthology.org/2022.acl-short.70/",
        "pdf_size": 497070,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15762299487302373379&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "AI Institute, University of South Carolina, SC, USA; Manipal Institute of Technology, Manipal, India; AI Institute, University of South Carolina, SC, USA",
        "aff_domain": "email.sc.edu;learner.manipal.edu;email.sc.edu",
        "email": "email.sc.edu;learner.manipal.edu;email.sc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of South Carolina;Manipal Institute of Technology",
        "aff_unique_dep": "AI Institute;",
        "aff_unique_url": "https://www.sc.edu;",
        "aff_unique_abbr": "USC;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Columbia;Manipal",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2022.findings-acl.22",
        "title": "A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE (CITATION).However, these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures. In this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), which is able to explore the pseudo-token space (i.e., latent semantic space) representation of a sentence while eliminating the impact of superficial features such as sentence length and syntax. Specifically, we introduce an additional pseudo token embedding layer independent of the BERT encoder to map each sentence into a sequence of pseudo tokens in a fixed length. Leveraging these pseudo sequences, we are able to construct same-length positive and negative pairs based on the attention mechanism to perform contrastive learning. In addition, we utilize both the gradient-updating and momentum-updating encoders to encode instances while dynamically maintaining an additional queue to store the representation of sentence embeddings, enhancing the encoder\u2019s learning performance for negative examples. Experiments show that our model outperforms the state-of-the-art baselines on six standard semantic textual similarity (STS) tasks. Furthermore, experiments on alignments and uniformity losses, as well as hard examples with different sentence lengths and syntax, consistently verify the effectiveness of our method.",
        "author": "Haochen Tan; Wei Shao; Han Wu; Ke Yang; Linqi Song",
        "authorids": "/h/haochen-tan/; /w/wei-shao/; /h/han-wu/; /k/ke-yang/; /l/linqi-song/",
        "bibtex": "@inproceedings{tan-etal-2022-sentence,\n    title = \"A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings\",\n    author = \"Tan, Haochen  and\n      Shao, Wei  and\n      Wu, Han  and\n      Yang, Ke  and\n      Song, Linqi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.22/\",\n    doi = \"10.18653/v1/2022.findings-acl.22\",\n    pages = \"246--256\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.22.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.22/",
        "pdf_size": 621326,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9984753764612620889&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "City University of Hong Kong Shenzhen Research Institute+Department of Computer Science, City University of Hong Kong; City University of Hong Kong Shenzhen Research Institute+Department of Computer Science, City University of Hong Kong; City University of Hong Kong Shenzhen Research Institute+Department of Computer Science, City University of Hong Kong; School of Integrated Circuits, Peking University; City University of Hong Kong Shenzhen Research Institute+Department of Computer Science, City University of Hong Kong",
        "aff_domain": "my.cityu.edu.hk; ; ; ;cityu.edu.hk",
        "email": "my.cityu.edu.hk; ; ; ;cityu.edu.hk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0+0;1;0+0",
        "aff_unique_norm": "City University of Hong Kong;Peking University",
        "aff_unique_dep": "Research Institute;School of Integrated Circuits",
        "aff_unique_url": "https://www.cityu.edu.hk;http://www.pku.edu.cn",
        "aff_unique_abbr": "CityU;PKU",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Shenzhen;Hong Kong SAR;",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.189",
        "title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Early exiting allows instances to exit at different layers according to the estimation of difficulty. Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such \u201clearn-to-exit\u201d modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient. HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models. Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
        "author": "Tianxiang Sun; Xiangyang Liu; Wei Zhu; Zhichao Geng; Lingling Wu; Yilong He; Yuan Ni; Guotong Xie; Xuanjing Huang; Xipeng Qiu",
        "authorids": "/t/tianxiang-sun/; /x/xiangyang-liu/; /w/wei-zhu/; /z/zhichao-geng/; /l/lingling-wu/; /y/yilong-he/; /y/yuan-ni/; /g/guotong-xie/; /x/xuan-jing-huang/; /x/xipeng-qiu/",
        "bibtex": "@inproceedings{sun-etal-2022-simple,\n    title = \"A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation\",\n    author = \"Sun, Tianxiang  and\n      Liu, Xiangyang  and\n      Zhu, Wei  and\n      Geng, Zhichao  and\n      Wu, Lingling  and\n      He, Yilong  and\n      Ni, Yuan  and\n      Xie, Guotong  and\n      Huang, Xuanjing  and\n      Qiu, Xipeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.189/\",\n    doi = \"10.18653/v1/2022.findings-acl.189\",\n    pages = \"2409--2421\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.189.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.189/",
        "pdf_size": 631649,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8209908373197861219&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; East China Normal University+Ping An Health and Technology Company Limited; School of Computer Science, Fudan University; School of Computer Science, Fudan University; Ping An Health and Technology Company Limited; Ping An Health and Technology Company Limited; Ping An Health and Technology Company Limited+Ping An Healthcare Technology+Ping An International Smart City Technology Company Limited; School of Computer Science, Fudan University; School of Computer Science, Fudan University+Peng Cheng Laboratory",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;1+2;0;0;2;2;2+3+4;0;0+5",
        "aff_unique_norm": "Fudan University;East China Normal University;Ping An Health and Technology Company Limited;Ping An Healthcare Technology;Ping An International Smart City Technology Company Limited;Pengcheng Laboratory",
        "aff_unique_dep": "School of Computer Science;;;;;Peng Cheng Laboratory",
        "aff_unique_url": "https://www.fudan.edu.cn;http://www.ecnu.edu.cn;https://www.pingan.com;https://www.pingan.com;;http://www.pcl.ac.cn",
        "aff_unique_abbr": "Fudan;ECNU;Ping An;PAHT;;PCL",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0;0+0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.57",
        "title": "A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity\u2019s output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures. Our code and models are publicly available at https://github.com/thunlp/PELT",
        "author": "Deming Ye; Yankai Lin; Peng Li; Maosong Sun; Zhiyuan Liu",
        "authorids": "/d/deming-ye/; /y/yankai-lin/; /p/peng-li/; /m/maosong-sun/; /z/zhiyuan-liu/",
        "bibtex": "@inproceedings{ye-etal-2022-simple,\n    title = \"A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models\",\n    author = \"Ye, Deming  and\n      Lin, Yankai  and\n      Li, Peng  and\n      Sun, Maosong  and\n      Liu, Zhiyuan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.57/\",\n    doi = \"10.18653/v1/2022.acl-short.57\",\n    pages = \"523--529\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.57.pdf",
        "site": "https://aclanthology.org/2022.acl-short.57/",
        "pdf_size": 1000855,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7060174359839364523&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University+Beijing National Research Center for Information Science and Technology; Pattern Recognition Center, WeChat AI; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University+Beijing National Research Center for Information Science and Technology+International Innovation Center of Tsinghua University+Jiangsu Collaborative Innovation Center for Language Ability+Institute Guo Qiang, Tsinghua University; Pattern Recognition Center, WeChat AI+Institute for AI Industry Research (AIR), Tsinghua University; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University+Beijing National Research Center for Information Science and Technology+International Innovation Center of Tsinghua University+Institute Guo Qiang, Tsinghua University",
        "aff_domain": "163.com; ; ;tsinghua.edu.cn; ",
        "email": "163.com; ; ;tsinghua.edu.cn; ",
        "github": "https://github.com/thunlp/PELT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;0+1+0+3+0;2+0;0+1+0+0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;WeChat AI;Jiangsu Collaborative Innovation Center for Language Ability",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;Pattern Recognition Center;Language Ability",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://wwwwechat.com;",
        "aff_unique_abbr": "THU;;WeChat AI;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0+0+0+0;0+0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.62",
        "title": "A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Few-Shot Relation Extraction aims at predicting the relation for a pair of entities in a sentence by training with a few labelled examples in each relation. Some recent works have introduced relation information (i.e., relation labels or descriptions) to assist model learning based on Prototype Network. However, most of them constrain the prototypes of each relation class implicitly with relation information, generally through designing complex network structures, like generating hybrid features, combining with contrastive learning or attention networks. We argue that relation information can be introduced more explicitly and effectively into the model. Thus, this paper proposes a direct addition approach to introduce relation information. Specifically, for each relation class, the relation representation is first generated by concatenating two views of relations (i.e., [CLS] token embedding and the mean value of embeddings of all tokens) and then directly added to the original prototype for both train and prediction. Experimental results on the benchmark dataset FewRel 1.0 show significant improvements and achieve comparable results to the state-of-the-art, which demonstrates the effectiveness of our proposed approach. Besides, further analyses verify that the direct addition is a much more effective way to integrate the relation representations and the original prototypes.",
        "author": "Yang Liu; Jinpeng Hu; Xiang Wan; Tsung-Hui Chang",
        "authorids": "/y/yang-liu-hk/; /j/jinpeng-hu/; /x/xiang-wan/; /t/tsung-hui-chang/",
        "bibtex": "@inproceedings{liu-etal-2022-simple,\n    title = \"A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction\",\n    author = \"Liu, Yang  and\n      Hu, Jinpeng  and\n      Wan, Xiang  and\n      Chang, Tsung-Hui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.62/\",\n    doi = \"10.18653/v1/2022.findings-acl.62\",\n    pages = \"757--763\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.62.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.62/",
        "pdf_size": 637613,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11376284836515615903&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 4,
        "aff": "The Chinese University of Hong Kong (Shenzhen)+Shenzhen Research Institute of Big Data; The Chinese University of Hong Kong (Shenzhen)+Shenzhen Research Institute of Big Data; Shenzhen Research Institute of Big Data+Pazhou Lab, Guangzhou, 510330, China; The Chinese University of Hong Kong (Shenzhen)+Shenzhen Research Institute of Big Data+Pazhou Lab, Guangzhou, 510330, China",
        "aff_domain": "link.cuhk.edu.cn;link.cuhk.edu.cn;sribd.cn;cuhk.edu.cn",
        "email": "link.cuhk.edu.cn;link.cuhk.edu.cn;sribd.cn;cuhk.edu.cn",
        "github": "https://github.com/lylylylylyly/SimpleFSRE",
        "project": "https://competitions.codalab.org/competitions/27980#results",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;1+2;0+1+2",
        "aff_unique_norm": "Chinese University of Hong Kong;Shenzhen Research Institute of Big Data;Pazhou Lab",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cuhk.edu.cn;http://www.sribd.cn;",
        "aff_unique_abbr": "CUHK;;",
        "aff_campus_unique_index": "0;0;2;0+2",
        "aff_campus_unique": "Shenzhen;;Guangzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.27",
        "title": "A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset includes a total of 40K dialogs and 500K utterances from four different domains: Chinese names, phone numbers, ID numbers and license plate numbers. The data is well annotated with sub-slot values, slot values, dialog states and actions. We find some new linguistic phenomena and interactive manners in SSTOD which raise critical challenges of building dialog agents for the task. We test three state-of-the-art dialog models on SSTOD and find they cannot handle the task well on any of the four domains. We also investigate an improved model by involving slot knowledge in a plug-in manner. More work should be done to meet the new challenges raised from SSTOD which widely exists in real-life applications. The dataset and code are publicly available via https://github.com/shunjiu/SSTOD.",
        "author": "Sai Zhang; Yuwei Hu; Yuchuan Wu; Jiaman Wu; Yongbin Li; Jian Sun; Caixia Yuan; Xiaojie Wang",
        "authorids": "/s/sai-zhang/; /y/yuwei-hu/; /y/yuchuan-wu/; /j/jiaman-wu/; /y/yongbin-li/; /j/jian-sun/; /c/caixia-yuan/; /x/xiaojie-wang/",
        "bibtex": "@inproceedings{zhang-etal-2022-slot,\n    title = \"A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\",\n    author = \"Zhang, Sai  and\n      Hu, Yuwei  and\n      Wu, Yuchuan  and\n      Wu, Jiaman  and\n      Li, Yongbin  and\n      Sun, Jian  and\n      Yuan, Caixia  and\n      Wang, Xiaojie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.27/\",\n    doi = \"10.18653/v1/2022.findings-acl.27\",\n    pages = \"309--321\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.27.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.27/",
        "pdf_size": 1023756,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9049644600246572834&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Independent Researcher; Independent Researcher; Independent Researcher; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn; ;outlook.com;gmail.com;hotmail.com;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn; ;outlook.com;gmail.com;hotmail.com;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/shunjiu/SSTOD",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;1;1;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Independent Researcher",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;",
        "aff_unique_abbr": "BUPT;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2022.acl-long.468",
        "title": "A Statutory Article Retrieval Dataset in French",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question. While recent advances in natural language processing have sparked considerable interest in many legal tasks, statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets. To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles. Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups. We find that fine-tuned dense retrieval models significantly outperform other systems. Our best performing baseline achieves 74.8% R@100, which is promising for the feasibility of the task and indicates there is still room for improvement. By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval. Our dataset and source code are publicly available.",
        "author": "Antoine Louis; Gerasimos Spanakis",
        "authorids": "/a/antoine-louis/; /g/gerasimos-spanakis/",
        "bibtex": "@inproceedings{louis-spanakis-2022-statutory,\n    title = \"A Statutory Article Retrieval Dataset in {F}rench\",\n    author = \"Louis, Antoine  and\n      Spanakis, Gerasimos\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.468/\",\n    doi = \"10.18653/v1/2022.acl-long.468\",\n    pages = \"6789--6803\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.468.pdf",
        "site": "https://aclanthology.org/2022.acl-long.468/",
        "pdf_size": 687716,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16548419480898067930&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Law & Tech Lab, Maastricht University; Law & Tech Lab, Maastricht University",
        "aff_domain": "maastrichtuniversity.nl;maastrichtuniversity.nl",
        "email": "maastrichtuniversity.nl;maastrichtuniversity.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Maastricht University",
        "aff_unique_dep": "Law & Tech Lab",
        "aff_unique_url": "https://www.maastrichtuniversity.nl",
        "aff_unique_abbr": "MU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.acl-long.211",
        "title": "A Taxonomy of Empathetic Questions in Social Dialogs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Effective question-asking is a crucial component of a successful conversational chatbot. It could help the bots manifest empathy and render the interaction more engaging by demonstrating attention to the speaker\u2019s emotions. However, current dialog generation approaches do not model this subtle emotion regulation technique due to the lack of a taxonomy of questions and their purpose in social chitchat. To address this gap, we have developed an empathetic question taxonomy (EQT), with special attention paid to questions\u2019 ability to capture communicative acts and their emotion-regulation intents. We further design a crowd-sourcing task to annotate a large subset of the EmpatheticDialogues dataset with the established labels. We use the crowd-annotated data to develop automatic labeling tools and produce labels for the whole dataset. Finally, we employ information visualization techniques to summarize co-occurrences of question acts and intents and their role in regulating interlocutor\u2019s emotion. These results reveal important question-asking strategies in social dialogs. The EQT classification scheme can facilitate computational analysis of questions in datasets. More importantly, it can inform future efforts in empathetic question generation using neural or hybrid methods.",
        "author": "Ekaterina Svikhnushina; Iuliana Voinea; Anuradha Welivita; Pearl Pu",
        "authorids": "/e/ekaterina-svikhnushina/; /i/iuliana-voinea/; /a/anuradha-welivita/; /p/pearl-pu/",
        "bibtex": "@inproceedings{svikhnushina-etal-2022-taxonomy,\n    title = \"A Taxonomy of Empathetic Questions in Social Dialogs\",\n    author = \"Svikhnushina, Ekaterina  and\n      Voinea, Iuliana  and\n      Welivita, Anuradha  and\n      Pu, Pearl\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.211/\",\n    doi = \"10.18653/v1/2022.acl-long.211\",\n    pages = \"2952--2973\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.211.pdf",
        "site": "https://aclanthology.org/2022.acl-long.211/",
        "pdf_size": 1351625,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11768454902698229753&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland; School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland; School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland; School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "github": "https://github.com/Sea94/EQT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "School of Computer and Communication Sciences",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lausanne",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2022.acl-long.464",
        "title": "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDeS (HAllucination DEtection dataSet). To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.",
        "author": "Tianyu Liu; Yizhe Zhang; Chris Brockett; Yi Mao; Zhifang Sui; Weizhu Chen; Bill Dolan",
        "authorids": "/t/tianyu-liu/; /y/yizhe-zhang/; /c/chris-brockett/; /y/yi-mao/; /z/zhifang-sui/; /w/weizhu-chen/; /w/william-b-dolan/",
        "bibtex": "@inproceedings{liu-etal-2022-token,\n    title = \"A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation\",\n    author = \"Liu, Tianyu  and\n      Zhang, Yizhe  and\n      Brockett, Chris  and\n      Mao, Yi  and\n      Sui, Zhifang  and\n      Chen, Weizhu  and\n      Dolan, Bill\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.464/\",\n    doi = \"10.18653/v1/2022.acl-long.464\",\n    pages = \"6723--6737\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.464.pdf",
        "site": "https://aclanthology.org/2022.acl-long.464/",
        "pdf_size": 2699499,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16118063797727852717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Peking University+Tencent Cloud Xiaowei; Meta AI; Microsoft Corporation; Microsoft Corporation; Peking University; Microsoft Corporation; Microsoft Corporation",
        "aff_domain": "pku.edu.cn;hotmail.com;microsoft.com;microsoft.com;pku.edu.cn;microsoft.com;microsoft.com",
        "email": "pku.edu.cn;hotmail.com;microsoft.com;microsoft.com;pku.edu.cn;microsoft.com;microsoft.com",
        "github": "https://github.com/microsoft/HaDes",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;3;3;0;3;3",
        "aff_unique_norm": "Peking University;Tencent;Meta;Microsoft",
        "aff_unique_dep": ";Tencent Cloud Xiaowei;Meta AI;Microsoft Corporation",
        "aff_unique_url": "http://www.pku.edu.cn;https://cloud.tencent.com;https://meta.com;https://www.microsoft.com",
        "aff_unique_abbr": "Peking U;Tencent;Meta;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;1;1;0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.114",
        "title": "A Transformational Biencoder with In-Domain Negative Sampling for Zero-Shot Entity Linking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent interest in entity linking has focused in the zero-shot scenario, where at test time the entity mention to be labelled is never seen during training, or may belong to a different domain from the source domain. Current work leverage pre-trained BERT with the implicit assumption that it bridges the gap between the source and target domain distributions. However, fine-tuned BERT has a considerable underperformance at zero-shot when applied in a different domain. We solve this problem by proposing a Transformational Biencoder that incorporates a transformation into BERT to perform a zero-shot transfer from the source domain during training. As like previous work, we rely on negative entities to encourage our model to discriminate the golden entities during training. To generate these negative entities, we propose a simple but effective strategy that takes the domain of the golden entity into perspective. Our experimental results on the benchmark dataset Zeshel show effectiveness of our approach and achieve new state-of-the-art.",
        "author": "Kai Sun; Richong Zhang; Samuel Mensah; Yongyi Mao; Xudong Liu",
        "authorids": "/k/kai-sun/; /r/richong-zhang/; /s/samuel-mensah/; /y/yongyi-mao/; /x/xudong-liu/",
        "bibtex": "@inproceedings{sun-etal-2022-transformational,\n    title = \"A Transformational Biencoder with In-Domain Negative Sampling for Zero-Shot Entity Linking\",\n    author = \"Sun, Kai  and\n      Zhang, Richong  and\n      Mensah, Samuel  and\n      Mao, Yongyi  and\n      Liu, Xudong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.114/\",\n    doi = \"10.18653/v1/2022.findings-acl.114\",\n    pages = \"1449--1458\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.114.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.114/",
        "pdf_size": 338548,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6385083023401629544&as_sdt=4005&sciodt=0,6&hl=en",
        "gs_version_total": 3,
        "aff": "SKLSDE, Beihang University, Beijing, China; SKLSDE, Beihang University, Beijing, China; Department of Computer Science, University of Sheffield, UK; School of Electrical Engineering and Computer Science, University of Ottawa, Canada; SKLSDE, Beihang University, Beijing, China",
        "aff_domain": "buaa.edu.cn;act.buaa.edu.cn;sheffield.ac.uk;uottawa.ca;act.buaa.edu.cn",
        "email": "buaa.edu.cn;act.buaa.edu.cn;sheffield.ac.uk;uottawa.ca;act.buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Beihang University;University of Sheffield;University of Ottawa",
        "aff_unique_dep": "SKLSDE;Department of Computer Science;School of Electrical Engineering and Computer Science",
        "aff_unique_url": "http://www.buaa.edu.cn;https://www.sheffield.ac.uk;https://www.uottawa.ca",
        "aff_unique_abbr": ";Sheffield;U Ottawa",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "China;United Kingdom;Canada"
    },
    {
        "id": "2022.acl-long.148",
        "title": "A Variational Hierarchical Model for Neural Cross-Lingual Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese). The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT&MS and CLS. Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective. However, it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize. To address this issue, we propose a hierarchical model for the CLS task, based on the conditional variational auto-encoder. The hierarchical model contains two kinds of latent variables at the local and global levels, respectively. At the local level, there are two latent variables, one for translation and the other for summarization. As for the global level, there is another latent variable for cross-lingual summarization conditioned on the two local-level variables. Experiments on two language directions (English-Chinese) verify the effectiveness and superiority of the proposed approach. In addition, we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting.",
        "author": "Yunlong Liang; Fandong Meng; Chulun Zhou; Jinan Xu; Yufeng Chen; Jinsong Su; Jie Zhou",
        "authorids": "/y/yunlong-liang/; /f/fandong-meng/; /c/chulun-zhou/; /j/jinan-xu/; /y/yufeng-chen/; /j/jinsong-su/; /j/jie-zhou/",
        "bibtex": "@inproceedings{liang-etal-2022-variational,\n    title = \"A Variational Hierarchical Model for Neural Cross-Lingual Summarization\",\n    author = \"Liang, Yunlong  and\n      Meng, Fandong  and\n      Zhou, Chulun  and\n      Xu, Jinan  and\n      Chen, Yufeng  and\n      Su, Jinsong  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.148/\",\n    doi = \"10.18653/v1/2022.acl-long.148\",\n    pages = \"2088--2099\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.148.pdf",
        "site": "https://aclanthology.org/2022.acl-long.148/",
        "pdf_size": 608027,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5842200948892103478&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; School of Informatics, Xiamen University; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, China; School of Informatics, Xiamen University; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "bjtu.edu.cn;tencent.com;stu.xmu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;xmu.edu.cn;tencent.com",
        "email": "bjtu.edu.cn;tencent.com;stu.xmu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;xmu.edu.cn;tencent.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;0;0;2;1",
        "aff_unique_norm": "Beijing Jiao Tong University;Tencent;Xiamen University",
        "aff_unique_dep": "Beijing Key Lab of Traffic Data Analysis and Mining;Pattern Recognition Center, WeChat AI;School of Informatics",
        "aff_unique_url": "http://www.bjtu.edu.cn;https://www.tencent.com;https://www.xmu.edu.cn",
        "aff_unique_abbr": ";Tencent;XMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.94",
        "title": "A Well-Composed Text is Half Done! Composition Sampling for Diverse Conditional Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose Composition Sampling, a simple but effective method to generate diverse outputs for conditional generation of higher quality compared to previous stochastic decoding strategies. It builds on recently proposed plan-based neural generation models (FROST, Narayan et al, 2021) that are trained to first create a composition of the output and then generate by conditioning on it and the input. Our approach avoids text degeneration by first sampling a composition in the form of an entity chain and then using beam search to generate the best possible text grounded to this entity chain. Experiments on summarization (CNN/DailyMail and XSum) and question generation (SQuAD), using existing and newly proposed automaticmetrics together with human-based evaluation, demonstrate that Composition Sampling is currently the best available decoding strategy for generating diverse meaningful outputs.",
        "author": "Shashi Narayan; Gon\u00e7alo Sim\u00f5es; Yao Zhao; Joshua Maynez; Dipanjan Das; Michael Collins; Mirella Lapata",
        "authorids": "/s/shashi-narayan/; /g/goncalo-simoes/; /y/yao-zhao/; /j/joshua-maynez/; /d/dipanjan-das/; /m/michael-collins/; /m/mirella-lapata/",
        "bibtex": "@inproceedings{narayan-etal-2022-well,\n    title = \"A Well-Composed Text is Half Done! Composition Sampling for Diverse Conditional Generation\",\n    author = \"Narayan, Shashi  and\n      Sim{\\~o}es, Gon{\\c{c}}alo  and\n      Zhao, Yao  and\n      Maynez, Joshua  and\n      Das, Dipanjan  and\n      Collins, Michael  and\n      Lapata, Mirella\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.94/\",\n    doi = \"10.18653/v1/2022.acl-long.94\",\n    pages = \"1319--1339\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.94.pdf",
        "site": "https://aclanthology.org/2022.acl-long.94/",
        "pdf_size": 629823,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11952710136529295691&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Google Research; Google Research; Google Brain; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.515",
        "title": "ABC: Attention with Bounded-memory Control",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.",
        "author": "Hao Peng; Jungo Kasai; Nikolaos Pappas; Dani Yogatama; Zhaofeng Wu; Lingpeng Kong; Roy Schwartz; Noah A. Smith",
        "authorids": "/h/hao-peng/; /j/jungo-kasai/; /n/nikolaos-pappas/; /d/dani-yogatama/; /z/zhaofeng-wu/; /l/lingpeng-kong/; /r/roy-schwartz/; /n/noah-a-smith/",
        "bibtex": "@inproceedings{peng-etal-2022-abc,\n    title = \"{ABC}: Attention with Bounded-memory Control\",\n    author = \"Peng, Hao  and\n      Kasai, Jungo  and\n      Pappas, Nikolaos  and\n      Yogatama, Dani  and\n      Wu, Zhaofeng  and\n      Kong, Lingpeng  and\n      Schwartz, Roy  and\n      Smith, Noah A.\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.515/\",\n    doi = \"10.18653/v1/2022.acl-long.515\",\n    pages = \"7469--7483\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.515.pdf",
        "site": "https://aclanthology.org/2022.acl-long.515/",
        "pdf_size": 610801,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10127078228209742188&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 8,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Amazon Web Services; DeepMind; Allen Institute for Artificial Intelligence; Department of Computer Science, The University of Hong Kong; School of Computer Science & Engineering, Hebrew University of Jerusalem; Paul G. Allen School of Computer Science & Engineering, University of Washington + Allen Institute for Artificial Intelligence",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu;deepmind.com;cs.washington.edu;cs.hku.hk;mail.huji.ac.il;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu;deepmind.com;cs.washington.edu;cs.hku.hk;mail.huji.ac.il;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;2;3;4;5;0+3",
        "aff_unique_norm": "University of Washington;Amazon;DeepMind;Allen Institute for Artificial Intelligence;University of Hong Kong;Hebrew University of Jerusalem",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;Amazon Web Services;;;Department of Computer Science;School of Computer Science & Engineering",
        "aff_unique_url": "https://www.washington.edu;https://aws.amazon.com;https://deepmind.com;https://allenai.org;https://www.hku.hk;https://www.huji.ac.il",
        "aff_unique_abbr": "UW;AWS;DeepMind;AI2;HKU;HUJI",
        "aff_campus_unique_index": "0;0;2;3;0",
        "aff_campus_unique": "Seattle;;Hong Kong SAR;Jerusalem",
        "aff_country_unique_index": "0;0;0;1;0;2;3;0+0",
        "aff_country_unique": "United States;United Kingdom;China;Israel"
    },
    {
        "id": "2022.findings-acl.244",
        "title": "AMR-DA: Data Augmentation by Abstract Meaning Representation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Abstract Meaning Representation (AMR) is a semantic representation for NLP/NLU. In this paper, we propose to use it for data augmentation in NLP. Our proposed data augmentation technique, called AMR-DA, converts a sample sentence to an AMR graph, modifies the graph according to various data augmentation policies, and then generates augmentations from graphs. Our method combines both sentence-level techniques like back translation and token-level techniques like EDA (Easy Data Augmentation). To evaluate the effectiveness of our method, we apply it to the tasks of semantic textual similarity (STS) and text classification. For STS, our experiments show that AMR-DA boosts the performance of the state-of-the-art models on several STS benchmarks. For text classification, AMR-DA outperforms EDA and AEDA and leads to more robust improvements.",
        "author": "Ziyi Shou; Yuxin Jiang; Fangzhen Lin",
        "authorids": "/z/ziyi-shou/; /y/yuxin-jiang/; /f/fangzhen-lin/",
        "bibtex": "@inproceedings{shou-etal-2022-amr,\n    title = \"{AMR-DA}: {D}ata Augmentation by {A}bstract {M}eaning {R}epresentation\",\n    author = \"Shou, Ziyi  and\n      Jiang, Yuxin  and\n      Lin, Fangzhen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.244/\",\n    doi = \"10.18653/v1/2022.findings-acl.244\",\n    pages = \"3082--3098\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.244.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.244/",
        "pdf_size": 913034,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4279878931061228057&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Engineering; DSA Thrust, Information Hub; Department of Computer Science and Engineering",
        "aff_domain": "cse.ust.hk;connect.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;connect.ust.hk;cse.ust.hk",
        "github": "https://github.com/zzshou/amr-data-augmentation",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;DSA Thrust",
        "aff_unique_dep": "Department of Computer Science and Engineering;Information Hub",
        "aff_unique_url": "https://cse.ucsd.edu;",
        "aff_unique_abbr": "UCSD CSE;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2022.findings-acl.193",
        "title": "ASCM: An Answer Space Clustered Prompting Method without Answer Engineering",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prompt-based learning, which exploits knowledge from pre-trained language models by providing textual prompts and designing appropriate answer-category mapping methods, has achieved impressive successes on few-shot text classification and natural language inference (NLI). Because of the diverse linguistic expression, there exist many answer tokens for the same category. However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance. To address this issue, we propose an answer space clustered prompting model (ASCM) together with a synonym initialization method (SI) which automatically categorizes all answer tokens in a semantic-clustered embedding space. We also propose a stable semi-supervised method named stair learning (SL) that orderly distills knowledge from better models to weaker models. Extensive experiments demonstrate that our ASCM+SL significantly outperforms existing state-of-the-art techniques in few-shot settings.",
        "author": "Zhen Wang; Yating Yang; Zhou Xi; Bo Ma; Lei Wang; Rui Dong; Azmat Anwar",
        "authorids": "/z/zhen-wang/; /y/yating-yang/; /z/zhou-xi/; /b/bo-ma/; /l/lei-wang/; /r/rui-dong/; /a/azmat-anwar/",
        "bibtex": "@inproceedings{wang-etal-2022-ascm,\n    title = \"{ASCM}: An Answer Space Clustered Prompting Method without Answer Engineering\",\n    author = \"Wang, Zhen  and\n      Yang, Yating  and\n      Xi, Zhou  and\n      Ma, Bo  and\n      Wang, Lei  and\n      Dong, Rui  and\n      Anwar, Azmat\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.193/\",\n    doi = \"10.18653/v1/2022.findings-acl.193\",\n    pages = \"2455--2469\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.193.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.193/",
        "pdf_size": 828238,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14831467785581702913&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 4,
        "aff": "Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences+University of Chinese Academy of Sciences+Xinjiang Laboratory of Minority Speech and Language Information Processing; Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences+University of Chinese Academy of Sciences+Xinjiang Laboratory of Minority Speech and Language Information Processing; Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences+University of Chinese Academy of Sciences+Xinjiang Laboratory of Minority Speech and Language Information Processing; Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences+University of Chinese Academy of Sciences+Xinjiang Laboratory of Minority Speech and Language Information Processing; Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences+University of Chinese Academy of Sciences+Xinjiang Laboratory of Minority Speech and Language Information Processing; Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences+University of Chinese Academy of Sciences+Xinjiang Laboratory of Minority Speech and Language Information Processing; Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences+Xinjiang Laboratory of Minority Speech and Language Information Processing",
        "aff_domain": "ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn",
        "email": "ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn;ms.xjb.ac.cn",
        "github": "https://github.com/miaomiao1215/ASCM",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1+2;0+1+2;0+1+2;0+1+2;0+1+2;0+1+2;0+2",
        "aff_unique_norm": "Xinjiang Technical Institute of Physics & Chemistry;University of Chinese Academy of Sciences;Xinjiang University",
        "aff_unique_dep": "Chinese Academy of Sciences;;Laboratory of Minority Speech and Language Information Processing",
        "aff_unique_url": ";http://www.ucas.ac.cn;http://www.xju.edu.cn/",
        "aff_unique_abbr": ";UCAS;",
        "aff_campus_unique_index": ";;;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.449",
        "title": "ASPECTNEWS: Aspect-Oriented Summarization of News Documents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generic summaries try to cover an entire document and query-based summaries try to answer document-specific questions. But real users\u2019 needs often fall in between these extremes and correspond to aspects, high-level topics discussed among similar types of documents. In this paper, we collect a dataset of realistic aspect-oriented summaries, AspectNews, which covers different subtopics about articles in news sub-domains. We annotate data across two domains of articles, earthquakes and fraud investigations, where each article is annotated with two distinct summaries focusing on different aspects for each domain. A system producing a single generic summary cannot concisely satisfy both aspects. Our focus in evaluation is how well existing techniques can generalize to these domains without seeing in-domain training data, so we turn to techniques to construct synthetic training data that have been used in query-focused summarization work. We compare several training schemes that differ in how strongly keywords are used and how oracle summaries are extracted. Our evaluation shows that our final approach yields (a) focused summaries, better than those from a generic summarization system or from keyword matching; (b) a system sensitive to the choice of keywords.",
        "author": "Ojas Ahuja; Jiacheng Xu; Akshay Gupta; Kevin Horecka; Greg Durrett",
        "authorids": "/o/ojas-ahuja/; /j/jiacheng-xu/; /a/akshay-gupta/; /k/kevin-horecka/; /g/greg-durrett/",
        "bibtex": "@inproceedings{ahuja-etal-2022-aspectnews,\n    title = \"{ASPECTNEWS}: Aspect-Oriented Summarization of News Documents\",\n    author = \"Ahuja, Ojas  and\n      Xu, Jiacheng  and\n      Gupta, Akshay  and\n      Horecka, Kevin  and\n      Durrett, Greg\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.449/\",\n    doi = \"10.18653/v1/2022.acl-long.449\",\n    pages = \"6494--6506\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.449.pdf",
        "site": "https://aclanthology.org/2022.acl-long.449/",
        "pdf_size": 408633,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5844642394882379286&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; Walmart NexTech; The University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu; ; ;cs.utexas.edu",
        "email": "utexas.edu;utexas.edu; ; ;cs.utexas.edu",
        "github": "https://github.com/oja/aosummexample",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Texas at Austin;Walmart",
        "aff_unique_dep": ";NexTech",
        "aff_unique_url": "https://www.utexas.edu;https://www.walmart.com",
        "aff_unique_abbr": "UT Austin;Walmart",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.214",
        "title": "ASSIST: Towards Label Noise-Robust Dialogue State Tracking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state tracking (DST). However, substantial noise has been discovered in its state annotations. Such noise brings about huge challenges for training DST models robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have been published recently, there are still lots of noisy labels, especially in the training set. Besides, it is costly to rectify all the problematic annotations. In this paper, instead of improving the annotation quality further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt dIalogue State Tracking), to train DST models robustly from noisy labels. ASSIST first generates pseudo labels for each sample in the training set by using an auxiliary model trained on a small clean dataset, then puts the generated pseudo labels and vanilla noisy labels together to train the primary model. We show the validity of ASSIST theoretically. Experimental results also demonstrate that ASSIST improves the joint goal accuracy of DST by up to 28.16% on MultiWOZ 2.0 and 8.41% on MultiWOZ 2.4, compared to using only the vanilla noisy labels.",
        "author": "Fanghua Ye; Yue Feng; Emine Yilmaz",
        "authorids": "/f/fanghua-ye/; /y/yue-feng/; /e/emine-yilmaz/",
        "bibtex": "@inproceedings{ye-etal-2022-assist,\n    title = \"{ASSIST}: Towards Label Noise-Robust Dialogue State Tracking\",\n    author = \"Ye, Fanghua  and\n      Feng, Yue  and\n      Yilmaz, Emine\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.214/\",\n    doi = \"10.18653/v1/2022.findings-acl.214\",\n    pages = \"2719--2731\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.214.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.214/",
        "pdf_size": 496258,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17700014320719490445&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University College London; University College London; University College London",
        "aff_domain": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk",
        "email": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.19",
        "title": "AbductionRules: Training Transformers to Explain Unexpected Inputs",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning - inference to the best explanation of an unexpected observation - has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model interpretability. This paper presents AbductionRules, a group of natural language datasets designed to train and test generalisable abduction over natural-language knowledge bases. We use these datasets to finetune pretrained Transformers and discuss their performance, finding that our models learned generalisable abductive techniques but also learned to exploit the structure of our data. Finally, we discuss the viability of this approach to abductive reasoning and ways in which it may be improved in future work.",
        "author": "Nathan Young; Qiming Bao; Joshua Bensemann; Michael Witbrock",
        "authorids": "/n/nathan-young/; /q/qiming-bao/; /j/joshua-bensemann/; /m/michael-j-witbrock/",
        "bibtex": "@inproceedings{young-etal-2022-abductionrules,\n    title = \"{A}bduction{R}ules: Training Transformers to Explain Unexpected Inputs\",\n    author = \"Young, Nathan  and\n      Bao, Qiming  and\n      Bensemann, Joshua  and\n      Witbrock, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.19/\",\n    doi = \"10.18653/v1/2022.findings-acl.19\",\n    pages = \"218--227\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.19.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.19/",
        "pdf_size": 189566,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4264339560214980315&as_sdt=8005&sciodt=0,7&hl=en",
        "gs_version_total": 5,
        "aff": "Strong AI Lab, School of Computer Science, University of Auckland; Strong AI Lab, School of Computer Science, University of Auckland; Strong AI Lab, School of Computer Science, University of Auckland; Strong AI Lab, School of Computer Science, University of Auckland",
        "aff_domain": "auckland.ac.nz;aucklanduni.ac.nz;auckland.ac.nz;auckland.ac.nz",
        "email": "auckland.ac.nz;aucklanduni.ac.nz;auckland.ac.nz;auckland.ac.nz",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Auckland",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.auckland.ac.nz",
        "aff_unique_abbr": "UoA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "2022.acl-long.181",
        "title": "Accelerating Code Search with Deep Hashing and Code Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Code search is to search reusable code snippets from source code corpus based on natural languages queries. Deep learning-based methods on code search have shown promising results. However, previous methods focus on retrieval accuracy, but lacked attention to the efficiency of the retrieval process. We propose a novel method CoSHC to accelerate code search with deep hashing and code classification, aiming to perform efficient code search without sacrificing too much accuracy. To evaluate the effectiveness of CoSHC, we apply our methodon five code search models. Extensive experimental results indicate that compared with previous code search baselines, CoSHC can save more than 90% of retrieval time meanwhile preserving at least 99% of retrieval accuracy.",
        "author": "Wenchao Gu; Yanlin Wang; Lun Du; Hongyu Zhang; Shi Han; Dongmei Zhang; Michael Lyu",
        "authorids": "/w/wenchao-gu/; /y/yanlin-wang/; /l/lun-du/; /h/hongyu-zhang/; /s/shi-han/; /d/dongmei-zhang/; /m/michael-lyu/",
        "bibtex": "@inproceedings{gu-etal-2022-accelerating,\n    title = \"Accelerating Code Search with Deep Hashing and Code Classification\",\n    author = \"Gu, Wenchao  and\n      Wang, Yanlin  and\n      Du, Lun  and\n      Zhang, Hongyu  and\n      Han, Shi  and\n      Zhang, Dongmei  and\n      Lyu, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.181/\",\n    doi = \"10.18653/v1/2022.acl-long.181\",\n    pages = \"2534--2544\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.181.pdf",
        "site": "https://aclanthology.org/2022.acl-long.181/",
        "pdf_size": 432319,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4830519139899403922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, The Chinese University of Hong Kong, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; The University of Newcastle, Australia; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, China",
        "aff_domain": "cse.cuhk.edu.hk;microsoft.com; ; ; ; ; ",
        "email": "cse.cuhk.edu.hk;microsoft.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;2;1;1;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Microsoft;University of Newcastle",
        "aff_unique_dep": "Department of Computer Science and Engineering;Research;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.microsoft.com/en-us/research/group/asia;https://www.newcastle.edu.au",
        "aff_unique_abbr": "CUHK;MSRA;UON",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;1;0;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.acl-long.460",
        "title": "Accurate Online Posterior Alignments for Principled Lexically-Constrained Decoding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Online alignment in machine translation refers to the task of aligning a target word to a source word when the target sequence has only been partially decoded. Good online alignments facilitate important applications such as lexically constrained translation where user-defined dictionaries are used to inject lexical constraints into the translation model. We propose a novel posterior alignment technique that is truly online in its execution and superior in terms of alignment error rates compared to existing methods. Our proposed inference technique jointly considers alignment and token probabilities in a principled manner and can be seamlessly integrated within existing constrained beam-search decoding algorithms. On five language pairs, including two distant language pairs, we achieve consistent drop in alignment error rates. When deployed on seven lexically constrained translation tasks, we achieve significant improvements in BLEU specifically around the constrained positions.",
        "author": "Soumya Chatterjee; Sunita Sarawagi; Preethi Jyothi",
        "authorids": "/s/soumya-chatterjee/; /s/sunita-sarawagi/; /p/preethi-jyothi/",
        "bibtex": "@inproceedings{chatterjee-etal-2022-accurate,\n    title = \"Accurate Online Posterior Alignments for Principled Lexically-Constrained Decoding\",\n    author = \"Chatterjee, Soumya  and\n      Sarawagi, Sunita  and\n      Jyothi, Preethi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.460/\",\n    doi = \"10.18653/v1/2022.acl-long.460\",\n    pages = \"6675--6689\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.460.pdf",
        "site": "https://aclanthology.org/2022.acl-long.460/",
        "pdf_size": 446102,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12064434358398066544&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IIT Bombay; IIT Bombay; IIT Bombay",
        "aff_domain": "cse.iitb.ac.in;iitb.ac.in;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;iitb.ac.in;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IITB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mumbai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.224",
        "title": "Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A limitation of current neural dialog models is that they tend to suffer from a lack of specificity and informativeness in generated responses, primarily due to dependence on training data that covers a limited variety of scenarios and conveys limited knowledge. One way to alleviate this issue is to extract relevant knowledge from external sources at decoding time and incorporate it into the dialog response. In this paper, we propose a post-hoc knowledge-injection technique where we first retrieve a diverse set of relevant knowledge snippets conditioned on both the dialog history and an initial response from an existing dialog model. We construct multiple candidate responses, individually injecting each retrieved snippet into the initial response using a gradient-based decoding method, and then select the final response with an unsupervised ranking step. Our experiments in goal-oriented and knowledge-grounded dialog settings demonstrate that human annotators judge the outputs from the proposed method to be more engaging and informative compared to responses from prior dialog systems. We further show that knowledge-augmentation promotes success in achieving conversational goals in both experimental settings.",
        "author": "Bodhisattwa Prasad Majumder; Harsh Jhamtani; Taylor Berg-Kirkpatrick; Julian McAuley",
        "authorids": "/b/bodhisattwa-prasad-majumder/; /h/harsh-jhamtani/; /t/taylor-berg-kirkpatrick/; /j/julian-mcauley/",
        "bibtex": "@inproceedings{majumder-etal-2022-achieving,\n    title = \"Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection\",\n    author = \"Majumder, Bodhisattwa Prasad  and\n      Jhamtani, Harsh  and\n      Berg-Kirkpatrick, Taylor  and\n      McAuley, Julian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.224/\",\n    doi = \"10.18653/v1/2022.acl-long.224\",\n    pages = \"3140--3153\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.224.pdf",
        "site": "https://aclanthology.org/2022.acl-long.224/",
        "pdf_size": 3052057,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YkETt1d4rZkJ:scholar.google.com/&scioq=Achieving+Conversational+Goals+with+Unsupervised+Post-hoc+Knowledge+Injection&hl=en&as_sdt=0,44",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, UC San Diego; School of Computer Science, Carnegie Mellon University; Department of Computer Science and Engineering, UC San Diego; Department of Computer Science and Engineering, UC San Diego",
        "aff_domain": "eng.ucsd.edu;cs.cmu.edu;eng.ucsd.edu;eng.ucsd.edu",
        "email": "eng.ucsd.edu;cs.cmu.edu;eng.ucsd.edu;eng.ucsd.edu",
        "github": "https://github.com/majumderb/poki3140",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, San Diego;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science and Engineering;School of Computer Science",
        "aff_unique_url": "https://www.ucsd.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UCSD;CMU",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "San Diego;Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.445",
        "title": "Achieving Reliable Human Assessment of Open-Domain Dialogue Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Evaluation of open-domain dialogue systems is highly challenging and development of better techniques is highlighted time and again as desperately needed. Despite substantial efforts to carry out reliable live evaluation of systems in recent competitions, annotations have been abandoned and reported as too unreliable to yield sensible results. This is a serious problem since automatic metrics are not known to provide a good indication of what may or may not be a high-quality conversation. Answering the distress call of competitions that have emphasized the urgent need for better evaluation techniques in dialogue, we present the successful development of human evaluation that is highly reliable while still remaining feasible and low cost. Self-replication experiments reveal almost perfectly repeatable results with a correlation of r=0.969. Furthermore, due to the lack of appropriate methods of statistical significance testing, the likelihood of potential improvements to systems occurring due to chance is rarely taken into account in dialogue evaluation, and the evaluation we propose facilitates application of standard tests. Since we have developed a highly reliable evaluation method, new insights into system performance can be revealed. We therefore include a comparison of state-of-the-art models (i) with and without personas, to measure the contribution of personas to conversation quality, as well as (ii) prescribed versus freely chosen topics. Interestingly with respect to personas, results indicate that personas do not positively contribute to conversation quality as expected.",
        "author": "Tianbo Ji; Yvette Graham; Gareth Jones; Chenyang Lyu; Qun Liu",
        "authorids": "/t/tianbo-ji/; /y/yvette-graham/; /g/gareth-jones/; /c/chenyang-lyu/; /q/qun-liu/",
        "bibtex": "@inproceedings{ji-etal-2022-achieving,\n    title = \"Achieving Reliable Human Assessment of Open-Domain Dialogue Systems\",\n    author = \"Ji, Tianbo  and\n      Graham, Yvette  and\n      Jones, Gareth  and\n      Lyu, Chenyang  and\n      Liu, Qun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.445/\",\n    doi = \"10.18653/v1/2022.acl-long.445\",\n    pages = \"6416--6437\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.445.pdf",
        "site": "https://aclanthology.org/2022.acl-long.445/",
        "pdf_size": 1409607,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1546663767009559959&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "ADAPT Centre + School of Computing, Dublin City University; ADAPT Centre + School of Computer Science and Statistics, Trinity College Dublin; ADAPT Centre + School of Computing, Dublin City University; School of Computing, Dublin City University; Noah\u2019s Ark Lab, Huawei",
        "aff_domain": "adaptcentre.ie;adaptcentre.ie;adaptcentre.ie;mail.dcu.ie;huawei.com",
        "email": "adaptcentre.ie;adaptcentre.ie;adaptcentre.ie;mail.dcu.ie;huawei.com",
        "github": "https://github.com/TianboJi/Dialogue-Eval6416",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+2;0+1;1;3",
        "aff_unique_norm": "ADAPT Centre;Dublin City University;Trinity College Dublin;Huawei",
        "aff_unique_dep": ";School of Computing;School of Computer Science and Statistics;Noah\u2019s Ark Lab",
        "aff_unique_url": "https://adaptcentre.ie;https://www.dcu.ie;https://www.tcd.ie;https://www.huawei.com",
        "aff_unique_abbr": "ADAPT;DCU;TCD;Huawei",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Dublin",
        "aff_country_unique_index": "0+0;0+0;0+0;0;1",
        "aff_country_unique": "Ireland;China"
    },
    {
        "id": "2022.acl-long.600",
        "title": "Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Recent studies have shown the advantages of evaluating NLG systems using pairwise comparisons as opposed to direct assessment. Given k systems, a naive approach for identifying the top-ranked system would be to uniformly obtain pairwise comparisons from all k \\choose 2 pairs of systems. However, this can be very expensive as the number of human annotations required would grow quadratically with k. In this work, we introduce Active Evaluation, a framework to efficiently identify the top-ranked system by actively choosing system pairs for comparison using dueling bandit algorithms. We perform extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation datasets spanning 5 tasks and show that the number of human annotations can be reduced by 80%. To further reduce the number of human annotations, we propose model-based dueling bandit algorithms which combine automatic evaluation metrics with human evaluations. Specifically, we eliminate sub-optimal systems even before the human annotation process and perform human evaluations only on test examples where the automatic metric is highly uncertain. This reduces the number of human annotations required further by 89%. In effect, we show that identifying the top-ranked system requires only a few hundred human annotations, which grow linearly with k. Lastly, we provide practical recommendations and best practices to identify the top-ranked system efficiently. Our code has been made publicly available at https://github.com/akashkm99/duelnlg",
        "author": "Akash Kumar Mohankumar; Mitesh Khapra",
        "authorids": "/a/akash-kumar-mohankumar/; /m/mitesh-m-khapra/",
        "bibtex": "@inproceedings{mohankumar-khapra-2022-active,\n    title = \"Active Evaluation: Efficient {NLG} Evaluation with Few Pairwise Comparisons\",\n    author = \"Mohankumar, Akash Kumar  and\n      Khapra, Mitesh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.600/\",\n    doi = \"10.18653/v1/2022.acl-long.600\",\n    pages = \"8761--8781\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.600.pdf",
        "site": "https://aclanthology.org/2022.acl-long.600/",
        "pdf_size": 1056114,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=149968628101084019&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Microsoft Bangalore, India + Indian Institute of Technology Madras RBCDSAI, IIT Madras; Indian Institute of Technology Madras RBCDSAI, IIT Madras",
        "aff_domain": "microsoft.com;cse.iitm.ac.in",
        "email": "microsoft.com;cse.iitm.ac.in",
        "github": "https://github.com/akashkm99/duelnlg",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "Microsoft;Indian Institute of Technology Madras",
        "aff_unique_dep": "Microsoft;RBCDSAI",
        "aff_unique_url": "https://www.microsoft.com;https://www.iitm.ac.in",
        "aff_unique_abbr": "Microsoft;IIT Madras",
        "aff_campus_unique_index": "0+1;1",
        "aff_campus_unique": "Bangalore;Madras",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.494",
        "title": "AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent machine reading comprehension datasets such as ReClor and LogiQA require performing logical reasoning over text. Conventional neural models are insufficient for logical reasoning, while symbolic reasoners cannot directly apply to text. To meet the challenge, we present a neural-symbolic approach which, to predict an answer, passes messages over a graph representing logical relations between text units. It incorporates an adaptive logic graph network (AdaLoGN) which adaptively infers logical relations to extend the graph and, essentially, realizes mutual and iterative reinforcement between neural and symbolic reasoning. We also implement a novel subgraph-to-node message passing mechanism to enhance context-option interaction for answering multiple-choice questions. Our approach shows promising results on ReClor and LogiQA.",
        "author": "Xiao Li; Gong Cheng; Ziheng Chen; Yawei Sun; Yuzhong Qu",
        "authorids": "/x/xiao-li/; /g/gong-cheng/; /z/ziheng-chen/; /y/yawei-sun/; /y/yuzhong-qu/",
        "bibtex": "@inproceedings{li-etal-2022-adalogn,\n    title = \"{A}da{L}o{GN}: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension\",\n    author = \"Li, Xiao  and\n      Cheng, Gong  and\n      Chen, Ziheng  and\n      Sun, Yawei  and\n      Qu, Yuzhong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.494/\",\n    doi = \"10.18653/v1/2022.acl-long.494\",\n    pages = \"7147--7161\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.494.pdf",
        "site": "https://aclanthology.org/2022.acl-long.494/",
        "pdf_size": 2346502,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18237045656371630233&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",
        "aff_domain": "smail.nju.edu.cn;nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.1",
        "title": "AdapLeR: Speeding up Inference by Adaptive Length Reduction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models have shown stellar performance in various downstream tasks. But, this usually comes at the cost of high latency and computation, hindering their usage in resource-limited settings. In this work, we propose a novel approach for reducing the computational cost of BERT with minimal loss in downstream performance. Our method dynamically eliminates less contributing tokens through layers, resulting in shorter lengths and consequently lower computational cost. To determine the importance of each token representation, we train a Contribution Predictor for each layer using a gradient-based saliency method. Our experiments on several diverse classification tasks show speedups up to 22x during inference time without much sacrifice in performance. We also validate the quality of the selected tokens in our method using human annotations in the ERASER benchmark. In comparison to other widely used strategies for selecting important tokens, such as saliency and attention, our proposed method has a significantly lower false positive rate in generating rationales. Our code is freely available at https://github.com/amodaresi/AdapLeR.",
        "author": "Ali Modarressi; Hosein Mohebbi; Mohammad Taher Pilehvar",
        "authorids": "/a/ali-modarressi/; /h/hosein-mohebbi/; /m/mohammad-taher-pilehvar/",
        "bibtex": "@inproceedings{modarressi-etal-2022-adapler,\n    title = \"{A}dap{L}e{R}: Speeding up Inference by Adaptive Length Reduction\",\n    author = \"Modarressi, Ali  and\n      Mohebbi, Hosein  and\n      Pilehvar, Mohammad Taher\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.1/\",\n    doi = \"10.18653/v1/2022.acl-long.1\",\n    pages = \"1--15\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.1.pdf",
        "site": "https://aclanthology.org/2022.acl-long.1/",
        "pdf_size": 1692369,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17617732007081733939&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Iran University of Science and Technology, Iran \u22c6; Cognitive Science and AI, Tilburg University, Netherlands \u22c6\u2020; Tehran Institute for Advanced Studies, Khatam University, Iran",
        "aff_domain": "comp.iust.ac.ir;uvt.nl;cam.ac.uk",
        "email": "comp.iust.ac.ir;uvt.nl;cam.ac.uk",
        "github": "https://github.com/amodaresi/AdapLeR",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Iran University of Science and Technology;Tilburg University;Tehran Institute for Advanced Studies",
        "aff_unique_dep": ";Cognitive Science and AI;",
        "aff_unique_url": ";https://www.tilburguniversity.edu/;",
        "aff_unique_abbr": ";Tilburg U;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Iran;Netherlands"
    },
    {
        "id": "2022.acl-long.519",
        "title": "Adapting Coreference Resolution Models through Active Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural coreference resolution models trained on one dataset may not transfer to new, low-resource domains. Active learning mitigates this problem by sampling a small subset of data for annotators to label. While active learning is well-defined for classification tasks, its application to coreference resolution is neither well-defined nor fully understood. This paper explores how to actively label coreference, examining sources of model uncertainty and document reading costs. We compare uncertainty sampling strategies and their advantages through thorough error analysis. In both synthetic and human experiments, labeling spans within the same document is more effective than annotating spans across documents. The findings contribute to a more realistic development of coreference resolution models.",
        "author": "Michelle Yuan; Patrick Xia; Chandler May; Benjamin Van Durme; Jordan Boyd-Graber",
        "authorids": "/m/michelle-yuan/; /p/patrick-xia/; /c/chandler-may/; /b/benjamin-van-durme/; /j/jordan-boyd-graber/",
        "bibtex": "@inproceedings{yuan-etal-2022-adapting,\n    title = \"Adapting Coreference Resolution Models through Active Learning\",\n    author = \"Yuan, Michelle  and\n      Xia, Patrick  and\n      May, Chandler  and\n      Van Durme, Benjamin  and\n      Boyd-Graber, Jordan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.519/\",\n    doi = \"10.18653/v1/2022.acl-long.519\",\n    pages = \"7533--7549\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.519.pdf",
        "site": "https://aclanthology.org/2022.acl-long.519/",
        "pdf_size": 2188723,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3273818669707971039&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Maryland; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; University of Maryland",
        "aff_domain": "cs.umd.edu;cs.jhu.edu; ; ;umiacs.umd.edu",
        "email": "cs.umd.edu;cs.jhu.edu; ; ;umiacs.umd.edu",
        "github": "https://github.com/forest-snow/incremental-coref7533",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Maryland;Johns Hopkins University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.jhu.edu",
        "aff_unique_abbr": "UMD;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.230",
        "title": "Adaptive Testing and Debugging of NLP Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.",
        "author": "Marco Tulio Ribeiro; Scott Lundberg",
        "authorids": "/m/marco-tulio-ribeiro/; /s/scott-lundberg/",
        "bibtex": "@inproceedings{ribeiro-lundberg-2022-adaptive,\n    title = \"Adaptive Testing and Debugging of {NLP} Models\",\n    author = \"Ribeiro, Marco Tulio  and\n      Lundberg, Scott\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.230/\",\n    doi = \"10.18653/v1/2022.acl-long.230\",\n    pages = \"3253--3267\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.230.pdf",
        "site": "https://aclanthology.org/2022.acl-long.230/",
        "pdf_size": 3279765,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11314483365298647781&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Microsoft Research; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "https://github.com/microsoft/adatest",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.291",
        "title": "Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: (1) lack of similar datasets/models from a related domain, (2) inability to sample useful logical forms directly from a grammar, and (3) privacy requirements for unlabeled natural utterances. Our goal is to improve a low-resource semantic parser using utterances collected through user interactions. In this highly challenging but realistic setting, we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms, before simulating corresponding natural language and filtering the resulting pairs. We find that such approaches are effective despite our restrictive setup: in a low-resource setting on the complex SMCalFlow calendaring dataset (Andreas et al. 2020), we observe 33% relative improvement over a non-data-augmented baseline in top-1 match.",
        "author": "Kevin Yang; Olivia Deng; Charles Chen; Richard Shin; Subhro Roy; Benjamin Van Durme",
        "authorids": "/k/kevin-yang/; /o/olivia-deng/; /c/charles-chen-jr/; /r/richard-shin/; /s/subhro-roy/; /b/benjamin-van-durme/",
        "bibtex": "@inproceedings{yang-etal-2022-addressing,\n    title = \"Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation\",\n    author = \"Yang, Kevin  and\n      Deng, Olivia  and\n      Chen, Charles  and\n      Shin, Richard  and\n      Roy, Subhro  and\n      Van Durme, Benjamin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.291/\",\n    doi = \"10.18653/v1/2022.findings-acl.291\",\n    pages = \"3685--3695\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.291.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.291/",
        "pdf_size": 704760,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12789442684036859872&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 6,
        "aff": "UC Berkeley; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines",
        "aff_domain": "berkeley.edu;microsoft.com; ; ; ; ",
        "email": "berkeley.edu;microsoft.com; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "University of California, Berkeley;Microsoft",
        "aff_unique_dep": ";Semantic Machines",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com",
        "aff_unique_abbr": "UC Berkeley;Microsoft",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.77",
        "title": "Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Modern writing assistance applications are always equipped with a Grammatical Error Correction (GEC) model to correct errors in user-entered sentences. Different scenarios have varying requirements for correction behavior, e.g., performing more precise corrections (high precision) or providing more candidates for users (high recall). However, previous works adjust such trade-off only for sequence labeling approaches. In this paper, we propose a simple yet effective counterpart \u2013 Align-and-Predict Decoding (APD) for the most popular sequence-to-sequence models to offer more flexibility for the precision-recall trade-off. During inference, APD aligns the already generated sequence with input and adjusts scores of the following tokens. Experiments in both English and Chinese GEC benchmarks show that our approach not only adapts a single model to precision-oriented and recall-oriented inference, but also maximizes its potential to achieve state-of-the-art results. Our code is available at https://github.com/AutoTemp/Align-and-Predict.",
        "author": "Xin Sun; Houfeng Wang",
        "authorids": "/x/xin-sun/; /h/houfeng-wang/",
        "bibtex": "@inproceedings{sun-wang-2022-adjusting,\n    title = \"Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction\",\n    author = \"Sun, Xin  and\n      Wang, Houfeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.77/\",\n    doi = \"10.18653/v1/2022.acl-short.77\",\n    pages = \"686--693\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.77.pdf",
        "site": "https://aclanthology.org/2022.acl-short.77/",
        "pdf_size": 364046,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7847247182933394137&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "MOE Key Lab of Computational Linguistics, School of Computer Science, Peking University; MOE Key Lab of Computational Linguistics, School of Computer Science, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/AutoTemp/Align-and-Predict",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.509",
        "title": "Adversarial Authorship Attribution for Deobfuscation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent advances in natural language processing have enabled powerful privacy-invasive authorship attribution. To counter authorship attribution, researchers have proposed a variety of rule-based and learning-based text obfuscation approaches. However, existing authorship obfuscation approaches do not consider the adversarial threat model. Specifically, they are not evaluated against adversarially trained authorship attributors that are aware of potential obfuscation. To fill this gap, we investigate the problem of adversarial authorship attribution for deobfuscation. We show that adversarially trained authorship attributors are able to degrade the effectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate the effectiveness of adversarial training when the attributor makes incorrect assumptions about whether and which obfuscator was used. While there is a a clear degradation in attribution accuracy, it is noteworthy that this degradation is still at or above the attribution accuracy of the attributor that is not adversarially trained at all. Our results motivate the need to develop authorship obfuscation approaches that are resistant to deobfuscation.",
        "author": "Wanyue Zhai; Jonathan Rusert; Zubair Shafiq; Padmini Srinivasan",
        "authorids": "/w/wanyue-zhai/; /j/jonathan-rusert/; /z/zubair-shafiq/; /p/padmini-srinivasan/",
        "bibtex": "@inproceedings{zhai-etal-2022-adversarial,\n    title = \"Adversarial Authorship Attribution for Deobfuscation\",\n    author = \"Zhai, Wanyue  and\n      Rusert, Jonathan  and\n      Shafiq, Zubair  and\n      Srinivasan, Padmini\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.509/\",\n    doi = \"10.18653/v1/2022.acl-long.509\",\n    pages = \"7372--7384\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.509.pdf",
        "site": "https://aclanthology.org/2022.acl-long.509/",
        "pdf_size": 725870,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10555233753495260353&as_sdt=8005&sciodt=0,7&hl=en",
        "gs_version_total": 0,
        "aff": "University of California, Davis; University of Iowa; University of California, Davis; University of Iowa",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/reginazhai/Authorship-Deobfuscation",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of California, Davis;University of Iowa",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucdavis.edu;https://www.uiowa.edu",
        "aff_unique_abbr": "UC Davis;UIowa",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Davis;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.174",
        "title": "Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-domain sentiment analysis has achieved promising results with the help of pre-trained language models. As GPT-3 appears, prompt tuning has been widely explored to enable better semantic modeling in many natural language processing tasks. However, directly using a fixed predefined template for cross-domain research cannot model different distributions of the [MASK] token in different domains, thus making underuse of the prompt tuning technique. In this paper, we propose a novel Adversarial Soft Prompt Tuning method (AdSPT) to better model cross-domain sentiment analysis. On the one hand, AdSPT adopts separate soft prompts instead of hard templates to learn different vectors for different domains, thus alleviating the domain discrepancy of the [MASK] token in the masked language modeling task. On the other hand, AdSPT uses a novel domain adversarial training strategy to learn domain-invariant representations between each source domain and the target domain. Experiments on a publicly available sentiment analysis dataset show that our model achieves the new state-of-the-art results for both single-source domain adaptation and multi-source domain adaptation.",
        "author": "Hui Wu; Xiaodong Shi",
        "authorids": "/h/hui-wu/; /x/xiaodong-shi/",
        "bibtex": "@inproceedings{wu-shi-2022-adversarial,\n    title = \"Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis\",\n    author = \"Wu, Hui  and\n      Shi, Xiaodong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.174/\",\n    doi = \"10.18653/v1/2022.acl-long.174\",\n    pages = \"2438--2447\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.174.pdf",
        "site": "https://aclanthology.org/2022.acl-long.174/",
        "pdf_size": 864633,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=275245584653780907&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Artificial Intelligence, School of Informatics, Xiamen University, China+National Institute for Data Science in Health and Medicine, Xiamen University, China+Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), Ministry of Culture and Tourism, China; Department of Artificial Intelligence, School of Informatics, Xiamen University, China+National Institute for Data Science in Health and Medicine, Xiamen University, China+Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), Ministry of Culture and Tourism, China",
        "aff_domain": "stu.xmu.edu.cn;xmu.edu.cn",
        "email": "stu.xmu.edu.cn;xmu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0+0;0+0+0",
        "aff_unique_norm": "Xiamen University",
        "aff_unique_dep": "Department of Artificial Intelligence, School of Informatics",
        "aff_unique_url": "https://www.xmu.edu.cn",
        "aff_unique_abbr": "XMU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.4",
        "title": "AlephBERT: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for English using PLMs are unprecedented, reported advances using PLMs for Hebrew are few and far between. The problem is twofold. First, so far, Hebrew resources for training large language models are not of the same magnitude as their English counterparts. Second, most benchmarks available to evaluate progress in Hebrew NLP require morphological boundaries which are not available in the output of standard PLMs. In this work we remedy both aspects. We present AlephBERT, a large PLM for Modern Hebrew, trained on larger vocabulary and a larger dataset than any Hebrew PLM before. Moreover, we introduce a novel neural architecture that recovers the morphological segments encoded in contextualized embedding vectors. Based on this new morphological component we offer an evaluation suite consisting of multiple tasks and benchmarks that cover sentence-level, word-level and sub-word level analyses. On all tasks, AlephBERT obtains state-of-the-art results beyond contemporary Hebrew baselines. We make our AlephBERT model, the morphological extraction model, and the Hebrew evaluation suite publicly available, for evaluating future Hebrew PLMs.",
        "author": "Amit Seker; Elron Bandel; Dan Bareket; Idan Brusilovsky; Refael Greenfeld; Reut Tsarfaty",
        "authorids": "/a/amit-seker/; /e/elron-bandel/; /d/dan-bareket/; /i/idan-brusilovsky/; /r/refael-greenfeld/; /r/reut-tsarfaty/",
        "bibtex": "@inproceedings{seker-etal-2022-alephbert,\n    title = \"{A}leph{BERT}: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level\",\n    author = \"Seker, Amit  and\n      Bandel, Elron  and\n      Bareket, Dan  and\n      Brusilovsky, Idan  and\n      Greenfeld, Refael  and\n      Tsarfaty, Reut\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.4/\",\n    doi = \"10.18653/v1/2022.acl-long.4\",\n    pages = \"46--56\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.4.pdf",
        "site": "https://aclanthology.org/2022.acl-long.4/",
        "pdf_size": 312259,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12334591861038431705&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Bar Ilan University, Ramat-Gan, Israel; Department of Computer Science, Bar Ilan University, Ramat-Gan, Israel; Department of Computer Science, Bar Ilan University, Ramat-Gan, Israel; Department of Computer Science, Bar Ilan University, Ramat-Gan, Israel; Department of Computer Science, Bar Ilan University, Ramat-Gan, Israel; Department of Computer Science, Bar Ilan University, Ramat-Gan, Israel",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Bar-Ilan University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.biu.ac.il",
        "aff_unique_abbr": "BIU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Ramat-Gan",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.findings-acl.267",
        "title": "Aligned Weight Regularizers for Pruning Pretrained Neural Networks",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pruning aims to reduce the number of parameters while maintaining performance close to the original network. This work proposes a novel self-distillation based pruning strategy, whereby the representational similarity between the pruned and unpruned versions of the same network is maximized. Unlike previous approaches that treat distillation and pruning separately, we use distillation to inform the pruning criteria, without requiring a separate student network as in knowledge distillation. We show that the proposed cross-correlation objective for self-distilled pruning implicitly encourages sparse solutions, naturally complementing magnitude-based pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that self-distilled pruning increases mono- and cross-lingual language model performance. Self-distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against (6 times) larger distilled networks. We also observe that self-distillation (1) maximizes class separability, (2) increases the signal-to-noise ratio, and (3) converges faster after pruning steps, providing further insights into why self-distilled pruning improves generalization.",
        "author": "James O\u2019 Neill; Sourav Dutta; Haytham Assem",
        "authorids": "/j/james-o-neill/; /s/sourav-dutta/; /h/haytham-assem/",
        "bibtex": "@inproceedings{o-neill-etal-2022-aligned,\n    title = \"Aligned Weight Regularizers for Pruning Pretrained Neural Networks\",\n    author = \"O{'} Neill, James  and\n      Dutta, Sourav  and\n      Assem, Haytham\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.267/\",\n    doi = \"10.18653/v1/2022.findings-acl.267\",\n    pages = \"3391--3401\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.267.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.267/",
        "pdf_size": 1095275,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12304770807594251809&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Huawei Research Center, Dublin, Ireland; Huawei Research Center, Dublin, Ireland; Huawei Research Center, Dublin, Ireland",
        "aff_domain": "huawei.com;huawei.com;huawei.com",
        "email": "huawei.com;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Huawei",
        "aff_unique_dep": "Huawei Research Center",
        "aff_unique_url": "https://www.huawei.com",
        "aff_unique_abbr": "Huawei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Dublin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Ireland"
    },
    {
        "id": "2022.acl-long.179",
        "title": "Alignment-Augmented Consistent Translation for Multilingual Open Information Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Progress with supervised Open Information Extraction (OpenIE) has been primarily limited to English due to the scarcity of training data in other languages. In this paper, we explore techniques to automatically convert English text for training OpenIE systems in other languages. We introduce the Alignment-Augmented Constrained Translation (AACTrans) model to translate English sentences and their corresponding extractions consistently with each other \u2014 with no changes to vocabulary or semantic meaning which may result from independent translations. Using the data generated with AACTrans, we train a novel two-stage generative OpenIE model, which we call Gen2OIE, that outputs for each sentence: 1) relations in the first stage and 2) all extractions containing the relation in the second stage. Gen2OIE increases relation coverage using a training data transformation technique that is generalizable to multiple languages, in contrast to existing models that use an English-specific training loss. Evaluations on 5 languages \u2014 Spanish, Portuguese, Chinese, Hindi and Telugu \u2014 show that the Gen2OIE with AACTrans data outperforms prior systems by a margin of 6-25% in F1.",
        "author": "Keshav Kolluru; Muqeeth Mohammed; Shubham Mittal; Soumen Chakrabarti; Mausam",
        "authorids": "/k/keshav-kolluru/; /m/muqeeth-mohammed/; /s/shubham-mittal/; /s/soumen-chakrabarti/; /m/mausam/",
        "bibtex": "@inproceedings{kolluru-etal-2022-alignment,\n    title = \"Alignment-Augmented Consistent Translation for Multilingual Open Information Extraction\",\n    author = \"Kolluru, Keshav  and\n      Mohammed, Muqeeth  and\n      Mittal, Shubham  and\n      Chakrabarti, Soumen  and\n      Mausam\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.179/\",\n    doi = \"10.18653/v1/2022.acl-long.179\",\n    pages = \"2502--2517\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.179.pdf",
        "site": "https://aclanthology.org/2022.acl-long.179/",
        "pdf_size": 920775,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3671881445713546061&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Indian Institute of Technology Delhi; Indian Institute of Technology Delhi; Indian Institute of Technology Delhi; Indian Institute of Technology Bombay; Indian Institute of Technology Delhi",
        "aff_domain": "gmail.com;gmail.com;gmail.com;cse.iitb.ac.in;cse.iitd.ac.in",
        "email": "gmail.com;gmail.com;gmail.com;cse.iitb.ac.in;cse.iitd.ac.in",
        "github": "github.com:dair-iitd/moie",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Delhi;Indian Institute of Technology Bombay",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitd.ac.in;https://www.iitb.ac.in",
        "aff_unique_abbr": "IIT Delhi;IIT Bombay",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Delhi;Bombay",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.363",
        "title": "Alternative Input Signals Ease Transfer in Multilingual Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work in multilingual machine translation (MMT) has focused on the potential of positive transfer between languages, particularly cases where higher-resourced languages can benefit lower-resourced ones. While training an MMT model, the supervision signals learned from one language pair can be transferred to the other via the tokens shared by multiple source languages. However, the transfer is inhibited when the token overlap among source languages is small, which manifests naturally when languages use different writing systems. In this paper, we tackle inhibited transfer by augmenting the training data with alternative signals that unify different writing systems, such as phonetic, romanized, and transliterated input. We test these signals on Indic and Turkic languages, two language families where the writing systems differ but languages still share common features. Our results indicate that a straightforward multi-source self-ensemble \u2013 training a model on a mixture of various signals and ensembling the outputs of the same model fed with different signals during inference, outperforms strong ensemble baselines by 1.3 BLEU points on both language families. Further, we find that incorporating alternative inputs via self-ensemble can be particularly effective when training set is small, leading to +5 BLEU when only 5% of the total training data is accessible. Finally, our analysis demonstrates that including alternative signals yields more consistency and translates named entities more accurately, which is crucial for increased factuality of automated systems.",
        "author": "Simeng Sun; Angela Fan; James Cross; Vishrav Chaudhary; Chau Tran; Philipp Koehn; Francisco Guzm\u00e1n",
        "authorids": "/s/simeng-sun/; /a/angela-fan/; /j/james-cross/; /v/vishrav-chaudhary/; /c/chau-tran/; /p/philipp-koehn/; /f/francisco-guzman/",
        "bibtex": "@inproceedings{sun-etal-2022-alternative,\n    title = \"Alternative Input Signals Ease Transfer in Multilingual Machine Translation\",\n    author = \"Sun, Simeng  and\n      Fan, Angela  and\n      Cross, James  and\n      Chaudhary, Vishrav  and\n      Tran, Chau  and\n      Koehn, Philipp  and\n      Guzm{\\'a}n, Francisco\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.363/\",\n    doi = \"10.18653/v1/2022.acl-long.363\",\n    pages = \"5291--5305\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.363.pdf",
        "site": "https://aclanthology.org/2022.acl-long.363/",
        "pdf_size": 734853,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4170876895463839124&as_sdt=1005&sciodt=0,4&hl=en",
        "gs_version_total": 6,
        "aff": "University of Massachusetts Amherst1+Meta AI; Meta AI; Meta AI; Microsoft Turing+Meta AI; Meta AI; Meta AI; Meta AI",
        "aff_domain": "umass.edu;fb.com;fb.com;microsoft.com;fb.com;fb.com;fb.com",
        "email": "umass.edu;fb.com;fb.com;microsoft.com;fb.com;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;2+1;1;1;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Meta;Microsoft",
        "aff_unique_dep": ";Meta AI;Microsoft Turing",
        "aff_unique_url": "https://www.umass.edu;https://meta.com;https://www.microsoft.com",
        "aff_unique_abbr": "UMass Amherst;Meta;Microsoft",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0+0;0;0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.435",
        "title": "AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pretrained multilingual models are able to perform cross-lingual transfer in a zero-shot setting, even for languages unseen during pretraining. However, prior work evaluating performance on unseen languages has largely been limited to low-level, syntactic tasks, and it remains unclear if zero-shot learning of high-level, semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018) to 10 Indigenous languages of the Americas. We conduct experiments with XLM-R, testing multiple zero-shot and translation-based approaches. Additionally, we explore model adaptation via continued pretraining and provide an analysis of the dataset by considering hypothesis-only models. We find that XLM-R\u2019s zero-shot performance is poor for all 10 languages, with an average performance of 38.48%. Continued pretraining offers improvements, with an average accuracy of 43.85%. Surprisingly, training on poorly translated data by far outperforms all other methods with an accuracy of 49.12%.",
        "author": "Abteen Ebrahimi; Manuel Mager; Arturo Oncevay; Vishrav Chaudhary; Luis Chiruzzo; Angela Fan; John Ortega; Ricardo Ramos; Annette Rios; Ivan Vladimir Meza Ruiz; Gustavo Gim\u00e9nez-Lugo; Elisabeth Mager; Graham Neubig; Alexis Palmer; Rolando Coto-Solano; Thang Vu; Katharina Kann",
        "authorids": "/a/abteen-ebrahimi/; /m/manuel-mager/; /a/arturo-oncevay/; /v/vishrav-chaudhary/; /l/luis-chiruzzo/; /a/angela-fan/; /j/john-ortega/; /r/ricardo-ramos/; /a/annette-rios-gonzales/; /i/ivan-meza-ruiz/; /g/gustavo-gimenez-lugo/; /e/elisabeth-maier/; /g/graham-neubig/; /a/alexis-palmer/; /r/rolando-coto-solano/; /t/thang-vu/; /k/katharina-von-der-wense/",
        "bibtex": "@inproceedings{ebrahimi-etal-2022-americasnli,\n    title = \"{A}mericas{NLI}: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages\",\n    author = \"Ebrahimi, Abteen  and\n      Mager, Manuel  and\n      Oncevay, Arturo  and\n      Chaudhary, Vishrav  and\n      Chiruzzo, Luis  and\n      Fan, Angela  and\n      Ortega, John  and\n      Ramos, Ricardo  and\n      Rios, Annette  and\n      Meza Ruiz, Ivan Vladimir  and\n      Gim{\\'e}nez-Lugo, Gustavo  and\n      Mager, Elisabeth  and\n      Neubig, Graham  and\n      Palmer, Alexis  and\n      Coto-Solano, Rolando  and\n      Vu, Thang  and\n      Kann, Katharina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.435/\",\n    doi = \"10.18653/v1/2022.acl-long.435\",\n    pages = \"6279--6299\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.435.pdf",
        "site": "https://aclanthology.org/2022.acl-long.435/",
        "pdf_size": 1479446,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7377847811359457429&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "University of Colorado Boulder; University of Stuttgart; University of Edinburgh; Microsoft Turing; Universidad de la Rep\u00fablica, Uruguay; Facebook AI Research; New York University; Universidad Tecnol\u00f3gica de Tlaxcala; University of Zurich; Universidad Nacional Aut\u00f3noma de M\u00e9xico; Universidade Tecnol\u00f3gica Federal do Paran\u00e1; Universidad Nacional Aut\u00f3noma de M\u00e9xico; Carnegie Mellon University; University of Colorado Boulder; Dartmouth College; University of Stuttgart; Dartmouth College",
        "aff_domain": ";;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 17,
        "aff_unique_index": "0;1;2;3;4;5;6;7;8;9;10;9;11;0;12;1;12",
        "aff_unique_norm": "University of Colorado;University of Stuttgart;University of Edinburgh;Microsoft;Universidad de la Rep\u00fablica;Meta;New York University;Universidad Tecnol\u00f3gica de Tlaxcala;University of Zurich;Universidad Nacional Aut\u00f3noma de M\u00e9xico;Universidade Tecnol\u00f3gica Federal do Paran\u00e1;Carnegie Mellon University;Dartmouth College",
        "aff_unique_dep": ";;;Microsoft Turing;;Facebook AI Research;;;;;;;",
        "aff_unique_url": "https://www.colorado.edu;https://www.uni-stuttgart.de;https://www.ed.ac.uk;https://www.microsoft.com;https://www.unorte.edu.uy;https://research.facebook.com;https://www.nyu.edu;http://www.utt.edu.mx;https://www.unizh.ch;https://www.unam.mx;https://www.utfpr.edu.br;https://www.cmu.edu;https://www.dartmouth.edu",
        "aff_unique_abbr": "CU;USTuttgart;Edinburgh;Microsoft;Udelar;FAIR;NYU;;UZH;UNAM;UTFPR;CMU;Dartmouth",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boulder;",
        "aff_country_unique_index": "0;1;2;0;3;0;0;4;5;4;6;4;0;0;0;1;0",
        "aff_country_unique": "United States;Germany;United Kingdom;Uruguay;Mexico;Switzerland;Brazil"
    },
    {
        "id": "2022.findings-acl.183",
        "title": "An Accurate Unsupervised Method for Joint Entity Alignment and Dangling Entity Detection",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge graph integration typically suffers from the widely existing dangling entities that cannot find alignment cross knowledge graphs (KGs). The dangling entity set is unavailable in most real-world scenarios, and manually mining the entity pairs that consist of entities with the same meaning is labor-consuming. In this paper, we propose a novel accurate Unsupervised method for joint Entity alignment (EA) and Dangling entity detection (DED), called UED. The UED mines the literal semantic information to generate pseudo entity pairs and globally guided alignment information for EA and then utilizes the EA results to assist the DED. We construct a medical cross-lingual knowledge graph dataset, MedED, providing data for both the EA and DED tasks. Extensive experiments demonstrate that in the EA task, UED achieves EA results comparable to those of state-of-the-art supervised EA baselines and outperforms the current state-of-the-art EA methods by combining supervised EA data. For the DED task, UED obtains high-quality results without supervision.",
        "author": "Shengxuan Luo; Sheng Yu",
        "authorids": "/s/shengxuan-luo/; /s/sheng-yu/",
        "bibtex": "@inproceedings{luo-yu-2022-accurate,\n    title = \"An Accurate Unsupervised Method for Joint Entity Alignment and Dangling Entity Detection\",\n    author = \"Luo, Shengxuan  and\n      Yu, Sheng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.183/\",\n    doi = \"10.18653/v1/2022.findings-acl.183\",\n    pages = \"2330--2339\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.183.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.183/",
        "pdf_size": 695551,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3981533956980357424&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Statistical Science, Tsinghua University + Department of Industrial Engineering, Tsinghua University; Center for Statistical Science, Tsinghua University + Department of Industrial Engineering, Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Center for Statistical Science",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.81",
        "title": "An Analysis of Negation in Natural Language Understanding Corpora",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks. We show that these corpora have few negations compared to general-purpose English, and that the few negations in them are often unimportant. Indeed, one can often ignore negations and still make the right predictions. Additionally, experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation, especially if the negations are important. We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present.",
        "author": "Md Mosharaf Hossain; Dhivya Chinnappa; Eduardo Blanco",
        "authorids": "/m/md-mosharaf-hossain/; /d/dhivya-chinnappa/; /e/eduardo-blanco/",
        "bibtex": "@inproceedings{hossain-etal-2022-analysis,\n    title = \"An Analysis of Negation in Natural Language Understanding Corpora\",\n    author = \"Hossain, Md Mosharaf  and\n      Chinnappa, Dhivya  and\n      Blanco, Eduardo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.81/\",\n    doi = \"10.18653/v1/2022.acl-short.81\",\n    pages = \"716--723\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.81.pdf",
        "site": "https://aclanthology.org/2022.acl-short.81/",
        "pdf_size": 159436,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13053454990891601876&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Engineering, University of North Texas; Thomson Reuters; School of Computing and Augmented Intelligence, Arizona State University",
        "aff_domain": "my.unt.edu;gmail.com;asu.edu",
        "email": "my.unt.edu;gmail.com;asu.edu",
        "github": "https://github.com/mosharafhossain/negation-and-nlu",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of North Texas;Thomson Reuters;Arizona State University",
        "aff_unique_dep": "Department of Computer Science and Engineering;;School of Computing and Augmented Intelligence",
        "aff_unique_url": "https://www.unt.edu;https://www.thomsonreuters.com;https://www.asu.edu",
        "aff_unique_abbr": "UNT;TR;ASU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tempe",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.405",
        "title": "An Effective and Efficient Entity Alignment Decoding Algorithm via Third-Order Tensor Isomorphism",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Entity alignment (EA) aims to discover the equivalent entity pairs between KGs, which is a crucial step for integrating multi-source KGs.For a long time, most researchers have regarded EA as a pure graph representation learning task and focused on improving graph encoders while paying little attention to the decoding process. In this paper, we propose an effective and efficient EA Decoding Algorithm via Third-order Tensor Isomorphism (DATTI).Specifically, we derive two sets of isomorphism equations: (1) Adjacency tensor isomorphism equations and (2) Gramian tensor isomorphism equations. By combining these equations, DATTI could effectively utilize the adjacency and inner correlation isomorphisms of KGs to enhance the decoding process of EA.Extensive experiments on public datasets indicate that our decoding algorithm can deliver significant performance improvements even on the most advanced EA methods, while the extra required time is less than 3 seconds.",
        "author": "Xin Mao; Meirong Ma; Hao Yuan; Jianchao Zhu; ZongYu Wang; Rui Xie; Wei Wu; Man Lan",
        "authorids": "/x/xinnian-mao/; /m/meirong-ma/; /h/hao-yuan/; /j/jianchao-zhu/; /z/zongyu-wang/; /r/rui-xie/; /w/wei-wu/; /m/man-lan/",
        "bibtex": "@inproceedings{mao-etal-2022-effective,\n    title = \"An Effective and Efficient Entity Alignment Decoding Algorithm via Third-Order Tensor Isomorphism\",\n    author = \"Mao, Xin  and\n      Ma, Meirong  and\n      Yuan, Hao  and\n      Zhu, Jianchao  and\n      Wang, ZongYu  and\n      Xie, Rui  and\n      Wu, Wei  and\n      Lan, Man\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.405/\",\n    doi = \"10.18653/v1/2022.acl-long.405\",\n    pages = \"5888--5898\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.405.pdf",
        "site": "https://aclanthology.org/2022.acl-long.405/",
        "pdf_size": 702176,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8516746258902306355&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, East China Normal University; Transsion Group; Transsion Group; Transsion Group; Meituan Group; Meituan Group; Meituan Group; School of Computer Science and Technology, East China Normal University",
        "aff_domain": "stu.ecnu.edu.cn;transsion.com;transsion.com;transsion.com;meituan.com;meituan.com;meituan.com;cs.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;transsion.com;transsion.com;transsion.com;meituan.com;meituan.com;meituan.com;cs.ecnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;2;2;2;0",
        "aff_unique_norm": "East China Normal University;Transsion Group;Meituan Group",
        "aff_unique_dep": "School of Computer Science and Technology;;",
        "aff_unique_url": "http://www.ecnu.edu.cn;http://www.transsiongroup.com;https://www.meituan.com",
        "aff_unique_abbr": "ECNU;;Meituan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.43",
        "title": "An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.",
        "author": "Valentin Hofmann; Hinrich Schuetze; Janet Pierrehumbert",
        "authorids": "/v/valentin-hofmann/; /h/hinrich-schutze/; /j/janet-pierrehumbert/",
        "bibtex": "@inproceedings{hofmann-etal-2022-embarrassingly,\n    title = \"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers\",\n    author = \"Hofmann, Valentin  and\n      Schuetze, Hinrich  and\n      Pierrehumbert, Janet\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.43/\",\n    doi = \"10.18653/v1/2022.acl-short.43\",\n    pages = \"385--393\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.43.pdf",
        "site": "https://aclanthology.org/2022.acl-short.43/",
        "pdf_size": 480260,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6597533374963597055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Information and Language Processing, LMU Munich; Center for Information and Language Processing, LMU Munich; Department of Engineering Science, University of Oxford + Faculty of Linguistics, University of Oxford",
        "aff_domain": "ling-phil.ox.ac.uk; ; ",
        "email": "ling-phil.ox.ac.uk; ; ",
        "github": "https://github.com/valentinhofmann/flota",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1+1",
        "aff_unique_norm": "LMU Munich;University of Oxford",
        "aff_unique_dep": "Center for Information and Language Processing;Department of Engineering Science",
        "aff_unique_url": "https://www.lmu.de;https://www.ox.ac.uk",
        "aff_unique_abbr": "LMU;Oxford",
        "aff_campus_unique_index": "0;0;1+1",
        "aff_campus_unique": "Munich;Oxford",
        "aff_country_unique_index": "0;0;1+1",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "2022.acl-long.434",
        "title": "An Empirical Study of Memorization in NLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A recent study by Feldman (2020) proposed a long-tail theory to explain the memorization behavior of deep learning models. However, memorization has not been empirically verified in the context of NLP, a gap addressed by this work. In this paper, we use three different NLP tasks to check if the long-tail theory holds. Our experiments demonstrate that top-ranked memorized training instances are likely atypical, and removing the top-memorized training instances leads to a more serious drop in test accuracy compared with removing training instances randomly. Furthermore, we develop an attribution method to better understand why a training instance is memorized. We empirically show that our memorization attribution method is faithful, and share our interesting finding that the top-memorized parts of a training instance tend to be features negatively correlated with the class label.",
        "author": "Xiaosen Zheng; Jing Jiang",
        "authorids": "/x/xiaosen-zheng/; /j/jing-jiang/",
        "bibtex": "@inproceedings{zheng-jiang-2022-empirical,\n    title = \"An Empirical Study of Memorization in {NLP}\",\n    author = \"Zheng, Xiaosen  and\n      Jiang, Jing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.434/\",\n    doi = \"10.18653/v1/2022.acl-long.434\",\n    pages = \"6265--6278\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.434.pdf",
        "site": "https://aclanthology.org/2022.acl-long.434/",
        "pdf_size": 683056,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8423669576895029861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Singapore Management University; Singapore Management University",
        "aff_domain": "phdcs.smu.edu.sg;smu.edu.sg",
        "email": "phdcs.smu.edu.sg;smu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Singapore Management University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.smu.edu.sg",
        "aff_unique_abbr": "SMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.acl-long.477",
        "title": "An Empirical Study on Explanations in Out-of-Domain Settings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work in Natural Language Processing has focused on developing approaches that extract faithful explanations, either via identifying the most important tokens in the input (i.e. post-hoc explanations) or by designing inherently faithful models that first select the most important tokens and then use them to predict the correct label (i.e. select-then-predict models). Currently, these approaches are largely evaluated on in-domain settings. Yet, little is known about how post-hoc explanations and inherently faithful models perform in out-of-domain settings. In this paper, we conduct an extensive empirical study that examines: (1) the out-of-domain faithfulness of post-hoc explanations, generated by five feature attribution methods; and (2) the out-of-domain performance of two inherently faithful models over six datasets. Contrary to our expectations, results show that in many cases out-of-domain post-hoc explanation faithfulness measured by sufficiency and comprehensiveness is higher compared to in-domain. We find this misleading and suggest using a random baseline as a yardstick for evaluating post-hoc explanation faithfulness. Our findings also show that select-then predict models demonstrate comparable predictive performance in out-of-domain settings to full-text trained models.",
        "author": "George Chrysostomou; Nikolaos Aletras",
        "authorids": "/g/george-chrysostomou/; /n/nikolaos-aletras/",
        "bibtex": "@inproceedings{chrysostomou-aletras-2022-empirical,\n    title = \"An Empirical Study on Explanations in Out-of-Domain Settings\",\n    author = \"Chrysostomou, George  and\n      Aletras, Nikolaos\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.477/\",\n    doi = \"10.18653/v1/2022.acl-long.477\",\n    pages = \"6920--6938\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.477.pdf",
        "site": "https://aclanthology.org/2022.acl-long.477/",
        "pdf_size": 1034856,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=486005928807397369&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Sheffield; Department of Computer Science, University of Sheffield",
        "aff_domain": "sheffield.ac.uk;sheffield.ac.uk",
        "email": "sheffield.ac.uk;sheffield.ac.uk",
        "github": "https://github.com/GChrysostomou/ood_faith",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Sheffield",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Sheffield",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.132",
        "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
        "author": "Nicholas Meade; Elinor Poole-Dayan; Siva Reddy",
        "authorids": "/n/nicholas-meade/; /e/elinor-poole-dayan/; /s/siva-reddy/",
        "bibtex": "@inproceedings{meade-etal-2022-empirical,\n    title = \"An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models\",\n    author = \"Meade, Nicholas  and\n      Poole-Dayan, Elinor  and\n      Reddy, Siva\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.132/\",\n    doi = \"10.18653/v1/2022.acl-long.132\",\n    pages = \"1878--1898\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.132.pdf",
        "site": "https://aclanthology.org/2022.acl-long.132/",
        "pdf_size": 339007,
        "gs_citation": 208,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16004764144675698137&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Mila and McGill University; Mila and McGill University; Mila and McGill University + Facebook CIFAR AI Chair",
        "aff_domain": "mila.quebec;mila.quebec;mila.quebec",
        "email": "mila.quebec;mila.quebec;mila.quebec",
        "github": "https://github.com/mcgill-nlp/bias-bench",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "McGill University;Meta",
        "aff_unique_dep": "Mila;Facebook CIFAR AI",
        "aff_unique_url": "https://www.mcgill.ca;https://www.facebook.com",
        "aff_unique_abbr": "McGill;FB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.acl-long.520",
        "title": "An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a framework for training non-autoregressive sequence-to-sequence models for editing tasks, where the original input sequence is iteratively edited to produce the output. We show that the imitation learning algorithms designed to train such models for machine translation introduces mismatches between training and inference that lead to undertraining and poor generalization in editing scenarios. We address this issue with two complementary strategies: 1) a roll-in policy that exposes the model to intermediate training sequences that it is more likely to encounter during inference, 2) a curriculum that presents easy-to-learn edit operations first, gradually increasing the difficulty of training samples as the model becomes competent. We show the efficacy of these strategies on two challenging English editing tasks: controllable text simplification and abstractive summarization. Our approach significantly improves output quality on both tasks and controls output complexity better on the simplification task.",
        "author": "Sweta Agrawal; Marine Carpuat",
        "authorids": "/s/sweta-agrawal/; /m/marine-carpuat/",
        "bibtex": "@inproceedings{agrawal-carpuat-2022-imitation,\n    title = \"An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models\",\n    author = \"Agrawal, Sweta  and\n      Carpuat, Marine\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.520/\",\n    doi = \"10.18653/v1/2022.acl-long.520\",\n    pages = \"7550--7563\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.520.pdf",
        "site": "https://aclanthology.org/2022.acl-long.520/",
        "pdf_size": 1831085,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6513243402545026318&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland",
        "aff_domain": "cs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.60",
        "title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.",
        "author": "Taylor Sorensen; Joshua Robinson; Christopher Rytting; Alexander Shaw; Kyle Rogers; Alexia Delorey; Mahmoud Khalil; Nancy Fulda; David Wingate",
        "authorids": "/t/taylor-sorensen/; /j/joshua-robinson/; /c/christopher-rytting/; /a/alexander-shaw/; /k/kyle-rogers/; /a/alexia-delorey/; /m/mahmoud-khalil/; /n/nancy-fulda/; /d/david-wingate/",
        "bibtex": "@inproceedings{sorensen-etal-2022-information,\n    title = \"An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels\",\n    author = \"Sorensen, Taylor  and\n      Robinson, Joshua  and\n      Rytting, Christopher  and\n      Shaw, Alexander  and\n      Rogers, Kyle  and\n      Delorey, Alexia  and\n      Khalil, Mahmoud  and\n      Fulda, Nancy  and\n      Wingate, David\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.60/\",\n    doi = \"10.18653/v1/2022.acl-long.60\",\n    pages = \"819--862\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.60.pdf",
        "site": "https://aclanthology.org/2022.acl-long.60/",
        "pdf_size": 1501175,
        "gs_citation": 177,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6670212790512916190&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, Brigham Young University; Computer Science Department, Brigham Young University; Computer Science Department, Brigham Young University; Computer Science Department, Brigham Young University; Computer Science Department, Brigham Young University; Computer Science Department, Brigham Young University; Computer Science Department, Brigham Young University; Computer Science Department, Brigham Young University; Computer Science Department, Brigham Young University",
        "aff_domain": "byu.edu;byu.edu;byu.edu; ; ; ; ;cs.byu.edu;cs.byu.edu",
        "email": "byu.edu;byu.edu;byu.edu; ; ; ; ;cs.byu.edu;cs.byu.edu",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Brigham Young University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.byu.edu",
        "aff_unique_abbr": "BYU",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Provo",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.338",
        "title": "An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study the interpretability issue of task-oriented dialogue systems in this paper. Previously, most neural-based task-oriented dialogue systems employ an implicit reasoning strategy that makes the model predictions uninterpretable to humans. To obtain a transparent reasoning process, we introduce neuro-symbolic to perform explicit reasoning that justifies model decisions by reasoning chains. Since deriving reasoning chains requires multi-hop reasoning for task-oriented dialogues, existing neuro-symbolic approaches would induce error propagation due to the one-phase design. To overcome this, we propose a two-phase approach that consists of a hypothesis generator and a reasoner. We first obtain multiple hypotheses, i.e., potential operations to perform the desired task, through the hypothesis generator. Each hypothesis is then verified by the reasoner, and the valid one is selected to conduct the final prediction. The whole system is trained by exploiting raw textual dialogues without using any reasoning chain annotations. Experimental studies on two public benchmark datasets demonstrate that the proposed approach not only achieves better results, but also introduces an interpretable decision process.",
        "author": "Shiquan Yang; Rui Zhang; Sarah Erfani; Jey Han Lau",
        "authorids": "/s/shiquan-yang/; /r/rui-zhang/; /s/sarah-erfani/; /j/jey-han-lau/",
        "bibtex": "@inproceedings{yang-etal-2022-interpretable,\n    title = \"An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation\",\n    author = \"Yang, Shiquan  and\n      Zhang, Rui  and\n      Erfani, Sarah  and\n      Lau, Jey Han\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.338/\",\n    doi = \"10.18653/v1/2022.acl-long.338\",\n    pages = \"4918--4935\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.338.pdf",
        "site": "https://aclanthology.org/2022.acl-long.338/",
        "pdf_size": 873787,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9675708837434865469&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Melbourne; www.ruizhang.info; The University of Melbourne; The University of Melbourne",
        "aff_domain": "student.unimelb.edu.au;yeah.net;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;yeah.net;unimelb.edu.au;unimelb.edu.au",
        "github": "https://github.com/shiquanyang/NS-Dial",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Melbourne;Ruizhang.INFO",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unimelb.edu.au;http://www.ruizhang.info",
        "aff_unique_abbr": "UniMelb;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia;"
    },
    {
        "id": "2022.acl-long.256",
        "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution (OOD) data. Recent work has explored using counterfactually-augmented data (CAD)\u2014data generated by minimally perturbing examples to flip the ground-truth label\u2014to identify robust features that are invariant under distribution shift. However, empirical results using CAD during training for OOD generalization have been mixed. To explain this discrepancy, through a toy theoretical example and empirical analysis on two crowdsourced CAD datasets, we show that: (a) while features perturbed in CAD are indeed robust features, it may prevent the model from learning unperturbed robust features; and (b) CAD may exacerbate existing spurious correlations in the data. Our results thus show that the lack of perturbation diversity limits CAD\u2019s effectiveness on OOD generalization, calling for innovative crowdsourcing procedures to elicit diverse perturbation of examples.",
        "author": "Nitish Joshi; He He",
        "authorids": "/n/nitish-joshi/; /h/he-he/",
        "bibtex": "@inproceedings{joshi-he-2022-investigation,\n    title = \"An Investigation of the (In)effectiveness of Counterfactually Augmented Data\",\n    author = \"Joshi, Nitish  and\n      He, He\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.256/\",\n    doi = \"10.18653/v1/2022.acl-long.256\",\n    pages = \"3668--3681\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.256.pdf",
        "site": "https://aclanthology.org/2022.acl-long.256/",
        "pdf_size": 371327,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3617172353999220422&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, New York University + Center for Data Science, New York University; Department of Computer Science, New York University + Center for Data Science, New York University",
        "aff_domain": "nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.103",
        "title": "An Isotropy Analysis in the Multilingual BERT Embedding Space",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge. However, less attention has been paid to their limitations. In this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier dimensions. We show that, unlike its monolingual counterpart, the multilingual BERT model exhibits no outlier dimension in its representations while it has a highly anisotropic space. There are a few dimensions in the monolingual BERT with high contributions to the anisotropic distribution. However, we observe no such dimensions in the multilingual BERT. Furthermore, our experimental results demonstrate that increasing the isotropy of multilingual space can significantly improve its representation power and performance, similarly to what had been observed for monolingual CWRs on semantic similarity tasks. Our analysis indicates that, despite having different degenerated directions, the embedding spaces in various languages tend to be partially similar with respect to their structures.",
        "author": "Sara Rajaee; Mohammad Taher Pilehvar",
        "authorids": "/s/sara-rajaee/; /m/mohammad-taher-pilehvar/",
        "bibtex": "@inproceedings{rajaee-pilehvar-2022-isotropy,\n    title = \"An Isotropy Analysis in the Multilingual {BERT} Embedding Space\",\n    author = \"Rajaee, Sara  and\n      Pilehvar, Mohammad Taher\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.103/\",\n    doi = \"10.18653/v1/2022.findings-acl.103\",\n    pages = \"1309--1316\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.103.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.103/",
        "pdf_size": 854973,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17293738164413704461&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Iran University of Science and Technology, Tehran, Iran + Tehran Institute for Advanced Studies, Khatam University, Iran; Iran University of Science and Technology, Tehran, Iran + Tehran Institute for Advanced Studies, Khatam University, Iran",
        "aff_domain": "comp.iust.ac.ir;cam.ac.uk",
        "email": "comp.iust.ac.ir;cam.ac.uk",
        "github": "https://github.com/Sara-Rajaee/Multilingual-Isotropy",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Iran University of Science and Technology;Tehran Institute for Advanced Studies",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iust.ac.ir;",
        "aff_unique_abbr": "IUST;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tehran;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Iran"
    },
    {
        "id": "2022.acl-long.14",
        "title": "An Unsupervised Multiple-Task and Multiple-Teacher Model for Cross-lingual Named Entity Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages. Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer. However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains. Other possible auxiliary tasks to improve the learning performance have not been fully investigated. In this study, based on the knowledge distillation framework and multi-task learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain. Specifically, an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain. Then, two tasks in the student model are supervised by these teachers simultaneously. Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.",
        "author": "Zhuoran Li; Chunming Hu; Xiaohui Guo; Junfan Chen; Wenyi Qin; Richong Zhang",
        "authorids": "/z/zhuoran-li/; /c/chunming-hu/; /x/xiaohui-guo/; /j/junfan-chen/; /w/wenyi-qin/; /r/richong-zhang/",
        "bibtex": "@inproceedings{li-etal-2022-unsupervised-multiple,\n    title = \"An Unsupervised Multiple-Task and Multiple-Teacher Model for Cross-lingual Named Entity Recognition\",\n    author = \"Li, Zhuoran  and\n      Hu, Chunming  and\n      Guo, Xiaohui  and\n      Chen, Junfan  and\n      Qin, Wenyi  and\n      Zhang, Richong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.14/\",\n    doi = \"10.18653/v1/2022.acl-long.14\",\n    pages = \"170--179\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.14.pdf",
        "site": "https://aclanthology.org/2022.acl-long.14/",
        "pdf_size": 569342,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14316328156891769268&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 5,
        "aff": "SKLSDE, School of Computer Science and Engineering, Beihang University, Beijing, China; SKLSDE, School of Computer Science and Engineering, Beihang University, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; SKLSDE, School of Computer Science and Engineering, Beihang University, Beijing, China; SKLSDE, School of Computer Science and Engineering, Beihang University, Beijing, China; SKLSDE, School of Computer Science and Engineering, Beihang University, Beijing, China",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn;hotmail.com;act.buaa.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;act.buaa.edu.cn;act.buaa.edu.cn;hotmail.com;act.buaa.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "BUAA",
        "aff_campus_unique_index": "0;0;1;0;0;0",
        "aff_campus_unique": "Beijing;Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.18",
        "title": "Analyzing Dynamic Adversarial Training Data in the Limit",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on DADC examples make 26% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.",
        "author": "Eric Wallace; Adina Williams; Robin Jia; Douwe Kiela",
        "authorids": "/e/eric-wallace/; /a/adina-williams/; /r/robin-jia/; /d/douwe-kiela/",
        "bibtex": "@inproceedings{wallace-etal-2022-analyzing,\n    title = \"Analyzing Dynamic Adversarial Training Data in the Limit\",\n    author = \"Wallace, Eric  and\n      Williams, Adina  and\n      Jia, Robin  and\n      Kiela, Douwe\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.18/\",\n    doi = \"10.18653/v1/2022.findings-acl.18\",\n    pages = \"202--217\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.18.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.18/",
        "pdf_size": 534614,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15746288587393392163&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "UC Berkeley; Facebook AI Research; Facebook AI Research + USC; Facebook AI Research",
        "aff_domain": "berkeley.edu;fb.com; ;fb.com",
        "email": "berkeley.edu;fb.com; ;fb.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1+2;1",
        "aff_unique_norm": "University of California, Berkeley;Meta;University of Southern California",
        "aff_unique_dep": ";Facebook AI Research;",
        "aff_unique_url": "https://www.berkeley.edu;https://research.facebook.com;https://www.usc.edu",
        "aff_unique_abbr": "UC Berkeley;FAIR;USC",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Berkeley;;Los Angeles",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.518",
        "title": "Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Vision and language navigation (VLN) is a challenging visually-grounded language understanding task. Given a natural language navigation instruction, a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route. Most prior work has been conducted in indoor scenarios where best results were obtained for navigation on routes that are similar to the training routes, with sharp drops in performance when testing on unseen environments. We focus on VLN in outdoor scenarios and find that in contrast to indoor VLN, most of the gain in outdoor VLN on unseen data is due to features like junction type embedding or heading delta that are specific to the respective environment graph, while image information plays a very minor role in generalizing VLN to unseen outdoor areas. These findings show a bias to specifics of graph representations of urban environments, demanding that VLN tasks grow in scale and diversity of geographical environments.",
        "author": "Raphael Schumann; Stefan Riezler",
        "authorids": "/r/raphael-schumann/; /s/stefan-riezler/",
        "bibtex": "@inproceedings{schumann-riezler-2022-analyzing,\n    title = \"Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas\",\n    author = \"Schumann, Raphael  and\n      Riezler, Stefan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.518/\",\n    doi = \"10.18653/v1/2022.acl-long.518\",\n    pages = \"7519--7532\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.518.pdf",
        "site": "https://aclanthology.org/2022.acl-long.518/",
        "pdf_size": 648192,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7985038817819739995&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Heidelberg University, Germany; Heidelberg University, Germany",
        "aff_domain": "cl.uni-heidelberg.de;cl.uni-heidelberg.de",
        "email": "cl.uni-heidelberg.de;cl.uni-heidelberg.de",
        "github": "https://github.com/raphael-sch/map2seq_vln",
        "project": "https://map2seq.schumann.pub/vln/",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Heidelberg University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-heidelberg.de",
        "aff_unique_abbr": "Uni Heidelberg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-short.3",
        "title": "Analyzing Wrap-Up Effects through an Information-Theoretic Lens",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Numerous analyses of reading time (RT) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension. However, data measured on words at the end of a sentence\u2013or even clause\u2013is often omitted due to the confounding factors introduced by so-called \u201cwrap-up effects,\u201d which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these effects is limited. In this work, we attempt to learn more about these processes by looking for the existence\u2013or absence\u2013of a link between wrap-up effects and information theoretic quantities, such as word and context information content. We find that the information distribution of prior context is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs), which lends support to several prior hypotheses about the processes involved in wrap-up effects.",
        "author": "Clara Meister; Tiago Pimentel; Thomas Clark; Ryan Cotterell; Roger Levy",
        "authorids": "/c/clara-meister/; /t/tiago-pimentel/; /t/thomas-clark/; /r/ryan-cotterell/; /r/roger-levy/",
        "bibtex": "@inproceedings{meister-etal-2022-analyzing,\n    title = \"Analyzing Wrap-Up Effects through an Information-Theoretic Lens\",\n    author = \"Meister, Clara  and\n      Pimentel, Tiago  and\n      Clark, Thomas  and\n      Cotterell, Ryan  and\n      Levy, Roger\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.3/\",\n    doi = \"10.18653/v1/2022.acl-short.3\",\n    pages = \"20--28\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.3.pdf",
        "site": "https://aclanthology.org/2022.acl-short.3/",
        "pdf_size": 3561520,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1971684722711156809&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 8,
        "aff": "ETH Z\u00fcrich; University of Cambridge; Massachusetts Institute of Technology; ETH Z\u00fcrich; Massachusetts Institute of Technology",
        "aff_domain": "inf.ethz.ch;cam.ac.uk;mit.edu;inf.ethz.ch;mit.edu",
        "email": "inf.ethz.ch;cam.ac.uk;mit.edu;inf.ethz.ch;mit.edu",
        "github": "https://github.com/rycolab/wrap-up-effects",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;2",
        "aff_unique_norm": "ETH Zurich;University of Cambridge;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ethz.ch;https://www.cam.ac.uk;https://web.mit.edu",
        "aff_unique_abbr": "ETHZ;Cambridge;MIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;2;0;2",
        "aff_country_unique": "Switzerland;United Kingdom;United States"
    },
    {
        "id": "2022.findings-acl.82",
        "title": "Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Machine reading comprehension (MRC) has drawn a lot of attention as an approach for assessing the ability of systems to understand natural language. Usually systems focus on selecting the correct answer to a question given a contextual paragraph. However, for many applications of multiple-choice MRC systems there are two additional considerations. For multiple-choice exams there is often a negative marking scheme; there is a penalty for an incorrect answer. In terms of an MRC system this means that the system is required to have an idea of the uncertainty in the predicted answer. The second consideration is that many multiple-choice questions have the option of none-of-the-above (NOA) indicating that none of the answers is applicable, rather than there always being the correct answer in the list of choices. This paper investigates both of these issues by making use of predictive uncertainty. Whether the system should propose an answer is a direct application of answer uncertainty. There are two possibilities when considering the NOA option. The simplest is to explicitly build a system on data that includes this option. Alternatively uncertainty can be applied to detect whether the other options include the correct answer. If the system is not sufficiently confident it will select NOA. As there is no standard corpus available to investigate these topics, the ReClor corpus is modified by removing the correct answer from a subset of possible answers. A high-performance MRC system is used to evaluate whether answer uncertainty can be applied in these situations. It is shown that uncertainty does allow questions that the system is not confident about to be detected. Additionally it is shown that uncertainty outperforms a system explicitly built with an NOA option.",
        "author": "Vatsal Raina; Mark Gales",
        "authorids": "/v/vatsal-raina/; /m/mark-gales/",
        "bibtex": "@inproceedings{raina-gales-2022-answer,\n    title = \"Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension\",\n    author = \"Raina, Vatsal  and\n      Gales, Mark\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.82/\",\n    doi = \"10.18653/v1/2022.findings-acl.82\",\n    pages = \"1020--1034\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.82.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.82/",
        "pdf_size": 691033,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12724203859832605286&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Cambridge University; Cambridge University",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.49",
        "title": "Answer-level Calibration for Free-form Multiple Choice Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.",
        "author": "Sawan Kumar",
        "authorids": "/s/sawan-kumar/",
        "bibtex": "@inproceedings{kumar-2022-answer,\n    title = \"Answer-level Calibration for Free-form Multiple Choice Question Answering\",\n    author = \"Kumar, Sawan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.49/\",\n    doi = \"10.18653/v1/2022.acl-long.49\",\n    pages = \"665--679\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.49.pdf",
        "site": "https://aclanthology.org/2022.acl-long.49/",
        "pdf_size": 304258,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5402778041113873544&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 3,
        "aff": "Indian Institute of Science, Bangalore",
        "aff_domain": "iisc.ac.in",
        "email": "iisc.ac.in",
        "github": "https://github.com/SawanKumar28/alc665",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.128",
        "title": "Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify Framework",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Open-domain questions are likely to be open-ended and ambiguous, leading to multiple valid answers. Existing approaches typically adopt the rerank-then-read framework, where a reader reads top-ranking evidence to predict answers. According to our empirical analysis, this framework faces three problems: first, to leverage a large reader under a memory constraint, the reranker should select only a few relevant passages to cover diverse answers, while balancing relevance and diversity is non-trivial; second, the small reading budget prevents the reader from accessing valuable retrieved evidence filtered out by the reranker; third, when using a generative reader to predict answers all at once based on all selected evidence, whether a valid answer will be predicted also pathologically depends on evidence of some other valid answer(s). To address these issues, we propose to answer open-domain multi-answer questions with a recall-then-verify framework, which separates the reasoning process of each answer so that we can make better use of retrieved evidence while also leveraging large models under the same memory constraint. Our framework achieves state-of-the-art results on two multi-answer datasets, and predicts significantly more gold answers than a rerank-then-read system that uses an oracle reranker.",
        "author": "Zhihong Shao; Minlie Huang",
        "authorids": "/z/zhihong-shao/; /m/minlie-huang/",
        "bibtex": "@inproceedings{shao-huang-2022-answering,\n    title = \"Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify Framework\",\n    author = \"Shao, Zhihong  and\n      Huang, Minlie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.128/\",\n    doi = \"10.18653/v1/2022.acl-long.128\",\n    pages = \"1825--1838\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.128.pdf",
        "site": "https://aclanthology.org/2022.acl-long.128/",
        "pdf_size": 860352,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3890807063384625820&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The CoAI group, DCST, Tsinghua University+Institute for Artificial Intelligence+State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; The CoAI group, DCST, Tsinghua University+Institute for Artificial Intelligence+State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China",
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1+2+0;0+1+2+0",
        "aff_unique_norm": "Tsinghua University;Institute for Artificial Intelligence;State Key Lab of Intelligent Technology and Systems",
        "aff_unique_dep": "Department of Computer Science and Technology;Artificial Intelligence;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;",
        "aff_unique_abbr": "THU;;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+1+0+0;0+1+0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.47",
        "title": "AraT5: Text-to-Text Transformers for Arabic Language Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transfer learning with a unified Transformer framework (T5) that converts all language problems into a text-to-text format was recently proposed as a simple and effective transfer learning approach. Although a multilingual version of the T5 model (mT5) was also introduced, it is not clear how well it can fare on non-English tasks involving diverse data. To investigate this question, we apply mT5 on a language with a wide variety of dialects\u2013Arabic. For evaluation, we introduce a novel benchmark for ARabic language GENeration (ARGEN), covering seven important tasks. For model comparison, we pre-train three powerful Arabic T5-style models and evaluate them on ARGEN. Although pre-trained with ~49 less data, our new models perform significantly better than mT5 on all ARGEN tasks (in 52 out of 59 test sets) and set several new SOTAs. Our models also establish new SOTA on the recently-proposed, large Arabic language understanding evaluation benchmark ARLUE (Abdul-Mageed et al., 2021). Our new models are publicly available. We also link to ARGEN datasets through our repository: https://github.com/UBC-NLP/araT5.",
        "author": "El Moatez Billah Nagoudi; AbdelRahim Elmadany; Muhammad Abdul-Mageed",
        "authorids": "/e/el-moatez-billah-nagoudi/; /a/abdelrahim-elmadany/; /m/muhammad-abdul-mageed/",
        "bibtex": "@inproceedings{nagoudi-etal-2022-arat5,\n    title = \"{A}ra{T}5: Text-to-Text Transformers for {A}rabic Language Generation\",\n    author = \"Nagoudi, El Moatez Billah  and\n      Elmadany, AbdelRahim  and\n      Abdul-Mageed, Muhammad\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.47/\",\n    doi = \"10.18653/v1/2022.acl-long.47\",\n    pages = \"628--647\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.47.pdf",
        "site": "https://aclanthology.org/2022.acl-long.47/",
        "pdf_size": 630621,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=454841203932129027&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Deep Learning and Natural Language Processing Group, The University of British Columbia; Deep Learning and Natural Language Processing Group, The University of British Columbia; Deep Learning and Natural Language Processing Group, The University of British Columbia",
        "aff_domain": "ubc.ca;ubc.ca;ubc.ca",
        "email": "ubc.ca;ubc.ca;ubc.ca",
        "github": "https://github.com/UBC-NLP/araT5",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.166",
        "title": "Are Prompt-based Models Clueless?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets. Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective. Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues. This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.",
        "author": "Pride Kavumba; Ryo Takahashi; Yusuke Oda",
        "authorids": "/p/pride-kavumba/; /r/ryo-takahashi/; /y/yusuke-oda/",
        "bibtex": "@inproceedings{kavumba-etal-2022-prompt,\n    title = \"Are Prompt-based Models Clueless?\",\n    author = \"Kavumba, Pride  and\n      Takahashi, Ryo  and\n      Oda, Yusuke\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.166/\",\n    doi = \"10.18653/v1/2022.acl-long.166\",\n    pages = \"2333--2352\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.166.pdf",
        "site": "https://aclanthology.org/2022.acl-long.166/",
        "pdf_size": 5066600,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15523635380610430712&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Tohoku University+RIKEN AIP; LegalForce Research; LegalForce Research+Tohoku University",
        "aff_domain": "dc.tohoku.ac.jp;legalforce.co.jp;legalforce.co.jp",
        "email": "dc.tohoku.ac.jp;legalforce.co.jp;legalforce.co.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;2+0",
        "aff_unique_norm": "Tohoku University;RIKEN;LegalForce Research",
        "aff_unique_dep": ";Advanced Institute for Computational Science;",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.aip.riken.jp;",
        "aff_unique_abbr": "Tohoku U;RIKEN AIP;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;1+0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2022.acl-short.2",
        "title": "Are Shortest Rationales the Best Explanations for Human Understanding?",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201cresponsible for\u201d corresponding output \u2014 to explain the model prediction, with the assumption that shorter rationales are more intuitive to humans. However, this assumption has yet to be validated. Is the shortest rationale indeed the most human-understandable? To answer this question, we design a self-explaining model, LimitedInk, which allows users to extract rationales at any target length. Compared to existing baselines, LimitedInk achieves compatible end-task performance and human-annotated rationale agreement, making it a suitable representation of the recent class of self-explaining models. We use LimitedInk to conduct a user study on the impact of rationale length, where we ask human judges to predict the sentiment label of documents based only on LimitedInk-generated rationales with different lengths. We show rationales that are too short do not help humans predict labels better than randomly masked text, suggesting the need for more careful design of the best human rationales.",
        "author": "Hua Shen; Tongshuang Wu; Wenbo Guo; Ting-Hao Huang",
        "authorids": "/h/hua-shen/; /t/tongshuang-wu/; /w/wenbo-guo/; /t/ting-hao-huang/",
        "bibtex": "@inproceedings{shen-etal-2022-shortest,\n    title = \"Are Shortest Rationales the Best Explanations for Human Understanding?\",\n    author = \"Shen, Hua  and\n      Wu, Tongshuang  and\n      Guo, Wenbo  and\n      Huang, Ting-Hao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.2/\",\n    doi = \"10.18653/v1/2022.acl-short.2\",\n    pages = \"10--19\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.2.pdf",
        "site": "https://aclanthology.org/2022.acl-short.2/",
        "pdf_size": 1875182,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18198074315543486191&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 4,
        "aff": "College of Information Sciences and Technology, Pennsylvania State University; Paul G. Allen School of Computer Science and Engineering, University of Washington; College of Information Sciences and Technology, Pennsylvania State University; College of Information Sciences and Technology, Pennsylvania State University",
        "aff_domain": "psu.edu;cs.washington.edu;psu.edu;psu.edu",
        "email": "psu.edu;cs.washington.edu;psu.edu;psu.edu",
        "github": "https://github.com/huashen218/LimitedInk.git",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Pennsylvania State University;University of Washington",
        "aff_unique_dep": "College of Information Sciences and Technology;Paul G. Allen School of Computer Science and Engineering",
        "aff_unique_url": "https://www.psu.edu;https://www.washington.edu",
        "aff_unique_abbr": "PSU;UW",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.53",
        "title": "As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.",
        "author": "Jannis Vamvas; Rico Sennrich",
        "authorids": "/j/jannis-vamvas/; /r/rico-sennrich/",
        "bibtex": "@inproceedings{vamvas-sennrich-2022-little,\n    title = \"As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning\",\n    author = \"Vamvas, Jannis  and\n      Sennrich, Rico\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.53/\",\n    doi = \"10.18653/v1/2022.acl-short.53\",\n    pages = \"490--500\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.53.pdf",
        "site": "https://aclanthology.org/2022.acl-short.53/",
        "pdf_size": 406376,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4838543912361833123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computational Linguistics, University of Zurich + School of Informatics, University of Edinburgh; Department of Computational Linguistics, University of Zurich + School of Informatics, University of Edinburgh",
        "aff_domain": "cl.uzh.ch;cl.uzh.ch",
        "email": "cl.uzh.ch;cl.uzh.ch",
        "github": "https://github.com/ZurichNLP/coverage-contrastive-conditioning490",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Zurich;University of Edinburgh",
        "aff_unique_dep": "Department of Computational Linguistics;School of Informatics",
        "aff_unique_url": "https://www.unizh.ch;https://www.ed.ac.uk",
        "aff_unique_abbr": "UZH;Edinburgh",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Edinburgh",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "Switzerland;United Kingdom"
    },
    {
        "id": "2022.findings-acl.211",
        "title": "Assessing Multilingual Fairness in Pre-trained Multimodal Representations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently pre-trained multimodal models, such as CLIP, have shown exceptional capabilities towards connecting images and natural language. The textual representations in English can be desirably transferred to multilingualism and support downstream multimodal tasks for different languages. Nevertheless, the principle of multilingual fairness is rarely scrutinized: do multilingual multimodal models treat languages equally? Are their performances biased towards particular languages? To answer these questions, we view language as the fairness recipient and introduce two new fairness notions, multilingual individual fairness and multilingual group fairness, for pre-trained multimodal models. Multilingual individual fairness requires that text snippets expressing similar semantics in different languages connect similarly to images, while multilingual group fairness requires equalized predictive performance across languages. We characterize the extent to which pre-trained multilingual vision-and-language representations are individually fair across languages. However, extensive experiments demonstrate that multilingual representations do not satisfy group fairness: (1) there is a severe multilingual accuracy disparity issue; (2) the errors exhibit biases across languages conditioning the group of people in the images, including race, gender and age.",
        "author": "Jialu Wang; Yang Liu; Xin Wang",
        "authorids": "/j/jialu-wang/; /y/yang-liu-umich/; /x/xin-wang/",
        "bibtex": "@inproceedings{wang-etal-2022-assessing,\n    title = \"Assessing Multilingual Fairness in Pre-trained Multimodal Representations\",\n    author = \"Wang, Jialu  and\n      Liu, Yang  and\n      Wang, Xin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.211/\",\n    doi = \"10.18653/v1/2022.findings-acl.211\",\n    pages = \"2681--2695\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.211.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.211/",
        "pdf_size": 581842,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=376473939726074077&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Engineering, University of California, Santa Cruz; Department of Computer Science and Engineering, University of California, Santa Cruz; Department of Computer Science and Engineering, University of California, Santa Cruz",
        "aff_domain": "ucsc.edu;ucsc.edu;ucsc.edu",
        "email": "ucsc.edu;ucsc.edu;ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Santa Cruz",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsc.edu",
        "aff_unique_abbr": "UCSC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Santa Cruz",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.313",
        "title": "Attention Mechanism with Energy-Friendly Operations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Attention mechanism has become the dominant module in natural language processing models. It is computationally intensive and depends on massive power-hungry multiplications. In this paper, we rethink variants of attention mechanism from the energy consumption aspects. After reaching the conclusion that the energy costs of several energy-friendly operations are far less than their multiplication counterparts, we build a novel attention model by replacing multiplications with either selective operations or additions. Empirical results on three machine translation tasks demonstrate that the proposed model, against the vanilla one, achieves competitable accuracy while saving 99% and 66% energy during alignment calculation and the whole attention procedure. Our code will be released upon the acceptance.",
        "author": "Yu Wan; Baosong Yang; Dayiheng Liu; Rong Xiao; Derek Wong; Haibo Zhang; Boxing Chen; Lidia Chao",
        "authorids": "/y/yu-wan/; /b/baosong-yang/; /d/dayiheng-liu/; /r/rong-xiao/; /d/derek-wong/; /h/haibo-zhang/; /b/boxing-chen/; /l/lidia-chao/",
        "bibtex": "@inproceedings{wan-etal-2022-attention,\n    title = \"Attention Mechanism with Energy-Friendly Operations\",\n    author = \"Wan, Yu  and\n      Yang, Baosong  and\n      Liu, Dayiheng  and\n      Xiao, Rong  and\n      Wong, Derek  and\n      Zhang, Haibo  and\n      Chen, Boxing  and\n      Chao, Lidia\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.313/\",\n    doi = \"10.18653/v1/2022.findings-acl.313\",\n    pages = \"3969--3976\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.313.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.313/",
        "pdf_size": 392899,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3436018584671994553&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "NLP2CT Lab, University of Macau + Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group + NLP2CT Lab, University of Macau; NLP2CT Lab, University of Macau; Alibaba Group; Alibaba Group; NLP2CT Lab, University of Macau",
        "aff_domain": "gmail.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;umac.mo;alibaba-inc.com;alibaba-inc.com;umac.mo",
        "email": "gmail.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;umac.mo;alibaba-inc.com;alibaba-inc.com;umac.mo",
        "github": "https://github.com/NLP2CT/E-Att",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;1;1+0;0;1;1;0",
        "aff_unique_norm": "University of Macau;Alibaba Group",
        "aff_unique_dep": "NLP2CT Lab;",
        "aff_unique_url": "https://www.um.edu.mo;https://www.alibaba.com",
        "aff_unique_abbr": "UM;Alibaba",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Macau SAR;",
        "aff_country_unique_index": "0+0;0;0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.11",
        "title": "Attention Temperature Matters in Abstractive Summarization Distillation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent progress of abstractive text summarization largely relies on large pre-trained sequence-to-sequence Transformer models, which are computationally expensive. This paper aims to distill these large models into smaller ones for faster inference and with minimal performance loss. Pseudo-labeling based methods are popular in sequence-to-sequence model distillation. In this paper, we find simply manipulating attention temperatures in Transformers can make pseudo labels easier to learn for student models. Our experiments on three summarization datasets show our proposed method consistently improves vanilla pseudo-labeling based methods. Further empirical analysis shows that both pseudo labels and summaries produced by our students are shorter and more abstractive.",
        "author": "Shengqiang Zhang; Xingxing Zhang; Hangbo Bao; Furu Wei",
        "authorids": "/s/shengqiang-zhang/; /x/xingxing-zhang/; /h/hangbo-bao/; /f/furu-wei/",
        "bibtex": "@inproceedings{zhang-etal-2022-attention,\n    title = \"Attention Temperature Matters in Abstractive Summarization Distillation\",\n    author = \"Zhang, Shengqiang  and\n      Zhang, Xingxing  and\n      Bao, Hangbo  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.11/\",\n    doi = \"10.18653/v1/2022.acl-long.11\",\n    pages = \"127--141\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.11.pdf",
        "site": "https://aclanthology.org/2022.acl-long.11/",
        "pdf_size": 524890,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10926558350008069877&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Peking University; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia",
        "aff_domain": "pku.edu.cn;microsoft.com;microsoft.com;microsoft.com",
        "email": "pku.edu.cn;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/Shengqiang-Zhang/plate",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Peking University;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "Peking U;MSR Asia",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.320",
        "title": "Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We explore how a multi-modal transformer trained for generation of longer image descriptions learns syntactic and semantic representations about entities and relations grounded in objects at the level of masked self-attention (text generation) and cross-modal attention (information fusion). We observe that cross-attention learns the visual grounding of noun phrases into objects and high-level semantic information about spatial relations, while text-to-text attention captures low-level syntactic knowledge between words. This concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-only). Our code is available here: https://github.com/GU-CLASP/attention-as-grounding.",
        "author": "Nikolai Ilinykh; Simon Dobnik",
        "authorids": "/n/nikolai-ilinykh/; /s/simon-dobnik/",
        "bibtex": "@inproceedings{ilinykh-dobnik-2022-attention,\n    title = \"Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer\",\n    author = \"Ilinykh, Nikolai  and\n      Dobnik, Simon\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.320/\",\n    doi = \"10.18653/v1/2022.findings-acl.320\",\n    pages = \"4062--4073\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.320.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.320/",
        "pdf_size": 781606,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15180997330135705181&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Centre for Linguistic Theory and Studies in Probability (CLASP) + Department of Philosophy, Linguistics and Theory of Science (FLoV) + University of Gothenburg, Sweden; Centre for Linguistic Theory and Studies in Probability (CLASP) + Department of Philosophy, Linguistics and Theory of Science (FLoV) + University of Gothenburg, Sweden",
        "aff_domain": "gu.se;gu.se",
        "email": "gu.se;gu.se",
        "github": "https://github.com/GU-CLASP/attention-as-grounding",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1+1;0+1+1",
        "aff_unique_norm": "Centre for Linguistic Theory and Studies in Probability;University of Gothenburg",
        "aff_unique_dep": "Linguistic Theory and Studies in Probability;Department of Philosophy, Linguistics and Theory of Science",
        "aff_unique_url": ";https://www.gu.se",
        "aff_unique_abbr": "CLASP;GU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1+1;1+1",
        "aff_country_unique": ";Sweden"
    },
    {
        "id": "2022.acl-short.48",
        "title": "Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Dense retrieval models, which aim at retrieving the most relevant document for an input query on a dense representation space, have gained considerable attention for their remarkable success. Yet, dense models require a vast amount of labeled training data for notable performance, whereas it is often challenging to acquire query-document pairs annotated by humans. To tackle this problem, we propose a simple but effective Document Augmentation for dense Retrieval (DAR) framework, which augments the representations of documents with their interpolation and perturbation. We validate the performance of DAR on retrieval tasks with two benchmark datasets, showing that the proposed DAR significantly outperforms relevant baselines on the dense retrieval of both the labeled and unlabeled documents.",
        "author": "Soyeong Jeong; Jinheon Baek; Sukmin Cho; Sung Ju Hwang; Jong Park",
        "authorids": "/s/soyeong-jeong/; /j/jinheon-baek/; /s/sukmin-cho/; /s/sung-ju-hwang/; /j/jong-c-park/",
        "bibtex": "@inproceedings{jeong-etal-2022-augmenting,\n    title = \"Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation\",\n    author = \"Jeong, Soyeong  and\n      Baek, Jinheon  and\n      Cho, Sukmin  and\n      Hwang, Sung Ju  and\n      Park, Jong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.48/\",\n    doi = \"10.18653/v1/2022.acl-short.48\",\n    pages = \"442--452\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.48.pdf",
        "site": "https://aclanthology.org/2022.acl-short.48/",
        "pdf_size": 546983,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3821876042624109735&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computing1+Graduate School of AI2; School of Computing1+Graduate School of AI2; School of Computing1+Graduate School of AI2; School of Computing1+Graduate School of AI2; School of Computing1+Graduate School of AI2",
        "aff_domain": "nlp.kaist.ac.kr;nlp.kaist.ac.kr;nlp.kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "nlp.kaist.ac.kr;nlp.kaist.ac.kr;nlp.kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "School of Computing;AI2",
        "aff_unique_dep": "Computing;Graduate School",
        "aff_unique_url": ";https://www.ai2.edu",
        "aff_unique_abbr": ";AI2",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2022.acl-long.72",
        "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
        "author": "Yue Guo; Yi Yang; Ahmed Abbasi",
        "authorids": "/y/yue-guo/; /y/yi-yang/; /a/ahmed-abbasi/",
        "bibtex": "@inproceedings{guo-etal-2022-auto,\n    title = \"Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts\",\n    author = \"Guo, Yue  and\n      Yang, Yi  and\n      Abbasi, Ahmed\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.72/\",\n    doi = \"10.18653/v1/2022.acl-long.72\",\n    pages = \"1012--1023\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.72.pdf",
        "site": "https://aclanthology.org/2022.acl-long.72/",
        "pdf_size": 445451,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6748260628347142494&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; University of Notre Dame",
        "aff_domain": "connect.ust.hk;ust.hk;nd.edu",
        "email": "connect.ust.hk;ust.hk;nd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;University of Notre Dame",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ust.hk;https://www.nd.edu",
        "aff_unique_abbr": "HKUST;Notre Dame",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.219",
        "title": "Automated Crossword Solving",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present the Berkeley Crossword Solver, a state-of-the-art approach for automatically solving crossword puzzles. Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions. Compared to existing approaches, our system improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.9% letter accuracy on themeless puzzles. Our system also won first place at the top human crossword tournament, which marks the first time that a computer program has surpassed human performance at this event. To facilitate research on question answering and crossword solving, we analyze our system\u2019s remaining errors and release a dataset of over six million question-answer pairs.",
        "author": "Eric Wallace; Nicholas Tomlin; Albert Xu; Kevin Yang; Eshaan Pathak; Matthew Ginsberg; Dan Klein",
        "authorids": "/e/eric-wallace/; /n/nicholas-tomlin/; /a/albert-xu/; /k/kevin-yang/; /e/eshaan-pathak/; /m/matthew-ginsberg/; /d/dan-klein/",
        "bibtex": "@inproceedings{wallace-etal-2022-automated,\n    title = \"Automated Crossword Solving\",\n    author = \"Wallace, Eric  and\n      Tomlin, Nicholas  and\n      Xu, Albert  and\n      Yang, Kevin  and\n      Pathak, Eshaan  and\n      Ginsberg, Matthew  and\n      Klein, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.219/\",\n    doi = \"10.18653/v1/2022.acl-long.219\",\n    pages = \"3073--3085\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.219.pdf",
        "site": "https://aclanthology.org/2022.acl-long.219/",
        "pdf_size": 1292309,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15524448056715417604&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; Matthew Ginsberg, LLC + UC Berkeley; UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu; ; ; ;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu; ; ; ;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1+0;0",
        "aff_unique_norm": "University of California, Berkeley;Matthew Ginsberg, LLC",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.10",
        "title": "Automatic Detection of Entity-Manipulated Text using Factual Knowledge",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In this work, we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article (e.g., replacing entities with factually incorrect entities). Such manipulated articles can mislead the reader by posing as a human written news article. We propose a neural network based detector that detects manipulated news articles by reasoning about the facts mentioned in the article. Our proposed detector exploits factual knowledge via graph convolutional neural network along with the textual information in the news article. We also create challenging datasets for this task by considering various strategies to generate the new replacement entity (e.g., entity generation from GPT-2). In all the settings, our proposed model either matches or outperforms the state-of-the-art detector in terms of accuracy. Our code and data are available at https://github.com/UBC-NLP/manipulated_entity_detection.",
        "author": "Ganesh Jawahar; Muhammad Abdul-Mageed; Laks Lakshmanan",
        "authorids": "/g/ganesh-jawahar/; /m/muhammad-abdul-mageed/; /l/laks-lakshmanan/",
        "bibtex": "@inproceedings{jawahar-etal-2022-automatic,\n    title = \"Automatic Detection of Entity-Manipulated Text using Factual Knowledge\",\n    author = \"Jawahar, Ganesh  and\n      Abdul-Mageed, Muhammad  and\n      Lakshmanan, Laks\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.10/\",\n    doi = \"10.18653/v1/2022.acl-short.10\",\n    pages = \"86--93\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.10.pdf",
        "site": "https://aclanthology.org/2022.acl-short.10/",
        "pdf_size": 347673,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10030268425650450388&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Deep Learning & Natural Language Processing Group+Data Management & Mining Group; Deep Learning & Natural Language Processing Group+Data Management & Mining Group; Deep Learning & Natural Language Processing Group+Data Management & Mining Group",
        "aff_domain": "gmail.com;cs.ubc.ca;cs.ubc.ca",
        "email": "gmail.com;cs.ubc.ca;cs.ubc.ca",
        "github": "https://github.com/UBC-NLP/manipulated_entity_detection",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "University of California, Berkeley;Data Management & Mining Group",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences;Data Management & Mining",
        "aff_unique_url": "https://www.eecs.berkeley.edu;",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2022.acl-long.274",
        "title": "Automatic Error Analysis for Document-level Information Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches, however, has been limited in a number of dimensions. In particular, the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make. We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction. We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains; and then, to gauge progress in IE since its inception 30 years ago, vs. four systems from the MUC-4 (1992) evaluation.",
        "author": "Aliva Das; Xinya Du; Barry Wang; Kejian Shi; Jiayuan Gu; Thomas Porter; Claire Cardie",
        "authorids": "/a/aliva-das/; /x/xinya-du/; /b/barry-wang/; /k/kejian-shi/; /j/jiayuan-gu/; /t/thomas-porter/; /c/claire-cardie/",
        "bibtex": "@inproceedings{das-etal-2022-automatic,\n    title = \"Automatic Error Analysis for Document-level Information Extraction\",\n    author = \"Das, Aliva  and\n      Du, Xinya  and\n      Wang, Barry  and\n      Shi, Kejian  and\n      Gu, Jiayuan  and\n      Porter, Thomas  and\n      Cardie, Claire\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.274/\",\n    doi = \"10.18653/v1/2022.acl-long.274\",\n    pages = \"3960--3975\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.274.pdf",
        "site": "https://aclanthology.org/2022.acl-long.274/",
        "pdf_size": 550703,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10963879115345999288&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University",
        "aff_domain": "cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu",
        "email": "cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu;cornell.edu",
        "github": "https://github.com/IceJinx33/auto-err-template-fill/",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.273",
        "title": "Automatic Identification and Classification of Bragging in Social Media",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Bragging is a speech act employed with the goal of constructing a favorable self-image through positive statements about oneself. It is widespread in daily communication and especially popular in social media, where users aim to build a positive image of their persona directly or indirectly. In this paper, we present the first large scale study of bragging in computational linguistics, building on previous research in linguistics and pragmatics. To facilitate this, we introduce a new publicly available data set of tweets annotated for bragging and their types. We empirically evaluate different transformer-based models injected with linguistic information in (a) binary bragging classification, i.e., if tweets contain bragging statements or not; and (b) multi-class bragging type prediction including not bragging. Our results show that our models can predict bragging with macro F1 up to 72.42 and 35.95 in the binary and multi-class classification tasks respectively. Finally, we present an extensive linguistic and error analysis of bragging prediction to guide future research on this topic.",
        "author": "Mali Jin; Daniel Preotiuc-Pietro; A. Seza Do\u011fru\u00f6z; Nikolaos Aletras",
        "authorids": "/m/mali-jin/; /d/daniel-preotiuc-pietro/; /a/a-seza-dogruoz/; /n/nikolaos-aletras/",
        "bibtex": "@inproceedings{jin-etal-2022-automatic,\n    title = \"Automatic Identification and Classification of Bragging in Social Media\",\n    author = {Jin, Mali  and\n      Preotiuc-Pietro, Daniel  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Aletras, Nikolaos},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.273/\",\n    doi = \"10.18653/v1/2022.acl-long.273\",\n    pages = \"3945--3959\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.273.pdf",
        "site": "https://aclanthology.org/2022.acl-long.273/",
        "pdf_size": 754671,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5043029460891857012&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Sheffield; Bloomberg; Ghent University; University of Sheffield",
        "aff_domain": "sheffield.ac.uk;bloomberg.net;ugent.be;sheffield.ac.uk",
        "email": "sheffield.ac.uk;bloomberg.net;ugent.be;sheffield.ac.uk",
        "github": "",
        "project": "https://archive.org/details/bragging_data",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Sheffield;Bloomberg;Ghent University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.bloomberg.com;https://www.ugent.be/en",
        "aff_unique_abbr": "Sheffield;Bloomberg;UGent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "United Kingdom;United States;Belgium"
    },
    {
        "id": "2022.findings-acl.60",
        "title": "Automatic Song Translation for Tonal Languages",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper develops automatic song translation (AST) for tonal languages and addresses the unique challenge of aligning words\u2019 tones with melody of a song in addition to conveying the original meaning. We propose three criteria for effective AST\u2014preserving meaning, singability and intelligibility\u2014and design metrics for these criteria. We develop a new benchmark for English\u2013Mandarin song translation and develop an unsupervised AST system, Guided AliGnment for Automatic Song Translation (GagaST), which combines pre-training with three decoding constraints. Both automatic and human evaluations show GagaST successfully balances semantics and singability.",
        "author": "Fenfei Guo; Chen Zhang; Zhirui Zhang; Qixin He; Kejun Zhang; Jun Xie; Jordan Boyd-Graber",
        "authorids": "/f/fenfei-guo/; /c/chen-zhang/; /z/zhirui-zhang/; /q/qixin-he/; /k/kejun-zhang/; /j/jun-xie/; /j/jordan-boyd-graber/",
        "bibtex": "@inproceedings{guo-etal-2022-automatic,\n    title = \"Automatic Song Translation for Tonal Languages\",\n    author = \"Guo, Fenfei  and\n      Zhang, Chen  and\n      Zhang, Zhirui  and\n      He, Qixin  and\n      Zhang, Kejun  and\n      Xie, Jun  and\n      Boyd-Graber, Jordan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.60/\",\n    doi = \"10.18653/v1/2022.findings-acl.60\",\n    pages = \"729--743\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.60.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.60/",
        "pdf_size": 2849552,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2462191112723541571&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 6,
        "aff": "University of Maryland; Zhejiang University; Tencent AI Lab; Purdue University; Zhejiang University; Alibaba DAMO Academy; CS, iSchool, UMIACS, LSC University of Maryland",
        "aff_domain": "umd.edu;zju.edu.cn;gmail.com;purdue.edu;zju.edu.cn;alibaba-inc.com;umiacs.umd.edu",
        "email": "umd.edu;zju.edu.cn;gmail.com;purdue.edu;zju.edu.cn;alibaba-inc.com;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;1;4;0",
        "aff_unique_norm": "University of Maryland;Zhejiang University;Tencent;Purdue University;Alibaba Group",
        "aff_unique_dep": ";;Tencent AI Lab;;DAMO Academy",
        "aff_unique_url": "https://www/umd.edu;https://www.zju.edu.cn;https://ai.tencent.com;https://www.purdue.edu;https://www.alibaba-group.com",
        "aff_unique_abbr": "UMD;ZJU;Tencent AI Lab;Purdue;Alibaba DAMO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.findings-acl.197",
        "title": "Automatic Speech Recognition and Query By Example for Creole Languages Documentation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We investigate the exploitation of self-supervised models for two Creole languages with few resources: Gwadloup\u00e9yen and Morisien. Automatic language processing tools are almost non-existent for these two languages. We propose to use about one hour of annotated data to design an automatic speech recognition system for each language. We evaluate how much data is needed to obtain a query-by-example system that is usable by linguists. Moreover, our experiments show that multilingual self-supervised models are not necessarily the most efficient for Creole languages.",
        "author": "C\u00e9cile Macaire; Didier Schwab; Benjamin Lecouteux; Emmanuel Schang",
        "authorids": "/c/cecile-macaire/; /d/didier-schwab/; /b/benjamin-lecouteux/; /e/emmanuel-schang/",
        "bibtex": "@inproceedings{macaire-etal-2022-automatic,\n    title = \"Automatic Speech Recognition and Query By Example for Creole Languages Documentation\",\n    author = \"Macaire, C{\\'e}cile  and\n      Schwab, Didier  and\n      Lecouteux, Benjamin  and\n      Schang, Emmanuel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.197/\",\n    doi = \"10.18653/v1/2022.findings-acl.197\",\n    pages = \"2512--2520\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.197.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.197/",
        "pdf_size": 166521,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16925151587948190429&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France; Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France; Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France; LLL, UMR 7270, Univ. Orl\u00e9ans & CNRS",
        "aff_domain": "univ-grenoble-alpes.fr;univ-grenoble-alpes.fr;univ-grenoble-alpes.fr;univ-orleans.fr",
        "email": "univ-grenoble-alpes.fr;univ-grenoble-alpes.fr;univ-grenoble-alpes.fr;univ-orleans.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Universite Grenoble Alpes;University of Orl\u00e9ans",
        "aff_unique_dep": "Laboratoire d'Informatique de Grenoble (LIG);LLL, UMR 7270",
        "aff_unique_url": "https://www.univ-grenoble-alpes.fr;https://www.univ-orleans.fr",
        "aff_unique_abbr": "UGA;Univ. Orl\u00e9ans",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Grenoble;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.findings-acl.190",
        "title": "Auxiliary tasks to boost Biaffine Semantic Dependency Parsing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The biaffine parser of (CITATION) was successfully extended to semantic dependency parsing (SDP) (CITATION). Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens).To circumvent such an independence of decision, while retaining the O(n2) complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs. Experiments on the three English acyclic datasets of SemEval-2015 task 18 (CITATION), and on French deep syntactic cyclic graphs (CITATION) show modest but systematic performance gains on a near-state-of-the-art baseline using transformer-based contextualized representations. This provides a simple and robust method to boost SDP performance.",
        "author": "Marie Candito",
        "authorids": "/m/marie-candito/",
        "bibtex": "@inproceedings{candito-2022-auxiliary,\n    title = \"Auxiliary tasks to boost Biaffine Semantic Dependency Parsing\",\n    author = \"Candito, Marie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.190/\",\n    doi = \"10.18653/v1/2022.findings-acl.190\",\n    pages = \"2422--2429\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.190.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.190/",
        "pdf_size": 297730,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7995263839441104495&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "LLF, Universit\u00e9 Paris Cit\u00e9 - CNRS, Paris, France",
        "aff_domain": "u-paris.fr",
        "email": "u-paris.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Universit\u00e9 Paris Cit\u00e9",
        "aff_unique_dep": "LLF",
        "aff_unique_url": "https://www.univ-paris.fr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.findings-acl.165",
        "title": "BBQ: A hand-built bias benchmark for question answering",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model\u2019s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model\u2019s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
        "author": "Alicia Parrish; Angelica Chen; Nikita Nangia; Vishakh Padmakumar; Jason Phang; Jana Thompson; Phu Mon Htut; Samuel Bowman",
        "authorids": "/a/alicia-parrish/; /a/angelica-chen/; /n/nikita-nangia/; /v/vishakh-padmakumar/; /j/jason-phang/; /j/jana-thompson/; /p/phu-mon-htut/; /s/samuel-bowman/",
        "bibtex": "@inproceedings{parrish-etal-2022-bbq,\n    title = \"{BBQ}: A hand-built bias benchmark for question answering\",\n    author = \"Parrish, Alicia  and\n      Chen, Angelica  and\n      Nangia, Nikita  and\n      Padmakumar, Vishakh  and\n      Phang, Jason  and\n      Thompson, Jana  and\n      Htut, Phu Mon  and\n      Bowman, Samuel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.165/\",\n    doi = \"10.18653/v1/2022.findings-acl.165\",\n    pages = \"2086--2105\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.165.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.165/",
        "pdf_size": 1143441,
        "gs_citation": 392,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8397018703299533742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "New York University Dept. of Linguistics; New York University Center for Data Science; New York University Dept. of Computer Science; New York University Dept. of Linguistics; New York University Center for Data Science; New York University Dept. of Computer Science; New York University Dept. of Computer Science; New York University Dept. of Linguistics+New York University Center for Data Science+New York University Dept. of Computer Science",
        "aff_domain": "nyu.edu; ; ; ; ; ; ;nyu.edu",
        "email": "nyu.edu; ; ; ; ; ; ;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0+0+0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Dept. of Linguistics",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0+0+0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.485",
        "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.",
        "author": "Wangchunshu Zhou; Canwen Xu; Julian McAuley",
        "authorids": "/w/wangchunshu-zhou/; /c/canwen-xu/; /j/julian-mcauley/",
        "bibtex": "@inproceedings{zhou-etal-2022-bert,\n    title = \"{BERT} Learns to Teach: Knowledge Distillation with Meta Learning\",\n    author = \"Zhou, Wangchunshu  and\n      Xu, Canwen  and\n      McAuley, Julian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.485/\",\n    doi = \"10.18653/v1/2022.acl-long.485\",\n    pages = \"7037--7049\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.485.pdf",
        "site": "https://aclanthology.org/2022.acl-long.485/",
        "pdf_size": 551423,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1310613232276839587&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University; University of California, San Diego; University of California, San Diego",
        "aff_domain": "stanford.edu;ucsd.edu;ucsd.edu",
        "email": "stanford.edu;ucsd.edu;ucsd.edu",
        "github": "https://github.com/JetRunner/MetaDistil",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Stanford University;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "Stanford;UCSD",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Stanford;San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.78",
        "title": "BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Morphologically-rich polysynthetic languages present a challenge for NLP systems due to data sparsity, and a common strategy to handle this issue is to apply subword segmentation. We investigate a wide variety of supervised and unsupervised morphological segmentation methods for four polysynthetic languages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika. Then, we compare the morphologically inspired segmentation methods against Byte-Pair Encodings (BPEs) as inputs for machine translation (MT) when translating to and from Spanish. We show that for all language pairs except for Nahuatl, an unsupervised morphological segmentation algorithm outperforms BPEs consistently and that, although supervised methods achieve better segmentation scores, they under-perform in MT challenges. Finally, we contribute two new morphological segmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus for Raramuri\u2013Spanish.",
        "author": "Manuel Mager; Arturo Oncevay; Elisabeth Mager; Katharina Kann; Thang Vu",
        "authorids": "/m/manuel-mager/; /a/arturo-oncevay/; /e/elisabeth-maier/; /k/katharina-von-der-wense/; /t/thang-vu/",
        "bibtex": "@inproceedings{mager-etal-2022-bpe,\n    title = \"{BPE} vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages\",\n    author = \"Mager, Manuel  and\n      Oncevay, Arturo  and\n      Mager, Elisabeth  and\n      Kann, Katharina  and\n      Vu, Thang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.78/\",\n    doi = \"10.18653/v1/2022.findings-acl.78\",\n    pages = \"961--971\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.78.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.78/",
        "pdf_size": 380519,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17990886170719929134&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "University of Stuttgart; University of Edinburgh; Universidad Nacional Aut\u00f3noma de M\u00e9xico; University of Colorado Boulder; University of Stuttgart",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "University of Stuttgart;University of Edinburgh;Universidad Nacional Aut\u00f3noma de M\u00e9xico;University of Colorado",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uni-stuttgart.de;https://www.ed.ac.uk;https://www.unam.mx;https://www.colorado.edu",
        "aff_unique_abbr": "USTuttgart;Edinburgh;UNAM;CU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Boulder",
        "aff_country_unique_index": "0;1;2;3;0",
        "aff_country_unique": "Germany;United Kingdom;Mexico;United States"
    },
    {
        "id": "2022.acl-long.207",
        "title": "BRIO: Bringing Order to Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.",
        "author": "Yixin Liu; Pengfei Liu; Dragomir Radev; Graham Neubig",
        "authorids": "/y/yixin-liu/; /p/pengfei-liu/; /d/dragomir-radev/; /g/graham-neubig/",
        "bibtex": "@inproceedings{liu-etal-2022-brio,\n    title = \"{BRIO}: Bringing Order to Abstractive Summarization\",\n    author = \"Liu, Yixin  and\n      Liu, Pengfei  and\n      Radev, Dragomir  and\n      Neubig, Graham\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.207/\",\n    doi = \"10.18653/v1/2022.acl-long.207\",\n    pages = \"2890--2903\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.207.pdf",
        "site": "https://aclanthology.org/2022.acl-long.207/",
        "pdf_size": 504891,
        "gs_citation": 334,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3459568979249942624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Yale University; Carnegie Mellon University; Yale University; Carnegie Mellon University",
        "aff_domain": "yale.edu;yale.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "yale.edu;yale.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/yixinL7/BRIO",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Yale University;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.yale.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Yale;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.279",
        "title": "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today\u2019s state of the art. We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models. These results question the importance of synthetic graphs used in modern text classifiers. In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an \ud835\udcaa(N2) graph, where N is the vocabulary plus corpus size. Finally, since Transformers need to compute \ud835\udcaa(L2) attention weights with sequence length L, the MLP models show higher training and inference speeds on datasets with long sequences.",
        "author": "Lukas Galke; Ansgar Scherp",
        "authorids": "/l/lukas-galke/; /a/ansgar-scherp/",
        "bibtex": "@inproceedings{galke-scherp-2022-bag,\n    title = \"Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide {MLP}\",\n    author = \"Galke, Lukas  and\n      Scherp, Ansgar\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.279/\",\n    doi = \"10.18653/v1/2022.acl-long.279\",\n    pages = \"4038--4051\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.279.pdf",
        "site": "https://aclanthology.org/2022.acl-long.279/",
        "pdf_size": 295862,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9273563609249820763&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of Kiel, Germany + MPI for Psycholinguistics, the Netherlands; University of Ulm, Germany",
        "aff_domain": "mpi.nl;uni-ulm.de",
        "email": "mpi.nl;uni-ulm.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Christian-Albrechts-Universit\u00e4t zu Kiel;Max Planck Institute for Psycholinguistics;University of Ulm",
        "aff_unique_dep": ";Psycholinguistics;",
        "aff_unique_url": "https://www.uni-kiel.de;https://www.mpi.nl;https://www.uni-ulm.de",
        "aff_unique_abbr": "CAU;MPI;Ulm",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Germany;Netherlands"
    },
    {
        "id": "2022.acl-long.307",
        "title": "BenchIE: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Intrinsic evaluations of OIE systems are carried out either manually\u2014with human evaluators judging the correctness of extractions\u2014or automatically, on standardized benchmarks. The latter, while much more cost-effective, is less reliable, primarily because of the incompleteness of the existing OIE benchmarks: the ground truth extractions do not include all acceptable variants of the same fact, leading to unreliable assessment of the models\u2019 performance. Moreover, the existing OIE benchmarks are available for English only. In this work, we introduce BenchIE: a benchmark and evaluation framework for comprehensive evaluation of OIE systems for English, Chinese, and German. In contrast to existing OIE benchmarks, BenchIE is fact-based, i.e., it takes into account informational equivalence of extractions: our gold standard consists of fact synsets, clusters in which we exhaustively list all acceptable surface forms of the same fact. Moreover, having in mind common downstream applications for OIE, we make BenchIE multi-faceted; i.e., we create benchmark variants that focus on different facets of OIE evaluation, e.g., compactness or minimality of extractions. We benchmark several state-of-the-art OIE systems using BenchIE and demonstrate that these systems are significantly less effective than indicated by existing OIE benchmarks. We make BenchIE (data and evaluation code) publicly available.",
        "author": "Kiril Gashteovski; Mingying Yu; Bhushan Kotnis; Carolin Lawrence; Mathias Niepert; Goran Glava\u0161",
        "authorids": "/k/kiril-gashteovski/; /m/mingying-yu/; /b/bhushan-kotnis/; /c/carolin-lawrence/; /m/mathias-niepert/; /g/goran-glavas/",
        "bibtex": "@inproceedings{gashteovski-etal-2022-benchie,\n    title = \"{B}ench{IE}: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation\",\n    author = \"Gashteovski, Kiril  and\n      Yu, Mingying  and\n      Kotnis, Bhushan  and\n      Lawrence, Carolin  and\n      Niepert, Mathias  and\n      Glava{\\v{s}}, Goran\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.307/\",\n    doi = \"10.18653/v1/2022.acl-long.307\",\n    pages = \"4472--4490\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.307.pdf",
        "site": "https://aclanthology.org/2022.acl-long.307/",
        "pdf_size": 476881,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18016469011252262180&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "NEC Laboratories Europe GmbH, Heidelberg, Germany+University of Stuttgart, Germany; NEC Laboratories Europe GmbH, Heidelberg, Germany+University of Mannheim+LMU Munich, Germany; NEC Laboratories Europe GmbH, Heidelberg, Germany; NEC Laboratories Europe GmbH, Heidelberg, Germany; NEC Laboratories Europe GmbH, Heidelberg, Germany+University of Stuttgart, Germany; University of Mannheim+LMU Munich, Germany",
        "aff_domain": "neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu;informatik.uni-mannheim.de",
        "email": "neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu;informatik.uni-mannheim.de",
        "github": "https://github.com/gkiril/benchie",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+2+3;0;0;0+1;2+3",
        "aff_unique_norm": "NEC Laboratories Europe;University of Stuttgart;University of Mannheim;Ludwig Maximilian University of Munich",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.nec-labs.eu;https://www.uni-stuttgart.de;https://www.uni-mannheim.de;https://www.lmu.de",
        "aff_unique_abbr": "NEC LE;USTuttgart;UM;LMU",
        "aff_campus_unique_index": "0;0+2;0;0;0;2",
        "aff_campus_unique": "Heidelberg;;Munich",
        "aff_country_unique_index": "0+0;0+0+0;0;0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.findings-acl.296",
        "title": "Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Question answering-based summarization evaluation metrics must automatically determine whether the QA model\u2019s prediction is correct or not, a task known as answer verification. In this work, we benchmark the lexical answer verification methods which have been used by current QA-based metrics as well as two more sophisticated text comparison methods, BERTScore and LERC. We find that LERC out-performs the other methods in some settings while remaining statistically indistinguishable from lexical overlap in others. However, our experiments reveal that improved verification performance does not necessarily translate to overall QA-based metric quality: In some scenarios, using a worse verification method \u2014 or using none at all \u2014 has comparable performance to using the best verification method, a result that we attribute to properties of the datasets.",
        "author": "Daniel Deutsch; Dan Roth",
        "authorids": "/d/daniel-deutsch/; /d/dan-roth/",
        "bibtex": "@inproceedings{deutsch-roth-2022-benchmarking,\n    title = \"Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics\",\n    author = \"Deutsch, Daniel  and\n      Roth, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.296/\",\n    doi = \"10.18653/v1/2022.findings-acl.296\",\n    pages = \"3759--3765\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.296.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.296/",
        "pdf_size": 234242,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8162603343465693425&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer and Information Science, University of Pennsylvania; Department of Computer and Information Science, University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "http://cogcomp.org/page/publication_view/966",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.96",
        "title": "Better Language Model with Hypernym Class Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",
        "author": "He Bai; Tong Wang; Alessandro Sordoni; Peng Shi",
        "authorids": "/h/he-bai/; /t/tong-wang/; /a/alessandro-sordoni/; /p/peng-shi/",
        "bibtex": "@inproceedings{bai-etal-2022-better,\n    title = \"Better Language Model with Hypernym Class Prediction\",\n    author = \"Bai, He  and\n      Wang, Tong  and\n      Sordoni, Alessandro  and\n      Shi, Peng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.96/\",\n    doi = \"10.18653/v1/2022.acl-long.96\",\n    pages = \"1352--1362\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.96.pdf",
        "site": "https://aclanthology.org/2022.acl-long.96/",
        "pdf_size": 752967,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15415675904321209079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Waterloo; Microsoft Research; Microsoft Research; University of Waterloo",
        "aff_domain": "uwaterloo.ca;microsoft.com;microsoft.com;uwaterloo.ca",
        "email": "uwaterloo.ca;microsoft.com;microsoft.com;uwaterloo.ca",
        "github": "https://github.com/richardbaihe/robustLM.git",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Waterloo;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://uwaterloo.ca;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UW;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.findings-acl.45",
        "title": "Better Quality Estimation for Low Resource Corpus Mining",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Quality Estimation (QE) models have the potential to change how we evaluate and maybe even train machine translation models. However, these models still lack the robustness to achieve general adoption. We show that Stateof-the-art QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform unexpectedly bad due to a lack of robustness to out-of-domain examples. We propose a combination of multitask training, data augmentation and contrastive learning to achieve better and more robust QE performance. We show that our method improves QE performance significantly in the MLQE challenge and the robustness of QE models when tested in the Parallel Corpus Mining setup. We increase the accuracy in PCM by more than 0.80, making it on par with state-of-the-art PCM methods that use millions of sentence pairs to train their models. In comparison, we use a thousand times less data, 7K parallel sentences in total, and propose a novel low resource PCM method.",
        "author": "Muhammed Kocyigit; Jiho Lee; Derry Wijaya",
        "authorids": "/m/muhammed-kocyigit/; /j/jiho-lee/; /d/derry-tanti-wijaya/",
        "bibtex": "@inproceedings{kocyigit-etal-2022-better,\n    title = \"Better Quality Estimation for Low Resource Corpus Mining\",\n    author = \"Kocyigit, Muhammed  and\n      Lee, Jiho  and\n      Wijaya, Derry\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.45/\",\n    doi = \"10.18653/v1/2022.findings-acl.45\",\n    pages = \"533--543\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.45.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.45/",
        "pdf_size": 770406,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8050085170893988318&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Boston University; Boston University; Boston University",
        "aff_domain": "bu.edu;bu.edu;bu.edu",
        "email": "bu.edu;bu.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.356",
        "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other\u2019s interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.",
        "author": "Jing Xu; Arthur Szlam; Jason Weston",
        "authorids": "/j/jing-xu/; /a/arthur-szlam/; /j/jason-weston/",
        "bibtex": "@inproceedings{xu-etal-2022-beyond,\n    title = \"Beyond Goldfish Memory: Long-Term Open-Domain Conversation\",\n    author = \"Xu, Jing  and\n      Szlam, Arthur  and\n      Weston, Jason\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.356/\",\n    doi = \"10.18653/v1/2022.acl-long.356\",\n    pages = \"5180--5197\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.356.pdf",
        "site": "https://aclanthology.org/2022.acl-long.356/",
        "pdf_size": 5118405,
        "gs_citation": 291,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11326048122879093264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Facebook AI Research; Facebook AI Research; Facebook AI Research",
        "aff_domain": "fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Facebook AI Research",
        "aff_unique_url": "https://research.facebook.com",
        "aff_unique_abbr": "FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.165",
        "title": "Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In dialogue state tracking, dialogue history is a crucial material, and its utilization varies between different models. However, no matter how the dialogue history is used, each existing model uses its own consistent dialogue history during the entire state tracking process, regardless of which slot is updated. Apparently, it requires different dialogue history to update different slots in different turns. Therefore, using consistent dialogue contents may lead to insufficient or redundant information for different slots, which affects the overall performance. To address this problem, we devise DiCoS-DST to dynamically select the relevant dialogue contents corresponding to each slot for state updating. Specifically, it first retrieves turn-level utterances of dialogue history and evaluates their relevance to the slot from a combination of three perspectives: (1) its explicit connection to the slot name; (2) its relevance to the current turn dialogue; (3) Implicit Mention Oriented Reasoning. Then these perspectives are combined to yield a decision, and only the selected dialogue contents are fed into State Generator, which explicitly minimizes the distracting information passed to the downstream state prediction. Experimental results show that our approach achieves new state-of-the-art performance on MultiWOZ 2.1 and MultiWOZ 2.2, and achieves superior performance on multiple mainstream benchmark datasets (including Sim-M, Sim-R, and DSTC2).",
        "author": "Jinyu Guo; Kai Shuang; Jijie Li; Zihan Wang; Yixuan Liu",
        "authorids": "/j/jinyu-guo/; /k/kai-shuang/; /j/jijie-li/; /z/zihan-wang/; /y/yixuan-liu/",
        "bibtex": "@inproceedings{guo-etal-2022-beyond,\n    title = \"Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking\",\n    author = \"Guo, Jinyu  and\n      Shuang, Kai  and\n      Li, Jijie  and\n      Wang, Zihan  and\n      Liu, Yixuan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.165/\",\n    doi = \"10.18653/v1/2022.acl-long.165\",\n    pages = \"2320--2332\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.165.pdf",
        "site": "https://aclanthology.org/2022.acl-long.165/",
        "pdf_size": 1524121,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13881830887278427684&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; Graduate School of Information Science and Technology, The University of Tokyo; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;tkl.iis.u-tokyo.ac.jp;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;tkl.iis.u-tokyo.ac.jp;bupt.edu.cn",
        "github": "https://github.com/guojinyu88/DiCoS-master",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;University of Tokyo",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology;Graduate School of Information Science and Technology",
        "aff_unique_url": "http://www.bupt.edu.cn;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "BUPT;UTokyo",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Beijing;Tokyo",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "China;Japan"
    },
    {
        "id": "2022.findings-acl.144",
        "title": "BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference. It is challenging because a sentence may contain multiple aspects or complicated (e.g., conditional, coordinating, or adversative) relations. Recently, exploiting dependency syntax information with graph neural networks has been the most popular trend. Despite its success, methods that heavily rely on the dependency tree pose challenges in accurately modeling the alignment of the aspects and their words indicative of sentiment, since the dependency tree may provide noisy signals of unrelated associations (e.g., the \u201cconj\u201d relation between \u201cgreat\u201d and \u201cdreadful\u201d in Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully exploits the syntax information (e.g., phrase segmentation and hierarchical structure) of the constituent tree of a sentence to model the sentiment-aware context of every single aspect (called intra-context) and the sentiment relations across aspects (called inter-context) for learning. Experiments on four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the state-of-the-art methods consistently.",
        "author": "Shuo Liang; Wei Wei; Xian-Ling Mao; Fei Wang; Zhiyong He",
        "authorids": "/s/shuo-liang/; /w/wei-wei/; /x/xian-ling-mao/; /f/fei-wang/; /z/zhiyong-he/",
        "bibtex": "@inproceedings{liang-etal-2022-bisyn,\n    title = \"{B}i{S}yn-{GAT}+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis\",\n    author = \"Liang, Shuo  and\n      Wei, Wei  and\n      Mao, Xian-Ling  and\n      Wang, Fei  and\n      He, Zhiyong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.144/\",\n    doi = \"10.18653/v1/2022.findings-acl.144\",\n    pages = \"1835--1848\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.144.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.144/",
        "pdf_size": 828133,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12322996969341048076&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 12,
        "aff": "Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology; Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology; School of Computer Science and Technology, Beijing Institute of Technology; Institute of Computig Technology, Chinese Academy of Sciences; Naval University of Engineering",
        "aff_domain": "hust.edu.cn;hust.edu.cn;bit.edu.cn;ict.ac.cn;outlook.com",
        "email": "hust.edu.cn;hust.edu.cn;bit.edu.cn;ict.ac.cn;outlook.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;3",
        "aff_unique_norm": "Huazhong University of Science and Technology;Beijing Institute of Technology;Chinese Academy of Sciences;Naval University of Engineering",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computer Science and Technology;Institute of Computing Technology;",
        "aff_unique_url": "http://www.hust.edu.cn;http://www.bit.edu.cn/;http://www.ict.ac.cn;http://www.nuoe.edu.cn/",
        "aff_unique_abbr": "HUST;BIT;CAS;NUOE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.138",
        "title": "BiTIIMT: A Bilingual Text-infilling Method for Interactive Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Interactive neural machine translation (INMT) is able to guarantee high-quality translations by taking human interactions into account. Existing IMT systems relying on lexical constrained decoding (LCD) enable humans to translate in a flexible translation order beyond the left-to-right. However, they typically suffer from two significant limitations in translation efficiency and quality due to the reliance on LCD. In this work, we propose a novel BiTIIMT system, Bilingual Text-Infilling for Interactive Neural Machine Translation. The key idea to BiTIIMT is Bilingual Text-infilling (BiTI) which aims to fill missing segments in a manually revised translation for a given source sentence. We propose a simple yet effective solution by casting this task as a sequence-to-sequence task. In this way, our system performs decoding without explicit constraints and makes full use of revised words for better translation prediction. Experiment results show that BiTiIMT performs significantly better and faster than state-of-the-art LCD-based IMT on three translation tasks.",
        "author": "Yanling Xiao; Lemao Liu; Guoping Huang; Qu Cui; Shujian Huang; Shuming Shi; Jiajun Chen",
        "authorids": "/y/yanling-xiao/; /l/lemao-liu/; /g/guoping-huang/; /q/qu-cui/; /s/shujian-huang/; /s/shuming-shi/; /j/jiajun-chen/",
        "bibtex": "@inproceedings{xiao-etal-2022-bitiimt,\n    title = \"{B}i{TIIMT}: A Bilingual Text-infilling Method for Interactive Machine Translation\",\n    author = \"Xiao, Yanling  and\n      Liu, Lemao  and\n      Huang, Guoping  and\n      Cui, Qu  and\n      Huang, Shujian  and\n      Shi, Shuming  and\n      Chen, Jiajun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.138/\",\n    doi = \"10.18653/v1/2022.acl-long.138\",\n    pages = \"1958--1969\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.138.pdf",
        "site": "https://aclanthology.org/2022.acl-long.138/",
        "pdf_size": 505018,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3419865507395946290&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, China + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing; Tencent AI Lab, China; Tencent AI Lab, China; Tencent AI Lab, China; National Key Laboratory for Novel Software Technology, Nanjing University, China + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing; Tencent AI Lab, China; National Key Laboratory for Novel Software Technology, Nanjing University, China + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing",
        "aff_domain": "smail.nju.edu.cn;tencent.com;tencent.com;tencent.com;nju.edu.cn;tencent.com;nju.edu.cn",
        "email": "smail.nju.edu.cn;tencent.com;tencent.com;tencent.com;nju.edu.cn;tencent.com;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;2;2;0+1;2;0+1",
        "aff_unique_norm": "Nanjing University;Collaborative Innovation Center of Novel Software Technology and Industrialization;Tencent",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;;Tencent AI Lab",
        "aff_unique_url": "http://www.nju.edu.cn;;https://ai.tencent.com",
        "aff_unique_abbr": "Nanjing U;;Tencent AI Lab",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.104",
        "title": "Bias Mitigation in Machine Translation Quality Estimation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Machine Translation Quality Estimation (QE) aims to build predictive models to assess the quality of machine-generated translations in the absence of reference translations. While state-of-the-art QE models have been shown to achieve good results, they over-rely on features that do not have a causal impact on the quality of a translation. In particular, there appears to be a partial input bias, i.e., a tendency to assign high-quality scores to translations that are fluent and grammatically correct, even though they do not preserve the meaning of the source. We analyse the partial input bias in further detail and evaluate four approaches to use auxiliary tasks for bias mitigation. Two approaches use additional data to inform and support the main task, while the other two are adversarial, actively discouraging the model from learning the bias. We compare the methods with respect to their ability to reduce the partial input bias while maintaining the overall performance. We find that training a multitask architecture with an auxiliary binary classification task that utilises additional augmented data best achieves the desired effects and generalises well to different languages and quality metrics.",
        "author": "Hanna Behnke; Marina Fomicheva; Lucia Specia",
        "authorids": "/h/hanna-behnke/; /m/marina-fomicheva/; /l/lucia-specia/",
        "bibtex": "@inproceedings{behnke-etal-2022-bias,\n    title = \"Bias Mitigation in Machine Translation Quality Estimation\",\n    author = \"Behnke, Hanna  and\n      Fomicheva, Marina  and\n      Specia, Lucia\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.104/\",\n    doi = \"10.18653/v1/2022.acl-long.104\",\n    pages = \"1475--1487\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.104.pdf",
        "site": "https://aclanthology.org/2022.acl-long.104/",
        "pdf_size": 728479,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18327649500364516250&as_sdt=8000005&sciodt=0,19&hl=en",
        "gs_version_total": 2,
        "aff": "Imperial College London; University of Sheffield; Imperial College London",
        "aff_domain": "ic.ac.uk;sheffield.ac.uk;ic.ac.uk",
        "email": "ic.ac.uk;sheffield.ac.uk;ic.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Imperial College London;University of Sheffield",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.sheffield.ac.uk",
        "aff_unique_abbr": "ICL;Sheffield",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.595",
        "title": "Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingual texts. We hypothesize that the cross-lingual alignment strategy is transferable, and therefore a model trained to align only two languages can encode multilingually more aligned representations. We thus introduce dual-pivot transfer: training on one language pair and evaluating on other pairs. To study this theory, we design unsupervised models trained on unpaired sentences and single-pair supervised models trained on bitexts, both based on the unsupervised language model XLM-R with its parameters frozen. The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitext mining on two datasets, where the unsupervised model reaches the state of the art of unsupervised retrieval, and the alternative single-pair supervised model approaches the performance of multilingually supervised models. The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with multilingual alignment.",
        "author": "Chih-chan Tien; Shane Steinert-Threlkeld",
        "authorids": "/c/chih-chan-tien/; /s/shane-steinert-threlkeld/",
        "bibtex": "@inproceedings{tien-steinert-threlkeld-2022-bilingual,\n    title = \"Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining\",\n    author = \"Tien, Chih-chan  and\n      Steinert-Threlkeld, Shane\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.595/\",\n    doi = \"10.18653/v1/2022.acl-long.595\",\n    pages = \"8696--8706\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.595.pdf",
        "site": "https://aclanthology.org/2022.acl-long.595/",
        "pdf_size": 341432,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1400700178704736415&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Chicago; University of Washington",
        "aff_domain": "uchicago.edu;uw.edu",
        "email": "uchicago.edu;uw.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Chicago;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uchicago.edu;https://www.washington.edu",
        "aff_unique_abbr": "UChicago;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.1",
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
        "author": "Elad Ben Zaken; Yoav Goldberg; Shauli Ravfogel",
        "authorids": "/e/elad-ben-zaken/; /y/yoav-goldberg/; /s/shauli-ravfogel/",
        "bibtex": "@inproceedings{ben-zaken-etal-2022-bitfit,\n    title = \"{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models\",\n    author = \"Ben Zaken, Elad  and\n      Goldberg, Yoav  and\n      Ravfogel, Shauli\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.1/\",\n    doi = \"10.18653/v1/2022.acl-short.1\",\n    pages = \"1--9\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.1.pdf",
        "site": "https://aclanthology.org/2022.acl-short.1/",
        "pdf_size": 328426,
        "gs_citation": 1295,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3058790633819184104&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, Bar Ilan University+Allen Institute for Artificial Intelligence; Computer Science Department, Bar Ilan University+Allen Institute for Artificial Intelligence; Computer Science Department, Bar Ilan University+Allen Institute for Artificial Intelligence",
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Bar-Ilan University;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Computer Science Department;",
        "aff_unique_url": "https://www.biu.ac.il;https://allenai.org",
        "aff_unique_abbr": "BIU;AI2",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2022.acl-long.171",
        "title": "Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Constituency parsing and nested named entity recognition (NER) are similar tasks since they both aim to predict a collection of nested and non-crossing spans. In this work, we cast nested NER to constituency parsing and propose a novel pointing mechanism for bottom-up parsing to tackle both tasks. The key idea is based on the observation that if we traverse a constituency tree in post-order, i.e., visiting a parent after its children, then two consecutively visited spans would share a boundary. Our model tracks the shared boundaries and predicts the next boundary at each step by leveraging a pointer network. As a result, it needs only linear steps to parse and thus is efficient. It also maintains a parsing configuration for structural consistency, i.e., always outputting valid trees. Experimentally, our model achieves the state-of-the-art performance on PTB among all BERT-based models (96.01 F1 score) and competitive performance on CTB7 in constituency parsing; and it also achieves strong performance on three benchmark datasets of nested NER: ACE2004, ACE2005, and GENIA. Our code will be available at https://github.com/xxxxx.",
        "author": "Songlin Yang; Kewei Tu",
        "authorids": "/s/songlin-yang/; /k/kewei-tu/",
        "bibtex": "@inproceedings{yang-tu-2022-bottom,\n    title = \"Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks\",\n    author = \"Yang, Songlin  and\n      Tu, Kewei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.171/\",\n    doi = \"10.18653/v1/2022.acl-long.171\",\n    pages = \"2403--2416\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.171.pdf",
        "site": "https://aclanthology.org/2022.acl-long.171/",
        "pdf_size": 538358,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3179151906359823968&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "School of Information Science and Technology, ShanghaiTech University + Shanghai Engineering Research Center of Intelligent Vision and Imaging; School of Information Science and Technology, ShanghaiTech University + Shanghai Engineering Research Center of Intelligent Vision and Imaging",
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn",
        "email": "shanghaitech.edu.cn;shanghaitech.edu.cn",
        "github": "https://github.com/sustcsonglin/pointer-net-for-nested",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "ShanghaiTech University;Shanghai Engineering Research Center of Intelligent Vision and Imaging",
        "aff_unique_dep": "School of Information Science and Technology;",
        "aff_unique_url": "https://www.shanghaitech.edu.cn;",
        "aff_unique_abbr": "ShanghaiTech;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.490",
        "title": "Boundary Smoothing for Named Entity Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural named entity recognition (NER) models may easily encounter the over-confidence issue, which degrades the performance and calibration. Inspired by label smoothing and driven by the ambiguity of boundary annotation in NER engineering, we propose boundary smoothing as a regularization technique for span-based neural NER models. It re-assigns entity probabilities from annotated spans to the surrounding ones. Built on a simple but strong baseline, our model achieves results better than or competitive with previous state-of-the-art systems on eight well-known NER benchmarks. Further empirical analysis suggests that boundary smoothing effectively mitigates over-confidence, improves model calibration, and brings flatter neural minima and more smoothed loss landscapes.",
        "author": "Enwei Zhu; Jinpeng Li",
        "authorids": "/e/enwei-zhu/; /j/jinpeng-li/",
        "bibtex": "@inproceedings{zhu-li-2022-boundary,\n    title = \"Boundary Smoothing for Named Entity Recognition\",\n    author = \"Zhu, Enwei  and\n      Li, Jinpeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.490/\",\n    doi = \"10.18653/v1/2022.acl-long.490\",\n    pages = \"7096--7108\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.490.pdf",
        "site": "https://aclanthology.org/2022.acl-long.490/",
        "pdf_size": 391628,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16219485710636468144&as_sdt=1005&sciodt=0,4&hl=en",
        "gs_version_total": 5,
        "aff": "HwaMei Hospital, University of Chinese Academy of Sciences + Ningbo Institute of Life and Health Industry, University of Chinese Academy of Sciences; HwaMei Hospital, University of Chinese Academy of Sciences + Ningbo Institute of Life and Health Industry, University of Chinese Academy of Sciences",
        "aff_domain": "ucas.ac.cn;ucas.ac.cn",
        "email": "ucas.ac.cn;ucas.ac.cn",
        "github": "https://github.com/syuoni/eznlp",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "University of Chinese Academy of Sciences",
        "aff_unique_dep": "HwaMei Hospital",
        "aff_unique_url": "http://www.ucas.ac.cn",
        "aff_unique_abbr": "UCAS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Ningbo",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.218",
        "title": "Breaking Down Multilingual Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While multilingual training is now an essential ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. These training settings expose the encoder and the decoder in a machine translation model with different data distributions. In this paper, we examine how different varieties of multilingual training contribute to learning these two components of the MT model. Specifically, we compare bilingual models with encoders and/or decoders initialized by multilingual training. We show that multilingual training is beneficial to encoders in general, while it only benefits decoders for low-resource languages (LRLs). We further find the important attention heads for each language pair and compare their correlations during inference. Our analysis sheds light on how multilingual translation models work and also enables us to propose methods to improve performance by training with highly related languages. Our many-to-one models for high-resource languages and one-to-many models for LRL outperform the best results reported by Aharoni et al. (2019).",
        "author": "Ting-Rui Chiang; Yi-Pei Chen; Yi-Ting Yeh; Graham Neubig",
        "authorids": "/t/ting-rui-chiang/; /y/yi-pei-chen/; /y/yi-ting-yeh/; /g/graham-neubig/",
        "bibtex": "@inproceedings{chiang-etal-2022-breaking,\n    title = \"Breaking Down Multilingual Machine Translation\",\n    author = \"Chiang, Ting-Rui  and\n      Chen, Yi-Pei  and\n      Yeh, Yi-Ting  and\n      Neubig, Graham\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.218/\",\n    doi = \"10.18653/v1/2022.findings-acl.218\",\n    pages = \"2766--2780\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.218.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.218/",
        "pdf_size": 5928441,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11695802271441467379&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Carnegie Mellon University; The University of Tokyo + Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;g.ecc.u-tokyo.ac.jp;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;g.ecc.u-tokyo.ac.jp;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Carnegie Mellon University;University of Tokyo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "CMU;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "2022.findings-acl.259",
        "title": "Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In recent years, large-scale pre-trained language models (PLMs) have made extraordinary progress in most NLP tasks. But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA) performance. The recent SOTA performance is yielded by a Guassian HMM variant proposed by He et al. (2018). However, as a generative model, HMM makes very strong independence assumptions, making it very challenging to incorporate contexualized word representations from PLMs. In this work, we for the first time propose a neural conditional random field autoencoder (CRF-AE) model for unsupervised POS tagging. The discriminative encoder of CRF-AE can straightforwardly incorporate ELMo word representations. Moreover, inspired by feature-rich HMM, we reintroduce hand-crafted features into the decoder of CRF-AE. Finally, experiments clearly show that our model outperforms previous state-of-the-art models by a large margin on Penn Treebank and multilingual Universal Dependencies treebank v2.0.",
        "author": "Houquan Zhou; Yang Li; Zhenghua Li; Min Zhang",
        "authorids": "/h/houquan-zhou/; /y/yang-li/; /z/zhenghua-li/; /m/min-zhang/",
        "bibtex": "@inproceedings{zhou-etal-2022-bridging,\n    title = \"Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised {POS} Tagging\",\n    author = \"Zhou, Houquan  and\n      Li, Yang  and\n      Li, Zhenghua  and\n      Zhang, Min\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.259/\",\n    doi = \"10.18653/v1/2022.findings-acl.259\",\n    pages = \"3276--3290\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.259.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.259/",
        "pdf_size": 492244,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10181511780669675714&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;stu.suda.edu.cn;suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "Institute of Artificial Intelligence, School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.456",
        "title": "Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Back-translation is a critical component of Unsupervised Neural Machine Translation (UNMT), which generates pseudo parallel data from target monolingual data. A UNMT model is trained on the pseudo parallel data with translated source, and translates natural source sentences in inference. The source discrepancy between training and inference hinders the translation performance of UNMT models. By carefully designing experiments, we identify two representative characteristics of the data gap in source: (1) style gap (i.e., translated vs. natural text style) that leads to poor generalization capability; (2) content gap that induces the model to produce hallucination content biased towards the target language. To narrow the data gap, we propose an online self-training approach, which simultaneously uses the pseudo parallel data {natural source, translated target} to mimic the inference scenario. Experimental results on several widely-used language pairs show that our approach outperforms two strong baselines (XLM and MASS) by remedying the style and content gaps.",
        "author": "Zhiwei He; Xing Wang; Rui Wang; Shuming Shi; Zhaopeng Tu",
        "authorids": "/z/zhiwei-he/; /x/xing-wang/; /r/rui-wang/; /s/shuming-shi/; /z/zhaopeng-tu/",
        "bibtex": "@inproceedings{he-etal-2022-bridging,\n    title = \"Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation\",\n    author = \"He, Zhiwei  and\n      Wang, Xing  and\n      Wang, Rui  and\n      Shi, Shuming  and\n      Tu, Zhaopeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.456/\",\n    doi = \"10.18653/v1/2022.acl-long.456\",\n    pages = \"6611--6623\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.456.pdf",
        "site": "https://aclanthology.org/2022.acl-long.456/",
        "pdf_size": 265969,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16735929331833091878&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Shanghai Jiao Tong University + Tencent AI Lab; Tencent AI Lab; Shanghai Jiao Tong University; Tencent AI Lab; Tencent AI Lab",
        "aff_domain": "sjtu.edu.cn;tencent.com;sjtu.edu.cn;tencent.com;tencent.com",
        "email": "sjtu.edu.cn;tencent.com;sjtu.edu.cn;tencent.com;tencent.com",
        "github": "https://github.com/zwhe99/SelfTraining4UNMT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;0;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://ai.tencent.com",
        "aff_unique_abbr": "SJTU;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.381",
        "title": "Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text-to-SQL parsers map natural language questions to programs that are executable over tables to generate answers, and are typically evaluated on large-scale datasets like Spider (Yu et al., 2018). We argue that existing benchmarks fail to capture a certain out-of-domain generalization problem that is of significant practical importance: matching domain specific phrases to composite operation over columns. To study this problem, we first propose a synthetic dataset along with a re-purposed train/test split of the Squall dataset (Shi et al., 2020) as new benchmarks to quantify domain generalization over column operations, and find existing state-of-the-art parsers struggle in these benchmarks. We propose to address this problem by incorporating prior domain knowledge by preprocessing table schemas, and design a method that consists of two components: schema expansion and schema pruning. This method can be easily applied to multiple existing base parsers, and we show that it significantly outperforms baseline parsers on this domain generalization problem, boosting the underlying parsers\u2019 overall performance by up to 13.8% relative accuracy gain (5.1% absolute) on the new Squall data split.",
        "author": "Chen Zhao; Yu Su; Adam Pauls; Emmanouil Antonios Platanios",
        "authorids": "/c/chen-zhao/; /y/yu-su/; /a/adam-pauls/; /e/emmanouil-antonios-platanios/",
        "bibtex": "@inproceedings{zhao-etal-2022-bridging,\n    title = \"Bridging the Generalization Gap in Text-to-{SQL} Parsing with Schema Expansion\",\n    author = \"Zhao, Chen  and\n      Su, Yu  and\n      Pauls, Adam  and\n      Platanios, Emmanouil Antonios\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.381/\",\n    doi = \"10.18653/v1/2022.acl-long.381\",\n    pages = \"5568--5578\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.381.pdf",
        "site": "https://aclanthology.org/2022.acl-long.381/",
        "pdf_size": 547991,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3708052425852786696&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 5,
        "aff": "New York University; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines",
        "aff_domain": "nyu.edu;microsoft.com;microsoft.com;microsoft.com",
        "email": "nyu.edu;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "New York University;Microsoft",
        "aff_unique_dep": ";Semantic Machines",
        "aff_unique_url": "https://www.nyu.edu;https://www.microsoft.com",
        "aff_unique_abbr": "NYU;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.12",
        "title": "Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines .",
        "author": "Chengyu Chuang; Yi Yang",
        "authorids": "/c/chengyu-chuang/; /y/yi-yang/",
        "bibtex": "@inproceedings{chuang-yang-2022-buy,\n    title = \"Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models\",\n    author = \"Chuang, Chengyu  and\n      Yang, Yi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.12/\",\n    doi = \"10.18653/v1/2022.acl-short.12\",\n    pages = \"100--105\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.12.pdf",
        "site": "https://aclanthology.org/2022.acl-short.12/",
        "pdf_size": 272828,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8010676432648911223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Mathematics and Economics, Hong Kong University of Science and Technology; Department of Information Systems and Operations Management, Hong Kong University of Science and Technology",
        "aff_domain": "connect.ust.hk;ust.hk",
        "email": "connect.ust.hk;ust.hk",
        "github": "https://github.com/MattioCh/Buy-Tesla-Sell-Ford",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Mathematics and Economics",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.41",
        "title": "C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We consider the problem of pretraining a two-stage open-domain question answering (QA) system (retriever + reader) with strong transfer capabilities. The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations. Specifically, the triplets should align well with downstream tasks by: (i) covering a wide range of domains (for open-domain applications), (ii) linking a question to its semantically relevant context with supporting evidence (for training the retriever), and (iii) identifying the correct answer in the context (for training the reader). Previous pretraining approaches generally fall short of one or more of these requirements. In this work, we automatically construct a large-scale corpus that meets all three criteria by consulting millions of references cited within Wikipedia. The well-aligned pretraining signals benefit both the retriever and the reader significantly. Our pretrained retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our pretrained reader, the entire system improves by up to 4% in exact match.",
        "author": "Xiang Yue; Xiaoman Pan; Wenlin Yao; Dian Yu; Dong Yu; Jianshu Chen",
        "authorids": "/x/xiang-yue/; /x/xiaoman-pan/; /w/wenlin-yao/; /d/dian-yu/; /d/dong-yu/; /j/jianshu-chen/",
        "bibtex": "@inproceedings{yue-etal-2022-c,\n    title = \"{C}-{MORE}: Pretraining to Answer Open-Domain Questions by Consulting Millions of References\",\n    author = \"Yue, Xiang  and\n      Pan, Xiaoman  and\n      Yao, Wenlin  and\n      Yu, Dian  and\n      Yu, Dong  and\n      Chen, Jianshu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.41/\",\n    doi = \"10.18653/v1/2022.acl-short.41\",\n    pages = \"371--377\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.41.pdf",
        "site": "https://aclanthology.org/2022.acl-short.41/",
        "pdf_size": 692497,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12406762391444887238&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Ohio State University; Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Tencent AI Lab",
        "aff_domain": "osu.edu;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "email": "osu.edu;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "github": "https://github.com/xiangyue9607/C-MORE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Ohio State University;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.osu.edu;https://ai.tencent.com",
        "aff_unique_abbr": "OSU;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.findings-acl.107",
        "title": "C3KG: A Chinese Commonsense Conversation Knowledge Graph",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Existing commonsense knowledge bases often organize tuples in an isolated manner, which is deficient for commonsense conversational models to plan the next steps. To fill the gap, we curate a large-scale multi-turn human-written conversation corpus, and create the first Chinese commonsense conversation knowledge graph which incorporates both social commonsense knowledge and dialog flow information. To show the potential of our graph, we develop a graph-conversation matching approach, and benchmark two graph-grounded conversational tasks. All the resources in this work will be released to foster future research.",
        "author": "Dawei Li; Yanran Li; Jiayi Zhang; Ke Li; Chen Wei; Jianwei Cui; Bin Wang",
        "authorids": "/d/dawei-li/; /y/yanran-li/; /j/jiayi-zhang/; /k/ke-li/; /c/chen-wei/; /j/jianwei-cui/; /b/bin-wang/",
        "bibtex": "@inproceedings{li-etal-2022-c3kg,\n    title = \"{C}$^3${KG}: A {C}hinese Commonsense Conversation Knowledge Graph\",\n    author = \"Li, Dawei  and\n      Li, Yanran  and\n      Zhang, Jiayi  and\n      Li, Ke  and\n      Wei, Chen  and\n      Cui, Jianwei  and\n      Wang, Bin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.107/\",\n    doi = \"10.18653/v1/2022.findings-acl.107\",\n    pages = \"1369--1383\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.107.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.107/",
        "pdf_size": 4555765,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16263436184297176775&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Xiaomi AI Lab; Xiaomi AI Lab; Xiaomi AI Lab; Xiaomi AI Lab; Xiaomi AI Lab; Xiaomi AI Lab; Xiaomi AI Lab",
        "aff_domain": "xiaomi.com;gmail.com;xiaomi.com;xiaomi.com;xiaomi.com;xiaomi.com;xiaomi.com",
        "email": "xiaomi.com;gmail.com;xiaomi.com;xiaomi.com;xiaomi.com;xiaomi.com;xiaomi.com",
        "github": "https://github.com/XiaoMi/C3KG",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Xiaomi Corporation",
        "aff_unique_dep": "Xiaomi AI Lab",
        "aff_unique_url": "https://www.xiaomi.com",
        "aff_unique_abbr": "Xiaomi",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.205",
        "title": "CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge graphs store a large number of factual triples while they are still incomplete, inevitably. The previous knowledge graph completion (KGC) models predict missing links between entities merely relying on fact-view data, ignoring the valuable commonsense knowledge. The previous knowledge graph embedding (KGE) techniques suffer from invalid negative sampling and the uncertainty of fact-view link prediction, limiting KGC\u2019s performance. To address the above challenges, we propose a novel and scalable Commonsense-Aware Knowledge Embedding (CAKE) framework to automatically extract commonsense from factual triples with entity concepts. The generated commonsense augments effective self-supervision to facilitate both high-quality negative sampling (NS) and joint commonsense and fact-view link prediction. Experimental results on the KGC task demonstrate that assembling our framework could enhance the performance of the original KGE models, and the proposed commonsense-aware NS module is superior to other NS techniques. Besides, our proposed framework could be easily adaptive to various KGE models and explain the predicted results.",
        "author": "Guanglin Niu; Bo Li; Yongfei Zhang; Shiliang Pu",
        "authorids": "/g/guanglin-niu/; /b/bo-li-bh/; /y/yongfei-zhang/; /s/shiliang-pu/",
        "bibtex": "@inproceedings{niu-etal-2022-cake,\n    title = \"{CAKE}: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion\",\n    author = \"Niu, Guanglin  and\n      Li, Bo  and\n      Zhang, Yongfei  and\n      Pu, Shiliang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.205/\",\n    doi = \"10.18653/v1/2022.acl-long.205\",\n    pages = \"2867--2877\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.205.pdf",
        "site": "https://aclanthology.org/2022.acl-long.205/",
        "pdf_size": 390248,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1246343335043263881&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China+Institute of Artificial Intelligence, Beihang University, Beijing, China+Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Institute of Artificial Intelligence, Beihang University, Beijing, China+Hangzhou Innovation Institute, Beihang University, Hangzhou, China+State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Hikvision Research Institute, Hangzhou, China",
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;hikvision.com",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;hikvision.com",
        "github": "https://github.com/ngl567/CAKE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+0;0+0+0;0;1",
        "aff_unique_norm": "Beihang University;Hikvision Research Institute",
        "aff_unique_dep": "Beijing Key Laboratory of Digital Media;",
        "aff_unique_url": "http://www.buaa.edu.cn;https://www.hikvision.com/cn/",
        "aff_unique_abbr": "Beihang;HRI",
        "aff_campus_unique_index": "0+0+1;0+1+0;0;1",
        "aff_campus_unique": "Beijing;Hangzhou",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.495",
        "title": "CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Model ensemble is a popular approach to produce a low-variance and well-generalized model. However, it induces large memory and inference costs, which is often not affordable for real-world deployment. Existing work has resorted to sharing weights among models. However, when increasing the proportion of the shared weights, the resulting models tend to be similar, and the benefits of using model ensemble diminish. To retain ensemble benefits while maintaining a low memory cost, we propose a consistency-regularized ensemble learning approach based on perturbed models, named CAMERO. Specifically, we share the weights of bottom layers across all models and apply different perturbations to the hidden representations for different models, which can effectively promote the model diversity. Meanwhile, we apply a prediction consistency regularizer across the perturbed models to control the variance due to the model diversity. Our experiments using large language models demonstrate that CAMERO significantly improves the generalization performance of the ensemble model. Specifically, CAMERO outperforms the standard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a significantly smaller model size (114.2M vs. 880.6M).",
        "author": "Chen Liang; Pengcheng He; Yelong Shen; Weizhu Chen; Tuo Zhao",
        "authorids": "/c/chen-liang/; /p/pengcheng-he/; /y/yelong-shen/; /w/weizhu-chen/; /t/tuo-zhao/",
        "bibtex": "@inproceedings{liang-etal-2022-camero,\n    title = \"{CAMERO}: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing\",\n    author = \"Liang, Chen  and\n      He, Pengcheng  and\n      Shen, Yelong  and\n      Chen, Weizhu  and\n      Zhao, Tuo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.495/\",\n    doi = \"10.18653/v1/2022.acl-long.495\",\n    pages = \"7162--7175\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.495.pdf",
        "site": "https://aclanthology.org/2022.acl-long.495/",
        "pdf_size": 498306,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6366886371433061110&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Georgia Institute of Technology; Microsoft Azure AI; Microsoft Azure AI; Microsoft Azure AI; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;microsoft.com;microsoft.com;microsoft.com;gatech.edu",
        "email": "gatech.edu;microsoft.com;microsoft.com;microsoft.com;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Microsoft",
        "aff_unique_dep": ";Azure AI",
        "aff_unique_url": "https://www.gatech.edu;https://azure.microsoft.com/en-us/ai",
        "aff_unique_abbr": "Georgia Tech;Microsoft Azure AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.443",
        "title": "CARETS: A Consistency And Robustness Evaluative Test Suite for VQA",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce CARETS, a systematic test suite to measure consistency and robustness of modern VQA models through a series of six fine-grained capability tests. In contrast to existing VQA test sets, CARETS features balanced question generation to create pairs of instances to test models, with each pair focusing on a specific capability such as rephrasing, logical symmetry or image obfuscation. We evaluate six modern VQA systems on CARETS and identify several actionable weaknesses in model comprehension, especially with concepts such as negation, disjunction, or hypernym invariance. Interestingly, even the most sophisticated models are sensitive to aspects such as swapping the order of terms in a conjunction or varying the number of answer choices mentioned in the question. We release CARETS to be used as an extensible tool for evaluating multi-modal model robustness.",
        "author": "Carlos E. Jimenez; Olga Russakovsky; Karthik Narasimhan",
        "authorids": "/c/carlos-e-jimenez/; /o/olga-russakovsky/; /k/karthik-narasimhan/",
        "bibtex": "@inproceedings{jimenez-etal-2022-carets,\n    title = \"{CARETS}: A Consistency And Robustness Evaluative Test Suite for {VQA}\",\n    author = \"Jimenez, Carlos E.  and\n      Russakovsky, Olga  and\n      Narasimhan, Karthik\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.443/\",\n    doi = \"10.18653/v1/2022.acl-long.443\",\n    pages = \"6392--6405\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.443.pdf",
        "site": "https://aclanthology.org/2022.acl-long.443/",
        "pdf_size": 1547934,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8201146736253439399&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Princeton University; Princeton University; Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu",
        "github": "https://github.com/princeton-nlp/CARETS",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.544",
        "title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Artificial Intelligence (AI), along with the recent progress in biomedical language understanding, is gradually offering great promise for medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most benchmarks are limited to English, which makes it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11 pre-trained Chinese models, and experimental results show that state-of-the-art neural models perform by far worse than the human ceiling.",
        "author": "Ningyu Zhang; Mosha Chen; Zhen Bi; Xiaozhuan Liang; Lei Li; Xin Shang; Kangping Yin; Chuanqi Tan; Jian Xu; Fei Huang; Luo Si; Yuan Ni; Guotong Xie; Zhifang Sui; Baobao Chang; Hui Zong; Zheng Yuan; Linfeng Li; Jun Yan; Hongying Zan; Kunli Zhang; Buzhou Tang; Qingcai Chen",
        "authorids": "/n/ningyu-zhang/; /m/mosha-chen/; /z/zhen-bi/; /x/xiaozhuan-liang/; /l/lei-li/; /x/xin-shang/; /k/kangping-yin/; /c/chuanqi-tan/; /j/jian-xu/; /f/fei-huang/; /l/luo-si/; /y/yuan-ni/; /g/guotong-xie/; /z/zhifang-sui/; /b/baobao-chang/; /h/hui-zong/; /z/zheng-yuan/; /l/linfeng-li/; /j/jun-yan/; /h/hongying-zan/; /k/kunli-zhang/; /b/buzhou-tang/; /q/qingcai-chen/",
        "bibtex": "@inproceedings{zhang-etal-2022-cblue,\n    title = \"{CBLUE}: A {C}hinese Biomedical Language Understanding Evaluation Benchmark\",\n    author = \"Zhang, Ningyu  and\n      Chen, Mosha  and\n      Bi, Zhen  and\n      Liang, Xiaozhuan  and\n      Li, Lei  and\n      Shang, Xin  and\n      Yin, Kangping  and\n      Tan, Chuanqi  and\n      Xu, Jian  and\n      Huang, Fei  and\n      Si, Luo  and\n      Ni, Yuan  and\n      Xie, Guotong  and\n      Sui, Zhifang  and\n      Chang, Baobao  and\n      Zong, Hui  and\n      Yuan, Zheng  and\n      Li, Linfeng  and\n      Yan, Jun  and\n      Zan, Hongying  and\n      Zhang, Kunli  and\n      Tang, Buzhou  and\n      Chen, Qingcai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.544/\",\n    doi = \"10.18653/v1/2022.acl-long.544\",\n    pages = \"7888--7915\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.544.pdf",
        "site": "https://aclanthology.org/2022.acl-long.544/",
        "pdf_size": 1529887,
        "gs_citation": 218,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9730122035252752302&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "AZFT Joint Lab for Knowledge Engine, Zhejiang University; Alibaba Group; AZFT Joint Lab for Knowledge Engine, Zhejiang University; AZFT Joint Lab for Knowledge Engine, Zhejiang University; AZFT Joint Lab for Knowledge Engine, Zhejiang University; School of Mathematical Science, Zhejiang University; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Pingan Health Technology; Pingan Health Technology+Ping An Health Cloud Company Limited+Ping An International Smart City Technology Co., Ltd; Key Laboratory of Computational Linguistics, Ministry of Education, Peking University; Key Laboratory of Computational Linguistics, Ministry of Education, Peking University; School of Life Sciences and Technology, Tongji University; Tsinghua University; Yidu Cloud Technology Inc; Yidu Cloud Technology Inc; School of Information Engineering, Zhengzhou University; School of Information Engineering, Zhengzhou University; Harbin Institute of Technology (Shenzhen); Harbin Institute of Technology (Shenzhen)+Peng Cheng Laboratory+Philips Research China",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;",
        "github": "https://github.com/CBLUEbenchmark/CBLUE",
        "project": "https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us",
        "author_num": 23,
        "aff_unique_index": "0;1;0;0;0;0;1;1;1;1;1;2;2+3+4;5;5;6;7;8;8;9;9;10;10+11+12",
        "aff_unique_norm": "Zhejiang University;Alibaba Group;Pingan Health Technology;Ping An Health Cloud Company Limited;Ping An International Smart City Technology Co., Ltd;Peking University;Tongji University;Tsinghua University;Yidu Cloud Technology Inc;Zhengzhou University;Harbin Institute of Technology;Pengcheng Laboratory;Philips Research",
        "aff_unique_dep": "AZFT Joint Lab for Knowledge Engine;;;;;Key Laboratory of Computational Linguistics;School of Life Sciences and Technology;;;School of Information Engineering;;Peng Cheng Laboratory;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.alibaba.com;https://www.pingan.com;https://www.pinganhealthcloud.com;;http://www.pku.edu.cn;https://www.tongji.edu.cn;https://www.tsinghua.edu.cn;https://www.yiducloud.com;http://www.zzu.edu.cn;http://en.hhit.edu.cn/;http://www.pcl.ac.cn;https://www.philips.com/research",
        "aff_unique_abbr": ";Alibaba;;;;PKU;Tongji;THU;;;HIT;PCL;Philips Res.",
        "aff_campus_unique_index": ";1;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0+0+0;0;0;0;0;0;0;0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.344",
        "title": "CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper addresses the problem of dialogue reasoning with contextualized commonsense inference. We curate CICERO, a dataset of dyadic conversations with five types of utterance-level reasoning-based inferences: cause, subsequent event, prerequisite, motivation, and emotional reaction. The dataset contains 53,105 of such inferences from 5,672 dialogues. We use this dataset to solve relevant generative and discriminative tasks: generation of cause and subsequent event; generation of prerequisite, motivation, and listener\u2019s emotional reaction; and selection of plausible alternatives. Our results ascertain the value of such dialogue-centric commonsense knowledge datasets. It is our hope that CICERO will open new research avenues into commonsense-based dialogue reasoning.",
        "author": "Deepanway Ghosal; Siqi Shen; Navonil Majumder; Rada Mihalcea; Soujanya Poria",
        "authorids": "/d/deepanway-ghosal/; /s/siqi-shen/; /n/navonil-majumder/; /r/rada-mihalcea/; /s/soujanya-poria/",
        "bibtex": "@inproceedings{ghosal-etal-2022-cicero,\n    title = \"{CICERO}: A Dataset for Contextualized Commonsense Inference in Dialogues\",\n    author = \"Ghosal, Deepanway  and\n      Shen, Siqi  and\n      Majumder, Navonil  and\n      Mihalcea, Rada  and\n      Poria, Soujanya\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.344/\",\n    doi = \"10.18653/v1/2022.acl-long.344\",\n    pages = \"5010--5028\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.344.pdf",
        "site": "https://aclanthology.org/2022.acl-long.344/",
        "pdf_size": 1851826,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11587364290307899109&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "DeCLaRe Lab, Singapore University of Technology and Design, Singapore; University of Michigan, USA; DeCLaRe; DeCLaRe; DeCLaRe",
        "aff_domain": "mymail.sutd.edu.sg;sutd.edu.sg;sutd.edu.sg;umich.edu;umich.edu",
        "email": "mymail.sutd.edu.sg;sutd.edu.sg;sutd.edu.sg;umich.edu;umich.edu",
        "github": "https://declare-lab.github.io/CICERO",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;2;2",
        "aff_unique_norm": "Singapore University of Technology and Design;University of Michigan;DeCLaRe",
        "aff_unique_dep": "DeCLaRe Lab;;",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.umich.edu;",
        "aff_unique_abbr": "SUTD;UM;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Singapore;United States;"
    },
    {
        "id": "2022.acl-long.421",
        "title": "CLIP Models are Few-Shot Learners: Empirical Studies on VQA and Visual Entailment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks. Previously, CLIP is only regarded as a powerful visual encoder. However, after being pre-trained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks. In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language. We first evaluate CLIP\u2019s zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task. Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task. We achieve competitive zero/few-shot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.",
        "author": "Haoyu Song; Li Dong; Weinan Zhang; Ting Liu; Furu Wei",
        "authorids": "/h/haoyu-song/; /l/li-dong/; /w/weinan-zhang/; /t/ting-liu/; /f/furu-wei/",
        "bibtex": "@inproceedings{song-etal-2022-clip,\n    title = \"{CLIP} Models are Few-Shot Learners: Empirical Studies on {VQA} and Visual Entailment\",\n    author = \"Song, Haoyu  and\n      Dong, Li  and\n      Zhang, Weinan  and\n      Liu, Ting  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.421/\",\n    doi = \"10.18653/v1/2022.acl-long.421\",\n    pages = \"6088--6100\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.421.pdf",
        "site": "https://aclanthology.org/2022.acl-long.421/",
        "pdf_size": 1711609,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18294096408613553884&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Harbin Institute of Technology\u2020; Microsoft Research\u2021; Harbin Institute of Technology\u2020; Harbin Institute of Technology\u2020; Microsoft Research\u2021",
        "aff_domain": "ir.hit.edu.cn;microsoft.com;ir.hit.edu.cn;ir.hit.edu.cn;microsoft.com",
        "email": "ir.hit.edu.cn;microsoft.com;ir.hit.edu.cn;ir.hit.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "Harbin Institute of Technology;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "HIT;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.451",
        "title": "CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task. In contrast, a hallmark of human intelligence is the ability to learn new concepts purely from language. Here, we explore training zero-shot classifiers for structured data purely from language. For this, we introduce CLUES, a benchmark for Classifier Learning Using natural language ExplanationS, consisting of a range of classification tasks over structured data along with natural language supervision in the form of explanations. CLUES consists of 36 real-world and 144 synthetic classification tasks. It contains crowdsourced explanations describing real-world tasks from multiple teachers and programmatically generated explanations for the synthetic tasks. To model the influence of explanations in classifying an example, we develop ExEnt, an entailment-based model that learns classifiers using explanations. ExEnt generalizes up to 18% better (relative) on novel tasks than a baseline that does not use explanations. We delineate key challenges for automated learning from explanations, addressing which can lead to progress on CLUES in the future. Code and datasets are available at: https://clues-benchmark.github.io.",
        "author": "Rakesh R. Menon; Sayan Ghosh; Shashank Srivastava",
        "authorids": "/r/rakesh-r-menon/; /s/sayan-ghosh/; /s/shashank-srivastava/",
        "bibtex": "@inproceedings{menon-etal-2022-clues,\n    title = \"{CLUES}: A Benchmark for Learning Classifiers using Natural Language Explanations\",\n    author = \"R. Menon, Rakesh  and\n      Ghosh, Sayan  and\n      Srivastava, Shashank\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.451/\",\n    doi = \"10.18653/v1/2022.acl-long.451\",\n    pages = \"6523--6546\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.451.pdf",
        "site": "https://aclanthology.org/2022.acl-long.451/",
        "pdf_size": 1543371,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1110993063889174&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "https://clues-benchmark.github.io",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.439",
        "title": "CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Named Entity Recognition (NER) in Few-Shot setting is imperative for entity tagging in low resource domains. Existing approaches only learn class-specific semantic features and intermediate representations from source domains. This affects generalizability to unseen target domains, resulting in suboptimal performances. To this end, we present CONTaiNER, a novel contrastive learning technique that optimizes the inter-token distribution distance for Few-Shot NER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a generalized objective of differentiating between token categories based on their Gaussian-distributed embeddings. This effectively alleviates overfitting issues originating from training domains. Our experiments in several traditional test domains (OntoNotes, CoNLL\u201903, WNUT \u201817, GUM) and a new large scale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER outperforms previous methods by 3%-13% absolute F1 points while showing consistent performance trends, even in challenging scenarios where previous approaches could not achieve appreciable performance.",
        "author": "Sarkar Snigdha Sarathi Das; Arzoo Katiyar; Rebecca Passonneau; Rui Zhang",
        "authorids": "/s/sarkar-snigdha-sarathi-das/; /a/arzoo-katiyar/; /r/rebecca-j-passonneau/; /r/rui-zhang/",
        "bibtex": "@inproceedings{das-etal-2022-container,\n    title = \"{CONT}ai{NER}: Few-Shot Named Entity Recognition via Contrastive Learning\",\n    author = \"Das, Sarkar Snigdha Sarathi  and\n      Katiyar, Arzoo  and\n      Passonneau, Rebecca  and\n      Zhang, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.439/\",\n    doi = \"10.18653/v1/2022.acl-long.439\",\n    pages = \"6338--6353\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.439.pdf",
        "site": "https://aclanthology.org/2022.acl-long.439/",
        "pdf_size": 1233933,
        "gs_citation": 211,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11191039603319474353&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 7,
        "aff": "Pennsylvania State University; Pennsylvania State University; Pennsylvania State University; Pennsylvania State University",
        "aff_domain": "psu.edu;psu.edu;psu.edu;psu.edu",
        "email": "psu.edu;psu.edu;psu.edu;psu.edu",
        "github": "https://github.com/psunlpgroup/CONTaiNER",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Pennsylvania State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.psu.edu",
        "aff_unique_abbr": "PSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.475",
        "title": "CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-hop question generation focuses on generating complex questions that require reasoning over multiple pieces of information of the input passage. Current models with state-of-the-art performance have been able to generate the correct questions corresponding to the answers. However, most models can not ensure the complexity of generated questions, so they may generate shallow questions that can be answered without multi-hop reasoning. To address this challenge, we propose the CQG, which is a simple and effective controlled framework. CQG employs a simple method to generate the multi-hop questions that contain key entities in multi-hop reasoning chains, which ensure the complexity and quality of the questions. In addition, we introduce a novel controlled Transformer-based decoder to guarantee that key entities appear in the questions. Experiment results show that our model greatly improves performance, which also outperforms the state-of-the-art model about 25% by 5 BLEU points on HotpotQA.",
        "author": "Zichu Fei; Qi Zhang; Tao Gui; Di Liang; Sirui Wang; Wei Wu; Xuanjing Huang",
        "authorids": "/z/zichu-fei/; /q/qi-zhang/; /t/tao-gui/; /d/di-liang/; /s/sirui-wang/; /w/wei-wu/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{fei-etal-2022-cqg,\n    title = \"{CQG}: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation\",\n    author = \"Fei, Zichu  and\n      Zhang, Qi  and\n      Gui, Tao  and\n      Liang, Di  and\n      Wang, Sirui  and\n      Wu, Wei  and\n      Huang, Xuanjing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.475/\",\n    doi = \"10.18653/v1/2022.acl-long.475\",\n    pages = \"6896--6906\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.475.pdf",
        "site": "https://aclanthology.org/2022.acl-long.475/",
        "pdf_size": 1278777,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10849185250845036722&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Fudan University1 + Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China2 + Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China3; School of Computer Science, Fudan University1 + Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China2 + Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China3; Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China3; Meituan Inc., Beijing, China4; Meituan Inc., Beijing, China4; Meituan Inc., Beijing, China4; School of Computer Science, Fudan University1 + Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China2 + Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China3",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;meituan.com;meituan.com;gmail.com;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;meituan.com;meituan.com;gmail.com;fudan.edu.cn",
        "github": "https://github.com/sion-zcfei/CQG",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1+0;0+1+0;0;2;2;2;0+1+0",
        "aff_unique_norm": "Fudan University;Shanghai Key Laboratory of Intelligent Information Processing;Meituan Inc.",
        "aff_unique_dep": "School of Computer Science;Intelligent Information Processing;",
        "aff_unique_url": "https://www.fudan.edu.cn;;https://www.meituan.com",
        "aff_unique_abbr": "Fudan;;Meituan",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.205",
        "title": "CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT, a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.",
        "author": "Tayfun Ates; M. Ate\u015fo\u011flu; \u00c7a\u011fatay Yi\u011fit; Ilker Kesen; Mert Kobas; Erkut Erdem; Aykut Erdem; Tilbe Goksun; Deniz Yuret",
        "authorids": "/t/tayfun-ates/; /m/m-atesoglu/; /c/cagatay-yigit/; /i/ilker-kesen/; /m/mert-kobas/; /e/erkut-erdem/; /a/aykut-erdem/; /t/tilbe-goksun/; /d/deniz-yuret/",
        "bibtex": "@inproceedings{ates-etal-2022-craft,\n    title = \"{CRAFT}: A Benchmark for Causal Reasoning About Forces and in{T}eractions\",\n    author = \"Ates, Tayfun  and\n      Ate{\\c{s}}o{\\u{g}}lu, M.  and\n      Yi{\\u{g}}it, {\\c{C}}a{\\u{g}}atay  and\n      Kesen, Ilker  and\n      Kobas, Mert  and\n      Erdem, Erkut  and\n      Erdem, Aykut  and\n      Goksun, Tilbe  and\n      Yuret, Deniz\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.205/\",\n    doi = \"10.18653/v1/2022.findings-acl.205\",\n    pages = \"2602--2627\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.205.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.205/",
        "pdf_size": 6628123,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4980074114399142604&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Hacettepe University, Computer Engineering Department; Ko\u00e7 University, KUIS AI Center; Ko\u00e7 University, Computer Engineering Department; Ko\u00e7 University, Psychology Department; Hacettepe University, Computer Engineering Department+Ko\u00e7 University, KUIS AI Center; Ko\u00e7 University, KUIS AI Center+Ko\u00e7 University, Computer Engineering Department; Ko\u00e7 University, KUIS AI Center+Ko\u00e7 University, Computer Engineering Department; Ko\u00e7 University, Psychology Department; Ko\u00e7 University, KUIS AI Center+Ko\u00e7 University, Computer Engineering Department",
        "aff_domain": "; ; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://sites.google.com/view/craft-benchmark",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;0+1;1+1;1+1;1;1+1",
        "aff_unique_norm": "Hacettepe University;Ko\u00e7 University",
        "aff_unique_dep": "Computer Engineering Department;KUIS AI Center",
        "aff_unique_url": "https://www.hacettepe.edu.tr;https://www.ku.edu.tr",
        "aff_unique_abbr": ";Ko\u00e7",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0;0+0;0+0;0;0+0",
        "aff_country_unique": "T\u00fcrkiye"
    },
    {
        "id": "2022.findings-acl.237",
        "title": "CRASpell: A Contextual Typo Robust Approach to Improve Chinese Spelling Correction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, Bert-based models have dominated the research of Chinese spelling correction (CSC). These methods have two limitations: (1) they have poor performance on multi-typo texts. In such texts, the context of each typo contains at least one misspelled character, which brings noise information. Such noisy context leads to the declining performance on multi-typo texts. (2) they tend to overcorrect valid expressions to more frequent expressions due to the masked token recovering task of Bert. We attempt to address these limitations in this paper. To make our model robust to contextual noise brought by typos, our approach first constructs a noisy context for each training sample. Then the correction model is forced to yield similar outputs based on the noisy and original contexts. Moreover, to address the overcorrection problem, copy mechanism is incorporated to encourage our model to prefer to choose the input character when the miscorrected and input character are both valid according to the given context. Experiments are conducted on widely used benchmarks. Our model achieves superior performance against state-of-the-art methods by a remarkable gain.",
        "author": "Shulin Liu; Shengkang Song; Tianchi Yue; Tao Yang; Huihui Cai; TingHao Yu; Shengli Sun",
        "authorids": "/s/shulin-liu/; /s/shengkang-song/; /t/tianchi-yue/; /t/tao-yang/; /h/huihui-cai/; /t/tinghao-yu/; /s/shengli-sun/",
        "bibtex": "@inproceedings{liu-etal-2022-craspell,\n    title = \"{CRAS}pell: A Contextual Typo Robust Approach to Improve {C}hinese Spelling Correction\",\n    author = \"Liu, Shulin  and\n      Song, Shengkang  and\n      Yue, Tianchi  and\n      Yang, Tao  and\n      Cai, Huihui  and\n      Yu, TingHao  and\n      Sun, Shengli\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.237/\",\n    doi = \"10.18653/v1/2022.findings-acl.237\",\n    pages = \"3008--3018\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.237.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.237/",
        "pdf_size": 487965,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9640582213547193974&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 3,
        "aff": "Tencent AI Platform Department, China+Peking University, China; Tencent AI Platform Department, China+Peking University, China; Tencent AI Platform Department, China; Tencent AI Platform Department, China; Tencent AI Platform Department, China; Tencent AI Platform Department, China; Peking University, China",
        "aff_domain": "tencent.com;stu.pku.edu.cn;tencent.com; ; ; ;pku.edu.cn",
        "email": "tencent.com;stu.pku.edu.cn;tencent.com; ; ; ;pku.edu.cn",
        "github": "https://github.com/liushulinle/CRASpell",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0;0;0;0;1",
        "aff_unique_norm": "Tencent;Peking University",
        "aff_unique_dep": "AI Platform Department;",
        "aff_unique_url": "https://www.tencent.com;http://www.pku.edu.cn",
        "aff_unique_abbr": "Tencent;Peking U",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.164",
        "title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.",
        "author": "Pei Ke; Hao Zhou; Yankai Lin; Peng Li; Jie Zhou; Xiaoyan Zhu; Minlie Huang",
        "authorids": "/p/pei-ke/; /h/hao-zhou/; /y/yankai-lin/; /p/peng-li/; /j/jie-zhou/; /x/xiaoyan-zhu/; /m/minlie-huang/",
        "bibtex": "@inproceedings{ke-etal-2022-ctrleval,\n    title = \"{CTRLE}val: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation\",\n    author = \"Ke, Pei  and\n      Zhou, Hao  and\n      Lin, Yankai  and\n      Li, Peng  and\n      Zhou, Jie  and\n      Zhu, Xiaoyan  and\n      Huang, Minlie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.164/\",\n    doi = \"10.18653/v1/2022.acl-long.164\",\n    pages = \"2306--2319\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.164.pdf",
        "site": "https://aclanthology.org/2022.acl-long.164/",
        "pdf_size": 748714,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7030666829945989216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; Pattern Recognition Center, WeChat AI, Tencent Inc., China; Pattern Recognition Center, WeChat AI, Tencent Inc., China; Institute for AI Industry Research (AIR), Tsinghua University, China; Pattern Recognition Center, WeChat AI, Tencent Inc., China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China",
        "aff_domain": "outlook.com;tencent.com;tencent.com;air.tsinghua.edu.cn;tencent.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "outlook.com;tencent.com;tencent.com;air.tsinghua.edu.cn;tencent.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/thu-coai/CTRLEval",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;0;1;0;0",
        "aff_unique_norm": "Tsinghua University;Tencent",
        "aff_unique_dep": "Institute for Artificial Intelligence;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "Tsinghua;Tencent",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.265",
        "title": "CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose a framework to modularize the training of neural language models that use diverse forms of context by eliminating the need to jointly train context and within-sentence encoders. Our approach, contextual universal embeddings (CUE), trains LMs on one type of contextual data and adapts to novel context types. The model consists of a pretrained neural sentence LM, a BERT-based contextual encoder, and a masked transfomer decoder that estimates LM probabilities using sentence-internal and contextual evidence. When contextually annotated data is unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy. Real context data can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder\u2019s embedding space. We validate the CUE framework on a NYTimes text corpus with multiple metadata types, for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context. Bootstrapping a contextual LM with only a subset of the metadata during training retains 85% of the achievable gain. Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context. Furthermore, we can swap one type of pretrained sentence LM for another without retraining the context encoders, by only adapting the decoder model. Overall, we obtain a modular framework that allows incremental, scalable training of context-enhanced LMs.",
        "author": "Scott Novotney; Sreeparna Mukherjee; Zeeshan Ahmed; Andreas Stolcke",
        "authorids": "/s/scott-novotney/; /s/sreeparna-mukherjee/; /z/zeeshan-ahmed/; /a/andreas-stolcke/",
        "bibtex": "@inproceedings{novotney-etal-2022-cue,\n    title = \"{CUE} Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals\",\n    author = \"Novotney, Scott  and\n      Mukherjee, Sreeparna  and\n      Ahmed, Zeeshan  and\n      Stolcke, Andreas\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.265/\",\n    doi = \"10.18653/v1/2022.findings-acl.265\",\n    pages = \"3368--3379\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.265.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.265/",
        "pdf_size": 694208,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3738713563080322427&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon Alexa",
        "aff_unique_url": "https://www.amazon.com/alexa",
        "aff_unique_abbr": "Amazon Alexa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.162",
        "title": "CaM-Gen: Causally Aware Metric-Guided Text Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. The relationship between the goal (metrics) of target content and the content itself is non-trivial. While large-scale language models show promising text generation capabilities, guiding the generated text with external metrics is challenging. These metrics and content tend to have inherent relationships and not all of them may be of consequence. We introduce CaM-Gen: Causally aware Generative Networks guided by user-defined target metrics incorporating the causal relationships between the metric and content features. We leverage causal inference techniques to identify causally significant aspects of a text that lead to the target metric and then explicitly guide generative models towards these by a feedback mechanism. We propose this mechanism for variational autoencoder and Transformer-based generative models. The proposed models beat baselines in terms of the target metric control while maintaining fluency and language quality of the generated text. To the best of our knowledge, this is one of the early attempts at controlled generation incorporating a metric guide using causal inference.",
        "author": "Navita Goyal; Roodram Paneri; Ayush Agarwal; Udit Kalani; Abhilasha Sancheti; Niyati Chhaya",
        "authorids": "/n/navita-goyal/; /r/roodram-paneri/; /a/ayush-agarwal/; /u/udit-kalani/; /a/abhilasha-sancheti/; /n/niyati-chhaya/",
        "bibtex": "@inproceedings{goyal-etal-2022-cam,\n    title = \"{C}a{M}-{G}en: {C}ausally Aware Metric-Guided Text Generation\",\n    author = \"Goyal, Navita  and\n      Paneri, Roodram  and\n      Agarwal, Ayush  and\n      Kalani, Udit  and\n      Sancheti, Abhilasha  and\n      Chhaya, Niyati\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.162/\",\n    doi = \"10.18653/v1/2022.findings-acl.162\",\n    pages = \"2047--2060\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.162.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.162/",
        "pdf_size": 1219885,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2811277294219284720&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 5,
        "aff": "University of Maryland\u2217; Microsoft\u2217; Tournafest\u2217; Adobe Systems\u2217; University of Maryland; Adobe Research",
        "aff_domain": "umd.edu;microsoft.com;gmail.com;adobe.com;umd.edu;adobe.com",
        "email": "umd.edu;microsoft.com;gmail.com;adobe.com;umd.edu;adobe.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;0;3",
        "aff_unique_norm": "University of Maryland;Microsoft;Tournafest;Adobe",
        "aff_unique_dep": ";Microsoft Corporation;;Adobe Systems",
        "aff_unique_url": "https://www/umd.edu;https://www.microsoft.com;;https://www.adobe.com",
        "aff_unique_abbr": "UMD;Microsoft;;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2022.acl-long.377",
        "title": "CaMEL: Case Marker Extraction without Labels",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce CaMEL (Case Marker Extraction without Labels), a novel and challenging task in computational morphology that is especially relevant for low-resource languages. We propose a first model for CaMEL that uses a massively multilingual corpus to extract case markers in 83 languages based only on a noun phrase chunker and an alignment system. To evaluate CaMEL, we automatically construct a silver standard from UniMorph. The case markers extracted by our model can be used to detect and visualise similarities and differences between the case systems of different languages as well as to annotate fine-grained deep cases in languages in which they are not overtly marked.",
        "author": "Leonie Weissweiler; Valentin Hofmann; Masoud Jalili Sabet; Hinrich Schuetze",
        "authorids": "/l/leonie-weissweiler/; /v/valentin-hofmann/; /m/masoud-jalili-sabet/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{weissweiler-etal-2022-camel,\n    title = \"{CaMEL}: {C}ase {M}arker {E}xtraction without {L}abels\",\n    author = \"Weissweiler, Leonie  and\n      Hofmann, Valentin  and\n      Jalili Sabet, Masoud  and\n      Schuetze, Hinrich\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.377/\",\n    doi = \"10.18653/v1/2022.acl-long.377\",\n    pages = \"5506--5516\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.377.pdf",
        "site": "https://aclanthology.org/2022.acl-long.377/",
        "pdf_size": 380846,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14491621030525636718&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Information and Language Processing, LMU Munich; Faculty of Linguistics, University of Oxford+Center for Information and Language Processing, LMU Munich; Center for Information and Language Processing, LMU Munich; Center for Information and Language Processing, LMU Munich",
        "aff_domain": "cis.lmu.de;ling-phil.ox.ac.uk;cis.lmu.de; ",
        "email": "cis.lmu.de;ling-phil.ox.ac.uk;cis.lmu.de; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "LMU Munich;University of Oxford",
        "aff_unique_dep": "Center for Information and Language Processing;Faculty of Linguistics",
        "aff_unique_url": "https://www.lmu.de;https://www.ox.ac.uk",
        "aff_unique_abbr": "LMU;Oxford",
        "aff_campus_unique_index": "0;1+0;0;0",
        "aff_campus_unique": "Munich;Oxford",
        "aff_country_unique_index": "0;1+0;0;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "2022.findings-acl.133",
        "title": "Calibration of Machine Reading Systems at Scale",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In typical machine learning systems, an estimate of the probability of the prediction is used to assess the system\u2019s confidence in the prediction. This confidence measure is usually uncalibrated; i.e. the system\u2019s confidence in the prediction does not match the true probability of the predicted output. In this paper, we present an investigation into calibrating open setting machine reading systemssuch as open-domain question answering and claim verification systems. We show that calibrating such complex systems which contain discrete retrieval and deep reading components is challenging and current calibration techniques fail to scale to these settings. We propose simple extensions to existing calibration approaches that allows us to adapt them to these settings. Our experimental results reveal that the approach works well, and can be useful to selectively predict answers when question answering systems are posed with unanswerable or out-of-the-training distribution questions.",
        "author": "Shehzaad Dhuliawala; Leonard Adolphs; Rajarshi Das; Mrinmaya Sachan",
        "authorids": "/s/shehzaad-dhuliawala/; /l/leonard-adolphs/; /r/rajarshi-das/; /m/mrinmaya-sachan/",
        "bibtex": "@inproceedings{dhuliawala-etal-2022-calibration,\n    title = \"Calibration of Machine Reading Systems at Scale\",\n    author = \"Dhuliawala, Shehzaad  and\n      Adolphs, Leonard  and\n      Das, Rajarshi  and\n      Sachan, Mrinmaya\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.133/\",\n    doi = \"10.18653/v1/2022.findings-acl.133\",\n    pages = \"1682--1693\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.133.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.133/",
        "pdf_size": 675395,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7397609796592604908&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "ETH Z\u00fcrich; ETH Z\u00fcrich; University of Massachusetts Amherst; ETH Z\u00fcrich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "ETH Zurich;University of Massachusetts Amherst",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.umass.edu",
        "aff_unique_abbr": "ETHZ;UMass Amherst",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Amherst",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "2022.acl-long.429",
        "title": "Can Explanations Be Useful for Calibrating Black Box Models?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "NLP practitioners often want to take existing trained models and apply them to data from new domains. While fine-tuning or few-shot learning can be used to adapt a base model, there is no single recipe for making these techniques work; moreover, one may not have access to the original model weights if it is deployed as a black box. We study how to improve a black box model\u2019s performance on a new domain by leveraging explanations of the model\u2019s behavior. Our approach first extracts a set of features combining human intuition about the task with model attributions generated by black box interpretation techniques, then uses a simple calibrator, in the form of a classifier, to predict whether the base model was correct or not. We experiment with our method on two tasks, extractive question answering and natural language inference, covering adaptation from several pairs of domains with limited target-domain data. The experimental results across all the domain pairs show that explanations are useful for calibrating these models, boosting accuracy when predictions do not have to be returned on every example. We further show that the calibration model transfers to some extent between tasks.",
        "author": "Xi Ye; Greg Durrett",
        "authorids": "/x/xi-ye/; /g/greg-durrett/",
        "bibtex": "@inproceedings{ye-durrett-2022-explanations,\n    title = \"Can Explanations Be Useful for Calibrating Black Box Models?\",\n    author = \"Ye, Xi  and\n      Durrett, Greg\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.429/\",\n    doi = \"10.18653/v1/2022.acl-long.429\",\n    pages = \"6199--6212\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.429.pdf",
        "site": "https://aclanthology.org/2022.acl-long.429/",
        "pdf_size": 476238,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16154255669308043422&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "https://github.com/xiye17/InterpCalib",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.543",
        "title": "Can Pre-trained Language Models Interpret Similes as Smart as Human?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Simile interpretation is a crucial task in natural language processing. Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art performance on many tasks. However, it remains under-explored whether PLMs can interpret similes or not. In this paper, we investigate the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, i.e., to let the PLMs infer the shared properties of similes. We construct our simile property probing datasets from both general textual corpora and human-designed questions, containing 1,633 examples covering seven main categories. Our empirical study based on the constructed datasets shows that PLMs can infer similes\u2019 shared properties while still underperforming humans. To bridge the gap with human performance, we additionally design a knowledge-enhanced training objective by incorporating the simile knowledge into PLMs via knowledge embedding methods. Our method results in a gain of 8.58% in the probing task and 1.37% in the downstream task of sentiment classification. The datasets and code are publicly available at https://github.com/Abbey4799/PLMs-Interpret-Simile.",
        "author": "Qianyu He; Sijie Cheng; Zhixu Li; Rui Xie; Yanghua Xiao",
        "authorids": "/q/qianyu-he/; /s/sijie-cheng/; /z/zhixu-li/; /r/rui-xie/; /y/yanghua-xiao/",
        "bibtex": "@inproceedings{he-etal-2022-pre,\n    title = \"Can Pre-trained Language Models Interpret Similes as Smart as Human?\",\n    author = \"He, Qianyu  and\n      Cheng, Sijie  and\n      Li, Zhixu  and\n      Xie, Rui  and\n      Xiao, Yanghua\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.543/\",\n    doi = \"10.18653/v1/2022.acl-long.543\",\n    pages = \"7875--7887\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.543.pdf",
        "site": "https://aclanthology.org/2022.acl-long.543/",
        "pdf_size": 1205462,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12960737448416685519&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University + Fudan-Aishu Cognitive Intelligence Joint Research Center, Shanghai, China; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University; Meituan, Shanghai, China; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University + Fudan-Aishu Cognitive Intelligence Joint Research Center, Shanghai, China",
        "aff_domain": "m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;meituan.com;fudan.edu.cn",
        "email": "m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;meituan.com;fudan.edu.cn",
        "github": "https://github.com/Abbey4799/PLMs-Interpret-Simile",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+0;0;1;0+0",
        "aff_unique_norm": "Fudan University;Meituan",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.meituan.com",
        "aff_unique_abbr": "Fudan;Meituan",
        "aff_campus_unique_index": "0;0+0;0;0;0+0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.398",
        "title": "Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prompt-based probing has been widely used in evaluating the abilities of pretrained language models (PLMs). Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable. Furthermore, the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks for evaluating and applying PLMs in real-world applications. To discover, understand and quantify the risks, this paper investigates the prompt-based probing from a causal view, highlights three critical biases which could induce biased results and conclusions, and proposes to conduct debiasing via causal intervention. This paper provides valuable insights for the design of unbiased datasets, better probing frameworks and more reliable evaluations of pretrained language models. Furthermore, our conclusions also echo that we need to rethink the criteria for identifying better pretrained language models.",
        "author": "Boxi Cao; Hongyu Lin; Xianpei Han; Fangchao Liu; Le Sun",
        "authorids": "/b/boxi-cao/; /h/hongyu-lin/; /x/xianpei-han/; /f/fangchao-liu/; /l/le-sun/",
        "bibtex": "@inproceedings{cao-etal-2022-prompt,\n    title = \"Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View\",\n    author = \"Cao, Boxi  and\n      Lin, Hongyu  and\n      Han, Xianpei  and\n      Liu, Fangchao  and\n      Sun, Le\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.398/\",\n    doi = \"10.18653/v1/2022.acl-long.398\",\n    pages = \"5796--5808\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.398.pdf",
        "site": "https://aclanthology.org/2022.acl-long.398/",
        "pdf_size": 507792,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12309936809782842964&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Chinese Information Processing Laboratory; Chinese Information Processing Laboratory; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Beijing Academy of Artificial Intelligence; Chinese Information Processing Laboratory+University of Chinese Academy of Sciences; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science",
        "aff_domain": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "email": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "github": "https://github.com/c-box/causalEval",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0+1+2;0+3;0+1",
        "aff_unique_norm": "Chinese Information Processing Laboratory;State Key Laboratory of Computer Science;Beijing Academy of Artificial Intelligence;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Information Processing;;;",
        "aff_unique_url": ";;https://www.baaic.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": ";;BAAI;UCAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.326",
        "title": "Can Synthetic Translations Improve Bitext Quality?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Synthetic translations have been used for a wide range of NLP tasks primarily as a means of data augmentation. This work explores, instead, how synthetic translations can be used to revise potentially imperfect reference translations in mined bitext. We find that synthetic samples can improve bitext quality without any additional bilingual supervision when they replace the originals based on a semantic equivalence classifier that helps mitigate NMT noise. The improved quality of the revised bitext is confirmed intrinsically via human evaluation and extrinsically through bilingual induction and MT tasks.",
        "author": "Eleftheria Briakou; Marine Carpuat",
        "authorids": "/e/eleftheria-briakou/; /m/marine-carpuat/",
        "bibtex": "@inproceedings{briakou-carpuat-2022-synthetic,\n    title = \"Can Synthetic Translations Improve Bitext Quality?\",\n    author = \"Briakou, Eleftheria  and\n      Carpuat, Marine\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.326/\",\n    doi = \"10.18653/v1/2022.acl-long.326\",\n    pages = \"4753--4766\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.326.pdf",
        "site": "https://aclanthology.org/2022.acl-long.326/",
        "pdf_size": 753235,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12427222795850926804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland",
        "aff_domain": "cs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.252",
        "title": "Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Unlike literal expressions, idioms\u2019 meanings do not directly follow from their parts, posing a challenge for neural machine translation (NMT). NMT models are often unable to translate idioms accurately and over-generate compositional, literal translations. In this work, we investigate whether the non-compositionality of idioms is reflected in the mechanics of the dominant NMT model, Transformer, by analysing the hidden states and attention patterns for models with English as source language and one of seven European languages as target language. When Transformer emits a non-literal translation - i.e. identifies the expression as idiomatic - the encoder processes idioms more strongly as single lexical units compared to literal expressions. This manifests in idioms\u2019 parts being grouped through attention and in reduced interaction between idioms and their context. In the decoder\u2019s cross-attention, figurative inputs result in reduced attention on source-side tokens. These results suggest that Transformer\u2019s tendency to process idioms as compositional expressions contributes to literal translations of idioms.",
        "author": "Verna Dankers; Christopher Lucas; Ivan Titov",
        "authorids": "/v/verna-dankers/; /c/christopher-lucas/; /i/ivan-titov/",
        "bibtex": "@inproceedings{dankers-etal-2022-transformer,\n    title = \"Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation\",\n    author = \"Dankers, Verna  and\n      Lucas, Christopher  and\n      Titov, Ivan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.252/\",\n    doi = \"10.18653/v1/2022.acl-long.252\",\n    pages = \"3608--3626\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.252.pdf",
        "site": "https://aclanthology.org/2022.acl-long.252/",
        "pdf_size": 1458030,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=867006282335956943&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ILCC, University of Edinburgh; ILCC, University of Edinburgh; ILCC, University of Edinburgh + ILLC, University of Amsterdam",
        "aff_domain": "gmail.com;inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "gmail.com;inf.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University of Edinburgh;University of Amsterdam",
        "aff_unique_dep": "ILCC;ILLC",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.uva.nl",
        "aff_unique_abbr": "Edinburgh;UvA",
        "aff_campus_unique_index": "0;0;0+1",
        "aff_campus_unique": "Edinburgh;Amsterdam",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "United Kingdom;Netherlands"
    },
    {
        "id": "2022.acl-long.536",
        "title": "Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Identifying argument components from unstructured texts and predicting the relationships expressed among them are two primary steps of argument mining. The intrinsic complexity of these tasks demands powerful learning models. While pretrained Transformer-based Language Models (LM) have been shown to provide state-of-the-art results over different NLP tasks, the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models. In this work, we propose a novel transfer learning strategy to overcome these challenges. We utilize argumentation-rich social discussions from the ChangeMyView subreddit as a source of unsupervised, argumentative discourse-aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task. Furthermore, we introduce a novel prompt-based strategy for inter-component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context. Exhaustive experiments show the generalization capability of our method on these two tasks over within-domain as well as out-of-domain datasets, outperforming several existing and employed strong baselines.",
        "author": "Subhabrata Dutta; Jeevesh Juneja; Dipankar Das; Tanmoy Chakraborty",
        "authorids": "/s/subhabrata-dutta/; /j/jeevesh-juneja/; /d/dipankar-das/; /t/tanmoy-chakraborty/",
        "bibtex": "@inproceedings{dutta-etal-2022-unsupervised,\n    title = \"Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?\",\n    author = \"Dutta, Subhabrata  and\n      Juneja, Jeevesh  and\n      Das, Dipankar  and\n      Chakraborty, Tanmoy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.536/\",\n    doi = \"10.18653/v1/2022.acl-long.536\",\n    pages = \"7774--7786\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.536.pdf",
        "site": "https://aclanthology.org/2022.acl-long.536/",
        "pdf_size": 881122,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=804885707579728061&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 7,
        "aff": "Jadavpur University; Delhi Technological University; Jadavpur University; IIIT-Delhi",
        "aff_domain": "gmail.com;gmail.com;gmail.com;iiitd.ac.in",
        "email": "gmail.com;gmail.com;gmail.com;iiitd.ac.in",
        "github": "https://github.com/Jeevesh8/arg_mining",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Jadavpur University;Delhi Technological University;Indraprastha Institute of Information Technology, Delhi",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.jaduniv.edu.in;https://www.dtu.ac.in;https://www.iiitdelhi.ac.in",
        "aff_unique_abbr": "JU;DTU;IIIT-D",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Delhi",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-short.73",
        "title": "Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Cognitively plausible visual dialogue models should keep a mental scoreboard of shared established facts in the dialogue context. We propose a theory-based evaluation method for investigating to what degree models pretrained on the VisDial dataset incrementally build representations that appropriately do scorekeeping. Our conclusion is that the ability to make the distinction between shared and privately known statements along the dialogue is moderately present in the analysed models, but not always incrementally consistent, which may partially be due to the limited need for grounding interactions in the original task.",
        "author": "Brielen Madureira; David Schlangen",
        "authorids": "/b/brielen-madureira/; /d/david-schlangen/",
        "bibtex": "@inproceedings{madureira-schlangen-2022-visual,\n    title = \"Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge\",\n    author = \"Madureira, Brielen  and\n      Schlangen, David\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.73/\",\n    doi = \"10.18653/v1/2022.acl-short.73\",\n    pages = \"651--664\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.73.pdf",
        "site": "https://aclanthology.org/2022.acl-short.73/",
        "pdf_size": 2318582,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9343498955008147917&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 6,
        "aff": "University of Potsdam, Germany; University of Potsdam, Germany",
        "aff_domain": "uni-potsdam.de;uni-potsdam.de",
        "email": "uni-potsdam.de;uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Potsdam",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-potsdam.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-short.84",
        "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata\u2014i.e. under \u201cwug test\u201d-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
        "author": "Ling Liu; Mans Hulden",
        "authorids": "/l/ling-liu/; /m/mans-hulden/",
        "bibtex": "@inproceedings{liu-hulden-2022-transformer,\n    title = \"Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models\",\n    author = \"Liu, Ling  and\n      Hulden, Mans\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.84/\",\n    doi = \"10.18653/v1/2022.acl-short.84\",\n    pages = \"739--749\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.84.pdf",
        "site": "https://aclanthology.org/2022.acl-short.84/",
        "pdf_size": 1101084,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10124563081954823913&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "University of Colorado; University of Colorado",
        "aff_domain": "colorado.edu;colorado.edu",
        "email": "colorado.edu;colorado.edu",
        "github": "https://github.com/LINGuistLIU/transformer-wug-test",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.61",
        "title": "Canary Extraction in Natural Language Understanding Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Natural Language Understanding (NLU) models can be trained on sensitive information such as phone numbers, zip-codes etc. Recent literature has focused on Model Inversion Attacks (ModIvA) that can extract training data from model parameters. In this work, we present a version of such an attack by extracting canaries inserted in NLU training data. In the attack, an adversary with open-box access to the model reconstructs the canaries contained in the model\u2019s training set. We evaluate our approach by performing text completion on canaries and demonstrate that by using the prefix (non-sensitive) tokens of the canary, we can generate the full canary. As an example, our attack is able to reconstruct a four digit code in the training dataset of the NLU model with a probability of 0.5 in its best configuration. As countermeasures, we identify several defense mechanisms that, when combined, effectively eliminate the risk of ModIvA in our experiments.",
        "author": "Rahil Parikh; Christophe Dupuy; Rahul Gupta",
        "authorids": "/r/rahil-parikh/; /c/christophe-dupuy/; /r/rahul-gupta/",
        "bibtex": "@inproceedings{parikh-etal-2022-canary,\n    title = \"Canary Extraction in Natural Language Understanding Models\",\n    author = \"Parikh, Rahil  and\n      Dupuy, Christophe  and\n      Gupta, Rahul\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.61/\",\n    doi = \"10.18653/v1/2022.acl-short.61\",\n    pages = \"552--560\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.61.pdf",
        "site": "https://aclanthology.org/2022.acl-short.61/",
        "pdf_size": 901786,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11275902181226740472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Systems Research, University of Maryland; Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Maryland;Amazon",
        "aff_unique_dep": "Institute for Systems Research;Amazon Alexa AI",
        "aff_unique_url": "https://www.umd.edu;https://www.amazon.com",
        "aff_unique_abbr": "UMD;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.120",
        "title": "Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels due to its subjectivity. Several recent efforts have been made to acknowledge and embrace the existence of ambiguity, and explore how to capture the human disagreement distribution. In contrast with directly learning from gold ambiguity labels, relying on special resource, we argue that the model has naturally captured the human ambiguity distribution as long as it\u2019s calibrated, i.e. the predictive probability can reflect the true correctness likelihood. Our experiments show that when model is well-calibrated, either by label smoothing or temperature scaling, it can obtain competitive performance as prior work, on both divergence scores between predictive probability and the true human opinion distribution, and the accuracy. This reveals the overhead of collecting gold ambiguity labels can be cut, by broadly solving how to calibrate the NLI network.",
        "author": "Yuxia Wang; Minghan Wang; Yimeng Chen; Shimin Tao; Jiaxin Guo; Chang Su; Min Zhang; Hao Yang",
        "authorids": "/y/yuxia-wang/; /m/minghan-wang/; /y/yimeng-chen/; /s/shimin-tao/; /j/jiaxin-guo/; /c/chang-su/; /m/min-zhang/; /h/hao-yang/",
        "bibtex": "@inproceedings{wang-etal-2022-capture,\n    title = \"Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference\",\n    author = \"Wang, Yuxia  and\n      Wang, Minghan  and\n      Chen, Yimeng  and\n      Tao, Shimin  and\n      Guo, Jiaxin  and\n      Su, Chang  and\n      Zhang, Min  and\n      Yang, Hao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.120/\",\n    doi = \"10.18653/v1/2022.findings-acl.120\",\n    pages = \"1524--1535\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.120.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.120/",
        "pdf_size": 406387,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=650754188894192616&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 3,
        "aff": "The University of Melbourne, Victoria, Australia; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China",
        "aff_domain": "student.unimelb.edu.au;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "email": "student.unimelb.edu.au;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;1;1;1",
        "aff_unique_norm": "University of Melbourne;Huawei",
        "aff_unique_dep": ";Translation Services Center",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.huawei.com",
        "aff_unique_abbr": "UniMelb;Huawei",
        "aff_campus_unique_index": "0;1;1;1;1;1;1;1",
        "aff_campus_unique": "Melbourne;Beijing",
        "aff_country_unique_index": "0;1;1;1;1;1;1;1",
        "aff_country_unique": "Australia;China"
    },
    {
        "id": "2022.acl-long.482",
        "title": "Challenges and Strategies in Cross-Cultural NLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.",
        "author": "Daniel Hershcovich; Stella Frank; Heather Lent; Miryam de Lhoneux; Mostafa Abdou; Stephanie Brandl; Emanuele Bugliarello; Laura Cabello Piqueras; Ilias Chalkidis; Ruixiang Cui; Constanza Fierro; Katerina Margatina; Phillip Rust; Anders S\u00f8gaard",
        "authorids": "/d/daniel-hershcovich/; /s/stella-frank/; /h/heather-lent/; /m/miryam-de-lhoneux/; /m/mostafa-abdou/; /s/stephanie-brandl/; /e/emanuele-bugliarello/; /l/laura-cabello-piqueras/; /i/ilias-chalkidis/; /r/ruixiang-cui/; /c/constanza-fierro/; /k/katerina-margatina/; /p/phillip-rust/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{hershcovich-etal-2022-challenges,\n    title = \"Challenges and Strategies in Cross-Cultural {NLP}\",\n    author = \"Hershcovich, Daniel  and\n      Frank, Stella  and\n      Lent, Heather  and\n      de Lhoneux, Miryam  and\n      Abdou, Mostafa  and\n      Brandl, Stephanie  and\n      Bugliarello, Emanuele  and\n      Cabello Piqueras, Laura  and\n      Chalkidis, Ilias  and\n      Cui, Ruixiang  and\n      Fierro, Constanza  and\n      Margatina, Katerina  and\n      Rust, Phillip  and\n      S{\\o}gaard, Anders\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.482/\",\n    doi = \"10.18653/v1/2022.acl-long.482\",\n    pages = \"6997--7013\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.482.pdf",
        "site": "https://aclanthology.org/2022.acl-long.482/",
        "pdf_size": 423379,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10008289207156452501&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Copenhagen; University of Trento; University of Copenhagen+Uppsala University+KU Leuven; University of Copenhagen; University of Copenhagen; University of Copenhagen; University of Copenhagen; University of Copenhagen; University of Copenhagen; University of Copenhagen; University of Copenhagen; University of Sheffield; University of Copenhagen; University of Copenhagen",
        "aff_domain": "di.ku.dk; ; ; ; ; ; ; ; ; ; ; ; ; ",
        "email": "di.ku.dk; ; ; ; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 14,
        "aff_unique_index": "0;1;0+2+3;0;0;0;0;0;0;0;0;4;0;0",
        "aff_unique_norm": "University of Copenhagen;University of Trento;Uppsala University;Katholieke Universiteit Leuven;University of Sheffield",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.ku.dk;https://www.unitn.it;https://www.uu.se;https://www.kuleuven.be;https://www.sheffield.ac.uk",
        "aff_unique_abbr": "UCPH;UniTN;UU;KU Leuven;Sheffield",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+2+3;0;0;0;0;0;0;0;0;4;0;0",
        "aff_country_unique": "Denmark;Italy;Sweden;Belgium;United Kingdom"
    },
    {
        "id": "2022.findings-acl.11",
        "title": "Challenges to Open-Domain Constituency Parsing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Neural constituency parsers have reached practical performance on news-domain benchmarks. However, their generalization ability to other domains remains weak. Existing findings on cross-domain constituency parsing are only made on a limited number of domains. Tracking this, we manually annotate a high-quality constituency treebank containing five domains. We analyze challenges to open-domain constituency parsing using a set of linguistic features on various strong constituency parsers. Primarily, we find that 1) BERT significantly increases parsers\u2019 cross-domain performance by reducing their sensitivity on the domain-variant features.2) Compared with single metrics such as unigram distribution and OOV rate, challenges to open-domain constituency parsing arise from complex features, including cross-domain lexical and constituent structure variations.",
        "author": "Sen Yang; Leyang Cui; Ruoxi Ning; Di Wu; Yue Zhang",
        "authorids": "/s/sen-yang/; /l/leyang-cui/; /r/ruoxi-ning/; /d/di-wu/; /y/yue-zhang/",
        "bibtex": "@inproceedings{yang-etal-2022-challenges,\n    title = \"Challenges to Open-Domain Constituency Parsing\",\n    author = \"Yang, Sen  and\n      Cui, Leyang  and\n      Ning, Ruoxi  and\n      Wu, Di  and\n      Zhang, Yue\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.11/\",\n    doi = \"10.18653/v1/2022.findings-acl.11\",\n    pages = \"112--127\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.11.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.11/",
        "pdf_size": 1661170,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13088869742869622113&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "\u2660School of Engineering, Westlake University + 3Institute of Advanced Technology, Westlake Institute for Advanced Study; \u2661Zhejiang University + \u2660School of Engineering, Westlake University; \u2661Zhejiang University; \u25b3Sichuan University; \u2660School of Engineering, Westlake University + 3Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "gmail.com;westlake.edu.cn;westlake.edu.cn;163.com;gmail.com",
        "email": "gmail.com;westlake.edu.cn;westlake.edu.cn;163.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2+0;2;3;0+1",
        "aff_unique_norm": "Westlake University;Westlake Institute for Advanced Study;Zhejiang University;Sichuan University",
        "aff_unique_dep": "School of Engineering;Institute of Advanced Technology;;",
        "aff_unique_url": "https://www.westlake.edu.cn;http://www.wias.org.cn/;https://www.zju.edu.cn;https://www.scu.edu.cn",
        "aff_unique_abbr": ";WIAS;ZJU;SCU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.278",
        "title": "Characterizing Idioms: Conventionality and Contingency",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Idioms are unlike most phrases in two important ways. First, words in an idiom have non-canonical meanings. Second, the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom. Linguistic theories differ on whether these properties depend on one another, as well as whether special theoretical machinery is needed to accommodate idioms. We define two measures that correspond to the properties above, and we show that idioms fall at the expected intersection of the two dimensions, but that the dimensions themselves are not correlated. Our results suggest that introducing special machinery to handle idioms may not be warranted.",
        "author": "Michaela Socolof; Jackie Cheung; Michael Wagner; Timothy O\u2019Donnell",
        "authorids": "/m/michaela-socolof/; /j/jackie-chi-kit-cheung/; /m/michael-wagner/; /t/timothy-odonnell/",
        "bibtex": "@inproceedings{socolof-etal-2022-characterizing,\n    title = \"Characterizing Idioms: Conventionality and Contingency\",\n    author = \"Socolof, Michaela  and\n      Cheung, Jackie  and\n      Wagner, Michael  and\n      O{'}Donnell, Timothy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.278/\",\n    doi = \"10.18653/v1/2022.acl-long.278\",\n    pages = \"4024--4037\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.278.pdf",
        "site": "https://aclanthology.org/2022.acl-long.278/",
        "pdf_size": 374006,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1198186223505413923&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "McGill University1 + Quebec AI Institue, Mila2 + Canada CIFAR AI Chair3; McGill University1 + Quebec AI Institue, Mila2 + Canada CIFAR AI Chair3; McGill University1; McGill University1 + Quebec AI Institue, Mila2 + Canada CIFAR AI Chair3",
        "aff_domain": "mail.mcgill.ca;cs.mcgill.ca; ;mcgill.ca",
        "email": "mail.mcgill.ca;cs.mcgill.ca; ;mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0+1+2;0;0+1+2",
        "aff_unique_norm": "McGill University;Quebec AI Institute;Canada CIFAR AI Chair",
        "aff_unique_dep": ";AI Institute;AI",
        "aff_unique_url": "https://www.mcgill.ca;https://www.quebecai.ca;https://www.cifar.ca/",
        "aff_unique_abbr": "McGill;Quebec AI;CIFAR AI",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.277",
        "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
        "author": "Shankar Kantharaj; Rixie Tiffany Leong; Xiang Lin; Ahmed Masry; Megh Thakkar; Enamul Hoque; Shafiq Joty",
        "authorids": "/s/shankar-kantharaj/; /r/rixie-tiffany-leong/; /x/xiang-lin/; /a/ahmed-masry/; /m/megh-thakkar/; /e/enamul-hoque/; /s/shafiq-joty/",
        "bibtex": "@inproceedings{kantharaj-etal-2022-chart,\n    title = \"Chart-to-Text: A Large-Scale Benchmark for Chart Summarization\",\n    author = \"Kantharaj, Shankar  and\n      Leong, Rixie Tiffany  and\n      Lin, Xiang  and\n      Masry, Ahmed  and\n      Thakkar, Megh  and\n      Hoque, Enamul  and\n      Joty, Shafiq\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.277/\",\n    doi = \"10.18653/v1/2022.acl-long.277\",\n    pages = \"4005--4023\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.277.pdf",
        "site": "https://aclanthology.org/2022.acl-long.277/",
        "pdf_size": 5102999,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6975759482991710257&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "York University, Canada; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; York University, Canada; Nanyang Technological University, Singapore; York University, Canada; Nanyang Technological University, Singapore + Salesforce Research Asia, Singapore",
        "aff_domain": "yorku.ca;ntu.edu.sg;ntu.edu.sg;yorku.ca;gmail.com;yorku.ca;ntu.edu.sg",
        "email": "yorku.ca;ntu.edu.sg;ntu.edu.sg;yorku.ca;gmail.com;yorku.ca;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;0;1;0;1+2",
        "aff_unique_norm": "York University;Nanyang Technological University;Salesforce Research Asia",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.yorku.ca;https://www.ntu.edu.sg;https://research.salesforce.com",
        "aff_unique_abbr": "York U;NTU;SRA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1;0;1+1",
        "aff_country_unique": "Canada;Singapore"
    },
    {
        "id": "2022.findings-acl.177",
        "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
        "author": "Ahmed Masry; Xuan Long Do; Jia Qing Tan; Shafiq Joty; Enamul Hoque",
        "authorids": "/a/ahmed-masry/; /x/xuan-long-do/; /j/jia-qing-tan/; /s/shafiq-joty/; /e/enamul-hoque/",
        "bibtex": "@inproceedings{masry-etal-2022-chartqa,\n    title = \"{C}hart{QA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning\",\n    author = \"Masry, Ahmed  and\n      Do, Xuan Long  and\n      Tan, Jia Qing  and\n      Joty, Shafiq  and\n      Hoque, Enamul\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.177/\",\n    doi = \"10.18653/v1/2022.findings-acl.177\",\n    pages = \"2263--2279\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.177.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.177/",
        "pdf_size": 3030985,
        "gs_citation": 617,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10343579365391430110&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 3,
        "aff": "York University, Canada; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore + Salesforce Research; York University, Canada",
        "aff_domain": "yorku.ca;e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg;yorku.ca",
        "email": "yorku.ca;e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg;yorku.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1+2;0",
        "aff_unique_norm": "York University;Nanyang Technological University;Salesforce",
        "aff_unique_dep": ";;Salesforce Research",
        "aff_unique_url": "https://www.yorku.ca;https://www.ntu.edu.sg;https://research.salesforce.com",
        "aff_unique_abbr": "York U;NTU;Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1+2;0",
        "aff_country_unique": "Canada;Singapore;United States"
    },
    {
        "id": "2022.acl-long.522",
        "title": "ChatMatch: Evaluating Chatbots by Autonomous Chat Tournaments",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing automatic evaluation systems of chatbots mostly rely on static chat scripts as ground truth, which is hard to obtain, and requires access to the models of the bots as a form of \u201cwhite-box testing\u201d. Interactive evaluation mitigates this problem but requires human involvement. In our work, we propose an interactive chatbot evaluation framework in which chatbots compete with each other like in a sports tournament, using flexible scoring metrics. This framework can efficiently rank chatbots independently from their model architectures and the domains for which they are trained.",
        "author": "Ruolan Yang; Zitong Li; Haifeng Tang; Kenny Zhu",
        "authorids": "/r/ruolan-yang/; /z/zitong-li/; /h/haifeng-tang/; /k/kenny-zhu/",
        "bibtex": "@inproceedings{yang-etal-2022-chatmatch,\n    title = \"{C}hat{M}atch: Evaluating Chatbots by Autonomous Chat Tournaments\",\n    author = \"Yang, Ruolan  and\n      Li, Zitong  and\n      Tang, Haifeng  and\n      Zhu, Kenny\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.522/\",\n    doi = \"10.18653/v1/2022.acl-long.522\",\n    pages = \"7579--7590\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.522.pdf",
        "site": "https://aclanthology.org/2022.acl-long.522/",
        "pdf_size": 992445,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8819375755963260340&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; China Merchants Bank Credit Card Center; Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cmbchina.com;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cmbchina.com;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;China Merchants Bank",
        "aff_unique_dep": ";Credit Card Center",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.cmbchina.com.cn",
        "aff_unique_abbr": "SJTU;CMB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.306",
        "title": "Chinese Synesthesia Detection: New Dataset and Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this paper, we introduce a new task called synesthesia detection, which aims to extract the sensory word of a sentence, and to predict the original and synesthetic sensory modalities of the corresponding sensory word. Synesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities. It involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought and action, which makes it become a bridge between figurative linguistic phenomenon and abstract cognition, and thus be helpful to understand the deep semantics. To address this, we construct a large-scale human-annotated Chinese synesthesia dataset, which contains 7,217 annotated sentences accompanied by 187 sensory words. Based on this dataset, we propose a family of strong and representative baseline models. Upon these baselines, we further propose a radical-based neural network model to identify the boundary of the sensory word, and to jointly detect the original and synesthetic sensory modalities for the word. Through extensive experiments, we observe that the importance of the proposed task and dataset can be verified by the statistics and progressive performances. In addition, our proposed model achieves state-of-the-art results on the synesthesia dataset.",
        "author": "Xiaotong Jiang; Qingqing Zhao; Yunfei Long; Zhongqing Wang",
        "authorids": "/x/xiaotong-jiang/; /q/qingqing-zhao/; /y/yunfei-long/; /z/zhongqing-wang/",
        "bibtex": "@inproceedings{jiang-etal-2022-chinese,\n    title = \"{C}hinese Synesthesia Detection: New Dataset and Models\",\n    author = \"Jiang, Xiaotong  and\n      Zhao, Qingqing  and\n      Long, Yunfei  and\n      Wang, Zhongqing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.306/\",\n    doi = \"10.18653/v1/2022.findings-acl.306\",\n    pages = \"3877--3887\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.306.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.306/",
        "pdf_size": 798500,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5939018490116107027&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Natural Language Processing Lab, Soochow University, Suzhou, China; Institute of Linguistics, Chinese Academy of Social Sciences, Beijing, China; University of Essex, UK; Natural Language Processing Lab, Soochow University, Suzhou, China",
        "aff_domain": "outlook.com;cass.org.cn;essex.ac.uk;suda.edu.cn",
        "email": "outlook.com;cass.org.cn;essex.ac.uk;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Soochow University;Chinese Academy of Social Sciences;University of Essex",
        "aff_unique_dep": "Natural Language Processing Lab;Institute of Linguistics;",
        "aff_unique_url": "http://www.soochow.edu.cn;http://www.cass.cn/;https://www.essex.ac.uk",
        "aff_unique_abbr": ";CASS;Essex",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Suzhou;Beijing;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2022.acl-long.17",
        "title": "CipherDAug: Ciphertext based Data Augmentation for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a novel data-augmentation technique for neural machine translation based on ROT-k ciphertexts. ROT-k is a simple letter substitution cipher that replaces a letter in the plaintext with the kth letter after it in the alphabet. We first generate multiple ROT-k ciphertexts using different values of k for the plaintext which is the source side of the parallel data. We then leverage this enciphered training data along with the original parallel data via multi-source training to improve neural machine translation. Our method, CipherDAug, uses a co-regularization-inspired training procedure, requires no external data sources other than the original training data, and uses a standard Transformer to outperform strong data augmentation techniques on several datasets by a significant margin. This technique combines easily with existing approaches to data augmentation, and yields particularly strong results in low-resource settings.",
        "author": "Nishant Kambhatla; Logan Born; Anoop Sarkar",
        "authorids": "/n/nishant-kambhatla/; /l/logan-born/; /a/anoop-sarkar/",
        "bibtex": "@inproceedings{kambhatla-etal-2022-cipherdaug,\n    title = \"{C}ipher{DA}ug: Ciphertext based Data Augmentation for Neural Machine Translation\",\n    author = \"Kambhatla, Nishant  and\n      Born, Logan  and\n      Sarkar, Anoop\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.17/\",\n    doi = \"10.18653/v1/2022.acl-long.17\",\n    pages = \"201--218\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.17.pdf",
        "site": "https://aclanthology.org/2022.acl-long.17/",
        "pdf_size": 671265,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8723106104384096764&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computing Science, Simon Fraser University; School of Computing Science, Simon Fraser University; School of Computing Science, Simon Fraser University",
        "aff_domain": "sfu.ca;sfu.ca;sfu.ca",
        "email": "sfu.ca;sfu.ca;sfu.ca",
        "github": "https://github.com/protonish/cipherdaug-nmt",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Simon Fraser University",
        "aff_unique_dep": "School of Computing Science",
        "aff_unique_url": "https://www.sfu.ca",
        "aff_unique_abbr": "SFU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Burnaby",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.183",
        "title": "ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generating new events given context with correlated ones plays a crucial role in many event-centric reasoning tasks. Existing works either limit their scope to specific scenarios or overlook event-level correlations. In this paper, we propose to pre-train a general Correlation-aware context-to-Event Transformer (ClarET) for event-centric reasoning. To achieve this, we propose three novel event-centric objectives, i.e., whole event recovering, contrastive event-correlation encoding and prompt-based event locating, which highlight event-level correlations with effective training. The proposed ClarET is applicable to a wide range of event-centric reasoning scenarios, considering its versatility of (i) event-correlation types (e.g., causal, temporal, contrast), (ii) application formulations (i.e., generation and classification), and (iii) reasoning types (e.g., abductive, counterfactual and ending reasoning). Empirical fine-tuning results, as well as zero- and few-shot learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations), verify its effectiveness and generalization ability.",
        "author": "Yucheng Zhou; Tao Shen; Xiubo Geng; Guodong Long; Daxin Jiang",
        "authorids": "/y/yucheng-zhou/; /t/tao-shen/; /x/xiubo-geng/; /g/guodong-long/; /d/daxin-jiang/",
        "bibtex": "@inproceedings{zhou-etal-2022-claret,\n    title = \"{C}lar{ET}: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification\",\n    author = \"Zhou, Yucheng  and\n      Shen, Tao  and\n      Geng, Xiubo  and\n      Long, Guodong  and\n      Jiang, Daxin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.183/\",\n    doi = \"10.18653/v1/2022.acl-long.183\",\n    pages = \"2559--2575\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.183.pdf",
        "site": "https://aclanthology.org/2022.acl-long.183/",
        "pdf_size": 901897,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3082975273185208572&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Australian AI Institute, School of CS, FEIT, University of Technology Sydney+Microsoft; Microsoft; Microsoft; Australian AI Institute, School of CS, FEIT, University of Technology Sydney; Microsoft",
        "aff_domain": "gmail.com;microsoft.com;microsoft.com;uts.edu.au;microsoft.com",
        "email": "gmail.com;microsoft.com;microsoft.com;uts.edu.au;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "University of Technology Sydney;Microsoft",
        "aff_unique_dep": "School of Computer Science;Microsoft Corporation",
        "aff_unique_url": "https://www.uts.edu.au;https://www.microsoft.com",
        "aff_unique_abbr": "UTS;Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Sydney;",
        "aff_country_unique_index": "0+1;1;1;0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "2022.findings-acl.43",
        "title": "Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Reddit is home to a broad spectrum of political activity, and users signal their political affiliations in multiple ways\u2014from self-declarations to community participation. Frequently, computational studies have treated political users as a single bloc, both in developing models to infer political leaning and in studying political behavior. Here, we test this assumption of political users and show that commonly-used political-inference models do not generalize, indicating heterogeneous types of political users. The models remain imprecise at best for most users, regardless of which sources of data or methods are used. Across a 14-year longitudinal analysis, we demonstrate that the choice in definition of a political user has significant implications for behavioral analysis. Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic\u2014but not all political users behave this way. Last, we identify a subset of political users who repeatedly flip affiliations, showing that these users are the most controversial of all, acting as provocateurs by more frequently bringing up politics, and are more likely to be banned, suspended, or deleted.",
        "author": "Kenan Alkiek; Bohan Zhang; David Jurgens",
        "authorids": "/k/kenan-alkiek/; /b/bohan-zhang/; /d/david-jurgens/",
        "bibtex": "@inproceedings{alkiek-etal-2022-classification,\n    title = \"Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis\",\n    author = \"Alkiek, Kenan  and\n      Zhang, Bohan  and\n      Jurgens, David\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.43/\",\n    doi = \"10.18653/v1/2022.findings-acl.43\",\n    pages = \"504--522\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.43.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.43/",
        "pdf_size": 5139968,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14128250838393491861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Michigan; University of Michigan; University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.484",
        "title": "Clickbait Spoiling via Question Answering and Passage Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce and study the task of clickbait spoiling: generating a short text that satisfies the curiosity induced by a clickbait post. Clickbait links to a web page and advertises its contents by arousing curiosity instead of providing an informative summary. Our contributions are approaches to classify the type of spoiler needed (i.e., a phrase or a passage), and to generate appropriate spoilers. A large-scale evaluation and error analysis on a new corpus of 5,000 manually spoiled clickbait posts\u2014the Webis Clickbait Spoiling Corpus 2022\u2014shows that our spoiler type classifier achieves an accuracy of 80%, while the question answering model DeBERTa-large outperforms all others in generating spoilers for both types.",
        "author": "Matthias Hagen; Maik Fr\u00f6be; Artur Jurk; Martin Potthast",
        "authorids": "/m/matthias-hagen/; /m/maik-frobe/; /a/artur-jurk/; /m/martin-potthast/",
        "bibtex": "@inproceedings{hagen-etal-2022-clickbait,\n    title = \"Clickbait Spoiling via Question Answering and Passage Retrieval\",\n    author = {Hagen, Matthias  and\n      Fr{\\\"o}be, Maik  and\n      Jurk, Artur  and\n      Potthast, Martin},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.484/\",\n    doi = \"10.18653/v1/2022.acl-long.484\",\n    pages = \"7025--7036\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.484.pdf",
        "site": "https://aclanthology.org/2022.acl-long.484/",
        "pdf_size": 2287072,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14829207149576766300&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Martin-Luther-Universit\u00e4t Halle-Wittenberg; Martin-Luther-Universit\u00e4t Halle-Wittenberg; Martin-Luther-Universit\u00e4t Halle-Wittenberg; Leipzig University",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "https://webis.de/data.html?q=clickbait",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Martin-Luther-Universit\u00e4t;Leipzig University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-halle.de;https://www.uni-leipzig.de",
        "aff_unique_abbr": "MLU;Uni Leipzig",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Halle-Wittenberg;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.526",
        "title": "Cluster & Tune: Boost Cold Start Performance in Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In real-world scenarios, a text classification task often begins with a cold start, when labeled data is scarce. In such cases, the common practice of fine-tuning pre-trained models, such as BERT, for a target classification task, is prone to produce poor performance. We suggest a method to boost the performance of such models by adding an intermediate unsupervised classification task, between the pre-training and fine-tuning phases. As such an intermediate task, we perform clustering and train the pre-trained model on predicting the cluster labels. We test this hypothesis on various data sets, and show that this additional classification phase can significantly improve performance, mainly for topical classification tasks, when the number of labeled instances available for fine-tuning is only a couple of dozen to a few hundred.",
        "author": "Eyal Shnarch; Ariel Gera; Alon Halfon; Lena Dankin; Leshem Choshen; Ranit Aharonov; Noam Slonim",
        "authorids": "/e/eyal-shnarch/; /a/ariel-gera/; /a/alon-halfon/; /l/lena-dankin/; /l/leshem-choshen/; /r/ranit-aharonov/; /n/noam-slonim/",
        "bibtex": "@inproceedings{shnarch-etal-2022-cluster,\n    title = \"Cluster {\\&} Tune: {B}oost Cold Start Performance in Text Classification\",\n    author = \"Shnarch, Eyal  and\n      Gera, Ariel  and\n      Halfon, Alon  and\n      Dankin, Lena  and\n      Choshen, Leshem  and\n      Aharonov, Ranit  and\n      Slonim, Noam\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.526/\",\n    doi = \"10.18653/v1/2022.acl-long.526\",\n    pages = \"7639--7653\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.526.pdf",
        "site": "https://aclanthology.org/2022.acl-long.526/",
        "pdf_size": 3739004,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11410406346744497935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; ; IBM Research",
        "aff_domain": "il.ibm.com;ibm.com;il.ibm.com;il.ibm.com;il.ibm.com;gmail.com;il.ibm.com",
        "email": "il.ibm.com;ibm.com;il.ibm.com;il.ibm.com;il.ibm.com;gmail.com;il.ibm.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.170",
        "title": "ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, a lot of research has been carried out to improve the efficiency of Transformer. Among them, the sparse pattern-based method is an important branch of efficient Transformers. However, some existing sparse methods usually use fixed patterns to select words, without considering similarities between words. Other sparse methods use clustering patterns to select words, but the clustering process is separate from the training process of the target task, which causes a decrease in effectiveness. To address these limitations, we design a neural clustering method, which can be seamlessly integrated into the Self-Attention Mechanism in Transformer. The clustering task and the target task are jointly trained and optimized to benefit each other, leading to significant effectiveness improvement. In addition, our method groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently, which improves the efficiency. We verified our method on machine translation, text classification, natural language inference, and text matching tasks. Experimental results show that our method outperforms two typical sparse attention methods, Reformer and Routing Transformer while having a comparable or even better time and memory efficiency.",
        "author": "Ningning Wang; Guobing Gan; Peng Zhang; Shuai Zhang; Junqiu Wei; Qun Liu; Xin Jiang",
        "authorids": "/n/ningning-wang/; /g/guobing-gan/; /p/peng-zhang/; /s/shuai-zhang/; /j/junqiu-wei/; /q/qun-liu/; /x/xin-jiang/",
        "bibtex": "@inproceedings{wang-etal-2022-clusterformer,\n    title = \"{C}luster{F}ormer: Neural Clustering Attention for Efficient and Effective Transformer\",\n    author = \"Wang, Ningning  and\n      Gan, Guobing  and\n      Zhang, Peng  and\n      Zhang, Shuai  and\n      Wei, Junqiu  and\n      Liu, Qun  and\n      Jiang, Xin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.170/\",\n    doi = \"10.18653/v1/2022.acl-long.170\",\n    pages = \"2390--2402\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.170.pdf",
        "site": "https://aclanthology.org/2022.acl-long.170/",
        "pdf_size": 707781,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=133460413067197432&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 2,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; The Hong Kong Polytechnic University, China; Huawei Noah\u2019s Ark Lab, China; Huawei Noah\u2019s Ark Lab, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn;polyu.edu.hk;huawei.com;huawei.com",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn;polyu.edu.hk;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;2;2",
        "aff_unique_norm": "Tianjin University;Hong Kong Polytechnic University;Huawei",
        "aff_unique_dep": "College of Intelligence and Computing;;Huawei Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.tju.edu.cn;https://www.polyu.edu.hk;https://www.huawei.com/en/ai/noahs-ark-lab",
        "aff_unique_abbr": "Tianjin University;PolyU;HNAL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tianjin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.188",
        "title": "Co-VQA : Answering by Interactive Sub Question Sequence",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Most existing approaches to Visual Question Answering (VQA) answer questions directly, however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS). By simulating the process, this paper proposes a conversation-based VQA (Co-VQA) framework, which consists of three components: Questioner, Oracle, and Answerer. Questioner raises the sub questions using an extending HRED model, and Oracle answers them one-by-one. An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed, where the question-answer pair is used to update the visual representation sequentially. To perform supervised learning for each model, we introduce a well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2 datasets. Experimental results show that our method achieves state-of-the-art on VQA-CP v2. Further analyses show that SQSs help build direct semantic connections between questions and images, provide question-adaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability.",
        "author": "Ruonan Wang; Yuxi Qian; Fangxiang Feng; Xiaojie Wang; Huixing Jiang",
        "authorids": "/r/ruonan-wang/; /y/yuxi-qian/; /f/fangxiang-feng/; /x/xiaojie-wang/; /h/huixing-jiang/",
        "bibtex": "@inproceedings{wang-etal-2022-co,\n    title = \"Co-{VQA} : Answering by Interactive Sub Question Sequence\",\n    author = \"Wang, Ruonan  and\n      Qian, Yuxi  and\n      Feng, Fangxiang  and\n      Wang, Xiaojie  and\n      Jiang, Huixing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.188/\",\n    doi = \"10.18653/v1/2022.findings-acl.188\",\n    pages = \"2396--2408\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.188.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.188/",
        "pdf_size": 3046985,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13608560011466203760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Meituan-Dianping Group",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;meituan.com",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;meituan.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Meituan-Dianping",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.meituan.com",
        "aff_unique_abbr": "BUPT;Meituan-Dianping",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.101",
        "title": "Co-training an Unsupervised Constituency Parser with Weak Supervision",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence. There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span. Through self-training and co-training with the two classifiers, we show that the interplay between them helps improve the accuracy of both, and as a result, effectively parse. A seed bootstrapping technique prepares the data to train these classifiers. Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F1 on the English (PTB) test set. In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results.",
        "author": "Nickil Maveli; Shay Cohen",
        "authorids": "/n/nickil-maveli/; /s/shay-b-cohen/",
        "bibtex": "@inproceedings{maveli-cohen-2022-co,\n    title = \"{C}o-training an {U}nsupervised {C}onstituency {P}arser with {W}eak {S}upervision\",\n    author = \"Maveli, Nickil  and\n      Cohen, Shay\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.101/\",\n    doi = \"10.18653/v1/2022.findings-acl.101\",\n    pages = \"1274--1291\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.101.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.101/",
        "pdf_size": 545480,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4316369922716056934&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;inf.ed.ac.uk",
        "email": "sms.ed.ac.uk;inf.ed.ac.uk",
        "github": "https://github.com/Nickil21/weakly-supervised-parsing",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.93",
        "title": "CoCoLM: Complex Commonsense Enhanced Language Model with Discourse Relations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between \u201cJim yells at Bob\u201d and \u201cBob is upset\u201d). To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graph ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualities. Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.",
        "author": "Changlong Yu; Hongming Zhang; Yangqiu Song; Wilfred Ng",
        "authorids": "/c/changlong-yu/; /h/hongming-zhang/; /y/yangqiu-song/; /w/wilfred-ng/",
        "bibtex": "@inproceedings{yu-etal-2022-cocolm,\n    title = \"{C}o{C}o{LM}: Complex Commonsense Enhanced Language Model with Discourse Relations\",\n    author = \"Yu, Changlong  and\n      Zhang, Hongming  and\n      Song, Yangqiu  and\n      Ng, Wilfred\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.93/\",\n    doi = \"10.18653/v1/2022.findings-acl.93\",\n    pages = \"1175--1187\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.93.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.93/",
        "pdf_size": 397648,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13218594681521354876&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "HKUST, Hong Kong, China; HKUST, Hong Kong, China + Tencent AI Lab, Seattle, U.S.; HKUST, Hong Kong, China; HKUST, Hong Kong, China",
        "aff_domain": "cse.ust.hk;tencent.com;cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;tencent.com;cse.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Tencent",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www.ust.hk;https://ai.tencent.com",
        "aff_unique_abbr": "HKUST;Tencent AI Lab",
        "aff_campus_unique_index": "0;0+1;0;0",
        "aff_campus_unique": "Hong Kong;Seattle",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-short.92",
        "title": "CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.",
        "author": "L\u00fctfi Kerem Senel; Timo Schick; Hinrich Schuetze",
        "authorids": "/l/lutfi-kerem-senel/; /t/timo-schick/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{senel-etal-2022-coda21,\n    title = \"{C}o{DA}21: Evaluating Language Understanding Capabilities of {NLP} Models With Context-Definition Alignment\",\n    author = {Senel, L{\\\"u}tfi Kerem  and\n      Schick, Timo  and\n      Schuetze, Hinrich},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.92/\",\n    doi = \"10.18653/v1/2022.acl-short.92\",\n    pages = \"815--824\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.92.pdf",
        "site": "https://aclanthology.org/2022.acl-short.92/",
        "pdf_size": 290871,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7929861827797326709&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "Center for Information and Language Processing (CIS), LMU Munich, Germany; Center for Information and Language Processing (CIS), LMU Munich, Germany; Center for Information and Language Processing (CIS), LMU Munich, Germany",
        "aff_domain": "gmail.com;cis.lmu.de; ",
        "email": "gmail.com;cis.lmu.de; ",
        "github": "https://github.com/lksenel/CoDA21",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "LMU Munich",
        "aff_unique_dep": "Center for Information and Language Processing (CIS)",
        "aff_unique_url": "https://www.lmu.de",
        "aff_unique_abbr": "LMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-short.91",
        "title": "Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs).Existing methods usually apply label attention with code representations to match related text snippets. Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. By aligning codes to concepts in UMLS, we collect synonyms of every code. Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. Experiments on the MIMIC-III dataset show that our proposed method outperforms previous state-of-the-art methods.",
        "author": "Zheng Yuan; Chuanqi Tan; Songfang Huang",
        "authorids": "/z/zheng-yuan/; /c/chuanqi-tan/; /s/songfang-huang/",
        "bibtex": "@inproceedings{yuan-etal-2022-code,\n    title = \"Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic {ICD} Coding\",\n    author = \"Yuan, Zheng  and\n      Tan, Chuanqi  and\n      Huang, Songfang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.91/\",\n    doi = \"10.18653/v1/2022.acl-short.91\",\n    pages = \"808--814\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.91.pdf",
        "site": "https://aclanthology.org/2022.acl-short.91/",
        "pdf_size": 357799,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8279070528342140915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Tsinghua University+Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "mails.tsinghua.edu.cn;alibaba-inc.com;alibaba-inc.com",
        "email": "mails.tsinghua.edu.cn;alibaba-inc.com;alibaba-inc.com",
        "github": "https://github.com/GanjinZero/ICD-MSMN",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Tsinghua University;Alibaba Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "THU;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.64",
        "title": "CogTaskonomy: Cognitively Inspired Task Taxonomy Is Beneficial to Transfer Learning in NLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Is there a principle to guide transfer learning across tasks in natural language processing (NLP)? Taxonomy (Zamir et al., 2018) finds that a structure exists among visual tasks, as a principle underlying transfer learning for them. In this paper, we propose a cognitively inspired framework, CogTaskonomy, to learn taxonomy for NLP tasks. The framework consists of Cognitive Representation Analytics (CRA) and Cognitive-Neural Mapping (CNM). The former employs Representational Similarity Analysis, which is commonly used in computational neuroscience to find a correlation between brain-activity measurement and computational modeling, to estimate task similarity with task-specific sentence representations. The latter learns to detect task relations by projecting neural representations from NLP models to cognitive signals (i.e., fMRI voxels). Experiments on 12 NLP tasks, where BERT/TinyBERT are used as the underlying models for transfer learning, demonstrate that the proposed CogTaxonomy is able to guide transfer learning, achieving performance competitive to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al., 2018) but without requiring exhaustive pairwise O(m2) task transferring. Analyses further discover that CNM is capable of learning model-agnostic task taxonomy.",
        "author": "Yifei Luo; Minghui Xu; Deyi Xiong",
        "authorids": "/y/yifei-luo/; /m/minghui-xu/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{luo-etal-2022-cogtaskonomy,\n    title = \"{C}og{T}askonomy: Cognitively Inspired Task Taxonomy Is Beneficial to Transfer Learning in {NLP}\",\n    author = \"Luo, Yifei  and\n      Xu, Minghui  and\n      Xiong, Deyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.64/\",\n    doi = \"10.18653/v1/2022.acl-long.64\",\n    pages = \"904--920\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.64.pdf",
        "site": "https://aclanthology.org/2022.acl-long.64/",
        "pdf_size": 1490339,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7858738226421620906&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "github": "https://github.com/tjunlp-lab/CogTaskonomy.git",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "College of Intelligence and Computing",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.565",
        "title": "Coherence boosting: When your pretrained language model is not paying enough attention",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Long-range semantic coherence remains a challenge in automatic language generation and understanding. We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction. We present coherence boosting, an inference procedure that increases a LM\u2019s focus on a long context. We show the benefits of coherence boosting with pretrained models by distributional analyses of generated ordinary text and dialog responses. It is also found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.",
        "author": "Nikolay Malkin; Zhen Wang; Nebojsa Jojic",
        "authorids": "/n/nikolay-malkin/; /z/zhen-wang/; /n/nebojsa-jojic/",
        "bibtex": "@inproceedings{malkin-etal-2022-coherence,\n    title = \"Coherence boosting: When your pretrained language model is not paying enough attention\",\n    author = \"Malkin, Nikolay  and\n      Wang, Zhen  and\n      Jojic, Nebojsa\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.565/\",\n    doi = \"10.18653/v1/2022.acl-long.565\",\n    pages = \"8214--8236\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.565.pdf",
        "site": "https://aclanthology.org/2022.acl-long.565/",
        "pdf_size": 2028499,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9667907914573099864&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Mila / Universit\u00e9 de Montr\u00e9al; Ohio State University; Microsoft Research",
        "aff_domain": "mila.quebec;osu.edu;microsoft.com",
        "email": "mila.quebec;osu.edu;microsoft.com",
        "github": "github.com/zhenwang9102/coherence-boosting",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Ohio State University;Microsoft",
        "aff_unique_dep": "Mila;;Microsoft Research",
        "aff_unique_url": "https://www.umontreal.ca;https://www.osu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UdeM;OSU;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Montr\u00e9al;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.findings-acl.106",
        "title": "Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Relations between words are governed by hierarchical structure rather than linear ordering. Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations\u2014for example, transforming declarative sentences into questions. However, syntactic evaluations of seq2seq models have only observed models that were not pre-trained on natural language data before being trained to perform syntactic transformations, in spite of the fact that pre-training has been found to induce hierarchical linguistic generalizations in language models; in other words, the syntactic capabilities of seq2seq models may have been greatly understated. We address this gap using the pre-trained seq2seq models T5 and BART, as well as their multilingual variants mT5 and mBART. We evaluate whether they generalize hierarchically on two transformations in two languages: question formation and passivization in English and German. We find that pre-trained seq2seq models generalize hierarchically when performing syntactic transformations, whereas models trained from scratch on syntactic transformations do not. This result presents evidence for the learnability of hierarchical syntactic information from non-annotated natural language text while also demonstrating that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive.",
        "author": "Aaron Mueller; Robert Frank; Tal Linzen; Luheng Wang; Sebastian Schuster",
        "authorids": "/a/aaron-mueller/; /r/robert-frank/; /t/tal-linzen/; /l/luheng-wang/; /s/sebastian-schuster/",
        "bibtex": "@inproceedings{mueller-etal-2022-coloring,\n    title = \"Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models\",\n    author = \"Mueller, Aaron  and\n      Frank, Robert  and\n      Linzen, Tal  and\n      Wang, Luheng  and\n      Schuster, Sebastian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.106/\",\n    doi = \"10.18653/v1/2022.findings-acl.106\",\n    pages = \"1352--1368\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.106.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.106/",
        "pdf_size": 1682918,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2980892867061466658&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Johns Hopkins University; Yale University; New York University; New York University; New York University",
        "aff_domain": "jhu.edu; ; ; ;nyu.edu",
        "email": "jhu.edu; ; ; ;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;2;2",
        "aff_unique_norm": "Johns Hopkins University;Yale University;New York University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.jhu.edu;https://www.yale.edu;https://www.nyu.edu",
        "aff_unique_abbr": "JHU;Yale;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.112",
        "title": "Combining (Second-Order) Graph-Based and Headed-Span-Based Projective Dependency Parsing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Graph-based methods, which decompose the score of a dependency tree into scores of dependency arcs, are popular in dependency parsing for decades. Recently, (CITATION) propose a headed-span-based method that decomposes the score of a dependency tree into scores of headed spans. They show improvement over first-order graph-based methods. However, their method does not score dependency arcs at all, and dependency arcs are implicitly induced by their cubic-time algorithm, which is possibly sub-optimal since modeling dependency arcs is intuitively useful. In this work, we aim to combine graph-based and headed-span-based methods, incorporating both arc scores and headed span scores into our model. First, we show a direct way to combine with O(n4) parsing complexity. To decrease complexity, inspired by the classical head-splitting trick, we show two O(n3) dynamic programming algorithms to combine first- and second-order graph-based and headed-span-based methods. Our experiments on PTB, CTB, and UD show that combining first-order graph-based and headed-span-based methods is effective. We also confirm the effectiveness of second-order graph-based parsing in the deep learning age, however, we observe marginal or no improvement when combining second-order graph-based and headed-span-based methods .",
        "author": "Songlin Yang; Kewei Tu",
        "authorids": "/s/songlin-yang/; /k/kewei-tu/",
        "bibtex": "@inproceedings{yang-tu-2022-combining,\n    title = \"Combining (Second-Order) Graph-Based and Headed-Span-Based Projective Dependency Parsing\",\n    author = \"Yang, Songlin  and\n      Tu, Kewei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.112/\",\n    doi = \"10.18653/v1/2022.findings-acl.112\",\n    pages = \"1428--1434\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.112.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.112/",
        "pdf_size": 270319,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16219075055318476033&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "School of Information Science and Technology, ShanghaiTech University + Shanghai Engineering Research Center of Intelligent Vision and Imaging; School of Information Science and Technology, ShanghaiTech University + Shanghai Engineering Research Center of Intelligent Vision and Imaging",
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn",
        "email": "shanghaitech.edu.cn;shanghaitech.edu.cn",
        "github": "https://github.com/sustcsonglin/span-based-dependency-parsing",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "ShanghaiTech University;Shanghai Engineering Research Center of Intelligent Vision and Imaging",
        "aff_unique_dep": "School of Information Science and Technology;",
        "aff_unique_url": "https://www.shanghaitech.edu.cn;",
        "aff_unique_abbr": "ShanghaiTech;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.153",
        "title": "Combining Feature and Instance Attribution to Detect Artifacts",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Training the deep neural networks that dominate NLP requires large datasets. These are often collected automatically or via crowdsourcing, and may exhibit systematic biases or annotation artifacts. By the latter we mean spurious correlations between inputs and outputs that do not represent a generally held causal relationship between features and classes; models that exploit such correlations may appear to perform a given task well, but fail on out of sample data. In this paper, we evaluate use of different attribution methods for aiding identification of training data artifacts. We propose new hybrid approaches that combine saliency maps (which highlight important input features) with instance attribution methods (which retrieve training samples influential to a given prediction). We show that this proposed training-feature attribution can be used to efficiently uncover artifacts in training data when a challenging validation set is available. We also carry out a small user study to evaluate whether these methods are useful to NLP researchers in practice, with promising results. We make code for all methods and experiments in this paper available.",
        "author": "Pouya Pezeshkpour; Sarthak Jain; Sameer Singh; Byron Wallace",
        "authorids": "/p/pouya-pezeshkpour/; /s/sarthak-jain/; /s/sameer-singh/; /b/byron-c-wallace/",
        "bibtex": "@inproceedings{pezeshkpour-etal-2022-combining,\n    title = \"Combining Feature and Instance Attribution to Detect Artifacts\",\n    author = \"Pezeshkpour, Pouya  and\n      Jain, Sarthak  and\n      Singh, Sameer  and\n      Wallace, Byron\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.153/\",\n    doi = \"10.18653/v1/2022.findings-acl.153\",\n    pages = \"1934--1946\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.153.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.153/",
        "pdf_size": 603188,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13529384384762667502&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Irvine; Northeastern University; University of California, Irvine; Northeastern University",
        "aff_domain": "uci.edu;northeastern.edu;uci.edu;northeastern.edu",
        "email": "uci.edu;northeastern.edu;uci.edu;northeastern.edu",
        "github": "https://github.com/pouyapez/artifact_detection",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of California, Irvine;Northeastern University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uci.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "UCI;NEU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.182",
        "title": "Combining Static and Contextualised Multilingual Embeddings",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from XLM-R, validate those embeddings with cross-lingual word retrieval, and then align them using VecMap. This results in high-quality, highly multilingual static embeddings. Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. We show positive results for multiple complex semantic tasks. We release the static embeddings and the continued pre-training code. Unlike most previous work, our continued pre-training approach does not require parallel text.",
        "author": "Katharina H\u00e4mmerl; Jind\u0159ich Libovick\u00fd; Alexander Fraser",
        "authorids": "/k/katharina-hammerl/; /j/jindrich-libovicky/; /a/alexander-fraser/",
        "bibtex": "@inproceedings{hammerl-etal-2022-combining,\n    title = \"Combining Static and Contextualised Multilingual Embeddings\",\n    author = {H{\\\"a}mmerl, Katharina  and\n      Libovick{\\'y}, Jind{\\v{r}}ich  and\n      Fraser, Alexander},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.182/\",\n    doi = \"10.18653/v1/2022.findings-acl.182\",\n    pages = \"2316--2329\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.182.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.182/",
        "pdf_size": 224380,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2776345710960536667&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Center for Information and Language Processing, LMU Munich, Germany+Faculty of Mathematics and Physics, Charles University, Czech Republic; Faculty of Mathematics and Physics, Charles University, Czech Republic; Center for Information and Language Processing, LMU Munich, Germany",
        "aff_domain": "cis.lmu.de;ufal.mff.cuni.cz;cis.lmu.de",
        "email": "cis.lmu.de;ufal.mff.cuni.cz;cis.lmu.de",
        "github": "github.com/KathyHaem/combining-static-contextual",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "LMU Munich;Charles University",
        "aff_unique_dep": "Center for Information and Language Processing;Faculty of Mathematics and Physics",
        "aff_unique_url": "https://www.lmu.de;https://www.cuni.cz",
        "aff_unique_abbr": "LMU;Charles University",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "Germany;Czech Republic"
    },
    {
        "id": "2022.acl-long.51",
        "title": "Compact Token Representations with Contextual Quantization for Efficient Document Re-ranking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer based re-ranking models can achieve high search relevance through context- aware soft matching of query tokens with document tokens. To alleviate runtime complexity of such inference, previous work has adopted a late interaction architecture with pre-computed contextual token representations at the cost of a large online storage. This paper proposes contextual quantization of token embeddings by decoupling document-specific and document-independent ranking contributions during codebook-based compression. This allows effective online decompression and embedding composition for better search relevance. This paper presents an evaluation of the above compact token representation model in terms of relevance and space efficiency.",
        "author": "Yingrui Yang; Yifan Qiao; Tao Yang",
        "authorids": "/y/yingrui-yang/; /y/yifan-qiao/; /t/tao-yang/",
        "bibtex": "@inproceedings{yang-etal-2022-compact,\n    title = \"Compact Token Representations with Contextual Quantization for Efficient Document Re-ranking\",\n    author = \"Yang, Yingrui  and\n      Qiao, Yifan  and\n      Yang, Tao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.51/\",\n    doi = \"10.18653/v1/2022.acl-long.51\",\n    pages = \"695--707\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.51.pdf",
        "site": "https://aclanthology.org/2022.acl-long.51/",
        "pdf_size": 600766,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14139470922102122074&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of California at Santa Barbara, USA; Department of Computer Science, University of California at Santa Barbara, USA; Department of Computer Science, University of California at Santa Barbara, USA",
        "aff_domain": "cs.ucsb.edu;cs.ucsb.edu;cs.ucsb.edu",
        "email": "cs.ucsb.edu;cs.ucsb.edu;cs.ucsb.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Santa Barbara",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucsb.edu",
        "aff_unique_abbr": "UCSB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Santa Barbara",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.261",
        "title": "Comparative Opinion Summarization via Collaborative Decoding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Opinion summarization focuses on generating summaries that reflect popular subjective information expressed in multiple online reviews. While generated summaries offer general and concise information about a particular hotel or product, the information may be insufficient to help the user compare multiple different choices. Thus, the user may still struggle with the question \u201cWhich one should I pick?\u201d In this paper, we propose the comparative opinion summarization task, which aims at generating two contrastive summaries and one common summary from two different candidate sets of reviews. We develop a comparative summarization framework CoCoSum, which consists of two base summarization models that jointly generate contrastive and common summaries. Experimental results on a newly created benchmark CoCoTrip show that CoCoSum can produce higher-quality contrastive and common summaries than state-of-the-art opinion summarization models. The dataset and code are available at https://github.com/megagonlabs/cocosum",
        "author": "Hayate Iso; Xiaolan Wang; Stefanos Angelidis; Yoshihiko Suhara",
        "authorids": "/h/hayate-iso/; /x/xiaolan-wang/; /s/stefanos-angelidis/; /y/yoshi-suhara/",
        "bibtex": "@inproceedings{iso-etal-2022-comparative,\n    title = \"Comparative Opinion Summarization via Collaborative Decoding\",\n    author = \"Iso, Hayate  and\n      Wang, Xiaolan  and\n      Angelidis, Stefanos  and\n      Suhara, Yoshihiko\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.261/\",\n    doi = \"10.18653/v1/2022.findings-acl.261\",\n    pages = \"3307--3324\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.261.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.261/",
        "pdf_size": 1862575,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15120474389988391928&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Megagon Labs; University of Edinburgh; Megagon Labs; Megagon Labs",
        "aff_domain": "megagon.ai;megagon.ai;ed.ac.uk;megagon.ai",
        "email": "megagon.ai;megagon.ai;ed.ac.uk;megagon.ai",
        "github": "https://github.com/megagonlabs/cocosum",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Megagon Labs;University of Edinburgh",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.megagonlabs.com;https://www.ed.ac.uk",
        "aff_unique_abbr": ";Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2022.findings-acl.2",
        "title": "Compilable Neural Code Generation with Compiler Feedback",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT.",
        "author": "Xin Wang; Yasheng Wang; Yao Wan; Fei Mi; Yitong Li; Pingyi Zhou; Jin Liu; Hao Wu; Xin Jiang; Qun Liu",
        "authorids": "/x/xin-wang/; /y/yasheng-wang/; /y/yao-wan/; /f/fei-mi/; /y/yitong-li/; /p/pingyi-zhou/; /j/jin-liu/; /h/hao-wu/; /x/xin-jiang/; /q/qun-liu/",
        "bibtex": "@inproceedings{wang-etal-2022-compilable,\n    title = \"Compilable Neural Code Generation with Compiler Feedback\",\n    author = \"Wang, Xin  and\n      Wang, Yasheng  and\n      Wan, Yao  and\n      Mi, Fei  and\n      Li, Yitong  and\n      Zhou, Pingyi  and\n      Liu, Jin  and\n      Wu, Hao  and\n      Jiang, Xin  and\n      Liu, Qun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.2/\",\n    doi = \"10.18653/v1/2022.findings-acl.2\",\n    pages = \"9--19\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.2.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.2/",
        "pdf_size": 571825,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6456457810015972920&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Wuhan University, China + Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; School of Computer Sci. & Tech., Huazhong University of Science and Technology, China; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab + Huawei Technologies Co., Ltd.; Huawei Noah\u2019s Ark Lab; School of Computer Science, Wuhan University, China; School of Information Science and Engineering, Yunnan University, China; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "whu.edu.cn;huawei.com;hust.edu.cn;huawei.com;huawei.com;huawei.com;whu.edu.cn;ynu.edu.cn;huawei.com;huawei.com",
        "email": "whu.edu.cn;huawei.com;hust.edu.cn;huawei.com;huawei.com;huawei.com;whu.edu.cn;ynu.edu.cn;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;1;2;1;1+1;1;0;3;1;1",
        "aff_unique_norm": "Wuhan University;Huawei;Huazhong University of Science and Technology;Yunnan University",
        "aff_unique_dep": "School of Computer Science;Noah\u2019s Ark Lab;School of Computer Sci. & Tech.;School of Information Science and Engineering",
        "aff_unique_url": "http://www.whu.edu.cn;https://www.huawei.com;http://www.hust.edu.cn;http://www.ynu.edu.cn",
        "aff_unique_abbr": "WHU;Huawei;HUST;YNU",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Wuhan;",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.32",
        "title": "Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "A Temporal Knowledge Graph (TKG) is a sequence of KGs corresponding to different timestamps. TKG reasoning aims to predict potential facts in the future given the historical KG sequences. One key of this task is to mine and understand evolutional patterns of facts from these sequences. The evolutional patterns are complex in two aspects, length-diversity and time-variability. Existing models for TKG reasoning focus on modeling fact sequences of a fixed length, which cannot discover complex evolutional patterns that vary in length. Furthermore, these models are all trained offline, which cannot well adapt to the changes of evolutional patterns from then on. Thus, we propose a new model, called Complex Evolutional Network (CEN), which uses a length-aware Convolutional Neural Network (CNN) to handle evolutional patterns of different lengths via an easy-to-difficult curriculum learning strategy. Besides, we propose to learn the model under the online setting so that it can adapt to the changes of evolutional patterns over time. Extensive experiments demonstrate that CEN obtains substantial performance improvement under both the traditional offline and the proposed online settings.",
        "author": "Zixuan Li; Saiping Guan; Xiaolong Jin; Weihua Peng; Yajuan Lyu; Yong Zhu; Long Bai; Wei Li; Jiafeng Guo; Xueqi Cheng",
        "authorids": "/z/zixuan-li/; /s/saiping-guan/; /x/xiaolong-jin/; /w/weihua-peng/; /y/yajuan-lyu/; /y/yong-zhu/; /l/long-bai/; /w/wei-li/; /j/jiafeng-guo/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{li-etal-2022-complex,\n    title = \"Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning\",\n    author = \"Li, Zixuan  and\n      Guan, Saiping  and\n      Jin, Xiaolong  and\n      Peng, Weihua  and\n      Lyu, Yajuan  and\n      Zhu, Yong  and\n      Bai, Long  and\n      Li, Wei  and\n      Guo, Jiafeng  and\n      Cheng, Xueqi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.32/\",\n    doi = \"10.18653/v1/2022.acl-short.32\",\n    pages = \"290--296\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.32.pdf",
        "site": "https://aclanthology.org/2022.acl-short.32/",
        "pdf_size": 374176,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1447936429041293822&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science and Technology, University of Chinese Academy of Sciences + CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences + Baidu Inc.; School of Computer Science and Technology, University of Chinese Academy of Sciences + CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; School of Computer Science and Technology, University of Chinese Academy of Sciences + CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; Baidu Inc.; Baidu Inc.; Baidu Inc.; School of Computer Science and Technology, University of Chinese Academy of Sciences + CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; Baidu Inc.; School of Computer Science and Technology, University of Chinese Academy of Sciences + CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; School of Computer Science and Technology, University of Chinese Academy of Sciences + CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;baidu.com;baidu.com;baidu.com;ict.ac.cn;baidu.com;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;baidu.com;baidu.com;baidu.com;ict.ac.cn;baidu.com;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1+2;0+1;0+1;2;2;2;0+1;2;0+1;0+1",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences;Baidu",
        "aff_unique_dep": "School of Computer Science and Technology;Institute of Computing Technology;Baidu Inc.",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.cas.ac.cn;https://www.baidu.com",
        "aff_unique_abbr": "UCAS;CAS;Baidu",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0+0;0;0;0;0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.125",
        "title": "Composable Sparse Fine-Tuning for Cross-Lingual Transfer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pretrained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.",
        "author": "Alan Ansell; Edoardo Ponti; Anna Korhonen; Ivan Vuli\u0107",
        "authorids": "/a/alan-ansell/; /e/edoardo-ponti/; /a/anna-korhonen/; /i/ivan-vulic/",
        "bibtex": "@inproceedings{ansell-etal-2022-composable,\n    title = \"Composable Sparse Fine-Tuning for Cross-Lingual Transfer\",\n    author = \"Ansell, Alan  and\n      Ponti, Edoardo  and\n      Korhonen, Anna  and\n      Vuli{\\'c}, Ivan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.125/\",\n    doi = \"10.18653/v1/2022.acl-long.125\",\n    pages = \"1778--1796\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.125.pdf",
        "site": "https://aclanthology.org/2022.acl-long.125/",
        "pdf_size": 2641345,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4939275622030651698&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technology Lab, University of Cambridge + Mila - Quebec AI Institute and McGill University; Language Technology Lab, University of Cambridge + Mila - Quebec AI Institute and McGill University; Language Technology Lab, University of Cambridge; Language Technology Lab, University of Cambridge",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/cambridgeltl/composable-sft",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0;0",
        "aff_unique_norm": "University of Cambridge;McGill University",
        "aff_unique_dep": "Language Technology Lab;Quebec AI Institute",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.mcgill.ca",
        "aff_unique_abbr": "Cambridge;McGill",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+1;0+1;0;0",
        "aff_country_unique": "United Kingdom;Canada"
    },
    {
        "id": "2022.findings-acl.239",
        "title": "Composing Structure-Aware Batches for Pairwise Sentence Classification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Identifying the relation between two sentences requires datasets with pairwise annotations. In many cases, these datasets contain instances that are annotated multiple times as part of different pairs. They constitute a structure that contains additional helpful information about the inter-relatedness of the text instances based on the annotations. This paper investigates how this kind of structural dataset information can be exploited during training. We propose three batch composition strategies to incorporate such information and measure their performance over 14 heterogeneous pairwise sentence classification tasks. Our results show statistically significant improvements (up to 3.9%) - independent of the pre-trained language model - for most tasks compared to baselines that follow a standard training procedure. Further, we see that even this baseline procedure can profit from having such structural information in a low-resource setting.",
        "author": "Andreas Waldis; Tilman Beck; Iryna Gurevych",
        "authorids": "/a/andreas-waldis/; /t/tilman-beck/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{waldis-etal-2022-composing,\n    title = \"Composing Structure-Aware Batches for Pairwise Sentence Classification\",\n    author = \"Waldis, Andreas  and\n      Beck, Tilman  and\n      Gurevych, Iryna\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.239/\",\n    doi = \"10.18653/v1/2022.findings-acl.239\",\n    pages = \"3031--3045\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.239.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.239/",
        "pdf_size": 1098911,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17671361887636080946&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Information Systems Research Lab, Department of Computer Science, Lucerne University of Applied Sciences and Arts + Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, Technical University of Darmstadt; Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, Technical University of Darmstadt; Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, Technical University of Darmstadt",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/UKPLab/acl2022-structure-batches",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Lucerne University of Applied Sciences and Arts;Technical University of Darmstadt",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.hslu.ch;https://www.tu-darmstadt.de",
        "aff_unique_abbr": "HSLU;TUD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lucerne;",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "id": "2022.acl-long.448",
        "title": "Compositional Generalization in Dependency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Compositionality\u2014 the ability to combine familiar units like words into novel phrases and sentences\u2014 has been the focus of intense interest in artificial intelligence in recent years. To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). This dataset maximizes the similarity between the test and train distributions over primitive units, like words, while maximizing the compound divergence: the dissimilarity between test and train distributions over larger structures, like phrases. Dependency parsing, however, lacks a compositional generalization benchmark. In this work, we introduce a gold-standard set of dependency parses for CFQ, and use this to analyze the behaviour of a state-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We find that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance. Additionally, we find the performance of the dependency parser does not uniformly degrade relative to compound divergence, and the parser performs differently on different splits with the same compound divergence. We explore a number of hypotheses for what causes the non-uniform degradation in dependency parsing performance, and identify a number of syntactic structures that drive the dependency parser\u2019s lower performance on the most challenging splits.",
        "author": "Emily Goodwin; Siva Reddy; Timothy O\u2019Donnell; Dzmitry Bahdanau",
        "authorids": "/e/emily-goodwin/; /s/siva-reddy/; /t/timothy-odonnell/; /d/dzmitry-bahdanau/",
        "bibtex": "@inproceedings{goodwin-etal-2022-compositional,\n    title = \"Compositional Generalization in Dependency Parsing\",\n    author = \"Goodwin, Emily  and\n      Reddy, Siva  and\n      O{'}Donnell, Timothy  and\n      Bahdanau, Dzmitry\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.448/\",\n    doi = \"10.18653/v1/2022.acl-long.448\",\n    pages = \"6482--6493\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.448.pdf",
        "site": "https://aclanthology.org/2022.acl-long.448/",
        "pdf_size": 396924,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15240240075620958955&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Linguistics, McGill University, Canada + Quebec Artificial Intelligence Institute (Mila) + Facebook CIFAR AI Chair; School of Computer Science, McGill University, Canada + Quebec Artificial Intelligence Institute (Mila) + Canada CIFAR AI Chair; Department of Linguistics, McGill University, Canada + Quebec Artificial Intelligence Institute (Mila) + Canada CIFAR AI Chair; ServiceNow Research",
        "aff_domain": "mail.mcgill.ca; ; ; ",
        "email": "mail.mcgill.ca; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0+1+3;0+1+3;4",
        "aff_unique_norm": "McGill University;Quebec Artificial Intelligence Institute;Meta;Canadian Institute for Advanced Research;ServiceNow",
        "aff_unique_dep": "Department of Linguistics;Artificial Intelligence;Facebook CIFAR AI;AI Chair;Research",
        "aff_unique_url": "https://www.mcgill.ca;https://mila.quebec;https://www.facebook.com;https://www.cifar.ca;https://www.servicenow.com",
        "aff_unique_abbr": "McGill;Mila;FB;CIFAR;ServiceNow",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+1;0+0+0;0+0+0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.findings-acl.270",
        "title": "Comprehensive Multi-Modal Interactions for Referring Image Segmentation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the natural language description. Addressing RIS efficiently requires considering the interactions happening across visual and linguistic modalities and the interactions within each modality. Existing methods are limited because they either compute different forms of interactions sequentially (leading to error propagation) or ignore intra-modal interactions. We address this limitation by performing all three interactions simultaneously through a Synchronous Multi-Modal Fusion Module (SFM). Moreover, to produce refined segmentation masks, we propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where linguistic features facilitate the exchange of contextual information across the visual hierarchy. We present thorough ablation studies and validate our approach\u2019s performance on four benchmark datasets, showing considerable performance gains over the existing state-of-the-art (SOTA) methods.",
        "author": "Kanishk Jain; Vineet Gandhi",
        "authorids": "/k/kanishk-jain/; /v/vineet-gandhi/",
        "bibtex": "@inproceedings{jain-gandhi-2022-comprehensive,\n    title = \"Comprehensive Multi-Modal Interactions for Referring Image Segmentation\",\n    author = \"Jain, Kanishk  and\n      Gandhi, Vineet\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.270/\",\n    doi = \"10.18653/v1/2022.findings-acl.270\",\n    pages = \"3427--3435\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.270.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.270/",
        "pdf_size": 1857985,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10301104201085667577&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "CVIT, KCIS, IIIT Hyderabad, India; CVIT, KCIS, IIIT Hyderabad, India",
        "aff_domain": "gmail.com;iiit.ac.in",
        "email": "gmail.com;iiit.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad",
        "aff_unique_dep": "",
        "aff_unique_url": "https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.findings-acl.64",
        "title": "Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "How to learn highly compact yet effective sentence representation? Pre-trained language models have been effective in many NLP tasks. However, these models are often huge and produce large sentence embeddings. Moreover, there is a big performance gap between large and small models. In this paper, we propose Homomorphic Projective Distillation (HPD) to learn compressed sentence embeddings. Our method augments a small Transformer encoder model with learnable projection layers to produce compact representations while mimicking a large pre-trained language model to retain the sentence representation quality. We evaluate our method with different model sizes on both semantic textual similarity (STS) and semantic retrieval (SR) tasks. Experiments show that our method achieves 2.7-4.5 points performance gain on STS tasks compared with previous best representations of the same size. In SR tasks, our method improves retrieval speed (8.2\u00d7) and memory usage (8.0\u00d7) compared with state-of-the-art large models. Our implementation is available at https://github.com/XuandongZhao/HPD.",
        "author": "Xuandong Zhao; Zhiguo Yu; Ming Wu; Lei Li",
        "authorids": "/x/xuandong-zhao/; /z/zhiguo-yu/; /m/ming-wu/; /l/lei-li/",
        "bibtex": "@inproceedings{zhao-etal-2022-compressing,\n    title = \"Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation\",\n    author = \"Zhao, Xuandong  and\n      Yu, Zhiguo  and\n      Wu, Ming  and\n      Li, Lei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.64/\",\n    doi = \"10.18653/v1/2022.findings-acl.64\",\n    pages = \"774--781\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.64.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.64/",
        "pdf_size": 388083,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17117144428312956691&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "UC Santa Barbara; Microsoft; Microsoft; UC Santa Barbara",
        "aff_domain": "cs.ucsb.edu;microsoft.com;microsoft.com;cs.ucsb.edu",
        "email": "cs.ucsb.edu;microsoft.com;microsoft.com;cs.ucsb.edu",
        "github": "https://github.com/XuandongZhao/HPD",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of California, Santa Barbara;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://www.ucsb.edu;https://www.microsoft.com",
        "aff_unique_abbr": "UCSB;Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Santa Barbara;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.331",
        "title": "Compression of Generative Pre-trained Language Models via Quantization",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.",
        "author": "Chaofan Tao; Lu Hou; Wei Zhang; Lifeng Shang; Xin Jiang; Qun Liu; Ping Luo; Ngai Wong",
        "authorids": "/c/chaofan-tao/; /l/lu-hou/; /w/wei-zhang/; /l/lifeng-shang/; /x/xin-jiang/; /q/qun-liu/; /p/ping-luo/; /n/ngai-wong/",
        "bibtex": "@inproceedings{tao-etal-2022-compression,\n    title = \"Compression of Generative Pre-trained Language Models via Quantization\",\n    author = \"Tao, Chaofan  and\n      Hou, Lu  and\n      Zhang, Wei  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Luo, Ping  and\n      Wong, Ngai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.331/\",\n    doi = \"10.18653/v1/2022.acl-long.331\",\n    pages = \"4821--4836\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.331.pdf",
        "site": "https://aclanthology.org/2022.acl-long.331/",
        "pdf_size": 2464560,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11241535588259427240&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The University of Hong Kong; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; The University of Hong Kong; The University of Hong Kong",
        "aff_domain": "connect.hku.hk;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;cs.hku.hk;eee.hku.hk",
        "email": "connect.hku.hk;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;cs.hku.hk;eee.hku.hk",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;1;0;0",
        "aff_unique_norm": "University of Hong Kong;Huawei",
        "aff_unique_dep": ";Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.hku.hk;https://www.huawei.com",
        "aff_unique_abbr": "HKU;Huawei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.99",
        "title": "Computational Historical Linguistics and Language Diversity in South Asia",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "South Asia is home to a plethora of languages, many of which severely lack access to new language technologies. This linguistic diversity also results in a research environment conducive to the study of comparative, contact, and historical linguistics\u2013fields which necessitate the gathering of extensive data from many languages. We claim that data scatteredness (rather than scarcity) is the primary obstacle in the development of South Asian language technology, and suggest that the study of language history is uniquely aligned with surmounting this obstacle. We review recent developments in and at the intersection of South Asian NLP and historical-comparative linguistics, describing our and others\u2019 current efforts in this area. We also offer new strategies towards breaking the data barrier.",
        "author": "Aryaman Arora; Adam Farris; Samopriya Basu; Suresh Kolichala",
        "authorids": "/a/aryaman-arora/; /a/adam-farris/; /s/samopriya-basu/; /s/suresh-kolichala/",
        "bibtex": "@inproceedings{arora-etal-2022-computational,\n    title = \"Computational Historical Linguistics and Language Diversity in {S}outh {A}sia\",\n    author = \"Arora, Aryaman  and\n      Farris, Adam  and\n      Basu, Samopriya  and\n      Kolichala, Suresh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.99/\",\n    doi = \"10.18653/v1/2022.acl-long.99\",\n    pages = \"1396--1409\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.99.pdf",
        "site": "https://aclanthology.org/2022.acl-long.99/",
        "pdf_size": 1323184,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2846263987087186284&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Georgetown University; San Mateo High School; University of North Carolina \u2013 Chapel Hill; Microsoft",
        "aff_domain": "georgetown.edu;gmail.com;live.unc.edu;gmail.com",
        "email": "georgetown.edu;gmail.com;live.unc.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Georgetown University;San Mateo High School;University of North Carolina;Microsoft",
        "aff_unique_dep": ";;;Microsoft Corporation",
        "aff_unique_url": "https://www.georgetown.edu;https://www.sanmateohs.org;https://www.unc.edu;https://www.microsoft.com",
        "aff_unique_abbr": "GU;;UNC;Microsoft",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chapel Hill",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.218",
        "title": "ConTinTin: Continual Learning from Task Instructions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The mainstream machine learning paradigms for NLP often work with two underlying presumptions. First, the target task is predefined and static; a system merely needs to learn to solve it exclusively. Second, the supervision of a task mainly comes from a set of labeled examples. A question arises: how to build a system that can keep learning new tasks from their instructions?This work defines a new learning paradigm ConTinTin (Continual Learning from Task Instructions), in which a system should learn a sequence of new tasks one by one, each task is explained by a piece of textual instruction. The system is required to (i) generate the expected outputs of a new task by learning from its instruction, (ii) transfer the knowledge acquired from upstream tasks to help solve downstream tasks (i.e., forward-transfer), and (iii) retain or even improve the performance on earlier tasks after learning new tasks (i.e., backward-transfer). This new problem is studied on a stream of more than 60 tasks, each equipped with an instruction. Technically, our method InstructionSpeak contains two strategies that make full use of task instructions to improve forward-transfer and backward-transfer: one is to learn from negative outputs, the other is to re-visit instructions of previous tasks. To our knowledge, this is the first time to study ConTinTin in NLP. In addition to the problem formulation and our promising approach, this work also contributes to providing rich analyses for the community to better understand this novel learning problem.",
        "author": "Wenpeng Yin; Jia Li; Caiming Xiong",
        "authorids": "/w/wenpeng-yin/; /j/jia-li/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{yin-etal-2022-contintin,\n    title = \"{C}on{T}in{T}in: Continual Learning from Task Instructions\",\n    author = \"Yin, Wenpeng  and\n      Li, Jia  and\n      Xiong, Caiming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.218/\",\n    doi = \"10.18653/v1/2022.acl-long.218\",\n    pages = \"3062--3072\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.218.pdf",
        "site": "https://aclanthology.org/2022.acl-long.218/",
        "pdf_size": 456329,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4467396948892687615&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "Temple University; Salesforce Research; Salesforce Research",
        "aff_domain": "temple.edu;salesforce.com;salesforce.com",
        "email": "temple.edu;salesforce.com;salesforce.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Temple University;Salesforce",
        "aff_unique_dep": ";Salesforce Research",
        "aff_unique_url": "https://www.temple.edu;https://research.salesforce.com",
        "aff_unique_abbr": "Temple;Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.169",
        "title": "Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Token-level adaptive training approaches can alleviate the token imbalance problem and thus improve neural machine translation, through re-weighting the losses of different target tokens based on specific statistical metrics (e.g., token frequency or mutual information). Given that standard translation models make predictions on the condition of previous target contexts, we argue that the above statistical metrics ignore target context information and may assign inappropriate weights to target tokens. While one possible solution is to directly take target contexts into these statistical metrics, the target-context-aware statistical computing is extremely expensive, and the corresponding storage overhead is unrealistic. To solve the above issues, we propose a target-context-aware metric, named conditional bilingual mutual information (CBMI), which makes it feasible to supplement target context information for statistical metrics. Particularly, our CBMI can be formalized as the log quotient of the translation model probability and language model probability by decomposing the conditional joint distribution. Thus CBMI can be efficiently calculated during model training without any pre-specific statistical calculations and large storage overhead. Furthermore, we propose an effective adaptive training approach based on both the token- and sentence-level CBMI. Experimental results on WMT14 English-German and WMT19 Chinese-English tasks show our approach can significantly outperform the Transformer baseline and other related methods.",
        "author": "Songming Zhang; Yijin Liu; Fandong Meng; Yufeng Chen; Jinan Xu; Jian Liu; Jie Zhou",
        "authorids": "/s/songming-zhang/; /y/yijin-liu/; /f/fandong-meng/; /y/yufeng-chen/; /j/jinan-xu/; /j/jian-liu/; /j/jie-zhou/",
        "bibtex": "@inproceedings{zhang-etal-2022-conditional,\n    title = \"Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation\",\n    author = \"Zhang, Songming  and\n      Liu, Yijin  and\n      Meng, Fandong  and\n      Chen, Yufeng  and\n      Xu, Jinan  and\n      Liu, Jian  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.169/\",\n    doi = \"10.18653/v1/2022.acl-long.169\",\n    pages = \"2377--2389\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.169.pdf",
        "site": "https://aclanthology.org/2022.acl-long.169/",
        "pdf_size": 469157,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2953853917383061933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China+Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "bjtu.edu.cn;tencent.com;tencent.com;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;tencent.com",
        "email": "bjtu.edu.cn;tencent.com;tencent.com;bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn;tencent.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;0;0;0;1",
        "aff_unique_norm": "Beijing Jiao Tong University;Tencent",
        "aff_unique_dep": "Beijing Key Lab of Traffic Data Analysis and Mining;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "http://www.bjtu.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "BJTU;Tencent",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.253",
        "title": "ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We describe a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply. We call this dataset ConditionalQA. In addition to conditional answers, the dataset also features:(1) long context documents with information that is related in logically complex ways;(2) multi-hop questions that require compositional logical reasoning;(3) a combination of extractive questions, yes/no questions, questions with multiple answers, and not-answerable questions;(4) questions asked without knowing the answers. We show that ConditionalQA is challenging for many of the existing QA models, especially in selecting answer conditions. We believe that this dataset will motivate further research in answering complex questions over long documents.",
        "author": "Haitian Sun; William Cohen; Ruslan Salakhutdinov",
        "authorids": "/h/haitian-sun/; /w/william-cohen/; /r/ruslan-salakhutdinov/",
        "bibtex": "@inproceedings{sun-etal-2022-conditionalqa,\n    title = \"{C}onditional{QA}: A Complex Reading Comprehension Dataset with Conditional Answers\",\n    author = \"Sun, Haitian  and\n      Cohen, William  and\n      Salakhutdinov, Ruslan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.253/\",\n    doi = \"10.18653/v1/2022.acl-long.253\",\n    pages = \"3627--3637\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.253.pdf",
        "site": "https://aclanthology.org/2022.acl-long.253/",
        "pdf_size": 455719,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4875706412977431393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Carnegie Mellon University; Google Research; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;google.com;cs.cmu.edu",
        "email": "cs.cmu.edu;google.com;cs.cmu.edu",
        "github": "",
        "project": "https://haitian-sun.github.io/conditionalqa/",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Google",
        "aff_unique_dep": "School of Computer Science;Google Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.google",
        "aff_unique_abbr": "CMU;Google Research",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Pittsburgh;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.206",
        "title": "Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most dominant neural machine translation (NMT) models are restricted to make predictions only according to the local context of preceding words in a left-to-right manner. Although many previous studies try to incorporate global information into NMT models, there still exist limitations on how to effectively exploit bidirectional global context. In this paper, we propose a Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework for NMT, where the NMT model is jointly trained with an auxiliary conditional masked language model (CMLM). The training consists of two stages: (1) multi-task joint training; (2) confidence based knowledge distillation. At the first stage, by sharing encoder parameters, the NMT model is additionally supervised by the signal from the CMLM decoder that contains bidirectional global contexts. Moreover, at the second stage, using the CMLM as teacher, we further pertinently incorporate bidirectional global context to the NMT model on its unconfidently-predicted target words via knowledge distillation. Experimental results show that our proposed CBBGCA training framework significantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on three large-scale translation datasets, namely WMT\u201914 English-to-German, WMT\u201919 Chinese-to-English and WMT\u201914 English-to-French, respectively.",
        "author": "Chulun Zhou; Fandong Meng; Jie Zhou; Min Zhang; Hongji Wang; Jinsong Su",
        "authorids": "/c/chulun-zhou/; /f/fandong-meng/; /j/jie-zhou/; /m/min-zhang/; /h/hongji-wang/; /j/jinsong-su/",
        "bibtex": "@inproceedings{zhou-etal-2022-confidence,\n    title = \"Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation\",\n    author = \"Zhou, Chulun  and\n      Meng, Fandong  and\n      Zhou, Jie  and\n      Zhang, Min  and\n      Wang, Hongji  and\n      Su, Jinsong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.206/\",\n    doi = \"10.18653/v1/2022.acl-long.206\",\n    pages = \"2878--2889\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.206.pdf",
        "site": "https://aclanthology.org/2022.acl-long.206/",
        "pdf_size": 466970,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=53044464106217546&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 4,
        "aff": "School of Informatics, Xiamen University, Xiamen + Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Harbin Institute of Technology, Shenzhen; School of Informatics, Xiamen University, Xiamen; School of Informatics, Xiamen University, Xiamen + Pengcheng Lab, Shenzhen",
        "aff_domain": "stu.xmu.edu.cn;tencent.com;tencent.com;hotmail.com;xmu.edu.cn;xmu.edu.cn",
        "email": "stu.xmu.edu.cn;tencent.com;tencent.com;hotmail.com;xmu.edu.cn;xmu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;2;0;0+3",
        "aff_unique_norm": "Xiamen University;Tencent;Harbin Institute of Technology;Pengcheng Lab",
        "aff_unique_dep": "School of Informatics;Pattern Recognition Center, WeChat AI;;",
        "aff_unique_url": "https://www.xmu.edu.cn;https://www.tencent.com;http://en.hhit.edu.cn/;",
        "aff_unique_abbr": "XMU;Tencent;HIT;",
        "aff_campus_unique_index": "0;2;0;0+2",
        "aff_campus_unique": "Xiamen;;Shenzhen",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.268",
        "title": "Consistent Representation Learning for Continual Relation Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones. Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting. However, these memory-based methods tend to overfit the memory samples and perform poorly on imbalanced datasets. To solve these challenges, a consistent representation learning method is proposed, which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory. Specifically, supervised contrastive learning based on a memory bank is first used to train each new task so that the model can effectively learn the relation representation. Then, contrastive replay is conducted of the samples in memory and makes the model retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task. The proposed method can better learn consistent representations to alleviate forgetting effectively. Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state-of-the-art baselines and yield strong robustness on the imbalanced dataset.",
        "author": "Kang Zhao; Hua Xu; Jiangong Yang; Kai Gao",
        "authorids": "/k/kang-zhao/; /h/hua-xu/; /j/jiangong-yang/; /k/kai-gao/",
        "bibtex": "@inproceedings{zhao-etal-2022-consistent,\n    title = \"Consistent Representation Learning for Continual Relation Extraction\",\n    author = \"Zhao, Kang  and\n      Xu, Hua  and\n      Yang, Jiangong  and\n      Gao, Kai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.268/\",\n    doi = \"10.18653/v1/2022.findings-acl.268\",\n    pages = \"3402--3411\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.268.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.268/",
        "pdf_size": 459573,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3744838778775986443&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China+School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang 050018, China; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China+School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang 050018, China; School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang 050018, China",
        "aff_domain": "gmail.com;tsinghua.edu.cn;163.com;hebust.edu.cn",
        "email": "gmail.com;tsinghua.edu.cn;163.com;hebust.edu.cn",
        "github": "https://github.com/thuiar/CRL",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0+1;1",
        "aff_unique_norm": "Tsinghua University;Hebei University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Technology;School of Information Science and Engineering",
        "aff_unique_url": "https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "Tsinghua;",
        "aff_campus_unique_index": "0+1;0;0+1;1",
        "aff_campus_unique": "Beijing;Shijiazhuang",
        "aff_country_unique_index": "0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.56",
        "title": "Constrained Multi-Task Learning for Bridging Resolution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We examine the extent to which supervised bridging resolvers can be improved without employing additional labeled bridging data by proposing a novel constrained multi-task learning framework for bridging resolution, within which we (1) design cross-task consistency constraints to guide the learning process; (2) pre-train the entity coreference model in the multi-task framework on the large amount of publicly available coreference data; and (3) integrating prior knowledge encoded in rule-based resolvers. Our approach achieves state-of-the-art results on three standard evaluation corpora.",
        "author": "Hideo Kobayashi; Yufang Hou; Vincent Ng",
        "authorids": "/h/hideo-kobayashi/; /y/yufang-hou/; /v/vincent-ng/",
        "bibtex": "@inproceedings{kobayashi-etal-2022-constrained,\n    title = \"Constrained Multi-Task Learning for Bridging Resolution\",\n    author = \"Kobayashi, Hideo  and\n      Hou, Yufang  and\n      Ng, Vincent\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.56/\",\n    doi = \"10.18653/v1/2022.acl-long.56\",\n    pages = \"759--770\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.56.pdf",
        "site": "https://aclanthology.org/2022.acl-long.56/",
        "pdf_size": 401862,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10825046068691636314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Human Language Technology Research Institute, University of Texas at Dallas, USA; IBM Research Europe, Ireland; Human Language Technology Research Institute, University of Texas at Dallas, USA",
        "aff_domain": "hlt.utdallas.edu;ie.ibm.com;hlt.utdallas.edu",
        "email": "hlt.utdallas.edu;ie.ibm.com;hlt.utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Texas at Dallas;IBM",
        "aff_unique_dep": "Human Language Technology Research Institute;IBM Research Europe",
        "aff_unique_url": "https://www.utdallas.edu;https://www.ibm.com/research/europe",
        "aff_unique_abbr": "UT Dallas;IBM RE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Dallas;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Ireland"
    },
    {
        "id": "2022.findings-acl.100",
        "title": "Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing re-ranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of high-quality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.",
        "author": "Mariano Felice; Shiva Taslimipoor; Paula Buttery",
        "authorids": "/m/mariano-felice/; /s/shiva-taslimipoor/; /p/paula-buttery/",
        "bibtex": "@inproceedings{felice-etal-2022-constructing,\n    title = \"Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers\",\n    author = \"Felice, Mariano  and\n      Taslimipoor, Shiva  and\n      Buttery, Paula\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.100/\",\n    doi = \"10.18653/v1/2022.findings-acl.100\",\n    pages = \"1263--1273\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.100.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.100/",
        "pdf_size": 319306,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=359589460159436258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "ALTA Institute, Computer Laboratory, University of Cambridge; ALTA Institute, Computer Laboratory, University of Cambridge; ALTA Institute, Computer Laboratory, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "https://github.com/CambridgeALTA/fce-cep-oc",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Computer Laboratory",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.315",
        "title": "Context Matters: A Pragmatic Study of PLMs\u2019 Negation Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In linguistics, there are two main perspectives on negation: a semantic and a pragmatic view. So far, research in NLP on negation has almost exclusively adhered to the semantic view. In this article, we adopt the pragmatic paradigm to conduct a study of negation understanding focusing on transformer-based PLMs. Our results differ from previous, semantics-based studies and therefore help to contribute a more comprehensive \u2013 and, given the results, much more optimistic \u2013 picture of the PLMs\u2019 negation understanding.",
        "author": "Reto Gubelmann; Siegfried Handschuh",
        "authorids": "/r/reto-gubelmann/; /s/siegfried-handschuh/",
        "bibtex": "@inproceedings{gubelmann-handschuh-2022-context,\n    title = \"Context Matters: A Pragmatic Study of {PLM}s' Negation Understanding\",\n    author = \"Gubelmann, Reto  and\n      Handschuh, Siegfried\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.315/\",\n    doi = \"10.18653/v1/2022.acl-long.315\",\n    pages = \"4602--4621\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.315.pdf",
        "site": "https://aclanthology.org/2022.acl-long.315/",
        "pdf_size": 6028258,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9575039418979030132&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of St.Gallen; University of St.Gallen",
        "aff_domain": "unisg.ch;unisg.ch",
        "email": "unisg.ch;unisg.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of St.Gallen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unisg.ch",
        "aff_unique_abbr": "HSG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2022.acl-long.334",
        "title": "Contextual Fine-to-Coarse Distillation for Coarse-grained Response Selection in Open-Domain Conversations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study the problem of coarse-grained response selection in retrieval-based dialogue systems. The problem is equally important with fine-grained response selection, but is less explored in existing literature. In this paper, we propose a Contextual Fine-to-Coarse (CFC) distilled model for coarse-grained response selection in open-domain conversations. In our CFC model, dense representations of query, candidate contexts and responses is learned based on the multi-tower architecture using contextual matching, and richer knowledge learned from the one-tower architecture (fine-grained) is distilled into the multi-tower architecture (coarse-grained) to enhance the performance of the retriever. To evaluate the performance of the proposed model, we construct two new datasets based on the Reddit comments dump and Twitter corpus. Extensive experimental results on the two datasets show that the proposed method achieves huge improvement over all evaluation metrics compared with traditional baseline methods.",
        "author": "Wei Chen; Yeyun Gong; Can Xu; Huang Hu; Bolun Yao; Zhongyu Wei; Zhihao Fan; Xiaowu Hu; Bartuer Zhou; Biao Cheng; Daxin Jiang; Nan Duan",
        "authorids": "/w/wei-chen/; /y/yeyun-gong/; /c/can-xu/; /h/huang-hu/; /b/bolun-yao/; /z/zhongyu-wei/; /z/zhihao-fan/; /x/xiaowu-hu/; /b/bartuer-zhou/; /b/biao-cheng/; /d/daxin-jiang/; /n/nan-duan/",
        "bibtex": "@inproceedings{chen-etal-2022-contextual,\n    title = \"Contextual Fine-to-Coarse Distillation for Coarse-grained Response Selection in Open-Domain Conversations\",\n    author = \"Chen, Wei  and\n      Gong, Yeyun  and\n      Xu, Can  and\n      Hu, Huang  and\n      Yao, Bolun  and\n      Wei, Zhongyu  and\n      Fan, Zhihao  and\n      Hu, Xiaowu  and\n      Zhou, Bartuer  and\n      Cheng, Biao  and\n      Jiang, Daxin  and\n      Duan, Nan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.334/\",\n    doi = \"10.18653/v1/2022.acl-long.334\",\n    pages = \"4865--4877\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.334.pdf",
        "site": "https://aclanthology.org/2022.acl-long.334/",
        "pdf_size": 506667,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14229135872039052461&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 4,
        "aff": "School of Data Science, Fudan University, China; Microsoft Research Asia, China; Microsoft Research Asia, China; Microsoft Research Asia, China; Nanjing University of Science and Technology, China; School of Data Science, Fudan University, China + Research Institute of Intelligent and Complex Systems, Fudan University, China; School of Data Science, Fudan University, China; Microsoft Research Asia, China; Microsoft Research Asia, China; Microsoft Research Asia, China; Microsoft Research Asia, China; Microsoft Research Asia, China",
        "aff_domain": "fudan.edu.cn;microsoft.com;microsoft.com;microsoft.com;njust.edu.cn;fudan.edu.cn;fudan.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "fudan.edu.cn;microsoft.com;microsoft.com;microsoft.com;njust.edu.cn;fudan.edu.cn;fudan.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;1;1;1;2;0+0;0;1;1;1;1;1",
        "aff_unique_norm": "Fudan University;Microsoft;Nanjing University of Science and Technology",
        "aff_unique_dep": "School of Data Science;Microsoft Research Asia;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.microsoft.com/en-us/research/group/asia;http://www.nust.edu.cn/",
        "aff_unique_abbr": "Fudan;MSRA;NUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.193",
        "title": "Contextual Representation Learning beyond Masked Language Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Currently, masked language modeling (e.g., BERT) is the prime choice to learn contextualized representations. Due to the pervasiveness, it naturally raises an interesting question: how do masked language models (MLMs) learn contextual representations? In this work, we analyze the learning dynamics of MLMs and find that it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs. To address these problems, we propose TACO, a simple yet effective representation learning approach to directly model global semantics. To be specific, TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations. Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over MLM.",
        "author": "Zhiyi Fu; Wangchunshu Zhou; Jingjing Xu; Hao Zhou; Lei Li",
        "authorids": "/z/zhiyi-fu/; /w/wangchunshu-zhou/; /j/jingjing-xu/; /h/hao-zhou/; /l/lei-li/",
        "bibtex": "@inproceedings{fu-etal-2022-contextual,\n    title = \"Contextual Representation Learning beyond Masked Language Modeling\",\n    author = \"Fu, Zhiyi  and\n      Zhou, Wangchunshu  and\n      Xu, Jingjing  and\n      Zhou, Hao  and\n      Li, Lei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.193/\",\n    doi = \"10.18653/v1/2022.acl-long.193\",\n    pages = \"2701--2714\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.193.pdf",
        "site": "https://aclanthology.org/2022.acl-long.193/",
        "pdf_size": 790032,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=525332087042030620&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Peking University; ByteDance AI Lab; University of California, Santa Barbara + ByteDance AI Lab; ByteDance AI Lab; University of California, Santa Barbara + ByteDance AI Lab",
        "aff_domain": "pku.edu.cn;bytedance.com;cs.ucsb.edu;bytedance.com;cs.ucsb.edu",
        "email": "pku.edu.cn;bytedance.com;cs.ucsb.edu;bytedance.com;cs.ucsb.edu",
        "github": "https://github.com/FUZHIYI/TACO",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2+1;1;2+1",
        "aff_unique_norm": "Peking University;ByteDance;University of California, Santa Barbara",
        "aff_unique_dep": ";AI Lab;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.bytedance.com;https://www.ucsb.edu",
        "aff_unique_abbr": "Peking U;ByteDance;UCSB",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Santa Barbara",
        "aff_country_unique_index": "0;0;1+0;0;1+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.198",
        "title": "Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing continual relation learning (CRL) methods rely on plenty of labeled training data for learning a new task, which can be hard to acquire in real scenario as getting large and representative labeled data is often expensive and time-consuming. It is therefore necessary for the model to learn novel relational patterns with very few labeled data while avoiding catastrophic forgetting of previous task knowledge. In this paper, we formulate this challenging yet practical problem as continual few-shot relation learning (CFRL). Based on the finding that learning for new emerging few-shot tasks often results in feature distributions that are incompatible with previous tasks\u2019 learned distributions, we propose a novel method based on embedding space regularization and data augmentation. Our method generalizes to new few-shot tasks and avoids catastrophic forgetting of previous tasks by enforcing extra constraints on the relational embeddings and by adding extra relevant data in a self-supervised manner. With extensive experiments we demonstrate that our method can significantly outperform previous state-of-the-art methods in CFRL task settings.",
        "author": "Chengwei Qin; Shafiq Joty",
        "authorids": "/c/chengwei-qin/; /s/shafiq-joty/",
        "bibtex": "@inproceedings{qin-joty-2022-continual,\n    title = \"Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation\",\n    author = \"Qin, Chengwei  and\n      Joty, Shafiq\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.198/\",\n    doi = \"10.18653/v1/2022.acl-long.198\",\n    pages = \"2776--2789\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.198.pdf",
        "site": "https://aclanthology.org/2022.acl-long.198/",
        "pdf_size": 948699,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18403662508070484774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Nanyang Technological University; Nanyang Technological University + Salesforce Research",
        "aff_domain": "e.ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;ntu.edu.sg",
        "github": "https://github.com/qcwthu/Continual_Fewshot_Relation_Learning",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Nanyang Technological University;Salesforce",
        "aff_unique_dep": ";Salesforce Research",
        "aff_unique_url": "https://www.ntu.edu.sg;https://research.salesforce.com",
        "aff_unique_abbr": "NTU;Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "2022.acl-long.408",
        "title": "Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we study how to continually pre-train language models for improving the understanding of math problems. Specifically, we focus on solving a fundamental challenge in modeling math problems, how to fuse the semantics of textual description and formulas, which are highly different in essence. To address this issue, we propose a new approach called COMUS to continually pre-train language models for math problem understanding with syntax-aware memory network. In this approach, we first construct the math syntax graph to model the structural semantic information, by combining the parsing trees of the text and formulas, and then design the syntax-aware memory networks to deeply fuse the features from the graph and text. With the help of syntax relations, we can model the interaction between the token from the text and its semantic-related nodes within the formulas, which is helpful to capture fine-grained semantic correlations between texts and formulas. Besides, we devise three continual pre-training tasks to further align and fuse the representations of the text and math syntax graph. Experimental results on four tasks in the math domain demonstrate the effectiveness of our approach. Our code and data are publicly available at the link: bluehttps://github.com/RUCAIBox/COMUS.",
        "author": "Zheng Gong; Kun Zhou; Xin Zhao; Jing Sha; Shijin Wang; Ji-Rong Wen",
        "authorids": "/z/zheng-gong/; /k/kun-zhou/; /w/wayne-xin-zhao/; /j/jing-sha/; /s/shijin-wang/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{gong-etal-2022-continual,\n    title = \"Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network\",\n    author = \"Gong, Zheng  and\n      Zhou, Kun  and\n      Zhao, Xin  and\n      Sha, Jing  and\n      Wang, Shijin  and\n      Wen, Ji-Rong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.408/\",\n    doi = \"10.18653/v1/2022.acl-long.408\",\n    pages = \"5923--5933\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.408.pdf",
        "site": "https://aclanthology.org/2022.acl-long.408/",
        "pdf_size": 1388144,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14656295459873676084&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China; School of Information, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China; iFLYTEK Research, Hefei, Anhui, China+State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China; State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China+AI Research(Central China), iFLYTEK, Wuhan, Hubei, China; Gaoling School of Artificial Intelligence, Renmin University of China+Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China",
        "aff_domain": "gmail.com; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ",
        "github": "https://github.com/RUCAIBox/COMUS",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;2+3;3+4;0+1",
        "aff_unique_norm": "Renmin University of China;Beijing Key Laboratory of Big Data Management and Analysis Methods;iFLYTEK Research;State Key Laboratory of Cognitive Intelligence;AI Research",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;Big Data Management and Analysis;;;AI Research",
        "aff_unique_url": "http://www.ruc.edu.cn;;https://www.iflytek.com;;",
        "aff_unique_abbr": "RUC;;iFLYTEK;;",
        "aff_campus_unique_index": "0+0;0;0+0;2+2;2+3;0+0",
        "aff_campus_unique": "Beijing;;Hefei;Central China",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.80",
        "title": "Continual Prompt Tuning for Dialog State Tracking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A desirable dialog system should be able to continually learn new skills without forgetting old ones, and thereby adapt to new domains or tasks in its life cycle. However, continually training a model often leads to a well-known catastrophic forgetting issue. In this paper, we present Continual Prompt Tuning, a parameter-efficient framework that not only avoids forgetting but also enables knowledge transfer between tasks. To avoid forgetting, we only learn and store a few prompt tokens\u2019 embeddings for each task while freezing the backbone pre-trained model. To achieve bi-directional knowledge transfer among tasks, we propose several techniques (continual prompt initialization, query fusion, and memory replay) to transfer knowledge from preceding tasks and a memory-guided technique to transfer knowledge from subsequent tasks. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method on continual learning for dialog state tracking, compared with state-of-the-art baselines.",
        "author": "Qi Zhu; Bing Li; Fei Mi; Xiaoyan Zhu; Minlie Huang",
        "authorids": "/q/qi-zhu/; /b/bing-li/; /f/fei-mi/; /x/xiaoyan-zhu/; /m/minlie-huang/",
        "bibtex": "@inproceedings{zhu-etal-2022-continual,\n    title = \"Continual Prompt Tuning for Dialog State Tracking\",\n    author = \"Zhu, Qi  and\n      Li, Bing  and\n      Mi, Fei  and\n      Zhu, Xiaoyan  and\n      Huang, Minlie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.80/\",\n    doi = \"10.18653/v1/2022.acl-long.80\",\n    pages = \"1124--1137\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.80.pdf",
        "site": "https://aclanthology.org/2022.acl-long.80/",
        "pdf_size": 504646,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17383824441918603303&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "CoAI Group, DCST, IAI, BNRIST, Tsinghua University; CoAI Group, DCST, IAI, BNRIST, Tsinghua University; Huawei Noah\u2019s Ark Lab; CoAI Group, DCST, IAI, BNRIST, Tsinghua University; CoAI Group, DCST, IAI, BNRIST, Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn; ; ; ;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn; ; ; ;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Tsinghua University;Huawei",
        "aff_unique_dep": "CoAI Group, DCST, IAI, BNRIST;Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": "THU;Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.255",
        "title": "Continual Sequence Generation with Adaptive Compositional Modules",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks. To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks. We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules. Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency. We make our code public at https://github.com/GT-SALT/Adaptive-Compositional-Modules.",
        "author": "Yanzhe Zhang; Xuezhi Wang; Diyi Yang",
        "authorids": "/y/yanzhe-zhang/; /x/xuezhi-wang/; /d/diyi-yang/",
        "bibtex": "@inproceedings{zhang-etal-2022-continual,\n    title = \"Continual Sequence Generation with Adaptive Compositional Modules\",\n    author = \"Zhang, Yanzhe  and\n      Wang, Xuezhi  and\n      Yang, Diyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.255/\",\n    doi = \"10.18653/v1/2022.acl-long.255\",\n    pages = \"3653--3667\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.255.pdf",
        "site": "https://aclanthology.org/2022.acl-long.255/",
        "pdf_size": 755503,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10151805077918933662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Georgia Institute of Technology; Google; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;google.com;gatech.edu",
        "email": "gatech.edu;google.com;gatech.edu",
        "github": "https://github.com/GT-SALT/Adaptive-Compositional-Modules",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.gatech.edu;https://www.google.com",
        "aff_unique_abbr": "Georgia Tech;Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.75",
        "title": "Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Multi-Label Text Classification (MLTC) is a fundamental and challenging task in natural language processing. Previous studies mainly focus on learning text representation and modeling label correlation but neglect the rich knowledge from the existing similar instances when predicting labels of a specific text. To make up for this oversight, we propose a k nearest neighbor (kNN) mechanism which retrieves several neighbor instances and interpolates the model output with their labels. Moreover, we design a multi-label contrastive learning objective that makes the model aware of the kNN classification process and improves the quality of the retrieved neighbors while inference. Extensive experiments show that our method can bring consistent and significant performance improvement to multiple MLTC models including the state-of-the-art pretrained and non-pretrained ones.",
        "author": "Xi\u2019ao Su; Ran Wang; Xinyu Dai",
        "authorids": "/x/xiao-su/; /r/ran-wang/; /x/xinyu-dai/",
        "bibtex": "@inproceedings{su-etal-2022-contrastive,\n    title = \"Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification\",\n    author = \"Su, Xi{'}ao  and\n      Wang, Ran  and\n      Dai, Xinyu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.75/\",\n    doi = \"10.18653/v1/2022.acl-short.75\",\n    pages = \"672--679\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.75.pdf",
        "site": "https://aclanthology.org/2022.acl-short.75/",
        "pdf_size": 563454,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1792341091382037573&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 2,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, China + Collaborative Innovation Center of Novel Software Technology and Industrialization, China; National Key Laboratory for Novel Software Technology, Nanjing University, China + Collaborative Innovation Center of Novel Software Technology and Industrialization, China; National Key Laboratory for Novel Software Technology, Nanjing University, China + Collaborative Innovation Center of Novel Software Technology and Industrialization, China",
        "aff_domain": "smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Nanjing University;Collaborative Innovation Center of Novel Software Technology and Industrialization",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;",
        "aff_unique_url": "http://www.nju.edu.cn;",
        "aff_unique_abbr": "Nanjing U;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.217",
        "title": "Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We examine the effects of contrastive visual semantic pretraining by comparing the geometry and semantic properties of contextualized English language representations formed by GPT-2 and CLIP, a zero-shot multimodal image classifier which adapts the GPT-2 architecture to encode image captions. We find that contrastive visual semantic pretraining significantly mitigates the anisotropy found in contextualized word embeddings from GPT-2, such that the intra-layer self-similarity (mean pairwise cosine similarity) of CLIP word embeddings is under .25 in all layers, compared to greater than .95 in the top layer of GPT-2. CLIP word embeddings outperform GPT-2 on word-level semantic intrinsic evaluation tasks, and achieve a new corpus-based state of the art for the RG65 evaluation, at .88. CLIP also forms fine-grained semantic representations of sentences, and obtains Spearman\u2019s \ud835\udf0c = .73 on the SemEval-2017 Semantic Textual Similarity Benchmark with no fine-tuning, compared to no greater than \ud835\udf0c = .45 in any layer of GPT-2. Finally, intra-layer self-similarity of CLIP sentence embeddings decreases as the layer index increases, finishing at .25 in the top layer, while the self-similarity of GPT-2 sentence embeddings formed using the EOS token increases layer-over-layer and never falls below .97. Our results indicate that high anisotropy is not an inevitable consequence of contextualization, and that visual semantic pretraining is beneficial not only for ordering visual representations, but also for encoding useful semantic representations of language, both on the word level and the sentence level.",
        "author": "Robert Wolfe; Aylin Caliskan",
        "authorids": "/r/robert-wolfe/; /a/aylin-caliskan/",
        "bibtex": "@inproceedings{wolfe-caliskan-2022-contrastive,\n    title = \"Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations\",\n    author = \"Wolfe, Robert  and\n      Caliskan, Aylin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.217/\",\n    doi = \"10.18653/v1/2022.acl-long.217\",\n    pages = \"3050--3061\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.217.pdf",
        "site": "https://aclanthology.org/2022.acl-long.217/",
        "pdf_size": 269644,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12966830811797772449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Washington; University of Washington",
        "aff_domain": "uw.edu;uw.edu",
        "email": "uw.edu;uw.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.46",
        "title": "Controllable Dictionary Example Generation: Generating Example Sentences for Specific Targeted Audiences",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Example sentences for targeted words in a dictionary play an important role to help readers understand the usage of words. Traditionally, example sentences in a dictionary are usually created by linguistics experts, which are labor-intensive and knowledge-intensive. In this paper, we introduce the problem of dictionary example sentence generation, aiming to automatically generate dictionary example sentences for targeted words according to the corresponding definitions. This task is challenging especially for polysemous words, because the generated sentences need to reflect different usages and meanings of these targeted words. Targeted readers may also have different backgrounds and educational levels. It is essential to generate example sentences that can be understandable for different backgrounds and levels of audiences. To solve these problems, we propose a controllable target-word-aware model for this task. Our proposed model can generate reasonable examples for targeted words, even for polysemous words. In addition, our model allows users to provide explicit control over attributes related to readability, such as length and lexical complexity, thus generating suitable examples for targeted audiences. Automatic and human evaluations on the Oxford dictionary dataset show that our model can generate suitable examples for targeted words with specific definitions while meeting the desired readability.",
        "author": "Xingwei He; Siu Ming Yiu",
        "authorids": "/x/xingwei-he/; /s/siu-ming-yiu/",
        "bibtex": "@inproceedings{he-yiu-2022-controllable,\n    title = \"Controllable Dictionary Example Generation: Generating Example Sentences for Specific Targeted Audiences\",\n    author = \"He, Xingwei  and\n      Yiu, Siu Ming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.46/\",\n    doi = \"10.18653/v1/2022.acl-long.46\",\n    pages = \"610--627\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.46.pdf",
        "site": "https://aclanthology.org/2022.acl-long.46/",
        "pdf_size": 11994542,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6068950916174146053&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China",
        "aff_domain": "gmail.com;cs.hku.hk",
        "email": "gmail.com;cs.hku.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Hong Kong",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.hku.hk",
        "aff_unique_abbr": "HKU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.229",
        "title": "Controllable Natural Language Generation with Contrastive Prefixes",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
        "author": "Jing Qian; Li Dong; Yelong Shen; Furu Wei; Weizhu Chen",
        "authorids": "/j/jing-qian/; /l/li-dong/; /y/yelong-shen/; /f/furu-wei/; /w/weizhu-chen/",
        "bibtex": "@inproceedings{qian-etal-2022-controllable,\n    title = \"Controllable Natural Language Generation with Contrastive Prefixes\",\n    author = \"Qian, Jing  and\n      Dong, Li  and\n      Shen, Yelong  and\n      Wei, Furu  and\n      Chen, Weizhu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.229/\",\n    doi = \"10.18653/v1/2022.findings-acl.229\",\n    pages = \"2912--2924\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.229.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.229/",
        "pdf_size": 1667035,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16062799638890457220&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Santa Barbara; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation",
        "aff_domain": "cs.ucsb.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "cs.ucsb.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of California, Santa Barbara;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://www.ucsb.edu;https://www.microsoft.com",
        "aff_unique_abbr": "UCSB;Microsoft",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Santa Barbara;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.10",
        "title": "Controlled Text Generation Using Dictionary Prior in Variational Autoencoders",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While variational autoencoders (VAEs) have been widely applied in text generation tasks, they are troubled by two challenges: insufficient representation capacity and poor controllability. The former results from the posterior collapse and restrictive assumption, which impede better representation learning. The latter arises as continuous latent variables in traditional formulations hinder VAEs from interpretability and controllability. In this paper, we propose Dictionary Prior (DPrior), a new data-driven prior that enjoys the merits of expressivity and controllability. To facilitate controlled text generation with DPrior, we propose to employ contrastive learning to separate the latent space into several parts. Extensive experiments on both language modeling and controlled text generation demonstrate the effectiveness of the proposed approach.",
        "author": "Xianghong Fang; Jian Li; Lifeng Shang; Xin Jiang; Qun Liu; Dit-Yan Yeung",
        "authorids": "/x/xianghong-fang/; /j/jian-li/; /l/lifeng-shang/; /x/xin-jiang/; /q/qun-liu/; /d/dit-yan-yeung/",
        "bibtex": "@inproceedings{fang-etal-2022-controlled,\n    title = \"Controlled Text Generation Using Dictionary Prior in Variational Autoencoders\",\n    author = \"Fang, Xianghong  and\n      Li, Jian  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Yeung, Dit-Yan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.10/\",\n    doi = \"10.18653/v1/2022.findings-acl.10\",\n    pages = \"97--111\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.10.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.10/",
        "pdf_size": 566250,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3626815947299609101&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The Hong Kong University of Science and Technology; Huawei Noah\u2019s Ark Lab + The Hong Kong University of Science and Technology; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; The Hong Kong University of Science and Technology",
        "aff_domain": "connect.ust.hk;huawei.com;huawei.com;huawei.com;huawei.com;cse.ust.hk",
        "email": "connect.ust.hk;huawei.com;huawei.com;huawei.com;huawei.com;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1+0;1;1;1;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Huawei",
        "aff_unique_dep": ";Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.ust.hk;https://www.huawei.com",
        "aff_unique_abbr": "HKUST;Huawei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.260",
        "title": "Controlling the Focus of Pretrained Language Generation Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model\u2019s focus. This work aims to develop a control mechanism by which a user can select spans of context as \u201chighlights\u201d for the model to focus on, and generate relevant output. To achieve this goal, we augment a pretrained model with trainable \u201cfocus vectors\u201d that are directly applied to the model\u2019s embeddings, while the model itself is kept fixed. These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. We also collect evaluation data where the highlight-generation pairs are annotated by humans. Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.",
        "author": "Jiabao Ji; Yoon Kim; James Glass; Tianxing He",
        "authorids": "/j/jiabao-ji/; /y/yoon-kim/; /j/james-glass/; /t/tianxing-he/",
        "bibtex": "@inproceedings{ji-etal-2022-controlling,\n    title = \"Controlling the Focus of Pretrained Language Generation Models\",\n    author = \"Ji, Jiabao  and\n      Kim, Yoon  and\n      Glass, James  and\n      He, Tianxing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.260/\",\n    doi = \"10.18653/v1/2022.findings-acl.260\",\n    pages = \"3291--3306\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.260.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.260/",
        "pdf_size": 1815378,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1271491182025063801&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Shanghai Jiao Tong University; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "gmail.com;mit.edu;mit.edu;mit.edu",
        "email": "gmail.com;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://web.mit.edu",
        "aff_unique_abbr": "SJTU;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-short.14",
        "title": "Counterfactual Explanations for Natural Language Interfaces",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "A key challenge facing natural language interfaces is enabling users to understand the capabilities of the underlying system. We propose a novel approach for generating explanations of a natural language interface based on semantic parsing. We focus on counterfactual explanations, which are post-hoc explanations that describe to the user how they could have minimally modified their utterance to achieve their desired goal. In particular, the user provides an utterance along with a demonstration of their desired goal; then, our algorithm synthesizes a paraphrase of their utterance that is guaranteed to achieve their goal. In two user studies, we demonstrate that our approach substantially improves user performance, and that it generates explanations that more closely match the user\u2019s intent compared to two ablations.",
        "author": "George Tolkachev; Stephen Mell; Stephan Zdancewic; Osbert Bastani",
        "authorids": "/g/george-tolkachev/; /s/stephen-mell/; /s/stephan-zdancewic/; /o/osbert-bastani/",
        "bibtex": "@inproceedings{tolkachev-etal-2022-counterfactual,\n    title = \"Counterfactual Explanations for Natural Language Interfaces\",\n    author = \"Tolkachev, George  and\n      Mell, Stephen  and\n      Zdancewic, Stephan  and\n      Bastani, Osbert\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.14/\",\n    doi = \"10.18653/v1/2022.acl-short.14\",\n    pages = \"113--118\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.14.pdf",
        "site": "https://aclanthology.org/2022.acl-short.14/",
        "pdf_size": 211207,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4880248308972983951&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "github": "https://github.com/georgeto20/counterfactual_explanations",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.440",
        "title": "Cree Corpus: A Collection of n\u00eahiyaw\u00eawin Resources",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Plains Cree (n\u00eahiyaw\u00eawin) is an Indigenous language that is spoken in Canada and the USA. It is the most widely spoken dialect of Cree and a morphologically complex language that is polysynthetic, highly inflective, and agglutinative. It is an extremely low resource language, with no existing corpus that is both available and prepared for supporting the development of language technologies. To support n\u00eahiyaw\u00eawin revitalization and preservation, we developed a corpus covering diverse genres, time periods, and texts for a variety of intended audiences. The data has been verified and cleaned; it is ready for use in developing language technologies for n\u00eahiyaw\u00eawin. The corpus includes the corresponding English phrases or audio files where available. We demonstrate the utility of the corpus through its community use and its use to build language technologies that can provide the types of support that community members have expressed are desirable. The corpus is available for public use.",
        "author": "Daniela Teodorescu; Josie Matalski; Delaney Lothian; Denilson Barbosa; Carrie Demmans Epp",
        "authorids": "/d/daniela-teodorescu/; /j/josie-matalski/; /d/delaney-lothian/; /d/denilson-barbosa/; /c/carrie-demmans-epp/",
        "bibtex": "@inproceedings{teodorescu-etal-2022-cree,\n    title = \"{C}ree Corpus: A Collection of n{\\^e}hiyaw{\\^e}win Resources\",\n    author = \"Teodorescu, Daniela  and\n      Matalski, Josie  and\n      Lothian, Delaney  and\n      Barbosa, Denilson  and\n      Demmans Epp, Carrie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.440/\",\n    doi = \"10.18653/v1/2022.acl-long.440\",\n    pages = \"6354--6364\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.440.pdf",
        "site": "https://aclanthology.org/2022.acl-long.440/",
        "pdf_size": 211105,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13102923649750140570&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Dept. of Computing Science, University of Alberta; Dept. of Computing Science, University of Alberta; Dept. of Computing Science, University of Alberta; Dept. of Computing Science, University of Alberta; Dept. of Computing Science, University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "https://github.com/EdTeKLA/IndigenousLanguages_Corpora",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Dept. of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.322",
        "title": "Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual pre-trained language models, such as mBERT and XLM-R, have shown impressive cross-lingual ability. Surprisingly, both of them use multilingual masked language model (MLM) without any cross-lingual supervision or aligned data. Despite the encouraging results, we still lack a clear understanding of why cross-lingual ability could emerge from multilingual MLM. In our work, we argue that cross-language ability comes from the commonality between languages. Specifically, we study three language properties: constituent order, composition and word co-occurrence. First, we create an artificial language by modifying property in source language. Then we study the contribution of modified property through the change of cross-language transfer results on target language. We conduct experiments on six languages and two cross-lingual NLP tasks (textual entailment, sentence retrieval). Our main conclusion is that the contribution of constituent order and word co-occurrence is limited, while the composition is more crucial to the success of cross-linguistic transfer.",
        "author": "Yuan Chai; Yaobo Liang; Nan Duan",
        "authorids": "/y/yuan-chai/; /y/yaobo-liang/; /n/nan-duan/",
        "bibtex": "@inproceedings{chai-etal-2022-cross,\n    title = \"Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure\",\n    author = \"Chai, Yuan  and\n      Liang, Yaobo  and\n      Duan, Nan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.322/\",\n    doi = \"10.18653/v1/2022.acl-long.322\",\n    pages = \"4702--4712\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.322.pdf",
        "site": "https://aclanthology.org/2022.acl-long.322/",
        "pdf_size": 421281,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8616843084346404406&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Beihang University; Microsoft Research Asia; Microsoft Research Asia",
        "aff_domain": "buaa.edu.cn;microsoft.com;microsoft.com",
        "email": "buaa.edu.cn;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Beihang University;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "http://www.buaa.edu.cn/;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "BUAA;MSR Asia",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.159",
        "title": "Cross-Lingual Contrastive Learning for Fine-Grained Entity Typing for Low-Resource Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fine-grained entity typing (FGET) aims to classify named entity mentions into fine-grained entity types, which is meaningful for entity-related NLP tasks. For FGET, a key challenge is the low-resource problem \u2014 the complex entity type hierarchy makes it difficult to manually label data. Especially for those languages other than English, human-labeled data is extremely scarce. In this paper, we propose a cross-lingual contrastive learning framework to learn FGET models for low-resource languages. Specifically, we use multi-lingual pre-trained language models (PLMs) as the backbone to transfer the typing knowledge from high-resource languages (such as English) to low-resource languages (such as Chinese). Furthermore, we introduce entity-pair-oriented heuristic rules as well as machine translation to obtain cross-lingual distantly-supervised data, and apply cross-lingual contrastive learning on the distantly-supervised data to enhance the backbone PLMs. Experimental results show that by applying our framework, we can easily learn effective FGET models for low-resource languages, even without any language-specific human-labeled data. Our code is also available at https://github.com/thunlp/CrossET.",
        "author": "Xu Han; Yuqi Luo; Weize Chen; Zhiyuan Liu; Maosong Sun; Zhou Botong; Hao Fei; Suncong Zheng",
        "authorids": "/x/xu-han/; /y/yuqi-luo/; /w/weize-chen/; /z/zhiyuan-liu/; /m/maosong-sun/; /z/zhou-botong/; /h/hao-fei/; /s/suncong-zheng/",
        "bibtex": "@inproceedings{han-etal-2022-cross,\n    title = \"Cross-Lingual Contrastive Learning for Fine-Grained Entity Typing for Low-Resource Languages\",\n    author = \"Han, Xu  and\n      Luo, Yuqi  and\n      Chen, Weize  and\n      Liu, Zhiyuan  and\n      Sun, Maosong  and\n      Botong, Zhou  and\n      Fei, Hao  and\n      Zheng, Suncong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.159/\",\n    doi = \"10.18653/v1/2022.acl-long.159\",\n    pages = \"2241--2250\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.159.pdf",
        "site": "https://aclanthology.org/2022.acl-long.159/",
        "pdf_size": 727513,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1776709932883021141&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University+Beijing Academy of Artificial Intelligence, BAAI; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University+Beijing Academy of Artificial Intelligence, BAAI; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University+Beijing Academy of Artificial Intelligence, BAAI; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University+Beijing Academy of Artificial Intelligence, BAAI; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University+Beijing Academy of Artificial Intelligence, BAAI; Tencent AI Platform Department, Tencent Inc; Tencent AI Platform Department, Tencent Inc; Tencent AI Platform Department, Tencent Inc",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn; ; ; ",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn; ; ; ",
        "github": "https://github.com/thunlp/CrossET",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1+0+0+2;0+1+0+0+2;0+1+0+0+2;0+1+0+0+2;0+1+0+0+2;3;3;3",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Beijing Academy of Artificial Intelligence;Tencent",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;;Tencent AI Platform Department",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.baaia.cn;https://www.tencent.com",
        "aff_unique_abbr": "THU;;BAAI;Tencent",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0+0+0;0+0+0+0+0;0+0+0+0+0;0+0+0+0+0;0+0+0+0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.288",
        "title": "Cross-Lingual Phrase Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-lingual retrieval aims to retrieve relevant text across languages. Current methods typically achieve cross-lingual retrieval by learning language-agnostic text representations in word or sentence level. However, how to learn phrase representations for cross-lingual phrase retrieval is still an open problem. In this paper, we propose , a cross-lingual phrase retriever that extracts phrase representations from unlabeled example sentences. Moreover, we create a large-scale cross-lingual phrase retrieval dataset, which contains 65K bilingual phrase pairs and 4.2M example sentences in 8 English-centric language pairs. Experimental results show that outperforms state-of-the-art baselines which utilize word-level or sentence-level representations. also shows impressive zero-shot transferability that enables the model to perform retrieval in an unseen language pair during training. Our dataset, code, and trained models are publicly available at github.com/cwszz/XPR/.",
        "author": "Heqi Zheng; Xiao Zhang; Zewen Chi; Heyan Huang; Yan Tan; Tian Lan; Wei Wei; Xian-Ling Mao",
        "authorids": "/h/heqi-zheng/; /x/xiao-zhang/; /z/zewen-chi/; /h/he-yan-huang/; /y/yan-tan/; /t/tian-lan/; /w/wei-wei/; /x/xian-ling-mao/",
        "bibtex": "@inproceedings{zheng-etal-2022-cross-lingual,\n    title = \"Cross-Lingual Phrase Retrieval\",\n    author = \"Zheng, Heqi  and\n      Zhang, Xiao  and\n      Chi, Zewen  and\n      Huang, Heyan  and\n      Tan, Yan  and\n      Lan, Tian  and\n      Wei, Wei  and\n      Mao, Xian-Ling\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.288/\",\n    doi = \"10.18653/v1/2022.acl-long.288\",\n    pages = \"4193--4204\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.288.pdf",
        "site": "https://aclanthology.org/2022.acl-long.288/",
        "pdf_size": 349508,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14259110279280717559&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications; School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications; School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications; School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; Huazhong University of Science and Technology; School of Computer Science and Technology, Beijing Institute of Technology",
        "aff_domain": "bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;hust.edu.cn;bit.edu.cn",
        "email": "bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;hust.edu.cn;bit.edu.cn",
        "github": "github.com/cwszz/XPR/",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0;0;2;0",
        "aff_unique_norm": "Beijing Institute of Technology;Beijing Engineering Research Center;Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Computer Science and Technology;High Volume Language Information Processing and Cloud Computing Applications;",
        "aff_unique_url": "http://www.bit.edu.cn/;;http://www.hust.edu.cn",
        "aff_unique_abbr": "BIT;;HUST",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.266",
        "title": "Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English. Our cross-lingual framework includes an offline unsupervised construction of a translated UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a fine-tuned pretrained transformer language model to filter candidates according to context. Our method exploits a small dataset of manually annotated UMLS mentions in the source language and uses this supervised data in two ways: to extend the unsupervised UMLS dictionary and to fine-tune the contextual filtering of candidate mentions in full documents. We demonstrate results of our approach on both Hebrew and English. We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset. We also achieve new SOTA on the English dataset MedMentions with +7.3 F1.",
        "author": "Rina Galperin; Shachar Schnapp; Michael Elhadad",
        "authorids": "/r/rina-galperin/; /s/shachar-schnapp/; /m/michael-elhadad/",
        "bibtex": "@inproceedings{galperin-etal-2022-cross,\n    title = \"Cross-Lingual {UMLS} Named Entity Linking using {UMLS} Dictionary Fine-Tuning\",\n    author = \"Galperin, Rina  and\n      Schnapp, Shachar  and\n      Elhadad, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.266/\",\n    doi = \"10.18653/v1/2022.findings-acl.266\",\n    pages = \"3380--3390\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.266.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.266/",
        "pdf_size": 796753,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9120063394277403634&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Ben-Gurion University; Ben-Gurion University; Ben-Gurion University",
        "aff_domain": "post.bgu.ac.il;post.bgu.ac.il;cs.bgu.ac.il",
        "email": "post.bgu.ac.il;post.bgu.ac.il;cs.bgu.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ben-Gurion University of the Negev",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bgu.ac.il",
        "aff_unique_abbr": "BGU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.findings-acl.54",
        "title": "Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Decoding language from non-invasive brain activity has attracted increasing attention from both researchers in neuroscience and natural language processing. Due to the noisy nature of brain recordings, existing work has simplified brain-to-word decoding as a binary classification task which is to discriminate a brain signal between its corresponding word and a wrong one. This pairwise classification task, however, cannot promote the development of practical neural decoders for two reasons. First, it has to enumerate all pairwise combinations in the test set, so it is inefficient to predict a word in a large vocabulary. Second, a perfect pairwise decoder cannot guarantee the performance on direct classification. To overcome these and go a step further to a realistic neural decoder, we propose a novel Cross-Modal Cloze (CMC) task which is to predict the target word encoded in the neural image with a context as prompt. Furthermore, to address this task, we propose a general approach that leverages the pre-trained language model to predict the target word. To validate our method, we perform experiments on more than 20 participants from two brain imaging datasets. Our method achieves 28.91% top-1 accuracy and 54.19% top-5 accuracy on average across all participants, significantly outperforming several baselines. This result indicates that our model can serve as a state-of-the-art baseline for the CMC task. More importantly, it demonstrates that it is feasible to decode a certain word within a large vocabulary from its neural brain activity.",
        "author": "Shuxian Zou; Shaonan Wang; Jiajun Zhang; Chengqing Zong",
        "authorids": "/s/shuxian-zou/; /s/shaonan-wang/; /j/jiajun-zhang/; /c/chengqing-zong/",
        "bibtex": "@inproceedings{zou-etal-2022-cross,\n    title = \"Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding\",\n    author = \"Zou, Shuxian  and\n      Wang, Shaonan  and\n      Zhang, Jiajun  and\n      Zong, Chengqing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.54/\",\n    doi = \"10.18653/v1/2022.findings-acl.54\",\n    pages = \"648--657\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.54.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.54/",
        "pdf_size": 384968,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5572869018226962138&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1+2;0+1+2",
        "aff_unique_norm": "University of Chinese Academy of Sciences;National Laboratory of Pattern Recognition;Chinese Academy of Sciences",
        "aff_unique_dep": "School of Artificial Intelligence;Institute of Automation;Center for Excellence in Brain Science and Intelligence Technology",
        "aff_unique_url": "http://www.ucas.ac.cn;;http://www.cas.cn",
        "aff_unique_abbr": "UCAS;;CAS",
        "aff_campus_unique_index": "0+0;0+0;0+0+0;0+0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.215",
        "title": "Cross-Modal Discrete Representation Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In contrast to recent advances focusing on high-level representation learning across modalities, in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words. Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities. Beyond the shared embedding space, we propose a Cross-Modal Code Matching objective that forces the representations from different views (modalities) to have a similar distribution over the discrete embedding space such that cross-modal objects/actions localization can be performed without direct supervision. We show that the proposed discretized multi-modal fine-grained representation (e.g., pixel/word/frame) can complement high-level summary representations (e.g., video/sentence/waveform) for improved performance on cross-modal retrieval tasks. We also observe that the discretized representation uses individual clusters to represent the same semantic concept across modalities.",
        "author": "Alexander Liu; SouYoung Jin; Cheng-I Lai; Andrew Rouditchenko; Aude Oliva; James Glass",
        "authorids": "/a/alex-liu/; /s/souyoung-jin/; /c/cheng-i-lai/; /a/andrew-rouditchenko/; /a/aude-oliva/; /j/james-glass/",
        "bibtex": "@inproceedings{liu-etal-2022-cross,\n    title = \"Cross-Modal Discrete Representation Learning\",\n    author = \"Liu, Alexander  and\n      Jin, SouYoung  and\n      Lai, Cheng-I  and\n      Rouditchenko, Andrew  and\n      Oliva, Aude  and\n      Glass, James\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.215/\",\n    doi = \"10.18653/v1/2022.acl-long.215\",\n    pages = \"3013--3035\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.215.pdf",
        "site": "https://aclanthology.org/2022.acl-long.215/",
        "pdf_size": 1394301,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13493013334063819589&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.244",
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.",
        "author": "Swaroop Mishra; Daniel Khashabi; Chitta Baral; Hannaneh Hajishirzi",
        "authorids": "/s/swaroop-mishra/; /d/daniel-khashabi/; /c/chitta-baral/; /h/hannaneh-hajishirzi/",
        "bibtex": "@inproceedings{mishra-etal-2022-cross,\n    title = \"Cross-Task Generalization via Natural Language Crowdsourcing Instructions\",\n    author = \"Mishra, Swaroop  and\n      Khashabi, Daniel  and\n      Baral, Chitta  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.244/\",\n    doi = \"10.18653/v1/2022.acl-long.244\",\n    pages = \"3470--3487\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.244.pdf",
        "site": "https://aclanthology.org/2022.acl-long.244/",
        "pdf_size": 1310977,
        "gs_citation": 697,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3782574285623732185&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Arizona State University; Allen Institute for AI+University of Washington; Arizona State University; Allen Institute for AI+University of Washington",
        "aff_domain": "asu.edu;allenai.org;asu.edu;cs.washington.edu",
        "email": "asu.edu;allenai.org;asu.edu;cs.washington.edu",
        "github": "",
        "project": "https://instructions.apps.allenai.org",
        "author_num": 4,
        "aff_unique_index": "0;1+2;0;1+2",
        "aff_unique_norm": "Arizona State University;Allen Institute for AI;University of Washington",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.asu.edu;https://allenai.org;https://www.washington.edu",
        "aff_unique_abbr": "ASU;AI2;UW",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.30",
        "title": "Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modelling prosody variation is critical for synthesizing natural and expressive speech in end-to-end text-to-speech (TTS) systems. In this paper, a cross-utterance conditional VAE (CUC-VAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and text features obtained from both past and future sentences. At inference time, instead of the standard Gaussian distribution used by VAE, CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information, which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody. The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness, intelligibility and quantitative measurements, including word error rates and the standard deviation of prosody attributes. Experimental results on LJ-Speech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins.",
        "author": "Yang Li; Cheng Yu; Guangzhi Sun; Hua Jiang; Fanglei Sun; Weiqin Zu; Ying Wen; Yang Yang; Jun Wang",
        "authorids": "/y/yang-li/; /c/cheng-yu/; /g/guangzhi-sun/; /h/hua-jiang/; /f/fanglei-sun/; /w/weiqin-zu/; /y/ying-wen/; /y/yang-yang/; /j/jun-wang/",
        "bibtex": "@inproceedings{li-etal-2022-cross-utterance,\n    title = \"Cross-Utterance Conditioned {VAE} for Non-Autoregressive Text-to-Speech\",\n    author = \"Li, Yang  and\n      Yu, Cheng  and\n      Sun, Guangzhi  and\n      Jiang, Hua  and\n      Sun, Fanglei  and\n      Zu, Weiqin  and\n      Wen, Ying  and\n      Yang, Yang  and\n      Wang, Jun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.30/\",\n    doi = \"10.18653/v1/2022.acl-long.30\",\n    pages = \"391--400\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.30.pdf",
        "site": "https://aclanthology.org/2022.acl-long.30/",
        "pdf_size": 1405067,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2755292400646532391&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "ShanghaiTech University; ShanghaiTech University; Cambridge University; Neurowave Ai Limited; ShanghaiTech University; ShanghaiTech University; Shanghai Jiao Tong University; ShanghaiTech University; University College London",
        "aff_domain": "cam.ac.uk;shanghaitech.edu.cn; ; ; ; ; ; ;",
        "email": "cam.ac.uk;shanghaitech.edu.cn; ; ; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;1;2;0;0;3;0;4",
        "aff_unique_norm": "ShanghaiTech University;University of Cambridge;Neurowave Ai Limited;Shanghai Jiao Tong University;University College London",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.shanghaitech.edu.cn;https://www.cam.ac.uk;;https://www.sjtu.edu.cn;https://www.ucl.ac.uk",
        "aff_unique_abbr": "ShanghaiTech;Cambridge;;SJTU;UCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1;1;0;0;0;0;1",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2022.findings-acl.210",
        "title": "Cross-domain Named Entity Recognition via Graph Matching",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Cross-domain NER is a practical yet challenging problem since the data scarcity in the real-world scenario. A common practice is first to learn a NER model in a rich-resource general domain and then adapt the model to specific domains. Due to the mismatch problem between entity types across domains, the wide knowledge in the general domain can not effectively transfer to the target domain NER model. To this end, we model the label relationship as a probability distribution and construct label graphs in both source and target label spaces. To enhance the contextual representation with label structures, we fuse the label graph into the word embedding output by BERT. By representing label relationships as graphs, we formulate cross-domain NER as a graph matching problem. Furthermore, the proposed method has good applicability with pre-training methods and is potentially capable of other cross-domain prediction tasks. Empirical results on four datasets show that our method outperforms a series of transfer learning, multi-task learning, and few-shot learning methods.",
        "author": "Junhao Zheng; Haibin Chen; Qianli Ma",
        "authorids": "/j/junhao-zheng/; /h/haibin-chen/; /q/qianli-ma/",
        "bibtex": "@inproceedings{zheng-etal-2022-cross,\n    title = \"Cross-domain Named Entity Recognition via Graph Matching\",\n    author = \"Zheng, Junhao  and\n      Chen, Haibin  and\n      Ma, Qianli\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.210/\",\n    doi = \"10.18653/v1/2022.findings-acl.210\",\n    pages = \"2670--2680\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.210.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.210/",
        "pdf_size": 1643935,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3943916489652406417&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China",
        "aff_domain": "outlook.com;foxmail.com;scut.edu.cn",
        "email": "outlook.com;foxmail.com;scut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "South China University of Technology",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.scut.edu.cn",
        "aff_unique_abbr": "SCUT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Guangzhou",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.96",
        "title": "Cross-lingual Inference with A Chinese Entailment Graph",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triples. In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the first Chinese fine-grained entity typing dataset under the FIGER type ontology. Through experiments on the Levy-Holt dataset, we verify the strength of our Chinese entailment graph, and reveal the cross-lingual complementarity: on the parallel Levy-Holt dataset, an ensemble of Chinese and English entailment graphs outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC points.",
        "author": "Tianyi Li; Sabine Weber; Mohammad Javad Hosseini; Liane Guillou; Mark Steedman",
        "authorids": "/t/tianyi-li/; /s/sabine-weber/; /m/mohammad-javad-hosseini/; /l/liane-guillou/; /m/mark-steedman/",
        "bibtex": "@inproceedings{li-etal-2022-cross,\n    title = \"Cross-lingual Inference with A {C}hinese Entailment Graph\",\n    author = \"Li, Tianyi  and\n      Weber, Sabine  and\n      Hosseini, Mohammad Javad  and\n      Guillou, Liane  and\n      Steedman, Mark\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.96/\",\n    doi = \"10.18653/v1/2022.findings-acl.96\",\n    pages = \"1214--1233\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.96.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.96/",
        "pdf_size": 1086808,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5942202975390418138&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh + Google Research; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;sms.ed.ac.uk;ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;sms.ed.ac.uk;ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0+1;0;0",
        "aff_unique_norm": "University of Edinburgh;Google",
        "aff_unique_dep": "School of Informatics;Google Research",
        "aff_unique_url": "https://www.ed.ac.uk;https://research.google",
        "aff_unique_abbr": "Edinburgh;Google Research",
        "aff_campus_unique_index": "0;0;0+1;0;0",
        "aff_campus_unique": "Edinburgh;Mountain View",
        "aff_country_unique_index": "0;0;0+1;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2022.findings-acl.319",
        "title": "CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Task-oriented personal assistants enable people to interact with a host of devices and services using natural language. One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages. Zero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as English with the aim of transferring it to the low-resource language(s). To this end, we introduce CrossAligner, the principal method of a variety of effective approaches for zero-shot cross-lingual transfer based on learning alignment from unlabelled parallel data. We present a quantitative analysis of individual methods as well as their weighted combinations, several of which exceed state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test sets and three benchmark multilingual datasets. A detailed qualitative error analysis of the best methods shows that our fine-tuned language models can zero-shot transfer the task knowledge better than anticipated.",
        "author": "Milan Gritta; Ruoyu Hu; Ignacio Iacobacci",
        "authorids": "/m/milan-gritta/; /r/ruoyu-hu/; /i/ignacio-iacobacci/",
        "bibtex": "@inproceedings{gritta-etal-2022-crossaligner,\n    title = \"{C}ross{A}ligner {\\&} Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding\",\n    author = \"Gritta, Milan  and\n      Hu, Ruoyu  and\n      Iacobacci, Ignacio\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.319/\",\n    doi = \"10.18653/v1/2022.findings-acl.319\",\n    pages = \"4048--4061\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.319.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.319/",
        "pdf_size": 406445,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12954012732340504254&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Huawei Noah\u2019s Ark Lab, London, UK; Imperial College London, UK; Huawei Noah\u2019s Ark Lab, London, UK",
        "aff_domain": "huawei.com;imperial.ac.uk;huawei.com",
        "email": "huawei.com;imperial.ac.uk;huawei.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Huawei;Imperial College London",
        "aff_unique_dep": "Huawei Noah\u2019s Ark Lab;",
        "aff_unique_url": "https://www.huawei.com/en/ai;https://www.imperial.ac.uk",
        "aff_unique_abbr": "HNA Lab;ICL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.222",
        "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.",
        "author": "Robert Logan IV; Ivana Balazevic; Eric Wallace; Fabio Petroni; Sameer Singh; Sebastian Riedel",
        "authorids": "/r/robert-logan-iv/; /i/ivana-balazevic/; /e/eric-wallace/; /f/fabio-petroni/; /s/sameer-singh/; /s/sebastian-riedel/",
        "bibtex": "@inproceedings{logan-iv-etal-2022-cutting,\n    title = \"Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models\",\n    author = \"Logan IV, Robert  and\n      Balazevic, Ivana  and\n      Wallace, Eric  and\n      Petroni, Fabio  and\n      Singh, Sameer  and\n      Riedel, Sebastian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.222/\",\n    doi = \"10.18653/v1/2022.findings-acl.222\",\n    pages = \"2824--2835\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.222.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.222/",
        "pdf_size": 664991,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2084898900539732268&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "UC Irvine; DeepMind; UC Berkeley; Facebook AI Research; UC Irvine; Facebook AI Research + University College London",
        "aff_domain": "uci.edu;deepmind.com;berkeley.edu;fb.com;uci.edu;fb.com",
        "email": "uci.edu;deepmind.com;berkeley.edu;fb.com;uci.edu;fb.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;0;3+4",
        "aff_unique_norm": "University of California, Irvine;DeepMind;University of California, Berkeley;Meta;University College London",
        "aff_unique_dep": ";;;Facebook AI Research;",
        "aff_unique_url": "https://www.uci.edu;https://deepmind.com;https://www.berkeley.edu;https://research.facebook.com;https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCI;DeepMind;UC Berkeley;FAIR;UCL",
        "aff_campus_unique_index": "0;2;0;",
        "aff_campus_unique": "Irvine;;Berkeley",
        "aff_country_unique_index": "0;1;0;0;0;0+1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2022.findings-acl.286",
        "title": "DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The task of joint dialog sentiment classification (DSC) and act recognition (DAR) aims to simultaneously predict the sentiment label and act label for each utterance in a dialog. In this paper, we put forward a new framework which models the explicit dependencies via integrating prediction-level interactions other than semantics-level interactions, more consistent with human intuition.Besides, we propose a speaker-aware temporal graph (SATG) and a dual-task relational temporal graph (DRTG) to introduce temporal relations into dialog understanding and dual-task reasoning. To implement our framework, we propose a novel model dubbed DARER, which first generates the context-, speaker- and temporal-sensitive utterance representations via modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG, in which process the estimated label distributions act as key clues in prediction-level interactions.Experiment results show that DARER outperforms existing models by large margins while requiring much less computation resource and costing less training time.Remarkably, on DSC task in Mastodon, DARER gains a relative improvement of about 25% over previous best model in terms of F1, with less than 50% parameters and about only 60% required GPU memory.",
        "author": "Bowen Xing; Ivor Tsang",
        "authorids": "/b/bowen-xing/; /i/ivor-tsang/",
        "bibtex": "@inproceedings{xing-tsang-2022-darer,\n    title = \"{DARER}: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition\",\n    author = \"Xing, Bowen  and\n      Tsang, Ivor\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.286/\",\n    doi = \"10.18653/v1/2022.findings-acl.286\",\n    pages = \"3611--3621\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.286.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.286/",
        "pdf_size": 931619,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13019990602768772830&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Australian Artificial Intelligence Institute, University of Technology Sydney, Australia; Centre for Frontier Artificial Intelligence Research, A*STAR, Singapore",
        "aff_domain": "gmail.com;ihpc.a-star.edu.sg",
        "email": "gmail.com;ihpc.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Technology Sydney;A*STAR",
        "aff_unique_dep": "Australian Artificial Intelligence Institute;Centre for Frontier Artificial Intelligence Research",
        "aff_unique_url": "https://www.uts.edu.au;https://www.a-star.edu.sg",
        "aff_unique_abbr": "UTS;A*STAR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Sydney;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Australia;Singapore"
    },
    {
        "id": "2022.acl-long.57",
        "title": "DEAM: Dialogue Coherence Evaluation using AMR-based Semantic Manipulations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automatic evaluation metrics are essential for the rapid development of open-domain dialogue systems as they facilitate hyper-parameter tuning and comparison between models. Although recently proposed trainable conversation-level metrics have shown encouraging results, the quality of the metrics is strongly dependent on the quality of training data. Prior works mainly resort to heuristic text-level manipulations (e.g. utterances shuffling) to bootstrap incoherent conversations (negative examples) from coherent dialogues (positive examples). Such approaches are insufficient to appropriately reflect the incoherence that occurs in interactions between advanced dialogue models and humans. To tackle this problem, we propose DEAM, a Dialogue coherence Evaluation metric that relies on Abstract Meaning Representation (AMR) to apply semantic-level Manipulations for incoherent (negative) data generation. AMRs naturally facilitate the injection of various types of incoherence sources, such as coreference inconsistency, irrelevancy, contradictions, and decrease engagement, at the semantic level, thus resulting in more natural incoherent samples. Our experiments show that DEAM achieves higher correlations with human judgments compared to baseline methods on several dialog datasets by significant margins. We also show that DEAM can distinguish between coherent and incoherent dialogues generated by baseline manipulations, whereas those baseline models cannot detect incoherent examples generated by DEAM. Our results demonstrate the potential of AMR-based semantic manipulations for natural negative example generation.",
        "author": "Sarik Ghazarian; Nuan Wen; Aram Galstyan; Nanyun Peng",
        "authorids": "/s/sarik-ghazarian/; /n/nuan-wen/; /a/aram-galstyan/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{ghazarian-etal-2022-deam,\n    title = \"{DEAM}: Dialogue Coherence Evaluation using {AMR}-based Semantic Manipulations\",\n    author = \"Ghazarian, Sarik  and\n      Wen, Nuan  and\n      Galstyan, Aram  and\n      Peng, Nanyun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.57/\",\n    doi = \"10.18653/v1/2022.acl-long.57\",\n    pages = \"771--785\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.57.pdf",
        "site": "https://aclanthology.org/2022.acl-long.57/",
        "pdf_size": 1650262,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=996009501858730472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Southern California / Information Sciences Institute; University of Southern California / Information Sciences Institute; University of Southern California / Information Sciences Institute; University of Southern California / Information Sciences Institute + Computer Science Department of University of California, Los Angeles",
        "aff_domain": "isi.edu;isi.edu;isi.edu;cs.ucla.edu",
        "email": "isi.edu;isi.edu;isi.edu;cs.ucla.edu",
        "github": "https://github.com/PlusLabNLP/DEAM",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "University of Southern California;University of California, Los Angeles",
        "aff_unique_dep": "Information Sciences Institute;Computer Science Department",
        "aff_unique_url": "https://www.usc.edu;https://www.ucla.edu",
        "aff_unique_abbr": "USC;UCLA",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.123",
        "title": "DEEP: DEnoising Entity Pre-training for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus. Earlier named entity translation methods mainly focus on phonetic transliteration, which ignores the sentence context for translation and is limited in domain and language coverage. To address this limitation, we propose DEEP, a DEnoising Entity Pre-training method that leverages large amounts of monolingual data and a knowledge base to improve named entity translation accuracy within sentences. Besides, we investigate a multi-task learning strategy that finetunes a pre-trained neural machine translation model on both entity-augmented monolingual data and parallel data to further improve entity translation. Experimental results on three language pairs demonstrate that DEEP results in significant improvements over strong denoising auto-encoding baselines, with a gain of up to 1.3 BLEU and up to 9.2 entity accuracy points for English-Russian translation.",
        "author": "Junjie Hu; Hiroaki Hayashi; Kyunghyun Cho; Graham Neubig",
        "authorids": "/j/junjie-hu/; /h/hiroaki-hayashi/; /k/kyunghyun-cho/; /g/graham-neubig/",
        "bibtex": "@inproceedings{hu-etal-2022-deep,\n    title = \"{DEEP}: {DE}noising Entity Pre-training for Neural Machine Translation\",\n    author = \"Hu, Junjie  and\n      Hayashi, Hiroaki  and\n      Cho, Kyunghyun  and\n      Neubig, Graham\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.123/\",\n    doi = \"10.18653/v1/2022.acl-long.123\",\n    pages = \"1753--1766\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.123.pdf",
        "site": "https://aclanthology.org/2022.acl-long.123/",
        "pdf_size": 491824,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15488856686370122761&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 6,
        "aff": "University of Wisconsin-Madison; Carnegie Mellon University; New York University; Carnegie Mellon University",
        "aff_domain": "wisc.edu;cs.cmu.edu;nyu.edu;cs.cmu.edu",
        "email": "wisc.edu;cs.cmu.edu;nyu.edu;cs.cmu.edu",
        "github": "https://github.com/JunjieHu/deep",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "University of Wisconsin-Madison;Carnegie Mellon University;New York University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.wisc.edu;https://www.cmu.edu;https://www.nyu.edu",
        "aff_unique_abbr": "UW-Madison;CMU;NYU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.67",
        "title": "DMix: Adaptive Distance-aware Interpolative Mixup",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities. We extend Mixup and propose DMix, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space. DMix leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations. We probe the effectiveness of DMix in conjunction with various similarity measures and qualitatively analyze the different components.DMix being generalizable, can be applied to various tasks, models and modalities.",
        "author": "Ramit Sawhney; Megh Thakkar; Shrey Pandit; Ritesh Soun; Di Jin; Diyi Yang; Lucie Flek",
        "authorids": "/r/ramit-sawhney/; /m/megh-thakkar/; /s/shrey-pandit/; /r/ritesh-soun/; /d/di-jin/; /d/diyi-yang/; /l/lucie-flek/",
        "bibtex": "@inproceedings{sawhney-etal-2022-dmix,\n    title = \"{DM}ix: Adaptive Distance-aware Interpolative Mixup\",\n    author = \"Sawhney, Ramit  and\n      Thakkar, Megh  and\n      Pandit, Shrey  and\n      Soun, Ritesh  and\n      Jin, Di  and\n      Yang, Diyi  and\n      Flek, Lucie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.67/\",\n    doi = \"10.18653/v1/2022.acl-short.67\",\n    pages = \"606--612\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.67.pdf",
        "site": "https://aclanthology.org/2022.acl-short.67/",
        "pdf_size": 644503,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16219460045559747046&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Conversational AI and Social Analytics (CAISA) Lab, University of Marburg; BITS, Pilani; BITS, Pilani; Sri Venkateswara College, DU; Amazon Alexa AI; Georgia Institute of Technology; Conversational AI and Social Analytics (CAISA) Lab, University of Marburg",
        "aff_domain": "mathematik.uni-marburg.de; ; ; ; ; ;uni-marburg.de",
        "email": "mathematik.uni-marburg.de; ; ; ; ; ;uni-marburg.de",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;2;3;4;0",
        "aff_unique_norm": "University of Marburg;Birla Institute of Technology and Science;Sri Venkateswara College;Amazon;Georgia Institute of Technology",
        "aff_unique_dep": "Conversational AI and Social Analytics (CAISA) Lab;;;Amazon Alexa AI;",
        "aff_unique_url": "https://www.uni-marburg.de;https://www.bits-pilani.ac.in;http://www.svc.ac.in;https://www.amazon.com;https://www.gatech.edu",
        "aff_unique_abbr": ";BITS Pilani;SV College;Amazon;Georgia Tech",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pilani",
        "aff_country_unique_index": "0;1;1;1;2;2;0",
        "aff_country_unique": "Germany;India;United States"
    },
    {
        "id": "2022.acl-short.22",
        "title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.",
        "author": "Zheng Li; Zijian Wang; Ming Tan; Ramesh Nallapati; Parminder Bhatia; Andrew Arnold; Bing Xiang; Dan Roth",
        "authorids": "/z/zheng-li/; /z/zijian-wang/; /m/ming-tan/; /r/ramesh-nallapati/; /p/parminder-bhatia/; /a/andrew-arnold/; /b/bing-xiang/; /d/dan-roth/",
        "bibtex": "@inproceedings{li-etal-2022-dq,\n    title = \"{DQ}-{BART}: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization\",\n    author = \"Li, Zheng  and\n      Wang, Zijian  and\n      Tan, Ming  and\n      Nallapati, Ramesh  and\n      Bhatia, Parminder  and\n      Arnold, Andrew  and\n      Xiang, Bing  and\n      Roth, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.22/\",\n    doi = \"10.18653/v1/2022.acl-short.22\",\n    pages = \"203--211\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.22.pdf",
        "site": "https://aclanthology.org/2022.acl-short.22/",
        "pdf_size": 334418,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11941009068230875070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Cornell University+AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs+University of Pennsylvania",
        "aff_domain": "cornell.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "cornell.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;1;1;1;1;1;1+2",
        "aff_unique_norm": "Cornell University;Amazon;University of Pennsylvania",
        "aff_unique_dep": ";AWS AI Labs;",
        "aff_unique_url": "https://www.cornell.edu;https://aws.amazon.com;https://www.upenn.edu",
        "aff_unique_abbr": "Cornell;AWS;UPenn",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.72",
        "title": "DS-TOD: Efficient Domain Specialization for Task-Oriented Dialog",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains. In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD. Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit \u2013 resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. We further propose a resource-efficient and modular domain specialization by means of domain adapters \u2013 additional parameter-light layers in which we encode the domain knowledge. Our experiments with prominent TOD tasks \u2013 dialog state tracking (DST) and response retrieval (RR) \u2013 encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.",
        "author": "Chia-Chien Hung; Anne Lauscher; Simone Ponzetto; Goran Glava\u0161",
        "authorids": "/c/chia-chien-hung/; /a/anne-lauscher/; /s/simone-paolo-ponzetto/; /g/goran-glavas/",
        "bibtex": "@inproceedings{hung-etal-2022-ds,\n    title = \"{DS}-{TOD}: Efficient Domain Specialization for Task-Oriented Dialog\",\n    author = \"Hung, Chia-Chien  and\n      Lauscher, Anne  and\n      Ponzetto, Simone  and\n      Glava{\\v{s}}, Goran\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.72/\",\n    doi = \"10.18653/v1/2022.findings-acl.72\",\n    pages = \"891--904\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.72.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.72/",
        "pdf_size": 446044,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16696814366553734500&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Data and Web Science Group, University of Mannheim, Germany; MilaNLP, Bocconi University, Italy; Data and Web Science Group, University of Mannheim, Germany; Center for Information and Language Processing, LMU Munich, Germany",
        "aff_domain": "informatik.uni-mannheim.de;unibocconi.it;informatik.uni-mannheim.de;informatik.uni-mannheim.de",
        "email": "informatik.uni-mannheim.de;unibocconi.it;informatik.uni-mannheim.de;informatik.uni-mannheim.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Mannheim;Bocconi University;LMU Munich",
        "aff_unique_dep": "Data and Web Science Group;MilaNLP;Center for Information and Language Processing",
        "aff_unique_url": "https://www.uni-mannheim.de;https://www.bocconi.edu;https://www.lmu.de",
        "aff_unique_abbr": ";;LMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Germany;Italy"
    },
    {
        "id": "2022.findings-acl.201",
        "title": "DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Due to the limitations of the model structure and pre-training objectives, existing vision-and-language generation models cannot utilize pair-wise images and text through bi-directional generation. In this paper, we propose DU-VLG, a framework which unifies vision-and-language generation as sequence generation problems. DU-VLG is trained with novel dual pre-training tasks: multi-modal denoising autoencoder tasks and modality translation tasks. To bridge the gap between image understanding and generation, we further design a novel commitment loss. We compare pre-training objectives on image captioning and text-to-image generation datasets. Results show that DU-VLG yields better performance than variants trained with uni-directional generation objectives or the variant without the commitment loss. We also obtain higher scores compared to previous state-of-the-art systems on three vision-and-language generation tasks. In addition, human judges further confirm that our model generates real and relevant images as well as faithful and informative captions.",
        "author": "Luyang Huang; Guocheng Niu; Jiachen Liu; Xinyan Xiao; Hua Wu",
        "authorids": "/l/luyang-huang/; /g/guocheng-niu/; /j/jiachen-liu/; /x/xinyan-xiao/; /h/hua-wu/",
        "bibtex": "@inproceedings{huang-etal-2022-du,\n    title = \"{DU}-{VLG}: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training\",\n    author = \"Huang, Luyang  and\n      Niu, Guocheng  and\n      Liu, Jiachen  and\n      Xiao, Xinyan  and\n      Wu, Hua\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.201/\",\n    doi = \"10.18653/v1/2022.findings-acl.201\",\n    pages = \"2552--2566\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.201.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.201/",
        "pdf_size": 7292723,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5342028461299282560&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Baidu Inc.",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.118",
        "title": "DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer-based models have achieved state-of-the-art performance on short-input summarization. However, they still struggle with summarizing longer text. In this paper, we present DYLE, a novel dynamic latent extraction approach for abstractive long-input summarization. DYLE jointly trains an extractor and a generator and treats the extracted text snippets as the latent variable, allowing dynamic snippet-level attention weights during decoding. To provide adequate supervision, we propose simple yet effective heuristics for oracle extraction as well as a consistency loss term, which encourages the extractor to approximate the averaged dynamic weights predicted by the generator. We evaluate our method on different long-document and long-dialogue summarization tasks: GovReport, QMSum, and arXiv. Experiment results show that DYLE outperforms all existing methods on GovReport and QMSum, with gains up to 6.1 ROUGE, while yielding strong results on arXiv. Further analysis shows that the proposed dynamic weights provide interpretability of our generation process.",
        "author": "Ziming Mao; Chen Henry Wu; Ansong Ni; Yusen Zhang; Rui Zhang; Tao Yu; Budhaditya Deb; Chenguang Zhu; Ahmed Awadallah; Dragomir Radev",
        "authorids": "/z/ziming-mao/; /c/chen-henry-wu/; /a/ansong-ni/; /y/yusen-zhang/; /r/rui-zhang/; /t/tao-yu/; /b/budhaditya-deb/; /c/chenguang-zhu/; /a/ahmed-awadallah/; /d/dragomir-radev/",
        "bibtex": "@inproceedings{mao-etal-2022-dyle,\n    title = \"{DYLE}: Dynamic Latent Extraction for Abstractive Long-Input Summarization\",\n    author = \"Mao, Ziming  and\n      Wu, Chen Henry  and\n      Ni, Ansong  and\n      Zhang, Yusen  and\n      Zhang, Rui  and\n      Yu, Tao  and\n      Deb, Budhaditya  and\n      Zhu, Chenguang  and\n      Awadallah, Ahmed  and\n      Radev, Dragomir\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.118/\",\n    doi = \"10.18653/v1/2022.acl-long.118\",\n    pages = \"1687--1698\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.118.pdf",
        "site": "https://aclanthology.org/2022.acl-long.118/",
        "pdf_size": 4124215,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14221354791389462497&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 8,
        "aff": "Yale University; Carnegie Mellon University; Penn State University; The University of Hong Kong; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Yale University",
        "aff_domain": "yale.edu;cmu.edu; ; ; ; ; ; ; ; ",
        "email": "yale.edu;cmu.edu; ; ; ; ; ; ; ; ",
        "github": "https://github.com/Yale-LILY/DYLE",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;2;3;4;4;4;4;4;0",
        "aff_unique_norm": "Yale University;Carnegie Mellon University;Penn State University;University of Hong Kong;Microsoft",
        "aff_unique_dep": ";;;;Microsoft Research",
        "aff_unique_url": "https://www.yale.edu;https://www.cmu.edu;https://www.psu.edu;https://www.hku.hk;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Yale;CMU;PSU;HKU;MSR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;1;0;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.findings-acl.141",
        "title": "DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies on a pre-trained general NMT model which is adapted to the new domain on a sample of in-domain parallel data. Without parallel data, there is no way to estimate the potential benefit of DA, nor the amount of parallel samples it would require. It is however a desirable functionality that could help MT practitioners to make an informed decision before investing resources in dataset creation. We propose a Domain adaptation Learning Curve prediction (DaLC) model that predicts prospective DA performance based on in-domain monolingual samples in the source language. Our model relies on the NMT encoder representations combined with various instance and corpus-level features. We demonstrate that instance-level is better able to distinguish between different domains compared to corpus-level frameworks proposed in previous studies Finally, we perform in-depth analyses of the results highlighting the limitations of our approach, and provide directions for future research.",
        "author": "Cheonbok Park; Hantae Kim; Ioan Calapodescu; Hyun Chang Cho; Vassilina Nikoulina",
        "authorids": "/c/cheonbok-park/; /h/hantae-kim/; /i/ioan-calapodescu/; /h/hyun-chang-cho/; /v/vassilina-nikoulina/",
        "bibtex": "@inproceedings{park-etal-2022-dalc,\n    title = \"{D}a{LC}: Domain Adaptation Learning Curve Prediction for Neural Machine Translation\",\n    author = \"Park, Cheonbok  and\n      Kim, Hantae  and\n      Calapodescu, Ioan  and\n      Cho, Hyun Chang  and\n      Nikoulina, Vassilina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.141/\",\n    doi = \"10.18653/v1/2022.findings-acl.141\",\n    pages = \"1789--1807\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.141.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.141/",
        "pdf_size": 751100,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4396844760660640954&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Papago, NA VER Corp.+NA VER LABS Europe; Papago, NA VER Corp.+NA VER LABS Europe; NA VER LABS Europe; Papago, NA VER Corp.+NA VER LABS Europe; NA VER LABS Europe",
        "aff_domain": "navercorp.com;navercorp.com;naverlabs.com;navercorp.com;naverlabs.com",
        "email": "navercorp.com;navercorp.com;naverlabs.com;navercorp.com;naverlabs.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;1;0+1;1",
        "aff_unique_norm": "NA VER Corp.;NAVER LABS Europe",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.naverlabs.com/europe",
        "aff_unique_abbr": ";NAVER LABS Europe",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;1;0+1;1",
        "aff_country_unique": "South Korea;Unknown"
    },
    {
        "id": "2022.findings-acl.160",
        "title": "Data Augmentation and Learned Layer Aggregation for Improved Multilingual Language Understanding in Dialogue",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. The annotation efforts might be substantially reduced by the methods that generalise well in zero- and few-shot scenarios, and also effectively leverage external unannotated data sources (e.g., Web-scale corpora). We propose two methods to this aim, offering improved dialogue natural language understanding (NLU) across multiple languages: 1) Multi-SentAugment, and 2) LayerAgg. Multi-SentAugment is a self-training method which augments available (typically few-shot) training data with similar (automatically labelled) in-domain sentences from large monolingual Web-scale corpora. LayerAgg learns to select and combine useful semantic information scattered across different layers of a Transformer model (e.g., mBERT); it is especially suited for zero-shot scenarios as semantically richer representations should strengthen the model\u2019s cross-lingual capabilities. Applying the two methods with state-of-the-art NLU models obtains consistent improvements across two standard multilingual NLU datasets covering 16 diverse languages. The gains are observed in zero-shot, few-shot, and even in full-data scenarios. The results also suggest that the two methods achieve a synergistic effect: the best overall performance in few-shot setups is attained when the methods are used together.",
        "author": "Evgeniia Razumovskaia; Ivan Vuli\u0107; Anna Korhonen",
        "authorids": "/e/evgeniia-razumovskaia/; /i/ivan-vulic/; /a/anna-korhonen/",
        "bibtex": "@inproceedings{razumovskaia-etal-2022-data,\n    title = \"Data Augmentation and Learned Layer Aggregation for Improved Multilingual Language Understanding in Dialogue\",\n    author = \"Razumovskaia, Evgeniia  and\n      Vuli{\\'c}, Ivan  and\n      Korhonen, Anna\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.160/\",\n    doi = \"10.18653/v1/2022.findings-acl.160\",\n    pages = \"2017--2033\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.160.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.160/",
        "pdf_size": 3737523,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15642208449182763493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Language Technology Lab, University of Cambridge; Language Technology Lab, University of Cambridge; Language Technology Lab, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Language Technology Lab",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-short.18",
        "title": "Data Contamination: From Memorization to Exploitation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Pretrained language models are typically trained on massive web-based datasets, which are often \u201ccontaminated\u201d with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation. Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.",
        "author": "Inbal Magar; Roy Schwartz",
        "authorids": "/i/inbal-magar/; /r/roy-schwartz/",
        "bibtex": "@inproceedings{magar-schwartz-2022-data,\n    title = \"Data Contamination: From Memorization to Exploitation\",\n    author = \"Magar, Inbal  and\n      Schwartz, Roy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.18/\",\n    doi = \"10.18653/v1/2022.acl-short.18\",\n    pages = \"157--165\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.18.pdf",
        "site": "https://aclanthology.org/2022.acl-short.18/",
        "pdf_size": 1272515,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15309965173727091063&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel",
        "aff_domain": "mail.huji.ac.il;mail.huji.ac.il",
        "email": "mail.huji.ac.il;mail.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.acl-long.239",
        "title": "Dataset Geography: Mapping Language Data to Language Users",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As language technologies become more ubiquitous, there are increasing efforts towards expanding the language diversity and coverage of natural language processing (NLP) systems. Arguably, the most important factor influencing the quality of modern NLP systems is data availability. In this work, we study the geographical representativeness of NLP datasets, aiming to quantify if and by how much do NLP datasets match the expected needs of the language speakers. In doing so, we use entity recognition and linking systems, also making important observations about their cross-lingual consistency and giving suggestions for more robust evaluation. Last, we explore some geographical and economic factors that may explain the observed dataset distributions.",
        "author": "Fahim Faisal; Yinkai Wang; Antonios Anastasopoulos",
        "authorids": "/f/fahim-faisal/; /y/yinkai-wang/; /a/antonios-anastasopoulos/",
        "bibtex": "@inproceedings{faisal-etal-2022-dataset,\n    title = \"Dataset Geography: Mapping Language Data to Language Users\",\n    author = \"Faisal, Fahim  and\n      Wang, Yinkai  and\n      Anastasopoulos, Antonios\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.239/\",\n    doi = \"10.18653/v1/2022.acl-long.239\",\n    pages = \"3381--3411\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.239.pdf",
        "site": "https://aclanthology.org/2022.acl-long.239/",
        "pdf_size": 8166075,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=434938614511647290&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, George Mason University, USA; Department of Computer Science, George Mason University, USA; Department of Computer Science, George Mason University, USA",
        "aff_domain": "gmu.edu;gmu.edu;gmu.edu",
        "email": "gmu.edu;gmu.edu;gmu.edu",
        "github": "https://github.com/ffaisal93/dataset_geography",
        "project": "https://nlp.cs.gmu.edu/project/datasetmaps/",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "George Mason University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.gmu.edu",
        "aff_unique_abbr": "GMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.59",
        "title": "De-Bias for Generative Extraction in Unified NER Task",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Named entity recognition (NER) is a fundamental task to recognize specific types of entities from a given sentence. Depending on how the entities appear in the sentence, it can be divided into three subtasks, namely, Flat NER, Nested NER, and Discontinuous NER. Among the existing approaches, only the generative model can be uniformly adapted to these three subtasks. However, when the generative model is applied to NER, its optimization objective is not consistent with the task, which makes the model vulnerable to the incorrect biases. In this paper, we analyze the incorrect biases in the generation process from a causality perspective and attribute them to two confounders: pre-context confounder and entity-order confounder. Furthermore, we design Intra- and Inter-entity Deconfounding Data Augmentation methods to eliminate the above confounders according to the theory of backdoor adjustment. Experiments show that our method can improve the performance of the generative NER model in various datasets.",
        "author": "Shuai Zhang; Yongliang Shen; Zeqi Tan; Yiquan Wu; Weiming Lu",
        "authorids": "/s/shuai-zhang/; /y/yongliang-shen/; /z/zeqi-tan/; /y/yiquan-wu/; /w/weiming-lu/",
        "bibtex": "@inproceedings{zhang-etal-2022-de,\n    title = \"De-Bias for Generative Extraction in Unified {NER} Task\",\n    author = \"Zhang, Shuai  and\n      Shen, Yongliang  and\n      Tan, Zeqi  and\n      Wu, Yiquan  and\n      Lu, Weiming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.59/\",\n    doi = \"10.18653/v1/2022.acl-long.59\",\n    pages = \"808--818\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.59.pdf",
        "site": "https://aclanthology.org/2022.acl-long.59/",
        "pdf_size": 298779,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7911487524486621335&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University",
        "aff_domain": "zju.edu.cn; ; ; ;zju.edu.cn",
        "email": "zju.edu.cn; ; ; ;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Computer Science and Technology",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.423",
        "title": "Debiased Contrastive Learning of Unsupervised Sentence Representations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, contrastive learning has been shown to be effective in improving pre-trained language models (PLM) to derive high-quality sentence representations. It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space. However, previous works mostly adopt in-batch negatives or sample from training data at random. Such a way may cause the sampling bias that improper negatives (false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space. To address it, we present a new framework DCLR (Debiased Contrastive Learning of unsupervised sentence Representations) to alleviate the influence of these improper negatives.In DCLR, we design an instance weighting method to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space.Experiments on seven semantic textual similarity tasks show that our approach is more effective than competitive baselines. Our code and data are publicly available at the link: bluehttps://github.com/RUCAIBox/DCLR.",
        "author": "Kun Zhou; Beichen Zhang; Xin Zhao; Ji-Rong Wen",
        "authorids": "/k/kun-zhou/; /b/beichen-zhang/; /w/wayne-xin-zhao/; /j/ji-rong-wen/",
        "bibtex": "@inproceedings{zhou-etal-2022-debiased,\n    title = \"Debiased Contrastive Learning of Unsupervised Sentence Representations\",\n    author = \"Zhou, Kun  and\n      Zhang, Beichen  and\n      Zhao, Xin  and\n      Wen, Ji-Rong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.423/\",\n    doi = \"10.18653/v1/2022.acl-long.423\",\n    pages = \"6120--6130\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.423.pdf",
        "site": "https://aclanthology.org/2022.acl-long.423/",
        "pdf_size": 3616871,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15046781546458972340&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 5,
        "aff": "School of Information, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods; Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods; Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods",
        "aff_domain": "163.com;gmail.com;gmail.com;ruc.edu.cn",
        "email": "163.com;gmail.com;gmail.com;ruc.edu.cn",
        "github": "https://github.com/RUCAIBox/DCLR",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0+1;0+1",
        "aff_unique_norm": "Renmin University of China;Beijing Key Laboratory of Big Data Management and Analysis Methods",
        "aff_unique_dep": "School of Information;Big Data Management and Analysis",
        "aff_unique_url": "http://www.ruc.edu.cn;",
        "aff_unique_abbr": "RUC;",
        "aff_campus_unique_index": ";1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.65",
        "title": "Debiasing Event Understanding for Visual Commonsense Tasks",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We study event understanding as a critical step towards visual commonsense tasks. Meanwhile, we argue that current object-based event understanding is purely likelihood-based, leading to incorrect event prediction, due to biased correlation between events and objects. We propose to mitigate such biases with do-calculus, proposed in causality research, but overcoming its limited robustness, by an optimized aggregation with association-based prediction.We show the effectiveness of our approach, intrinsically by comparing our generated events with ground-truth event annotation, and extrinsically by downstream commonsense tasks.",
        "author": "Minji Seo; YeonJoon Jung; Seungtaek Choi; Seung-won Hwang; Bei Liu",
        "authorids": "/m/minji-seo/; /y/yeonjoon-jung/; /s/seungtaek-choi/; /s/seung-won-hwang/; /b/bei-liu/",
        "bibtex": "@inproceedings{seo-etal-2022-debiasing,\n    title = \"Debiasing Event Understanding for Visual Commonsense Tasks\",\n    author = \"Seo, Minji  and\n      Jung, YeonJoon  and\n      Choi, Seungtaek  and\n      Hwang, Seung-won  and\n      Liu, Bei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.65/\",\n    doi = \"10.18653/v1/2022.findings-acl.65\",\n    pages = \"782--787\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.65.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.65/",
        "pdf_size": 618171,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13388307986220057879&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Seoul National University; Yonsei University; Yonsei University; Seoul National University; Microsoft Research Asia",
        "aff_domain": "snu.ac.kr;yonsei.ac.kr;yonsei.ac.kr;snu.ac.kr;microsoft.com",
        "email": "snu.ac.kr;yonsei.ac.kr;yonsei.ac.kr;snu.ac.kr;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;2",
        "aff_unique_norm": "Seoul National University;Yonsei University;Microsoft",
        "aff_unique_dep": ";;Research",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.yonsei.ac.kr;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "SNU;Yonsei;MSR Asia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "South Korea;China"
    },
    {
        "id": "2022.acl-long.156",
        "title": "Decoding Part-of-Speech from Human EEG Signals",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This work explores techniques to predict Part-of-Speech (PoS) tags from neural signals measured at millisecond resolution with electroencephalography (EEG) during text reading. We first show that information about word length, frequency and word class is encoded by the brain at different post-stimulus latencies. We then demonstrate that pre-training on averaged EEG data and data augmentation techniques boost PoS decoding accuracy for single EEG trials. Finally, applying optimised temporally-resolved decoding techniques we show that Transformers substantially outperform linear-SVMs on PoS tagging of unigram and bigram data.",
        "author": "Alex Murphy; Bernd Bohnet; Ryan McDonald; Uta Noppeney",
        "authorids": "/a/alex-murphy/; /b/bernd-bohnet/; /r/ryan-mcdonald/; /u/uta-noppeney/",
        "bibtex": "@inproceedings{murphy-etal-2022-decoding,\n    title = \"Decoding Part-of-Speech from Human {EEG} Signals\",\n    author = \"Murphy, Alex  and\n      Bohnet, Bernd  and\n      McDonald, Ryan  and\n      Noppeney, Uta\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.156/\",\n    doi = \"10.18653/v1/2022.acl-long.156\",\n    pages = \"2201--2210\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.156.pdf",
        "site": "https://aclanthology.org/2022.acl-long.156/",
        "pdf_size": 5683166,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6441681096102663039&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Birmingham, UK; Google Research; ASAPP; Donders Institute for Brain, Cognition and Behaviour",
        "aff_domain": "gmail.com;google.com;asapp.com;donders.ru.nl",
        "email": "gmail.com;google.com;asapp.com;donders.ru.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "University of Birmingham;Google;ASAPP;Donders Institute for Brain, Cognition and Behaviour",
        "aff_unique_dep": ";Google Research;;",
        "aff_unique_url": "https://www.birmingham.ac.uk;https://research.google;https://www.asapp.com;https://www.donders.ru.nl",
        "aff_unique_abbr": "Birmingham;Google Research;ASAPP;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;1;2",
        "aff_country_unique": "United Kingdom;United States;Netherlands"
    },
    {
        "id": "2022.findings-acl.124",
        "title": "Decomposed Meta-Learning for Few-Shot Named Entity Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples. In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. In particular, we take the few-shot span detection as a sequence labeling problem and train the span detector by introducing the model-agnostic meta-learning (MAML) algorithm to find a good model parameter initialization that could fast adapt to new entity classes. For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes. Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.",
        "author": "Tingting Ma; Huiqiang Jiang; Qianhui Wu; Tiejun Zhao; Chin-Yew Lin",
        "authorids": "/t/tingting-ma/; /h/huiqiang-jiang/; /q/qianhui-wu/; /t/tiejun-zhao/; /c/chin-yew-lin/",
        "bibtex": "@inproceedings{ma-etal-2022-decomposed,\n    title = \"Decomposed Meta-Learning for Few-Shot Named Entity Recognition\",\n    author = \"Ma, Tingting  and\n      Jiang, Huiqiang  and\n      Wu, Qianhui  and\n      Zhao, Tiejun  and\n      Lin, Chin-Yew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.124/\",\n    doi = \"10.18653/v1/2022.findings-acl.124\",\n    pages = \"1584--1596\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.124.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.124/",
        "pdf_size": 1198385,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7650208198047778504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Harbin Institute of Technology; Microsoft Research Asia; Microsoft Research Asia; Harbin Institute of Technology; Microsoft Research Asia",
        "aff_domain": "gmail.com;microsoft.com;microsoft.com;hit.edu.cn;microsoft.com",
        "email": "gmail.com;microsoft.com;microsoft.com;hit.edu.cn;microsoft.com",
        "github": "https://github.com/microsoft/vert-papers/tree/master/papers/DecomposedMetaNER",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "Harbin Institute of Technology;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "HIT;MSR Asia",
        "aff_campus_unique_index": "0;1;1;0;1",
        "aff_campus_unique": "Harbin;Asia",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.577",
        "title": "Deduplicating Training Data Makes Language Models Better",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets\u2014for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.",
        "author": "Katherine Lee; Daphne Ippolito; Andrew Nystrom; Chiyuan Zhang; Douglas Eck; Chris Callison-Burch; Nicholas Carlini",
        "authorids": "/k/katherine-lee/; /d/daphne-ippolito/; /a/andrew-nystrom/; /c/chiyuan-zhang/; /d/douglas-eck/; /c/chris-callison-burch/; /n/nicholas-carlini/",
        "bibtex": "@inproceedings{lee-etal-2022-deduplicating,\n    title = \"Deduplicating Training Data Makes Language Models Better\",\n    author = \"Lee, Katherine  and\n      Ippolito, Daphne  and\n      Nystrom, Andrew  and\n      Zhang, Chiyuan  and\n      Eck, Douglas  and\n      Callison-Burch, Chris  and\n      Carlini, Nicholas\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.577/\",\n    doi = \"10.18653/v1/2022.acl-long.577\",\n    pages = \"8424--8445\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.577.pdf",
        "site": "https://aclanthology.org/2022.acl-long.577/",
        "pdf_size": 634490,
        "gs_citation": 656,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8231025444181701462&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 7,
        "aff": "Google Research, Brain Team\u2020; Google Research, Brain Team\u2020\u2021; Google Research, Brain Team\u2020; Google Research, Brain Team\u2020; Google Research, Brain Team\u2020; University of Pennsylvania\u2021; Google Research, Brain Team\u2020",
        "aff_domain": "google.com;seas.upenn.edu; ; ; ; ; ",
        "email": "google.com;seas.upenn.edu; ; ; ; ; ",
        "github": "https://github.com/goog/l.Vare-research/dedup/l.Varicate-text-datasets",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "Google;University of Pennsylvania",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.upenn.edu",
        "aff_unique_abbr": "Google;UPenn",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.343",
        "title": "Deep Inductive Logic Reasoning for Multi-Hop Reading Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-hop reading comprehension requires an ability to reason across multiple documents. On the one hand, deep learning approaches only implicitly encode query-related information into distributed embeddings which fail to uncover the discrete relational reasoning process to infer the correct answer. On the other hand, logic-based approaches provide interpretable rules to infer the target answer, but mostly work on structured data where entities and relations are well-defined. In this paper, we propose a deep-learning based inductive logic reasoning method that firstly extracts query-related (candidate-related) information, and then conducts logic reasoning among the filtered information by inducing feasible rules that entail the target relation. The reasoning process is accomplished via attentive memories with novel differentiable logic operators. To demonstrate the effectiveness of our model, we evaluate it on two reading comprehension datasets, namely WikiHop and MedHop.",
        "author": "Wenya Wang; Sinno Pan",
        "authorids": "/w/wenya-wang/; /s/sinno-pan/",
        "bibtex": "@inproceedings{wang-pan-2022-deep,\n    title = \"Deep Inductive Logic Reasoning for Multi-Hop Reading Comprehension\",\n    author = \"Wang, Wenya  and\n      Pan, Sinno\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.343/\",\n    doi = \"10.18653/v1/2022.acl-long.343\",\n    pages = \"4999--5009\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.343.pdf",
        "site": "https://aclanthology.org/2022.acl-long.343/",
        "pdf_size": 398846,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17073598347002230529&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.findings-acl.217",
        "title": "Deep Reinforcement Learning for Entity Alignment",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Embedding-based methods have attracted increasing attention in recent entity alignment (EA) studies. Although great promise they can offer, there are still several limitations. The most notable is that they identify the aligned entities based on cosine similarity, ignoring the semantics underlying the embeddings themselves. Furthermore, these methods are shortsighted, heuristically selecting the closest entity as the target and allowing multiple entities to match the same candidate. To address these limitations, we model entity alignment as a sequential decision-making task, in which an agent sequentially decides whether two entities are matched or mismatched based on their representation vectors. The proposed reinforcement learning (RL)-based entity alignment framework can be flexibly adapted to most embedding-based EA methods. The experimental results demonstrate that it consistently advances the performance of several state-of-the-art methods, with a maximum improvement of 31.1% on Hits@1.",
        "author": "Lingbing Guo; Yuqiang Han; Qiang Zhang; Huajun Chen",
        "authorids": "/l/lingbing-guo/; /y/yuqiang-han/; /q/qiang-zhang/; /h/huajun-chen/",
        "bibtex": "@inproceedings{guo-etal-2022-deep,\n    title = \"Deep Reinforcement Learning for Entity Alignment\",\n    author = \"Guo, Lingbing  and\n      Han, Yuqiang  and\n      Zhang, Qiang  and\n      Chen, Huajun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.217/\",\n    doi = \"10.18653/v1/2022.findings-acl.217\",\n    pages = \"2754--2765\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.217.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.217/",
        "pdf_size": 2744201,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10840587826388050383&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "1College of Computer Science and Technology, Zhejiang University + 2Hangzhou Innovation Center, Zhejiang University + 3Alibaba-Zhejiang University Joint Reseach Institute of Frontier Technologies; 1College of Computer Science and Technology, Zhejiang University + 2Hangzhou Innovation Center, Zhejiang University; 1College of Computer Science and Technology, Zhejiang University + 2Hangzhou Innovation Center, Zhejiang University; 1College of Computer Science and Technology, Zhejiang University + 2Hangzhou Innovation Center, Zhejiang University + 3Alibaba-Zhejiang University Joint Reseach Institute of Frontier Technologies",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+0;0+0;0+0;0+0+0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Computer Science and Technology",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Hangzhou",
        "aff_country_unique_index": "0+0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.67",
        "title": "DeepStruct: Pretraining of Language Models for Structure Prediction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.",
        "author": "Chenguang Wang; Xiao Liu; Zui Chen; Haoyun Hong; Jie Tang; Dawn Song",
        "authorids": "/c/chenguang-wang/; /x/xiao-liu/; /z/zui-chen/; /h/haoyun-hong/; /j/jie-tang/; /d/dawn-song/",
        "bibtex": "@inproceedings{wang-etal-2022-deepstruct,\n    title = \"{D}eep{S}truct: Pretraining of Language Models for Structure Prediction\",\n    author = \"Wang, Chenguang  and\n      Liu, Xiao  and\n      Chen, Zui  and\n      Hong, Haoyun  and\n      Tang, Jie  and\n      Song, Dawn\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.67/\",\n    doi = \"10.18653/v1/2022.findings-acl.67\",\n    pages = \"803--823\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.67.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.67/",
        "pdf_size": 2518385,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11389456579842227988&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "UC Berkeley+Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; UC Berkeley",
        "aff_domain": "berkeley.edu;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;berkeley.edu",
        "email": "berkeley.edu;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;berkeley.edu",
        "github": "https://github.com/cgraywang/deepstruct",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;0",
        "aff_unique_norm": "University of California, Berkeley;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "UC Berkeley;THU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0+1;1;1;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.173",
        "title": "Dependency Parsing as MRC-based Span-Span Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Higher-order methods for dependency parsing can partially but not fully address the issue that edges in dependency trees should be constructed at the text span/subtree level rather than word level. In this paper, we propose a new method for dependency parsing to address this issue. The proposed method constructs dependency trees by directly modeling span-span (in other words, subtree-subtree) relations. It consists of two modules: the text span proposal module which proposes candidate text spans, each of which represents a subtree in the dependency tree denoted by (root, start, end); and the span linking module, which constructs links between proposed spans. We use the machine reading comprehension (MRC) framework as the backbone to formalize the span linking module, where one span is used as query to extract the text span/subtree it should be linked to. The proposed method has the following merits: (1) it addresses the fundamental problem that edges in a dependency tree should be constructed between subtrees; (2) the MRC framework allows the method to retrieve missing spans in the span proposal stage, which leads to higher recall for eligible spans. Extensive experiments on the PTB, CTB and Universal Dependencies (UD) benchmarks demonstrate the effectiveness of the proposed method. The code is available at https://github.com/ShannonAI/mrc-for-dependency-parsing",
        "author": "Leilei Gan; Yuxian Meng; Kun Kuang; Xiaofei Sun; Chun Fan; Fei Wu; Jiwei Li",
        "authorids": "/l/leilei-gan/; /y/yuxian-meng/; /k/kun-kuang/; /x/xiaofei-sun/; /c/chun-fan/; /f/fei-wu/; /j/jiwei-li/",
        "bibtex": "@inproceedings{gan-etal-2022-dependency,\n    title = \"Dependency Parsing as {MRC}-based Span-Span Prediction\",\n    author = \"Gan, Leilei  and\n      Meng, Yuxian  and\n      Kuang, Kun  and\n      Sun, Xiaofei  and\n      Fan, Chun  and\n      Wu, Fei  and\n      Li, Jiwei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.173/\",\n    doi = \"10.18653/v1/2022.acl-long.173\",\n    pages = \"2427--2437\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.173.pdf",
        "site": "https://aclanthology.org/2022.acl-long.173/",
        "pdf_size": 514428,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17951030247525026018&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "College of Computer Science and Technology, Zhejiang University+Shanghai Institute for Advanced Study of Zhejiang University+Shanghai AI Laboratory; Shannon.AI+Shanghai Institute for Advanced Study of Zhejiang University+Shanghai AI Laboratory; College of Computer Science and Technology, Zhejiang University+Shanghai Institute for Advanced Study of Zhejiang University+Shanghai AI Laboratory; College of Computer Science and Technology, Zhejiang University+Shannon.AI+Shanghai Institute for Advanced Study of Zhejiang University+Shanghai AI Laboratory; Peng Cheng Laboratory+National Biomedical Imaging Center, Peking University+Computer Center, Peking University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University+Shannon.AI+Shanghai Institute for Advanced Study of Zhejiang University+Shanghai AI Laboratory",
        "aff_domain": "zju.edu.cn;shannonai.com;zju.edu.cn;shannonai.com;pku.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;shannonai.com;zju.edu.cn;shannonai.com;pku.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "https://github.com/ShannonAI/mrc-for-dependency-parsing",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+0+1;2+0+1;0+0+1;0+2+0+1;3+4+4;0;0+2+0+1",
        "aff_unique_norm": "Zhejiang University;Shanghai AI Laboratory;Shannon.AI;Pengcheng Laboratory;Peking University",
        "aff_unique_dep": "College of Computer Science and Technology;;;Peng Cheng Laboratory;National Biomedical Imaging Center",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.shanghai-ai-lab.com;https://www.shannon.ai;http://www.pcl.ac.cn;http://www.pku.edu.cn",
        "aff_unique_abbr": "ZJU;SAIL;Shannon.AI;PCL;PKU",
        "aff_campus_unique_index": "1;1;1;1;2;1",
        "aff_campus_unique": ";Shanghai;Beijing",
        "aff_country_unique_index": "0+0+0;1+0+0;0+0+0;0+1+0+0;0+0+0;0;0+1+0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.535",
        "title": "Dependency-based Mixture Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Various models have been proposed to incorporate knowledge of syntactic structures into neural language models. However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and GPT-2. In this paper, we introduce the Dependency-based Mixture Language Models. In detail, we first train neural language models with a novel dependency modeling objective to learn the probability distribution of future dependent tokens given context. We then formulate the next-token probability by mixing the previous dependency modeling probability distributions with self-attention. Extensive experiments and human evaluations show that our method can be easily and effectively applied to different neural language models while improving neural text generation on various tasks.",
        "author": "Zhixian Yang; Xiaojun Wan",
        "authorids": "/z/zhixian-yang/; /x/xiaojun-wan/",
        "bibtex": "@inproceedings{yang-wan-2022-dependency,\n    title = \"Dependency-based Mixture Language Models\",\n    author = \"Yang, Zhixian  and\n      Wan, Xiaojun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.535/\",\n    doi = \"10.18653/v1/2022.acl-long.535\",\n    pages = \"7758--7773\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.535.pdf",
        "site": "https://aclanthology.org/2022.acl-long.535/",
        "pdf_size": 462225,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16562515392911762722&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University; Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/FadedCosine/Dependency-Guided-Neural-Text-Generation",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0+0;0+0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.19",
        "title": "Detecting Annotation Errors in Morphological Data with the Transformer",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Annotation errors that stem from various sources are usually unavoidable when performing large-scale annotation of linguistic data. In this paper, we evaluate the feasibility of using the Transformer model to detect various types of annotator errors in morphological data sets that contain inflected word forms. We evaluate our error detection model on four languages by introducing three different types of artificial errors in the data: (1) typographic errors, where single characters in the data are inserted, replaced, or deleted; (2) linguistic confusion errors where two inflected forms are systematically swapped; and (3) self-adversarial errors where the Transformer model itself is used to generate plausible-looking, but erroneous forms by retrieving high-scoring predictions from the search beam. Results show that the Transformer model can with perfect, or near-perfect recall detect errors in all three scenarios, even when significant amounts of the annotated data (5%-30%) are corrupted on all languages tested. Precision varies across the languages and types of errors, but is high enough that the model can be very effectively used to flag suspicious entries in large data sets for further scrutiny by human annotators.",
        "author": "Ling Liu; Mans Hulden",
        "authorids": "/l/ling-liu/; /m/mans-hulden/",
        "bibtex": "@inproceedings{liu-hulden-2022-detecting,\n    title = \"Detecting Annotation Errors in Morphological Data with the Transformer\",\n    author = \"Liu, Ling  and\n      Hulden, Mans\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.19/\",\n    doi = \"10.18653/v1/2022.acl-short.19\",\n    pages = \"166--174\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.19.pdf",
        "site": "https://aclanthology.org/2022.acl-short.19/",
        "pdf_size": 900905,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7298319124056499219&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Colorado; University of Colorado",
        "aff_domain": "colorado.edu;colorado.edu",
        "email": "colorado.edu;colorado.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.268",
        "title": "Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This work presents a new resource for borrowing identification and analyzes the performance and errors of several models on this task. We introduce a new annotated corpus of Spanish newswire rich in unassimilated lexical borrowings\u2014words from one language that are introduced into another without orthographic adaptation\u2014and use it to evaluate how several sequence labeling models (CRF, BiLSTM-CRF, and Transformer-based models) perform. The corpus contains 370,000 tokens and is larger, more borrowing-dense, OOV-rich, and topic-varied than previous corpora available for this task. Our results show that a BiLSTM-CRF model fed with subword embeddings along with either Transformer-based embeddings pretrained on codeswitched data or a combination of contextualized word embeddings outperforms results obtained by a multilingual BERT-based model.",
        "author": "Elena \u00c1lvarez-Mellado; Constantine Lignos",
        "authorids": "/e/elena-alvarez-mellado/; /c/constantine-lignos/",
        "bibtex": "@inproceedings{alvarez-mellado-lignos-2022-detecting,\n    title = \"Detecting Unassimilated Borrowings in {S}panish: {A}n Annotated Corpus and Approaches to Modeling\",\n    author = \"{\\'A}lvarez-Mellado, Elena  and\n      Lignos, Constantine\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.268/\",\n    doi = \"10.18653/v1/2022.acl-long.268\",\n    pages = \"3868--3888\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.268.pdf",
        "site": "https://aclanthology.org/2022.acl-long.268/",
        "pdf_size": 749512,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2789477200562594006&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "NLP & IR group, School of Computer Science, UNED; Michtom School of Computer Science, Brandeis University",
        "aff_domain": "lsi.uned.es;brandeis.edu",
        "email": "lsi.uned.es;brandeis.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universidad Nacional de Educaci\u00f3n a Distancia;Brandeis University",
        "aff_unique_dep": "School of Computer Science;Michtom School of Computer Science",
        "aff_unique_url": "https://www.uned.es;https://www.brandeis.edu",
        "aff_unique_abbr": "UNED;Brandeis",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Spain;United States"
    },
    {
        "id": "2022.findings-acl.200",
        "title": "Detecting Various Types of Noise for Neural Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The filtering and/or selection of training data is one of the core aspects to be considered when building a strong machine translation system. In their influential work, Khayrallah and Koehn (2018) investigated the impact of different types of noise on the performance of machine translation systems. In the same year the WMT introduced a shared task on parallel corpus filtering, which went on to be repeated in the following years, and resulted in many different filtering approaches being proposed. In this work we aim to combine the recent achievements in data filtering with the original analysis of Khayrallah and Koehn (2018) and investigate whether state-of-the-art filtering systems are capable of removing all the suggested noise types. We observe that most of these types of noise can be detected with an accuracy of over 90% by modern filtering systems when operating in a well studied high resource setting. However, we also find that when confronted with more refined noise categories or when working with a less common language pair, the performance of the filtering systems is far from optimal, showing that there is still room for improvement in this area of research.",
        "author": "Christian Herold; Jan Rosendahl; Joris Vanvinckenroye; Hermann Ney",
        "authorids": "/c/christian-herold/; /j/jan-rosendahl/; /j/joris-vanvinckenroye/; /h/hermann-ney/",
        "bibtex": "@inproceedings{herold-etal-2022-detecting,\n    title = \"Detecting Various Types of Noise for Neural Machine Translation\",\n    author = \"Herold, Christian  and\n      Rosendahl, Jan  and\n      Vanvinckenroye, Joris  and\n      Ney, Hermann\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.200/\",\n    doi = \"10.18653/v1/2022.findings-acl.200\",\n    pages = \"2542--2551\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.200.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.200/",
        "pdf_size": 386288,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6488005842067932571&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University; Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University; Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University; Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University",
        "aff_domain": "i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de",
        "email": "i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "RWTH Aachen University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.rwth-aachen.de",
        "aff_unique_abbr": "RWTH",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Aachen",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.findings-acl.289",
        "title": "Detection of Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples. However, detecting adversarial examples may be crucial for automated tasks (e.g. review sentiment analysis) that wish to amass information about a certain population and additionally be a step towards a robust defense system. To this end, we release a dataset for four popular attack methods on four datasets and four models to encourage further research in this field. Along with it, we propose a competitive baseline based on density estimation that has the highest auc on 29 out of 30 dataset-attack-model combinations. The source code is released (https://github.com/bangawayoo/adversarial-examples-in-text-classification).",
        "author": "KiYoon Yoo; Jangho Kim; Jiho Jang; Nojun Kwak",
        "authorids": "/k/kiyoon-yoo/; /j/jangho-kim/; /j/jiho-jang/; /n/nojun-kwak/",
        "bibtex": "@inproceedings{yoo-etal-2022-detection,\n    title = \"Detection of Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation\",\n    author = \"Yoo, KiYoon  and\n      Kim, Jangho  and\n      Jang, Jiho  and\n      Kwak, Nojun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.289/\",\n    doi = \"10.18653/v1/2022.findings-acl.289\",\n    pages = \"3656--3672\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.289.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.289/",
        "pdf_size": 666725,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14533551363175073397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Seoul National University; NAVER WEBTOON AI; Seoul National University; Seoul National University",
        "aff_domain": "snu.ac.kr;webtoonscorp.com;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;webtoonscorp.com;snu.ac.kr;snu.ac.kr",
        "github": "https://github.com/bangawayoo/adversarial-examples-in-text-classification",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Seoul National University;NAVER Corporation",
        "aff_unique_dep": ";WEBTOON AI",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.naver.com",
        "aff_unique_abbr": "SNU;NAVER",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.findings-acl.156",
        "title": "Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.",
        "author": "Khalil Mrini; Shaoliang Nie; Jiatao Gu; Sinong Wang; Maziar Sanjabi; Hamed Firooz",
        "authorids": "/k/khalil-mrini/; /s/shaoliang-nie/; /j/jiatao-gu/; /s/sinong-wang/; /m/maziar-sanjabi/; /h/hamed-firooz/",
        "bibtex": "@inproceedings{mrini-etal-2022-detection,\n    title = \"Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem\",\n    author = \"Mrini, Khalil  and\n      Nie, Shaoliang  and\n      Gu, Jiatao  and\n      Wang, Sinong  and\n      Sanjabi, Maziar  and\n      Firooz, Hamed\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.156/\",\n    doi = \"10.18653/v1/2022.findings-acl.156\",\n    pages = \"1972--1983\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.156.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.156/",
        "pdf_size": 409265,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6h5AnZBmwNMJ:scholar.google.com/&scioq=Detection,+Disambiguation,+Re-ranking:+Autoregressive+Entity+Linking+as+a+Multi-Task+Problem&hl=en&as_sdt=0,44",
        "gs_version_total": 0,
        "aff": "University of California, San Diego, La Jolla, CA 92093; Meta AI, Menlo Park, CA 94025; Meta AI, Menlo Park, CA 94025; Meta AI, Menlo Park, CA 94025; Meta AI, Menlo Park, CA 94025; Meta AI, Menlo Park, CA 94025",
        "aff_domain": "ucsd.edu;fb.com;fb.com;fb.com;fb.com;fb.com",
        "email": "ucsd.edu;fb.com;fb.com;fb.com;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "University of California, San Diego;Meta",
        "aff_unique_dep": ";Meta AI",
        "aff_unique_url": "https://www.ucsd.edu;https://meta.ai",
        "aff_unique_abbr": "UCSD;Meta AI",
        "aff_campus_unique_index": "0;1;1;1;1;1",
        "aff_campus_unique": "La Jolla;Menlo Park",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.60",
        "title": "Developmental Negation Processing in Transformer Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer\u2019s ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of negation, by framing the problem as a natural language inference (NLI) task. We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them. We find that models perform consistently better only on certain categories, suggesting clear distinctions in how they are processed.",
        "author": "Antonio Laverghetta Jr.; John Licato",
        "authorids": "/a/antonio-laverghetta-jr/; /j/john-licato/",
        "bibtex": "@inproceedings{laverghetta-jr-licato-2022-developmental,\n    title = \"Developmental Negation Processing in Transformer Language Models\",\n    author = \"Laverghetta Jr., Antonio  and\n      Licato, John\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.60/\",\n    doi = \"10.18653/v1/2022.acl-short.60\",\n    pages = \"545--551\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.60.pdf",
        "site": "https://aclanthology.org/2022.acl-short.60/",
        "pdf_size": 1833447,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=807052424400986386&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Advancing Machine and Human Reasoning (AMHR) Lab, Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA; Advancing Machine and Human Reasoning (AMHR) Lab, Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA",
        "aff_domain": "usf.edu;usf.edu",
        "email": "usf.edu;usf.edu",
        "github": "https://github.com/Advancing-Machine-Human-Reasoning-Lab/negation-processing-ACL-2022",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of South Florida",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.usf.edu",
        "aff_unique_abbr": "USF",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tampa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.298",
        "title": "DiBiMT: A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Lexical ambiguity poses one of the greatest challenges in the field of Machine Translation. Over the last few decades, multiple efforts have been undertaken to investigate incorrect translations caused by the polysemous nature of words. Within this body of research, some studies have posited that models pick up semantic biases existing in the training data, thus producing translation errors. In this paper, we present DiBiMT, the first entirely manually-curated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation of nominal and verbal words in five different language combinations, namely, English and one or other of the following languages: Chinese, German, Italian, Russian and Spanish. Furthermore, we test state-of-the-art Machine Translation systems, both commercial and non-commercial ones, against our new test bed and provide a thorough statistical and linguistic analysis of the results. We release DiBiMT at https://nlp.uniroma1.it/dibimt as a closed benchmark with a public leaderboard.",
        "author": "Niccol\u00f2 Campolungo; Federico Martelli; Francesco Saina; Roberto Navigli",
        "authorids": "/n/niccolo-campolungo/; /f/federico-martelli/; /f/francesco-saina/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{campolungo-etal-2022-dibimt,\n    title = \"{D}i{B}i{MT}: A Novel Benchmark for Measuring {W}ord {S}ense {D}isambiguation Biases in {M}achine {T}ranslation\",\n    author = \"Campolungo, Niccol{\\`o}  and\n      Martelli, Federico  and\n      Saina, Francesco  and\n      Navigli, Roberto\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.298/\",\n    doi = \"10.18653/v1/2022.acl-long.298\",\n    pages = \"4331--4352\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.298.pdf",
        "site": "https://aclanthology.org/2022.acl-long.298/",
        "pdf_size": 676069,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2721216397869630710&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Sapienza University of Rome; Sapienza University of Rome; SSML Carlo Bo, Rome; Sapienza University of Rome",
        "aff_domain": "di.uniroma1.it;di.uniroma1.it;ssmlcarlobo.it;diag.uniroma1.it",
        "email": "di.uniroma1.it;di.uniroma1.it;ssmlcarlobo.it;diag.uniroma1.it",
        "github": "",
        "project": "https://nlp.uniroma1.it/dibimt",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Sapienza University of Rome;SSML Carlo Bo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uniroma1.it;",
        "aff_unique_abbr": "Sapienza;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Rome",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.acl-short.95",
        "title": "DiS-ReX: A Multilingual Dataset for Distantly Supervised Relation Extraction",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Our goal is to study the novel task of distant supervision for multilingual relation extraction (Multi DS-RE). Research in Multi DS-RE has remained limited due to the absence of a reliable benchmarking dataset. The only available dataset for this task, RELX-Distant (K\u00f6ksal and \u00d6zg\u00fcr, 2020), displays several unrealistic characteristics, leading to a systematic overestimation of model performance. To alleviate these concerns, we release a new benchmark dataset for the task, named DiS-ReX. We also modify the widely-used bag attention models using an mBERT encoder and provide the first baseline results on the proposed task. We show that DiS-ReX serves as a more challenging dataset than RELX-Distant, leaving ample room for future research in this domain.",
        "author": "Abhyuday Bhartiya; Kartikeya Badola; Mausam",
        "authorids": "/a/abhyuday-bhartiya/; /k/kartikeya-badola/; /m/mausam/",
        "bibtex": "@inproceedings{bhartiya-etal-2022-dis,\n    title = \"{D}i{S}-{R}e{X}: A Multilingual Dataset for Distantly Supervised Relation Extraction\",\n    author = \"Bhartiya, Abhyuday  and\n      Badola, Kartikeya  and\n      Mausam\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.95/\",\n    doi = \"10.18653/v1/2022.acl-short.95\",\n    pages = \"849--863\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.95.pdf",
        "site": "https://aclanthology.org/2022.acl-short.95/",
        "pdf_size": 387347,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3877395186814636067&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Indian Institute of Technology New Delhi, India; Indian Institute of Technology New Delhi, India; Indian Institute of Technology New Delhi, India",
        "aff_domain": "gmail.com;gmail.com;cse.iitd.ac.in",
        "email": "gmail.com;gmail.com;cse.iitd.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitdelhi.ac.in",
        "aff_unique_abbr": "IIT Delhi",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New Delhi",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.263",
        "title": "DialFact: A Benchmark for Fact-Checking in Dialogue",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fact-checking is an essential tool to mitigate the spread of misinformation and disinformation. We introduce the task of fact-checking in dialogue, which is a relatively unexplored area. We construct DialFact, a testing benchmark dataset of 22,245 annotated conversational claims, paired with pieces of evidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable claim detection task distinguishes whether a response carries verifiable factual information; 2) Evidence retrieval task retrieves the most relevant Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue response to be supported, refuted, or not enough information. We found that existing fact-checking models trained on non-dialogue data like FEVER fail to perform well on our task, and thus, we propose a simple yet data-efficient solution to effectively improve fact-checking performance in dialogue. We point out unique challenges in DialFact such as handling the colloquialisms, coreferences, and retrieval ambiguities in the error analysis to shed light on future research in this direction.",
        "author": "Prakhar Gupta; Chien-Sheng Wu; Wenhao Liu; Caiming Xiong",
        "authorids": "/p/prakhar-gupta/; /c/chien-sheng-wu/; /w/wenhao-liu/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{gupta-etal-2022-dialfact,\n    title = \"{D}ial{F}act: A Benchmark for Fact-Checking in Dialogue\",\n    author = \"Gupta, Prakhar  and\n      Wu, Chien-Sheng  and\n      Liu, Wenhao  and\n      Xiong, Caiming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.263/\",\n    doi = \"10.18653/v1/2022.acl-long.263\",\n    pages = \"3785--3801\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.263.pdf",
        "site": "https://aclanthology.org/2022.acl-long.263/",
        "pdf_size": 554860,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15337272571164419611&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technologies Institute, Carnegie Mellon University\u2020; Salesforce AI Research\u2021; Salesforce AI Research\u2021; Salesforce AI Research\u2021",
        "aff_domain": "cmu.edu;salesforce.com;salesforce.com;salesforce.com",
        "email": "cmu.edu;salesforce.com;salesforce.com;salesforce.com",
        "github": "https://github.com/salesforce/DialFact",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Carnegie Mellon University;Salesforce",
        "aff_unique_dep": "Language Technologies Institute;Salesforce AI Research",
        "aff_unique_url": "https://www.cmu.edu;https://www.salesforce.com",
        "aff_unique_abbr": "CMU;Salesforce AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.333",
        "title": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses. In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses. With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental results show that our model achieves the new state-of-the-art results on all these datasets.",
        "author": "Wei Chen; Yeyun Gong; Song Wang; Bolun Yao; Weizhen Qi; Zhongyu Wei; Xiaowu Hu; Bartuer Zhou; Yi Mao; Weizhu Chen; Biao Cheng; Nan Duan",
        "authorids": "/w/wei-chen/; /y/yeyun-gong/; /s/song-wang/; /b/bolun-yao/; /w/weizhen-qi/; /z/zhongyu-wei/; /x/xiaowu-hu/; /b/bartuer-zhou/; /y/yi-mao/; /w/weizhu-chen/; /b/biao-cheng/; /n/nan-duan/",
        "bibtex": "@inproceedings{chen-etal-2022-dialogved,\n    title = \"{D}ialog{VED}: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation\",\n    author = \"Chen, Wei  and\n      Gong, Yeyun  and\n      Wang, Song  and\n      Yao, Bolun  and\n      Qi, Weizhen  and\n      Wei, Zhongyu  and\n      Hu, Xiaowu  and\n      Zhou, Bartuer  and\n      Mao, Yi  and\n      Chen, Weizhu  and\n      Cheng, Biao  and\n      Duan, Nan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.333/\",\n    doi = \"10.18653/v1/2022.acl-long.333\",\n    pages = \"4852--4864\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.333.pdf",
        "site": "https://aclanthology.org/2022.acl-long.333/",
        "pdf_size": 373769,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12300159306850432590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Data Science, Fudan University, China; Microsoft Research Asia, China; Microsoft, US; Nanjing University of Science and Technology, China; Microsoft Research Asia, China; School of Data Science, Fudan University, China; Microsoft Research Asia, China; Microsoft Research Asia, China; Microsoft, US; Microsoft, US; Microsoft Research Asia, China; Microsoft Research Asia, China",
        "aff_domain": "fudan.edu.cn;microsoft.com;microsoft.com;njust.edu.cn;microsoft.com;fudan.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "fudan.edu.cn;microsoft.com;microsoft.com;njust.edu.cn;microsoft.com;fudan.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;1;1;2;1;0;1;1;1;1;1;1",
        "aff_unique_norm": "Fudan University;Microsoft;Nanjing University of Science and Technology",
        "aff_unique_dep": "School of Data Science;Microsoft Research Asia;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.microsoft.com/en-us/research/group/asia;http://www.nust.edu.cn/",
        "aff_unique_abbr": "Fudan;MSRA;NUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0;1;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.302",
        "title": "Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Annotating task-oriented dialogues is notorious for the expensive and difficult data collection process. Few-shot dialogue state tracking (DST) is a realistic solution to this problem. In this paper, we hypothesize that dialogue summaries are essentially unstructured dialogue states; hence, we propose to reformulate dialogue state tracking as a dialogue summarization problem. To elaborate, we train a text-to-text language model with synthetic template-based dialogue summaries, generated by a set of rules from the dialogue states. Then, the dialogue states can be recovered by inversely applying the summary generation rules. We empirically show that our method DS2 outperforms previous works on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and multi-domain settings. Our method also exhibits vast speedup during both training and inference as it can generate all states at once. Finally, based on our analysis, we discover that the naturalness of the summary templates plays a key role for successful training.",
        "author": "Jamin Shin; Hangyeol Yu; Hyeongdon Moon; Andrea Madotto; Juneyoung Park",
        "authorids": "/j/jamin-shin/; /h/hangyeol-yu/; /h/hyeongdon-moon/; /a/andrea-madotto/; /j/juneyoung-park/",
        "bibtex": "@inproceedings{shin-etal-2022-dialogue,\n    title = \"Dialogue Summaries as Dialogue States ({DS}2), Template-Guided Summarization for Few-shot Dialogue State Tracking\",\n    author = \"Shin, Jamin  and\n      Yu, Hangyeol  and\n      Moon, Hyeongdon  and\n      Madotto, Andrea  and\n      Park, Juneyoung\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.302/\",\n    doi = \"10.18653/v1/2022.findings-acl.302\",\n    pages = \"3824--3846\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.302.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.302/",
        "pdf_size": 689126,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3126030022059987679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Riiid AI Research; Riiid AI Research; Riiid AI Research; The Hong Kong University of Science and Technology; Riiid AI Research",
        "aff_domain": "gmail.com;riiid.co;riiid.co;connect.ust.hk;riiid.co",
        "email": "gmail.com;riiid.co;riiid.co;connect.ust.hk;riiid.co",
        "github": "github.com/jshin49/ds2",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Riiid;Hong Kong University of Science and Technology",
        "aff_unique_dep": "AI Research;",
        "aff_unique_url": "https://www.riiid.com;https://www.ust.hk",
        "aff_unique_abbr": "Riiid;HKUST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "South Korea;China"
    },
    {
        "id": "2022.findings-acl.150",
        "title": "Dict-BERT: Enhancing Language Model Pre-training with Dictionary",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora. Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. Therefore, the embeddings of rare words on the tail are usually poorly optimized. In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e.g., Wiktionary). To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence. In addition to training with the masked language modeling objective, we propose two novel self-supervised pre-training tasks on word and sentence-level alignment between input text sequence and rare word definitions to enhance language modeling representation with dictionary. We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets. Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks.",
        "author": "Wenhao Yu; Chenguang Zhu; Yuwei Fang; Donghan Yu; Shuohang Wang; Yichong Xu; Michael Zeng; Meng Jiang",
        "authorids": "/w/wenhao-yu/; /c/chenguang-zhu/; /y/yuwei-fang/; /d/donghan-yu/; /s/shuohang-wang/; /y/yichong-xu/; /m/michael-zeng/; /m/meng-jiang/",
        "bibtex": "@inproceedings{yu-etal-2022-dict,\n    title = \"Dict-{BERT}: Enhancing Language Model Pre-training with Dictionary\",\n    author = \"Yu, Wenhao  and\n      Zhu, Chenguang  and\n      Fang, Yuwei  and\n      Yu, Donghan  and\n      Wang, Shuohang  and\n      Xu, Yichong  and\n      Zeng, Michael  and\n      Jiang, Meng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.150/\",\n    doi = \"10.18653/v1/2022.findings-acl.150\",\n    pages = \"1907--1918\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.150.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.150/",
        "pdf_size": 598235,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7955035590205722186&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Notre Dame; Microsoft Cognitive Services Research; Microsoft Cognitive Services Research; Carnegie Mellon University; Microsoft Cognitive Services Research; Microsoft Cognitive Services Research; Microsoft Cognitive Services Research; University of Notre Dame",
        "aff_domain": "nd.edu;microsoft.com; ; ; ; ; ;nd.edu",
        "email": "nd.edu;microsoft.com; ; ; ; ; ;nd.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;2;1;1;1;0",
        "aff_unique_norm": "University of Notre Dame;Microsoft;Carnegie Mellon University",
        "aff_unique_dep": ";Cognitive Services Research;",
        "aff_unique_url": "https://www.nd.edu;https://www.microsoft.com;https://www.cmu.edu",
        "aff_unique_abbr": "Notre Dame;Microsoft;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.109",
        "title": "Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist\u2019s reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses. A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report. These tasks include acquisition of salient content from the report and generation of a concise, easily consumable IMPRESSIONS section. Prior research on radiology report summarization has focused on single-step end-to-end models \u2013 which subsume the task of salient content acquisition. To fully explore the cascade structure and explainability of radiology report summarization, we introduce two innovations. First, we design a two-step approach: extractive summarization followed by abstractive summarization. Second, we additionally break down the extractive part into two independent tasks: extraction of salient (1) sentences and (2) keywords. Experiments on English radiology reports from two clinical sites show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score of 3-4%.",
        "author": "Sanjeev Kumar Karn; Ning Liu; Hinrich Schuetze; Oladimeji Farri",
        "authorids": "/s/sanjeev-kumar-karn/; /n/ning-liu/; /h/hinrich-schutze/; /o/oladimeji-farri/",
        "bibtex": "@inproceedings{karn-etal-2022-differentiable,\n    title = \"Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization\",\n    author = \"Karn, Sanjeev Kumar  and\n      Liu, Ning  and\n      Schuetze, Hinrich  and\n      Farri, Oladimeji\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.109/\",\n    doi = \"10.18653/v1/2022.acl-long.109\",\n    pages = \"1542--1553\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.109.pdf",
        "site": "https://aclanthology.org/2022.acl-long.109/",
        "pdf_size": 458793,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2427914658875387742&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Digital Technology and Innovation, Siemens Healthineers, Princeton; Corporate Technology, Siemens AG, Beijing; Center for Information and Language Processing (CIS), LMU Munich; Digital Technology and Innovation, Siemens Healthineers, Princeton",
        "aff_domain": "siemens-healthineers.com;siemens.com;cislmu.org;siemens-healthineers.com",
        "email": "siemens-healthineers.com;siemens.com;cislmu.org;siemens-healthineers.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Siemens Healthineers;Siemens AG;LMU Munich",
        "aff_unique_dep": "Digital Technology and Innovation;Corporate Technology;Center for Information and Language Processing (CIS)",
        "aff_unique_url": "https://www.siemens-healthineers.com;https://www.siemens.com;https://www.lmu.de",
        "aff_unique_abbr": "Siemens;Siemens;LMU",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Princeton;Beijing;Munich",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "United States;China;Germany"
    },
    {
        "id": "2022.findings-acl.167",
        "title": "Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for Under-Documented Languages",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent progress in NLP is driven by pretrained models leveraging massive datasets and has predominantly benefited the world\u2019s political and economic superpowers. Technologically underserved languages are left behind because they lack such resources. Hundreds of underserved languages, nevertheless, have available data sources in the form of interlinear glossed text (IGT) from language documentation efforts. IGT remains underutilized in NLP work, perhaps because its annotations are only semi-structured and often language-specific. With this paper, we make the case that IGT data can be leveraged successfully provided that target language expertise is available. We specifically advocate for collaboration with documentary linguists. Our paper provides a roadmap for successful projects utilizing IGT data: (1) It is essential to define which NLP tasks can be accomplished with the given IGT data and how these will benefit the speech community. (2) Great care and target language expertise is required when converting the data into structured formats commonly employed in NLP. (3) Task-specific and user-specific evaluation can help to ascertain that the tools which are created benefit the target language speech community. We illustrate each step through a case study on developing a morphological reinflection system for the Tsimchianic language Gitksan.",
        "author": "Clarissa Forbes; Farhan Samir; Bruce Oliver; Changbing Yang; Edith Coates; Garrett Nicolai; Miikka Silfverberg",
        "authorids": "/c/clarissa-forbes/; /f/farhan-samir/; /b/bruce-oliver/; /c/changbing-yang/; /e/edith-coates/; /g/garrett-nicolai/; /m/miikka-silfverberg/",
        "bibtex": "@inproceedings{forbes-etal-2022-dim,\n    title = \"Dim Wihl Gat Tun: {T}he Case for Linguistic Expertise in {NLP} for Under-Documented Languages\",\n    author = \"Forbes, Clarissa  and\n      Samir, Farhan  and\n      Oliver, Bruce  and\n      Yang, Changbing  and\n      Coates, Edith  and\n      Nicolai, Garrett  and\n      Silfverberg, Miikka\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.167/\",\n    doi = \"10.18653/v1/2022.findings-acl.167\",\n    pages = \"2116--2130\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.167.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.167/",
        "pdf_size": 362844,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12782888545240846113&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Independent Researcher; University of British Columbia; University of British Columbia; University of British Columbia; University of British Columbia; University of British Columbia; University of British Columbia",
        "aff_domain": "alumni.ubc.ca;ubc.ca;ubc.ca;ubc.ca;ubc.ca;ubc.ca;ubc.ca",
        "email": "alumni.ubc.ca;ubc.ca;ubc.ca;ubc.ca;ubc.ca;ubc.ca;ubc.ca",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "Independent Researcher;University of British Columbia",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ubc.ca",
        "aff_unique_abbr": ";UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1;1;1",
        "aff_country_unique": ";Canada"
    },
    {
        "id": "2022.acl-long.235",
        "title": "Direct Speech-to-Speech Translation With Discrete Units",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the discrete representations of the target speech. When target text transcripts are available, we design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features. When trained without any text transcripts, our model performance is comparable to models that predict spectrograms and are trained with text supervision, showing the potential of our system for translation between unwritten languages.",
        "author": "Ann Lee; Peng-Jen Chen; Changhan Wang; Jiatao Gu; Sravya Popuri; Xutai Ma; Adam Polyak; Yossi Adi; Qing He; Yun Tang; Juan Pino; Wei-Ning Hsu",
        "authorids": "/a/ann-lee/; /p/peng-jen-chen/; /c/changhan-wang/; /j/jiatao-gu/; /s/sravya-popuri/; /x/xutai-ma/; /a/adam-polyak/; /y/yossi-adi/; /q/qing-he/; /y/yun-tang/; /j/juan-pino/; /w/wei-ning-hsu/",
        "bibtex": "@inproceedings{lee-etal-2022-direct,\n    title = \"Direct Speech-to-Speech Translation With Discrete Units\",\n    author = \"Lee, Ann  and\n      Chen, Peng-Jen  and\n      Wang, Changhan  and\n      Gu, Jiatao  and\n      Popuri, Sravya  and\n      Ma, Xutai  and\n      Polyak, Adam  and\n      Adi, Yossi  and\n      He, Qing  and\n      Tang, Yun  and\n      Pino, Juan  and\n      Hsu, Wei-Ning\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.235/\",\n    doi = \"10.18653/v1/2022.acl-long.235\",\n    pages = \"3327--3339\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.235.pdf",
        "site": "https://aclanthology.org/2022.acl-long.235/",
        "pdf_size": 565515,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15439075021958971393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI+Johns Hopkins University; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI",
        "aff_domain": "fb.com; ; ; ; ; ; ; ; ; ; ;fb.com",
        "email": "fb.com; ; ; ; ; ; ; ; ; ; ;fb.com",
        "github": "",
        "project": "https://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0;0+1;0;0;0;0;0;0",
        "aff_unique_norm": "Meta;Johns Hopkins University",
        "aff_unique_dep": "Meta AI;",
        "aff_unique_url": "https://meta.com;https://www.jhu.edu",
        "aff_unique_abbr": "Meta;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.51",
        "title": "Direct parsing to sentiment graphs",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions.",
        "author": "David Samuel; Jeremy Barnes; Robin Kurtz; Stephan Oepen; Lilja \u00d8vrelid; Erik Velldal",
        "authorids": "/d/david-samuel/; /j/jeremy-barnes/; /r/robin-kurtz/; /s/stephan-oepen/; /l/lilja-ovrelid/; /e/erik-velldal/",
        "bibtex": "@inproceedings{samuel-etal-2022-direct,\n    title = \"Direct parsing to sentiment graphs\",\n    author = \"Samuel, David  and\n      Barnes, Jeremy  and\n      Kurtz, Robin  and\n      Oepen, Stephan  and\n      {\\O}vrelid, Lilja  and\n      Velldal, Erik\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.51/\",\n    doi = \"10.18653/v1/2022.acl-short.51\",\n    pages = \"470--478\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.51.pdf",
        "site": "https://aclanthology.org/2022.acl-short.51/",
        "pdf_size": 382291,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1681100736493152721&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Oslo, Language Technology Group; University of the Basque Country UPV/EHU, HiTZ Center \u2013 Ixa; National Library of Sweden, KBLab; University of Oslo, Language Technology Group; University of Oslo, Language Technology Group; University of Oslo, Language Technology Group",
        "aff_domain": "ifi.uio.no;ehu.eus;kb.se;ifi.uio.no;ifi.uio.no;ifi.uio.no",
        "email": "ifi.uio.no;ehu.eus;kb.se;ifi.uio.no;ifi.uio.no;ifi.uio.no",
        "github": "github.com/jerbarnes/direct_parsing_to_sent_graph",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;0;0",
        "aff_unique_norm": "University of Oslo;University of the Basque Country;National Library of Sweden",
        "aff_unique_dep": "Language Technology Group;HiTZ Center \u2013 Ixa;KBLab",
        "aff_unique_url": "https://www.uio.no;https://www.ehu.eus/en;https://www.kb.se",
        "aff_unique_abbr": "UiO;UPV/EHU;KB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0;0;0",
        "aff_country_unique": "Norway;Spain;Sweden"
    },
    {
        "id": "2022.findings-acl.298",
        "title": "Discontinuous Constituency and BERT: A Case Study of Dutch",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this paper, we set out to quantify the syntactic capacity of BERT in the evaluation regime of non-context free patterns, as occurring in Dutch. We devise a test suite based on a mildly context-sensitive formalism, from which we derive grammars that capture the linguistic phenomena of control verb nesting and verb raising. The grammars, paired with a small lexicon, provide us with a large collection of naturalistic utterances, annotated with verb-subject pairings, that serve as the evaluation test bed for an attention-based span selection probe. Our results, backed by extensive analysis, suggest that the models investigated fail in the implicit acquisition of the dependencies examined.",
        "author": "Konstantinos Kogkalidis; Gijs Wijnholds",
        "authorids": "/k/konstantinos-kogkalidis/; /g/gijs-wijnholds/",
        "bibtex": "@inproceedings{kogkalidis-wijnholds-2022-discontinuous,\n    title = \"Discontinuous Constituency and {BERT}: A Case Study of {D}utch\",\n    author = \"Kogkalidis, Konstantinos  and\n      Wijnholds, Gijs\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.298/\",\n    doi = \"10.18653/v1/2022.findings-acl.298\",\n    pages = \"3776--3785\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.298.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.298/",
        "pdf_size": 288318,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11425146653804797645&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 6,
        "aff": "Utrecht Institute of Linguistics OTS, Utrecht University; Utrecht Institute of Linguistics OTS, Utrecht University",
        "aff_domain": "uu.nl;uu.nl",
        "email": "uu.nl;uu.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Utrecht University",
        "aff_unique_dep": "Utrecht Institute of Linguistics OTS",
        "aff_unique_url": "https://www.uu.nl",
        "aff_unique_abbr": "UU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Utrecht",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.acl-long.145",
        "title": "Discrete Opinion Tree Induction for Aspect-based Sentiment Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Dependency trees have been intensively used with graph neural networks for aspect-based sentiment classification. Though being effective, such methods rely on external dependency parsers, which can be unavailable for low-resource languages or perform worse in low-resource domains. In addition, dependency trees are also not optimized for aspect-based sentiment classification. In this paper, we propose an aspect-specific and language-agnostic discrete latent opinion tree model as an alternative structure to explicit dependency trees. To ease the learning of complicated structured latent variables, we build a connection between aspect-to-context attention scores and syntactic distances, inducing trees from the attention scores. Results on six English benchmarks and one Chinese dataset show that our model can achieve competitive performance and interpretability.",
        "author": "Chenhua Chen; Zhiyang Teng; Zhongqing Wang; Yue Zhang",
        "authorids": "/c/chenhua-chen/; /z/zhiyang-teng/; /z/zhongqing-wang/; /y/yue-zhang/",
        "bibtex": "@inproceedings{chen-etal-2022-discrete,\n    title = \"Discrete Opinion Tree Induction for Aspect-based Sentiment Analysis\",\n    author = \"Chen, Chenhua  and\n      Teng, Zhiyang  and\n      Wang, Zhongqing  and\n      Zhang, Yue\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.145/\",\n    doi = \"10.18653/v1/2022.acl-long.145\",\n    pages = \"2051--2064\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.145.pdf",
        "site": "https://aclanthology.org/2022.acl-long.145/",
        "pdf_size": 632561,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=969446521813396276&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Engineering, Westlake University, China + Institute of Advanced Technology, Westlake Institute for Advanced Study; School of Engineering, Westlake University, China + Institute of Advanced Technology, Westlake Institute for Advanced Study; Soochow University; School of Engineering, Westlake University, China + Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn;westlake.edu.cn;suda.edu.cn;wias.org.cn",
        "email": "westlake.edu.cn;westlake.edu.cn;suda.edu.cn;wias.org.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;2;0+1",
        "aff_unique_norm": "Westlake University;Westlake Institute for Advanced Study;Soochow University",
        "aff_unique_dep": "School of Engineering;Institute of Advanced Technology;",
        "aff_unique_url": "https://www.westlake.edu.cn;http://www.wias.org.cn/;https://www.soochow.edu.cn",
        "aff_unique_abbr": ";WIAS;Soochow U",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.15",
        "title": "Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method.",
        "author": "Gianluca Moro; Luca Ragazzi; Lorenzo Valgimigli; Davide Freddi",
        "authorids": "/g/gianluca-moro/; /l/luca-ragazzi/; /l/lorenzo-valgimigli/; /d/davide-freddi/",
        "bibtex": "@inproceedings{moro-etal-2022-discriminative,\n    title = \"Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature\",\n    author = \"Moro, Gianluca  and\n      Ragazzi, Luca  and\n      Valgimigli, Lorenzo  and\n      Freddi, Davide\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.15/\",\n    doi = \"10.18653/v1/2022.acl-long.15\",\n    pages = \"180--189\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.15.pdf",
        "site": "https://aclanthology.org/2022.acl-long.15/",
        "pdf_size": 679539,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11602044256959404188&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Engineering (DISI), University of Bologna, Cesena Campus+CNIT; Department of Computer Science and Engineering (DISI), University of Bologna, Cesena Campus; Department of Computer Science and Engineering (DISI), University of Bologna, Cesena Campus; Department of Computer Science and Engineering (DISI), University of Bologna, Cesena Campus",
        "aff_domain": "unibo.it;unibo.it;unibo.it;studio.unibo.it",
        "email": "unibo.it;unibo.it;unibo.it;studio.unibo.it",
        "github": "",
        "project": "https://disi-unibo-nlp.github.io/projects/damen",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "University of Bologna;Consorzio Nazionale Interuniversitario per le Telecomunicazioni",
        "aff_unique_dep": "Department of Computer Science and Engineering (DISI);",
        "aff_unique_url": "https://www.unibo.it;https://www.cnit.it",
        "aff_unique_abbr": "UNIBO;CNIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cesena;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.acl-short.6",
        "title": "Disentangled Knowledge Transfer for OOD Intent Discovery with Unified Contrastive Learning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Discovering Out-of-Domain(OOD) intents is essential for developing new skills in a task-oriented dialogue system. The key challenge is how to transfer prior IND knowledge to OOD clustering. Different from existing work based on shared intent representation, we propose a novel disentangled knowledge transfer method via a unified multi-head contrastive learning framework. We aim to bridge the gap between IND pre-training and OOD clustering. Experiments and analysis on two benchmark datasets show the effectiveness of our method.",
        "author": "Yutao Mou; Keqing He; Yanan Wu; Zhiyuan Zeng; Hong Xu; Huixing Jiang; Wei Wu; Weiran Xu",
        "authorids": "/y/yutao-mou/; /k/keqing-he/; /y/yanan-wu/; /z/zhiyuan-zeng/; /h/hong-xu/; /h/huixing-jiang/; /w/wei-wu/; /w/weiran-xu/",
        "bibtex": "@inproceedings{mou-etal-2022-disentangled,\n    title = \"Disentangled Knowledge Transfer for {OOD} Intent Discovery with Unified Contrastive Learning\",\n    author = \"Mou, Yutao  and\n      He, Keqing  and\n      Wu, Yanan  and\n      Zeng, Zhiyuan  and\n      Xu, Hong  and\n      Jiang, Huixing  and\n      Wu, Wei  and\n      Xu, Weiran\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.6/\",\n    doi = \"10.18653/v1/2022.acl-short.6\",\n    pages = \"46--53\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.6.pdf",
        "site": "https://aclanthology.org/2022.acl-short.6/",
        "pdf_size": 851951,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18016673484705171077&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "Pattern Recognition & Intelligent System Laboratory+Beijing University of Posts and Telecommunications; Meituan Group; Pattern Recognition & Intelligent System Laboratory+Beijing University of Posts and Telecommunications; Pattern Recognition & Intelligent System Laboratory+Beijing University of Posts and Telecommunications; Pattern Recognition & Intelligent System Laboratory+Beijing University of Posts and Telecommunications; Meituan Group; Meituan Group; Pattern Recognition & Intelligent System Laboratory+Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;meituan.com;meituan.com;bupt.edu.cn",
        "email": "bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;meituan.com;meituan.com;bupt.edu.cn",
        "github": "https://github.com/myt517/DKT",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;0+1;0+1;0+1;2;2;0+1",
        "aff_unique_norm": "Pattern Recognition & Intelligent System Laboratory;Beijing University of Posts and Telecommunications;Meituan Group",
        "aff_unique_dep": "Pattern Recognition & Intelligent System Laboratory;;",
        "aff_unique_url": ";http://www.bupt.edu.cn/;https://www.meituan.com",
        "aff_unique_abbr": ";BUPT;Meituan",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "1;1;1;1;1;1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2022.acl-long.293",
        "title": "Disentangled Sequence to Sequence Learning for Compositional Generalization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There is mounting evidence that existing neural network models, in particular the very popular sequence-to-sequence architecture, struggle to systematically generalize to unseen compositions of seen components. We demonstrate that one of the reasons hindering compositional generalization relates to representations being entangled. We propose an extension to sequence-to-sequence models which encourage disentanglement by adaptively re-encoding (at each time step) the source input. Specifically, we condition the source representations on the newly decoded target context which makes it easier for the encoder to exploit specialized information for each prediction rather than capturing it all in a single forward pass. Experimental results on semantic parsing and machine translation empirically show that our proposal delivers more disentangled representations and better generalization.",
        "author": "Hao Zheng; Mirella Lapata",
        "authorids": "/h/hao-zheng/; /m/mirella-lapata/",
        "bibtex": "@inproceedings{zheng-lapata-2022-disentangled,\n    title = \"Disentangled Sequence to Sequence Learning for Compositional Generalization\",\n    author = \"Zheng, Hao  and\n      Lapata, Mirella\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.293/\",\n    doi = \"10.18653/v1/2022.acl-long.293\",\n    pages = \"4256--4268\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.293.pdf",
        "site": "https://aclanthology.org/2022.acl-long.293/",
        "pdf_size": 453864,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12684731341123726004&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;inf.ed.ac.uk",
        "github": "https://github.com/mswellhao/Dangle",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.498",
        "title": "Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we study the named entity recognition (NER) problem under distant supervision. Due to the incompleteness of the external dictionaries and/or knowledge bases, such distantly annotated training data usually suffer from a high false negative rate. To this end, we formulate the Distantly Supervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU) learning and propose a theoretically and practically novel CONFidence-based MPU (Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of two steps. First, a confidence score is estimated for each token of being an entity token. Then, the proposed Conf-MPU risk estimation is applied to train a multi-class classifier for the NER task. Thorough experiments on two benchmark datasets labeled by various external knowledge demonstrate the superiority of the proposed Conf-MPU over existing DS-NER methods. Our code is available at Github.",
        "author": "Kang Zhou; Yuepei Li; Qi Li",
        "authorids": "/k/kang-zhou/; /y/yuepei-li/; /q/qi-li/",
        "bibtex": "@inproceedings{zhou-etal-2022-distantly,\n    title = \"Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning\",\n    author = \"Zhou, Kang  and\n      Li, Yuepei  and\n      Li, Qi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.498/\",\n    doi = \"10.18653/v1/2022.acl-long.498\",\n    pages = \"7198--7211\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.498.pdf",
        "site": "https://aclanthology.org/2022.acl-long.498/",
        "pdf_size": 2004308,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9955908513416050072&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Iowa State University, Ames, Iowa, USA; Department of Computer Science, Iowa State University, Ames, Iowa, USA; Department of Computer Science, Iowa State University, Ames, Iowa, USA",
        "aff_domain": "iastate.edu;iastate.edu;iastate.edu",
        "email": "iastate.edu;iastate.edu;iastate.edu",
        "github": "https://github.com/kangISU/Conf-MPU-DS-NER",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Iowa State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.iastate.edu",
        "aff_unique_abbr": "ISU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ames",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.73",
        "title": "Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.",
        "author": "Jiayi Wang; Rongzhou Bao; Zhuosheng Zhang; Hai Zhao",
        "authorids": "/j/jiayi-wang/; /r/rongzhou-bao/; /z/zhuosheng-zhang/; /h/hai-zhao/",
        "bibtex": "@inproceedings{wang-etal-2022-distinguishing,\n    title = \"Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model\",\n    author = \"Wang, Jiayi  and\n      Bao, Rongzhou  and\n      Zhang, Zhuosheng  and\n      Zhao, Hai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.73/\",\n    doi = \"10.18653/v1/2022.findings-acl.73\",\n    pages = \"905--915\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.73.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.73/",
        "pdf_size": 415114,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10546872773996773247&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;outlook.com;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;outlook.com;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/LilyNLP/Distinguishing-Non-Natural",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.79",
        "title": "Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We introduce distributed NLI, a new NLU task with a goal to predict the distribution of human judgements for natural language inference. We show that by applying additional distribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation, models can capture human judgement distribution more effectively than the softmax baseline. We show that MC Dropout is able to achieve decent performance without any distribution annotations while Re-Calibration can give further improvements with extra distribution annotations, suggesting the value of multiple annotations for one example in modeling the distribution of human judgements. Despite these improvements, the best results are still far below the estimated human upper-bound, indicating that predicting the distribution of human judgements is still an open, challenging problem with a large room for improvements. We showcase the common errors for MC Dropout and Re-Calibration. Finally, we give guidelines on the usage of these methods with different levels of data availability and encourage future work on modeling the human opinion distribution for language reasoning.",
        "author": "Xiang Zhou; Yixin Nie; Mohit Bansal",
        "authorids": "/x/xiang-zhou/; /y/yixin-nie/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{zhou-etal-2022-distributed,\n    title = \"Distributed {NLI}: Learning to Predict Human Opinion Distributions for Language Reasoning\",\n    author = \"Zhou, Xiang  and\n      Nie, Yixin  and\n      Bansal, Mohit\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.79/\",\n    doi = \"10.18653/v1/2022.findings-acl.79\",\n    pages = \"972--987\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.79.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.79/",
        "pdf_size": 484829,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5424234370088119262&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/easonnie/ChaosNLI",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.139",
        "title": "Distributionally Robust Finetuning BERT for Covariate Drift in Spoken Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this study, we investigate robustness against covariate drift in spoken language understanding (SLU). Covariate drift can occur in SLUwhen there is a drift between training and testing regarding what users request or how they request it. To study this we propose a method that exploits natural variations in data to create a covariate drift in SLU datasets. Experiments show that a state-of-the-art BERT-based model suffers performance loss under this drift. To mitigate the performance loss, we investigate distributionally robust optimization (DRO) for finetuning BERT-based models. We discuss some recent DRO methods, propose two new variants and empirically show that DRO improves robustness under drift.",
        "author": "Samuel Broscheit; Quynh Do; Judith Gaspers",
        "authorids": "/s/samuel-broscheit/; /q/quynh-do/; /j/judith-gaspers/",
        "bibtex": "@inproceedings{broscheit-etal-2022-distributionally,\n    title = \"Distributionally Robust Finetuning {BERT} for Covariate Drift in Spoken Language Understanding\",\n    author = \"Broscheit, Samuel  and\n      Do, Quynh  and\n      Gaspers, Judith\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.139/\",\n    doi = \"10.18653/v1/2022.acl-long.139\",\n    pages = \"1970--1985\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.139.pdf",
        "site": "https://aclanthology.org/2022.acl-long.139/",
        "pdf_size": 346093,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8059972824540142212&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Data and Web Science Research Group, University of Mannheim, Germany+Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "University of Mannheim;Amazon",
        "aff_unique_dep": "Data and Web Science Research Group;Amazon Alexa AI",
        "aff_unique_url": "https://www.uni-mannheim.de;https://www.amazon.com",
        "aff_unique_abbr": ";Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "2022.acl-long.555",
        "title": "Ditch the Gold Standard: Re-evaluating Conversational Question Answering",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Conversational question answering aims to provide natural-language answers to users in information-seeking conversations. Existing conversational QA benchmarks compare models with pre-collected human-human conversations, using ground-truth answers provided in conversational history. It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations. In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems, where human evaluators converse with models and judge the correctness of their answers. We find that the distribution of human machine conversations differs drastically from that of human-human conversations, and there is a disagreement between human and gold-history evaluation in terms of model ranking. We further investigate how to improve automatic evaluations, and propose a question rewriting mechanism based on predicted history, which better correlates with human judgments. Finally, we analyze the impact of various modeling strategies and discuss future directions towards building better conversational question answering systems.",
        "author": "Huihan Li; Tianyu Gao; Manan Goenka; Danqi Chen",
        "authorids": "/h/huihan-li/; /t/tianyu-gao/; /m/manan-goenka/; /d/danqi-chen/",
        "bibtex": "@inproceedings{li-etal-2022-ditch,\n    title = \"Ditch the Gold Standard: Re-evaluating Conversational Question Answering\",\n    author = \"Li, Huihan  and\n      Gao, Tianyu  and\n      Goenka, Manan  and\n      Chen, Danqi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.555/\",\n    doi = \"10.18653/v1/2022.acl-long.555\",\n    pages = \"8074--8085\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.555.pdf",
        "site": "https://aclanthology.org/2022.acl-long.555/",
        "pdf_size": 904085,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2833451607056502419&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "github": "https://github.com/princeton-nlp/EvalConvQA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.149",
        "title": "Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge. In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs. Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.",
        "author": "Wenhao Yu; Chenguang Zhu; Lianhui Qin; Zhihan Zhang; Tong Zhao; Meng Jiang",
        "authorids": "/w/wenhao-yu/; /c/chenguang-zhu/; /l/lianhui-qin/; /z/zhihan-zhang/; /t/tong-zhao/; /m/meng-jiang/",
        "bibtex": "@inproceedings{yu-etal-2022-diversifying,\n    title = \"Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts\",\n    author = \"Yu, Wenhao  and\n      Zhu, Chenguang  and\n      Qin, Lianhui  and\n      Zhang, Zhihan  and\n      Zhao, Tong  and\n      Jiang, Meng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.149/\",\n    doi = \"10.18653/v1/2022.findings-acl.149\",\n    pages = \"1896--1906\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.149.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.149/",
        "pdf_size": 456452,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11744305159097412928&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 12,
        "aff": "University of Notre Dame; Microsoft Cognitive Services Research; University of Washington; University of Notre Dame; University of Notre Dame; University of Notre Dame",
        "aff_domain": "nd.edu;microsoft.com;cs.washington.edu;nd.edu;nd.edu;nd.edu",
        "email": "nd.edu;microsoft.com;cs.washington.edu;nd.edu;nd.edu;nd.edu",
        "github": "https://github.com/DM2-ND/MoKGE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;0;0",
        "aff_unique_norm": "University of Notre Dame;Microsoft;University of Washington",
        "aff_unique_dep": ";Cognitive Services Research;",
        "aff_unique_url": "https://www.nd.edu;https://www.microsoft.com;https://www.washington.edu",
        "aff_unique_abbr": "Notre Dame;Microsoft;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.287",
        "title": "Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Text semantic matching is a fundamental task that has been widely used in various scenarios, such as community question answering, information retrieval, and recommendation. Most state-of-the-art matching models, e.g., BERT, directly perform text comparison by processing each word uniformly. However, a query sentence generally comprises content that calls for different levels of matching granularity. Specifically, keywords represent factual information such as action, entity, and event that should be strictly matched, while intents convey abstract concepts and ideas that can be paraphrased into various expressions. In this work, we propose a simple yet effective training strategy for text semantic matching in a divide-and-conquer manner by disentangling keywords from intents. Our approach can be easily combined with pre-trained language models (PLM) without influencing their inference efficiency, achieving stable performance improvements against a wide range of PLMs on three benchmarks.",
        "author": "Yicheng Zou; Hongwei Liu; Tao Gui; Junzhe Wang; Qi Zhang; Meng Tang; Haixiang Li; Daniell Wang",
        "authorids": "/y/yicheng-zou/; /h/hongwei-liu/; /t/tao-gui/; /j/junzhe-wang/; /q/qi-zhang/; /m/meng-tang/; /h/haixiang-li/; /d/daniell-wang/",
        "bibtex": "@inproceedings{zou-etal-2022-divide,\n    title = \"Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents\",\n    author = \"Zou, Yicheng  and\n      Liu, Hongwei  and\n      Gui, Tao  and\n      Wang, Junzhe  and\n      Zhang, Qi  and\n      Tang, Meng  and\n      Li, Haixiang  and\n      Wang, Daniell\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.287/\",\n    doi = \"10.18653/v1/2022.findings-acl.287\",\n    pages = \"3622--3632\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.287.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.287/",
        "pdf_size": 1673531,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12280154671741274144&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 9,
        "aff": "Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China+Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China+Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Shanghai, China; Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; IPS, Tencent PCG, Beijing, China; IPS, Tencent PCG, Beijing, China; IPS, Tencent PCG, Beijing, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;tencent.com;tencent.com;tencent.com",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;0;0;0;2;2;2",
        "aff_unique_norm": "Fudan University;Shanghai Collaborative Innovation Center of Intelligent Visual Computing;Tencent",
        "aff_unique_dep": "Institute of Modern Languages and Linguistics;;Tencent PCG",
        "aff_unique_url": "https://www.fudan.edu.cn/en/;;https://www.tencent.com",
        "aff_unique_abbr": "Fudan;;Tencent",
        "aff_campus_unique_index": "0+0;0+0;0;0;0;1;1;1",
        "aff_campus_unique": "Shanghai;Beijing",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.141",
        "title": "Divide and Denoise: Learning from Noisy Labels in Fine-Grained Entity Typing with Cluster-Wise Loss Correction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fine-grained Entity Typing (FET) has made great progress based on distant supervision but still suffers from label noise. Existing FET noise learning methods rely on prediction distributions in an instance-independent manner, which causes the problem of confirmation bias. In this work, we propose a clustering-based loss correction framework named Feature Cluster Loss Correction (FCLC), to address these two problems. FCLC first train a coarse backbone model as a feature extractor and noise estimator. Loss correction is then applied to each feature cluster, learning directly from the noisy labels. Experimental results on three public datasets show that FCLC achieves the best performance over existing competitive systems. Auxiliary experiments further demonstrate that FCLC is stable to hyperparameters and it does help mitigate confirmation bias. We also find that in the extreme case of no clean data, the FCLC framework still achieves competitive performance.",
        "author": "Kunyuan Pang; Haoyu Zhang; Jie Zhou; Ting Wang",
        "authorids": "/k/kunyuan-pang/; /h/haoyu-zhang/; /j/jie-zhou/; /t/ting-wang/",
        "bibtex": "@inproceedings{pang-etal-2022-divide,\n    title = \"Divide and Denoise: Learning from Noisy Labels in Fine-Grained Entity Typing with Cluster-Wise Loss Correction\",\n    author = \"Pang, Kunyuan  and\n      Zhang, Haoyu  and\n      Zhou, Jie  and\n      Wang, Ting\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.141/\",\n    doi = \"10.18653/v1/2022.acl-long.141\",\n    pages = \"1997--2006\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.141.pdf",
        "site": "https://aclanthology.org/2022.acl-long.141/",
        "pdf_size": 3306793,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=323086309195549545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer, National University of Defense Technology; Artificial Intelligence Research Center, Defense Innovation Institute; Shanghai Jiao Tong University; School of Computer, National University of Defense Technology",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;sjtu.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;sjtu.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "National University of Defense Technology;Defense Innovation Institute;Shanghai Jiao Tong University",
        "aff_unique_dep": "School of Computer;Artificial Intelligence Research Center;",
        "aff_unique_url": "http://www.nudt.edu.cn/;;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "NUDT;;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.312",
        "title": "Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-encoder models are a broad family of context-aware neural machine translation systems that aim to improve translation quality by encoding document-level contextual information alongside the current sentence. The context encoding is undertaken by contextual parameters, trained on document-level data. In this work, we discuss the difficulty of training these parameters effectively, due to the sparsity of the words in need of context (i.e., the training signal), and their relevant context. We propose to pre-train the contextual parameters over split sentence pairs, which makes an efficient use of the available data for two reasons. Firstly, it increases the contextual training signal by breaking intra-sentential syntactic relations, and thus pushing the model to search the context for disambiguating clues more frequently. Secondly, it eases the retrieval of relevant context, since context segments become shorter. We propose four different splitting methods, and evaluate our approach with BLEU and contrastive test sets. Results show that it consistently improves learning of contextual parameters, both in low and high resource settings.",
        "author": "Lorenzo Lupo; Marco Dinarelli; Laurent Besacier",
        "authorids": "/l/lorenzo-lupo/; /m/marco-dinarelli/; /l/laurent-besacier/",
        "bibtex": "@inproceedings{lupo-etal-2022-divide,\n    title = \"Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models\",\n    author = \"Lupo, Lorenzo  and\n      Dinarelli, Marco  and\n      Besacier, Laurent\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.312/\",\n    doi = \"10.18653/v1/2022.acl-long.312\",\n    pages = \"4557--4572\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.312.pdf",
        "site": "https://aclanthology.org/2022.acl-long.312/",
        "pdf_size": 356757,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4300429331285002125&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Universit\u00e9 Grenoble Alpes, France; Universit\u00e9 Grenoble Alpes, France; Universit\u00e9 Grenoble Alpes, France+Naver Labs Europe, France",
        "aff_domain": "univ-grenoble-alpes.fr;univ-grenoble-alpes.fr;naverlabs.com",
        "email": "univ-grenoble-alpes.fr;univ-grenoble-alpes.fr;naverlabs.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Universit\u00e9 Grenoble Alpes;NAVER LABS Europe",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.univ-grenoble-alpes.fr;https://labs.naver.com",
        "aff_unique_abbr": "UGA;NLE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.findings-acl.282",
        "title": "Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In recent years, pre-trained language models (PLMs) have been shown to capture factual knowledge from massive texts, which encourages the proposal of PLM-based knowledge graph completion (KGC) models. However, these models are still quite behind the SOTA KGC models in terms of performance. In this work, we find two main reasons for the weak performance: (1) Inaccurate evaluation setting. The evaluation setting under the closed-world assumption (CWA) may underestimate the PLM-based KGC models since they introduce more external knowledge; (2) Inappropriate utilization of PLMs. Most PLM-based KGC models simply splice the labels of entities and relations as inputs, leading to incoherent sentences that do not take full advantage of the implicit knowledge in PLMs. To alleviate these problems, we highlight a more accurate evaluation setting under the open-world assumption (OWA), which manual checks the correctness of knowledge that is not in KGs. Moreover, motivated by prompt tuning, we propose a novel PLM-based KGC model named PKGC. The basic idea is to convert each triple and its support information into natural prompt sentences, which is further fed into PLMs for classification. Experiment results on two KGC datasets demonstrate OWA is more reliable for evaluating KGC, especially on the link prediction, and the effectiveness of our PKCG model on both CWA and OWA settings.",
        "author": "Xin Lv; Yankai Lin; Yixin Cao; Lei Hou; Juanzi Li; Zhiyuan Liu; Peng Li; Jie Zhou",
        "authorids": "/x/xin-lv/; /y/yankai-lin/; /y/yixin-cao/; /l/lei-hou/; /j/juanzi-li/; /z/zhiyuan-liu/; /p/peng-li/; /j/jie-zhou/",
        "bibtex": "@inproceedings{lv-etal-2022-pre,\n    title = \"Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach\",\n    author = \"Lv, Xin  and\n      Lin, Yankai  and\n      Cao, Yixin  and\n      Hou, Lei  and\n      Li, Juanzi  and\n      Liu, Zhiyuan  and\n      Li, Peng  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.282/\",\n    doi = \"10.18653/v1/2022.findings-acl.282\",\n    pages = \"3570--3581\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.282.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.282/",
        "pdf_size": 741493,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2493430879168810431&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Technology, BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China; Pattern Recognition Center, WeChat AI, Tencent Inc., China; Singapore Management University, Singapore; Department of Computer Science and Technology, BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China; Department of Computer Science and Technology, BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China; Department of Computer Science and Technology, BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China; Institute for AI Industry Research (AIR), Tsinghua University, China+Pattern Recognition Center, WeChat AI, Tencent Inc., China; Pattern Recognition Center, WeChat AI, Tencent Inc., China",
        "aff_domain": "mails.tsinghua.edu.cn; ;tsinghua.edu.cn; ; ; ; ;",
        "email": "mails.tsinghua.edu.cn; ;tsinghua.edu.cn; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;3;0+1;0+1;0+1;1+2;2",
        "aff_unique_norm": "BNRist;Tsinghua University;Tencent;Singapore Management University",
        "aff_unique_dep": "Department of Computer Science and Technology;Institute for Artificial Intelligence;Pattern Recognition Center, WeChat AI;",
        "aff_unique_url": ";https://www.tsinghua.edu.cn;https://www.tencent.com;https://www.smu.edu.sg",
        "aff_unique_abbr": ";THU;Tencent;SMU",
        "aff_campus_unique_index": "1;1;1;1;",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "1;1;2;1;1;1;1+1;1",
        "aff_country_unique": ";China;Singapore"
    },
    {
        "id": "2022.acl-long.296",
        "title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Learned self-attention functions in state-of-the-art NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pre-trained self-attention for human attention depends on \u2018what is in the tail\u2019, e.g., the syntactic nature of rare contexts. Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lower-entropy attention vectors are more faithful.",
        "author": "Oliver Eberle; Stephanie Brandl; Jonas Pilot; Anders S\u00f8gaard",
        "authorids": "/o/oliver-eberle/; /s/stephanie-brandl/; /j/jonas-pilot/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{eberle-etal-2022-transformer,\n    title = \"Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?\",\n    author = \"Eberle, Oliver  and\n      Brandl, Stephanie  and\n      Pilot, Jonas  and\n      S{\\o}gaard, Anders\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.296/\",\n    doi = \"10.18653/v1/2022.acl-long.296\",\n    pages = \"4295--4309\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.296.pdf",
        "site": "https://aclanthology.org/2022.acl-long.296/",
        "pdf_size": 1114850,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11581948440900889985&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Machine Learning Group, TU Berlin, Germany+University of Copenhagen, Denmark; Machine Learning Group, TU Berlin, Germany+University of Copenhagen, Denmark; Machine Learning Group, TU Berlin, Germany; University of Copenhagen, Denmark",
        "aff_domain": "tu-berlin.de;di.ku.dk;di.ku.dk; ",
        "email": "tu-berlin.de;di.ku.dk;di.ku.dk; ",
        "github": "github.com/oeberle/task_gaze_transformers",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0;1",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;University of Copenhagen",
        "aff_unique_dep": "Machine Learning Group;",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.ku.dk",
        "aff_unique_abbr": "TU Berlin;UCPH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berlin;",
        "aff_country_unique_index": "0+1;0+1;0;1",
        "aff_country_unique": "Germany;Denmark"
    },
    {
        "id": "2022.acl-long.523",
        "title": "Do self-supervised speech models develop human-like perception biases?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Self-supervised models for speech processing form representational spaces without using any external labels. Increasingly, they appear to be a feasible way of at least partially eliminating costly manual annotations, a problem of particular concern for low-resource languages. But what kind of representational spaces do these models construct?Human perception specializes to the sounds of listeners\u2019 native languages. Does the same thing happen in self-supervised models? We examine the representational spaces of three kinds of state of the art self-supervised models: wav2vec, HuBERT and contrastive predictive coding (CPC), and compare them with the perceptual spaces of French-speaking and English-speaking human listeners, both globally and taking account of the behavioural differences between the two language groups. We show that the CPC model shows a small native language effect, but that wav2vec and HuBERT seem to develop a universal speech perception space which is not language specific. A comparison against the predictions of supervised phone recognisers suggests that all three self-supervised models capture relatively fine-grained perceptual phenomena, while supervised models are better at capturing coarser, phone-level effects, and effects of listeners\u2019 native language, on perception.",
        "author": "Juliette Millet; Ewan Dunbar",
        "authorids": "/j/juliette-millet/; /e/ewan-dunbar/",
        "bibtex": "@inproceedings{millet-dunbar-2022-self,\n    title = \"Do self-supervised speech models develop human-like perception biases?\",\n    author = \"Millet, Juliette  and\n      Dunbar, Ewan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.523/\",\n    doi = \"10.18653/v1/2022.acl-long.523\",\n    pages = \"7591--7605\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.523.pdf",
        "site": "https://aclanthology.org/2022.acl-long.523/",
        "pdf_size": 1656560,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12626493426223688728&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "CoML, ENS/CNRS/EHESS/INRIA/PSL, LLF, University of Paris, CNRS, CRI, FAN, IIFR, University of Paris, Paris, France; CoML, ENS/CNRS/EHESS/INRIA/PSL, Paris, France + University of Toronto, Toronto, Canada",
        "aff_domain": "cri-paris.org;utoronto.ca",
        "email": "cri-paris.org;utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "University of Paris;\u00c9cole Normale Sup\u00e9rieure;University of Toronto",
        "aff_unique_dep": ";CoML;",
        "aff_unique_url": "https://www.universite-paris.fr;https://www.ens.fr;https://www.utoronto.ca",
        "aff_unique_abbr": "UP;ENS;U of T",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Paris;Toronto",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "France;Canada"
    },
    {
        "id": "2022.acl-long.533",
        "title": "DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing (NLP) algorithms have become very successful, but they still struggle when applied to out-of-distribution examples. In this paper we propose a controllable generation approach in order to deal with this domain adaptation (DA) challenge. Given an input text example, our DoCoGen algorithm generates a domain-counterfactual textual example (D-con) - that is similar to the original in all aspects, including the task label, but its domain is changed to a desired one. Importantly, DoCoGen is trained using only unlabeled examples from multiple domains - no NLP task labels or parallel pairs of textual examples and their domain-counterfactuals are required. We show that DoCoGen can generate coherent counterfactuals consisting of multiple sentences. We use the D-cons generated by DoCoGen to augment a sentiment classifier and a multi-label intent classifier in 20 and 78 DA setups, respectively, where source-domain labeled data is scarce. Our model outperforms strong baselines and improves the accuracy of a state-of-the-art unsupervised DA algorithm.",
        "author": "Nitay Calderon; Eyal Ben-David; Amir Feder; Roi Reichart",
        "authorids": "/n/nitay-calderon/; /e/eyal-ben-david/; /a/amir-feder/; /r/roi-reichart/",
        "bibtex": "@inproceedings{calderon-etal-2022-docogen,\n    title = \"{D}o{C}o{G}en: {D}omain Counterfactual Generation for Low Resource Domain Adaptation\",\n    author = \"Calderon, Nitay  and\n      Ben-David, Eyal  and\n      Feder, Amir  and\n      Reichart, Roi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.533/\",\n    doi = \"10.18653/v1/2022.acl-long.533\",\n    pages = \"7727--7746\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.533.pdf",
        "site": "https://aclanthology.org/2022.acl-long.533/",
        "pdf_size": 484073,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13374262047657371815&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Technion - Israel Institute of Technology; Technion - Israel Institute of Technology; Technion - Israel Institute of Technology; Technion - Israel Institute of Technology",
        "aff_domain": "campus.technion.ac.il;campus.technion.ac.il;campus.technion.ac.il;technion.ac.il",
        "email": "campus.technion.ac.il;campus.technion.ac.il;campus.technion.ac.il;technion.ac.il",
        "github": "https://github.com/nitaytech/DoCoGen",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.acl-long.79",
        "title": "Doctor Recommendation in Online Health Forums via Expertise Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Huge volumes of patient queries are daily generated on online health forums, rendering manual doctor allocation a labor-intensive task. To better help patients, this paper studies a novel task of doctor recommendation to enable automatic pairing of a patient to a doctor with relevant expertise. While most prior work in recommendation focuses on modeling target users from their past behavior, we can only rely on the limited words in a query to infer a patient\u2019s needs for privacy reasons. For doctor modeling, we study the joint effects of their profiles and previous dialogues with other patients and explore their interactions via self-learning. The learned doctor embeddings are further employed to estimate their capabilities of handling a patient query with a multi-head attention mechanism. For experiments, a large-scale dataset is collected from Chunyu Yisheng, a Chinese online health forum, where our model exhibits the state-of-the-art results, outperforming baselines only consider profiles and past dialogues to characterize a doctor.",
        "author": "Xiaoxin Lu; Yubo Zhang; Jing Li; Shi Zong",
        "authorids": "/x/xiaoxin-lu/; /y/yubo-zhang/; /j/jing-li/; /s/shi-zong/",
        "bibtex": "@inproceedings{lu-etal-2022-doctor,\n    title = \"Doctor Recommendation in Online Health Forums via Expertise Learning\",\n    author = \"Lu, Xiaoxin  and\n      Zhang, Yubo  and\n      Li, Jing  and\n      Zong, Shi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.79/\",\n    doi = \"10.18653/v1/2022.acl-long.79\",\n    pages = \"1111--1123\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.79.pdf",
        "site": "https://aclanthology.org/2022.acl-long.79/",
        "pdf_size": 1617964,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8781824104800993242&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computing, The Hong Kong Polytechnic University, HKSAR, China+Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computing, The Hong Kong Polytechnic University, HKSAR, China+Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computing, The Hong Kong Polytechnic University, HKSAR, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China",
        "aff_domain": "connect.polyu.hk;connect.polyu.hk;polyu.edu.hk;nju.edu.cn",
        "email": "connect.polyu.hk;connect.polyu.hk;polyu.edu.hk;nju.edu.cn",
        "github": "https://github.com/polyusmart/Doctor-Recommendation",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0;1",
        "aff_unique_norm": "Hong Kong Polytechnic University;Nanjing University",
        "aff_unique_dep": "Department of Computing;Department of Computer Science and Technology",
        "aff_unique_url": "https://www.polyu.edu.hk;http://www.nju.edu.cn",
        "aff_unique_abbr": "PolyU;Nanjing U",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Nanjing",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.130",
        "title": "Document-Level Event Argument Extraction via Optimal Transport",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Event Argument Extraction (EAE) is one of the sub-tasks of event extraction, aiming to recognize the role of each entity mention toward a specific event trigger. Despite the success of prior works in sentence-level EAE, the document-level setting is less explored. In particular, whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE, prior document-level EAE models totally ignore syntactic structures for documents. Hence, in this work, we study the importance of syntactic structures in document-level EAE. Specifically, we propose to employ Optimal Transport (OT) to induce structures of documents based on sentence-level syntactic structures and tailored to EAE task. Furthermore, we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE. We perform extensive experiments on the benchmark document-level EAE dataset RAMS that leads to the state-of-the-art performance. Moreover, our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.",
        "author": "Amir Pouran Ben Veyseh; Minh Van Nguyen; Franck Dernoncourt; Bonan Min; Thien Nguyen",
        "authorids": "/a/amir-pouran-ben-veyseh/; /m/minh-van-nguyen/; /f/franck-dernoncourt/; /b/bonan-min/; /t/thien-nguyen/",
        "bibtex": "@inproceedings{pouran-ben-veyseh-etal-2022-document,\n    title = \"Document-Level Event Argument Extraction via Optimal Transport\",\n    author = \"Pouran Ben Veyseh, Amir  and\n      Nguyen, Minh Van  and\n      Dernoncourt, Franck  and\n      Min, Bonan  and\n      Nguyen, Thien\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.130/\",\n    doi = \"10.18653/v1/2022.findings-acl.130\",\n    pages = \"1648--1658\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.130.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.130/",
        "pdf_size": 303548,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=288190407514921544&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Adobe Research, San Jose, CA, USA; Raytheon BBN Technologies, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA",
        "aff_domain": "cs.uoregon.edu;cs.uoregon.edu;adobe.com;raytheon.com;cs.uoregon.edu",
        "email": "cs.uoregon.edu;cs.uoregon.edu;adobe.com;raytheon.com;cs.uoregon.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of Oregon;Adobe;Raytheon BBN Technologies",
        "aff_unique_dep": "Department of Computer and Information Science;Adobe Research;",
        "aff_unique_url": "https://www.uoregon.edu;https://research.adobe.com;https://www.raytheonbbn.com",
        "aff_unique_abbr": "UO;Adobe;BBN",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Eugene;San Jose;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.132",
        "title": "Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.",
        "author": "Qingyu Tan; Ruidan He; Lidong Bing; Hwee Tou Ng",
        "authorids": "/q/qingyu-tan/; /r/ruidan-he/; /l/lidong-bing/; /h/hwee-tou-ng/",
        "bibtex": "@inproceedings{tan-etal-2022-document,\n    title = \"Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation\",\n    author = \"Tan, Qingyu  and\n      He, Ruidan  and\n      Bing, Lidong  and\n      Ng, Hwee Tou\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.132/\",\n    doi = \"10.18653/v1/2022.findings-acl.132\",\n    pages = \"1672--1681\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.132.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.132/",
        "pdf_size": 533929,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11109196080958118867&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 6,
        "aff": "DAMO Academy, Alibaba Group+Department of Computer Science, National University of Singapore; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; Department of Computer Science, National University of Singapore",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;comp.nus.edu.sg",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;comp.nus.edu.sg",
        "github": "https://github.com/tonytan48/KD-DocRE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Alibaba Group;National University of Singapore",
        "aff_unique_dep": "DAMO Academy;Department of Computer Science",
        "aff_unique_url": "https://www.alibaba-group.com;https://www.nus.edu.sg",
        "aff_unique_abbr": "Alibaba;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.acl-short.11",
        "title": "Does BERT Know that the IS-A Relation Is Transitive?",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "The success of a natural language processing (NLP) system on a task does not amount to fully understanding the complexity of the task, typified by many deep learning models. One such question is: can a black-box model make logically consistent predictions for transitive relations? Recent studies suggest that pre-trained BERT can capture lexico-semantic clues from words in the context. However, to what extent BERT captures the transitive nature of some lexical relations is unclear. From a probing perspective, we examine WordNet word senses and the IS-A relation, which is a transitive relation. That is, for senses A, B, and C, A is-a B and B is-a C entail A is-a C. We aim to quantify how much BERT agrees with the transitive property of IS-A relations, via a minimalist probing setting. Our investigation reveals that BERT\u2019s predictions do not fully obey the transitivity property of the IS-A relation.",
        "author": "Ruixi Lin; Hwee Tou Ng",
        "authorids": "/r/ruixi-lin/; /h/hwee-tou-ng/",
        "bibtex": "@inproceedings{lin-ng-2022-bert,\n    title = \"Does {BERT} Know that the {IS}-A Relation Is Transitive?\",\n    author = \"Lin, Ruixi  and\n      Ng, Hwee Tou\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.11/\",\n    doi = \"10.18653/v1/2022.acl-short.11\",\n    pages = \"94--99\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.11.pdf",
        "site": "https://aclanthology.org/2022.acl-short.11/",
        "pdf_size": 268588,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15087076190256265076&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "https://github.com/nusnlp/probe-bert-transitivity",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.findings-acl.181",
        "title": "Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood. They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without explicit supervision. In this paper, we examine the extent to which BERT is able to perform lexically-independent subject-verb number agreement (NA) on targeted syntactic templates. To do so, we disrupt the lexical patterns found in naturally occurring stimuli for each targeted structure in a novel fine-grained analysis of BERT\u2019s behavior. Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.",
        "author": "Karim Lasri; Alessandro Lenci; Thierry Poibeau",
        "authorids": "/k/karim-lasri/; /a/alessandro-lenci/; /t/thierry-poibeau/",
        "bibtex": "@inproceedings{lasri-etal-2022-bert,\n    title = \"Does {BERT} really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task\",\n    author = \"Lasri, Karim  and\n      Lenci, Alessandro  and\n      Poibeau, Thierry\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.181/\",\n    doi = \"10.18653/v1/2022.findings-acl.181\",\n    pages = \"2309--2315\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.181.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.181/",
        "pdf_size": 225387,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15877381054603711760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Lattice (\u00c9cole Normale Sup\u00e9rieure-PSL, CNRS, U. Sorbonne Nouvelle); University of Pisa; Lattice (\u00c9cole Normale Sup\u00e9rieure-PSL, CNRS, U. Sorbonne Nouvelle)",
        "aff_domain": "ens.psl.eu;unipi.it;ens.psl.eu",
        "email": "ens.psl.eu;unipi.it;ens.psl.eu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure-PSL;University of Pisa",
        "aff_unique_dep": "Lattice;",
        "aff_unique_url": "https://www.ens.psl.eu;https://www.unipi.it",
        "aff_unique_abbr": "ENS-PSL;UNIP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;Italy"
    },
    {
        "id": "2022.acl-long.432",
        "title": "Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "DocRED is a widely used dataset for document-level relation extraction. In the large-scale annotation, a recommend-revise scheme is adopted to reduce the workload. Within this scheme, annotators are provided with candidate relation instances from distant supervision, and they then manually supplement and remove relational facts based on the recommendations. However, when comparing DocRED with a subset relabeled from scratch, we find that this scheme results in a considerable amount of false negative samples and an obvious bias towards popular entities and relations. Furthermore, we observe that the models trained on DocRED have low recall on our relabeled dataset and inherit the same bias in the training data. Through the analysis of annotators\u2019 behaviors, we figure out the underlying reason for the problems above: the scheme actually discourages annotators from supplementing adequate instances in the revision phase. We appeal to future research to take into consideration the issues with the recommend-revise scheme when designing new models and annotation schemes. The relabeled dataset is released at https://github.com/AndrewZhe/Revisit-DocRED, to serve as a more reliable test set of document RE models.",
        "author": "Quzhe Huang; Shibo Hao; Yuan Ye; Shengqi Zhu; Yansong Feng; Dongyan Zhao",
        "authorids": "/q/quzhe-huang/; /s/shibo-hao/; /y/yuan-ye/; /s/shengqi-zhu/; /y/yansong-feng/; /d/dongyan-zhao/",
        "bibtex": "@inproceedings{huang-etal-2022-recommend,\n    title = \"Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in {D}oc{RED}\",\n    author = \"Huang, Quzhe  and\n      Hao, Shibo  and\n      Ye, Yuan  and\n      Zhu, Shengqi  and\n      Feng, Yansong  and\n      Zhao, Dongyan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.432/\",\n    doi = \"10.18653/v1/2022.acl-long.432\",\n    pages = \"6241--6252\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.432.pdf",
        "site": "https://aclanthology.org/2022.acl-long.432/",
        "pdf_size": 513742,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5084990552563717357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Wangxuan Institute of Computer Technology, Peking University, China; Wangxuan Institute of Computer Technology, Peking University, China; Wangxuan Institute of Computer Technology, Peking University, China; University of Washington; Wangxuan Institute of Computer Technology, Peking University, China; Wangxuan Institute of Computer Technology, Peking University, China",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;uw.edu;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;uw.edu;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/AndrewZhe/Revisit-DocRED",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "Peking University;University of Washington",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.washington.edu",
        "aff_unique_abbr": "PKU;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.6",
        "title": "Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Complex word identification (CWI) is a cornerstone process towards proper text simplification. CWI is highly dependent on context, whereas its difficulty is augmented by the scarcity of available datasets which vary greatly in terms of domains and languages. As such, it becomes increasingly more difficult to develop a robust model that generalizes across a wide array of input examples. In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations. This technique addresses the problem of working with multiple domains, inasmuch as it creates a way of smoothing the differences between the explored datasets. Moreover, we also propose a similar auxiliary task, namely text simplification, that can be used to complement lexical complexity prediction. Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset. At the same time, we obtain an increase of 3% in Pearson scores, while considering a cross-lingual setup relying on the Complex Word Identification 2018 dataset. In addition, our model yields state-of-the-art results in terms of Mean Absolute Error.",
        "author": "George-Eduard Zaharia; R\u0103zvan-Alexandru Sm\u0103du; Dumitru Cercel; Mihai Dascalu",
        "authorids": "/g/george-eduard-zaharia/; /r/razvan-alexandru-smadu/; /d/dumitru-cercel/; /m/mihai-dascalu/",
        "bibtex": "@inproceedings{zaharia-etal-2022-domain,\n    title = \"Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification\",\n    author = \"Zaharia, George-Eduard  and\n      Sm{\\u{a}}du, R{\\u{a}}zvan-Alexandru  and\n      Cercel, Dumitru  and\n      Dascalu, Mihai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.6/\",\n    doi = \"10.18653/v1/2022.acl-long.6\",\n    pages = \"70--80\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.6.pdf",
        "site": "https://aclanthology.org/2022.acl-long.6/",
        "pdf_size": 416109,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10247057176690559616&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University Politehnica of Bucharest, Faculty of Automatic Control and Computers; University Politehnica of Bucharest, Faculty of Automatic Control and Computers; University Politehnica of Bucharest, Faculty of Automatic Control and Computers; University Politehnica of Bucharest, Faculty of Automatic Control and Computers",
        "aff_domain": "stud.acs.upb.ro;stud.acs.upb.ro;upb.ro;upb.ro",
        "email": "stud.acs.upb.ro;stud.acs.upb.ro;upb.ro;upb.ro",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University Politehnica of Bucharest",
        "aff_unique_dep": "Faculty of Automatic Control and Computers",
        "aff_unique_url": "https://www.upb.ro",
        "aff_unique_abbr": "UPB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Romania"
    },
    {
        "id": "2022.findings-acl.49",
        "title": "Domain Generalisation of NMT: Fusing Adapters with Leave-One-Domain-Out Training",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Generalising to unseen domains is under-explored and remains a challenge in neural machine translation. Inspired by recent research in parameter-efficient transfer learning from pretrained models, this paper proposes a fusion-based generalisation method that learns to combine domain-specific parameters. We propose a leave-one-domain-out training strategy to avoid information leaking to address the challenge of not knowing the test domain during training time. Empirical results on three language pairs show that our proposed fusion method outperforms other baselines up to +0.8 BLEU score on average.",
        "author": "Thuy-Trang Vu; Shahram Khadivi; Dinh Phung; Gholamreza Haffari",
        "authorids": "/t/thuy-vu/; /s/shahram-khadivi/; /d/dinh-phung/; /g/gholamreza-haffari/",
        "bibtex": "@inproceedings{vu-etal-2022-domain,\n    title = \"Domain Generalisation of {NMT}: Fusing Adapters with Leave-One-Domain-Out Training\",\n    author = \"Vu, Thuy-Trang  and\n      Khadivi, Shahram  and\n      Phung, Dinh  and\n      Haffari, Gholamreza\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.49/\",\n    doi = \"10.18653/v1/2022.findings-acl.49\",\n    pages = \"582--588\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.49.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.49/",
        "pdf_size": 337409,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1222341045132815883&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Data Science and AI, Monash University, Australia+eBay Inc.; eBay Inc.; Department of Data Science and AI, Monash University, Australia; Department of Data Science and AI, Monash University, Australia",
        "aff_domain": "monash.edu;ebay.com;monash.edu;monash.edu",
        "email": "monash.edu;ebay.com;monash.edu;monash.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "Monash University;eBay Inc.",
        "aff_unique_dep": "Department of Data Science and AI;",
        "aff_unique_url": "https://www.monash.edu;https://www.ebayinc.com",
        "aff_unique_abbr": "Monash;eBay",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;0;0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "2022.acl-long.116",
        "title": "Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Since the development and wide use of pretrained language models (PLMs), several approaches have been applied to boost their performance on downstream tasks in specific domains, such as biomedical or scientific domains. Additional pre-training with in-domain texts is the most common approach for providing domain-specific knowledge to PLMs. However, these pre-training methods require considerable in-domain data and training resources and a longer training time. Moreover, the training must be re-performed whenever a new PLM emerges. In this study, we propose a domain knowledge transferring (DoKTra) framework for PLMs without additional in-domain pretraining. Specifically, we extract the domain knowledge from an existing in-domain pretrained language model and transfer it to other PLMs by applying knowledge distillation. In particular, we employ activation boundary distillation, which focuses on the activation of hidden neurons. We also apply an entropy regularization term in both teacher training and distillation to encourage the model to generate reliable output probabilities, and thus aid the distillation. By applying the proposed DoKTra framework to downstream tasks in the biomedical, clinical, and financial domains, our student models can retain a high percentage of teacher performance and even outperform the teachers in certain tasks. Our code is available at https://github.com/DMCB-GIST/DoKTra.",
        "author": "Dongha Choi; HongSeok Choi; Hyunju Lee",
        "authorids": "/d/dongha-choi/; /h/hongseok-choi/; /h/hyunju-lee/",
        "bibtex": "@inproceedings{choi-etal-2022-domain,\n    title = \"Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation\",\n    author = \"Choi, Dongha  and\n      Choi, HongSeok  and\n      Lee, Hyunju\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.116/\",\n    doi = \"10.18653/v1/2022.acl-long.116\",\n    pages = \"1658--1669\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.116.pdf",
        "site": "https://aclanthology.org/2022.acl-long.116/",
        "pdf_size": 792762,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12162704466463075211&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Artificial Intelligence Graduate School; School of Electrical Engineering and Computer Science; Artificial Intelligence Graduate School+School of Electrical Engineering and Computer Science",
        "aff_domain": "gm.gist.ac.kr;gist.ac.kr;gist.ac.kr",
        "email": "gm.gist.ac.kr;gist.ac.kr;gist.ac.kr",
        "github": "https://github.com/DMCB-GIST/DoKTra",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "Artificial Intelligence Graduate School;Electrical Engineering and Computer Science",
        "aff_unique_dep": "Graduate School;School of Electrical Engineering and Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.findings-acl.56",
        "title": "Domain Representative Keywords Selection: A Probabilistic Approach",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose a probabilistic approach to select a subset of a target domain representative keywords from a candidate set, contrasting with a context domain. Such a task is crucial for many downstream tasks in natural language processing. To contrast the target domain and the context domain, we adapt the two-component mixture model concept to generate a distribution of candidate keywords. It provides more importance to the distinctive keywords of the target domain than common keywords contrasting with the context domain. To support the representativeness of the selected keywords towards the target domain, we introduce an optimization algorithm for selecting the subset from the generated candidate distribution. We have shown that the optimization algorithm can be efficiently implemented with a near-optimal approximation guarantee. Finally, extensive experiments on multiple domains demonstrate the superiority of our approach over other baselines for the tasks of keyword summary generation and trending keywords selection.",
        "author": "Pritom Saha Akash; Jie Huang; Kevin Chang; Yunyao Li; Lucian Popa; ChengXiang Zhai",
        "authorids": "/p/pritom-saha-akash/; /j/jie-huang/; /k/kevin-chang/; /y/yunyao-li/; /l/lucian-popa/; /c/chengxiang-zhai/",
        "bibtex": "@inproceedings{akash-etal-2022-domain,\n    title = \"Domain Representative Keywords Selection: A Probabilistic Approach\",\n    author = \"Akash, Pritom Saha  and\n      Huang, Jie  and\n      Chang, Kevin  and\n      Li, Yunyao  and\n      Popa, Lucian  and\n      Zhai, ChengXiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.56/\",\n    doi = \"10.18653/v1/2022.findings-acl.56\",\n    pages = \"679--692\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.56.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.56/",
        "pdf_size": 1211904,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=635271127740829140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; Apple, USA + IBM Research, USA; IBM Research, USA; University of Illinois at Urbana-Champaign, USA",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;apple.com;us.ibm.com;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;apple.com;us.ibm.com;illinois.edu",
        "github": "https://github.com/pritomsaha/keyword-selection",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1+2;2;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Apple;IBM",
        "aff_unique_dep": ";Apple Inc.;IBM Research",
        "aff_unique_url": "https://illinois.edu;https://www.apple.com;https://www.ibm.com/research",
        "aff_unique_abbr": "UIUC;Apple;IBM",
        "aff_campus_unique_index": "0;0;0;;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.189",
        "title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Solving crossword puzzles requires diverse reasoning capabilities, access to a vast amount of knowledge about language and the world, and the ability to satisfy the constraints imposed by the structure of the puzzle. In this work, we introduce solving crossword puzzles as a new natural language understanding task. We release a corpus of crossword puzzles collected from the New York Times daily crossword spanning 25 years and comprised of a total of around nine thousand puzzles. These puzzles include a diverse set of clues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank, abbreviations, prefixes/suffixes, wordplay, and cross-lingual, as well as clues that depend on the answers to other clues. We separately release the clue-answer pairs from these puzzles as an open-domain question answering dataset containing over half a million unique clue-answer pairs. For the question answering task, our baselines include several sequence-to-sequence and retrieval-based generative models. We also introduce a non-parametric constraint satisfaction baseline for solving the entire crossword puzzle. Finally, we propose an evaluation framework which consists of several complementary performance metrics.",
        "author": "Saurabh Kulshreshtha; Olga Kovaleva; Namrata Shivagunde; Anna Rumshisky",
        "authorids": "/s/saurabh-kulshreshtha/; /o/olga-kovaleva/; /n/namrata-shivagunde/; /a/anna-rumshisky/",
        "bibtex": "@inproceedings{kulshreshtha-etal-2022-across,\n    title = \"Down and Across: Introducing Crossword-Solving as a New {NLP} Benchmark\",\n    author = \"Kulshreshtha, Saurabh  and\n      Kovaleva, Olga  and\n      Shivagunde, Namrata  and\n      Rumshisky, Anna\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.189/\",\n    doi = \"10.18653/v1/2022.acl-long.189\",\n    pages = \"2648--2659\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.189.pdf",
        "site": "https://aclanthology.org/2022.acl-long.189/",
        "pdf_size": 296631,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18009220957741319901&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Massachusetts Lowell; Department of Computer Science, University of Massachusetts Lowell; Department of Computer Science, University of Massachusetts Lowell; Department of Computer Science, University of Massachusetts Lowell",
        "aff_domain": "cs.uml.edu;cs.uml.edu;cs.uml.edu;cs.uml.edu",
        "email": "cs.uml.edu;cs.uml.edu;cs.uml.edu;cs.uml.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Massachusetts Lowell",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uml.edu",
        "aff_unique_abbr": "UMass Lowell",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lowell",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.105",
        "title": "DuReadervis: A Chinese Dataset for Open-domain Document Visual Question Answering",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Open-domain question answering has been used in a wide range of applications, such as web search and enterprise search, which usually takes clean texts extracted from various formats of documents (e.g., web pages, PDFs, or Word documents) as the information source. However, designing different text extraction approaches is time-consuming and not scalable. In order to reduce human cost and improve the scalability of QA systems, we propose and study an Open-domain Document Visual Question Answering (Open-domain DocVQA) task, which requires answering questions based on a collection of document images directly instead of only document texts, utilizing layouts and visual features additionally. Towards this end, we introduce the first Chinese Open-domain DocVQA dataset called DuReadervis, containing about 15K question-answering pairs and 158K document images from the Baidu search engine. There are three main challenges in DuReadervis: (1) long document understanding, (2) noisy texts, and (3) multi-span answer extraction. The extensive experiments demonstrate that the dataset is challenging. Additionally, we propose a simple approach that incorporates the layout and visual features, and the experimental results show the effectiveness of the proposed approach. The dataset and code will be publicly available at https://github.com/baidu/DuReader/tree/master/DuReader-vis.",
        "author": "Le Qi; Shangwen Lv; Hongyu Li; Jing Liu; Yu Zhang; Qiaoqiao She; Hua Wu; Haifeng Wang; Ting Liu",
        "authorids": "/l/le-qi/; /s/shangwen-lv/; /h/hongyu-li/; /j/jing-liu/; /y/yu-zhang/; /q/qiaoqiao-she/; /h/hua-wu/; /h/haifeng-wang/; /t/ting-liu/",
        "bibtex": "@inproceedings{qi-etal-2022-dureadervis,\n    title = \"$\\textrm{DuReader}_{\\textrm{vis}}$: A {C}hinese Dataset for Open-domain Document Visual Question Answering\",\n    author = \"Qi, Le  and\n      Lv, Shangwen  and\n      Li, Hongyu  and\n      Liu, Jing  and\n      Zhang, Yu  and\n      She, Qiaoqiao  and\n      Wu, Hua  and\n      Wang, Haifeng  and\n      Liu, Ting\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.105/\",\n    doi = \"10.18653/v1/2022.findings-acl.105\",\n    pages = \"1338--1351\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.105.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.105/",
        "pdf_size": 2345214,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15782124635105320320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China + Baidu Inc.; Baidu Inc.; Baidu Inc.; Baidu Inc.; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China + Baidu Inc.; Baidu Inc.; Baidu Inc.; Baidu Inc.; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China",
        "aff_domain": "ir.hit.edu.cn;baidu.com;baidu.com;baidu.com;ir.hit.edu.cn;baidu.com;baidu.com;baidu.com;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;baidu.com;baidu.com;baidu.com;ir.hit.edu.cn;baidu.com;baidu.com;baidu.com;ir.hit.edu.cn",
        "github": "https://github.com/baidu/DuReader/tree/master/DuReader-vis",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;1;1;1;0+1;1;1;1;0",
        "aff_unique_norm": "Harbin Institute of Technology;Baidu",
        "aff_unique_dep": "Research Center for Social Computing and Information Retrieval;Baidu Inc.",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.baidu.com",
        "aff_unique_abbr": "HIT;Baidu",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.8",
        "title": "Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prompt-based paradigm has shown its competitive performance in many NLP tasks. However, its success heavily depends on prompt design, and the effectiveness varies upon the model and training data. In this paper, we propose a novel dual context-guided continuous prompt (DCCP) tuning method. To explore the rich contextual information in language structure and close the gap between discrete prompt tuning and continuous prompt tuning, DCCP introduces two auxiliary training objectives and constructs input in a pair-wise fashion. Experimental results demonstrate that our method is applicable to many NLP tasks, and can often outperform existing prompt tuning methods by a large margin in the few-shot setting.",
        "author": "Jie Zhou; Le Tian; Houjin Yu; Zhou Xiao; Hui Su; Jie Zhou",
        "authorids": "/j/jie-zhou/; /l/le-tian/; /h/houjin-yu/; /z/zhou-xiao/; /h/hui-su/; /j/jie-zhou/",
        "bibtex": "@inproceedings{zhou-etal-2022-dual,\n    title = \"Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning\",\n    author = \"Zhou, Jie  and\n      Tian, Le  and\n      Yu, Houjin  and\n      Xiao, Zhou  and\n      Su, Hui  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.8/\",\n    doi = \"10.18653/v1/2022.findings-acl.8\",\n    pages = \"79--84\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.8.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.8/",
        "pdf_size": 629276,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16683385102193680612&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 2,
        "aff": "Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China",
        "aff_domain": "tencent.com; ; ; ; ; ",
        "email": "tencent.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Tencent",
        "aff_unique_dep": "Pattern Recognition Center",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.361",
        "title": "Dynamic Global Memory for Document-level Argument Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global contextual understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.",
        "author": "Xinya Du; Sha Li; Heng Ji",
        "authorids": "/x/xinya-du/; /s/sha-li/; /h/heng-ji/",
        "bibtex": "@inproceedings{du-etal-2022-dynamic,\n    title = \"Dynamic Global Memory for Document-level Argument Extraction\",\n    author = \"Du, Xinya  and\n      Li, Sha  and\n      Ji, Heng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.361/\",\n    doi = \"10.18653/v1/2022.acl-long.361\",\n    pages = \"5264--5275\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.361.pdf",
        "site": "https://aclanthology.org/2022.acl-long.361/",
        "pdf_size": 1823725,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9212803478102837240&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Illinois Urbana-Champaign; Department of Computer Science, University of Illinois Urbana-Champaign; Department of Computer Science, University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "https://github.com/xinyadu/memory_docie",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.358",
        "title": "Dynamic Prefix-Tuning for Generative Template-based Event Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We consider event extraction in a generative manner with template-based conditional generation. Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information. In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DynPref) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context. Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE.Additionally, our model is proven to be portable to new types of events effectively.",
        "author": "Xiao Liu; Heyan Huang; Ge Shi; Bo Wang",
        "authorids": "/x/xiao-liu/; /h/he-yan-huang/; /g/ge-shi/; /b/bo-wang/",
        "bibtex": "@inproceedings{liu-etal-2022-dynamic,\n    title = \"Dynamic Prefix-Tuning for Generative Template-based Event Extraction\",\n    author = \"Liu, Xiao  and\n      Huang, Heyan  and\n      Shi, Ge  and\n      Wang, Bo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.358/\",\n    doi = \"10.18653/v1/2022.acl-long.358\",\n    pages = \"5216--5228\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.358.pdf",
        "site": "https://aclanthology.org/2022.acl-long.358/",
        "pdf_size": 922443,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10998085604264007877&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications+Key Laboratory of Intelligent Information Processing and Information Security, Ministry of Industry and Information Technology, China+Southeast Academy of Information Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications+Key Laboratory of Intelligent Information Processing and Information Security, Ministry of Industry and Information Technology, China+Southeast Academy of Information Technology, Beijing Institute of Technology; Faculty of Information Technology, Beijing University of Technology; School of Computer Science and Technology, Beijing Institute of Technology+Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications+Key Laboratory of Intelligent Information Processing and Information Security, Ministry of Industry and Information Technology, China+Southeast Academy of Information Technology, Beijing Institute of Technology",
        "aff_domain": "bit.edu.cn;bit.edu.cn;bjut.edu.cn;bit.edu.cn",
        "email": "bit.edu.cn;bit.edu.cn;bjut.edu.cn;bit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2+0;0+1+2+0;3;0+1+2+0",
        "aff_unique_norm": "Beijing Institute of Technology;Beijing Engineering Research Center;Ministry of Industry and Information Technology;Beijing University of Technology",
        "aff_unique_dep": "School of Computer Science and Technology;High Volume Language Information Processing and Cloud Computing Applications;Key Laboratory of Intelligent Information Processing and Information Security;Faculty of Information Technology",
        "aff_unique_url": "http://www.bit.edu.cn/;;;http://www.bit.edu.cn/",
        "aff_unique_abbr": "BIT;;;BIT",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0+0;0+0+0+0;0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.10",
        "title": "Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Dialogue State Tracking (DST) aims to keep track of users\u2019 intentions during the course of a conversation. In DST, modelling the relations among domains and slots is still an under-studied problem. Existing approaches that have considered such relations generally fall short in: (1) fusing prior slot-domain membership relations and dialogue-aware dynamic slot relations explicitly, and (2) generalizing to unseen domains. To address these issues, we propose a novel Dynamic Schema Graph Fusion Network (DSGFNet), which generates a dynamic schema graph to explicitly fuse the prior slot-domain membership relations and dialogue-aware dynamic slot relations. It also uses the schemata to facilitate knowledge transfer to new domains. DSGFNet consists of a dialogue utterance encoder, a schema graph encoder, a dialogue-aware schema graph evolving network, and a schema graph enhanced dialogue state decoder. Empirical results on benchmark datasets (i.e., SGD, MultiWOZ2.1, and MultiWOZ2.2), show that DSGFNet outperforms existing methods.",
        "author": "Yue Feng; Aldo Lipani; Fanghua Ye; Qiang Zhang; Emine Yilmaz",
        "authorids": "/y/yue-feng/; /a/aldo-lipani/; /f/fanghua-ye/; /q/qiang-zhang/; /e/emine-yilmaz/",
        "bibtex": "@inproceedings{feng-etal-2022-dynamic,\n    title = \"Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking\",\n    author = \"Feng, Yue  and\n      Lipani, Aldo  and\n      Ye, Fanghua  and\n      Zhang, Qiang  and\n      Yilmaz, Emine\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.10/\",\n    doi = \"10.18653/v1/2022.acl-long.10\",\n    pages = \"115--126\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.10.pdf",
        "site": "https://aclanthology.org/2022.acl-long.10/",
        "pdf_size": 583889,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8320767434994483184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University College London, London, UK; University College London, London, UK; University College London, London, UK; Zhejiang University, Hangzhou, China; University College London, London, UK",
        "aff_domain": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;zju.edu.cn;ucl.ac.uk",
        "email": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;zju.edu.cn;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University College London;Zhejiang University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucl.ac.uk;http://www.zju.edu.cn",
        "aff_unique_abbr": "UCL;ZJU",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "London;Hangzhou",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "2022.findings-acl.32",
        "title": "Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source. This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. Previous work has attempted to mitigate this problem by regularizing specific terms from pre-defined static dictionaries. While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the dictionaries require regular manual updates from human experts. In this paper, we propose to automatically identify and reduce spurious correlations using attribution methods with dynamic refinement of the list of terms that need to be regularized during training. Our approach is flexible and improves the cross-corpora performance over previous work independently and in combination with pre-defined dictionaries.",
        "author": "Tulika Bose; Nikolaos Aletras; Irina Illina; Dominique Fohr",
        "authorids": "/t/tulika-bose/; /n/nikolaos-aletras/; /i/irina-illina/; /d/dominique-fohr/",
        "bibtex": "@inproceedings{bose-etal-2022-dynamically,\n    title = \"Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection\",\n    author = \"Bose, Tulika  and\n      Aletras, Nikolaos  and\n      Illina, Irina  and\n      Fohr, Dominique\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.32/\",\n    doi = \"10.18653/v1/2022.findings-acl.32\",\n    pages = \"372--382\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.32.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.32/",
        "pdf_size": 393739,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13881935556930719643&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Universite de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France; University of Sheffield, United Kingdom; Universite de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France; Universite de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France",
        "aff_domain": "loria.fr;sheffield.ac.uk;loria.fr;loria.fr",
        "email": "loria.fr;sheffield.ac.uk;loria.fr;loria.fr",
        "github": "https://github.com/tbose20/D-Ref",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Universite de Lorraine;University of Sheffield",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.univ-lorraine.fr;https://www.sheffield.ac.uk",
        "aff_unique_abbr": "UL;Sheffield",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nancy;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "France;United Kingdom"
    },
    {
        "id": "2022.findings-acl.311",
        "title": "E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve. More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer. Empirical results suggest that this benchmark is very challenging for some state-of-the-art models for both explanation generation and analogical question answering tasks, which invites further research in this area.",
        "author": "Jiangjie Chen; Rui Xu; Ziquan Fu; Wei Shi; Zhongqiao Li; Xinbo Zhang; Changzhi Sun; Lei Li; Yanghua Xiao; Hao Zhou",
        "authorids": "/j/jiangjie-chen/; /r/rui-xu/; /z/ziquan-fu/; /w/wei-shi/; /z/zhongqiao-li/; /x/xinbo-zhang/; /c/changzhi-sun/; /l/lei-li/; /y/yanghua-xiao/; /h/hao-zhou/",
        "bibtex": "@inproceedings{chen-etal-2022-e,\n    title = \"{E}-{KAR}: A Benchmark for Rationalizing Natural Language Analogical Reasoning\",\n    author = \"Chen, Jiangjie  and\n      Xu, Rui  and\n      Fu, Ziquan  and\n      Shi, Wei  and\n      Li, Zhongqiao  and\n      Zhang, Xinbo  and\n      Sun, Changzhi  and\n      Li, Lei  and\n      Xiao, Yanghua  and\n      Zhou, Hao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.311/\",\n    doi = \"10.18653/v1/2022.findings-acl.311\",\n    pages = \"3941--3955\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.311.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.311/",
        "pdf_size": 1219513,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1146901476040987198&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University+ByteDance AI Lab; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University; Brain Technologies, Inc.; South China University of Technology; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University; ByteDance AI Lab; ByteDance AI Lab; University of California Santa Barbara; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University+Fudan-Aishu Cognitive Intelligence Joint Research Center; ByteDance AI Lab",
        "aff_domain": "fudan.edu.cn; ; ; ; ; ;bytedance.com; ; ;fudan.edu.cn",
        "email": "fudan.edu.cn; ; ; ; ; ;bytedance.com; ; ;fudan.edu.cn",
        "github": "",
        "project": "https://ekar-leaderboard.github.io",
        "author_num": 10,
        "aff_unique_index": "0+1;0;2;3;0;1;1;4;0+0;1",
        "aff_unique_norm": "Fudan University;ByteDance;Brain Technologies, Inc.;South China University of Technology;University of California, Santa Barbara",
        "aff_unique_dep": "School of Computer Science;AI Lab;;;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.bytedance.com;;https://www.scut.edu.cn;https://www.ucsb.edu",
        "aff_unique_abbr": "Fudan;ByteDance;;SCUT;UCSB",
        "aff_campus_unique_index": "0;0;0;2;0",
        "aff_campus_unique": "Shanghai;;Santa Barbara",
        "aff_country_unique_index": "0+0;0;1;0;0;0;0;1;0+0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.359",
        "title": "E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Building huge and highly capable language models has been a trend in the past years. Despite their great performance, they incur high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, called E-LANG, which distributes the inference between large accurate Super-models and light-weight Swift models. To this end, a decision making module routes the inputs to Super or Swift models based on the energy characteristics of the representations in the latent space. This method is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, reassembling of modules, or re-training. Unlike existing methods that are only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation. The E-LANG performance is verified through a set of experiments with T5 and BERT backbones on GLUE, SuperGLUE, and WMT. In particular, we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2X less computations. Code and demo are available in supplementary materials.",
        "author": "Mohammad Akbari; Amin Banitalebi-Dehkordi; Yong Zhang",
        "authorids": "/m/mohammad-akbari/; /a/amin-banitalebi-dehkordi/; /y/yong-zhang/",
        "bibtex": "@inproceedings{akbari-etal-2022-e,\n    title = \"{E}-{LANG}: Energy-Based Joint Inferencing of Super and Swift Language Models\",\n    author = \"Akbari, Mohammad  and\n      Banitalebi-Dehkordi, Amin  and\n      Zhang, Yong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.359/\",\n    doi = \"10.18653/v1/2022.acl-long.359\",\n    pages = \"5229--5244\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.359.pdf",
        "site": "https://aclanthology.org/2022.acl-long.359/",
        "pdf_size": 1570190,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3848088173937684382&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Huawei Technologies Canada Co., Ltd.; Huawei Technologies Canada Co., Ltd.; Huawei Technologies Canada Co., Ltd.",
        "aff_domain": "huawei.com;huawei.com;huawei.com",
        "email": "huawei.com;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Huawei",
        "aff_unique_dep": "Huawei Technologies",
        "aff_unique_url": "https://www.huawei.com/ca-en/",
        "aff_unique_abbr": "Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.560",
        "title": "EAG: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior performance against the conventional MNMT by constructing multi-way aligned corpus, i.e., aligning bilingual training examples from different language pairs when either their source or target sides are identical. However, since exactly identical sentences from different language pairs are scarce, the power of the multi-way aligned corpus is limited by its scale. To handle this problem, this paper proposes \u201cExtract and Generate\u201d (EAG), a two-step approach to construct large-scale and high-quality multi-way aligned corpus from bilingual data. Specifically, we first extract candidate aligned examples by pairing the bilingual examples from different language pairs with highly similar source or target sentences; and then generate the final aligned examples from the candidates with a well-trained generation model. With this two-step pipeline, EAG can construct a large-scale and multi-way aligned corpus whose diversity is almost identical to the original bilingual corpus. Experiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show that the proposed method achieves significant improvements over strong baselines, with +1.1 and +1.4 BLEU points improvements on the two datasets respectively.",
        "author": "Yulin Xu; Zhen Yang; Fandong Meng; Jie Zhou",
        "authorids": "/y/yulin-xu/; /z/zhen-yang/; /f/fandong-meng/; /j/jie-zhou/",
        "bibtex": "@inproceedings{xu-etal-2022-eag,\n    title = \"{EAG}: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation\",\n    author = \"Xu, Yulin  and\n      Yang, Zhen  and\n      Meng, Fandong  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.560/\",\n    doi = \"10.18653/v1/2022.acl-long.560\",\n    pages = \"8141--8153\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.560.pdf",
        "site": "https://aclanthology.org/2022.acl-long.560/",
        "pdf_size": 459171,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8210144045609380135&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 4,
        "aff": "Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "gmail.com;tencent.com;tencent.com;tencent.com",
        "email": "gmail.com;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tencent",
        "aff_unique_dep": "Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.216",
        "title": "ECO v1: Towards Event-Centric Opinion Mining",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Events are considered as the fundamental building blocks of the world. Mining event-centric opinions can benefit decision making, people communication, and social good. Unfortunately, there is little literature addressing event-centric opinion mining, although which significantly diverges from the well-studied entity-centric opinion mining in connotation, structure, and expression. In this paper, we propose and formulate the task of event-centric opinion mining based on event-argument structure and expression categorizing theory. We also benchmark this task by constructing a pioneer corpus and designing a two-step benchmark framework. Experiment results show that event-centric opinion mining is feasible and challenging, and the proposed task, dataset, and baselines are beneficial for future studies.",
        "author": "Ruoxi Xu; Hongyu Lin; Meng Liao; Xianpei Han; Jin Xu; Wei Tan; Yingfei Sun; Le Sun",
        "authorids": "/r/ruoxi-xu/; /h/hongyu-lin/; /m/meng-liao/; /x/xianpei-han/; /j/jin-xu/; /w/wei-tan/; /y/yingfei-sun/; /l/le-sun/",
        "bibtex": "@inproceedings{xu-etal-2022-eco,\n    title = \"{ECO} v1: Towards Event-Centric Opinion Mining\",\n    author = \"Xu, Ruoxi  and\n      Lin, Hongyu  and\n      Liao, Meng  and\n      Han, Xianpei  and\n      Xu, Jin  and\n      Tan, Wei  and\n      Sun, Yingfei  and\n      Sun, Le\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.216/\",\n    doi = \"10.18653/v1/2022.findings-acl.216\",\n    pages = \"2743--2753\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.216.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.216/",
        "pdf_size": 727581,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4416339720097054154&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Institute of Software, Chinese Academy of Sciences; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Institute of Software, Chinese Academy of Sciences; Data Quality Team, WeChat, Tencent Inc.; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Institute of Software, Chinese Academy of Sciences; Data Quality Team, WeChat, Tencent Inc.; Data Quality Team, WeChat, Tencent Inc.; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Institute of Software, Chinese Academy of Sciences",
        "aff_domain": "iscas.ac.cn;iscas.ac.cn;tencent.com;iscas.ac.cn;tencent.com;tencent.com;ucas.ac.cn;iscas.ac.cn",
        "email": "iscas.ac.cn;iscas.ac.cn;tencent.com;iscas.ac.cn;tencent.com;tencent.com;ucas.ac.cn;iscas.ac.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1+2;0+1+2;3;0+1+2;3;3;4;0+1+2",
        "aff_unique_norm": "Chinese Information Processing Laboratory;State Key Laboratory of Computer Science;Chinese Academy of Sciences;Tencent;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Information Processing;;Institute of Software;Data Quality Team;School of Electronic, Electrical and Communication Engineering",
        "aff_unique_url": ";;http://www.ios.ac.cn;https://www.tencent.com;http://www.ucas.ac.cn",
        "aff_unique_abbr": ";;CAS;Tencent;UCAS",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0;0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.295",
        "title": "ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "State-of-the-art neural models typically encode document-query pairs using cross-attention for re-ranking. To this end, models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach. These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost. This paper proposes a new training and inference paradigm for re-ranking. We propose to finetune a pretrained encoder-decoder model using in the form of document to query generation. Subsequently, we show that this encoder-decoder architecture can be decomposed into a decoder-only language model during inference. This results in significant inference time speedups since the decoder-only architecture only needs to learn to interpret static encoder embeddings during inference. Our experiments show that this new paradigm achieves results that are comparable to the more expensive cross-attention ranking approaches while being up to 6.8X faster. We believe this work paves the way for more efficient neural rankers that leverage large pretrained models.",
        "author": "Kai Hui; Honglei Zhuang; Tao Chen; Zhen Qin; Jing Lu; Dara Bahri; Ji Ma; Jai Gupta; Cicero Nogueira dos Santos; Yi Tay; Donald Metzler",
        "authorids": "/k/kai-hui/; /h/honglei-zhuang/; /t/tao-chen/; /z/zhen-qin/; /j/jing-lu/; /d/dara-bahri/; /j/ji-ma/; /j/jai-gupta/; /c/cicero-dos-santos/; /y/yi-tay/; /d/donald-metzler/",
        "bibtex": "@inproceedings{hui-etal-2022-ed2lm,\n    title = \"{ED}2{LM}: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference\",\n    author = \"Hui, Kai  and\n      Zhuang, Honglei  and\n      Chen, Tao  and\n      Qin, Zhen  and\n      Lu, Jing  and\n      Bahri, Dara  and\n      Ma, Ji  and\n      Gupta, Jai  and\n      Nogueira dos Santos, Cicero  and\n      Tay, Yi  and\n      Metzler, Donald\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.295/\",\n    doi = \"10.18653/v1/2022.findings-acl.295\",\n    pages = \"3747--3758\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.295.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.295/",
        "pdf_size": 355192,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14847752361203420265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com; ; ; ; ; ; ; ; ; ; ",
        "email": "google.com; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.283",
        "title": "EICO: Improving Few-Shot Text Classification via Explicit and Implicit Consistency Regularization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While the prompt-based fine-tuning methods had advanced few-shot natural language understanding tasks, self-training methods are also being explored. This work revisits the consistency regularization in self-training and presents explicit and implicit consistency regularization enhanced language model (EICO). By employing both explicit and implicit consistency regularization, EICO advances the performance of prompt-based few-shot text classification. For implicit consistency regularization, we generate pseudo-label from the weakly-augmented view and predict pseudo-label from the strongly-augmented view. For explicit consistency regularization, we minimize the difference between the prediction of the augmentation view and the prediction of the original view. We conducted extensive experiments on six text classification datasets and found that with sixteen labeled examples, EICO achieves competitive performance compared to existing self-training few-shot learning methods.",
        "author": "Lei Zhao; Cheng Yao",
        "authorids": "/l/lei-zhao/; /c/cheng-yao/",
        "bibtex": "@inproceedings{zhao-yao-2022-eico,\n    title = \"{EICO}: Improving Few-Shot Text Classification via Explicit and Implicit Consistency Regularization\",\n    author = \"Zhao, Lei  and\n      Yao, Cheng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.283/\",\n    doi = \"10.18653/v1/2022.findings-acl.283\",\n    pages = \"3582--3587\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.283.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.283/",
        "pdf_size": 404519,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7775366613934222548&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.220",
        "title": "ELLE: Efficient Lifelong Pre-training for Emerging Data",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM\u2019s width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at https://github.com/thunlp/ELLE.",
        "author": "Yujia Qin; Jiajie Zhang; Yankai Lin; Zhiyuan Liu; Peng Li; Maosong Sun; Jie Zhou",
        "authorids": "/y/yujia-qin/; /j/jiajie-zhang/; /y/yankai-lin/; /z/zhiyuan-liu/; /p/peng-li/; /m/maosong-sun/; /j/jie-zhou/",
        "bibtex": "@inproceedings{qin-etal-2022-elle,\n    title = \"{ELLE}: Efficient Lifelong Pre-training for Emerging Data\",\n    author = \"Qin, Yujia  and\n      Zhang, Jiajie  and\n      Lin, Yankai  and\n      Liu, Zhiyuan  and\n      Li, Peng  and\n      Sun, Maosong  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.220/\",\n    doi = \"10.18653/v1/2022.findings-acl.220\",\n    pages = \"2789--2810\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.220.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.220/",
        "pdf_size": 1453111,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12916330843873919402&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Technology, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc.; Department of Computer Science and Technology, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Artificial Intelligence; Institute for AI Industry Research (AIR), Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Artificial Intelligence+Jiangsu Collaborative Innovation Center for Language Ability, Xuzhou, China; Pattern Recognition Center, WeChat AI, Tencent Inc.",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ; ; ;",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ; ; ;",
        "github": "https://github.com/thunlp/ELLE",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1+0;0+1+0;2;0+1+0+0+3;0;0+1+0+0+3+4;2",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Tencent;Beijing Academy of Artificial Intelligence;Jiangsu Collaborative Innovation Center for Language Ability",
        "aff_unique_dep": "Department of Computer Science and Technology;;Pattern Recognition Center, WeChat AI;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.tencent.com;https://www.baaic.cn;",
        "aff_unique_abbr": "THU;;Tencent;BAAI;",
        "aff_campus_unique_index": "0+0;0+0;0+0+2;0+0+2+3",
        "aff_campus_unique": "Beijing;;Shanghai;Xuzhou",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0+0+0;0;0+0+0+0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.305",
        "title": "EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model\u2019s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model\u2019s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",
        "author": "Bugeun Kim; Kyung Seo Ki; Sangkyu Rhim; Gahgene Gweon",
        "authorids": "/b/bugeun-kim/; /k/kyung-seo-ki/; /s/sangkyu-rhim/; /g/gahgene-gweon/",
        "bibtex": "@inproceedings{kim-etal-2022-ept,\n    title = \"{EPT}-{X}: An Expression-Pointer Transformer model that generates e{X}planations for numbers\",\n    author = \"Kim, Bugeun  and\n      Ki, Kyung Seo  and\n      Rhim, Sangkyu  and\n      Gweon, Gahgene\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.305/\",\n    doi = \"10.18653/v1/2022.acl-long.305\",\n    pages = \"4442--4458\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.305.pdf",
        "site": "https://aclanthology.org/2022.acl-long.305/",
        "pdf_size": 1303943,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=842305141490952473&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Intelligence and Information, Seoul National University; Department of Intelligence and Information, Seoul National University; Department of Intelligence and Information, Seoul National University; Department of Intelligence and Information, Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Intelligence and Information",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.acl-long.52",
        "title": "Early Stopping Based on Unlabeled Samples in Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Early stopping, which is widely used to prevent overfitting, is generally based on a separate validation set. However, in low resource settings, validation-based stopping can be risky because a small validation set may not be sufficiently representative, and the reduction in the number of samples by validation split may result in insufficient samples for training. In this study, we propose an early stopping method that uses unlabeled samples. The proposed method is based on confidence and class distribution similarities. To further improve the performance, we present a calibration method to better estimate the class distribution of the unlabeled samples. The proposed method is advantageous because it does not require a separate validation set and provides a better stopping point by using a large unlabeled set. Extensive experiments are conducted on five text classification datasets and several stop-methods are compared. Our results show that the proposed model even performs better than using an additional validation set as well as the existing stop-methods, in both balanced and imbalanced data settings. Our code is available at https://github.com/DMCB-GIST/BUS-stop.",
        "author": "HongSeok Choi; Dongha Choi; Hyunju Lee",
        "authorids": "/h/hongseok-choi/; /d/dongha-choi/; /h/hyunju-lee/",
        "bibtex": "@inproceedings{choi-etal-2022-early,\n    title = \"Early Stopping Based on Unlabeled Samples in Text Classification\",\n    author = \"Choi, HongSeok  and\n      Choi, Dongha  and\n      Lee, Hyunju\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.52/\",\n    doi = \"10.18653/v1/2022.acl-long.52\",\n    pages = \"708--718\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.52.pdf",
        "site": "https://aclanthology.org/2022.acl-long.52/",
        "pdf_size": 821993,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16474904277781994901&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Electrical Engineering and Computer Science; Arti\ufb01cial Intelligence Graduate School; School of Electrical Engineering and Computer Science + Arti\ufb01cial Intelligence Graduate School",
        "aff_domain": "gist.ac.kr;gm.gist.ac.kr;gist.ac.kr",
        "email": "gist.ac.kr;gm.gist.ac.kr;gist.ac.kr",
        "github": "https://github.com/DMCB-GIST/BUS-stop",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "Electrical Engineering and Computer Science;Artificial Intelligence Graduate School",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science;Artificial Intelligence",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.acl-long.348",
        "title": "Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-centric Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generating educational questions of fairytales or storybooks is vital for improving children\u2019s literacy ability. However, it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness. In this paper, we propose a novel question generation method that first learns the question type distribution of an input story paragraph, and then summarizes salient events which can be used to generate high-cognitive-demand questions. To train the event-centric summarizer, we finetune a pre-trained transformer-based sequence-to-sequence model using silver samples composed by educational question-answer pairs. On a newly proposed educational question-answering dataset FairytaleQA, we show good performance of our method on both automatic and human evaluation metrics. Our work indicates the necessity of decomposing question type distribution learning and event-centric summary generation for educational question generation.",
        "author": "Zhenjie Zhao; Yufang Hou; Dakuo Wang; Mo Yu; Chengzhong Liu; Xiaojuan Ma",
        "authorids": "/z/zhenjie-zhao/; /y/yufang-hou/; /d/dakuo-wang/; /m/mo-yu/; /c/chengzhong-liu/; /x/xiaojuan-ma/",
        "bibtex": "@inproceedings{zhao-etal-2022-educational,\n    title = \"Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-centric Summarization\",\n    author = \"Zhao, Zhenjie  and\n      Hou, Yufang  and\n      Wang, Dakuo  and\n      Yu, Mo  and\n      Liu, Chengzhong  and\n      Ma, Xiaojuan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.348/\",\n    doi = \"10.18653/v1/2022.acl-long.348\",\n    pages = \"5073--5085\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.348.pdf",
        "site": "https://aclanthology.org/2022.acl-long.348/",
        "pdf_size": 602729,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12120403032363451047&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Nanjing University of Information Science and Technology+Nankai University; IBM Research Europe; IBM Research+WeChat AI; WeChat AI; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology",
        "aff_domain": "nuist.edu.cn;ie.ibm.com;ibm.com;tencent.com;connect.ust.hk;cse.ust.hk",
        "email": "nuist.edu.cn;ie.ibm.com;ibm.com;tencent.com;connect.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;2+3;3;4;4",
        "aff_unique_norm": "Nanjing University of Information Science and Technology;Nankai University;IBM;WeChat;Hong Kong University of Science and Technology",
        "aff_unique_dep": ";;Research;WeChat AI;",
        "aff_unique_url": "http://www.nuist.edu.cn;http://www.nankai.edu.cn;https://www.ibm.com/research/europe;https://www.wechat.com;https://www.ust.hk",
        "aff_unique_abbr": ";NKU;IBM Research Europe;WeChat AI;HKUST",
        "aff_campus_unique_index": ";;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0;1;2+0;0;0;0",
        "aff_country_unique": "China;Unknown;United States"
    },
    {
        "id": "2022.acl-long.291",
        "title": "Effective Token Graph Modeling using a Novel Labeling Strategy for Structured Sentiment Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The state-of-the-art model for structured sentiment analysis casts the task as a dependency parsing problem, which has some limitations: (1) The label proportions for span prediction and span relation prediction are imbalanced. (2) The span lengths of sentiment tuple components may be very large in this task, which will further exacerbates the imbalance problem. (3) Two nodes in a dependency graph cannot have multiple arcs, therefore some overlapped sentiment tuples cannot be recognized. In this work, we propose nichetargeting solutions for these issues. First, we introduce a novel labeling strategy, which contains two sets of token pair labels, namely essential label set and whole label set. The essential label set consists of the basic labels for this task, which are relatively balanced and applied in the prediction layer. The whole label set includes rich labels to help our model capture various token relations, which are applied in the hidden layer to softly influence our model. Moreover, we also propose an effective model to well collaborate with our labeling strategy, which is equipped with the graph attention networks to iteratively refine token representations, and the adaptive multi-label classifier to dynamically predict multiple relations between token pairs. We perform extensive experiments on 5 benchmark datasets in four languages. Experimental results show that our model outperforms previous SOTA models by a large margin.",
        "author": "Wenxuan Shi; Fei Li; Jingye Li; Hao Fei; Donghong Ji",
        "authorids": "/w/wenxuan-shi/; /f/fei-li/; /j/jingye-li/; /h/hao-fei/; /d/donghong-ji/",
        "bibtex": "@inproceedings{shi-etal-2022-effective,\n    title = \"Effective Token Graph Modeling using a Novel Labeling Strategy for Structured Sentiment Analysis\",\n    author = \"Shi, Wenxuan  and\n      Li, Fei  and\n      Li, Jingye  and\n      Fei, Hao  and\n      Ji, Donghong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.291/\",\n    doi = \"10.18653/v1/2022.acl-long.291\",\n    pages = \"4232--4241\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.291.pdf",
        "site": "https://aclanthology.org/2022.acl-long.291/",
        "pdf_size": 445897,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7820101157140966175&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 5,
        "aff": "Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China",
        "aff_domain": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn",
        "email": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn",
        "github": "https://github.com/Xgswlg/TGLS",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Wuhan University",
        "aff_unique_dep": "School of Cyber Science and Engineering",
        "aff_unique_url": "http://www.whu.edu.cn",
        "aff_unique_abbr": "WHU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Wuhan",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.111",
        "title": "Effective Unsupervised Constrained Text Generation based on Perturbed Masking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step. Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit. Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty. Since PMCTG does not require supervised data, it could be applied to different generation tasks. We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords- to-sentence generation and paraphrasing.",
        "author": "Yingwen Fu; Wenjie Ou; Zhou Yu; Yue Lin",
        "authorids": "/y/yingwen-fu/; /w/wenjie-ou/; /z/zhou-yu/; /y/yue-lin/",
        "bibtex": "@inproceedings{fu-etal-2022-effective,\n    title = \"Effective Unsupervised Constrained Text Generation based on Perturbed Masking\",\n    author = \"Fu, Yingwen  and\n      Ou, Wenjie  and\n      Yu, Zhou  and\n      Lin, Yue\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.111/\",\n    doi = \"10.18653/v1/2022.findings-acl.111\",\n    pages = \"1417--1427\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.111.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.111/",
        "pdf_size": 333446,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16723850692683834358&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "Guangdong University of Foreign Studies; NetEase Games AI Lab + Columbia University; Columbia University; NetEase Games AI Lab",
        "aff_domain": "gdufs.edu.cn;corp.netease.com;columbia.edu;corp.netease.com",
        "email": "gdufs.edu.cn;corp.netease.com;columbia.edu;corp.netease.com",
        "github": "https://github.com/fyinh/PMCTG",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;2;1",
        "aff_unique_norm": "Guangdong University of Foreign Studies;NetEase Games;Columbia University",
        "aff_unique_dep": ";AI Lab;",
        "aff_unique_url": "http://www.gdufs.edu.cn;https://game.163.com;https://www.columbia.edu",
        "aff_unique_abbr": "GDUFS;NetEase Games AI Lab;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.36",
        "title": "Efficient Argument Structure Extraction with Transfer Learning and Active Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The automation of extracting argument structures faces a pair of challenges on (1) encoding long-term contexts to facilitate comprehensive understanding, and (2) improving data efficiency since constructing high-quality argument structures is time-consuming. In this work, we propose a novel context-aware Transformer-based argument structure prediction model which, on five different domains, significantly outperforms models that rely on features or only encode limited contexts. To tackle the difficulty of data annotation, we examine two complementary methods: (i) transfer learning to leverage existing annotated data to boost model performance in a new target domain, and (ii) active learning to strategically identify a small amount of samples for annotation. We further propose model-independent sample acquisition strategies, which can be generalized to diverse domains. With extensive experiments, we show that our simple-yet-effective acquisition strategies yield competitive results against three strong comparisons. Combined with transfer learning, substantial F1 score boost (5-25) can be further achieved during the early iterations of active learning across domains.",
        "author": "Xinyu Hua; Lu Wang",
        "authorids": "/x/xinyu-hua/; /l/lu-wang/",
        "bibtex": "@inproceedings{hua-wang-2022-efficient,\n    title = \"Efficient Argument Structure Extraction with Transfer Learning and Active Learning\",\n    author = \"Hua, Xinyu  and\n      Wang, Lu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.36/\",\n    doi = \"10.18653/v1/2022.findings-acl.36\",\n    pages = \"423--437\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.36.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.36/",
        "pdf_size": 728271,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13459303793527235287&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Bloomberg; Computer Science and Engineering, University of Michigan",
        "aff_domain": "bloomberg.net;umich.edu",
        "email": "bloomberg.net;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Bloomberg;University of Michigan",
        "aff_unique_dep": ";Computer Science and Engineering",
        "aff_unique_url": "https://www.bloomberg.com;https://www.umich.edu",
        "aff_unique_abbr": "Bloomberg;UM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.79",
        "title": "Efficient Classification of Long Documents Using Transformers",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets \u2014 both in terms of accuracy as well as time and space overheads. Our datasets cover binary, multi-class, and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or towards the end of the document). Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets. These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models.",
        "author": "Hyunji Park; Yogarshi Vyas; Kashif Shah",
        "authorids": "/h/hyunji-park/; /y/yogarshi-vyas/; /k/kashif-shah/",
        "bibtex": "@inproceedings{park-etal-2022-efficient,\n    title = \"Efficient Classification of Long Documents Using Transformers\",\n    author = \"Park, Hyunji  and\n      Vyas, Yogarshi  and\n      Shah, Kashif\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.79/\",\n    doi = \"10.18653/v1/2022.acl-short.79\",\n    pages = \"702--709\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.79.pdf",
        "site": "https://aclanthology.org/2022.acl-short.79/",
        "pdf_size": 212372,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12121850303747036794&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Illinois\u2217; AWS AI Labs; Microsoft\u2217",
        "aff_domain": "illinois.edu;amazon.com;microsoft.com",
        "email": "illinois.edu;amazon.com;microsoft.com",
        "github": "https://github.com/amazon-research/efficient-longdoc-classification",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Illinois;Amazon;Microsoft",
        "aff_unique_dep": ";AWS AI Labs;Microsoft Corporation",
        "aff_unique_url": "https://illinois.edu;https://aws.amazon.com;https://www.microsoft.com",
        "aff_unique_abbr": "UIUC;AWS;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.154",
        "title": "Efficient Cluster-Based k-Nearest-Neighbor Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as a non-parametric solution for domain adaptation in neural machine translation (NMT). It aims to alleviate the performance degradation of advanced MT systems in translating out-of-domain sentences by coordinating with an additional token-level feature-based retrieval module constructed from in-domain data. Previous studies (Khandelwal et al., 2021; Zheng et al., 2021) have already demonstrated that non-parametric NMT is even superior to models fine-tuned on out-of-domain data. In spite of this success, kNN retrieval is at the expense of high latency, in particular for large datastores. To make it practical, in this paper, we explore a more efficient kNN-MT and propose to use clustering to improve the retrieval efficiency. Concretely, we first propose a cluster-based Compact Network for feature reduction in a contrastive learning manner to compress context features into 90+% lower dimensional vectors. We then suggest a cluster-based pruning solution to filter out 10% 40% redundant nodes in large datastores while retaining translation quality. Our proposed methods achieve better or comparable performance while reducing up to 57% inference latency against the advanced non-parametric MT model on several machine translation benchmarks. Experimental results indicate that the proposed methods maintain the most useful information of the original datastore and the Compact Network shows good generalization on unseen domains. Codes are available at https://github.com/tjunlp-lab/PCKMT.",
        "author": "Dexin Wang; Kai Fan; Boxing Chen; Deyi Xiong",
        "authorids": "/d/dexin-wang/; /k/kai-fan/; /b/boxing-chen/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{wang-etal-2022-efficient,\n    title = \"Efficient Cluster-Based $k$-Nearest-Neighbor Machine Translation\",\n    author = \"Wang, Dexin  and\n      Fan, Kai  and\n      Chen, Boxing  and\n      Xiong, Deyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.154/\",\n    doi = \"10.18653/v1/2022.acl-long.154\",\n    pages = \"2175--2187\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.154.pdf",
        "site": "https://aclanthology.org/2022.acl-long.154/",
        "pdf_size": 4115029,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12205140729084765125&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "Tianjin University; Alibaba DAMO Academy; Alibaba DAMO Academy; Tianjin University",
        "aff_domain": "tju.edu.cn;alibaba-inc.com;alibaba-inc.com;tju.edu.cn",
        "email": "tju.edu.cn;alibaba-inc.com;alibaba-inc.com;tju.edu.cn",
        "github": "https://github.com/tjunlp-lab/PCKMT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Tianjin University;Alibaba Group",
        "aff_unique_dep": ";DAMO Academy",
        "aff_unique_url": "http://www.tju.edu.cn;https://www.alibaba-group.com",
        "aff_unique_abbr": "TJU;Alibaba DAMO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.194",
        "title": "Efficient Hyper-parameter Search for Knowledge Graph Embedding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage. Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves 9.1% average relative improvement for four embedding models on the large-scale KGs in open graph benchmark. Our code is released in https://github.com/AutoML-Research/KGTuner.",
        "author": "Yongqi Zhang; Zhanke Zhou; Quanming Yao; Yong Li",
        "authorids": "/y/yongqi-zhang/; /z/zhanke-zhou/; /q/quanming-yao/; /y/yong-li/",
        "bibtex": "@inproceedings{zhang-etal-2022-efficient,\n    title = \"Efficient Hyper-parameter Search for Knowledge Graph Embedding\",\n    author = \"Zhang, Yongqi  and\n      Zhou, Zhanke  and\n      Yao, Quanming  and\n      Li, Yong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.194/\",\n    doi = \"10.18653/v1/2022.acl-long.194\",\n    pages = \"2715--2735\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.194.pdf",
        "site": "https://aclanthology.org/2022.acl-long.194/",
        "pdf_size": 5098413,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12083049623671045174&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 0,
        "aff": "Paradigm Inc., Beijing, China; Hong Kong Baptist University, Hong Kong, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China",
        "aff_domain": "4paradigm.com;comp.hkbu.edu.hk;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "4paradigm.com;comp.hkbu.edu.hk;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/AutoML-Research/KGTuner",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Paradigm Inc.;Hong Kong Baptist University;Tsinghua University",
        "aff_unique_dep": ";;Department of Electronic Engineering",
        "aff_unique_url": ";https://www.hkbu.edu.hk;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": ";HKBU;THU",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";Hong Kong;Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.90",
        "title": "Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sentence compression reduces the length of text by removing non-essential content while preserving important facts and grammaticality. Unsupervised objective driven methods for sentence compression can be used to create customized models without the need for ground-truth training data, while allowing flexibility in the objective function(s) that are used for learning and inference. Recent unsupervised sentence compression approaches use custom objectives to guide discrete search; however, guided search is expensive at inference time. In this work, we explore the use of reinforcement learning to train effective sentence compression models that are also fast when generating predictions. In particular, we cast the task as binary sequence labelling and fine-tune a pre-trained transformer using a simple policy gradient approach. Our approach outperforms other unsupervised models while also being more efficient at inference time.",
        "author": "Demian Ghalandari; Chris Hokamp; Georgiana Ifrim",
        "authorids": "/d/demian-ghalandari/; /c/chris-hokamp/; /g/georgiana-ifrim/",
        "bibtex": "@inproceedings{ghalandari-etal-2022-efficient,\n    title = \"Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning\",\n    author = \"Ghalandari, Demian  and\n      Hokamp, Chris  and\n      Ifrim, Georgiana\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.90/\",\n    doi = \"10.18653/v1/2022.acl-long.90\",\n    pages = \"1267--1280\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.90.pdf",
        "site": "https://aclanthology.org/2022.acl-long.90/",
        "pdf_size": 702859,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6604622938631539563&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 5,
        "aff": "Aylien Ltd., Dublin, Ireland + Insight Centre for Data Analytics, University College Dublin, Ireland; Aylien Ltd., Dublin, Ireland; Insight Centre for Data Analytics, University College Dublin, Ireland",
        "aff_domain": "aylien.com;aylien.com;ucd.ie",
        "email": "aylien.com;aylien.com;ucd.ie",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "Aylien Ltd.;University College Dublin",
        "aff_unique_dep": ";Insight Centre for Data Analytics",
        "aff_unique_url": "https://www.aylien.com;https://www.ucd.ie",
        "aff_unique_abbr": ";UCD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Ireland"
    },
    {
        "id": "2022.findings-acl.121",
        "title": "Efficient, Uncertainty-based Moderation of Neural Networks Text Classifiers",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "To maximize the accuracy and increase the overall acceptance of text classifiers, we propose a framework for the efficient, in-operation moderation of classifiers\u2019 output. Our framework focuses on use cases in which F1-scores of modern Neural Networks classifiers (ca. 90%) are still inapplicable in practice. We suggest a semi-automated approach that uses prediction uncertainties to pass unconfident, probably incorrect classifications to human moderators. To minimize the workload, we limit the human moderated data to the point where the accuracy gains saturate and further human effort does not lead to substantial improvements. A series of benchmarking experiments based on three different datasets and three state-of-the-art classifiers show that our framework can improve the classification F1-scores by 5.1 to 11.2% (up to approx. 98 to 99%), while reducing the moderation load up to 73.3% compared to a random moderation.",
        "author": "Jakob Smedegaard Andersen; Walid Maalej",
        "authorids": "/j/jakob-smedegaard-andersen/; /w/walid-maalej/",
        "bibtex": "@inproceedings{andersen-maalej-2022-efficient,\n    title = \"Efficient, Uncertainty-based Moderation of Neural Networks Text Classifiers\",\n    author = \"Andersen, Jakob Smedegaard  and\n      Maalej, Walid\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.121/\",\n    doi = \"10.18653/v1/2022.findings-acl.121\",\n    pages = \"1536--1546\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.121.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.121/",
        "pdf_size": 328144,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12793835545285360847&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Hamburg; University of Hamburg",
        "aff_domain": "uni-hamburg.de;uni-hamburg.de",
        "email": "uni-hamburg.de;uni-hamburg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Hamburg",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-hamburg.de",
        "aff_unique_abbr": "UHH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.findings-acl.23",
        "title": "Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).",
        "author": "Yiqing Xie; Jiaming Shen; Sha Li; Yuning Mao; Jiawei Han",
        "authorids": "/y/yiqing-xie/; /j/jiaming-shen/; /s/sha-li/; /y/yuning-mao/; /j/jiawei-han/",
        "bibtex": "@inproceedings{xie-etal-2022-eider,\n    title = \"Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion\",\n    author = \"Xie, Yiqing  and\n      Shen, Jiaming  and\n      Li, Sha  and\n      Mao, Yuning  and\n      Han, Jiawei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.23/\",\n    doi = \"10.18653/v1/2022.findings-acl.23\",\n    pages = \"257--268\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.23.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.23/",
        "pdf_size": 567398,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5993915249058989267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "github": "https://github.com/Veronicium/Eider",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.126",
        "title": "EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Emotion recognition in conversation (ERC) aims to analyze the speaker\u2019s state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, we design an end-to-end ERC model called EmoCaps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model. Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.",
        "author": "Zaijing Li; Fengxiao Tang; Ming Zhao; Yusen Zhu",
        "authorids": "/z/zaijing-li/; /f/fengxiao-tang/; /m/ming-zhao/; /y/yusen-zhu/",
        "bibtex": "@inproceedings{li-etal-2022-emocaps,\n    title = \"{E}mo{C}aps: Emotion Capsule based Model for Conversational Emotion Recognition\",\n    author = \"Li, Zaijing  and\n      Tang, Fengxiao  and\n      Zhao, Ming  and\n      Zhu, Yusen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.126/\",\n    doi = \"10.18653/v1/2022.findings-acl.126\",\n    pages = \"1610--1618\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.126.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.126/",
        "pdf_size": 596539,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4713874324270170150&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Mathematics, Hunan University, Changsha, China",
        "aff_domain": "csu.edu.cn;csu.edu.cn;csu.edu.cn;163.com",
        "email": "csu.edu.cn;csu.edu.cn;csu.edu.cn;163.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Central South University;Hunan University",
        "aff_unique_dep": "School of Computer Science and Engineering;School of Mathematics",
        "aff_unique_url": "http://www.csu.edu.cn;http://www.hnu.edu.cn",
        "aff_unique_abbr": "CSU;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Changsha",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.221",
        "title": "EnCBP: A New Benchmark Dataset for Finer-Grained Cultural Background Prediction in English",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While cultural backgrounds have been shown to affect linguistic expressions, existing natural language processing (NLP) research on culture modeling is overly coarse-grained and does not examine cultural differences among speakers of the same language. To address this problem and augment NLP models with cultural background features, we collect, annotate, manually validate, and benchmark EnCBP, a finer-grained news-based cultural background prediction dataset in English. Through language modeling (LM) evaluations and manual analyses, we confirm that there are noticeable differences in linguistic expressions among five English-speaking countries and across four states in the US. Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic (PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2, Emotion, and Go-Emotions) show that, while introducing cultural background information does not benefit the Go-Emotions task due to text domain conflicts, it noticeably improves deep learning (DL) model performance on other tasks. Our findings strongly support the importance of cultural background modeling to a wide variety of NLP tasks and demonstrate the applicability of EnCBP in culture-related research.",
        "author": "Weicheng Ma; Samiha Datta; Lili Wang; Soroush Vosoughi",
        "authorids": "/w/weicheng-ma/; /s/samiha-datta/; /l/lili-wang/; /s/soroush-vosoughi/",
        "bibtex": "@inproceedings{ma-etal-2022-encbp,\n    title = \"{E}n{CBP}: A New Benchmark Dataset for Finer-Grained Cultural Background Prediction in {E}nglish\",\n    author = \"Ma, Weicheng  and\n      Datta, Samiha  and\n      Wang, Lili  and\n      Vosoughi, Soroush\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.221/\",\n    doi = \"10.18653/v1/2022.findings-acl.221\",\n    pages = \"2811--2823\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.221.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.221/",
        "pdf_size": 396942,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14839378674548787325&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Dartmouth College; Department of Computer Science, Dartmouth College; Department of Computer Science, Dartmouth College; Department of Computer Science, Dartmouth College",
        "aff_domain": "dartmouth.edu;dartmouth.edu;dartmouth.edu;dartmouth.edu",
        "email": "dartmouth.edu;dartmouth.edu;dartmouth.edu;dartmouth.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Dartmouth College",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://dartmouth.edu",
        "aff_unique_abbr": "Dartmouth",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.187",
        "title": "Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data- and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7\u00d7 fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.",
        "author": "Wenliang Dai; Lu Hou; Lifeng Shang; Xin Jiang; Qun Liu; Pascale Fung",
        "authorids": "/w/wenliang-dai/; /l/lu-hou/; /l/lifeng-shang/; /x/xin-jiang/; /q/qun-liu/; /p/pascale-fung/",
        "bibtex": "@inproceedings{dai-etal-2022-enabling,\n    title = \"Enabling Multimodal Generation on {CLIP} via Vision-Language Knowledge Distillation\",\n    author = \"Dai, Wenliang  and\n      Hou, Lu  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Fung, Pascale\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.187/\",\n    doi = \"10.18653/v1/2022.findings-acl.187\",\n    pages = \"2383--2395\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.187.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.187/",
        "pdf_size": 1329967,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13022999821013347032&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Hong Kong University of Science and Technology; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Hong Kong University of Science and Technology",
        "aff_domain": "connect.ust.hk;huawei.com;huawei.com;huawei.com;huawei.com;ece.ust.hk",
        "email": "connect.ust.hk;huawei.com;huawei.com;huawei.com;huawei.com;ece.ust.hk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Huawei",
        "aff_unique_dep": ";Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.ust.hk;https://www.huawei.com",
        "aff_unique_abbr": "HKUST;Huawei",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.256",
        "title": "Encoding and Fusing Semantic Connection and Linguistic Evidence for Implicit Discourse Relation Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prior studies use one attention mechanism to improve contextual semantic representation learning for implicit discourse relation recognition (IDRR). However, diverse relation senses may benefit from different attention mechanisms. We also argue that some linguistic relation in between two words can be further exploited for IDRR. This paper proposes a Multi-Attentive Neural Fusion (MANF) model to encode and fuse both semantic connection and linguistic evidence for IDRR. In MANF, we design a Dual Attention Network (DAN) to learn and fuse two kinds of attentive representation for arguments as its semantic connection. We also propose an Offset Matrix Network (OMN) to encode the linguistic relations of word-pairs as linguistic evidence. Our MANF model achieves the state-of-the-art results on the PDTB 3.0 corpus.",
        "author": "Wei Xiang; Bang Wang; Lu Dai; Yijun Mo",
        "authorids": "/w/wei-xiang/; /b/bang-wang/; /l/lu-dai/; /y/yijun-mo/",
        "bibtex": "@inproceedings{xiang-etal-2022-encoding,\n    title = \"Encoding and Fusing Semantic Connection and Linguistic Evidence for Implicit Discourse Relation Recognition\",\n    author = \"Xiang, Wei  and\n      Wang, Bang  and\n      Dai, Lu  and\n      Mo, Yijun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.256/\",\n    doi = \"10.18653/v1/2022.findings-acl.256\",\n    pages = \"3247--3257\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.256.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.256/",
        "pdf_size": 2861775,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11918506473134492674&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China",
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Electronic Information and Communications",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Wuhan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.596",
        "title": "End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language spatial video grounding aims to detect the relevant objects in video frames with descriptive sentences as the query. In spite of the great advances, most existing methods rely on dense video frame annotations, which require a tremendous amount of human effort. To achieve effective grounding under a limited annotation budget, we investigate one-shot video grounding and learn to ground natural language in all video frames with solely one frame labeled, in an end-to-end manner. One major challenge of end-to-end one-shot video grounding is the existence of videos frames that are either irrelevant to the language query or the labeled frame. Another challenge relates to the limited supervision, which might result in ineffective representation learning. To address these challenges, we designed an end-to-end model via Information Tree for One-Shot video grounding (IT-OS). Its key module, the information tree, can eliminate the interference of irrelevant frames based on branch search and branch cropping techniques. In addition, several self-supervised tasks are proposed based on the information tree to improve the representation learning under insufficient labeling. Experiments on the benchmark dataset demonstrate the effectiveness of our model.",
        "author": "Mengze Li; Tianbao Wang; Haoyu Zhang; Shengyu Zhang; Zhou Zhao; Jiaxu Miao; Wenqiao Zhang; Wenming Tan; Jin Wang; Peng Wang; Shiliang Pu; Fei Wu",
        "authorids": "/m/mengze-li/; /t/tianbao-wang/; /h/haoyu-zhang/; /s/shengyu-zhang/; /z/zhou-zhao/; /j/jiaxu-miao/; /w/wenqiao-zhang/; /w/wenming-tan/; /j/jin-wang/; /p/peng-wang/; /s/shiliang-pu/; /f/fei-wu/",
        "bibtex": "@inproceedings{li-etal-2022-end,\n    title = \"End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding\",\n    author = \"Li, Mengze  and\n      Wang, Tianbao  and\n      Zhang, Haoyu  and\n      Zhang, Shengyu  and\n      Zhao, Zhou  and\n      Miao, Jiaxu  and\n      Zhang, Wenqiao  and\n      Tan, Wenming  and\n      Wang, Jin  and\n      Wang, Peng  and\n      Pu, Shiliang  and\n      Wu, Fei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.596/\",\n    doi = \"10.18653/v1/2022.acl-long.596\",\n    pages = \"8707--8717\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.596.pdf",
        "site": "https://aclanthology.org/2022.acl-long.596/",
        "pdf_size": 5081136,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12618418619691924071&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Shanghai Institute for Advanced Study of Zhejiang University+Shanghai AI Laboratory; Zhejiang University; Zhejiang University; Hikvision Research Institute; Hikvision Research Institute; Northwestern Polytechnical University; Hikvision Research Institute; Shanghai Institute for Advanced Study of Zhejiang University+Shanghai AI Laboratory",
        "aff_domain": "zju.edu.cn; ; ; ;zju.edu.cn;yahoo;zju.edu.cn; ; ; ; ;zju.edu.cn",
        "email": "zju.edu.cn; ; ; ;zju.edu.cn;yahoo;zju.edu.cn; ; ; ; ;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0+1;0;0;2;2;3;2;0+1",
        "aff_unique_norm": "Zhejiang University;Shanghai AI Laboratory;Hikvision Research Institute;Northwestern Polytechnical University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.shanghai-ai-lab.com;https://www.hikvision.com/cn/;https://www.nwpu.edu.cn",
        "aff_unique_abbr": "ZJU;SAIL;Hikvision;NWPU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0;0;0;0;0+0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.46",
        "title": "End-to-End Segmentation-based News Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this paper, we bring a new way of digesting news content by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section. We make two contributions towards this new task. First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned heading-style section summaries. Second, we propose a novel segmentation-based language generation model adapted from pre-trained language models that can jointly segment a document and produce the summary for each section. Experimental results on SegNews demonstrate that our model can outperform several state-of-the-art sequence-to-sequence generation models for this new task.",
        "author": "Yang Liu; Chenguang Zhu; Michael Zeng",
        "authorids": "/y/yang-liu-microsoft/; /c/chenguang-zhu/; /m/michael-zeng/",
        "bibtex": "@inproceedings{liu-etal-2022-end,\n    title = \"End-to-End Segmentation-based News Summarization\",\n    author = \"Liu, Yang  and\n      Zhu, Chenguang  and\n      Zeng, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.46/\",\n    doi = \"10.18653/v1/2022.findings-acl.46\",\n    pages = \"544--554\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.46.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.46/",
        "pdf_size": 993850,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15904309879337975344&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Cognitive Services Research; Microsoft Cognitive Services Research; Microsoft Cognitive Services Research",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/nlpyang/segnews544",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Cognitive Services Research",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.113",
        "title": "End-to-End Speech Translation for Code Switched Speech",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages. CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems. In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translation. To evaluate model performance on this task, we create a novel ST corpus derived from existing public data sets. We explore various ST architectures across two dimensions: cascaded (transcribe then translate) vs end-to-end (jointly transcribe and translate) and unidirectional (source -> target) vs bidirectional (source <-> target). We show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is used.",
        "author": "Orion Weller; Matthias Sperber; Telmo Pires; Hendra Setiawan; Christian Gollan; Dominic Telaar; Matthias Paulik",
        "authorids": "/o/orion-weller/; /m/matthias-sperber/; /t/telmo-pires/; /h/hendra-setiawan/; /c/christian-gollan/; /d/dominic-telaar/; /m/matthias-paulik/",
        "bibtex": "@inproceedings{weller-etal-2022-end,\n    title = \"End-to-End Speech Translation for Code Switched Speech\",\n    author = \"Weller, Orion  and\n      Sperber, Matthias  and\n      Pires, Telmo  and\n      Setiawan, Hendra  and\n      Gollan, Christian  and\n      Telaar, Dominic  and\n      Paulik, Matthias\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.113/\",\n    doi = \"10.18653/v1/2022.findings-acl.113\",\n    pages = \"1435--1448\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.113.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.113/",
        "pdf_size": 766249,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15817269191392953183&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Johns Hopkins University; Apple; Apple; Apple; Apple; Apple; Apple",
        "aff_domain": "cs.jhu.edu;apple.com; ; ; ; ; ",
        "email": "cs.jhu.edu;apple.com; ; ; ; ; ",
        "github": "https://github.com/apple/ml-code-switched-speech-translation",
        "project": "https://bbc.in/3jgwzZ2",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "Johns Hopkins University;Apple",
        "aff_unique_dep": ";Apple Inc.",
        "aff_unique_url": "https://www.jhu.edu;https://www.apple.com",
        "aff_unique_abbr": "JHU;Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.212",
        "title": "Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is an emerging sentiment analysis task. Most of the existing studies focus on devising a new tagging scheme that enables the model to extract the sentiment triplets in an end-to-end fashion. However, these methods ignore the relations between words for ASTE task. In this paper, we propose an Enhanced Multi-Channel Graph Convolutional Network model (EMC-GCN) to fully utilize the relations between words. Specifically, we first define ten types of relations for ASTE task, and then adopt a biaffine attention module to embed these relations as an adjacent tensor between words in a sentence. After that, our EMC-GCN transforms the sentence into a multi-channel graph by treating words and the relation adjacent tensor as nodes and edges, respectively. Thus, relation-aware node representations can be learnt. Furthermore, we consider diverse linguistic features to enhance our EMC-GCN model. Finally, we design an effective refining strategy on EMC-GCN for word-pair representation refinement, which considers the implicit results of aspect and opinion extraction when determining whether word pairs match or not. Extensive experimental results on the benchmark datasets demonstrate that the effectiveness and robustness of our proposed model, which outperforms state-of-the-art methods significantly.",
        "author": "Hao Chen; Zepeng Zhai; Fangxiang Feng; Ruifan Li; Xiaojie Wang",
        "authorids": "/h/hao-chen/; /z/zepeng-zhai/; /f/fangxiang-feng/; /r/ruifan-li/; /x/xiaojie-wang/",
        "bibtex": "@inproceedings{chen-etal-2022-enhanced,\n    title = \"Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction\",\n    author = \"Chen, Hao  and\n      Zhai, Zepeng  and\n      Feng, Fangxiang  and\n      Li, Ruifan  and\n      Wang, Xiaojie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.212/\",\n    doi = \"10.18653/v1/2022.acl-long.212\",\n    pages = \"2974--2985\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.212.pdf",
        "site": "https://aclanthology.org/2022.acl-long.212/",
        "pdf_size": 5869215,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4126422623351147914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/CCChenhao997/EMCGCN-ASTE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "School of Artificial Intelligence",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.140",
        "title": "Enhancing Chinese Pre-trained Language Model via Heterogeneous Linguistics Graph",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Chinese pre-trained language models usually exploit contextual character information to learn representations, while ignoring the linguistics knowledge, e.g., word and sentence information. Hence, we propose a task-free enhancement module termed as Heterogeneous Linguistics Graph (HLG) to enhance Chinese pre-trained language models by integrating linguistics knowledge. Specifically, we construct a hierarchical heterogeneous graph to model the characteristics linguistics structure of Chinese language, and conduct a graph-based method to summarize and concretize information on different granularities of Chinese linguistics hierarchies. Experimental results demonstrate our model has the ability to improve the performance of vanilla BERT, BERTwwm and ERNIE 1.0 on 6 natural language processing tasks with 10 benchmark datasets. Further, the detailed experimental analyses have proven that this kind of modelization achieves more improvements compared with previous strong baseline MWA. Meanwhile, our model introduces far fewer parameters (about half of MWA) and the training/inference speed is about 7x faster than MWA.",
        "author": "Yanzeng Li; Jiangxia Cao; Xin Cong; Zhenyu Zhang; Bowen Yu; Hongsong Zhu; Tingwen Liu",
        "authorids": "/y/yanzeng-li/; /j/jiangxia-cao/; /x/xin-cong/; /z/zhenyu-zhang/; /b/bowen-yu/; /h/hongsong-zhu/; /t/tingwen-liu/",
        "bibtex": "@inproceedings{li-etal-2022-enhancing,\n    title = \"Enhancing {C}hinese Pre-trained Language Model via Heterogeneous Linguistics Graph\",\n    author = \"Li, Yanzeng  and\n      Cao, Jiangxia  and\n      Cong, Xin  and\n      Zhang, Zhenyu  and\n      Yu, Bowen  and\n      Zhu, Hongsong  and\n      Liu, Tingwen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.140/\",\n    doi = \"10.18653/v1/2022.acl-long.140\",\n    pages = \"1986--1996\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.140.pdf",
        "site": "https://aclanthology.org/2022.acl-long.140/",
        "pdf_size": 542899,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14737869084983948616&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Wangxuan Institute of Computer Technology, Peking University; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences",
        "aff_domain": "stu.pku.edu.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "stu.pku.edu.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "https://github.com/lsvih/HLG",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1+2;1+2;1+2;1+2;1+2;1+2",
        "aff_unique_norm": "Peking University;Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.pku.edu.cn;http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "PKU;CAS;UCAS",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.134",
        "title": "Enhancing Cross-lingual Natural Language Inference by Prompt-learning from Cross-lingual Templates",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-lingual natural language inference (XNLI) is a fundamental task in cross-lingual natural language understanding. Recently this task is commonly addressed by pre-trained cross-lingual language models. Existing methods usually enhance pre-trained language models with additional data, such as annotated parallel corpora. These additional data, however, are rare in practice, especially for low-resource languages. Inspired by recent promising results achieved by prompt-learning, this paper proposes a novel prompt-learning based framework for enhancing XNLI. It reformulates the XNLI problem to a masked language modeling problem by constructing cloze-style questions through cross-lingual templates. To enforce correspondence between different languages, the framework augments a new question for every question using a sampled template in another language and then introduces a consistency loss to make the answer probability distribution obtained from the new question as similar as possible with the corresponding distribution obtained from the original question. Experimental results on two benchmark datasets demonstrate that XNLI models enhanced by our proposed framework significantly outperform original ones under both the full-shot and few-shot cross-lingual transfer settings.",
        "author": "Kunxun Qi; Hai Wan; Jianfeng Du; Haolan Chen",
        "authorids": "/k/kunxun-qi/; /h/hai-wan/; /j/jianfeng-du/; /h/haolan-chen/",
        "bibtex": "@inproceedings{qi-etal-2022-enhancing,\n    title = \"Enhancing Cross-lingual Natural Language Inference by Prompt-learning from Cross-lingual Templates\",\n    author = \"Qi, Kunxun  and\n      Wan, Hai  and\n      Du, Jianfeng  and\n      Chen, Haolan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.134/\",\n    doi = \"10.18653/v1/2022.acl-long.134\",\n    pages = \"1910--1923\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.134.pdf",
        "site": "https://aclanthology.org/2022.acl-long.134/",
        "pdf_size": 736171,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4247202214135200257&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China+Key Laboratory of Machine Intelligence and Advanced Computing (Sun Yat-sen University), Ministry of Education, China+Pazhou Lab, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China+Key Laboratory of Machine Intelligence and Advanced Computing (Sun Yat-sen University), Ministry of Education, China+Pazhou Lab, Guangzhou, China; Guangzhou Key Laboratory of Multilingual Intelligent Processing, Guangdong University of Foreign Studies, Guangzhou, China+Pazhou Lab, Guangzhou, China; Platform and Content Group, Tencent, Shenzhen, China",
        "aff_domain": "mail.sysu.edu.cn;gdufs.edu.cn; ; ",
        "email": "mail.sysu.edu.cn;gdufs.edu.cn; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+1;0+0+1;2+1;3",
        "aff_unique_norm": "Sun Yat-sen University;Pazhou Lab;Guangdong University of Foreign Studies;Tencent",
        "aff_unique_dep": "School of Computer Science and Engineering;;Guangzhou Key Laboratory of Multilingual Intelligent Processing;Platform and Content Group",
        "aff_unique_url": "http://www.sysu.edu.cn;;;https://www.tencent.com",
        "aff_unique_abbr": "SYSU;;;Tencent",
        "aff_campus_unique_index": "0+0;0+0;0+0;2",
        "aff_campus_unique": "Guangzhou;;Shenzhen",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.138",
        "title": "Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We study how to enhance text representation via textual commonsense. We point out that commonsense has the nature of domain discrepancy. Namely, commonsense has different data formats and is domain-independent from the downstream task. This nature brings challenges to introducing commonsense in general text understanding tasks. A typical method of introducing textual knowledge is continuing pre-training over the commonsense corpus. However, it will cause catastrophic forgetting to the downstream task due to the domain discrepancy. In addition, previous methods of directly using textual descriptions as extra input information cannot apply to large-scale commonsense. In this paper, we propose to use large-scale out-of-domain commonsense to enhance text representation. In order to effectively incorporate the commonsense, we proposed OK-Transformer (Out-of-domain Knowledge enhanced Transformer). OK-Transformer effectively integrates commonsense descriptions and enhances them to the target text representation. In addition, OK-Transformer can adapt to the Transformer-based language models (e.g. BERT, RoBERTa) for free, without pre-training on large-scale unsupervised corpora. We have verified the effectiveness of OK-Transformer in multiple applications such as commonsense reasoning, general text classification, and low-resource commonsense settings.",
        "author": "Wanyun Cui; Xingran Chen",
        "authorids": "/w/wanyun-cui/; /x/xingran-chen/",
        "bibtex": "@inproceedings{cui-chen-2022-enhancing,\n    title = \"Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense\",\n    author = \"Cui, Wanyun  and\n      Chen, Xingran\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.138/\",\n    doi = \"10.18653/v1/2022.findings-acl.138\",\n    pages = \"1746--1756\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.138.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.138/",
        "pdf_size": 573080,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5119741076458904708&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai University of Finance and Economics; Shanghai University of Finance and Economics",
        "aff_domain": "sufe.edu.cn;gmail.com",
        "email": "sufe.edu.cn;gmail.com",
        "github": "https://github.com/chenxran/ok-transformer",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Shanghai University of Finance and Economics",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.sufe.edu.cn",
        "aff_unique_abbr": "SUFE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.266",
        "title": "Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we investigate improvements to the GEC sequence tagging architecture with a focus on ensembling of recent cutting-edge Transformer-based encoders in Large configurations. We encourage ensembling models by majority votes on span-level edits because this approach is tolerant to the model architecture and vocabulary size. Our best ensemble achieves a new SOTA result with an F0.5 score of 76.05 on BEA-2019 (test), even without pre-training on synthetic datasets. In addition, we perform knowledge distillation with a trained ensemble to generate new synthetic training datasets, \u201cTroy-Blogs\u201d and \u201cTroy-1BW\u201d. Our best single sequence tagging model that is pretrained on the generated Troy- datasets in combination with the publicly available synthetic PIE dataset achieves a near-SOTA result with an F0.5 score of 73.21 on BEA-2019 (test). The code, datasets, and trained models are publicly available.",
        "author": "Maksym Tarnavskyi; Artem Chernodub; Kostiantyn Omelianchuk",
        "authorids": "/m/maksym-tarnavskyi/; /a/artem-chernodub/; /k/kostiantyn-omelianchuk/",
        "bibtex": "@inproceedings{tarnavskyi-etal-2022-ensembling,\n    title = \"Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction\",\n    author = \"Tarnavskyi, Maksym  and\n      Chernodub, Artem  and\n      Omelianchuk, Kostiantyn\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.266/\",\n    doi = \"10.18653/v1/2022.acl-long.266\",\n    pages = \"3842--3852\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.266.pdf",
        "site": "https://aclanthology.org/2022.acl-long.266/",
        "pdf_size": 600948,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3572241942492879274&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII",
        "gs_version_total": 5,
        "aff": "Ukrainian Catholic University, Faculty of Applied Sciences+Grammarly; Grammarly; Grammarly",
        "aff_domain": "ucu.edu.ua;grammarly.com;grammarly.com",
        "email": "ucu.edu.ua;grammarly.com;grammarly.com",
        "github": "https://github.com/MaksTarnavskyi/gector-largetarget",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Ukrainian Catholic University;Grammarly",
        "aff_unique_dep": "Faculty of Applied Sciences;",
        "aff_unique_url": "https://ucu.edu.ua;https://www.grammarly.com",
        "aff_unique_abbr": "UCU;Grammarly",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "Ukraine;United States"
    },
    {
        "id": "2022.acl-long.237",
        "title": "EntSUM: A Data Set for Entity-Centric Extractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Controllable summarization aims to provide summaries that take into account user-specified aspects and preferences to better assist them with their information need, as opposed to the standard summarization setup which build a single generic summary of a document. We introduce a human-annotated data set EntSUM for controllable summarization with a focus on named entities as the aspects to control. We conduct an extensive quantitative analysis to motivate the task of entity-centric summarization and show that existing methods for controllable summarization fail to generate entity-centric summaries. We propose extensions to state-of-the-art summarization approaches that achieve substantially better results on our data set. Our analysis and results show the challenging nature of this task and of the proposed data set.",
        "author": "Mounica Maddela; Mayank Kulkarni; Daniel Preotiuc-Pietro",
        "authorids": "/m/mounica-maddela/; /m/mayank-kulkarni/; /d/daniel-preotiuc-pietro/",
        "bibtex": "@inproceedings{maddela-etal-2022-entsum,\n    title = \"{E}nt{SUM}: A Data Set for Entity-Centric Extractive Summarization\",\n    author = \"Maddela, Mounica  and\n      Kulkarni, Mayank  and\n      Preotiuc-Pietro, Daniel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.237/\",\n    doi = \"10.18653/v1/2022.acl-long.237\",\n    pages = \"3355--3366\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.237.pdf",
        "site": "https://aclanthology.org/2022.acl-long.237/",
        "pdf_size": 1871797,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9667971492007956465&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Georgia Institute of Technology+Bloomberg; Bloomberg; Bloomberg",
        "aff_domain": "cc.gatech.edu;bloomberg.net;bloomberg.net",
        "email": "cc.gatech.edu;bloomberg.net;bloomberg.net",
        "github": "https://github.com/bloomberg/entsum",
        "project": "https://zenodo.org/record/6359875",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Georgia Institute of Technology;Bloomberg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.bloomberg.com",
        "aff_unique_abbr": "Georgia Tech;Bloomberg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.406",
        "title": "Entailment Graph Learning with Textual Entailment and Soft Transitivity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Typed entailment graphs try to learn the entailment relations between predicates from text and model them as edges between predicate nodes. The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity. We propose a two-stage method, Entailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns the local entailment relations by recognizing the textual entailment between template sentences formed by typed CCG-parsed predicates. Based on the generated local graph, EGT2 then uses three novel soft transitivity constraints to consider the logical transitivity in entailment structures. Experiments on benchmark datasets show that EGT2 can well model the transitivity in entailment graph to alleviate the sparsity, and leads to signifcant improvement over current state-of-the-art methods.",
        "author": "Zhibin Chen; Yansong Feng; Dongyan Zhao",
        "authorids": "/z/zhibin-chen/; /y/yansong-feng/; /d/dongyan-zhao/",
        "bibtex": "@inproceedings{chen-etal-2022-entailment,\n    title = \"Entailment Graph Learning with Textual Entailment and Soft Transitivity\",\n    author = \"Chen, Zhibin  and\n      Feng, Yansong  and\n      Zhao, Dongyan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.406/\",\n    doi = \"10.18653/v1/2022.acl-long.406\",\n    pages = \"5899--5910\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.406.pdf",
        "site": "https://aclanthology.org/2022.acl-long.406/",
        "pdf_size": 358462,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18411618677358734963&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 6,
        "aff": "Wangxuan Institute of Computer Technology, Peking University, China+Center for Data Science, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China; Wangxuan Institute of Computer Technology, Peking University, China+Center for Data Science, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China; Wangxuan Institute of Computer Technology, Peking University, China+Center for Data Science, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/ZacharyChenpk/EGT2",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0+0;0+0+0;0+0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.537",
        "title": "Entity-based Neural Local Coherence Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models. Recent neural coherence models encode the input document using large-scale pretrained language models. Hence their basis for computing local coherence are words and even sub-words. The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role. Still, these models achieve state-of-the-art performance in several end applications. In contrast to these models, we compute coherence on the basis of entities by constraining the input to noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences leading to the notion of focus. This brings our model linguistically in line with pre-neural models of computing coherence. It also gives us better insight into the behaviour of the model thus leading to better explainability. Our approach is also in accord with a recent study (O\u2019Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models. We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications.",
        "author": "Sungho Jeon; Michael Strube",
        "authorids": "/s/sungho-jeon/; /m/michael-strube/",
        "bibtex": "@inproceedings{jeon-strube-2022-entity,\n    title = \"Entity-based Neural Local Coherence Modeling\",\n    author = \"Jeon, Sungho  and\n      Strube, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.537/\",\n    doi = \"10.18653/v1/2022.acl-long.537\",\n    pages = \"7787--7805\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.537.pdf",
        "site": "https://aclanthology.org/2022.acl-long.537/",
        "pdf_size": 336696,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12069107093274072339&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Heidelberg Institute for Theoretical Studies gGmbH; Heidelberg Institute for Theoretical Studies gGmbH",
        "aff_domain": "h-its.org;h-its.org",
        "email": "h-its.org;h-its.org",
        "github": "https://github.com/sdeva14/acl22-entity-neural-local-cohe",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Heidelberg Institute for Theoretical Studies",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hits.org",
        "aff_unique_abbr": "HITS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.findings-acl.88",
        "title": "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance. Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected. Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy. We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.",
        "author": "Giuseppe Attanasio; Debora Nozza; Dirk Hovy; Elena Baralis",
        "authorids": "/g/giuseppe-attanasio/; /d/debora-nozza/; /d/dirk-hovy/; /e/elena-baralis/",
        "bibtex": "@inproceedings{attanasio-etal-2022-entropy,\n    title = \"Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists\",\n    author = \"Attanasio, Giuseppe  and\n      Nozza, Debora  and\n      Hovy, Dirk  and\n      Baralis, Elena\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.88/\",\n    doi = \"10.18653/v1/2022.findings-acl.88\",\n    pages = \"1105--1119\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.88.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.88/",
        "pdf_size": 506416,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6483719412845482614&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Bocconi University, Milan, Italy+Politecnico di Torino, Turin, Italy; Bocconi University, Milan, Italy; Bocconi University, Milan, Italy; Politecnico di Torino, Turin, Italy",
        "aff_domain": "unibocconi.it;unibocconi.it;unibocconi.it;polito.it",
        "email": "unibocconi.it;unibocconi.it;unibocconi.it;polito.it",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Bocconi University;Politecnico di Torino",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bocconi.edu;https://www.polito.it",
        "aff_unique_abbr": "Bocconi;Polito",
        "aff_campus_unique_index": "0+1;0;0;1",
        "aff_campus_unique": "Milan;Turin",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.acl-short.20",
        "title": "Estimating the Entropy of Linguistic Distributions",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropymust typically be estimated from observed data because researchers do not have access to the underlying probability distribution. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. We end this paper with a concrete recommendation for the entropy estimators that should be used in future linguistic studies.",
        "author": "Aryaman Arora; Clara Meister; Ryan Cotterell",
        "authorids": "/a/aryaman-arora/; /c/clara-meister/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{arora-etal-2022-estimating,\n    title = \"Estimating the Entropy of Linguistic Distributions\",\n    author = \"Arora, Aryaman  and\n      Meister, Clara  and\n      Cotterell, Ryan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.20/\",\n    doi = \"10.18653/v1/2022.acl-short.20\",\n    pages = \"175--195\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.20.pdf",
        "site": "https://aclanthology.org/2022.acl-short.20/",
        "pdf_size": 571787,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11271845407443948303&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Georgetown University; ETH Z\u00fcrich; ETH Z\u00fcrich",
        "aff_domain": "georgetown.edu;inf.ethz.ch;inf.ethz.ch",
        "email": "georgetown.edu;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Georgetown University;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.georgetown.edu;https://www.ethz.ch",
        "aff_unique_abbr": "GU;ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "2022.acl-long.573",
        "title": "Ethics Sheets for AI Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Several high-profile events, such as the mass testing of emotion recognition systems on vulnerable sub-populations and using question answering systems to make moral judgments, have highlighted how technology will often lead to more adverse outcomes for those that are already marginalized. At issue here are not just individual systems and datasets, but also the AI tasks themselves. In this position paper, I make a case for thinking about ethical considerations not just at the level of individual models and datasets, but also at the level of AI tasks. I will present a new form of such an effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the assumptions and ethical considerations hidden in how a task is commonly framed and in the choices we make regarding the data, method, and evaluation. I will also present a template for ethics sheets with 50 ethical considerations, using the task of emotion recognition as a running example. Ethics sheets are a mechanism to engage with and document ethical considerations before building datasets and systems. Similar to survey articles, a small number of carefully created ethics sheets can serve numerous researchers and developers.",
        "author": "Saif Mohammad",
        "authorids": "/s/saif-mohammad/",
        "bibtex": "@inproceedings{mohammad-2022-ethics,\n    title = \"Ethics Sheets for {AI} Tasks\",\n    author = \"Mohammad, Saif\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.573/\",\n    doi = \"10.18653/v1/2022.acl-long.573\",\n    pages = \"8368--8379\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.573.pdf",
        "site": "https://aclanthology.org/2022.acl-long.573/",
        "pdf_size": 164546,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8634331794261506731&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 10,
        "aff": "National Research Council Canada",
        "aff_domain": "nrc-cnrc.gc.ca",
        "email": "nrc-cnrc.gc.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "National Research Council Canada",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nrc-cnrc.gc.ca",
        "aff_unique_abbr": "NRC-CNRC",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.399",
        "title": "Evaluating Extreme Hierarchical Multi-label Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Several natural language processing (NLP) tasks are defined as a classification problem in its most complex form: Multi-label Hierarchical Extreme classification, in which items may be associated with multiple classes from a set of thousands of possible classes organized in a hierarchy and with a highly unbalanced distribution both in terms of class frequency and the number of labels per item. We analyze the state of the art of evaluation metrics based on a set of formal properties and we define an information theoretic based metric inspired by the Information Contrast Model (ICM). Experiments on synthetic data and a case study on real data show the suitability of the ICM for such scenarios.",
        "author": "Enrique Amigo; Agust\u00edn Delgado",
        "authorids": "/e/enrique-amigo/; /a/agustin-delgado/",
        "bibtex": "@inproceedings{amigo-delgado-2022-evaluating,\n    title = \"Evaluating Extreme Hierarchical Multi-label Classification\",\n    author = \"Amigo, Enrique  and\n      Delgado, Agust{\\'i}n\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.399/\",\n    doi = \"10.18653/v1/2022.acl-long.399\",\n    pages = \"5809--5819\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.399.pdf",
        "site": "https://aclanthology.org/2022.acl-long.399/",
        "pdf_size": 341715,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15231959729401468447&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "UNED; UNED",
        "aff_domain": "lsi.uned.es;lsi.uned.es",
        "email": "lsi.uned.es;lsi.uned.es",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universidad Nacional de Educacion a Distancia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uned.es",
        "aff_unique_abbr": "UNED",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2022.acl-long.506",
        "title": "Evaluating Factuality in Text Simplification",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Automated simplification models aim to make input texts more readable. Such methods have the potential to make complex information accessible to a wider audience, e.g., providing access to recent medical literature which might otherwise be impenetrable for a lay reader. However, such models risk introducing errors into automatically simplified texts, for instance by inserting statements unsupported by the corresponding original text, or by omitting key information. Providing more readable but inaccurate versions of texts may in many cases be worse than providing no such access at all. The problem of factual accuracy (and the lack thereof) has received heightened attention in the context of summarization models, but the factuality of automatically simplified texts has not been investigated. We introduce a taxonomy of errors that we use to analyze both references drawn from standard simplification datasets and state-of-the-art model outputs. We find that errors often appear in both that are not captured by existing evaluation metrics, motivating a need for research into ensuring the factual accuracy of automated simplification models.",
        "author": "Ashwin Devaraj; William Sheffield; Byron Wallace; Junyi Jessy Li",
        "authorids": "/a/ashwin-devaraj/; /w/william-sheffield/; /b/byron-c-wallace/; /j/junyi-jessy-li/",
        "bibtex": "@inproceedings{devaraj-etal-2022-evaluating,\n    title = \"Evaluating Factuality in Text Simplification\",\n    author = \"Devaraj, Ashwin  and\n      Sheffield, William  and\n      Wallace, Byron  and\n      Li, Junyi Jessy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.506/\",\n    doi = \"10.18653/v1/2022.acl-long.506\",\n    pages = \"7331--7345\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.506.pdf",
        "site": "https://aclanthology.org/2022.acl-long.506/",
        "pdf_size": 291686,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13748243682155654068&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science, The University of Texas at Austin; Mathematics, The University of Texas at Austin; Khoury College of Computer Sciences, Northeastern University; Linguistics, The University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;northeastern.edu;utexas.edu",
        "email": "utexas.edu;utexas.edu;northeastern.edu;utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Texas at Austin;Northeastern University",
        "aff_unique_dep": "Computer Science;Khoury College of Computer Sciences",
        "aff_unique_url": "https://www.utexas.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "UT Austin;NU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.269",
        "title": "Event Transition Planning for Open-ended Text Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Open-ended text generation tasks, such as dialogue generation and story completion, require models to generate a coherent continuation given limited preceding context. The open-ended nature of these tasks brings new challenges to the neural auto-regressive text generators nowadays. Despite these neural models are good at producing human-like text, it is difficult for them to arrange causalities and relations between given facts and possible ensuing events. To bridge this gap, we propose a novel two-stage method which explicitly arranges the ensuing events in open-ended text generation. Our approach can be understood as a specially-trained coarse-to-fine algorithm, where an event transition planner provides a \u201ccoarse\u201d plot skeleton and a text generator in the second stage refines the skeleton. Experiments on two open-ended text generation tasks demonstrate that our proposed method effectively improves the quality of the generated text, especially in coherence and diversity. We will release the codes to the community for further exploration.",
        "author": "Qintong Li; Piji Li; Wei Bi; Zhaochun Ren; Yuxuan Lai; Lingpeng Kong",
        "authorids": "/q/qintong-li/; /p/piji-li/; /w/wei-bi/; /z/zhaochun-ren/; /y/yuxuan-lai/; /l/lingpeng-kong/",
        "bibtex": "@inproceedings{li-etal-2022-event,\n    title = \"Event Transition Planning for Open-ended Text Generation\",\n    author = \"Li, Qintong  and\n      Li, Piji  and\n      Bi, Wei  and\n      Ren, Zhaochun  and\n      Lai, Yuxuan  and\n      Kong, Lingpeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.269/\",\n    doi = \"10.18653/v1/2022.findings-acl.269\",\n    pages = \"3412--3426\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.269.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.269/",
        "pdf_size": 621102,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5678594870685344012&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, The University of Hong Kong+Shanghai Artificial Intelligence Laboratory; Tencent AI Lab; Tencent AI Lab; Shandong University; Department of Computer Science, The University of Hong Kong+Shanghai Artificial Intelligence Laboratory; Department of Computer Science, The University of Hong Kong+Shanghai Artificial Intelligence Laboratory",
        "aff_domain": "connect.hku.hk;gmail.com;gmail.com;tencent.com;sdu.edu.cn;cs.hku.hk",
        "email": "connect.hku.hk;gmail.com;gmail.com;tencent.com;sdu.edu.cn;cs.hku.hk",
        "github": "https://github.com/qtli/EventPlanforTextGen",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;2;3;0+1;0+1",
        "aff_unique_norm": "University of Hong Kong;Shanghai Artificial Intelligence Laboratory;Tencent;Shandong University",
        "aff_unique_dep": "Department of Computer Science;;Tencent AI Lab;",
        "aff_unique_url": "https://www.hku.hk;http://www.shailab.org/;https://ai.tencent.com;http://www.sdu.edu.cn",
        "aff_unique_abbr": "HKU;Shanghai AI Lab;Tencent AI Lab;SDU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.26",
        "title": "Event-Event Relation Extraction using Probabilistic Box Embedding",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "To understand a story with multiple events, it is important to capture the proper relations across these events. However, existing event relation extraction (ERE) framework regards it as a multi-class classification task and do not guarantee any coherence between different relation types, such as anti-symmetry. If a phone line \u201cdied\u201d after \u201cstorm\u201d, then it is obvious that the \u201cstorm\u201d happened before the \u201cdied\u201d. Current framework of event relation extraction do not guarantee this coherence and thus enforces it via constraint loss function (Wang et al., 2020). In this work, we propose to modify the underlying ERE model to guarantee coherence by representing each event as a box representation (BERE) without applying explicit constraints. From our experiments, BERE also shows stronger conjunctive constraint satisfaction while performing on par or better in F1 compared to previous models with constraint injection.",
        "author": "EunJeong Hwang; Jay-Yoon Lee; Tianyi Yang; Dhruvesh Patel; Dongxu Zhang; Andrew McCallum",
        "authorids": "/e/eunjeong-hwang/; /j/jay-yoon-lee/; /t/tianyi-yang/; /d/dhruvesh-patel/; /d/dongxu-zhang/; /a/andrew-mccallum/",
        "bibtex": "@inproceedings{hwang-etal-2022-event,\n    title = \"Event-Event Relation Extraction using Probabilistic Box Embedding\",\n    author = \"Hwang, EunJeong  and\n      Lee, Jay-Yoon  and\n      Yang, Tianyi  and\n      Patel, Dhruvesh  and\n      Zhang, Dongxu  and\n      McCallum, Andrew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.26/\",\n    doi = \"10.18653/v1/2022.acl-short.26\",\n    pages = \"235--244\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.26.pdf",
        "site": "https://aclanthology.org/2022.acl-short.26/",
        "pdf_size": 531872,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15829767608155506574&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "College of Information and Computer Science, University of Massachusetts Amherst; College of Information and Computer Science, University of Massachusetts Amherst; College of Information and Computer Science, University of Massachusetts Amherst; College of Information and Computer Science, University of Massachusetts Amherst; College of Information and Computer Science, University of Massachusetts Amherst; College of Information and Computer Science, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "github": "https://github.com/iesl/CE2ERE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "College of Information and Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.61",
        "title": "Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language. Thus, the majority of the world\u2019s languages cannot benefit from recent progress in NLP as they have no or limited textual data. To expand possibilities of using NLP technology in these under-represented languages, we systematically study strategies that relax the reliance on conventional language resources through the use of bilingual lexicons, an alternative resource with much better language coverage. We analyze different strategies to synthesize textual or labeled data using lexicons, and how this data can be combined with monolingual or parallel text when available. For 19 under-represented languages across 3 tasks, our methods lead to consistent improvements of up to 5 and 15 points with and without extra monolingual text respectively. Overall, our study highlights how NLP methods can be adapted to thousands more languages that are under-served by current technology.",
        "author": "Xinyi Wang; Sebastian Ruder; Graham Neubig",
        "authorids": "/x/xinyi-wang/; /s/sebastian-ruder/; /g/graham-neubig/",
        "bibtex": "@inproceedings{wang-etal-2022-expanding,\n    title = \"Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation\",\n    author = \"Wang, Xinyi  and\n      Ruder, Sebastian  and\n      Neubig, Graham\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.61/\",\n    doi = \"10.18653/v1/2022.acl-long.61\",\n    pages = \"863--877\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.61.pdf",
        "site": "https://aclanthology.org/2022.acl-long.61/",
        "pdf_size": 415809,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3508406193920184928&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Language Technology Institute, Carnegie Mellon University; Google Research; Language Technology Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;google.com;cs.cmu.edu",
        "email": "cs.cmu.edu;google.com;cs.cmu.edu",
        "github": "https://github.com/cindyxinyiwang/expand-via-lexicon-based-adaptation",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Google",
        "aff_unique_dep": "Language Technology Institute;Google Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.google",
        "aff_unique_abbr": "CMU;Google Research",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Pittsburgh;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.85",
        "title": "Explaining Classes through Stable Word Attributions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Input saliency methods have recently become a popular tool for explaining predictions of deep learning models in NLP. Nevertheless, there has been little work investigating methods for aggregating prediction-level explanations to the class level, nor has a framework for evaluating such class explanations been established. We explore explanations based on XLM-R and the Integrated Gradients input attribution method, and propose 1) the Stable Attribution Class Explanation method (SACX) to extract keyword lists of classes in text classification tasks, and 2) a framework for the systematic evaluation of the keyword lists. We find that explanations of individual predictions are prone to noise, but that stable explanations can be effectively identified through repeated training and explanation. We evaluate on web register data and show that the class explanations are linguistically meaningful and distinguishing of the classes.",
        "author": "Samuel R\u00f6nnqvist; Aki-Juhani Kyr\u00f6l\u00e4inen; Amanda Myntti; Filip Ginter; Veronika Laippala",
        "authorids": "/s/samuel-ronnqvist/; /a/aki-juhani-kyrolainen/; /a/amanda-myntti/; /f/filip-ginter/; /v/veronika-laippala/",
        "bibtex": "@inproceedings{ronnqvist-etal-2022-explaining,\n    title = \"Explaining Classes through Stable Word Attributions\",\n    author = {R{\\\"o}nnqvist, Samuel  and\n      Kyr{\\\"o}l{\\\"a}inen, Aki-Juhani  and\n      Myntti, Amanda  and\n      Ginter, Filip  and\n      Laippala, Veronika},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.85/\",\n    doi = \"10.18653/v1/2022.findings-acl.85\",\n    pages = \"1063--1074\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.85.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.85/",
        "pdf_size": 5181571,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3195770783661713718&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "University of Turku, Finland; University of Turku, Finland; University of Turku, Finland; University of Turku, Finland; University of Turku, Finland",
        "aff_domain": "utu.fi;utu.fi;utu.fi;utu.fi;utu.fi",
        "email": "utu.fi;utu.fi;utu.fi;utu.fi;utu.fi",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Turku",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utu.fi",
        "aff_unique_abbr": "UTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "2022.acl-long.85",
        "title": "Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks. However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs. Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task, e.g., generating a graph that is connected and acyclic can be attributed to its structural constraints, while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts. In this work, we study pre-trained language models that generate explanation graphs in an end-to-end manner and analyze their ability to learn the structural constraints and semantics of such graphs. We first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent. Since curating large amount of human-annotated graphs is expensive and tedious, we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs. Next, we leverage these graphs in different contrastive learning models with Max-Margin and InfoNCE losses. Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks. Lastly, we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human-like negative graphs can lead to further improvements.",
        "author": "Swarnadeep Saha; Prateek Yadav; Mohit Bansal",
        "authorids": "/s/swarnadeep-saha/; /p/prateek-yadav/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{saha-etal-2022-explanation,\n    title = \"Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning\",\n    author = \"Saha, Swarnadeep  and\n      Yadav, Prateek  and\n      Bansal, Mohit\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.85/\",\n    doi = \"10.18653/v1/2022.acl-long.85\",\n    pages = \"1190--1208\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.85.pdf",
        "site": "https://aclanthology.org/2022.acl-long.85/",
        "pdf_size": 1041834,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16358927660238403113&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/swarnaHub/ExplagraphGen",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.36",
        "title": "Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline. Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods. Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration, rather than lack of relevant knowledge in the representations. We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient.",
        "author": "Mohsen Tabasi; Kiamehr Rezaee; Mohammad Taher Pilehvar",
        "authorids": "/m/mohsen-tabasi/; /k/kiamehr-rezaee/; /m/mohammad-taher-pilehvar/",
        "bibtex": "@inproceedings{tabasi-etal-2022-exploiting,\n    title = \"Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task\",\n    author = \"Tabasi, Mohsen  and\n      Rezaee, Kiamehr  and\n      Pilehvar, Mohammad Taher\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.36/\",\n    doi = \"10.18653/v1/2022.acl-short.36\",\n    pages = \"325--332\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.36.pdf",
        "site": "https://aclanthology.org/2022.acl-short.36/",
        "pdf_size": 1302789,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11507037597407508490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of CE, Iran University of Science and Technology, Tehran, Iran; Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK+*Work done as a Master\u2019s student at IUST; Tehran Institute for Advanced Studies, Khatam University, Tehran, Iran",
        "aff_domain": "comp.iust.ac.ir;cardiff.ac.uk;cam.ac.uk",
        "email": "comp.iust.ac.ir;cardiff.ac.uk;cam.ac.uk",
        "github": "https://github.com/tabasy/similarity_prompting",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+0;2",
        "aff_unique_norm": "Iran University of Science and Technology;Cardiff University;Tehran Institute for Advanced Studies",
        "aff_unique_dep": "Department of CE;School of Computer Science and Informatics;",
        "aff_unique_url": ";https://www.cardiff.ac.uk;",
        "aff_unique_abbr": ";Cardiff;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Tehran;Cardiff;",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "Iran;United Kingdom"
    },
    {
        "id": "2022.acl-long.133",
        "title": "Exploring and Adapting Chinese GPT to Pinyin Input Method",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While GPT has become the de-facto method for text generation tasks, its application to pinyin input method remains unexplored. In this work, we make the first exploration to leverage Chinese GPT for pinyin input method. We find that a frozen GPT achieves state-of-the-art performance on perfect pinyin. However, the performance drops dramatically when the input includes abbreviated pinyin.A reason is that an abbreviated pinyin can be mapped to many perfect pinyin, which links to even larger number of Chinese characters. We mitigate this issue with two strategies,including enriching the context with pinyin and optimizing the training process to help distinguish homophones. To further facilitate the evaluation of pinyin input method, we create a dataset consisting of 270K instances from fifteen domains. Results show that our approach improves the performance on abbreviated pinyin across all domains. Model analysis demonstrates that both strategiescontribute to the performance boost.",
        "author": "Minghuan Tan; Yong Dai; Duyu Tang; Zhangyin Feng; Guoping Huang; Jing Jiang; Jiwei Li; Shuming Shi",
        "authorids": "/m/minghuan-tan/; /y/yong-dai/; /d/duyu-tang/; /z/zhangyin-feng/; /g/guoping-huang/; /j/jing-jiang/; /j/jiwei-li/; /s/shuming-shi/",
        "bibtex": "@inproceedings{tan-etal-2022-exploring,\n    title = \"Exploring and Adapting {C}hinese {GPT} to {P}inyin Input Method\",\n    author = \"Tan, Minghuan  and\n      Dai, Yong  and\n      Tang, Duyu  and\n      Feng, Zhangyin  and\n      Huang, Guoping  and\n      Jiang, Jing  and\n      Li, Jiwei  and\n      Shi, Shuming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.133/\",\n    doi = \"10.18653/v1/2022.acl-long.133\",\n    pages = \"1899--1909\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.133.pdf",
        "site": "https://aclanthology.org/2022.acl-long.133/",
        "pdf_size": 398968,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1281592072258299069&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Singapore Management University; Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Singapore Management University; Zhejiang University; Tencent AI Lab",
        "aff_domain": "phdcs.smu.edu.sg;tencent.com;tencent.com;tencent.com;tencent.com;smu.edu.sg;zju.edu.cn;tencent.com",
        "email": "phdcs.smu.edu.sg;tencent.com;tencent.com;tencent.com;tencent.com;smu.edu.sg;zju.edu.cn;tencent.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;0;2;1",
        "aff_unique_norm": "Singapore Management University;Tencent;Zhejiang University",
        "aff_unique_dep": ";Tencent AI Lab;",
        "aff_unique_url": "https://www.smu.edu.sg;https://ai.tencent.com;https://www.zju.edu.cn",
        "aff_unique_abbr": "SMU;Tencent AI Lab;ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;0;1;1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2022.findings-acl.324",
        "title": "Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this paper, we explore the capacity of a language model-based method for grammatical error detection in detail. We first show that 5 to 10% of training data are enough for a BERT-based error detection method to achieve performance equivalent to what a non-language model-based method can achieve with the full training data; recall improves much faster with respect to training data size in the BERT-based method than in the non-language model method. This suggests that (i) the BERT-based method should have a good knowledge of the grammar required to recognize certain types of error and that (ii) it can transform the knowledge into error detection rules by fine-tuning with few training samples, which explains its high generalization ability in grammatical error detection. We further show with pseudo error data that it actually exhibits such nice properties in learning rules for recognizing various types of error. Finally, based on these findings, we discuss a cost-effective method for detecting grammatical errors with feedback comments explaining relevant grammatical rules to learners.",
        "author": "Ryo Nagata; Manabu Kimura; Kazuaki Hanawa",
        "authorids": "/r/ryo-nagata/; /m/manabu-kimura/; /k/kazuaki-hanawa/",
        "bibtex": "@inproceedings{nagata-etal-2022-exploring,\n    title = \"Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors\",\n    author = \"Nagata, Ryo  and\n      Kimura, Manabu  and\n      Hanawa, Kazuaki\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.324/\",\n    doi = \"10.18653/v1/2022.findings-acl.324\",\n    pages = \"4107--4118\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.324.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.324/",
        "pdf_size": 648095,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2045946434105478033&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 5,
        "aff": "Konan University / Japan; GRAS Group, Inc. / Japan; RIKEN / Japan + Tohoku University / Japan",
        "aff_domain": "ml.hyogo-u.ac.jp;gras-group.co.jp;riken.jp",
        "email": "ml.hyogo-u.ac.jp;gras-group.co.jp;riken.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "Konan University;GRAS Group, Inc.;RIKEN;Tohoku University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.konan-u.ac.jp;;https://www.riken.jp;https://www.tohoku.ac.jp",
        "aff_unique_abbr": "Konan U;;RIKEN;Tohoku U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.findings-acl.248",
        "title": "Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Contrastive learning is emerging as a powerful technique for extracting knowledge from unlabeled data. This technique requires a balanced mixture of two ingredients: positive (similar) and negative (dissimilar) samples. This is typically achieved by maintaining a queue of negative samples during training. Prior works in the area typically uses a fixed-length negative sample queue, but how the negative sample size affects the model performance remains unclear. The opaque impact of the number of negative samples on performance when employing contrastive learning aroused our in-depth exploration. This paper presents a momentum contrastive learning model with negative sample queue for sentence embedding, namely MoCoSE. We add the prediction layer to the online branch to make the model asymmetric and together with EMA update mechanism of the target branch to prevent the model from collapsing. We define a maximum traceable distance metric, through which we learn to what extent the text contrastive learning benefits from the historical information of negative samples. Our experiments find that the best results are obtained when the maximum traceable distance is at a certain range, demonstrating that there is an optimal range of historical information for a negative sample queue. We evaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS) task and obtain an average Spearman\u2019s correlation of 77.27%. Source code is available here.",
        "author": "Rui Cao; Yihao Wang; Yuxin Liang; Ling Gao; Jie Zheng; Jie Ren; Zheng Wang",
        "authorids": "/r/rui-cao/; /y/yihao-wang/; /y/yuxin-liang/; /l/ling-gao/; /j/jie-zheng/; /j/jie-ren/; /z/zheng-wang/",
        "bibtex": "@inproceedings{cao-etal-2022-exploring,\n    title = \"Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding\",\n    author = \"Cao, Rui  and\n      Wang, Yihao  and\n      Liang, Yuxin  and\n      Gao, Ling  and\n      Zheng, Jie  and\n      Ren, Jie  and\n      Wang, Zheng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.248/\",\n    doi = \"10.18653/v1/2022.findings-acl.248\",\n    pages = \"3138--3152\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.248.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.248/",
        "pdf_size": 411960,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18015782984200653335&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Northwest University; Northwest University; Northwest University; Northwest University+Shaanxi Normal University; Northwest University; Shaanxi Normal University; University of Leeds",
        "aff_domain": "stumail.nwu.edu.cn;stumail.nwu.edu.cn;stumail.nwu.edu.cn;nwu.edu.cn;nwu.edu.cn;snnu.edu.cn;leeds.ac.uk",
        "email": "stumail.nwu.edu.cn;stumail.nwu.edu.cn;stumail.nwu.edu.cn;nwu.edu.cn;nwu.edu.cn;snnu.edu.cn;leeds.ac.uk",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0+1;0;1;2",
        "aff_unique_norm": "Northwest University;Shaanxi Normal University;University of Leeds",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nwu.edu.cn;http://www.snnu.edu.cn;https://www.leeds.ac.uk",
        "aff_unique_abbr": "NWU;SNU;Leeds",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0;0;1",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2022.acl-long.177",
        "title": "ExtEnD: Extractive Entity Disambiguation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pre-trained language models. However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective. In contrast with this trend, here we propose ExtEnD, a novel local formulation for ED where we frame this task as a text extraction problem, and present two Transformer-based architectures that implement it. Based on experiments in and out of domain, and training over two different data regimes, we find our approach surpasses all its competitors in terms of both data efficiency and raw performance. ExtEnD outperforms its alternatives by as few as 6 F1 points on the more constrained of the two data regimes and, when moving to the other higher-resourced regime, sets a new state of the art on 4 out of 4 benchmarks under consideration, with average improvements of 0.7 F1 points overall and 1.1 F1 points out of domain. In addition, to gain better insights from our results, we also perform a fine-grained evaluation of our performances on different classes of label frequency, along with an ablation study of our architectural choices and an error analysis. We release our code and models for research purposes at https://github.com/SapienzaNLP/extend.",
        "author": "Edoardo Barba; Luigi Procopio; Roberto Navigli",
        "authorids": "/e/edoardo-barba/; /l/luigi-procopio/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{barba-etal-2022-extend,\n    title = \"{E}xt{E}n{D}: Extractive Entity Disambiguation\",\n    author = \"Barba, Edoardo  and\n      Procopio, Luigi  and\n      Navigli, Roberto\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.177/\",\n    doi = \"10.18653/v1/2022.acl-long.177\",\n    pages = \"2478--2488\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.177.pdf",
        "site": "https://aclanthology.org/2022.acl-long.177/",
        "pdf_size": 540791,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17097877708529934756&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome",
        "aff_domain": "uniroma1.it;uniroma1.it;uniroma1.it",
        "email": "uniroma1.it;uniroma1.it;uniroma1.it",
        "github": "https://github.com/SapienzaNLP/extend",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sapienza University of Rome",
        "aff_unique_dep": "Sapienza NLP Group",
        "aff_unique_url": "https://www.uniroma1.it",
        "aff_unique_abbr": "Sapienza",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Rome",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.findings-acl.9",
        "title": "Extract-Select: A Span Selection Framework for Nested Named Entity Recognition with Generative Adversarial Training",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Nested named entity recognition (NER) is a task in which named entities may overlap with each other. Span-based approaches regard nested NER as a two-stage span enumeration and classification task, thus having the innate ability to handle this task. However, they face the problems of error propagation, ignorance of span boundary, difficulty in long entity recognition and requirement on large-scale annotated data. In this paper, we propose Extract-Select, a span selection framework for nested NER, to tackle these problems. Firstly, we introduce a span selection framework in which nested entities with different input categories would be separately extracted by the extractor, thus naturally avoiding error propagation in two-stage span-based approaches. In the inference phase, the trained extractor selects final results specific to the given entity category. Secondly, we propose a hybrid selection strategy in the extractor, which not only makes full use of span boundary but also improves the ability of long entity recognition. Thirdly, we design a discriminator to evaluate the extraction result, and train both extractor and discriminator with generative adversarial training (GAT). The use of GAT greatly alleviates the stress on the dataset size. Experimental results on four benchmark datasets demonstrate that Extract-Select outperforms competitive nested NER models, obtaining state-of-the-art results. The proposed model also performs well when less labeled data are given, proving the effectiveness of GAT.",
        "author": "Peixin Huang; Xiang Zhao; Minghao Hu; Yang Fang; Xinyi Li; Weidong Xiao",
        "authorids": "/p/peixin-huang/; /x/xiang-zhao/; /m/minghao-hu/; /y/yang-fang/; /x/xinyi-li/; /w/weidong-xiao/",
        "bibtex": "@inproceedings{huang-etal-2022-extract,\n    title = \"Extract-Select: A Span Selection Framework for Nested Named Entity Recognition with Generative Adversarial Training\",\n    author = \"Huang, Peixin  and\n      Zhao, Xiang  and\n      Hu, Minghao  and\n      Fang, Yang  and\n      Li, Xinyi  and\n      Xiao, Weidong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.9/\",\n    doi = \"10.18653/v1/2022.findings-acl.9\",\n    pages = \"85--96\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.9.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.9/",
        "pdf_size": 604094,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12169685655001694917&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "National University of Defense Technology; National University of Defense Technology + Collaborative Innovation Center of Geospatial Technology; Information Research Center of Military Science; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology + Collaborative Innovation Center of Geospatial Technology",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;gmail.com;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;gmail.com;nudt.edu.cn;nudt.edu.cn;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+1;2;0;0;0+1",
        "aff_unique_norm": "National University of Defense Technology;Collaborative Innovation Center of Geospatial Technology;Information Research Center of Military Science",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.nudt.edu.cn/;;",
        "aff_unique_abbr": "NUDT;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2022.findings-acl.48",
        "title": "Extracting Latent Steering Vectors from Pretrained Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.",
        "author": "Nishant Subramani; Nivedita Suresh; Matthew Peters",
        "authorids": "/n/nishant-subramani/; /n/nivedita-suresh/; /m/matthew-e-peters/",
        "bibtex": "@inproceedings{subramani-etal-2022-extracting,\n    title = \"Extracting Latent Steering Vectors from Pretrained Language Models\",\n    author = \"Subramani, Nishant  and\n      Suresh, Nivedita  and\n      Peters, Matthew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.48/\",\n    doi = \"10.18653/v1/2022.findings-acl.48\",\n    pages = \"566--581\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.48.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.48/",
        "pdf_size": 1056125,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6871149061031577959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Allen Institute for Artificial Intelligence, Seattle, WA, USA; Arrive Bio, San Francisco, CA, USA; Allen Institute for Artificial Intelligence, Seattle, WA, USA",
        "aff_domain": "allenai.org;arrivebio.com;allenai.org",
        "email": "allenai.org;arrivebio.com;allenai.org",
        "github": "https://github.com/nishantsubramani/steering_vectors",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Allen Institute for Artificial Intelligence;Arrive Bio",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://allenai.org;",
        "aff_unique_abbr": "AI2;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Seattle;San Francisco",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.225",
        "title": "Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Online escort advertisement websites are widely used for advertising victims of human trafficking. Domain experts agree that advertising multiple people in the same ad is a strong indicator of trafficking. Thus, extracting person names from the text of these ads can provide valuable clues for further analysis. However, Named-Entity Recognition (NER) on escort ads is challenging because the text can be noisy, colloquial and often lacking proper grammar and punctuation. Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task. In this paper, we propose NEAT (Name Extraction Against Trafficking) for extracting person names. It effectively combines classic rule-based and dictionary extractors with a contextualized language model to capture ambiguous names (e.g penny, hazel) and adapts to adversarial changes in the text by expanding its dictionary. NEAT shows 19% improvement on average in the F1 classification score for name extraction compared to previous state-of-the-art in two domain-specific datasets.",
        "author": "Yifei Li; Pratheeksha Nair; Kellin Pelrine; Reihaneh Rabbany",
        "authorids": "/y/yifei-li/; /p/pratheeksha-nair/; /k/kellin-pelrine/; /r/reihaneh-rabbany/",
        "bibtex": "@inproceedings{li-etal-2022-extracting,\n    title = \"Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking\",\n    author = \"Li, Yifei  and\n      Nair, Pratheeksha  and\n      Pelrine, Kellin  and\n      Rabbany, Reihaneh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.225/\",\n    doi = \"10.18653/v1/2022.findings-acl.225\",\n    pages = \"2854--2868\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.225.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.225/",
        "pdf_size": 366971,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17515590826867746511&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science, McGill University + Mila \u2013 Quebec AI Institute; School of Computer Science, McGill University + Mila \u2013 Quebec AI Institute; School of Computer Science, McGill University + Mila \u2013 Quebec AI Institute; School of Computer Science, McGill University + Mila \u2013 Quebec AI Institute",
        "aff_domain": "mcgill.ca;mila.quebec;mcgill.ca;mcgill.ca",
        "email": "mcgill.ca;mila.quebec;mcgill.ca;mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "McGill University;Quebec AI Institute",
        "aff_unique_dep": "School of Computer Science;AI",
        "aff_unique_url": "https://www.mcgill.ca;https://mila.quebec",
        "aff_unique_abbr": "McGill;Mila",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Montreal;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.209",
        "title": "FIBER: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER \u2013 a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework. The fill-in-the-blanks setting tests a model\u2019s understanding of a video by requiring it to predict a masked noun phrase in the caption of the video, given the video and the surrounding text. The FIBER benchmark does not share the weaknesses of the current state-of-the-art language-informed video understanding tasks, namely: (1) video question answering using multiple-choice questions, where models perform relatively well because they exploit linguistic biases in the task formulation, thus making our framework challenging for the current state-of-the-art systems to solve; and (2) video captioning, which relies on an open-ended evaluation framework that is often inaccurate because system answers may be perceived as incorrect if they differ in form from the ground truth. The FIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/.",
        "author": "Santiago Castro; Ruoyao Wang; Pingxuan Huang; Ian Stewart; Oana Ignat; Nan Liu; Jonathan Stroud; Rada Mihalcea",
        "authorids": "/s/santiago-castro/; /r/ruoyao-wang/; /p/pingxuan-huang/; /i/ian-stewart/; /o/oana-ignat/; /n/nan-liu/; /j/jonathan-stroud/; /r/rada-mihalcea/",
        "bibtex": "@inproceedings{castro-etal-2022-fiber,\n    title = \"{FIBER}: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework\",\n    author = \"Castro, Santiago  and\n      Wang, Ruoyao  and\n      Huang, Pingxuan  and\n      Stewart, Ian  and\n      Ignat, Oana  and\n      Liu, Nan  and\n      Stroud, Jonathan  and\n      Mihalcea, Rada\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.209/\",\n    doi = \"10.18653/v1/2022.acl-long.209\",\n    pages = \"2925--2940\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.209.pdf",
        "site": "https://aclanthology.org/2022.acl-long.209/",
        "pdf_size": 7589379,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10490525060879114485&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA; University of Michigan \u2013 Ann Arbor, USA",
        "aff_domain": "umich.edu; ; ; ; ; ; ; ",
        "email": "umich.edu; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://lit.eecs.umich.edu/fiber/",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.82",
        "title": "FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, a commonly used language to perform computations on numerical values in spreadsheets, is a valuable supervision for numerical reasoning in tables. Considering large amounts of spreadsheets available on the web, we propose FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining. Two novel self-supervised pretraining objectives are derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP). While our proposed objectives are generic for encoders, to better capture spreadsheet table layouts and structures, FORTAP is built upon TUTA, the first transformer-based method for spreadsheet table pretraining with tree attention. FORTAP outperforms state-of-the-art methods by large margins on three representative datasets of formula prediction, question answering, and cell type classification, showing the great potential of leveraging formulas for table pretraining.",
        "author": "Zhoujun Cheng; Haoyu Dong; Ran Jia; Pengfei Wu; Shi Han; Fan Cheng; Dongmei Zhang",
        "authorids": "/z/zhoujun-cheng/; /h/haoyu-dong/; /r/ran-jia/; /p/pengfei-wu/; /s/shi-han/; /f/fan-cheng/; /d/dongmei-zhang/",
        "bibtex": "@inproceedings{cheng-etal-2022-fortap,\n    title = \"{FORTAP}: Using Formulas for Numerical-Reasoning-Aware Table Pretraining\",\n    author = \"Cheng, Zhoujun  and\n      Dong, Haoyu  and\n      Jia, Ran  and\n      Wu, Pengfei  and\n      Han, Shi  and\n      Cheng, Fan  and\n      Zhang, Dongmei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.82/\",\n    doi = \"10.18653/v1/2022.acl-long.82\",\n    pages = \"1150--1166\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.82.pdf",
        "site": "https://aclanthology.org/2022.acl-long.82/",
        "pdf_size": 4284984,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6848407624338472778&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai 200240, China; Microsoft Research Asia, China; Microsoft Research Asia, China; Fudan University, China; Microsoft Research Asia, China; MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai 200240, China; Microsoft Research Asia, China",
        "aff_domain": "sjtu.edu.cn;microsoft.com;microsoft.com;fudan.edu.cn;microsoft.com;sjtu.edu.cn;microsoft.com",
        "email": "sjtu.edu.cn;microsoft.com;microsoft.com;fudan.edu.cn;microsoft.com;sjtu.edu.cn;microsoft.com",
        "github": "https://github.com/microsoft/TUTA_table_understanding",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;2;1;0;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Microsoft;Fudan University",
        "aff_unique_dep": "AI Institute;Microsoft Research Asia;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.microsoft.com/en-us/research/group/asia;https://www.fudan.edu.cn",
        "aff_unique_abbr": "SJTU;MSRA;Fudan",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.354",
        "title": "FaVIQ: FAct Verification from Information-seeking Questions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite significant interest in developing general purpose fact checking models, it is challenging to construct a large-scale fact verification dataset with realistic real-world claims. Existing claims are either authored by crowdworkers, thereby introducing subtle biases thatare difficult to control for, or manually verified by professional fact checkers, causing them to be expensive and limited in scale. In this paper, we construct a large-scale challenging fact verification dataset called FAVIQ, consisting of 188k claims derived from an existing corpus of ambiguous information-seeking questions. The ambiguities in the questions enable automatically constructing true and false claims that reflect user confusions (e.g., the year of the movie being filmed vs. being released). Claims in FAVIQ are verified to be natural, contain little lexical bias, and require a complete understanding of the evidence for verification. Our experiments show that the state-of-the-art models are far from solving our new task. Moreover, training on our data helps in professional fact-checking, outperforming models trained on the widely used dataset FEVER or in-domain data by up to 17% absolute. Altogether, our data will serve as a challenging benchmark for natural language understanding and support future progress in professional fact checking.",
        "author": "Jungsoo Park; Sewon Min; Jaewoo Kang; Luke Zettlemoyer; Hannaneh Hajishirzi",
        "authorids": "/j/jungsoo-park/; /s/sewon-min/; /j/jaewoo-kang/; /l/luke-zettlemoyer/; /h/hannaneh-hajishirzi/",
        "bibtex": "@inproceedings{park-etal-2022-faviq,\n    title = \"{F}a{VIQ}: {FA}ct Verification from Information-seeking Questions\",\n    author = \"Park, Jungsoo  and\n      Min, Sewon  and\n      Kang, Jaewoo  and\n      Zettlemoyer, Luke  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.354/\",\n    doi = \"10.18653/v1/2022.acl-long.354\",\n    pages = \"5154--5166\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.354.pdf",
        "site": "https://aclanthology.org/2022.acl-long.354/",
        "pdf_size": 561065,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8606768868388186011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Korea University; University of Washington; Korea University; University of Washington; University of Washington+Allen Institute of AI",
        "aff_domain": "korea.ac.kr;cs.washington.edu;korea.ac.kr;cs.washington.edu;cs.washington.edu",
        "email": "korea.ac.kr;cs.washington.edu;korea.ac.kr;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "https://faviq.github.io",
        "author_num": 5,
        "aff_unique_index": "0;1;0;1;1+2",
        "aff_unique_norm": "Korea University;University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": ";;AI Research",
        "aff_unique_url": "https://www.korea.ac.kr;https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "KU;UW;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;1+1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2022.findings-acl.66",
        "title": "Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA. We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades. FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer. Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.",
        "author": "Yao Zhang; Peiyao Li; Hongru Liang; Adam Jatowt; Zhenglu Yang",
        "authorids": "/y/yao-zhang/; /p/peiyao-li/; /h/hongru-liang/; /a/adam-jatowt/; /z/zhenglu-yang/",
        "bibtex": "@inproceedings{zhang-etal-2022-fact,\n    title = \"Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs\",\n    author = \"Zhang, Yao  and\n      Li, Peiyao  and\n      Liang, Hongru  and\n      Jatowt, Adam  and\n      Yang, Zhenglu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.66/\",\n    doi = \"10.18653/v1/2022.findings-acl.66\",\n    pages = \"788--802\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.66.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.66/",
        "pdf_size": 1811649,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2798114929249453228&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 4,
        "aff": "TKLNDST, CS, Nankai University, China; TKLNDST, CS, Nankai University, China; Sichuan University, China; University of Innsbruck, Austria; TKLNDST, CS, Nankai University, China",
        "aff_domain": "mail.nankai.edu.cn;mail.nankai.edu.cn;scu.edu.cn;uibk.ac.at;nankai.edu.cn",
        "email": "mail.nankai.edu.cn;mail.nankai.edu.cn;scu.edu.cn;uibk.ac.at;nankai.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Nankai University;Sichuan University;University of Innsbruck",
        "aff_unique_dep": "Computer Science;;",
        "aff_unique_url": ";https://www.scu.edu.cn;https://www.uibk.ac.at",
        "aff_unique_abbr": ";SCU;UIBK",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "China;Austria"
    },
    {
        "id": "2022.findings-acl.240",
        "title": "Factual Consistency of Multilingual Pretrained Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages. We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
        "author": "Constanza Fierro; Anders S\u00f8gaard",
        "authorids": "/c/constanza-fierro/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{fierro-sogaard-2022-factual,\n    title = \"Factual Consistency of Multilingual Pretrained Language Models\",\n    author = \"Fierro, Constanza  and\n      S{\\o}gaard, Anders\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.240/\",\n    doi = \"10.18653/v1/2022.findings-acl.240\",\n    pages = \"3046--3052\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.240.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.240/",
        "pdf_size": 2984229,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4900886223162147172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Copenhagen; University of Copenhagen",
        "aff_domain": "di.ku.dk;di.ku.dk",
        "email": "di.ku.dk;di.ku.dk",
        "github": "https://github.com/coastalcph/mpararelon",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Copenhagen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ku.dk",
        "aff_unique_abbr": "UCPH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "2022.acl-long.77",
        "title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model\u2019s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.",
        "author": "Soumya Sanyal; Harman Singh; Xiang Ren",
        "authorids": "/s/soumya-sanyal/; /h/harman-singh/; /x/xiang-ren/",
        "bibtex": "@inproceedings{sanyal-etal-2022-fairr,\n    title = \"{F}ai{RR}: Faithful and Robust Deductive Reasoning over Natural Language\",\n    author = \"Sanyal, Soumya  and\n      Singh, Harman  and\n      Ren, Xiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.77/\",\n    doi = \"10.18653/v1/2022.acl-long.77\",\n    pages = \"1075--1093\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.77.pdf",
        "site": "https://aclanthology.org/2022.acl-long.77/",
        "pdf_size": 518479,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12349261321618606307&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Southern California; Indian Institute of Technology, Delhi; University of Southern California",
        "aff_domain": "usc.edu;gmail.com;usc.edu",
        "email": "usc.edu;gmail.com;usc.edu",
        "github": "https://github.com/INK-USC/FaiRR",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Southern California;Indian Institute of Technology Delhi",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://www.iitdelhi.ac.in",
        "aff_unique_abbr": "USC;IIT Delhi",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Los Angeles;Delhi",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2022.acl-long.541",
        "title": "Fair and Argumentative Language Modeling for Computational Argumentation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although much work in NLP has focused on measuring and mitigating stereotypical bias in semantic spaces, research addressing bias in computational argumentation is still in its infancy. In this paper, we address this research gap and conduct a thorough investigation of bias in argumentative language models. To this end, we introduce ABBA, a novel resource for bias measurement specifically tailored to argumentation. We employ our resource to assess the effect of argumentative fine-tuning and debiasing on the intrinsic bias found in transformer-based language models using a lightweight adapter-based approach that is more sustainable and parameter-efficient than full fine-tuning. Finally, we analyze the potential impact of language model debiasing on the performance in argument quality prediction, a downstream task of computational argumentation. Our results show that we are able to successfully and sustainably remove bias in general and argumentative language models while preserving (and sometimes improving) model performance in downstream tasks. We make all experimental code and data available at https://github.com/umanlp/FairArgumentativeLM.",
        "author": "Carolin Holtermann; Anne Lauscher; Simone Ponzetto",
        "authorids": "/c/carolin-holtermann/; /a/anne-lauscher/; /s/simone-paolo-ponzetto/",
        "bibtex": "@inproceedings{holtermann-etal-2022-fair,\n    title = \"Fair and Argumentative Language Modeling for Computational Argumentation\",\n    author = \"Holtermann, Carolin  and\n      Lauscher, Anne  and\n      Ponzetto, Simone\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.541/\",\n    doi = \"10.18653/v1/2022.acl-long.541\",\n    pages = \"7841--7861\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.541.pdf",
        "site": "https://aclanthology.org/2022.acl-long.541/",
        "pdf_size": 8561550,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9630942119107195871&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 6,
        "aff": "Data and Web Science Group, University of Mannheim, Germany; MilaNLP, Bocconi University, Italy; Data and Web Science Group, University of Mannheim, Germany",
        "aff_domain": "mail.uni-mannheim.de;unibocconi.it;informatik.uni-mannheim.de",
        "email": "mail.uni-mannheim.de;unibocconi.it;informatik.uni-mannheim.de",
        "github": "https://github.com/umanlp/FairArgumentativeLM",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Mannheim;Bocconi University",
        "aff_unique_dep": "Data and Web Science Group;MilaNLP",
        "aff_unique_url": "https://www.uni-mannheim.de;https://www.bocconi.edu",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;Italy"
    },
    {
        "id": "2022.acl-long.301",
        "title": "FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.",
        "author": "Ilias Chalkidis; Tommaso Pasini; Sheng Zhang; Letizia Tomada; Sebastian Schwemer; Anders S\u00f8gaard",
        "authorids": "/i/ilias-chalkidis/; /t/tommaso-pasini/; /s/sheng-zhang/; /l/letizia-tomada/; /s/sebastian-schwemer/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{chalkidis-etal-2022-fairlex,\n    title = \"{F}air{L}ex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing\",\n    author = \"Chalkidis, Ilias  and\n      Pasini, Tommaso  and\n      Zhang, Sheng  and\n      Tomada, Letizia  and\n      Schwemer, Sebastian  and\n      S{\\o}gaard, Anders\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.301/\",\n    doi = \"10.18653/v1/2022.acl-long.301\",\n    pages = \"4389--4406\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.301.pdf",
        "site": "https://aclanthology.org/2022.acl-long.301/",
        "pdf_size": 510433,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13592117166249606952&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Copenhagen, Denmark; Department of Computer Science, University of Copenhagen, Denmark; National University of Defense Technology, People\u2019s Republic of China; Faculty of Law, University of Copenhagen, Denmark; Faculty of Law, University of Copenhagen, Denmark; Department of Computer Science, University of Copenhagen, Denmark",
        "aff_domain": "di.ku.dk; ; ; ; ; ",
        "email": "di.ku.dk; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "University of Copenhagen;National University of Defense Technology",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ku.dk;http://www.nudt.edu.cn/",
        "aff_unique_abbr": "UCPH;NUDT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "Denmark;China"
    },
    {
        "id": "2022.acl-long.100",
        "title": "Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite recent progress in abstractive summarization, systems still suffer from faithfulness errors. While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive. In this work, we present a framework for evaluating the effective faithfulness of summarization systems, by generating a faithfulness-abstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as well as recently proposed methods for improving faithfulness, fail to consistently improve over the control at the same level of abstractiveness. Finally, we learn a selector to identify the most faithful and abstractive summary for a given document, and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets. Moreover, we show that our system is able to achieve a better faithfulness-abstractiveness trade-off than the control at the same level of abstractiveness.",
        "author": "Faisal Ladhak; Esin Durmus; He He; Claire Cardie; Kathleen McKeown",
        "authorids": "/f/faisal-ladhak/; /e/esin-durmus/; /h/he-he/; /c/claire-cardie/; /k/kathleen-mckeown/",
        "bibtex": "@inproceedings{ladhak-etal-2022-faithful,\n    title = \"Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization\",\n    author = \"Ladhak, Faisal  and\n      Durmus, Esin  and\n      He, He  and\n      Cardie, Claire  and\n      McKeown, Kathleen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.100/\",\n    doi = \"10.18653/v1/2022.acl-long.100\",\n    pages = \"1410--1421\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.100.pdf",
        "site": "https://aclanthology.org/2022.acl-long.100/",
        "pdf_size": 358844,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9324520508143136437&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Columbia University; Stanford University; New York University; Cornell University; Columbia University",
        "aff_domain": "cs.columbia.edu; ; ; ; ",
        "email": "cs.columbia.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "Columbia University;Stanford University;New York University;Cornell University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.columbia.edu;https://www.stanford.edu;https://www.nyu.edu;https://www.cornell.edu",
        "aff_unique_abbr": "Columbia;Stanford;NYU;Cornell",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.34",
        "title": "Fantastic Questions and Where to Find Them: FairytaleQA \u2013 An Authentic Dataset for Narrative Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements. Drawing on the reading education research, we introduce FairytaleQA, a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations. Our dataset is valuable in two folds: First, we ran existing QA models on our dataset and confirmed that this annotation helps assess models\u2019 fine-grained learning skills. Second, the dataset supports question generation (QG) task in the education domain. Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.",
        "author": "Ying Xu; Dakuo Wang; Mo Yu; Daniel Ritchie; Bingsheng Yao; Tongshuang Wu; Zheng Zhang; Toby Jia-Jun Li; Nora Bradford; Branda Sun; Tran Bao Hoang; Yisi Sang; Yufang Hou; Xiaojuan Ma; Diyi Yang; Nanyun Peng; Zhou Yu; Mark Warschauer",
        "authorids": "/y/ying-xu/; /d/dakuo-wang/; /m/mo-yu/; /d/daniel-ritchie/; /b/bingsheng-yao/; /t/tongshuang-wu/; /z/zheng-zhang/; /t/toby-jia-jun-li/; /n/nora-bradford/; /b/branda-sun/; /t/tran-bao-hoang/; /y/yisi-sang/; /y/yufang-hou/; /x/xiaojuan-ma/; /d/diyi-yang/; /n/nanyun-peng/; /z/zhou-yu/; /m/mark-warschauer/",
        "bibtex": "@inproceedings{xu-etal-2022-fantastic,\n    title = \"Fantastic Questions and Where to Find Them: {F}airytale{QA} {--} An Authentic Dataset for Narrative Comprehension\",\n    author = \"Xu, Ying  and\n      Wang, Dakuo  and\n      Yu, Mo  and\n      Ritchie, Daniel  and\n      Yao, Bingsheng  and\n      Wu, Tongshuang  and\n      Zhang, Zheng  and\n      Li, Toby Jia-Jun  and\n      Bradford, Nora  and\n      Sun, Branda  and\n      Hoang, Tran Bao  and\n      Sang, Yisi  and\n      Hou, Yufang  and\n      Ma, Xiaojuan  and\n      Yang, Diyi  and\n      Peng, Nanyun  and\n      Yu, Zhou  and\n      Warschauer, Mark\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.34/\",\n    doi = \"10.18653/v1/2022.acl-long.34\",\n    pages = \"447--460\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.34.pdf",
        "site": "https://aclanthology.org/2022.acl-long.34/",
        "pdf_size": 325147,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4974488498799573541&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 10,
        "aff": "University of California Irvine; IBM Research; WeChat AI, Tencent; University of California Irvine; Rensselaer Polytechnic Institute; University of Washington; University of Notre Dame; University of Notre Dame; University of California Irvine; University of California Irvine; University of California Irvine; Syracuse University; IBM Research Ireland; Hong Kong Univ. of Sci and Tech; Georgia Institute of Technology; University of California Los Angeles; Columbia University; University of California Irvine",
        "aff_domain": "uci.edu;ibm.com;tencent.com;uci.edu;rpi.edu; ; ; ; ; ; ; ; ; ; ; ; ;uci.edu",
        "email": "uci.edu;ibm.com;tencent.com;uci.edu;rpi.edu; ; ; ; ; ; ; ; ; ; ; ; ;uci.edu",
        "github": "https://github.com/uci-soe/FairytaleQAData",
        "project": "",
        "author_num": 18,
        "aff_unique_index": "0;1;2;0;3;4;5;5;0;0;0;6;1;7;8;9;10;0",
        "aff_unique_norm": "University of California, Irvine;IBM;Tencent;Rensselaer Polytechnic Institute;University of Washington;University of Notre Dame;Syracuse University;Hong Kong University of Science and Technology;Georgia Institute of Technology;University of California, Los Angeles;Columbia University",
        "aff_unique_dep": ";IBM Research;WeChat AI;;;;;;;;",
        "aff_unique_url": "https://www.uci.edu;https://www.ibm.com/research;https://www.tencent.com;https://www.rpi.edu;https://www.washington.edu;https://www.nd.edu;https://www.syracuse.edu;https://www.ust.hk;https://www.gatech.edu;https://www.ucla.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UCI;IBM;Tencent;RPI;UW;Notre Dame;Syracuse;HKUST;Georgia Tech;UCLA;Columbia",
        "aff_campus_unique_index": "0;0;0;0;0;2;3;0",
        "aff_campus_unique": "Irvine;;Hong Kong SAR;Los Angeles",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0;0;0;0;0;2;1;0;0;0;0",
        "aff_country_unique": "United States;China;Ireland"
    },
    {
        "id": "2022.acl-long.556",
        "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \u201cfantastic\u201d and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.",
        "author": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp",
        "authorids": "/y/yao-lu/; /m/max-bartolo/; /a/alastair-moore/; /s/sebastian-riedel/; /p/pontus-stenetorp/",
        "bibtex": "@inproceedings{lu-etal-2022-fantastically,\n    title = \"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\",\n    author = \"Lu, Yao  and\n      Bartolo, Max  and\n      Moore, Alastair  and\n      Riedel, Sebastian  and\n      Stenetorp, Pontus\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.556/\",\n    doi = \"10.18653/v1/2022.acl-long.556\",\n    pages = \"8086--8098\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.556.pdf",
        "site": "https://aclanthology.org/2022.acl-long.556/",
        "pdf_size": 1624577,
        "gs_citation": 1172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7057209814332192717&as_sdt=8005&sciodt=0,7&hl=en",
        "gs_version_total": 10,
        "aff": "University College London; University College London; Mishcon de Reya LLP; University College London; University College London",
        "aff_domain": "cs.ucl.ac.uk;cs.ucl.ac.uk;mishcon.com;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;cs.ucl.ac.uk;mishcon.com;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University College London;Mishcon de Reya",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.mishcon.com",
        "aff_unique_abbr": "UCL;Mishcon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.47",
        "title": "Fast Nearest Neighbor Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Though nearest neighbor Machine Translation (kNN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. kNN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast kNN-MT to address this issue. Fast kNN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast kNN-MT is two-orders faster than kNN-MT, and is only two times slower than the standard NMT model. Fast kNN-MT enables the practical use of kNN-MT systems in real-world MT applications. The code is available at https://github.com/ShannonAI/fast-knn-nmt.",
        "author": "Yuxian Meng; Xiaoya Li; Xiayu Zheng; Fei Wu; Xiaofei Sun; Tianwei Zhang; Jiwei Li",
        "authorids": "/y/yuxian-meng/; /x/xiaoya-li/; /x/xiayu-zheng/; /f/fei-wu/; /x/xiaofei-sun/; /t/tianwei-zhang/; /j/jiwei-li/",
        "bibtex": "@inproceedings{meng-etal-2022-fast,\n    title = \"Fast Nearest Neighbor Machine Translation\",\n    author = \"Meng, Yuxian  and\n      Li, Xiaoya  and\n      Zheng, Xiayu  and\n      Wu, Fei  and\n      Sun, Xiaofei  and\n      Zhang, Tianwei  and\n      Li, Jiwei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.47/\",\n    doi = \"10.18653/v1/2022.findings-acl.47\",\n    pages = \"555--565\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.47.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.47/",
        "pdf_size": 407420,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6244356947292903784&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "College of Computer Science and Technology, Zhejiang University + Shannon.AI; Shannon.AI; Peking University; College of Computer Science and Technology, Zhejiang University + Shannon.AI + Shanghai Institute for Advanced Study of Zhejiang University + Shanghai AI Laboratory; College of Computer Science and Technology, Zhejiang University + Shannon.AI; Nanyang Technological University; College of Computer Science and Technology, Zhejiang University + Shannon.AI",
        "aff_domain": "shannonai.com;shannonai.com;pku.edu.cn;zju.edu.cn;shannonai.com;ntu.edu.sg;shannonai.com",
        "email": "shannonai.com;shannonai.com;pku.edu.cn;zju.edu.cn;shannonai.com;ntu.edu.sg;shannonai.com",
        "github": "https://github.com/ShannonAI/fast-knn-nmtmodel",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;2;0+1+0+3;0+1;4;0+1",
        "aff_unique_norm": "Zhejiang University;Shannon.AI;Peking University;Shanghai AI Laboratory;Nanyang Technological University",
        "aff_unique_dep": "College of Computer Science and Technology;;;;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.shannon.ai;http://www.pku.edu.cn;https://www.shanghai-ai-lab.com;https://www.ntu.edu.sg",
        "aff_unique_abbr": "ZJU;Shannon.AI;Peking U;SAIL;NTU",
        "aff_campus_unique_index": ";1;;",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+1;1;0;0+1+0+0;0+1;2;0+1",
        "aff_country_unique": "China;United States;Singapore"
    },
    {
        "id": "2022.acl-long.403",
        "title": "Feeding What You Need by Understanding What You Learned",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it. Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match (EM) and F1. However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus. In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status. Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner. Based on it, we further uncover and disentangle the connections between various data properties and model performance. Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency. Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22% / 8.71% improvement of EM / F1 on MRC tasks.",
        "author": "Xiaoqiang Wang; Bang Liu; Fangli Xu; Bo Long; Siliang Tang; Lingfei Wu",
        "authorids": "/x/xiaoqiang-wang/; /b/bang-liu/; /f/fangli-xu/; /b/bo-long/; /s/siliang-tang/; /l/lingfei-wu/",
        "bibtex": "@inproceedings{wang-etal-2022-feeding,\n    title = \"Feeding What You Need by Understanding What You Learned\",\n    author = \"Wang, Xiaoqiang  and\n      Liu, Bang  and\n      Xu, Fangli  and\n      Long, Bo  and\n      Tang, Siliang  and\n      Wu, Lingfei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.403/\",\n    doi = \"10.18653/v1/2022.acl-long.403\",\n    pages = \"5858--5874\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.403.pdf",
        "site": "https://aclanthology.org/2022.acl-long.403/",
        "pdf_size": 835612,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15598273271905801889&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 5,
        "aff": "Zhejiang University; Universit\u00e9 de Montr\u00e9al & Mila; Squirrel AI Learning; JD.COM; JD.COM Silicon Valley Research Center; JD.COM Silicon Valley Research Center",
        "aff_domain": "zju.edu.cn;zju.edu.cn;umontreal.ca;yixue.us;jd.com;jd.com",
        "email": "zju.edu.cn;zju.edu.cn;umontreal.ca;yixue.us;jd.com;jd.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;3;3",
        "aff_unique_norm": "Zhejiang University;Universit\u00e9 de Montr\u00e9al;Squirrel Ai Learning;JD.com",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.umontreal.ca;https://www.squirrelai.com/;https://www.jd.com",
        "aff_unique_abbr": "ZJU;UdeM;;JD",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Silicon Valley",
        "aff_country_unique_index": "0;1;0;0;2;2",
        "aff_country_unique": "China;Canada;United States"
    },
    {
        "id": "2022.acl-long.43",
        "title": "Few-Shot Class-Incremental Learning for Named Entity Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous work of class-incremental learning for Named Entity Recognition (NER) relies on the assumption that there exists abundance of labeled data for the training of new classes. In this work, we study a more challenging but practical problem, i.e., few-shot class-incremental learning for NER, where an NER model is trained with only few labeled samples of the new classes, without forgetting knowledge of the old ones. To alleviate the problem of catastrophic forgetting in few-shot class-incremental learning, we reconstruct synthetic training data of the old classes using the trained NER model, augmenting the training of new classes. We further develop a framework that distills from the existing model with both synthetic data, and real data from the current training set. Experimental results show that our approach achieves significant improvements over existing baselines.",
        "author": "Rui Wang; Tong Yu; Handong Zhao; Sungchul Kim; Subrata Mitra; Ruiyi Zhang; Ricardo Henao",
        "authorids": "/r/rui-wang/; /t/tong-yu/; /h/handong-zhao/; /s/sungchul-kim/; /s/subrata-mitra/; /r/ruiyi-zhang/; /r/ricardo-henao/",
        "bibtex": "@inproceedings{wang-etal-2022-shot,\n    title = \"Few-Shot Class-Incremental Learning for Named Entity Recognition\",\n    author = \"Wang, Rui  and\n      Yu, Tong  and\n      Zhao, Handong  and\n      Kim, Sungchul  and\n      Mitra, Subrata  and\n      Zhang, Ruiyi  and\n      Henao, Ricardo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.43/\",\n    doi = \"10.18653/v1/2022.acl-long.43\",\n    pages = \"571--582\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.43.pdf",
        "site": "https://aclanthology.org/2022.acl-long.43/",
        "pdf_size": 803777,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10853340370221969648&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Duke University; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Duke University",
        "aff_domain": "duke.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;duke.edu",
        "email": "duke.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;duke.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;0",
        "aff_unique_norm": "Duke University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.duke.edu;https://research.adobe.com",
        "aff_unique_abbr": "Duke;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.584",
        "title": "Few-Shot Learning with Siamese Networks and Label Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study the problem of building text classifiers with little or no training data, commonly known as zero and few-shot text classification. In recent years, an approach based on neural textual entailment models has been found to give strong results on a diverse range of tasks. In this work, we show that with proper pre-training, Siamese Networks that embed texts and labels offer a competitive alternative. These models allow for a large reduction in inference cost: constant in the number of labels rather than linear. Furthermore, we introduce label tuning, a simple and computationally efficient approach that allows to adapt the models in a few-shot setup by only changing the label embeddings. While giving lower performance than model fine-tuning, this approach has the architectural advantage that a single encoder can be shared by many different tasks.",
        "author": "Thomas M\u00fcller; Guillermo P\u00e9rez-Torr\u00f3; Marc Franco-Salvador",
        "authorids": "/t/thomas-mueller/; /g/guillermo-perez-torro/; /m/marc-franco-salvador/",
        "bibtex": "@inproceedings{muller-etal-2022-shot,\n    title = \"Few-Shot Learning with {S}iamese Networks and Label Tuning\",\n    author = {M{\\\"u}ller, Thomas  and\n      P{\\'e}rez-Torr{\\'o}, Guillermo  and\n      Franco-Salvador, Marc},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.584/\",\n    doi = \"10.18653/v1/2022.acl-long.584\",\n    pages = \"8532--8545\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.584.pdf",
        "site": "https://aclanthology.org/2022.acl-long.584/",
        "pdf_size": 315794,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6458124140312667703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Symanto Research, Valencia, Spain; Symanto Research, Valencia, Spain; Symanto Research, Valencia, Spain",
        "aff_domain": "symanto.com;symanto.com;symanto.com",
        "email": "symanto.com;symanto.com;symanto.com",
        "github": "",
        "project": "https://www.symanto.com",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Symanto Research",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Valencia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2022.acl-long.111",
        "title": "Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformer Architectures",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The enrichment of tabular datasets using external sources has gained significant attention in recent years. Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions. In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular datasets using unstructured data. By training over multiple datasets, our approach is able to develop generic models that can be applied to additional datasets with minimal training (i.e., few-shot). Our approach is based on an adaptation of BERT, for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences. Our evaluation, conducted on 17 datasets, shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.",
        "author": "Asaf Harari; Gilad Katz",
        "authorids": "/a/asaf-harari/; /g/gilad-katz/",
        "bibtex": "@inproceedings{harari-katz-2022-shot,\n    title = \"Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformer Architectures\",\n    author = \"Harari, Asaf  and\n      Katz, Gilad\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.111/\",\n    doi = \"10.18653/v1/2022.acl-long.111\",\n    pages = \"1577--1591\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.111.pdf",
        "site": "https://aclanthology.org/2022.acl-long.111/",
        "pdf_size": 572456,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9435060718472488197&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 4,
        "aff": "Ben-Gurion University of the Negev; Ben-Gurion University of the Negev",
        "aff_domain": "post.bgu.ac.il;post.bgu.ac.il",
        "email": "post.bgu.ac.il;post.bgu.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ben-Gurion University of the Negev",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bgu.ac.il",
        "aff_unique_abbr": "BGU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.acl-long.514",
        "title": "Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Style transfer is the task of rewriting a sentence into a target style while approximately preserving content. While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al. 2021) has attempted \u201cfew-shot\u201d style transfer using only 3-10 sentences at inference for style extraction. In this work we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available. We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim. We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages. Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob. We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simplification, gender neutralization, text anonymization) all without retraining the model. Finally, we find model evaluation to be difficult due to the lack of datasets and metrics for many languages. To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages, and use this data to design our automatic evaluations.",
        "author": "Kalpesh Krishna; Deepak Nathani; Xavier Garcia; Bidisha Samanta; Partha Talukdar",
        "authorids": "/k/kalpesh-krishna/; /d/deepak-nathani/; /x/xavier-garcia/; /b/bidisha-samanta/; /p/partha-talukdar/",
        "bibtex": "@inproceedings{krishna-etal-2022-shot,\n    title = \"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings\",\n    author = \"Krishna, Kalpesh  and\n      Nathani, Deepak  and\n      Garcia, Xavier  and\n      Samanta, Bidisha  and\n      Talukdar, Partha\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.514/\",\n    doi = \"10.18653/v1/2022.acl-long.514\",\n    pages = \"7439--7468\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.514.pdf",
        "site": "https://aclanthology.org/2022.acl-long.514/",
        "pdf_size": 1277606,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5888445359070671910&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "University of Massachusetts Amherst; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "cs.umass.edu;google.com;google.com;google.com;google.com",
        "email": "cs.umass.edu;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "https://martiansideofthemoon.github.io/2022/03/03/acl22.html",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.umass.edu;https://research.google",
        "aff_unique_abbr": "UMass Amherst;Google Research",
        "aff_campus_unique_index": "0;1;1;1;1",
        "aff_campus_unique": "Amherst;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.392",
        "title": "Few-shot Named Entity Recognition with Self-describing Networks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Few-shot NER needs to effectively capture information from limited instances and transfer useful knowledge from external resources. In this paper, we propose a self-describing mechanism for few-shot NER, which can effectively leverage illustrative instances and precisely transfer knowledge from external resources by describing both entity types and mentions using a universal concept set. Specifically, we design Self-describing Networks (SDNet), a Seq2Seq generation model which can universally describe mentions using concepts, automatically map novel entity types to concepts, and adaptively recognize entities on-demand. We pre-train SDNet with large-scale corpus, and conduct experiments on 8 benchmarks from different domains. Experiments show that SDNet achieves competitive performances on all benchmarks and achieves the new state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and robustness.",
        "author": "Jiawei Chen; Qing Liu; Hongyu Lin; Xianpei Han; Le Sun",
        "authorids": "/j/jiawei-chen/; /q/qing-liu/; /h/hongyu-lin/; /x/xianpei-han/; /l/le-sun/",
        "bibtex": "@inproceedings{chen-etal-2022-shot,\n    title = \"Few-shot Named Entity Recognition with Self-describing Networks\",\n    author = \"Chen, Jiawei  and\n      Liu, Qing  and\n      Lin, Hongyu  and\n      Han, Xianpei  and\n      Sun, Le\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.392/\",\n    doi = \"10.18653/v1/2022.acl-long.392\",\n    pages = \"5711--5722\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.392.pdf",
        "site": "https://aclanthology.org/2022.acl-long.392/",
        "pdf_size": 706812,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=980692392368004437&as_sdt=40000005&sciodt=0,22&hl=en",
        "gs_version_total": 6,
        "aff": "Chinese Information Processing Laboratory + University of Chinese Academy of Sciences; Chinese Information Processing Laboratory + University of Chinese Academy of Sciences; Chinese Information Processing Laboratory; Chinese Information Processing Laboratory + State Key Laboratory of Computer Science + Beijing Academy of Artificial Intelligence; Chinese Information Processing Laboratory + State Key Laboratory of Computer Science",
        "aff_domain": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "email": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0;0+2+3;0+2",
        "aff_unique_norm": "Chinese Information Processing Laboratory;University of Chinese Academy of Sciences;State Key Laboratory of Computer Science;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Information Processing;;;",
        "aff_unique_url": ";http://www.ucas.ac.cn;;https://www.baaic.cn",
        "aff_unique_abbr": ";UCAS;;BAAI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.38",
        "title": "FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The few-shot natural language understanding (NLU) task has attracted much recent attention. However, prior methods have been evaluated under a disparate set of protocols, which hinders fair comparison and measuring the progress of the field. To address this issue, we introduce an evaluation framework that improves previous evaluation procedures in three key aspects, i.e., test performance, dev-test correlation, and stability. Under this new evaluation framework, we re-evaluate several state-of-the-art few-shot methods for NLU tasks. Our framework reveals new insights: (1) both the absolute performance and relative gap of the methods were not accurately estimated in prior literature; (2) no single method dominates most tasks with consistent performance; (3) improvements of some methods diminish with a larger pretrained model; and (4) gains from different methods are often complementary and the best combined model performs close to a strong fully-supervised baseline. We open-source our toolkit, FewNLU, that implements our evaluation framework along with a number of state-of-the-art methods.",
        "author": "Yanan Zheng; Jing Zhou; Yujie Qian; Ming Ding; Chonghua Liao; Li Jian; Ruslan Salakhutdinov; Jie Tang; Sebastian Ruder; Zhilin Yang",
        "authorids": "/y/yanan-zheng/; /j/jing-zhou/; /y/yujie-qian/; /m/ming-ding/; /c/chonghua-liao/; /l/li-jian/; /r/ruslan-salakhutdinov/; /j/jie-tang/; /s/sebastian-ruder/; /z/zhilin-yang/",
        "bibtex": "@inproceedings{zheng-etal-2022-fewnlu,\n    title = \"{F}ew{NLU}: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding\",\n    author = \"Zheng, Yanan  and\n      Zhou, Jing  and\n      Qian, Yujie  and\n      Ding, Ming  and\n      Liao, Chonghua  and\n      Jian, Li  and\n      Salakhutdinov, Ruslan  and\n      Tang, Jie  and\n      Ruder, Sebastian  and\n      Yang, Zhilin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.38/\",\n    doi = \"10.18653/v1/2022.acl-long.38\",\n    pages = \"501--516\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.38.pdf",
        "site": "https://aclanthology.org/2022.acl-long.38/",
        "pdf_size": 432373,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=346546967746030095&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Tsinghua University+BAAI; Tsinghua University; MIT CSAIL; Tsinghua University; Tsinghua University; Tsinghua University; Carnegie Mellon University; Tsinghua University+BAAI+Shanghai Qi Zhi Institute; Google Research; Tsinghua University+Shanghai Qi Zhi Institute",
        "aff_domain": "tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ; ; ; ;tsinghua.edu.cn;google.com;tsinghua.edu.cn",
        "email": "tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ; ; ; ;tsinghua.edu.cn;google.com;tsinghua.edu.cn",
        "github": "https://github.com/THUDM/FewNLU",
        "project": "https://fewnlu.github.io",
        "author_num": 10,
        "aff_unique_index": "0+1;0;2;0;0;0;3;0+1+4;5;0+4",
        "aff_unique_norm": "Tsinghua University;Beijing Academy of Artificial Intelligence;Massachusetts Institute of Technology;Carnegie Mellon University;Shanghai Qi Zhi Institute;Google",
        "aff_unique_dep": ";;Computer Science and Artificial Intelligence Laboratory;;;Google Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baaic.cn;https://www.csail.mit.edu;https://www.cmu.edu;https://www.qz.io;https://research.google",
        "aff_unique_abbr": "THU;BAAI;MIT CSAIL;CMU;;Google Research",
        "aff_campus_unique_index": ";1;;2;",
        "aff_campus_unique": ";Cambridge;Mountain View",
        "aff_country_unique_index": "0+0;0;1;0;0;0;1;0+0+0;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.303",
        "title": "FiNER: Financial Numeric Entity Recognition for XBRL Tagging",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce XBRL tagging as a new entity extraction task for the financial domain and release FiNER-139, a dataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction datasets, FiNER-139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms BERT\u2019s performance, allowing word-level BILSTMs to perform better. To improve BERT\u2019s performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes. We also experiment with FIN-BERT, an existing BERT model for the financial domain, and release our own BERT (SEC-BERT), pre-trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on XBRL tagging.",
        "author": "Lefteris Loukas; Manos Fergadiotis; Ilias Chalkidis; Eirini Spyropoulou; Prodromos Malakasiotis; Ion Androutsopoulos; Georgios Paliouras",
        "authorids": "/l/lefteris-loukas/; /m/manos-fergadiotis/; /i/ilias-chalkidis/; /e/eirini-spyropoulou/; /p/prodromos-malakasiotis/; /i/ion-androutsopoulos/; /g/georgios-paliouras/",
        "bibtex": "@inproceedings{loukas-etal-2022-finer,\n    title = \"{F}i{NER}: Financial Numeric Entity Recognition for {XBRL} Tagging\",\n    author = \"Loukas, Lefteris  and\n      Fergadiotis, Manos  and\n      Chalkidis, Ilias  and\n      Spyropoulou, Eirini  and\n      Malakasiotis, Prodromos  and\n      Androutsopoulos, Ion  and\n      Paliouras, Georgios\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.303/\",\n    doi = \"10.18653/v1/2022.acl-long.303\",\n    pages = \"4419--4431\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.303.pdf",
        "site": "https://aclanthology.org/2022.acl-long.303/",
        "pdf_size": 920212,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13127373637897190401&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Institute of Informatics and Telecommunications, NCSR \u201cDemokritos\u201d + Department of Informatics, Athens University of Economics and Business; Institute of Informatics and Telecommunications, NCSR \u201cDemokritos\u201d + Department of Informatics, Athens University of Economics and Business; Department of Computer Science, University of Copenhagen; Institute of Informatics and Telecommunications, NCSR \u201cDemokritos\u201d; Institute of Informatics and Telecommunications, NCSR \u201cDemokritos\u201d + Department of Informatics, Athens University of Economics and Business; Institute of Informatics and Telecommunications, NCSR \u201cDemokritos\u201d + Department of Informatics, Athens University of Economics and Business; Institute of Informatics and Telecommunications, NCSR \u201cDemokritos\u201d",
        "aff_domain": "aueb.gr; ; ; ; ; ; ",
        "email": "aueb.gr; ; ; ; ; ; ",
        "github": "https://github.com/nlpaueb/finer",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;2;0;0+1;0+1;0",
        "aff_unique_norm": "NCSR Demokritos;Athens University of Economics and Business;University of Copenhagen",
        "aff_unique_dep": "Institute of Informatics and Telecommunications;Department of Informatics;Department of Computer Science",
        "aff_unique_url": "https://www.demokritos.gr;https://www.aueb.gr;https://www.ku.dk",
        "aff_unique_abbr": "NCSR Demokritos;AUEB;UCPH",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Athens",
        "aff_country_unique_index": "0+0;0+0;1;0;0+0;0+0;0",
        "aff_country_unique": "Greece;Denmark"
    },
    {
        "id": "2022.acl-long.388",
        "title": "Finding Structural Knowledge in Multimodal-BERT",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we investigate the knowledge learned in the embeddings of multimodal-BERT models. More specifically, we probe their capabilities of storing the grammatical structure of linguistic data and the structure learned over objects in visual data. To reach that goal, we first make the inherent structure of language and visuals explicit by a dependency parse of the sentences that describe the image and by the dependencies between the object regions in the image, respectively. We call this explicit visual structure the scene tree, that is based on the dependency tree of the language description. Extensive probing experiments show that the multimodal-BERT models do not encode these scene trees.",
        "author": "Victor Milewski; Miryam de Lhoneux; Marie-Francine Moens",
        "authorids": "/v/victor-milewski/; /m/miryam-de-lhoneux/; /m/marie-francine-moens/",
        "bibtex": "@inproceedings{milewski-etal-2022-finding,\n    title = \"Finding Structural Knowledge in Multimodal-{BERT}\",\n    author = \"Milewski, Victor  and\n      de Lhoneux, Miryam  and\n      Moens, Marie-Francine\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.388/\",\n    doi = \"10.18653/v1/2022.acl-long.388\",\n    pages = \"5658--5671\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.388.pdf",
        "site": "https://aclanthology.org/2022.acl-long.388/",
        "pdf_size": 854785,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1411329962842274839&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, KU Leuven + Department of Computer Science, University of Copenhagen + Department of Linguistics and Philology, Uppsala University; Department of Computer Science, KU Leuven + Department of Computer Science, University of Copenhagen + Department of Linguistics and Philology, Uppsala University; Department of Computer Science, KU Leuven",
        "aff_domain": "kuleuven.be; ; ",
        "email": "kuleuven.be; ; ",
        "github": "https://github.com/VSJMilewski/multimodal-probes",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;0+1+2;0",
        "aff_unique_norm": "KU Leuven;University of Copenhagen;Uppsala University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;Department of Linguistics and Philology",
        "aff_unique_url": "https://www.kuleuven.be;https://www.ku.dk;https://www.uu.se",
        "aff_unique_abbr": "KU Leuven;UCPH;UU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1+2;0+1+2;0",
        "aff_country_unique": "Belgium;Denmark;Sweden"
    },
    {
        "id": "2022.findings-acl.115",
        "title": "Finding the Dominant Winning Ticket in Pre-Trained Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture. In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performance. To achieve this, we regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the \u201cdominant winning ticket\u201d). Empirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each parameter matrix. Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.",
        "author": "Zhuocheng Gong; Di He; Yelong Shen; Tie-Yan Liu; Weizhu Chen; Dongyan Zhao; Ji-Rong Wen; Rui Yan",
        "authorids": "/z/zhuocheng-gong/; /d/di-he/; /y/yelong-shen/; /t/tie-yan-liu/; /w/weizhu-chen/; /d/dongyan-zhao/; /j/ji-rong-wen/; /r/rui-yan/",
        "bibtex": "@inproceedings{gong-etal-2022-finding,\n    title = \"Finding the Dominant Winning Ticket in Pre-Trained Language Models\",\n    author = \"Gong, Zhuocheng  and\n      He, Di  and\n      Shen, Yelong  and\n      Liu, Tie-Yan  and\n      Chen, Weizhu  and\n      Zhao, Dongyan  and\n      Wen, Ji-Rong  and\n      Yan, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.115/\",\n    doi = \"10.18653/v1/2022.findings-acl.115\",\n    pages = \"1459--1472\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.115.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.115/",
        "pdf_size": 668627,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6822333050713910809&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Wangxuan Institute of Computer Technology, Peking University, China; Microsoft Research, Beijing, China; Microsoft Azure AI; Microsoft Research, Beijing, China; Microsoft Azure AI; Wangxuan Institute of Computer Technology, Peking University, China+Artificial Intelligence Institute of Peking University+State Key Laboratory of Media Convergence Production; Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;pku.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "email": "pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;pku.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;0+0+2;3;3",
        "aff_unique_norm": "Peking University;Microsoft;State Key Laboratory of Media Convergence Production;Renmin University of China",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;Microsoft Research;;Gaoling School of Artificial Intelligence",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.microsoft.com/en-us/research/group/microsoft-research-asia;;http://www.ruc.edu.cn",
        "aff_unique_abbr": "PKU;MSR;;RUC",
        "aff_campus_unique_index": "1;1;;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;1;0;1;0+0+0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.330",
        "title": "Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer-based pre-trained models, such as BERT, have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, deploying these models can be prohibitively costly, as the standard self-attention mechanism of the Transformer suffers from quadratic computational cost in the input sequence length. To confront this, we propose FCA, a fine- and coarse-granularity hybrid self-attention that reduces the computation cost through progressively shortening the computational sequence length in self-attention. Specifically, FCA conducts an attention-based scoring strategy to determine the informativeness of tokens at each layer. Then, the informative tokens serve as the fine-granularity computing units in self-attention and the uninformative tokens are replaced with one or several clusters as the coarse-granularity computing units in self-attention. Experiments on the standard GLUE benchmark show that BERT with FCA achieves 2x reduction in FLOPs over original BERT with <1% loss in accuracy. We show that FCA offers a significantly better trade-off between accuracy and FLOPs compared to prior methods.",
        "author": "Jing Zhao; Yifan Wang; Junwei Bao; Youzheng Wu; Xiaodong He",
        "authorids": "/j/jing-zhao/; /y/yifan-wang/; /j/junwei-bao/; /y/youzheng-wu/; /x/xiaodong-he/",
        "bibtex": "@inproceedings{zhao-etal-2022-fine,\n    title = \"Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient {BERT}\",\n    author = \"Zhao, Jing  and\n      Wang, Yifan  and\n      Bao, Junwei  and\n      Wu, Youzheng  and\n      He, Xiaodong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.330/\",\n    doi = \"10.18653/v1/2022.acl-long.330\",\n    pages = \"4811--4820\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.330.pdf",
        "site": "https://aclanthology.org/2022.acl-long.330/",
        "pdf_size": 870692,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1793862668850780788&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "JD AI Research, Beijing, China; JD AI Research, Beijing, China; JD AI Research, Beijing, China; JD AI Research, Beijing, China; JD AI Research, Beijing, China",
        "aff_domain": "jd.com;jd.com;jd.com;jd.com;jd.com",
        "email": "jd.com;jd.com;jd.com;jd.com;jd.com",
        "github": "https://github.com/pierre-zhao/FCA-BERT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "JD",
        "aff_unique_dep": "JD AI Research",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.471",
        "title": "Fine-Grained Controllable Text Generation Using Non-Residual Prompting",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The introduction of immensely large Causal Language Models (CLMs) has rejuvenated the interest in open-ended text generation. However, controlling the generative process for these Transformer-based models is at large an unsolved problem. Earlier work has explored either plug-and-play decoding strategies, or more powerful but blunt approaches such as prompting. There hence currently exists a trade-off between fine-grained control, and the capability for more expressive high-level instructions. To alleviate this trade-off, we propose an encoder-decoder architecture that enables intermediate text prompts at arbitrary time steps. We propose a resource-efficient method for converting a pre-trained CLM into this architecture, and demonstrate its potential on various experiments, including the novel task of contextualized word inclusion. Our method provides strong results on multiple experimental settings, proving itself to be both expressive and versatile.",
        "author": "Fredrik Carlsson; Joey \u00d6hman; Fangyu Liu; Severine Verlinden; Joakim Nivre; Magnus Sahlgren",
        "authorids": "/f/fredrik-carlsson/; /j/joey-ohman/; /f/fangyu-liu/; /s/severine-verlinden/; /j/joakim-nivre/; /m/magnus-sahlgren/",
        "bibtex": "@inproceedings{carlsson-etal-2022-fine,\n    title = \"Fine-Grained Controllable Text Generation Using Non-Residual Prompting\",\n    author = {Carlsson, Fredrik  and\n      {\\\"O}hman, Joey  and\n      Liu, Fangyu  and\n      Verlinden, Severine  and\n      Nivre, Joakim  and\n      Sahlgren, Magnus},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.471/\",\n    doi = \"10.18653/v1/2022.acl-long.471\",\n    pages = \"6837--6857\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.471.pdf",
        "site": "https://aclanthology.org/2022.acl-long.471/",
        "pdf_size": 1664231,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6477756272012425753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Research Institutes of Sweden; AI Sweden; University of Cambridge; AI Sweden; Research Institutes of Sweden; AI Sweden",
        "aff_domain": "ri.se;ai.se;cam.ac.uk;ai.se;ri.se;ai.se",
        "email": "ri.se;ai.se;cam.ac.uk;ai.se;ri.se;ai.se",
        "github": "https://Github.com/FreddeFrallan/Non-Residual-Prompting",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;1;0;1",
        "aff_unique_norm": "Research Institutes of Sweden;AI Sweden;University of Cambridge",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ri.se/en;https://www.aisweden.org;https://www.cam.ac.uk",
        "aff_unique_abbr": "RISE;AI Sweden;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "Sweden;United Kingdom"
    },
    {
        "id": "2022.acl-short.56",
        "title": "Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action spaces. In these games, the agent learns to explore the environment via natural language interactions with the game simulator. A fundamental challenge in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment. We propose CommExpl, an exploration technique that injects external commonsense knowledge, via a pretrained language model (LM), into the agent during training when the agent is the most uncertain about its next action. Our method exhibits improvement on the collected game scores during the training in four out of nine games from Jericho. Additionally, the produced trajectory of actions exhibit lower perplexity, when tested with a pretrained LM, indicating better closeness to human language.",
        "author": "Dongwon Ryu; Ehsan Shareghi; Meng Fang; Yunqiu Xu; Shirui Pan; Reza Haf",
        "authorids": "/d/dongwon-ryu/; /e/ehsan-shareghi/; /m/meng-fang/; /y/yunqiu-xu/; /s/shirui-pan/; /r/reza-haf/",
        "bibtex": "@inproceedings{ryu-etal-2022-fire,\n    title = \"Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games\",\n    author = \"Ryu, Dongwon  and\n      Shareghi, Ehsan  and\n      Fang, Meng  and\n      Xu, Yunqiu  and\n      Pan, Shirui  and\n      Haf, Reza\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.56/\",\n    doi = \"10.18653/v1/2022.acl-short.56\",\n    pages = \"515--522\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.56.pdf",
        "site": "https://aclanthology.org/2022.acl-short.56/",
        "pdf_size": 1257619,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6085189689462264829&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Data Science & AI, Monash University; Department of Data Science & AI, Monash University + Language Technology Lab, University of Cambridge; Eindhoven University of Technology; University of Technology Sydney; Department of Data Science & AI, Monash University; Department of Data Science & AI, Monash University",
        "aff_domain": "monash.edu;monash.edu;tue.nl;student.uts.edu.au;monash.edu;monash.edu",
        "email": "monash.edu;monash.edu;tue.nl;student.uts.edu.au;monash.edu;monash.edu",
        "github": "https://github.com/ktr0921/comm-expl-kg-a2c",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+1;2;3;0;0",
        "aff_unique_norm": "Monash University;University of Cambridge;Eindhoven University of Technology;University of Technology Sydney",
        "aff_unique_dep": "Department of Data Science & AI;Language Technology Lab;;",
        "aff_unique_url": "https://www.monash.edu;https://www.cam.ac.uk;https://www.tue.nl;https://www.uts.edu.au",
        "aff_unique_abbr": "Monash;Cambridge;TU/e;UTS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0+1;2;0;0;0",
        "aff_country_unique": "Australia;United Kingdom;Netherlands"
    },
    {
        "id": "2022.findings-acl.301",
        "title": "First the Worst: Finding Better Gender Translations During Beam Search",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach\u2019s utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.",
        "author": "Danielle Saunders; Rosie Sallis; Bill Byrne",
        "authorids": "/d/danielle-saunders/; /r/rosie-sallis/; /b/bill-byrne/",
        "bibtex": "@inproceedings{saunders-etal-2022-first,\n    title = \"First the Worst: Finding Better Gender Translations During Beam Search\",\n    author = \"Saunders, Danielle  and\n      Sallis, Rosie  and\n      Byrne, Bill\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.301/\",\n    doi = \"10.18653/v1/2022.findings-acl.301\",\n    pages = \"3814--3823\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.301.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.301/",
        "pdf_size": 366545,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2326652170984900598&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Engineering, University of Cambridge, UK; Department of Engineering, University of Cambridge, UK; Department of Engineering, University of Cambridge, UK",
        "aff_domain": "cantab.ac.uk;cantab.ac.uk;cam.ac.uk",
        "email": "cantab.ac.uk;cantab.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.563",
        "title": "Flexible Generation from Fragmentary Linguistic Input",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The dominant paradigm for high-performance models in novel NLP tasks today is direct specialization for the task via training from scratch or fine-tuning large pre-trained models. But does direct specialization capture how humans approach novel language tasks? We hypothesize that human performance is better characterized by flexible inference through composition of basic computational motifs available to the human language user. To test this hypothesis, we formulate a set of novel fragmentary text completion tasks, and compare the behavior of three direct-specialization models against a new model we introduce, GibbsComplete, which composes two basic computational motifs central to contemporary models: masked and autoregressive word prediction. We conduct three types of evaluation: human judgments of completion quality, satisfaction of syntactic constraints imposed by the input fragment, and similarity to human behavior in the structural statistics of the completions. With no task-specific parameter tuning, GibbsComplete performs comparably to direct-specialization models in the first two evaluations, and outperforms all direct-specialization models in the third evaluation. These results support our hypothesis that human behavior in novel language tasks and environments may be better characterized by flexible composition of basic computational motifs rather than by direct specialization.",
        "author": "Peng Qian; Roger Levy",
        "authorids": "/p/peng-qian/; /r/roger-levy/",
        "bibtex": "@inproceedings{qian-levy-2022-flexible,\n    title = \"Flexible Generation from Fragmentary Linguistic Input\",\n    author = \"Qian, Peng  and\n      Levy, Roger\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.563/\",\n    doi = \"10.18653/v1/2022.acl-long.563\",\n    pages = \"8176--8196\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.563.pdf",
        "site": "https://aclanthology.org/2022.acl-long.563/",
        "pdf_size": 1413535,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7247129467854344227&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.592",
        "title": "FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most previous methods for text data augmentation are limited to simple tasks and weak baselines. We explore data augmentation on hard tasks (i.e., few-shot natural language understanding) and strong baselines (i.e., pretrained models with over one billion parameters). Under this setting, we reproduced a large number of previous augmentation methods and found that these methods bring marginal gains at best and sometimes degrade the performance much. To address this challenge, we propose a novel data augmentation method FlipDA that jointly uses a generative model and a classifier to generate label-flipped data. Central to the idea of FlipDA is the discovery that generating label-flipped data is more crucial to the performance than generating label-preserved data. Experiments show that FlipDA achieves a good tradeoff between effectiveness and robustness\u2014it substantially improves many tasks while not negatively affecting the others.",
        "author": "Jing Zhou; Yanan Zheng; Jie Tang; Li Jian; Zhilin Yang",
        "authorids": "/j/jing-zhou/; /y/yanan-zheng/; /j/jie-tang/; /l/li-jian/; /z/zhilin-yang/",
        "bibtex": "@inproceedings{zhou-etal-2022-flipda,\n    title = \"{F}lip{DA}: Effective and Robust Data Augmentation for Few-Shot Learning\",\n    author = \"Zhou, Jing  and\n      Zheng, Yanan  and\n      Tang, Jie  and\n      Jian, Li  and\n      Yang, Zhilin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.592/\",\n    doi = \"10.18653/v1/2022.acl-long.592\",\n    pages = \"8646--8665\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.592.pdf",
        "site": "https://aclanthology.org/2022.acl-long.592/",
        "pdf_size": 366270,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4196862539740858260&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "1Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University + 3Shanghai Qi Zhi Institute; 2Department of Computer Science and Technology, Tsinghua University + 4Beijing Academy of Arti\ufb01cial Intelligence (BAAI); 2Department of Computer Science and Technology, Tsinghua University + 4Beijing Academy of Arti\ufb01cial Intelligence (BAAI); 1Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University + 3Shanghai Qi Zhi Institute; 1Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University + 3Shanghai Qi Zhi Institute",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/zhouj8553/FlipDA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+2;0+2;0+1;0+1",
        "aff_unique_norm": "Tsinghua University;Shanghai Qi Zhi Institute;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Institute for Interdisciplinary Information Sciences (IIIS);;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.baai.ac.cn",
        "aff_unique_abbr": "Tsinghua;;BAAI",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.386",
        "title": "Flooding-X: Improving BERT\u2019s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Adversarial robustness has attracted much attention recently, and the mainstream solution is adversarial training. However, the tradition of generating adversarial perturbations for each input embedding (in the settings of NLP) scales up the training computational complexity by the number of gradient steps it takes to obtain the adversarial samples. To address this problem, we leverage Flooding method which primarily aims at better generalization and we find promising in defending adversarial attacks. We further propose an effective criterion to bring hyper-parameter-dependent flooding into effect with a narrowed-down search space by measuring how the gradient steps taken within one epoch affect the loss of each batch. Our approach requires zero adversarial sample for training, and its time consumption is equivalent to fine-tuning, which can be 2-15 times faster than standard adversarial training. We experimentally show that our method improves BERT\u2019s resistance to textual adversarial attacks by a large margin, and achieves state-of-the-art robust accuracy on various text classification and GLUE tasks.",
        "author": "Qin Liu; Rui Zheng; Bao Rong; Jingyi Liu; ZhiHua Liu; Zhanzhan Cheng; Liang Qiao; Tao Gui; Qi Zhang; Xuanjing Huang",
        "authorids": "/q/qin-liu/; /r/rui-zheng/; /b/bao-rong/; /j/jingyi-liu/; /z/zhihua-liu/; /z/zhanzhan-cheng/; /l/liang-qiao/; /t/tao-gui/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{liu-etal-2022-flooding,\n    title = \"Flooding-{X}: Improving {BERT}`s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning\",\n    author = \"Liu, Qin  and\n      Zheng, Rui  and\n      Rong, Bao  and\n      Liu, Jingyi  and\n      Liu, ZhiHua  and\n      Cheng, Zhanzhan  and\n      Qiao, Liang  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.386/\",\n    doi = \"10.18653/v1/2022.acl-long.386\",\n    pages = \"5634--5644\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.386.pdf",
        "site": "https://aclanthology.org/2022.acl-long.386/",
        "pdf_size": 526460,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17439898452791864904&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; Hikvision Research Institute, Hangzhou, China; Hikvision Research Institute, Hangzhou, China; Institute of Modern Languages and Linguistics, Fudan University; School of Computer Science, Fudan University+Shanghai Key Laboratory of Intelligent Information Processing; School of Computer Science, Fudan University",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;hikvision.com;hikvision.com;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;hikvision.com;hikvision.com;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;1;1;0;0+2;0",
        "aff_unique_norm": "Fudan University;Hikvision Research Institute;Shanghai Key Laboratory of Intelligent Information Processing",
        "aff_unique_dep": "School of Computer Science;;Intelligent Information Processing",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.hikvision.com/cn/;",
        "aff_unique_abbr": "Fudan;HRI;",
        "aff_campus_unique_index": "1;1;",
        "aff_campus_unique": ";Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.89",
        "title": "Flow-Adapter Architecture for Unsupervised Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we propose a flow-adapter architecture for unsupervised NMT. It leverages normalizing flows to explicitly model the distributions of sentence-level latent representations, which are subsequently used in conjunction with the attention mechanism for the translation task. The primary novelties of our model are: (a) capturing language-specific sentence representations separately for each language using normalizing flows and (b) using a simple transformation of these latent representations for translating from one language to another. This architecture allows for unsupervised training of each language independently. While there is prior work on latent variables for supervised MT, to the best of our knowledge, this is the first work that uses latent variables and normalizing flows for unsupervised MT. We obtain competitive results on several unsupervised MT benchmarks.",
        "author": "Yihong Liu; Haris Jabbar; Hinrich Schuetze",
        "authorids": "/y/yihong-liu/; /h/haris-jabbar/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{liu-etal-2022-flow,\n    title = \"Flow-Adapter Architecture for Unsupervised Machine Translation\",\n    author = \"Liu, Yihong  and\n      Jabbar, Haris  and\n      Schuetze, Hinrich\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.89/\",\n    doi = \"10.18653/v1/2022.acl-long.89\",\n    pages = \"1253--1266\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.89.pdf",
        "site": "https://aclanthology.org/2022.acl-long.89/",
        "pdf_size": 618015,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8182813206536048006&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Informatics, Technical University of Munich; Center for Information and Language Processing, LMU Munich; Center for Information and Language Processing, LMU Munich",
        "aff_domain": "tum.de;cis.lmu.de; ",
        "email": "tum.de;cis.lmu.de; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Technical University of Munich;LMU Munich",
        "aff_unique_dep": "Department of Informatics;Center for Information and Language Processing",
        "aff_unique_url": "https://www.tum.de;https://www.lmu.de",
        "aff_unique_abbr": "TUM;LMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.findings-acl.323",
        "title": "Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Automatic email to-do item generation is the task of generating to-do items from a given email to help people overview emails and schedule daily work. Different from prior research on email summarization, to-do item generation focuses on generating action mentions to provide more structured summaries of email text. Prior work either requires large amount of annotation for key sentences with potential actions or fails to pay attention to nuanced actions from these unstructured emails, and thus often lead to unfaithful summaries. To fill these gaps, we propose a simple and effective learning to highlight and summarize framework (LHS) to learn to identify the most salient text and actions, and incorporate these structured representations to generate more faithful to-do items. Experiments show that our LHS model outperforms the baselines and achieves the state-of-the-art performance in terms of both quantitative evaluation and human judgement. We also discussed specific challenges that current models faced with email to-do summarization.",
        "author": "Kexun Zhang; Jiaao Chen; Diyi Yang",
        "authorids": "/k/kexun-zhang/; /j/jiaao-chen/; /d/diyi-yang/",
        "bibtex": "@inproceedings{zhang-etal-2022-focus,\n    title = \"Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization\",\n    author = \"Zhang, Kexun  and\n      Chen, Jiaao  and\n      Yang, Diyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.323/\",\n    doi = \"10.18653/v1/2022.findings-acl.323\",\n    pages = \"4095--4106\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.323.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.323/",
        "pdf_size": 453218,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5338001529618943240&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "zju.edu.cn;gatech.edu;cc.gatech.edu",
        "email": "zju.edu.cn;gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Zhejiang University;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.gatech.edu",
        "aff_unique_abbr": "ZJU;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-short.74",
        "title": "Focus on the Target\u2019s Vocabulary: Masked Label Smoothing for Machine Translation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. However, we argue that simply applying both techniques can be conflicting and even leads to sub-optimal performance. When allocating smoothed probability, original label smoothing treats the source-side words that would never appear in the target language equally to the real target-side words, which could bias the translation model. To address this issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the soft label probability of source-side words to zero. Simple yet effective, MLS manages to better integrate label smoothing with vocabulary sharing. Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation from both translation quality and model\u2019s calibration. Our code is released at https://github.com/PKUnlp-icler/MLS",
        "author": "Liang Chen; Runxin Xu; Baobao Chang",
        "authorids": "/l/liang-chen/; /r/runxin-xu/; /b/baobao-chang/",
        "bibtex": "@inproceedings{chen-etal-2022-focus,\n    title = \"Focus on the Target`s Vocabulary: Masked Label Smoothing for Machine Translation\",\n    author = \"Chen, Liang  and\n      Xu, Runxin  and\n      Chang, Baobao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.74/\",\n    doi = \"10.18653/v1/2022.acl-short.74\",\n    pages = \"665--671\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.74.pdf",
        "site": "https://aclanthology.org/2022.acl-short.74/",
        "pdf_size": 276839,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8944655648837646265&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China",
        "aff_domain": "outlook.com;gmail.com;pku.edu.cn",
        "email": "outlook.com;gmail.com;pku.edu.cn",
        "github": "",
        "project": "PKUnlp-icler",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.260",
        "title": "FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sequence modeling has demonstrated state-of-the-art performance on natural language and document understanding tasks. However, it is challenging to correctly serialize tokens in form-like documents in practice due to their variety of layout patterns. We propose FormNet, a structure-aware sequence model to mitigate the suboptimal serialization of forms. First, we design Rich Attention that leverages the spatial relationship between tokens in a form for more precise attention score calculation. Second, we construct Super-Tokens for each word by embedding representations from their neighboring tokens through graph convolutions. FormNet therefore explicitly recovers local syntactic information that may have been lost during serialization. In experiments, FormNet outperforms existing methods with a more compact model size and less pre-training data, establishing new state-of-the-art performance on CORD, FUNSD and Payment benchmarks.",
        "author": "Chen-Yu Lee; Chun-Liang Li; Timothy Dozat; Vincent Perot; Guolong Su; Nan Hua; Joshua Ainslie; Renshen Wang; Yasuhisa Fujii; Tomas Pfister",
        "authorids": "/c/chen-yu-lee/; /c/chun-liang-li/; /t/timothy-dozat/; /v/vincent-perot/; /g/guolong-su/; /n/nan-hua/; /j/joshua-ainslie/; /r/renshen-wang/; /y/yasuhisa-fujii/; /t/tomas-pfister/",
        "bibtex": "@inproceedings{lee-etal-2022-formnet,\n    title = \"{F}orm{N}et: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction\",\n    author = \"Lee, Chen-Yu  and\n      Li, Chun-Liang  and\n      Dozat, Timothy  and\n      Perot, Vincent  and\n      Su, Guolong  and\n      Hua, Nan  and\n      Ainslie, Joshua  and\n      Wang, Renshen  and\n      Fujii, Yasuhisa  and\n      Pfister, Tomas\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.260/\",\n    doi = \"10.18653/v1/2022.acl-long.260\",\n    pages = \"3735--3754\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.260.pdf",
        "site": "https://aclanthology.org/2022.acl-long.260/",
        "pdf_size": 3588869,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1536086578918233912&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Google Cloud AI Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Cloud AI Research",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Cloud AI Research",
        "aff_unique_url": "https://cloud.google.com/ai",
        "aff_unique_abbr": "Google Cloud AI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.583",
        "title": "French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting. Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States. We seek to widen the scope of bias studies by creating material to measure social bias in language models (LMs) against specific demographic groups in France. We build on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language. We introduce 1,679 sentence pairs in French that cover stereotypes in ten types of bias like gender and age. 1,467 sentence pairs are translated from CrowS-pairs and 212 are newly crowdsourced. The sentence pairs contrast stereotypes concerning underadvantaged groups with the same sentence concerning advantaged groups. We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories. We report on the translation process from English into French, which led to a characterization of stereotypes in CrowS-pairs including the identification of US-centric cultural traits. We offer guidelines to further extend the dataset to other languages and cultural environments.",
        "author": "Aur\u00e9lie N\u00e9v\u00e9ol; Yoann Dupont; Julien Bezan\u00e7on; Kar\u00ebn Fort",
        "authorids": "/a/aurelie-neveol/; /y/yoann-dupont/; /j/julien-bezancon/; /k/karen-fort/",
        "bibtex": "@inproceedings{neveol-etal-2022-french,\n    title = \"{F}rench {C}row{S}-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than {E}nglish\",\n    author = {N{\\'e}v{\\'e}ol, Aur{\\'e}lie  and\n      Dupont, Yoann  and\n      Bezan{\\c{c}}on, Julien  and\n      Fort, Kar{\\\"e}n},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.583/\",\n    doi = \"10.18653/v1/2022.acl-long.583\",\n    pages = \"8521--8531\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.583.pdf",
        "site": "https://aclanthology.org/2022.acl-long.583/",
        "pdf_size": 222712,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5747299991214615641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Universit\u00e9 Paris-Saclay, CNRS, LISN; ObTIC, Sorbonne Universit\u00e9; Sorbonne Universit\u00e9; Universit\u00e9 de Lorraine, CNRS, Inria, LORIA + Sorbonne Universit\u00e9",
        "aff_domain": "lisn.fr;etu.sorbonne-universite.fr;etu.sorbonne-universite.fr;loria.fr",
        "email": "lisn.fr;etu.sorbonne-universite.fr;etu.sorbonne-universite.fr;loria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;2+1",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay;Sorbonne Universit\u00e9;Universit\u00e9 de Lorraine",
        "aff_unique_dep": "CNRS, LISN;ObTIC;",
        "aff_unique_url": "https://www.universite-paris-saclay.fr;https://www.sorbonne-universite.fr;https://www.univ-lorraine.fr",
        "aff_unique_abbr": "UPS;;UL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.findings-acl.89",
        "title": "From BERT\u2018s Point of View: Revealing the Prevailing Contextual Differences",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood. While much research in the field of BERTology has tested whether specific knowledge can be extracted from layer activations, we invert the popular probing design to analyze the prevailing differences and clusters in BERT\u2019s high dimensional space. By extracting coarse features from masked token representations and predicting them by probing models with access to only partial information we can apprehend the variation from \u2018BERT\u2019s point of view\u2019. By applying our new methodology to different datasets we show how much the differences can be described by syntax but further how they are to a great extent shaped by the most simple positional information.",
        "author": "Carolin M. Schuster; Simon Hegelich",
        "authorids": "/c/carolin-m-schuster/; /s/simon-hegelich/",
        "bibtex": "@inproceedings{schuster-hegelich-2022-berts,\n    title = \"From {BERT}{\\textquoteleft}s {P}oint of {V}iew: {R}evealing the {P}revailing {C}ontextual {D}ifferences\",\n    author = \"Schuster, Carolin M.  and\n      Hegelich, Simon\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.89/\",\n    doi = \"10.18653/v1/2022.findings-acl.89\",\n    pages = \"1120--1138\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.89.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.89/",
        "pdf_size": 2798526,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18384483116189756194&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Technical University of Munich; Technical University of Munich",
        "aff_domain": "tum.de; ",
        "email": "tum.de; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technical University of Munich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tum.de",
        "aff_unique_abbr": "TUM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.480",
        "title": "From Simultaneous to Streaming Machine Translation by Leveraging Streaming History",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Simultaneous Machine Translation is the task of incrementally translating an input sentence before it is fully available. Currently, simultaneous translation is carried out by translating each sentence independently of the previously translated text. More generally, Streaming MT can be understood as an extension of Simultaneous MT to the incremental translation of a continuous input text stream. In this work, a state-of-the-art simultaneous sentence-level MT system is extended to the streaming setup by leveraging the streaming history. Extensive empirical results are reported on IWSLT Translation Tasks, showing that leveraging the streaming history leads to significant quality gains. In particular, the proposed system proves to compare favorably to the best performing systems.",
        "author": "Javier Iranzo-S\u00e1nchez; Jorge Civera; Alfons Juan",
        "authorids": "/j/javier-iranzo-sanchez/; /j/jorge-civera/; /a/alfons-juan/",
        "bibtex": "@inproceedings{iranzo-sanchez-etal-2022-simultaneous,\n    title = \"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History\",\n    author = \"Iranzo-S{\\'a}nchez, Javier  and\n      Civera, Jorge  and\n      Juan, Alfons\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.480/\",\n    doi = \"10.18653/v1/2022.acl-long.480\",\n    pages = \"6972--6985\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.480.pdf",
        "site": "https://aclanthology.org/2022.acl-long.480/",
        "pdf_size": 343338,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7890879700070437662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning and Language Processing Group, Valencian Research Institute for Artificial Intelligence, Universitat Polit\u00e8cnica de Val\u00e8ncia; Machine Learning and Language Processing Group, Valencian Research Institute for Artificial Intelligence, Universitat Polit\u00e8cnica de Val\u00e8ncia; Machine Learning and Language Processing Group, Valencian Research Institute for Artificial Intelligence, Universitat Polit\u00e8cnica de Val\u00e8ncia",
        "aff_domain": "vrain.upv.es;vrain.upv.es;vrain.upv.es",
        "email": "vrain.upv.es;vrain.upv.es;vrain.upv.es",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universitat Polit\u00e8cnica de Val\u00e8ncia",
        "aff_unique_dep": "Machine Learning and Language Processing Group",
        "aff_unique_url": "https://www.upv.es",
        "aff_unique_abbr": "UPV",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Valencia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2022.findings-acl.264",
        "title": "From Stance to Concern: Adaptation of Propositional Analysis to New Tasks and Domains",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present a generalized paradigm for adaptation of propositional analysis (predicate-argument pairs) to new tasks and domains. We leverage an analogy between stances (belief-driven sentiment) and concerns (topical issues with moral dimensions/endorsements) to produce an explanatory representation. A key contribution is the combination of semi-automatic resource building for extraction of domain-dependent concern types (with 2-4 hours of human labor per domain) and an entirely automatic procedure for extraction of domain-independent moral dimensions and endorsement values. Prudent (automatic) selection of terms from propositional structures for lexical expansion (via semantic similarity) produces new moral dimension lexicons at three levels of granularity beyond a strong baseline lexicon. We develop a ground truth (GT) based on expert annotators and compare our concern detection output to GT, to yield 231% improvement in recall over baseline, with only a 10% loss in precision. F1 yields 66% improvement over baseline and 97.8% of human performance. Our lexically based approach yields large savings over approaches that employ costly human labor and model building. We provide to the community a newly expanded moral dimension/value lexicon, annotation guidelines, and GT.",
        "author": "Brodie Mather; Bonnie Dorr; Adam Dalton; William de Beaumont; Owen Rambow; Sonja Schmer-Galunder",
        "authorids": "/b/brodie-mather/; /b/bonnie-dorr/; /a/adam-dalton/; /w/william-de-beaumont/; /o/owen-rambow/; /s/sonja-schmer-galunder/",
        "bibtex": "@inproceedings{mather-etal-2022-stance,\n    title = \"From Stance to Concern: Adaptation of Propositional Analysis to New Tasks and Domains\",\n    author = \"Mather, Brodie  and\n      Dorr, Bonnie  and\n      Dalton, Adam  and\n      de Beaumont, William  and\n      Rambow, Owen  and\n      Schmer-Galunder, Sonja\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.264/\",\n    doi = \"10.18653/v1/2022.findings-acl.264\",\n    pages = \"3354--3367\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.264.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.264/",
        "pdf_size": 259657,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15194067066540375224&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Human & Machine Cognition; Institute for Human & Machine Cognition; Institute for Human & Machine Cognition; Institute for Human & Machine Cognition; Stony Brook University; Smart Information Flow Technologies",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;2",
        "aff_unique_norm": "Institute for Human & Machine Cognition;Stony Brook University;Smart Information Flow Technologies",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ihmc.us;https://www.stonybrook.edu;https://www.sift.net",
        "aff_unique_abbr": "IHMC;SBU;SIFT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.385",
        "title": "From text to talk: Harnessing conversational corpora for humane and diversity-aware language technology",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Informal social interaction is the primordial home of human language. Linguistically diverse conversational corpora are an important and largely untapped resource for computational linguistics and language technology. Through the efforts of a worldwide language documentation movement, such corpora are increasingly becoming available. We show how interactional data from 63 languages (26 families) harbours insights about turn-taking, timing, sequential structure and social action, with implications for language technology, natural language understanding, and the design of conversational interfaces. Harnessing linguistically diverse conversational corpora will provide the empirical foundations for flexible, localizable, humane language technologies of the future.",
        "author": "Mark Dingemanse; Andreas Liesenfeld",
        "authorids": "/m/mark-dingemanse/; /a/andreas-liesenfeld/",
        "bibtex": "@inproceedings{dingemanse-liesenfeld-2022-text,\n    title = \"From text to talk: {H}arnessing conversational corpora for humane and diversity-aware language technology\",\n    author = \"Dingemanse, Mark  and\n      Liesenfeld, Andreas\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.385/\",\n    doi = \"10.18653/v1/2022.acl-long.385\",\n    pages = \"5614--5633\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.385.pdf",
        "site": "https://aclanthology.org/2022.acl-long.385/",
        "pdf_size": 1873404,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14733998496106806913&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Center for Language Studies, Radboud University; Center for Language Studies, Radboud University",
        "aff_domain": "ru.nl;ru.nl",
        "email": "ru.nl;ru.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Radboud University",
        "aff_unique_dep": "Center for Language Studies",
        "aff_unique_url": "https://www.ru.nl",
        "aff_unique_abbr": "RU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.acl-long.259",
        "title": "From the Detection of Toxic Spans in Online Discussions to the Analysis of Toxic-to-Civil Transfer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study the task of toxic spans detection, which concerns the detection of the spans that make a text toxic, when detecting such spans is possible. We introduce a dataset for this task, ToxicSpans, which we release publicly. By experimenting with several methods, we show that sequence labeling models perform best, but methods that add generic rationale extraction mechanisms on top of classifiers trained to predict if a post is toxic or not are also surprisingly promising. Finally, we use ToxicSpans and systems trained on it, to provide further analysis of state-of-the-art toxic to non-toxic transfer systems, as well as of human performance on that latter task. Our work highlights challenges in finer toxicity detection and mitigation.",
        "author": "John Pavlopoulos; Leo Laugier; Alexandros Xenos; Jeffrey Sorensen; Ion Androutsopoulos",
        "authorids": "/j/john-pavlopoulos/; /l/leo-laugier/; /a/alexandros-xenos/; /j/jeffrey-sorensen/; /i/ion-androutsopoulos/",
        "bibtex": "@inproceedings{pavlopoulos-etal-2022-detection,\n    title = \"From the Detection of Toxic Spans in Online Discussions to the Analysis of Toxic-to-Civil Transfer\",\n    author = \"Pavlopoulos, John  and\n      Laugier, Leo  and\n      Xenos, Alexandros  and\n      Sorensen, Jeffrey  and\n      Androutsopoulos, Ion\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.259/\",\n    doi = \"10.18653/v1/2022.acl-long.259\",\n    pages = \"3721--3734\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.259.pdf",
        "site": "https://aclanthology.org/2022.acl-long.259/",
        "pdf_size": 393413,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15509261577876436746&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer and Systems Sciences, Stockholm University, Sweden+Department of Informatics, Athens University of Economics and Business, Greece; T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France; Department of Informatics, Athens University of Economics and Business, Greece; Google; Department of Informatics, Athens University of Economics and Business, Greece",
        "aff_domain": "aueb.gr;aueb.gr;aueb.gr;telecom-paris.fr;google.com",
        "email": "aueb.gr;aueb.gr;aueb.gr;telecom-paris.fr;google.com",
        "github": "https://github.com/ipavlopoulos/toxic_spans",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;1;3;1",
        "aff_unique_norm": "Stockholm University;Athens University of Economics and Business;T\u00e9l\u00e9com Paris;Google",
        "aff_unique_dep": "Department of Computer and Systems Sciences;Department of Informatics;;Google",
        "aff_unique_url": "https://www.su.se;https://www.aueb.gr;https://www.telecom-paris.fr;https://www.google.com",
        "aff_unique_abbr": "SU;AUEB;T\u00e9l\u00e9com Paris;Google",
        "aff_campus_unique_index": "1;1;2;1",
        "aff_campus_unique": ";Athens;Mountain View",
        "aff_country_unique_index": "0+1;2;1;3;1",
        "aff_country_unique": "Sweden;Greece;France;United States"
    },
    {
        "id": "2022.acl-long.93",
        "title": "FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fast and reliable evaluation metrics are key to R&D progress. While traditional natural language generation metrics are fast, they are not very reliable. Conversely, new metrics based on large pretrained language models are much more reliable, but require significant computational resources. In this paper, we propose FrugalScore, an approach to learn a fixed, low cost version of any expensive NLG metric, while retaining most of its original performance. Experiments with BERTScore and MoverScore on summarization and translation show that FrugalScore is on par with the original metrics (and sometimes better), while having several orders of magnitude less parameters and running several times faster. On average over all learned metrics, tasks, and variants, FrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35 times less parameters than the original metrics. We make our trained metrics publicly available, to benefit the entire NLP community and in particular researchers and practitioners with limited resources.",
        "author": "Moussa Kamal Eddine; Guokan Shang; Antoine Tixier; Michalis Vazirgiannis",
        "authorids": "/m/moussa-kamal-eddine/; /g/guokan-shang/; /a/antoine-tixier/; /m/michalis-vazirgiannis/",
        "bibtex": "@inproceedings{kamal-eddine-etal-2022-frugalscore,\n    title = \"{F}rugal{S}core: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation\",\n    author = \"Kamal Eddine, Moussa  and\n      Shang, Guokan  and\n      Tixier, Antoine  and\n      Vazirgiannis, Michalis\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.93/\",\n    doi = \"10.18653/v1/2022.acl-long.93\",\n    pages = \"1305--1318\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.93.pdf",
        "site": "https://aclanthology.org/2022.acl-long.93/",
        "pdf_size": 339392,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9241562130812128780&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "\u00c9cole Polytechnique; Linagora; \u00c9cole Polytechnique; \u00c9cole Polytechnique+AUEB",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "https://github.com/moussaKam/FrugalScore",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0+2",
        "aff_unique_norm": "Ecole Polytechnique;LINAGORA;Athens University of Economics and Business",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.polytechnique.edu;https://www.linagora.com;https://www.aueb.gr",
        "aff_unique_abbr": "X;;AUEB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "France;Greece"
    },
    {
        "id": "2022.acl-long.389",
        "title": "Fully Hyperbolic Neural Networks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Hyperbolic neural networks have shown great potential for modeling complex data. However, existing hyperbolic networks are not completely hyperbolic, as they encode features in the hyperbolic space yet formalize most of their operations in the tangent space (a Euclidean subspace) at the origin of the hyperbolic model. This hybrid method greatly limits the modeling ability of networks. In this paper, we propose a fully hyperbolic framework to build hyperbolic networks based on the Lorentz model by adapting the Lorentz transformations (including boost and rotation) to formalize essential operations of neural networks. Moreover, we also prove that linear transformation in tangent spaces used by existing hyperbolic networks is a relaxation of the Lorentz rotation and does not include the boost, implicitly limiting the capabilities of existing hyperbolic networks. The experimental results on four NLP tasks show that our method has better performance for building both shallow and deep networks. Our code will be released to facilitate follow-up research.",
        "author": "Weize Chen; Xu Han; Yankai Lin; Hexu Zhao; Zhiyuan Liu; Peng Li; Maosong Sun; Jie Zhou",
        "authorids": "/w/weize-chen/; /x/xu-han/; /y/yankai-lin/; /h/hexu-zhao/; /z/zhiyuan-liu/; /p/peng-li/; /m/maosong-sun/; /j/jie-zhou/",
        "bibtex": "@inproceedings{chen-etal-2022-fully,\n    title = \"Fully Hyperbolic Neural Networks\",\n    author = \"Chen, Weize  and\n      Han, Xu  and\n      Lin, Yankai  and\n      Zhao, Hexu  and\n      Liu, Zhiyuan  and\n      Li, Peng  and\n      Sun, Maosong  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.389/\",\n    doi = \"10.18653/v1/2022.acl-long.389\",\n    pages = \"5672--5686\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.389.pdf",
        "site": "https://aclanthology.org/2022.acl-long.389/",
        "pdf_size": 1696554,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16920406929558690676&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Technology, Tsinghua University + International Innovation Center of Tsinghua University + Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology, Tsinghua University + International Innovation Center of Tsinghua University + Institute for Artificial Intelligence, Tsinghua University; Pattern Recognition Center, WeChat AI, Tencent Inc; Department of Computer Science and Technology, Tsinghua University + International Innovation Center of Tsinghua University + Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology, Tsinghua University + International Innovation Center of Tsinghua University + Institute for Artificial Intelligence, Tsinghua University + Beijing Academy of Artificial Intelligence; Institute for AI Industry Research (AIR), Tsinghua University; Department of Computer Science and Technology, Tsinghua University + International Innovation Center of Tsinghua University + Institute for Artificial Intelligence, Tsinghua University + Beijing Academy of Artificial Intelligence; Pattern Recognition Center, WeChat AI, Tencent Inc",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ;tsinghua.edu.cn; ;tsinghua.edu.cn; ",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ;tsinghua.edu.cn; ;tsinghua.edu.cn; ",
        "github": "https://github.com/chenweize1998/fully-hyperbolic-nn",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+0+0;0+0+0;1;0+0+0;0+0+0+2;0;0+0+0+2;1",
        "aff_unique_norm": "Tsinghua University;Tencent;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Department of Computer Science and Technology;Pattern Recognition Center, WeChat AI;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.tencent.com;https://www.baaic.cn",
        "aff_unique_abbr": "THU;Tencent;BAAI",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0;0+0+0+0;0;0+0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.121",
        "title": "Fully-Semantic Parsing and Generation: the BabelNet Meaning Representation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A language-independent representation of meaning is one of the most coveted dreams in Natural Language Understanding. With this goal in mind, several formalisms have been proposed as frameworks for meaning representation in Semantic Parsing. And yet, the dependencies these formalisms share with respect to language-specific repositories of knowledge make the objective of closing the gap between high- and low-resourced languages hard to accomplish. In this paper, we present the BabelNet Meaning Representation (BMR), an interlingual formalism that abstracts away from language-specific constraints by taking advantage of the multilingual semantic resources of BabelNet and VerbAtlas. We describe the rationale behind the creation of BMR and put forward BMR 1.0, a dataset labeled entirely according to the new formalism. Moreover, we show how BMR is able to outperform previous formalisms thanks to its fully-semantic framing, which enables top-notch multilingual parsing and generation. We release the code at https://github.com/SapienzaNLP/bmr.",
        "author": "Abelardo Carlos Mart\u00ednez Lorenzo; Marco Maru; Roberto Navigli",
        "authorids": "/a/abelardo-carlos-martinez-lorenzo/; /m/marco-maru/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{martinez-lorenzo-etal-2022-fully,\n    title = \"{F}ully-{S}emantic {P}arsing and {G}eneration: the {B}abel{N}et {M}eaning {R}epresentation\",\n    author = \"Mart{\\'i}nez Lorenzo, Abelardo Carlos  and\n      Maru, Marco  and\n      Navigli, Roberto\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.121/\",\n    doi = \"10.18653/v1/2022.acl-long.121\",\n    pages = \"1727--1741\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.121.pdf",
        "site": "https://aclanthology.org/2022.acl-long.121/",
        "pdf_size": 2420446,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5058261406997527753&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Sapienza NLP Group, Sapienza University of Rome + Babelscape, Rome; Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome",
        "aff_domain": "uniroma1.it;babelscape.com; ",
        "email": "uniroma1.it;babelscape.com; ",
        "github": "https://github.com/SapienzaNLP/bmr",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Sapienza University of Rome;Babelscape",
        "aff_unique_dep": "Sapienza NLP Group;",
        "aff_unique_url": "https://www.uniroma1.it;https://www.babelscape.com",
        "aff_unique_abbr": "Sapienza;",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Rome",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.findings-acl.250",
        "title": "Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Nested entities are observed in many domains due to their compositionality, which cannot be easily recognized by the widely-used sequence labeling framework.A natural solution is to treat the task as a span classification problem. To learn better span representation and increase classification performance, it is crucial to effectively integrate heterogeneous factors including inside tokens, boundaries, labels, and related spans which could be contributing to nested entities recognition. To fuse these heterogeneous factors, we propose a novel triaffine mechanism including triaffine attention and scoring.Triaffine attention uses boundaries and labels as queries and uses inside tokens and related spans as keys and values for span representations.Triaffine scoring interacts with boundaries and span representations for classification. Experiments show that our proposed method outperforms previous span-based methods, achieves the state-of-the-art F1 scores on nested NER datasets GENIA and KBP2017, and shows comparable results on ACE2004 and ACE2005.",
        "author": "Zheng Yuan; Chuanqi Tan; Songfang Huang; Fei Huang",
        "authorids": "/z/zheng-yuan/; /c/chuanqi-tan/; /s/songfang-huang/; /f/fei-huang/",
        "bibtex": "@inproceedings{yuan-etal-2022-fusing,\n    title = \"Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition\",\n    author = \"Yuan, Zheng  and\n      Tan, Chuanqi  and\n      Huang, Songfang  and\n      Huang, Fei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.250/\",\n    doi = \"10.18653/v1/2022.findings-acl.250\",\n    pages = \"3174--3186\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.250.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.250/",
        "pdf_size": 620285,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=326739813811865930&as_sdt=5,36&sciodt=0,36&hl=en",
        "gs_version_total": 5,
        "aff": "Tsinghua University+Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "mails.tsinghua.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "mails.tsinghua.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "Tsinghua University;Alibaba Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "THU;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.318",
        "title": "GCPG: A General Framework for Controllable Paraphrase Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Controllable paraphrase generation (CPG) incorporates various external conditions to obtain desirable paraphrases. However, existing works only highlight a special condition under two indispensable aspects of CPG (i.e., lexically and syntactically CPG) individually, lacking a unified circumstance to explore and analyze their effectiveness. In this paper, we propose a general controllable paraphrase generation framework (GCPG), which represents both lexical and syntactical conditions as text sequences and uniformly processes them in an encoder-decoder paradigm. Under GCPG, we reconstruct commonly adopted lexical condition (i.e., Keywords) and syntactical conditions (i.e., Part-Of-Speech sequence, Constituent Tree, Masked Template and Sentential Exemplar) and study the combination of the two types. In particular, for Sentential Exemplar condition, we propose a novel exemplar construction method \u2014 Syntax-Similarity based Exemplar (SSE). SSE retrieves a syntactically similar but lexically different sentence as the exemplar for each target sentence, avoiding exemplar-side words copying problem. Extensive experiments demonstrate that GCPG with SSE achieves state-of-the-art performance on two popular benchmarks. In addition, the combination of lexical and syntactical conditions shows the significant controllable ability of paraphrase generation, and these empirical results could provide novel insight to user-oriented paraphrasing.",
        "author": "Kexin Yang; Dayiheng Liu; Wenqiang Lei; Baosong Yang; Haibo Zhang; Xue Zhao; Wenqing Yao; Boxing Chen",
        "authorids": "/k/kexin-yang/; /d/dayiheng-liu/; /w/wenqiang-lei/; /b/baosong-yang/; /h/haibo-zhang/; /x/xue-zhao/; /w/wenqing-yao/; /b/boxing-chen/",
        "bibtex": "@inproceedings{yang-etal-2022-gcpg,\n    title = \"{GCPG}: A General Framework for Controllable Paraphrase Generation\",\n    author = \"Yang, Kexin  and\n      Liu, Dayiheng  and\n      Lei, Wenqiang  and\n      Yang, Baosong  and\n      Zhang, Haibo  and\n      Zhao, Xue  and\n      Yao, Wenqing  and\n      Chen, Boxing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.318/\",\n    doi = \"10.18653/v1/2022.findings-acl.318\",\n    pages = \"4035--4047\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.318.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.318/",
        "pdf_size": 631005,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5423789186598857166&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Alibaba Group\u2660; Alibaba Group\u2660; National University of Singapore\u2662; Alibaba Group\u2660; Alibaba Group\u2660; Alibaba Group\u2660; Alibaba Group\u2660; Alibaba Group\u2660",
        "aff_domain": "gmail.com;gmail.com; ; ; ; ; ; ",
        "email": "gmail.com;gmail.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;0;0;0;0;0",
        "aff_unique_norm": "Alibaba Group;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.alibaba.com;https://www.nus.edu.sg",
        "aff_unique_abbr": "Alibaba;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.acl-long.191",
        "title": "GL-CLeF: A Global\u2013Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Due to high data demands of current methods, attention to zero-shot cross-lingual spoken language understanding (SLU) has grown, as such approaches greatly reduce human annotation effort. However, existing models solely rely on shared parameters, which can only perform implicit alignment across languages. We present Global-Local Contrastive Learning Framework (GL-CLeF) to address this shortcoming. Specifically, we employ contrastive learning, leveraging bilingual dictionaries to construct multilingual views of the same utterance, then encourage their representations to be more similar than negative example pairs, which achieves to explicitly align representations of similar sentences across languages. In addition, a key step in GL-CLeF is a proposed Local and Global component, which achieves a fine-grained cross-lingual transfer (i.e., sentence-level Local intent transfer, token-level Local slot transfer, and semantic-level Global transfer across intent and slot). Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and successfully pulls representations of similar sentences across languages closer.",
        "author": "Libo Qin; Qiguang Chen; Tianbao Xie; Qixin Li; Jian-Guang Lou; Wanxiang Che; Min-Yen Kan",
        "authorids": "/l/libo-qin/; /q/qiguang-chen/; /t/tianbao-xie/; /q/qixin-li/; /j/jian-guang-lou/; /w/wanxiang-che/; /m/min-yen-kan/",
        "bibtex": "@inproceedings{qin-etal-2022-gl,\n    title = \"{GL}-{CL}e{F}: A Global{--}Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding\",\n    author = \"Qin, Libo  and\n      Chen, Qiguang  and\n      Xie, Tianbao  and\n      Li, Qixin  and\n      Lou, Jian-Guang  and\n      Che, Wanxiang  and\n      Kan, Min-Yen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.191/\",\n    doi = \"10.18653/v1/2022.acl-long.191\",\n    pages = \"2677--2686\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.191.pdf",
        "site": "https://aclanthology.org/2022.acl-long.191/",
        "pdf_size": 2888505,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10798368169356275175&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Microsoft Research Asia; Harbin Institute of Technology; National University of Singapore",
        "aff_domain": "ir.hit.edu.cn; ;ir.hit.edu.cn;ir.hit.edu.cn;microsoft.com;ir.hit.edu.cn;comp.nus.edu.sg",
        "email": "ir.hit.edu.cn; ;ir.hit.edu.cn;ir.hit.edu.cn;microsoft.com;ir.hit.edu.cn;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;0;2",
        "aff_unique_norm": "Harbin Institute of Technology;Microsoft;National University of Singapore",
        "aff_unique_dep": ";Research;",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.microsoft.com/en-us/research/group/asia;https://www.nus.edu.sg",
        "aff_unique_abbr": "HIT;MSR Asia;NUS",
        "aff_campus_unique_index": "0;0;0;0;1;0",
        "aff_campus_unique": "Harbin;Asia;",
        "aff_country_unique_index": "0;0;0;0;0;0;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.acl-long.26",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25\u00d7 parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
        "author": "Zhengxiao Du; Yujie Qian; Xiao Liu; Ming Ding; Jiezhong Qiu; Zhilin Yang; Jie Tang",
        "authorids": "/z/zhengxiao-du/; /y/yujie-qian/; /x/xiao-liu/; /m/ming-ding/; /j/jiezhong-qiu/; /z/zhilin-yang/; /j/jie-tang/",
        "bibtex": "@inproceedings{du-etal-2022-glm,\n    title = \"{GLM}: General Language Model Pretraining with Autoregressive Blank Infilling\",\n    author = \"Du, Zhengxiao  and\n      Qian, Yujie  and\n      Liu, Xiao  and\n      Ding, Ming  and\n      Qiu, Jiezhong  and\n      Yang, Zhilin  and\n      Tang, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.26/\",\n    doi = \"10.18653/v1/2022.acl-long.26\",\n    pages = \"320--335\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.26.pdf",
        "site": "https://aclanthology.org/2022.acl-long.26/",
        "pdf_size": 723218,
        "gs_citation": 1644,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4670351190176031491&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Tsinghua University+Beijing Academy of Arti\ufb01cial Intelligence (BAAI); MIT CSAIL; Tsinghua University+Beijing Academy of Arti\ufb01cial Intelligence (BAAI); Tsinghua University+Beijing Academy of Arti\ufb01cial Intelligence (BAAI); Tsinghua University+Beijing Academy of Arti\ufb01cial Intelligence (BAAI); Tsinghua University+Shanghai Qi Zhi Institute; Tsinghua University+Beijing Academy of Arti\ufb01cial Intelligence (BAAI)",
        "aff_domain": "mails.tsinghua.edu.cn;csail.mit.edu; ; ; ;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;csail.mit.edu; ; ; ;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/THUDM/GLM",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;0+1;0+1;0+1;0+3;0+1",
        "aff_unique_norm": "Tsinghua University;Beijing Academy of Artificial Intelligence;Massachusetts Institute of Technology;Shanghai Qi Zhi Institute",
        "aff_unique_dep": ";;Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baai.ac.cn;https://www.csail.mit.edu;https://www.qz.io",
        "aff_unique_abbr": "THU;BAAI;MIT CSAIL;",
        "aff_campus_unique_index": ";1;;;;;",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+0;1;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.131",
        "title": "GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Deep learning (DL) techniques involving fine-tuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer\u2019s disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research. As an alternative to fitting model parameters directly, we propose a novel method by which a Transformer DL model (GPT-2) pre-trained on general English text is paired with an artificially degraded version of itself (GPT-D), to compute the ratio between these two models\u2019 perplexities on language from cognitively healthy and impaired individuals. This technique approaches state-of-the-art performance on text data from a widely used \u201cCookie Theft\u201d picture description task, and unlike established alternatives also generalizes well to spontaneous conversations. Furthermore, GPT-D generates text with characteristics known to be associated with AD, demonstrating the induction of dementia-related linguistic anomalies. Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models, the language that they produce, and the deleterious effects of dementia on human speech and language characteristics.",
        "author": "Changye Li; David Knopman; Weizhe Xu; Trevor Cohen; Serguei Pakhomov",
        "authorids": "/c/changye-li/; /d/david-knopman/; /w/weizhe-xu/; /t/trevor-cohen/; /s/serguei-pakhomov/",
        "bibtex": "@inproceedings{li-etal-2022-gpt,\n    title = \"{GPT}-{D}: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models\",\n    author = \"Li, Changye  and\n      Knopman, David  and\n      Xu, Weizhe  and\n      Cohen, Trevor  and\n      Pakhomov, Serguei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.131/\",\n    doi = \"10.18653/v1/2022.acl-long.131\",\n    pages = \"1866--1877\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.131.pdf",
        "site": "https://aclanthology.org/2022.acl-long.131/",
        "pdf_size": 1007062,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10894716060735247727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Institute of Health Informatics, University of Minnesota; Mayo Clinic, Rochester, MN; Biomedical and Health Informatics, University of Washington; Biomedical and Health Informatics, University of Washington; Pharmaceutical Care and Health Systems, University of Minnesota",
        "aff_domain": "umn.edu;mayo.edu;uw.edu;uw.edu;umn.edu",
        "email": "umn.edu;mayo.edu;uw.edu;uw.edu;umn.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "University of Minnesota;Mayo Clinic;University of Washington",
        "aff_unique_dep": "Institute of Health Informatics;;Biomedical and Health Informatics",
        "aff_unique_url": "https://www.umn.edu;https://www.mayoclinic.org;https://www.washington.edu",
        "aff_unique_abbr": ";Mayo Clinic;UW",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Rochester",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.77",
        "title": "GRS: Combining Generation and Revision in Unsupervised Sentence Simplification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose GRS: an unsupervised approach to sentence simplification that combines text generation and text revision. We start with an iterative framework in which an input sentence is revised using explicit edit operations, and add paraphrasing as a new edit operation. This allows us to combine the advantages of generative and revision-based approaches: paraphrasing captures complex edit operations, and the use of explicit edit operations in an iterative manner provides controllability and interpretability. We demonstrate these advantages of GRS compared to existing methods on the Newsela and ASSET datasets.",
        "author": "Mohammad Dehghan; Dhruv Kumar; Lukasz Golab",
        "authorids": "/m/mohammad-dehghan/; /d/dhruv-kumar/; /l/lukasz-golab/",
        "bibtex": "@inproceedings{dehghan-etal-2022-grs,\n    title = \"{GRS}: Combining Generation and Revision in Unsupervised Sentence Simplification\",\n    author = \"Dehghan, Mohammad  and\n      Kumar, Dhruv  and\n      Golab, Lukasz\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.77/\",\n    doi = \"10.18653/v1/2022.findings-acl.77\",\n    pages = \"949--960\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.77.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.77/",
        "pdf_size": 399720,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2270782249469599646&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Waterloo; Grammarly; University of Waterloo",
        "aff_domain": "uwaterloo.ca;uwaterloo.ca;grammarly.com",
        "email": "uwaterloo.ca;uwaterloo.ca;grammarly.com",
        "github": "https://github.com/imohammad12/GRS",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Waterloo;Grammarly",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://uwaterloo.ca;https://www.grammarly.com",
        "aff_unique_abbr": "UW;Grammarly",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.findings-acl.238",
        "title": "Gaussian Multi-head Attention for Simultaneous Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Simultaneous machine translation (SiMT) outputs translation while receiving the streaming source inputs, and hence needs a policy to determine where to start translating. The alignment between target and source words often implies the most informative source word for each target word, and hence provides the unified control over translation quality and latency, but unfortunately the existing SiMT methods do not explicitly model the alignment to perform the control. In this paper, we propose Gaussian Multi-head Attention (GMA) to develop a new SiMT policy by modeling alignment and translation in a unified manner. For SiMT policy, GMA models the aligned source position of each target word, and accordingly waits until its aligned position to start translating. To integrate the learning of alignment into the translation model, a Gaussian distribution centered on predicted aligned position is introduced as an alignment-related prior, which cooperates with translation-related soft attention to determine the final attention. Experiments on En-Vi and De-En tasks show that our method outperforms strong baselines on the trade-off between translation and latency.",
        "author": "Shaolei Zhang; Yang Feng",
        "authorids": "/s/shaolei-zhang/; /y/yang-feng/",
        "bibtex": "@inproceedings{zhang-feng-2022-gaussian,\n    title = \"{G}aussian Multi-head Attention for Simultaneous Machine Translation\",\n    author = \"Zhang, Shaolei  and\n      Feng, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.238/\",\n    doi = \"10.18653/v1/2022.findings-acl.238\",\n    pages = \"3019--3030\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.238.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.238/",
        "pdf_size": 1640864,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6447772592778254244&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/ictnlp/GMA",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.213",
        "title": "Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Data modification, either via additional training datasets, data augmentation, debiasing, and dataset filtering, has been proposed as an effective solution for generalizing to out-of-domain (OOD) inputs, in both natural language processing and computer vision literature. However, the effect of data modification on adversarial robustness remains unclear. In this work, we conduct a comprehensive study of common data modification strategies and evaluate not only their in-domain and OOD performance, but also their adversarial robustness (AR).We also present results on a two-dimensional synthetic dataset to visualize the effect of each method on the training distribution. This work serves as an empirical study towards understanding the relationship between generalizing to unseen domains and defending against adversarial perturbations. Our findings suggest that more data (either via additional datasets or data augmentation) benefits both OOD accuracy and AR.However, data filtering (previously shown to improve OOD accuracy on natural language inference) hurts OOD accuracy on other tasks such as question answering and image classification. We provide insights from our experiments to inform future work in this direction.",
        "author": "Tejas Gokhale; Swaroop Mishra; Man Luo; Bhavdeep Sachdeva; Chitta Baral",
        "authorids": "/t/tejas-gokhale/; /s/swaroop-mishra/; /m/man-luo/; /b/bhavdeep-sachdeva/; /c/chitta-baral/",
        "bibtex": "@inproceedings{gokhale-etal-2022-generalized,\n    title = \"\\textit{Generalized but not Robust?} Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness\",\n    author = \"Gokhale, Tejas  and\n      Mishra, Swaroop  and\n      Luo, Man  and\n      Sachdeva, Bhavdeep  and\n      Baral, Chitta\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.213/\",\n    doi = \"10.18653/v1/2022.findings-acl.213\",\n    pages = \"2705--2718\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.213.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.213/",
        "pdf_size": 581553,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7761901102673929375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Arizona State University; Arizona State University; Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.225",
        "title": "Generated Knowledge Prompting for Commonsense Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at github.com/liujch1998/GKP",
        "author": "Jiacheng Liu; Alisa Liu; Ximing Lu; Sean Welleck; Peter West; Ronan Le Bras; Yejin Choi; Hannaneh Hajishirzi",
        "authorids": "/j/jiacheng-liu/; /a/alisa-liu/; /x/ximing-lu/; /s/sean-welleck/; /p/peter-west/; /r/ronan-le-bras/; /y/yejin-choi/; /h/hannaneh-hajishirzi/",
        "bibtex": "@inproceedings{liu-etal-2022-generated,\n    title = \"Generated Knowledge Prompting for Commonsense Reasoning\",\n    author = \"Liu, Jiacheng  and\n      Liu, Alisa  and\n      Lu, Ximing  and\n      Welleck, Sean  and\n      West, Peter  and\n      Le Bras, Ronan  and\n      Choi, Yejin  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.225/\",\n    doi = \"10.18653/v1/2022.acl-long.225\",\n    pages = \"3154--3169\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.225.pdf",
        "site": "https://aclanthology.org/2022.acl-long.225/",
        "pdf_size": 838523,
        "gs_citation": 361,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8985499412913210509&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660Allen Institute for Artificial Intelligence; Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660Allen Institute for Artificial Intelligence",
        "aff_domain": "cs.washington.edu; ; ; ; ; ; ; ",
        "email": "cs.washington.edu; ; ; ; ; ; ; ",
        "github": "github.com/liujch1998/GKP",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;1;0;0",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.586",
        "title": "Generating Biographies on Wikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information. To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally. To this end, we curate a dataset of 1,500 biographies about women. We analyze our generated text to understand how differences in available web evidence data affect generation. We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation. We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.",
        "author": "Angela Fan; Claire Gardent",
        "authorids": "/a/angela-fan/; /c/claire-gardent/",
        "bibtex": "@inproceedings{fan-gardent-2022-generating,\n    title = \"Generating Biographies on {W}ikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies\",\n    author = \"Fan, Angela  and\n      Gardent, Claire\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.586/\",\n    doi = \"10.18653/v1/2022.acl-long.586\",\n    pages = \"8561--8576\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.586.pdf",
        "site": "https://aclanthology.org/2022.acl-long.586/",
        "pdf_size": 740501,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "FAIR / LORIA, Universit\u00e9 de Lorraine; CNRS/LORIA, Nancy, France",
        "aff_domain": "fb.com;loria.fr",
        "email": "fb.com;loria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universit\u00e9 de Lorraine;CNRS",
        "aff_unique_dep": "FAIR / LORIA;LORIA",
        "aff_unique_url": "https://www.univ-lorraine.fr;https://www.loria.fr",
        "aff_unique_abbr": ";CNRS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Nancy",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.acl-long.190",
        "title": "Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.",
        "author": "Yuxiang Wu; Matt Gardner; Pontus Stenetorp; Pradeep Dasigi",
        "authorids": "/y/yuxiang-wu/; /m/matt-gardner/; /p/pontus-stenetorp/; /p/pradeep-dasigi/",
        "bibtex": "@inproceedings{wu-etal-2022-generating,\n    title = \"Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets\",\n    author = \"Wu, Yuxiang  and\n      Gardner, Matt  and\n      Stenetorp, Pontus  and\n      Dasigi, Pradeep\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.190/\",\n    doi = \"10.18653/v1/2022.acl-long.190\",\n    pages = \"2660--2676\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.190.pdf",
        "site": "https://aclanthology.org/2022.acl-long.190/",
        "pdf_size": 1477907,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3276896451483384917&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University College London\u2020; Microsoft Semantic Machines\u2021; University College London\u2020; Allen Institute for AI\u00a7",
        "aff_domain": "cs.ucl.ac.uk;microsoft.com;cs.ucl.ac.uk;allenai.org",
        "email": "cs.ucl.ac.uk;microsoft.com;cs.ucl.ac.uk;allenai.org",
        "github": "https://github.com/jimmycode/gen-debiased-nli",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University College London;Microsoft;Allen Institute for AI",
        "aff_unique_dep": ";Semantic Machines;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.microsoft.com;https://allenai.org",
        "aff_unique_abbr": "UCL;Microsoft;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2022.acl-long.175",
        "title": "Generating Scientific Claims for Zero-Shot Scientific Fact Checking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise. To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its usefulness in zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new supervised method for generating claims supported by the literature, as well as KBIN, a novel method for generating claim negations. Additionally, we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN, achieve up to 90% performance of fully supervised models trained on manually annotated claims and evidence. A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines",
        "author": "Dustin Wright; David Wadden; Kyle Lo; Bailey Kuehl; Arman Cohan; Isabelle Augenstein; Lucy Lu Wang",
        "authorids": "/d/dustin-wright/; /d/david-wadden/; /k/kyle-lo/; /b/bailey-kuehl/; /a/arman-cohan/; /i/isabelle-augenstein/; /l/lucy-lu-wang/",
        "bibtex": "@inproceedings{wright-etal-2022-generating,\n    title = \"Generating Scientific Claims for Zero-Shot Scientific Fact Checking\",\n    author = \"Wright, Dustin  and\n      Wadden, David  and\n      Lo, Kyle  and\n      Kuehl, Bailey  and\n      Cohan, Arman  and\n      Augenstein, Isabelle  and\n      Wang, Lucy Lu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.175/\",\n    doi = \"10.18653/v1/2022.acl-long.175\",\n    pages = \"2448--2460\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.175.pdf",
        "site": "https://aclanthology.org/2022.acl-long.175/",
        "pdf_size": 445195,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14808292054991015129&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Computer Science, University of Copenhagen, Denmark; University of Washington, Seattle, WA, USA; Allen Institute for Artificial Intelligence, Seattle, WA, USA; Allen Institute for Artificial Intelligence, Seattle, WA, USA; Allen Institute for Artificial Intelligence, Seattle, WA, USA; Dept. of Computer Science, University of Copenhagen, Denmark; Allen Institute for Artificial Intelligence, Seattle, WA, USA",
        "aff_domain": "di.ku.dk;di.ku.dk;cs.washington.edu;allenai.org;allenai.org;allenai.org;allenai.org",
        "email": "di.ku.dk;di.ku.dk;cs.washington.edu;allenai.org;allenai.org;allenai.org;allenai.org",
        "github": "https://github.com/allenai/scientific-claim-generation",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;2;2;0;2",
        "aff_unique_norm": "University of Copenhagen;University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Dept. of Computer Science;;",
        "aff_unique_url": "https://www.ku.dk;https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UCPH;UW;AI2",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;1;1;1;1;0;1",
        "aff_country_unique": "Denmark;United States"
    },
    {
        "id": "2022.acl-long.569",
        "title": "Generating Scientific Definitions with Controllable Complexity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Unfamiliar terminology and complex language can present barriers to understanding science. Natural language processing stands to help address these issues by automatically defining unfamiliar terms. We introduce a new task and dataset for defining scientific terms and controlling the complexity of generated definitions as a way of adapting to a specific reader\u2019s background knowledge. We test four definition generation methods for this new task, finding that a sequence-to-sequence approach is most successful. We then explore the version of the task in which definitions are generated at a target complexity level. We introduce a novel reranking approach and find in human evaluations that it offers superior fluency while also controlling complexity, compared to several controllable generation baselines.",
        "author": "Tal August; Katharina Reinecke; Noah A. Smith",
        "authorids": "/t/tal-august/; /k/katharina-reinecke/; /n/noah-a-smith/",
        "bibtex": "@inproceedings{august-etal-2022-generating,\n    title = \"Generating Scientific Definitions with Controllable Complexity\",\n    author = \"August, Tal  and\n      Reinecke, Katharina  and\n      Smith, Noah A.\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.569/\",\n    doi = \"10.18653/v1/2022.acl-long.569\",\n    pages = \"8298--8317\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.569.pdf",
        "site": "https://aclanthology.org/2022.acl-long.569/",
        "pdf_size": 927599,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16314715243022852794&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Allen Institute for Artificial Intelligence, Seattle, WA, USA + Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "0;0;0+0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.280",
        "title": "Generative Pretraining for Paraphrase Evaluation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce ParaBLEU, a paraphrase representation learning model and evaluation metric for text generation. Unlike previous approaches, ParaBLEU learns to understand paraphrasis using generative conditioning as a pretraining objective. ParaBLEU correlates more strongly with human judgements than existing metrics, obtaining new state-of-the-art results on the 2017 WMT Metrics Shared Task. We show that our model is robust to data scarcity, exceeding previous state-of-the-art performance using only 50% of the available training data and surpassing BLEU, ROUGE and METEOR with only 40 labelled examples. Finally, we demonstrate that ParaBLEU can be used to conditionally generate novel paraphrases from a single demonstration, which we use to confirm our hypothesis that it learns abstract, generalized paraphrase representations.",
        "author": "Jack Weston; Raphael Lenain; Udeepa Meepegama; Emil Fristed",
        "authorids": "/j/jack-weston/; /r/raphael-lenain/; /u/udeepa-meepegama/; /e/emil-fristed/",
        "bibtex": "@inproceedings{weston-etal-2022-generative,\n    title = \"Generative Pretraining for Paraphrase Evaluation\",\n    author = \"Weston, Jack  and\n      Lenain, Raphael  and\n      Meepegama, Udeepa  and\n      Fristed, Emil\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.280/\",\n    doi = \"10.18653/v1/2022.acl-long.280\",\n    pages = \"4052--4073\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.280.pdf",
        "site": "https://aclanthology.org/2022.acl-long.280/",
        "pdf_size": 611283,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1793722690760150686&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Novoic; Novoic; Novoic; Novoic",
        "aff_domain": "novoic.com;novoic.com;novoic.com;novoic.com",
        "email": "novoic.com;novoic.com;novoic.com;novoic.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Novoic",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.novoic.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.115",
        "title": "GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Over the last few years, there has been a move towards data curation for multilingual task-oriented dialogue (ToD) systems that can serve people speaking different languages. However, existing multilingual ToD datasets either have a limited coverage of languages due to the high cost of data curation, or ignore the fact that dialogue entities barely exist in countries speaking these languages. To tackle these limitations, we introduce a novel data curation method that generates GlobalWoZ \u2014 a large-scale multilingual ToD dataset globalized from an English ToD dataset for three unexplored use cases of multilingual ToD systems. Our method is based on translating dialogue templates and filling them with local entities in the target-language countries. Besides, we extend the coverage of target languages to 20 languages. We will release our dataset and a set of strong baselines to encourage research on multilingual ToD systems for real use cases.",
        "author": "Bosheng Ding; Junjie Hu; Lidong Bing; Mahani Aljunied; Shafiq Joty; Luo Si; Chunyan Miao",
        "authorids": "/b/bosheng-ding/; /j/junjie-hu/; /l/lidong-bing/; /m/mahani-aljunied/; /s/shafiq-joty/; /l/luo-si/; /c/chunyan-miao/",
        "bibtex": "@inproceedings{ding-etal-2022-globalwoz,\n    title = \"{G}lobal{W}o{Z}: Globalizing {M}ulti{W}o{Z} to Develop Multilingual Task-Oriented Dialogue Systems\",\n    author = \"Ding, Bosheng  and\n      Hu, Junjie  and\n      Bing, Lidong  and\n      Aljunied, Mahani  and\n      Joty, Shafiq  and\n      Si, Luo  and\n      Miao, Chunyan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.115/\",\n    doi = \"10.18653/v1/2022.acl-long.115\",\n    pages = \"1639--1657\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.115.pdf",
        "site": "https://aclanthology.org/2022.acl-long.115/",
        "pdf_size": 1242569,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1863494813898291808&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "DAMO Academy, Alibaba Group+Nanyang Technological University, Singapore; University of Wisconsin-Madison; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; Nanyang Technological University, Singapore; DAMO Academy, Alibaba Group; Nanyang Technological University, Singapore",
        "aff_domain": "alibaba-inc.com;wisc.edu;alibaba-inc.com;alibaba-inc.com;ntu.edu.sg;alibaba-inc.com;ntu.edu.sg",
        "email": "alibaba-inc.com;wisc.edu;alibaba-inc.com;alibaba-inc.com;ntu.edu.sg;alibaba-inc.com;ntu.edu.sg",
        "github": "",
        "project": "https://ntunlpsg.github.io/project/globalwoz/",
        "author_num": 7,
        "aff_unique_index": "0+1;2;0;0;1;0;1",
        "aff_unique_norm": "Alibaba Group;Nanyang Technological University;University of Wisconsin-Madison",
        "aff_unique_dep": "DAMO Academy;;",
        "aff_unique_url": "https://www.alibaba-group.com;https://www.ntu.edu.sg;https://www.wisc.edu",
        "aff_unique_abbr": "Alibaba;NTU;UW-Madison",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Madison",
        "aff_country_unique_index": "0+1;2;0;0;1;0;1",
        "aff_country_unique": "China;Singapore;United States"
    },
    {
        "id": "2022.findings-acl.12",
        "title": "Going \u201cDeeper\u201d: Structured Sememe Prediction via Transformer with Tree Attention",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Sememe knowledge bases (SKBs), which annotate words with the smallest semantic units (i.e., sememes), have proven beneficial to many NLP tasks. Building an SKB is very time-consuming and labor-intensive. Therefore, some studies have tried to automate the building process by predicting sememes for the unannotated words. However, all existing sememe prediction studies ignore the hierarchical structures of sememes, which are important in the sememe-based semantic description system. In this work, we tackle the structured sememe prediction problem for the first time, which is aimed at predicting a sememe tree with hierarchical structures rather than a set of sememes. We design a sememe tree generation model based on Transformer with adjusted attention mechanism, which shows its superiority over the baselines in experiments. We also conduct a series of quantitative and qualitative analyses of the effectiveness of our model. All the code and data of this paper are available at https://github.com/thunlp/STG.",
        "author": "Yining Ye; Fanchao Qi; Zhiyuan Liu; Maosong Sun",
        "authorids": "/y/yining-ye/; /f/fanchao-qi/; /z/zhiyuan-liu/; /m/maosong-sun/",
        "bibtex": "@inproceedings{ye-etal-2022-going,\n    title = \"Going {\\textquotedblleft}Deeper{\\textquotedblright}: Structured Sememe Prediction via Transformer with Tree Attention\",\n    author = \"Ye, Yining  and\n      Qi, Fanchao  and\n      Liu, Zhiyuan  and\n      Sun, Maosong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.12/\",\n    doi = \"10.18653/v1/2022.findings-acl.12\",\n    pages = \"128--138\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.12.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.12/",
        "pdf_size": 515047,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13161349339345092082&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China + Beijing National Research Center for Information Science and Technology + Institute Guo Qiang, Tsinghua University, Beijing, China + International Innovation Center of Tsinghua University, Shanghai, China + Beijing Academy of Artificial Intelligence; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China + Beijing National Research Center for Information Science and Technology + Institute Guo Qiang, Tsinghua University, Beijing, China + International Innovation Center of Tsinghua University, Shanghai, China + Beijing Academy of Artificial Intelligence; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China + Beijing National Research Center for Information Science and Technology + Institute Guo Qiang, Tsinghua University, Beijing, China + International Innovation Center of Tsinghua University, Shanghai, China + Beijing Academy of Artificial Intelligence; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China + Beijing National Research Center for Information Science and Technology + Institute Guo Qiang, Tsinghua University, Beijing, China + International Innovation Center of Tsinghua University, Shanghai, China + Beijing Academy of Artificial Intelligence",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/thunlp/STG",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+0+0+2;0+1+0+0+2;0+1+0+0+2;0+1+0+0+2",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.baaic.cn",
        "aff_unique_abbr": "THU;;BAAI",
        "aff_campus_unique_index": "0+0+2;0+0+2;0+0+2;0+0+2",
        "aff_campus_unique": "Beijing;;Shanghai",
        "aff_country_unique_index": "0+0+0+0+0;0+0+0+0+0;0+0+0+0+0;0+0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.192",
        "title": "Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates. Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence. However, such methods may suffer from error propagation induced by entity span detection, high cost due to enumeration of all possible text spans, and omission of inter-dependencies among token labels in a sentence. Here we present a simple demonstration-based learning method for NER, which lets the input be prefaced by task demonstrations for in-context learning. We perform a systematic study on demonstration strategy regarding what to include (entity examples, with or without surrounding context), how to select the examples, and what templates to use. Results on in-domain learning and domain adaptation show that the model\u2019s performance in low-resource settings can be largely improved with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train instances). We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance.",
        "author": "Dong-Ho Lee; Akshen Kadakia; Kangmin Tan; Mahak Agarwal; Xinyu Feng; Takashi Shibuya; Ryosuke Mitani; Toshiyuki Sekiya; Jay Pujara; Xiang Ren",
        "authorids": "/d/dong-ho-lee/; /a/akshen-kadakia/; /k/kangmin-tan/; /m/mahak-agarwal/; /x/xinyu-feng/; /t/takashi-shibuya/; /r/ryosuke-mitani/; /t/toshiyuki-sekiya/; /j/jay-pujara/; /x/xiang-ren/",
        "bibtex": "@inproceedings{lee-etal-2022-good,\n    title = \"Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource {NER}\",\n    author = \"Lee, Dong-Ho  and\n      Kadakia, Akshen  and\n      Tan, Kangmin  and\n      Agarwal, Mahak  and\n      Feng, Xinyu  and\n      Shibuya, Takashi  and\n      Mitani, Ryosuke  and\n      Sekiya, Toshiyuki  and\n      Pujara, Jay  and\n      Ren, Xiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.192/\",\n    doi = \"10.18653/v1/2022.acl-long.192\",\n    pages = \"2687--2700\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.192.pdf",
        "site": "https://aclanthology.org/2022.acl-long.192/",
        "pdf_size": 521539,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12494744339498954738&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; R&D Center, Sony Group Corporation; R&D Center, Sony Group Corporation; R&D Center, Sony Group Corporation; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu;sony.com;sony.com;sony.com;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu;sony.com;sony.com;sony.com;usc.edu;usc.edu",
        "github": "https://github.com/INK-USC/fewNER",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;1;1;1;0;0",
        "aff_unique_norm": "University of Southern California;Sony Group Corporation",
        "aff_unique_dep": "Department of Computer Science;R&D Center",
        "aff_unique_url": "https://www.usc.edu;https://www.sony.com",
        "aff_unique_abbr": "USC;Sony",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;1;1;1;0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "2022.findings-acl.224",
        "title": "Good Night at 4 pm?! Time Expressions in Different Cultures",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose the task of culture-specific time expression grounding, i.e. mapping from expressions such as \u201cmorning\u201d in English or \u201cManh\u00e3\u201d in Portuguese to specific hours in the day. We propose 3 language-agnostic methods, one of which achieves promising results on gold standard annotations that we collected for a small number of languages. We then apply this method to 27 languages and analyze the similarities across languages in the grounding of time expressions.",
        "author": "Vered Shwartz",
        "authorids": "/v/vered-shwartz/",
        "bibtex": "@inproceedings{shwartz-2022-good,\n    title = \"Good Night at 4 pm?! Time Expressions in Different Cultures\",\n    author = \"Shwartz, Vered\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.224/\",\n    doi = \"10.18653/v1/2022.findings-acl.224\",\n    pages = \"2842--2853\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.224.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.224/",
        "pdf_size": 481469,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6766294956044396868&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of British Columbia",
        "aff_domain": "cs.ubc.ca",
        "email": "cs.ubc.ca",
        "github": "https://github.com/vered1986/time_expressions",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.320",
        "title": "Graph Enhanced Contrastive Learning for Radiology Findings Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians. Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists, and thus automatic impression generation has attracted substantial attention. With the encoder-decoder framework, most previous studies explore incorporating extra knowledge (e.g., static pre-defined clinical ontologies or extra background information). Yet, they encode such knowledge by a separate encoder to treat it as an extra input to their models, which is limited in leveraging their relations with the original findings. To address the limitation, we propose a unified framework for exploiting both extra knowledge and the original findings in an integrated way so that the critical information (i.e., key words and their relations) can be extracted in an appropriate way to facilitate impression generation. In detail, for each input findings, it is encoded by a text encoder and a graph is constructed through its entities and dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is adopted to model relation information in the constructed graph. Finally, to emphasize the key words in the findings, contrastive learning is introduced to map positive samples (constructed by masking non-key words) closer and push apart negative ones (constructed by masking key words). The experimental results on two datasets, OpenI and MIMIC-CXR, confirm the effectiveness of our proposed method, where the state-of-the-art results are achieved.",
        "author": "Jinpeng Hu; Zhuo Li; Zhihong Chen; Zhen Li; Xiang Wan; Tsung-Hui Chang",
        "authorids": "/j/jinpeng-hu/; /z/zhuo-li/; /z/zhihong-chen/; /z/zhen-li/; /x/xiang-wan/; /t/tsung-hui-chang/",
        "bibtex": "@inproceedings{hu-etal-2022-graph,\n    title = \"Graph Enhanced Contrastive Learning for Radiology Findings Summarization\",\n    author = \"Hu, Jinpeng  and\n      Li, Zhuo  and\n      Chen, Zhihong  and\n      Li, Zhen  and\n      Wan, Xiang  and\n      Chang, Tsung-Hui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.320/\",\n    doi = \"10.18653/v1/2022.acl-long.320\",\n    pages = \"4677--4688\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.320.pdf",
        "site": "https://aclanthology.org/2022.acl-long.320/",
        "pdf_size": 756182,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13396221799781872646&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The Chinese University of Hong Kong (Shenzhen)+Shenzhen Research Institute of Big Data; The Chinese University of Hong Kong (Shenzhen)+Shenzhen Research Institute of Big Data; The Chinese University of Hong Kong (Shenzhen)+Shenzhen Research Institute of Big Data; The Chinese University of Hong Kong (Shenzhen)+Shenzhen Research Institute of Big Data; Shenzhen Research Institute of Big Data+Pazhou Lab, Guangzhou, 510330, China; The Chinese University of Hong Kong (Shenzhen)+Shenzhen Research Institute of Big Data+Pazhou Lab, Guangzhou, 510330, China",
        "aff_domain": "link.cuhk.edu.cn;link.cuhk.edu.cn;link.cuhk.edu.cn;link.cuhk.edu.cn;sribd.cn;link.cuhk.edu.cn",
        "email": "link.cuhk.edu.cn;link.cuhk.edu.cn;link.cuhk.edu.cn;link.cuhk.edu.cn;sribd.cn;link.cuhk.edu.cn",
        "github": "https://github.com/jinpeng01/AIG_CL",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;0+1;1+2;0+1+2",
        "aff_unique_norm": "Chinese University of Hong Kong;Shenzhen Research Institute of Big Data;Pazhou Lab",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cuhk.edu.cn;http://www.sribd.cn;",
        "aff_unique_abbr": "CUHK;;",
        "aff_campus_unique_index": "0;0;0;0;2;0+2",
        "aff_campus_unique": "Shenzhen;;Guangzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.108",
        "title": "Graph Neural Networks for Multiparallel Word Alignment",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences. We show that community detection algorithms can provide valuable information for multiparallel word alignment. Our method outperforms previous work on three word alignment datasets and on a downstream task.",
        "author": "Ayyoob Imani; L\u00fctfi Kerem Senel; Masoud Jalili Sabet; Fran\u00e7ois Yvon; Hinrich Schuetze",
        "authorids": "/a/ayyoob-imani/; /l/lutfi-kerem-senel/; /m/masoud-jalili-sabet/; /f/francois-yvon/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{imani-etal-2022-graph,\n    title = \"Graph Neural Networks for Multiparallel Word Alignment\",\n    author = {Imani, Ayyoob  and\n      Senel, L{\\\"u}tfi Kerem  and\n      Jalili Sabet, Masoud  and\n      Yvon, Fran{\\c{c}}ois  and\n      Schuetze, Hinrich},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.108/\",\n    doi = \"10.18653/v1/2022.findings-acl.108\",\n    pages = \"1384--1396\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.108.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.108/",
        "pdf_size": 749296,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16379973604777774141&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 13,
        "aff": "Center for Information and Language Processing (CIS), LMU Munich, Germany; Center for Information and Language Processing (CIS), LMU Munich, Germany; Center for Information and Language Processing (CIS), LMU Munich, Germany; Universit\u00e9 Paris-Saclay, CNRS, LISN, France; Center for Information and Language Processing (CIS), LMU Munich, Germany",
        "aff_domain": "cis.lmu.de;cis.lmu.de;cis.lmu.de;limsi.fr; ",
        "email": "cis.lmu.de;cis.lmu.de;cis.lmu.de;limsi.fr; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "LMU Munich;Universit\u00e9 Paris-Saclay",
        "aff_unique_dep": "Center for Information and Language Processing (CIS);CNRS, LISN",
        "aff_unique_url": "https://www.lmu.de;https://www.universite-paris-saclay.fr",
        "aff_unique_abbr": "LMU;UPS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Germany;France"
    },
    {
        "id": "2022.acl-long.415",
        "title": "Graph Pre-training for AMR Parsing and Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure. Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively. However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge. To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs. In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training. We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks. Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model. To our knowledge, we are the first to consider pre-training on semantic graphs.",
        "author": "Xuefeng Bai; Yulong Chen; Yue Zhang",
        "authorids": "/x/xuefeng-bai/; /y/yulong-chen/; /y/yue-zhang/",
        "bibtex": "@inproceedings{bai-etal-2022-graph,\n    title = \"Graph Pre-training for {AMR} Parsing and Generation\",\n    author = \"Bai, Xuefeng  and\n      Chen, Yulong  and\n      Zhang, Yue\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.415/\",\n    doi = \"10.18653/v1/2022.acl-long.415\",\n    pages = \"6001--6015\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.415.pdf",
        "site": "https://aclanthology.org/2022.acl-long.415/",
        "pdf_size": 360854,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12475612999043573109&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University, China+School of Engineering, Westlake University, China; Zhejiang University, China+School of Engineering, Westlake University, China; School of Engineering, Westlake University, China+Institute of Advanced Technology, Westlake Institute for Advanced Study, China",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;1+2",
        "aff_unique_norm": "Zhejiang University;Westlake University;Westlake Institute for Advanced Study",
        "aff_unique_dep": ";School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.westlake.edu.cn;http://www.wias.org.cn/",
        "aff_unique_abbr": "ZJU;;WIAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.215",
        "title": "Graph Refinement for Coreference Resolution",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The state-of-the-art models for coreference resolution are based on independent mention pair-wise decisions. We propose a modelling approach that learns coreference at the document-level and takes global decisions. For this purpose, we model coreference links in a graph structure where the nodes are tokens in the text, and the edges represent the relationship between them. Our model predicts the graph in a non-autoregressive manner, then iteratively refines it based on previous predictions, allowing global dependencies between decisions. The experimental results show improvements over various baselines, reinforcing the hypothesis that document-level information improves conference resolution.",
        "author": "Lesly Miculicich; James Henderson",
        "authorids": "/l/lesly-miculicich-werlen/; /j/james-henderson/",
        "bibtex": "@inproceedings{miculicich-henderson-2022-graph,\n    title = \"Graph Refinement for Coreference Resolution\",\n    author = \"Miculicich, Lesly  and\n      Henderson, James\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.215/\",\n    doi = \"10.18653/v1/2022.findings-acl.215\",\n    pages = \"2732--2742\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.215.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.215/",
        "pdf_size": 513387,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15025522480283692288&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft + EPFL/Idiap; Idiap Research Institute",
        "aff_domain": "microsoft.com;idiap.ch",
        "email": "microsoft.com;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Microsoft;EPFL;Idiap Research Institute",
        "aff_unique_dep": "Microsoft Corporation;Idiap Research Institute;",
        "aff_unique_url": "https://www.microsoft.com;https://www.epfl.ch;https://www.idiap.ch",
        "aff_unique_abbr": "Microsoft;EPFL;Idiap",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "2022.acl-long.437",
        "title": "Guided Attention Multimodal Multitask Financial Forecasting with Inter-Company Relationships and Global and Local News",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most works on financial forecasting use information directly associated with individual companies (e.g., stock prices, news on the company) to predict stock returns for trading. We refer to such company-specific information as local information. Stock returns may also be influenced by global information (e.g., news on the economy in general), and inter-company relationships. Capturing such diverse information is challenging due to the low signal-to-noise ratios, different time-scales, sparsity and distributions of global and local information from different modalities. In this paper, we propose a model that captures both global and local multimodal information for investment and risk management-related forecasting tasks. Our proposed Guided Attention Multimodal Multitask Network (GAME) model addresses these challenges by using novel attention modules to guide learning with global and local information from different modalities and dynamic inter-company relationship networks. Our extensive experiments show that GAME outperforms other state-of-the-art models in several forecasting tasks and important real-world application case studies.",
        "author": "Gary Ang; Ee-Peng Lim",
        "authorids": "/g/gary-ang/; /e/ee-peng-lim/",
        "bibtex": "@inproceedings{ang-lim-2022-guided,\n    title = \"Guided Attention Multimodal Multitask Financial Forecasting with Inter-Company Relationships and Global and Local News\",\n    author = \"Ang, Gary  and\n      Lim, Ee-Peng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.437/\",\n    doi = \"10.18653/v1/2022.acl-long.437\",\n    pages = \"6313--6326\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.437.pdf",
        "site": "https://aclanthology.org/2022.acl-long.437/",
        "pdf_size": 788599,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7372848464774179140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Singapore Management University; Singapore Management University",
        "aff_domain": "phdcs.smu.edu.sg;smu.edu.sg",
        "email": "phdcs.smu.edu.sg;smu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Singapore Management University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.smu.edu.sg",
        "aff_unique_abbr": "SMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.acl-long.58",
        "title": "HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Document structure is critical for efficient information consumption. However, it is challenging to encode it efficiently into the modern Transformer architecture. In this work, we present HIBRIDS, which injects Hierarchical Biases foR Incorporating Document Structure into attention score calculation. We further present a new task, hierarchical question-summary generation, for summarizing salient content in the source document into a hierarchy of questions and summaries, where each follow-up question inquires about the content of its parent question-summary pair. We also annotate a new dataset with 6,153 question-summary hierarchies labeled on government reports. Experiment results show that our model produces better question-summary hierarchies than comparisons on both hierarchy quality and content coverage, a finding also echoed by human judges. Additionally, our model improves the generation of long-form summaries from long government reports and Wikipedia articles, as measured by ROUGE scores.",
        "author": "Shuyang Cao; Lu Wang",
        "authorids": "/s/shuyang-cao/; /l/lu-wang/",
        "bibtex": "@inproceedings{cao-wang-2022-hibrids,\n    title = \"{HIBRIDS}: Attention with Hierarchical Biases for Structure-aware Long Document Summarization\",\n    author = \"Cao, Shuyang  and\n      Wang, Lu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.58/\",\n    doi = \"10.18653/v1/2022.acl-long.58\",\n    pages = \"786--807\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.58.pdf",
        "site": "https://aclanthology.org/2022.acl-long.58/",
        "pdf_size": 3573121,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9923988918771301044&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "https://shuyangcao.github.io/projects/structure_long_summ",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.236",
        "title": "HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, context-dependent text-to-SQL semantic parsing which translates natural language into SQL in an interaction process has attracted a lot of attentions. Previous works leverage context dependence information either from interaction history utterances or previous predicted queries but fail in taking advantage of both of them since of the mismatch between the natural language and logic-form SQL. In this work, we propose a History Information Enhanced text-to-SQL model (HIE-SQL) to exploit context dependence information from both history utterances and the last predicted SQL query. In view of the mismatch, we treat natural language and SQL as two modalities and propose a bimodal pre-trained model to bridge the gap between them. Besides, we design a schema-linking graph to enhance connections from utterances and the SQL query to database schema. We show our history information enhanced methods improve the performance of HIE-SQL by a significant margin, which achieves new state-of-the-art results on two context-dependent text-to-SQL benchmarks, the SparC and CoSQL datasets, at the writing time.",
        "author": "Yanzhao Zheng; Haibin Wang; Baohua Dong; Xingjun Wang; Changshan Li",
        "authorids": "/y/yanzhao-zheng/; /h/haibin-wang/; /b/baohua-dong/; /x/xingjun-wang/; /c/changshan-li/",
        "bibtex": "@inproceedings{zheng-etal-2022-hie,\n    title = \"{HIE}-{SQL}: History Information Enhanced Network for Context-Dependent Text-to-{SQL} Semantic Parsing\",\n    author = \"Zheng, Yanzhao  and\n      Wang, Haibin  and\n      Dong, Baohua  and\n      Wang, Xingjun  and\n      Li, Changshan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.236/\",\n    doi = \"10.18653/v1/2022.findings-acl.236\",\n    pages = \"2997--3007\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.236.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.236/",
        "pdf_size": 1162234,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10768086318153297325&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China + Tsinghua University",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;mails.tsinghua.edu.cn",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;mails.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Alibaba Group;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.alibaba.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Alibaba;THU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hangzhou;",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.278",
        "title": "HLDC: Hindi Legal Documents Corpus",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Many populous countries including India are burdened with a considerable backlog of legal cases. Development of automated systems that could process legal documents and augment legal practitioners can mitigate this. However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems. The problem gets even more pronounced in the case of low resource languages such as Hindi. In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents in Hindi. Documents are cleaned and structured to enable the development of downstream applications. Further, as a use-case for the corpus, we introduce the task of bail prediction. We experiment with a battery of models and propose a Multi-Task Learning (MTL) based model for the same. MTL models use summarization as an auxiliary task along with bail prediction as the main task. Experiments with different models are indicative of the need for further research in this area.",
        "author": "Arnav Kapoor; Mudit Dhawan; Anmol Goel; Arjun T H; Akshala Bhatnagar; Vibhu Agrawal; Amul Agrawal; Arnab Bhattacharya; Ponnurangam Kumaraguru; Ashutosh Modi",
        "authorids": "/a/arnav-kapoor/; /m/mudit-dhawan/; /a/anmol-goel/; /a/arjun-t-h/; /a/akshala-bhatnagar/; /v/vibhu-agrawal/; /a/amul-agrawal/; /a/arnab-bhattacharya/; /p/ponnurangam-kumaraguru/; /a/ashutosh-modi/",
        "bibtex": "@inproceedings{kapoor-etal-2022-hldc,\n    title = \"{HLDC}: {H}indi Legal Documents Corpus\",\n    author = \"Kapoor, Arnav  and\n      Dhawan, Mudit  and\n      Goel, Anmol  and\n      T H, Arjun  and\n      Bhatnagar, Akshala  and\n      Agrawal, Vibhu  and\n      Agrawal, Amul  and\n      Bhattacharya, Arnab  and\n      Kumaraguru, Ponnurangam  and\n      Modi, Ashutosh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.278/\",\n    doi = \"10.18653/v1/2022.findings-acl.278\",\n    pages = \"3521--3536\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.278.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.278/",
        "pdf_size": 2088607,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11650840287211023123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "IIIT Hyderabad; IIIT Delhi; IIIT Hyderabad; IIIT Hyderabad; IIIT Delhi; IIIT Delhi; IIIT Hyderabad; IIT Kanpur; IIIT Hyderabad; IIT Kanpur",
        "aff_domain": "research.iiit.ac.in;iiitd.ac.in;research.iiit.ac.in;research.iiit.ac.in;iiitd.ac.in;iiitd.ac.in;students.iiit.ac.in;cse.iitk.ac.in;iiit.ac.in;cse.iitk.ac.in",
        "email": "research.iiit.ac.in;iiitd.ac.in;research.iiit.ac.in;research.iiit.ac.in;iiitd.ac.in;iiitd.ac.in;students.iiit.ac.in;cse.iitk.ac.in;iiit.ac.in;cse.iitk.ac.in",
        "github": "https://github.com/Exploration-Lab/HLDC",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;0;0;1;1;0;2;0;2",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;International Institute of Information Technology, Delhi;Indian Institute of Technology Kanpur",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://iiit Hyderabad.ac.in;https://www.iiitdelhi.ac.in;https://www.iitk.ac.in",
        "aff_unique_abbr": "IIIT-H;IIIT-D;IITK",
        "aff_campus_unique_index": "0;1;0;0;1;1;0;2;0;2",
        "aff_campus_unique": "Hyderabad;Delhi;Kanpur",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.373",
        "title": "HOLM: Hallucinating Objects with Language Models for Referring Expression Recognition in Partially-Observed Scenes",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "AI systems embodied in the physical world face a fundamental challenge of partial observability; operating with only a limited view and knowledge of the environment. This creates challenges when AI systems try to reason about language and its relationship with the environment: objects referred to through language (e.g. giving many instructions) are not immediately visible. Actions by the AI system may be required to bring these objects in view. A good benchmark to study this challenge is Dynamic Referring Expression Recognition (dRER) task, where the goal is to find a target location by dynamically adjusting the field of view (FoV) in a partially observed 360 scenes. In this paper, we introduce HOLM, Hallucinating Objects with Language Models, to address the challenge of partial observability. HOLM uses large pre-trained language models (LMs) to infer object hallucinations for the unobserved part of the environment. Our core intuition is that if a pair of objects co-appear in an environment frequently, our usage of language should reflect this fact about the world. Based on this intuition, we prompt language models to extract knowledge about object affinities which gives us a proxy for spatial relationships of objects. Our experiments show that HOLM performs better than the state-of-the-art approaches on two datasets for dRER; allowing to study generalization for both indoor and outdoor settings.",
        "author": "Volkan Cirik; Louis-Philippe Morency; Taylor Berg-Kirkpatrick",
        "authorids": "/v/volkan-cirik/; /l/louis-philippe-morency/; /t/taylor-berg-kirkpatrick/",
        "bibtex": "@inproceedings{cirik-etal-2022-holm,\n    title = \"{HOLM}: Hallucinating Objects with Language Models for Referring Expression Recognition in Partially-Observed Scenes\",\n    author = \"Cirik, Volkan  and\n      Morency, Louis-Philippe  and\n      Berg-Kirkpatrick, Taylor\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.373/\",\n    doi = \"10.18653/v1/2022.acl-long.373\",\n    pages = \"5440--5453\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.373.pdf",
        "site": "https://aclanthology.org/2022.acl-long.373/",
        "pdf_size": 1521591,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5156019184592579698&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; University of California San Diego",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;eng.ucsd.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;eng.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://ucsd.edu",
        "aff_unique_abbr": "CMU;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.69",
        "title": "HYPHEN: Hyperbolic Hawkes Attention For Text Streams",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Analyzing the temporal sequence of texts from sources such as social media, news, and parliamentary debates is a challenging problem as it exhibits time-varying scale-free properties and fine-grained timing irregularities. We propose a Hyperbolic Hawkes Attention Network (HYPHEN), which learns a data-driven hyperbolic space and models irregular powerlaw excitations using a hyperbolic Hawkes process. Through quantitative and exploratory experiments over financial NLP, suicide ideation detection, and political debate analysis we demonstrate HYPHEN\u2019s practical applicability for modeling online text sequences in a geometry agnostic manner.",
        "author": "Shivam Agarwal; Ramit Sawhney; Sanchit Ahuja; Ritesh Soun; Sudheer Chava",
        "authorids": "/s/shivam-agarwal/; /r/ramit-sawhney/; /s/sanchit-ahuja/; /r/ritesh-soun/; /s/sudheer-chava/",
        "bibtex": "@inproceedings{agarwal-etal-2022-hyphen,\n    title = \"{HYPHEN}: Hyperbolic {H}awkes Attention For Text Streams\",\n    author = \"Agarwal, Shivam  and\n      Sawhney, Ramit  and\n      Ahuja, Sanchit  and\n      Soun, Ritesh  and\n      Chava, Sudheer\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.69/\",\n    doi = \"10.18653/v1/2022.acl-short.69\",\n    pages = \"620--627\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.69.pdf",
        "site": "https://aclanthology.org/2022.acl-short.69/",
        "pdf_size": 1176145,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14807099602514845310&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Financial Services Innovation Lab, Georgia Institute of Technology; Financial Services Innovation Lab, Georgia Institute of Technology; Financial Services Innovation Lab, Georgia Institute of Technology; Financial Services Innovation Lab, Georgia Institute of Technology; Financial Services Innovation Lab, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;scheller.gatech.edu; ; ; ",
        "email": "gatech.edu;scheller.gatech.edu; ; ; ",
        "github": "https://github.com/gtfintechlab/HYPHEN-ACL620",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "Financial Services Innovation Lab",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.236",
        "title": "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "State-of-the-art abstractive summarization systems often generate hallucinations; i.e., content that is not directly inferable from the source text. Despite being assumed to be incorrect, we find that much hallucinated content is actually consistent with world knowledge, which we call factual hallucinations. Including these factual hallucinations in a summary can be beneficial because they provide useful background information. In this work, we propose a novel detection approach that separates factual from non-factual hallucinations of entities. Our method is based on an entity\u2019s prior and posterior probabilities according to pre-trained and finetuned masked language models, respectively. Empirical results suggest that our method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks. Furthermore, we use our method as a reward signal to train a summarization system using an off-line reinforcement learning (RL) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness.",
        "author": "Meng Cao; Yue Dong; Jackie Cheung",
        "authorids": "/m/meng-cao/; /y/yue-dong/; /j/jackie-chi-kit-cheung/",
        "bibtex": "@inproceedings{cao-etal-2022-hallucinated,\n    title = \"Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization\",\n    author = \"Cao, Meng  and\n      Dong, Yue  and\n      Cheung, Jackie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.236/\",\n    doi = \"10.18653/v1/2022.acl-long.236\",\n    pages = \"3340--3354\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.236.pdf",
        "site": "https://aclanthology.org/2022.acl-long.236/",
        "pdf_size": 579906,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10331250949718154372&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, McGill University, Montreal, QC, Canada + MILA, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada + MILA, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada + MILA, Montreal, QC, Canada",
        "aff_domain": "mail.mcgill.ca;mail.mcgill.ca;cs.mcgill.ca",
        "email": "mail.mcgill.ca;mail.mcgill.ca;cs.mcgill.ca",
        "github": "https://github.com/mcao516/EntFASource",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "McGill University;Mila",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.mcgill.ca;https://mila.quebec",
        "aff_unique_abbr": "McGill;MILA",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-short.4",
        "title": "Have my arguments been replied to? Argument Pair Extraction as Machine Reading Comprehension",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Argument pair extraction (APE) aims to automatically mine argument pairs from two interrelated argumentative documents. Existing studies typically identify argument pairs indirectly by predicting sentence-level relations between two documents, neglecting the modeling of the holistic argument-level interactions. Towards this issue, we propose to address APE via a machine reading comprehension (MRC) framework with two phases. The first phase employs an argument mining (AM) query to identify all arguments in two documents. The second phase considers each identified argument as an APE query to extract its paired arguments from another document, allowing to better capture the argument-level interactions. Also, this framework enables these two phases to be jointly trained in a single MRC model, thereby maximizing the mutual benefits of them. Experimental results demonstrate that our approach achieves the best performance, outperforming the state-of-the-art method by 7.11% in F1 score.",
        "author": "Jianzhu Bao; Jingyi Sun; Qinglin Zhu; Ruifeng Xu",
        "authorids": "/j/jianzhu-bao/; /j/jingyi-sun/; /q/qinglin-zhu/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{bao-etal-2022-arguments,\n    title = \"Have my arguments been replied to? Argument Pair Extraction as Machine Reading Comprehension\",\n    author = \"Bao, Jianzhu  and\n      Sun, Jingyi  and\n      Zhu, Qinglin  and\n      Xu, Ruifeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.4/\",\n    doi = \"10.18653/v1/2022.acl-short.4\",\n    pages = \"29--35\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.4.pdf",
        "site": "https://aclanthology.org/2022.acl-short.4/",
        "pdf_size": 317384,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=857781160423715867&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 2,
        "aff": "Harbin Institute of Technology (Shenzhen), China+Joint Lab of China Merchants Securities and HITSZ; Harbin Institute of Technology (Shenzhen), China+Joint Lab of China Merchants Securities and HITSZ; Harbin Institute of Technology (Shenzhen), China+Joint Lab of China Merchants Securities and HITSZ; Harbin Institute of Technology (Shenzhen), China+Peng Cheng Laboratory, Shenzhen, China",
        "aff_domain": "gmail.com;gmail.com;stu.hit.edu.cn;hit.edu.cn",
        "email": "gmail.com;gmail.com;stu.hit.edu.cn;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+2",
        "aff_unique_norm": "Harbin Institute of Technology;China Merchants Securities;Pengcheng Laboratory",
        "aff_unique_dep": ";Joint Lab;Peng Cheng Laboratory",
        "aff_unique_url": "http://en.hhit.edu.cn/;http://www.cms.com.cn;",
        "aff_unique_abbr": "HIT;CMS;",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.155",
        "title": "Headed-Span-Based Projective Dependency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a new method for projective dependency parsing based on headed spans. In a projective dependency tree, the largest subtree rooted at each word covers a contiguous sequence (i.e., a span) in the surface order. We call such a span marked by a root word headed span. A projective dependency tree can be represented as a collection of headed spans. We decompose the score of a dependency tree into the scores of the headed spans and design a novel O(n3) dynamic programming algorithm to enable global training and exact inference. Our model achieves state-of-the-art or competitive results on PTB, CTB, and UD",
        "author": "Songlin Yang; Kewei Tu",
        "authorids": "/s/songlin-yang/; /k/kewei-tu/",
        "bibtex": "@inproceedings{yang-tu-2022-headed,\n    title = \"Headed-Span-Based Projective Dependency Parsing\",\n    author = \"Yang, Songlin  and\n      Tu, Kewei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.155/\",\n    doi = \"10.18653/v1/2022.acl-long.155\",\n    pages = \"2188--2200\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.155.pdf",
        "site": "https://aclanthology.org/2022.acl-long.155/",
        "pdf_size": 347879,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17133409160167753440&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Information Science and Technology, ShanghaiTech University + Shanghai Engineering Research Center of Intelligent Vision and Imaging; School of Information Science and Technology, ShanghaiTech University + Shanghai Engineering Research Center of Intelligent Vision and Imaging",
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn",
        "email": "shanghaitech.edu.cn;shanghaitech.edu.cn",
        "github": "https://github.com/sustcsonglin/span-based-dependency-parsing",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "ShanghaiTech University;Shanghai Engineering Research Center of Intelligent Vision and Imaging",
        "aff_unique_dep": "School of Information Science and Technology;",
        "aff_unique_url": "https://www.shanghaitech.edu.cn;",
        "aff_unique_abbr": "ShanghaiTech;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.349",
        "title": "HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-type-dependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.",
        "author": "Jia-Chen Gu; Chao-Hong Tan; Chongyang Tao; Zhen-Hua Ling; Huang Hu; Xiubo Geng; Daxin Jiang",
        "authorids": "/j/jia-chen-gu/; /c/chao-hong-tan/; /c/chongyang-tao/; /z/zhen-hua-ling/; /h/huang-hu/; /x/xiubo-geng/; /d/daxin-jiang/",
        "bibtex": "@inproceedings{gu-etal-2022-hetermpc,\n    title = \"{H}eter{MPC}: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations\",\n    author = \"Gu, Jia-Chen  and\n      Tan, Chao-Hong  and\n      Tao, Chongyang  and\n      Ling, Zhen-Hua  and\n      Hu, Huang  and\n      Geng, Xiubo  and\n      Jiang, Daxin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.349/\",\n    doi = \"10.18653/v1/2022.acl-long.349\",\n    pages = \"5086--5097\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.349.pdf",
        "site": "https://aclanthology.org/2022.acl-long.349/",
        "pdf_size": 1103547,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1408928612690751672&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "1National Engineering Research Center for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; 1National Engineering Research Center for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; 2Microsoft, Beijing, China; 1National Engineering Research Center for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; 2Microsoft, Beijing, China; 2Microsoft, Beijing, China; 2Microsoft, Beijing, China",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;microsoft.com;ustc.edu.cn;microsoft.com;microsoft.com;microsoft.com",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;microsoft.com;ustc.edu.cn;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;1;1;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft",
        "aff_unique_dep": "National Engineering Research Center for Speech and Language Information Processing;Microsoft",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com",
        "aff_unique_abbr": "USTC;MSFT",
        "aff_campus_unique_index": "0;0;1;0;1;1;1",
        "aff_campus_unique": "Hefei;Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.142",
        "title": "Hey AI, Can You Solve Complex Tasks by Talking to Agents?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Training giant models from scratch for each complex task is resource- and data-inefficient. To help develop models that can leverage existing systems, we propose a new challenge: Learning to solve complex tasks by communicating with existing agents (or models) in natural language. We design a synthetic benchmark, CommaQA, with three complex reasoning tasks (explicit, implicit, numeric) designed to be solved by communicating with existing QA agents. For instance, using text and table QA agents to answer questions such as \u201cWho had the longest javelin throw from USA?\u201d. We show that black-box models struggle to learn this task from scratch (accuracy under 50%) even with access to each agent\u2019s knowledge and gold facts supervision. In contrast, models that learn to communicate with agents outperform black-box models, reaching scores of 100% when given gold decomposition supervision. However, we show that the challenge of learning to solve complex tasks by communicating with existing agents without relying on any auxiliary supervision or data still remains highly elusive. We will release CommaQA, along with a compositional generalization test split, to advance research in this direction.",
        "author": "Tushar Khot; Kyle Richardson; Daniel Khashabi; Ashish Sabharwal",
        "authorids": "/t/tushar-khot/; /k/kyle-richardson/; /d/daniel-khashabi/; /a/ashish-sabharwal/",
        "bibtex": "@inproceedings{khot-etal-2022-hey,\n    title = \"Hey {AI}, Can You Solve Complex Tasks by Talking to Agents?\",\n    author = \"Khot, Tushar  and\n      Richardson, Kyle  and\n      Khashabi, Daniel  and\n      Sabharwal, Ashish\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.142/\",\n    doi = \"10.18653/v1/2022.findings-acl.142\",\n    pages = \"1808--1823\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.142.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.142/",
        "pdf_size": 949396,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13401047242494563265&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 6,
        "aff": "Allen Institute for AI, Seattle, WA, U.S.A.; Allen Institute for AI, Seattle, WA, U.S.A.; Allen Institute for AI, Seattle, WA, U.S.A.; Allen Institute for AI, Seattle, WA, U.S.A.",
        "aff_domain": "allenai.org;allenai.org;allenai.org;allenai.org",
        "email": "allenai.org;allenai.org;allenai.org;allenai.org",
        "github": "https://github.com/allenai/commaqa",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Allen Institute for AI",
        "aff_unique_dep": "",
        "aff_unique_url": "https://allenai.org",
        "aff_unique_abbr": "AI2",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.202",
        "title": "HiCLRE: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Distant supervision assumes that any sentence containing the same entity pairs reflects identical relationships. Previous works of distantly supervised relation extraction (DSRE) task generally focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels. In this paper, we propose a hierarchical contrastive learning Framework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy sentences, which integrate the global structural information and local fine-grained interaction. Specifically, we propose a three-level hierarchical learning framework to interact with cross levels, generating the de-noising context-aware representations via adapting the existing multi-head self-attention, named Multi-Granularity Recontextualization. Meanwhile, pseudo positive samples are also provided in the specific level for contrastive learning via a dynamic gradient-based data augmentation strategy, named Dynamic Gradient Adversarial Perturbation. Experiments demonstrate that HiCLRE significantly outperforms strong baselines in various mainstream DSRE datasets.",
        "author": "Dongyang Li; Taolin Zhang; Nan Hu; Chengyu Wang; Xiaofeng He",
        "authorids": "/d/dongyang-li/; /t/taolin-zhang/; /n/nan-hu/; /c/chengyu-wang/; /x/xiaofeng-he/",
        "bibtex": "@inproceedings{li-etal-2022-hiclre,\n    title = \"{H}i{CLRE}: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction\",\n    author = \"Li, Dongyang  and\n      Zhang, Taolin  and\n      Hu, Nan  and\n      Wang, Chengyu  and\n      He, Xiaofeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.202/\",\n    doi = \"10.18653/v1/2022.findings-acl.202\",\n    pages = \"2567--2578\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.202.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.202/",
        "pdf_size": 1067720,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8104425630078276983&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Technology, East China Normal University + Shanghai Key Laboratory of Trsustworthy Computing; Alibaba Group + School of Software Engineering, East China Normal University; School of Computer Science and Technology, East China Normal University; Alibaba Group; School of Computer Science and Technology, East China Normal University + Shanghai Research Institute for Intelligent Autonomous Systems",
        "aff_domain": "gmail.com;gmail.com;gmail.com;alibaba-inc.com;cs.ecnu.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;alibaba-inc.com;cs.ecnu.edu.cn",
        "github": "https://github.com/MatNLP/HiCLRE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2+0;0;2;0+3",
        "aff_unique_norm": "East China Normal University;Shanghai Key Laboratory of Trustworthy Computing;Alibaba Group;Shanghai Research Institute for Intelligent Autonomous Systems",
        "aff_unique_dep": "School of Computer Science and Technology;Key Laboratory of Trustworthy Computing;;",
        "aff_unique_url": "http://www.ecnu.edu.cn;;https://www.alibaba.com;",
        "aff_unique_abbr": "ECNU;;Alibaba;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.102",
        "title": "HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially. Using various experimental settings on three datasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected. It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains. The ablation study demonstrates that the hierarchical position information is the main contributor to our model\u2019s SOTA performance.",
        "author": "Qian Ruan; Malte Ostendorff; Georg Rehm",
        "authorids": "/q/qian-ruan/; /m/malte-ostendorff/; /g/georg-rehm/",
        "bibtex": "@inproceedings{ruan-etal-2022-histruct,\n    title = \"{H}i{S}truct+: Improving Extractive Text Summarization with Hierarchical Structure Information\",\n    author = \"Ruan, Qian  and\n      Ostendorff, Malte  and\n      Rehm, Georg\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.102/\",\n    doi = \"10.18653/v1/2022.findings-acl.102\",\n    pages = \"1292--1308\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.102.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.102/",
        "pdf_size": 1507806,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7097872271181904600&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "DFKI GmbH, Germany; DFKI GmbH, Germany; DFKI GmbH, Germany",
        "aff_domain": "gmail.com;dfki.de;dfki.de",
        "email": "gmail.com;dfki.de;dfki.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.dfki.de",
        "aff_unique_abbr": "DFKI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.78",
        "title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge numerical reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics. We present a new dataset, HiTab, to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) QA pairs are not proposed by annotators from scratch, but are revised from real and meaningful sentences authored by analysts. (3) to reveal complex numerical reasoning in statistical reports, we provide fine-grained annotations of quantity and entity alignment. Experiments suggest that this HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research. Targeting hierarchical structure, we devise a hierarchy-aware logical form for symbolic reasoning over tables, which shows high effectiveness. Targeting table reasoning, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG, and largely reduce spurious predictions in QA and produce better descriptions in NLG.",
        "author": "Zhoujun Cheng; Haoyu Dong; Zhiruo Wang; Ran Jia; Jiaqi Guo; Yan Gao; Shi Han; Jian-Guang Lou; Dongmei Zhang",
        "authorids": "/z/zhoujun-cheng/; /h/haoyu-dong/; /z/zhiruo-wang/; /r/ran-jia/; /j/jiaqi-guo/; /y/yan-gao/; /s/shi-han/; /j/jian-guang-lou/; /d/dongmei-zhang/",
        "bibtex": "@inproceedings{cheng-etal-2022-hitab,\n    title = \"{H}i{T}ab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\",\n    author = \"Cheng, Zhoujun  and\n      Dong, Haoyu  and\n      Wang, Zhiruo  and\n      Jia, Ran  and\n      Guo, Jiaqi  and\n      Gao, Yan  and\n      Han, Shi  and\n      Lou, Jian-Guang  and\n      Zhang, Dongmei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.78/\",\n    doi = \"10.18653/v1/2022.acl-long.78\",\n    pages = \"1094--1110\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.78.pdf",
        "site": "https://aclanthology.org/2022.acl-long.78/",
        "pdf_size": 2197862,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4973884067470252930&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Shanghai Jiao Tong University; Microsoft Research Asia+Shanghai Jiao Tong University; Carnegie Mellon University; Xi\u2019an Jiaotong University; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia",
        "aff_domain": "sjtu.edu.cn;microsoft.com;cs.cmu.edu;microsoft.com;stu.xjtu.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "sjtu.edu.cn;microsoft.com;cs.cmu.edu;microsoft.com;stu.xjtu.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/microsoft/HiTab",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1+0;2;3;1;1;1;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Microsoft;Carnegie Mellon University;Xi'an Jiao Tong University",
        "aff_unique_dep": ";Research;;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.microsoft.com/en-us/research/group/asia;https://www.cmu.edu;https://www.xjtu.edu.cn",
        "aff_unique_abbr": "SJTU;MSR Asia;CMU;XJTU",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0+0;1;0;0;0;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-short.37",
        "title": "Hierarchical Curriculum Learning for AMR Parsing",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Abstract Meaning Representation (AMR) parsing aims to translate sentences to semantic representation with a hierarchical structure, and is recently empowered by pretrained sequence-to-sequence models. However, there exists a gap between their flat training objective (i.e., equally treats all output tokens) and the hierarchical AMR structure, which limits the model generalization. To bridge this gap, we propose a Hierarchical Curriculum Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula (IC). SC switches progressively from core to detail AMR semantic elements while IC transits from structure-simple to -complex AMR instances during training. Through these two warming-up processes, HCL reduces the difficulty of learning complex structures, thus the flat model can better adapt to the AMR hierarchy. Extensive experiments on AMR2.0, AMR3.0, structure-complex and out-of-distribution situations verify the effectiveness of HCL.",
        "author": "Peiyi Wang; Liang Chen; Tianyu Liu; Damai Dai; Yunbo Cao; Baobao Chang; Zhifang Sui",
        "authorids": "/p/peiyi-wang/; /l/liang-chen/; /t/tianyu-liu/; /d/damai-dai/; /y/yunbo-cao/; /b/baobao-chang/; /z/zhifang-sui/",
        "bibtex": "@inproceedings{wang-etal-2022-hierarchical,\n    title = \"Hierarchical Curriculum Learning for {AMR} Parsing\",\n    author = \"Wang, Peiyi  and\n      Chen, Liang  and\n      Liu, Tianyu  and\n      Dai, Damai  and\n      Cao, Yunbo  and\n      Chang, Baobao  and\n      Sui, Zhifang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.37/\",\n    doi = \"10.18653/v1/2022.acl-short.37\",\n    pages = \"333--339\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.37.pdf",
        "site": "https://aclanthology.org/2022.acl-short.37/",
        "pdf_size": 708529,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17239774558115687846&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Tencent Cloud Xiaowei; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Tencent Cloud Xiaowei; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China",
        "aff_domain": "gmail.com;outlook.com;tencent.com;tencent.com;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "gmail.com;outlook.com;tencent.com;tencent.com;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;1;0;0",
        "aff_unique_norm": "Peking University;Tencent",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics;Tencent Cloud Xiaowei",
        "aff_unique_url": "http://www.pku.edu.cn;https://cloud.tencent.com",
        "aff_unique_abbr": "PKU;Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.57",
        "title": "Hierarchical Inductive Transfer for Continual Dialogue Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pre-trained models have achieved excellent performance on the dialogue task. However, for the continual increase of online chit-chat scenarios, directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks. In this work, we propose a hierarchical inductive transfer framework to learn and deploy the dialogue skills continually and efficiently. First, we introduce the adapter module into pre-trained models for learning new dialogue tasks. As the only trainable module, it is beneficial for the dialogue system on the embedded devices to acquire new dialogue skills with negligible additional parameters. Then, for alleviating knowledge interference between tasks yet benefiting the regularization between them, we further design hierarchical inductive transfer that enables new tasks to use general knowledge in the base adapter without being misled by diverse knowledge in task-specific adapters. Empirical evaluation and analysis indicate that our framework obtains comparable performance under deployment-friendly model capacity.",
        "author": "Shaoxiong Feng; Xuancheng Ren; Kan Li; Xu Sun",
        "authorids": "/s/shaoxiong-feng/; /x/xuancheng-ren/; /k/kan-li/; /x/xu-sun/",
        "bibtex": "@inproceedings{feng-etal-2022-hierarchical,\n    title = \"Hierarchical Inductive Transfer for Continual Dialogue Learning\",\n    author = \"Feng, Shaoxiong  and\n      Ren, Xuancheng  and\n      Li, Kan  and\n      Sun, Xu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.57/\",\n    doi = \"10.18653/v1/2022.findings-acl.57\",\n    pages = \"693--699\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.57.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.57/",
        "pdf_size": 1605417,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5844410807569860857&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Beijing Institute of Technology+University of Technology Sydney; MOE Key Laboratory of Computational Linguistics, School of CS, Peking University; Beijing Institute of Technology; MOE Key Laboratory of Computational Linguistics, School of CS, Peking University+Beijing Academy of Artificial Intelligence",
        "aff_domain": "bit.edu.cn;pku.edu.cn;bit.edu.cn;pku.edu.cn",
        "email": "bit.edu.cn;pku.edu.cn;bit.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;0;2+3",
        "aff_unique_norm": "Beijing Institute of Technology;University of Technology Sydney;Peking University;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": ";;School of Computer Science;",
        "aff_unique_url": "http://www.bit.edu.cn/;https://www.uts.edu.au;http://www.pku.edu.cn;https://www.baaic.cn",
        "aff_unique_abbr": "BIT;UTS;PKU;BAAI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;0+0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.findings-acl.170",
        "title": "Hierarchical Recurrent Aggregative Generation for Few-Shot NLG",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large pretrained models enable transfer learning to low-resource domains for language generation tasks. However, previous end-to-end approaches do not account for the fact that some generation sub-tasks, specifically aggregation and lexicalisation, can benefit from transfer learning in different extents. To exploit these varying potentials for transfer learning, we propose a new hierarchical approach for few-shot and zero-shot generation. Our approach consists of a three-moduled jointly trained architecture: the first module independently lexicalises the distinct units of information in the input as sentence sub-units (e.g. phrases), the second module recurrently aggregates these sub-units to generate a unified intermediate output, while the third module subsequently post-edits it to generate a coherent and fluent final text. We perform extensive empirical analysis and ablation studies on few-shot and zero-shot settings across 4 datasets. Automatic and human evaluation shows that the proposed hierarchical approach is consistently capable of achieving state-of-the-art results when compared to previous work.",
        "author": "Giulio Zhou; Gerasimos Lampouras; Ignacio Iacobacci",
        "authorids": "/g/giulio-zhou/; /g/gerasimos-lampouras/; /i/ignacio-iacobacci/",
        "bibtex": "@inproceedings{zhou-etal-2022-hierarchical,\n    title = \"Hierarchical Recurrent Aggregative Generation for Few-Shot {NLG}\",\n    author = \"Zhou, Giulio  and\n      Lampouras, Gerasimos  and\n      Iacobacci, Ignacio\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.170/\",\n    doi = \"10.18653/v1/2022.findings-acl.170\",\n    pages = \"2167--2181\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.170.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.170/",
        "pdf_size": 481414,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:78fmO8fSzOIJ:scholar.google.com/&scioq=Hierarchical+Recurrent+Aggregative+Generation+for+Few-Shot+NLG&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Huawei Noah\u2019s Ark Lab, London, UK; Huawei Noah\u2019s Ark Lab, London, UK; Huawei Noah\u2019s Ark Lab, London, UK",
        "aff_domain": "huawei.com;huawei.com;huawei.com",
        "email": "huawei.com;huawei.com;huawei.com",
        "github": "https://github.com/huawei-noah/noah-research/NLP/",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Huawei",
        "aff_unique_dep": "Huawei Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.huawei.com/en/ai",
        "aff_unique_abbr": "HNA Lab",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.178",
        "title": "Hierarchical Sketch Induction for Paraphrase Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a generative model of paraphrase generation, that encourages syntactic diversity by conditioning on an explicit syntactic sketch. We introduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE), a method for learning decompositions of dense encodings as a sequence of discrete latent variables that make iterative refinements of increasing granularity. This hierarchy of codes is learned through end-to-end training, and represents fine-to-coarse grained information about the input. We use HRQ-VAE to encode the syntactic form of an input sentence as a path through the hierarchy, allowing us to more easily predict syntactic sketches at test time. Extensive experiments, including a human evaluation, confirm that HRQ-VAE learns a hierarchical representation of the input space, and generates paraphrases of higher quality than previous systems.",
        "author": "Tom Hosking; Hao Tang; Mirella Lapata",
        "authorids": "/t/tom-hosking/; /h/hao-tang/; /m/mirella-lapata/",
        "bibtex": "@inproceedings{hosking-etal-2022-hierarchical,\n    title = \"Hierarchical Sketch Induction for Paraphrase Generation\",\n    author = \"Hosking, Tom  and\n      Tang, Hao  and\n      Lapata, Mirella\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.178/\",\n    doi = \"10.18653/v1/2022.acl-long.178\",\n    pages = \"2489--2501\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.178.pdf",
        "site": "https://aclanthology.org/2022.acl-long.178/",
        "pdf_size": 2662013,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11556671086367358521&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 7,
        "aff": "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.243",
        "title": "How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Cross-lingual Entity Typing (CLET) aims at improving the quality of entity type prediction by transferring semantic knowledge learned from rich-resourced languages to low-resourced languages. In this paper, by utilizing multilingual transfer learning via the mixture-of-experts approach, our model dynamically capture the relationship between target language and each source language, and effectively generalize to predict types of unseen entities in new languages. Extensive experiments on multi-lingual datasets show that our method significantly outperforms multiple baselines and can robustly handle negative transfer. We questioned the relationship between language similarity and the performance of CLET. A series of experiments refute the commonsense that the more source the better, and suggest the Similarity Hypothesis for CLET.",
        "author": "Hailong Jin; Tiansi Dong; Lei Hou; Juanzi Li; Hui Chen; Zelin Dai; Qu Yincen",
        "authorids": "/h/hailong-jin/; /t/tiansi-dong/; /l/lei-hou/; /j/juanzi-li/; /h/hui-chen/; /z/zelin-dai/; /q/qu-yincen/",
        "bibtex": "@inproceedings{jin-etal-2022-cross,\n    title = \"How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?\",\n    author = \"Jin, Hailong  and\n      Dong, Tiansi  and\n      Hou, Lei  and\n      Li, Juanzi  and\n      Chen, Hui  and\n      Dai, Zelin  and\n      Yincen, Qu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.243/\",\n    doi = \"10.18653/v1/2022.findings-acl.243\",\n    pages = \"3071--3081\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.243.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.243/",
        "pdf_size": 2948350,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=924705174714951467&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Technology, BNRist+KIRC, Institute for Arti\ufb01cial Intelligence, Tsinghua University, Beijing 100084, China; B-IT, University of Bonn, Germany; Department of Computer Science and Technology, BNRist+KIRC, Institute for Arti\ufb01cial Intelligence, Tsinghua University, Beijing 100084, China; Department of Computer Science and Technology, BNRist+KIRC, Institute for Arti\ufb01cial Intelligence, Tsinghua University, Beijing 100084, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China",
        "aff_domain": "mail.tsinghua.edu.cn; ;mail.tsinghua.edu.cn; ; ; ; ",
        "email": "mail.tsinghua.edu.cn; ;mail.tsinghua.edu.cn; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;0+1;0+1;3;3;3",
        "aff_unique_norm": "BNRist;Tsinghua University;University of Bonn;Alibaba Group",
        "aff_unique_dep": "Department of Computer Science and Technology;Institute for Arti\ufb01cial Intelligence;B-IT;",
        "aff_unique_url": ";https://www.tsinghua.edu.cn;https://www.uni-bonn.de;https://www.alibaba.com",
        "aff_unique_abbr": ";THU;Uni Bonn;Alibaba",
        "aff_campus_unique_index": "1;1;1;2;2;2",
        "aff_campus_unique": ";Beijing;Hangzhou",
        "aff_country_unique_index": "1;2;1;1;1;1;1",
        "aff_country_unique": ";China;Germany"
    },
    {
        "id": "2022.acl-short.54",
        "title": "How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "This work addresses the question of the localization of syntactic information encoded in the transformers representations. We tackle this question from two perspectives, considering the object-past participle agreement in French, by identifying, first, in which part of the sentence and, second, in which part of the representation the syntactic information is encoded. The results of our experiments, using probing, causal analysis and feature selection method, show that syntactic information is encoded locally in a way consistent with the French grammar.",
        "author": "Bingzhi Li; Guillaume Wisniewski; Benoit Crabb\u00e9",
        "authorids": "/b/bingzhi-li/; /g/guillaume-wisniewski/; /b/benoit-crabbe/",
        "bibtex": "@inproceedings{li-etal-2022-distributed,\n    title = \"How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks\",\n    author = \"Li, Bingzhi  and\n      Wisniewski, Guillaume  and\n      Crabb{\\'e}, Benoit\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.54/\",\n    doi = \"10.18653/v1/2022.acl-short.54\",\n    pages = \"501--507\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.54.pdf",
        "site": "https://aclanthology.org/2022.acl-short.54/",
        "pdf_size": 388874,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9449034346054702547&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Universit\u00e9 de Paris, LLF, CNRS; Universit\u00e9 de Paris, LLF, CNRS; Universit\u00e9 de Paris, LLF, CNRS",
        "aff_domain": "etu.u-paris.fr;u-paris.fr;u-paris.fr",
        "email": "etu.u-paris.fr;u-paris.fr;u-paris.fr",
        "github": "",
        "project": "https://gitlab.huma-num.fr/bli/syntactic-info-distribution501",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universit\u00e9 de Paris",
        "aff_unique_dep": "LLF",
        "aff_unique_url": "https://www.universitedeparis.fr",
        "aff_unique_abbr": "UP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.acl-long.531",
        "title": "How Do Seq2Seq Models Perform on End-to-End Data-to-Text Generation?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With the rapid development of deep learning, Seq2Seq paradigm has become prevalent for end-to-end data-to-text generation, and the BLEU scores have been increasing in recent years. However, it is widely recognized that there is still a gap between the quality of the texts generated by models and the texts written by human. In order to better understand the ability of Seq2Seq models, evaluate their performance and analyze the results, we choose to use Multidimensional Quality Metric(MQM) to evaluate several representative Seq2Seq models on end-to-end data-to-text generation. We annotate the outputs of five models on four datasets with eight error types and find that 1) copy mechanism is helpful for the improvement in Omission and Inaccuracy Extrinsic errors but it increases other types of errors such as Addition; 2) pre-training techniques are highly effective, and pre-training strategy and model size are very significant; 3) the structure of the dataset also influences the model\u2019s performance greatly; 4) some specific types of errors are generally challenging for seq2seq models.",
        "author": "Xunjian Yin; Xiaojun Wan",
        "authorids": "/x/xunjian-yin/; /x/xiaojun-wan/",
        "bibtex": "@inproceedings{yin-wan-2022-seq2seq,\n    title = \"How Do {S}eq2{S}eq Models Perform on End-to-End Data-to-Text Generation?\",\n    author = \"Yin, Xunjian  and\n      Wan, Xiaojun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.531/\",\n    doi = \"10.18653/v1/2022.acl-long.531\",\n    pages = \"7701--7710\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.531.pdf",
        "site": "https://aclanthology.org/2022.acl-long.531/",
        "pdf_size": 214743,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5857946872113891429&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 2,
        "aff": "Wangxuan Institute of Computer Technology, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University; Wangxuan Institute of Computer Technology, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.249",
        "title": "How Do We Answer Complex Questions: Discourse Structure of Long-form Answers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Long-form answers, consisting of multiple sentences, can provide nuanced and comprehensive answers to a broader set of questions. To better understand this complex and understudied task, we study the functional structure of long-form answers collected from three datasets, ELI5, WebGPT and Natural Questions. Our main goal is to understand how humans organize information to craft complex answers. We develop an ontology of six sentence-level functional roles for long-form answers, and annotate 3.9k sentences in 640 answer paragraphs. Different answer collection methods manifest in different discourse structures. We further analyze model-generated answers \u2013 finding that annotators agree less with each other when annotating model-generated answers compared to annotating human-written answers. Our annotated data enables training a strong classifier that can be used for automatic analysis. We hope our work can inspire future research on discourse-level modeling and evaluation of long-form QA systems.",
        "author": "Fangyuan Xu; Junyi Jessy Li; Eunsol Choi",
        "authorids": "/f/fangyuan-xu/; /j/junyi-jessy-li/; /e/eunsol-choi/",
        "bibtex": "@inproceedings{xu-etal-2022-answer,\n    title = \"How Do We Answer Complex Questions: Discourse Structure of Long-form Answers\",\n    author = \"Xu, Fangyuan  and\n      Li, Junyi Jessy  and\n      Choi, Eunsol\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.249/\",\n    doi = \"10.18653/v1/2022.acl-long.249\",\n    pages = \"3556--3572\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.249.pdf",
        "site": "https://aclanthology.org/2022.acl-long.249/",
        "pdf_size": 1596042,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11426597683155029275&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science; Department of Linguistics; Department of Computer Science",
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu",
        "email": "utexas.edu;utexas.edu;utexas.edu",
        "github": "https://github.com/utcsnlp/lfqa_discourse",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Unknown Institution;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science;Department of Linguistics",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.findings-acl.136",
        "title": "How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs\u2019 ability to fill in the missing factual words in cloze-style prompts such as \u201dDante was born in [MASK].\u201d However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.",
        "author": "Shaobo Li; Xiaoguang Li; Lifeng Shang; Zhenhua Dong; Chengjie Sun; Bingquan Liu; Zhenzhou Ji; Xin Jiang; Qun Liu",
        "authorids": "/s/shaobo-li/; /x/xiaoguang-li/; /l/lifeng-shang/; /z/zhenhua-dong/; /c/cheng-jie-sun/; /b/bingquan-liu/; /z/zhenzhou-ji/; /x/xin-jiang/; /q/qun-liu/",
        "bibtex": "@inproceedings{li-etal-2022-pre,\n    title = \"How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis\",\n    author = \"Li, Shaobo  and\n      Li, Xiaoguang  and\n      Shang, Lifeng  and\n      Dong, Zhenhua  and\n      Sun, Chengjie  and\n      Liu, Bingquan  and\n      Ji, Zhenzhou  and\n      Jiang, Xin  and\n      Liu, Qun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.136/\",\n    doi = \"10.18653/v1/2022.findings-acl.136\",\n    pages = \"1720--1732\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.136.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.136/",
        "pdf_size": 469844,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16114751458012436154&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Harbin Institute of Technology; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "insun.hit.edu.cn;huawei.com;huawei.com;huawei.com;hit.edu.cn;hit.edu.cn;hit.edu.cn;huawei.com;huawei.com",
        "email": "insun.hit.edu.cn;huawei.com;huawei.com;huawei.com;hit.edu.cn;hit.edu.cn;hit.edu.cn;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;0;0;0;1;1",
        "aff_unique_norm": "Harbin Institute of Technology;Huawei",
        "aff_unique_dep": ";Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.huawei.com",
        "aff_unique_abbr": "HIT;Huawei",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.108",
        "title": "How can NLP Help Revitalize Endangered Languages? A Case Study and Roadmap for the Cherokee Language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "More than 43% of the languages spoken in the world are endangered, and language loss currently occurs at an accelerated rate because of globalization and neocolonialism. Saving and revitalizing endangered languages has become very important for maintaining the cultural diversity on our planet. In this work, we focus on discussing how NLP can help revitalize endangered languages. We first suggest three principles that may help NLP practitioners to foster mutual understanding and collaboration with language communities, and we discuss three ways in which NLP can potentially assist in language education. We then take Cherokee, a severely-endangered Native American language, as a case study. After reviewing the language\u2019s history, linguistic features, and existing resources, we (in collaboration with Cherokee community members) arrive at a few meaningful ways NLP practitioners can collaborate with community partners. We suggest two approaches to enrich the Cherokee language\u2019s resources with machine-in-the-loop processing, and discuss several NLP tools that people from the Cherokee community have shown interest in. We hope that our work serves not only to inform the NLP community about Cherokee, but also to provide inspiration for future work on endangered languages in general.",
        "author": "Shiyue Zhang; Ben Frey; Mohit Bansal",
        "authorids": "/s/shiyue-zhang/; /b/ben-frey/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{zhang-etal-2022-nlp,\n    title = \"How can {NLP} Help Revitalize Endangered Languages? A Case Study and Roadmap for the {C}herokee Language\",\n    author = \"Zhang, Shiyue  and\n      Frey, Ben  and\n      Bansal, Mohit\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.108/\",\n    doi = \"10.18653/v1/2022.acl-long.108\",\n    pages = \"1529--1541\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.108.pdf",
        "site": "https://aclanthology.org/2022.acl-long.108/",
        "pdf_size": 242814,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1303445967880799371&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "UNCChapelHill; UNCChapelHill; UNCChapelHill",
        "aff_domain": "cs.unc.edu;email.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;email.unc.edu;cs.unc.edu",
        "github": "https://github.com/ZhangShiyue/RevitalizeCherokee",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.16",
        "title": "How does the pre-training objective affect what large language models learn about linguistic properties?",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.",
        "author": "Ahmed Alajrami; Nikolaos Aletras",
        "authorids": "/a/ahmed-alajrami/; /n/nikolaos-aletras/",
        "bibtex": "@inproceedings{alajrami-aletras-2022-pre,\n    title = \"How does the pre-training objective affect what large language models learn about linguistic properties?\",\n    author = \"Alajrami, Ahmed  and\n      Aletras, Nikolaos\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.16/\",\n    doi = \"10.18653/v1/2022.acl-short.16\",\n    pages = \"131--147\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.16.pdf",
        "site": "https://aclanthology.org/2022.acl-short.16/",
        "pdf_size": 226420,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8519046425748394829&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Sheffield, UK; Department of Computer Science, University of Sheffield, UK",
        "aff_domain": "sheffield.ac.uk;sheffield.ac.uk",
        "email": "sheffield.ac.uk;sheffield.ac.uk",
        "github": "https://github.com/aajrami/acl2022-pre-training-objectives-probing",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Sheffield",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Sheffield",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-short.87",
        "title": "How reparametrization trick broke differentially-private text representation learning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "As privacy gains traction in the NLP community, researchers have started adopting various approaches to privacy-preserving methods. One of the favorite privacy frameworks, differential privacy (DP), is perhaps the most compelling thanks to its fundamental theoretical guarantees. Despite the apparent simplicity of the general concept of differential privacy, it seems non-trivial to get it right when applying it to NLP. In this short paper, we formally analyze several recent NLP papers proposing text representation learning using DPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and reveal their false claims of being differentially private. Furthermore, we also show a simple yet general empirical sanity check to determine whether a given implementation of a DP mechanism almost certainly violates the privacy loss guarantees. Our main goal is to raise awareness and help the community understand potential pitfalls of applying differential privacy to text representation learning.",
        "author": "Ivan Habernal",
        "authorids": "/i/ivan-habernal/",
        "bibtex": "@inproceedings{habernal-2022-reparametrization,\n    title = \"How reparametrization trick broke differentially-private text representation learning\",\n    author = \"Habernal, Ivan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.87/\",\n    doi = \"10.18653/v1/2022.acl-short.87\",\n    pages = \"771--777\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.87.pdf",
        "site": "https://aclanthology.org/2022.acl-short.87/",
        "pdf_size": 751622,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17394484597382803247&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Trustworthy Human Language Technologies, Department of Computer Science, Technical University of Darmstadt",
        "aff_domain": "tu-darmstadt.de",
        "email": "tu-darmstadt.de",
        "github": "",
        "project": "www.trusthlt.org",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Technical University of Darmstadt",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.tu-darmstadt.de",
        "aff_unique_abbr": "TUD",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.394",
        "title": "Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In recent years, machine learning models have rapidly become better at generating clinical consultation notes; yet, there is little work on how to properly evaluate the generated consultation notes to understand the impact they may have on both the clinician using them and the patient\u2019s clinical safety. To address this we present an extensive human evaluation study of consultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii) write their own notes, (iii) post-edit a number of automatically generated notes, and (iv) extract all the errors, both quantitative and qualitative. We then carry out a correlation study with 18 automatic quality metrics and the human judgements. We find that a simple, character-based Levenshtein distance metric performs on par if not better than common model-based metrics like BertScore. All our findings and annotations are open-sourced.",
        "author": "Francesco Moramarco; Alex Papadopoulos Korfiatis; Mark Perera; Damir Juric; Jack Flann; Ehud Reiter; Anya Belz; Aleksandar Savkov",
        "authorids": "/f/francesco-moramarco/; /a/alex-papadopoulos-korfiatis/; /m/mark-perera/; /d/damir-juric/; /j/jack-flann/; /e/ehud-reiter/; /a/anja-belz/; /a/aleksandar-savkov/",
        "bibtex": "@inproceedings{moramarco-etal-2022-human,\n    title = \"Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation\",\n    author = \"Moramarco, Francesco  and\n      Papadopoulos Korfiatis, Alex  and\n      Perera, Mark  and\n      Juric, Damir  and\n      Flann, Jack  and\n      Reiter, Ehud  and\n      Belz, Anya  and\n      Savkov, Aleksandar\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.394/\",\n    doi = \"10.18653/v1/2022.acl-long.394\",\n    pages = \"5739--5754\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.394.pdf",
        "site": "https://aclanthology.org/2022.acl-long.394/",
        "pdf_size": 2848837,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2323052942787864165&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Babylon+University of Aberdeen; Babylon+University of Aberdeen; Babylon+University of Aberdeen; Babylon+University of Aberdeen; Babylon+University of Aberdeen; University of Aberdeen; University of Aberdeen; Babylon+University of Aberdeen",
        "aff_domain": "babylonhealth.co.uk;babylonhealth.co.uk;babylonhealth.co.uk;babylonhealth.co.uk;babylonhealth.co.uk;abdn.ac.uk;abdn.ac.uk;babylonhealth.co.uk",
        "email": "babylonhealth.co.uk;babylonhealth.co.uk;babylonhealth.co.uk;babylonhealth.co.uk;babylonhealth.co.uk;abdn.ac.uk;abdn.ac.uk;babylonhealth.co.uk",
        "github": "https://github.com/babylonhealth/primock575739",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1;1;1;0+1",
        "aff_unique_norm": "Babylon;University of Aberdeen",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.abdn.ac.uk",
        "aff_unique_abbr": ";Aberdeen",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0+1;0+1;0+1;1;1;0+1",
        "aff_country_unique": "Iraq;United Kingdom"
    },
    {
        "id": "2022.findings-acl.52",
        "title": "Human Language Modeling",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Natural language is generated by people, yet traditional language modeling views words or documents as if generated independently. Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. We introduce, HaRT, a large-scale transformer model for solving HuLM, pre-trained on approximately 100,000 social media users, and demonstrate it\u2019s effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels. Results on all tasks meet or surpass the current state-of-the-art.",
        "author": "Nikita Soni; Matthew Matero; Niranjan Balasubramanian; H. Andrew Schwartz",
        "authorids": "/n/nikita-soni/; /m/matthew-matero/; /n/niranjan-balasubramanian/; /h/h-andrew-schwartz/",
        "bibtex": "@inproceedings{soni-etal-2022-human,\n    title = \"Human Language Modeling\",\n    author = \"Soni, Nikita  and\n      Matero, Matthew  and\n      Balasubramanian, Niranjan  and\n      Schwartz, H. Andrew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.52/\",\n    doi = \"10.18653/v1/2022.findings-acl.52\",\n    pages = \"622--636\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.52.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.52/",
        "pdf_size": 504957,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1546017191352974116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University",
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "github": "https://github.com/humanlab/HaRT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stony Brook",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.41",
        "title": "HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "A pressing challenge in current dialogue systems is to successfully converse with users on topics with information distributed across different modalities. Previous work in multiturn dialogue systems has primarily focused on either text or table information. In more realistic scenarios, having a joint understanding of both is critical as knowledge is typically distributed over both unstructured and structured forms. We present a new dialogue dataset, HybriDialogue, which consists of crowdsourced natural conversations grounded on both Wikipedia text and tables. The conversations are created through the decomposition of complex multihop questions into simple, realistic multiturn dialogue interactions. We propose retrieval, system state tracking, and dialogue response generation tasks for our dataset and conduct baseline experiments for each. Our results show that there is still ample opportunity for improvement, demonstrating the importance of building stronger dialogue systems that can reason over the complex setting of informationseeking dialogue grounded on tables and text.",
        "author": "Kai Nakamura; Sharon Levy; Yi-Lin Tuan; Wenhu Chen; William Yang Wang",
        "authorids": "/k/kai-nakamura/; /s/sharon-levy/; /y/yi-lin-tuan/; /w/wenhu-chen/; /w/william-yang-wang/",
        "bibtex": "@inproceedings{nakamura-etal-2022-hybridialogue,\n    title = \"{H}ybri{D}ialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data\",\n    author = \"Nakamura, Kai  and\n      Levy, Sharon  and\n      Tuan, Yi-Lin  and\n      Chen, Wenhu  and\n      Wang, William Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.41/\",\n    doi = \"10.18653/v1/2022.findings-acl.41\",\n    pages = \"481--492\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.41.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.41/",
        "pdf_size": 2339000,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12097023346763623561&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "California Institute of Technology; University of California, Santa Barbara; University of California, Santa Barbara; University of Waterloo, Vector Institute; University of California, Santa Barbara",
        "aff_domain": "gmail.com;cs.ucsb.edu;cs.ucsb.edu;uwaterloo.ca;cs.ucsb.edu",
        "email": "gmail.com;cs.ucsb.edu;cs.ucsb.edu;uwaterloo.ca;cs.ucsb.edu",
        "github": "https://github.com/entitize/HybridDialogue481",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "California Institute of Technology;University of California, Santa Barbara;University of Waterloo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.caltech.edu;https://www.ucsb.edu;https://uwaterloo.ca",
        "aff_unique_abbr": "Caltech;UCSB;UW",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Pasadena;Santa Barbara;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2022.acl-long.136",
        "title": "Hybrid Semantics for Goal-Directed Natural Language Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We consider the problem of generating natural language given a communicative goal and a world description. We ask the question: is it possible to combine complementary meaning representations to scale a goal-directed NLG system without losing expressiveness? In particular, we consider using two meaning representations, one based on logical semantics and the other based on distributional semantics. We build upon an existing goal-directed generation system, S-STRUCT, which models sentence generation as planning in a Markov decision process. We develop a hybrid approach, which uses distributional semantics to quickly and imprecisely add the main elements of the sentence and then uses first-order logic based semantics to more slowly add the precise details. We find that our hybrid method allows S-STRUCT\u2019s generation to scale significantly better in early phases of generation and that the hybrid can often generate sentences with the same quality as S-STRUCT in substantially less time. However, we also observe and give insight into cases where the imprecision in distributional semantics leads to generation that is not as good as using pure logical semantics.",
        "author": "Connor Baumler; Soumya Ray",
        "authorids": "/c/connor-baumler/; /s/soumya-ray/",
        "bibtex": "@inproceedings{baumler-ray-2022-hybrid,\n    title = \"Hybrid Semantics for Goal-Directed Natural Language Generation\",\n    author = \"Baumler, Connor  and\n      Ray, Soumya\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.136/\",\n    doi = \"10.18653/v1/2022.acl-long.136\",\n    pages = \"1936--1946\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.136.pdf",
        "site": "https://aclanthology.org/2022.acl-long.136/",
        "pdf_size": 320843,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=382234183561582618&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Case Western Reserve University; Case Western Reserve University",
        "aff_domain": "umd.edu;case.edu",
        "email": "umd.edu;case.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Case Western Reserve University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.case.edu",
        "aff_unique_abbr": "CWRU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.29",
        "title": "Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge-based visual question answering (QA) aims to answer a question which requires visually-grounded external knowledge beyond image content itself. Answering complex questions that require multi-hop reasoning under weak supervision is considered as a challenging problem since i) no supervision is given to the reasoning process and ii) high-order semantics of multi-hop knowledge facts need to be captured. In this paper, we introduce a concept of hypergraph to encode high-level semantics of a question and a knowledge base, and to learn high-order associations between them. The proposed model, Hypergraph Transformer, constructs a question hypergraph and a query-aware knowledge hypergraph, and infers an answer by encoding inter-associations between two hypergraphs and intra-associations in both hypergraph itself. Extensive experiments on two knowledge-based visual QA and two knowledge-based textual QA demonstrate the effectiveness of our method, especially for multi-hop reasoning problem. Our source code is available at https://github.com/yujungheo/kbvqa-public.",
        "author": "Yu-Jung Heo; Eun-Sol Kim; Woo Suk Choi; Byoung-Tak Zhang",
        "authorids": "/y/yu-jung-heo/; /e/eun-sol-kim/; /w/woo-suk-choi/; /b/byoung-tak-zhang/",
        "bibtex": "@inproceedings{heo-etal-2022-hypergraph,\n    title = \"Hypergraph {T}ransformer: {W}eakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering\",\n    author = \"Heo, Yu-Jung  and\n      Kim, Eun-Sol  and\n      Choi, Woo Suk  and\n      Zhang, Byoung-Tak\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.29/\",\n    doi = \"10.18653/v1/2022.acl-long.29\",\n    pages = \"373--390\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.29.pdf",
        "site": "https://aclanthology.org/2022.acl-long.29/",
        "pdf_size": 5911791,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8043298522162017748&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Seoul National University+Surromind; Department of Computer Science, Hanyang University; Seoul National University; Seoul National University+AI Institute (AIIS), Seoul National University",
        "aff_domain": "bi.snu.ac.kr;hanyang.ac.kr;bi.snu.ac.kr;bi.snu.ac.kr",
        "email": "bi.snu.ac.kr;hanyang.ac.kr;bi.snu.ac.kr;bi.snu.ac.kr",
        "github": "https://github.com/yujungheo/kbvqa-public",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;0;0+0",
        "aff_unique_norm": "Seoul National University;Surromind;Hanyang University",
        "aff_unique_dep": ";;Department of Computer Science",
        "aff_unique_url": "https://www.snu.ac.kr;;http://www.hanyang.ac.kr",
        "aff_unique_abbr": "SNU;;HYU",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "2022.acl-long.493",
        "title": "Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "To alleviate the data scarcity problem in training question answering systems, recent works propose additional intermediate pre-training for dense passage retrieval (DPR). However, there still remains a large discrepancy between the provided upstream signals and the downstream question-passage relevance, which leads to less improvement. To bridge this gap, we propose the HyperLink-induced Pre-training (HLP), a method to pre-train the dense retriever with the text relevance induced by hyperlink-based topology within Web documents. We demonstrate that the hyperlink-based structures of dual-link and co-mention can provide effective relevance signals for large-scale pre-training that better facilitate downstream passage retrieval. We investigate the effectiveness of our approach across a wide range of open-domain QA datasets under zero-shot, few-shot, multi-hop, and out-of-domain scenarios. The experiments show our HLP outperforms the BM25 by up to 7 points as well as other pre-training methods by more than 10 points in terms of top-20 retrieval accuracy under the zero-shot scenario. Furthermore, HLP significantly outperforms other pre-training methods under the other scenarios.",
        "author": "Jiawei Zhou; Xiaoguang Li; Lifeng Shang; Lan Luo; Ke Zhan; Enrui Hu; Xinyu Zhang; Hao Jiang; Zhao Cao; Fan Yu; Xin Jiang; Qun Liu; Lei Chen",
        "authorids": "/j/jiawei-zhou/; /x/xiaoguang-li/; /l/lifeng-shang/; /l/lan-luo/; /k/ke-zhan/; /e/enrui-hu/; /x/xinyu-zhang/; /h/hao-jiang/; /z/zhao-cao/; /f/fan-yu/; /x/xin-jiang/; /q/qun-liu/; /l/lei-chen/",
        "bibtex": "@inproceedings{zhou-etal-2022-hyperlink,\n    title = \"Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering\",\n    author = \"Zhou, Jiawei  and\n      Li, Xiaoguang  and\n      Shang, Lifeng  and\n      Luo, Lan  and\n      Zhan, Ke  and\n      Hu, Enrui  and\n      Zhang, Xinyu  and\n      Jiang, Hao  and\n      Cao, Zhao  and\n      Yu, Fan  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Chen, Lei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.493/\",\n    doi = \"10.18653/v1/2022.acl-long.493\",\n    pages = \"7135--7146\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.493.pdf",
        "site": "https://aclanthology.org/2022.acl-long.493/",
        "pdf_size": 5393963,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12428777167627235591&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The Hong Kong University of Science and Technology; Huawei Noah\u2019s Ark Lab; Distributed and Parallel Software Lab, Huawei; Distributed and Parallel Software Lab, Huawei; Distributed and Parallel Software Lab, Huawei; Distributed and Parallel Software Lab, Huawei; Distributed and Parallel Software Lab, Huawei; Distributed and Parallel Software Lab, Huawei; Distributed and Parallel Software Lab, Huawei; Distributed and Parallel Software Lab, Huawei; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; The Hong Kong University of Science and Technology",
        "aff_domain": "ust.hk;ust.hk;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "email": "ust.hk;ust.hk;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "github": "https://github.com/jzhoubu/HLP",
        "project": "",
        "author_num": 13,
        "aff_unique_index": "0;1;1;1;1;1;1;1;1;1;1;1;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Huawei",
        "aff_unique_dep": ";Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.ust.hk;https://www.huawei.com",
        "aff_unique_abbr": "HKUST;Huawei",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.162",
        "title": "IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Traditionally, a debate usually requires a manual preparation process, including reading plenty of articles, selecting the claims, identifying the stances of the claims, seeking the evidence for the claims, etc. As the AI debate attracts more attention these years, it is worth exploring the methods to automate the tedious process involved in the debating system. In this work, we introduce a comprehensive and large dataset named IAM, which can be applied to a series of argument mining tasks, including claim extraction, stance classification, evidence extraction, etc. Our dataset is collected from over 1k articles related to 123 topics. Near 70k sentences in the dataset are fully annotated based on their argument properties (e.g., claims, stances, evidence, etc.). We further propose two new integrated argument mining tasks associated with the debate preparation process: (1) claim extraction with stance classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a pipeline approach and an end-to-end method for each integrated task separately. Promising experimental results are reported to show the values and challenges of our proposed tasks, and motivate future research on argument mining.",
        "author": "Liying Cheng; Lidong Bing; Ruidan He; Qian Yu; Yan Zhang; Luo Si",
        "authorids": "/l/liying-cheng/; /l/lidong-bing/; /r/ruidan-he/; /q/qian-yu/; /y/yan-zhang/; /l/luo-si/",
        "bibtex": "@inproceedings{cheng-etal-2022-iam,\n    title = \"{IAM}: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks\",\n    author = \"Cheng, Liying  and\n      Bing, Lidong  and\n      He, Ruidan  and\n      Yu, Qian  and\n      Zhang, Yan  and\n      Si, Luo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.162/\",\n    doi = \"10.18653/v1/2022.acl-long.162\",\n    pages = \"2277--2287\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.162.pdf",
        "site": "https://aclanthology.org/2022.acl-long.162/",
        "pdf_size": 225451,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1036302012734118272&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "DAMO Academy, Alibaba Group+Singapore University of Technology and Design; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; The Chinese University of Hong Kong; National University of Singapore; DAMO Academy, Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;se.cuhk.edu.hk;nus.edu.sg;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;se.cuhk.edu.hk;nus.edu.sg;alibaba-inc.com",
        "github": "https://github.com/LiyingCheng95/IAM",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0;2;3;0",
        "aff_unique_norm": "Alibaba Group;Singapore University of Technology and Design;Chinese University of Hong Kong;National University of Singapore",
        "aff_unique_dep": "DAMO Academy;;;",
        "aff_unique_url": "https://www.alibaba-group.com;https://www.sutd.edu.sg;https://www.cuhk.edu.hk;https://www.nus.edu.sg",
        "aff_unique_abbr": "Alibaba;SUTD;CUHK;NUS",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+1;0;0;0;1;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.acl-long.240",
        "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students\u2019 potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5% instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2% higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.",
        "author": "Neeraj Varshney; Swaroop Mishra; Chitta Baral",
        "authorids": "/n/neeraj-varshney/; /s/swaroop-mishra/; /c/chitta-baral/",
        "bibtex": "@inproceedings{varshney-etal-2022-ildae,\n    title = \"{ILDAE}: Instance-Level Difficulty Analysis of Evaluation Data\",\n    author = \"Varshney, Neeraj  and\n      Mishra, Swaroop  and\n      Baral, Chitta\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.240/\",\n    doi = \"10.18653/v1/2022.acl-long.240\",\n    pages = \"3412--3425\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.240.pdf",
        "site": "https://aclanthology.org/2022.acl-long.240/",
        "pdf_size": 2018775,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5331846265159201948&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 5,
        "aff": "Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "github": "https://github.com/nrjvarshney/ILDAE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.369",
        "title": "IMPLI: Investigating NLI Models\u2019 Performance on Figurative Language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language inference (NLI) has been widely used as a task to train and evaluate models for language understanding. However, the ability of NLI models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied. We introduce the IMPLI (Idiomatic and Metaphoric Paired Language Inference) dataset, an English dataset consisting of paired sentences spanning idioms and metaphors. We develop novel methods to generate 24k semiautomatic pairs as well as manually creating 1.8k gold pairs. We use IMPLI to evaluate NLI models based on RoBERTa fine-tuned on the widely used MNLI dataset. We then show that while they can reliably detect entailment relationship between figurative phrases with their literal counterparts, they perform poorly on similarly structured examples where pairs are designed to be non-entailing. This suggests the limits of current NLI models with regard to understanding figurative language and this dataset serves as a benchmark for future improvements in this direction.",
        "author": "Kevin Stowe; Prasetya Utama; Iryna Gurevych",
        "authorids": "/k/kevin-stowe/; /p/prasetya-utama/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{stowe-etal-2022-impli,\n    title = \"{IMPLI}: Investigating {NLI} Models' Performance on Figurative Language\",\n    author = \"Stowe, Kevin  and\n      Utama, Prasetya  and\n      Gurevych, Iryna\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.369/\",\n    doi = \"10.18653/v1/2022.acl-long.369\",\n    pages = \"5375--5388\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.369.pdf",
        "site": "https://aclanthology.org/2022.acl-long.369/",
        "pdf_size": 925948,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12748584493862871243&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Ubiquitous Knowledge Processing Lab (UKP) Department of Computer Science Technical University of Darmstadt; Bloomberg, London, United Kingdom + Ubiquitous Knowledge Processing Lab (UKP) Department of Computer Science Technical University of Darmstadt; Ubiquitous Knowledge Processing Lab (UKP) Department of Computer Science Technical University of Darmstadt",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/UKPLab/acl2022-impli",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Technical University of Darmstadt;Bloomberg",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.tu-darmstadt.de;https://www.bloomberg.com",
        "aff_unique_abbr": "TUD;Bloomberg",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "2022.acl-long.200",
        "title": "Identifying Chinese Opinion Expressions with Extremely-Noisy Crowdsourcing Annotations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent works of opinion expression identification (OEI) rely heavily on the quality and scale of the manually-constructed training corpus, which could be extremely difficult to satisfy. Crowdsourcing is one practical solution for this problem, aiming to create a large-scale but quality-unguaranteed corpus. In this work, we investigate Chinese OEI with extremely-noisy crowdsourcing annotations, constructing a dataset at a very low cost. Following Zhang el al. (2021), we train the annotator-adapter model by regarding all annotations as gold-standard in terms of crowd annotators, and test the model by using a synthetic expert, which is a mixture of all annotators. As this annotator-mixture for testing is never modeled explicitly in the training phase, we propose to generate synthetic training samples by a pertinent mixup strategy to make the training and testing highly consistent. The simulation experiments on our constructed dataset show that crowdsourcing is highly promising for OEI, and our proposed annotator-mixup can further enhance the crowdsourcing modeling.",
        "author": "Xin Zhang; Guangwei Xu; Yueheng Sun; Meishan Zhang; Xiaobin Wang; Min Zhang",
        "authorids": "/x/xin-zhang/; /g/guangwei-xu/; /y/yueheng-sun/; /m/meishan-zhang/; /x/xiaobin-wang/; /m/min-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2022-identifying,\n    title = \"Identifying {C}hinese Opinion Expressions with Extremely-Noisy Crowdsourcing Annotations\",\n    author = \"Zhang, Xin  and\n      Xu, Guangwei  and\n      Sun, Yueheng  and\n      Zhang, Meishan  and\n      Wang, Xiaobin  and\n      Zhang, Min\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.200/\",\n    doi = \"10.18653/v1/2022.acl-long.200\",\n    pages = \"2801--2813\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.200.pdf",
        "site": "https://aclanthology.org/2022.acl-long.200/",
        "pdf_size": 543373,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8317510125549799132&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of New Media and Communication, Tianjin University, China; School of New Media and Communication, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China + School of New Media and Communication, Tianjin University, China; Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China; Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China",
        "aff_domain": "tju.edu.cn;tju.edu.cn;gmail.com;gmail.com;foxmail.com;hit.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;gmail.com;gmail.com;foxmail.com;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1+0;1;1",
        "aff_unique_norm": "Tianjin University;Harbin Institute of Technology",
        "aff_unique_dep": "School of New Media and Communication;Institute of Computing and Intelligence",
        "aff_unique_url": "http://www.tju.edu.cn;http://www.hhit.edu.cn",
        "aff_unique_abbr": "Tianjin University;HIT",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.318",
        "title": "Identifying Moments of Change from Longitudinal User Text",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Identifying changes in individuals\u2019 behaviour and mood, as observed via content shared on online platforms, is increasingly gaining importance. Most research to-date on this topic focuses on either: (a) identifying individuals at risk or with a certain mental health condition given a batch of posts or (b) providing equivalent labels at the post level. A disadvantage of such work is the lack of a strong temporal component and the inability to make longitudinal assessments following an individual\u2019s trajectory and allowing timely interventions. Here we define a new task, that of identifying moments of change in individuals on the basis of their shared content online. The changes we consider are sudden shifts in mood (switches) or gradual mood progression (escalations). We have created detailed guidelines for capturing moments of change and a corpus of 500 manually annotated user timelines (18.7K posts). We have developed a variety of baseline models drawing inspiration from related tasks and show that the best performance is obtained through context aware sequential modelling. We also introduce new metrics for capturing rare events in temporal windows.",
        "author": "Adam Tsakalidis; Federico Nanni; Anthony Hills; Jenny Chim; Jiayu Song; Maria Liakata",
        "authorids": "/a/adam-tsakalidis/; /f/federico-nanni/; /a/anthony-hills/; /j/jenny-chim/; /j/jiayu-song/; /m/maria-liakata/",
        "bibtex": "@inproceedings{tsakalidis-etal-2022-identifying,\n    title = \"Identifying Moments of Change from Longitudinal User Text\",\n    author = \"Tsakalidis, Adam  and\n      Nanni, Federico  and\n      Hills, Anthony  and\n      Chim, Jenny  and\n      Song, Jiayu  and\n      Liakata, Maria\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.318/\",\n    doi = \"10.18653/v1/2022.acl-long.318\",\n    pages = \"4647--4660\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.318.pdf",
        "site": "https://aclanthology.org/2022.acl-long.318/",
        "pdf_size": 1482689,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3419477926087447969&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Queen Mary University of London; The Alan Turing Institute; Queen Mary University of London; Queen Mary University of London; Queen Mary University of London; Queen Mary University of London + The Alan Turing Institute + University of Warwick",
        "aff_domain": "qmul.ac.uk; ; ; ; ;qmul.ac.uk",
        "email": "qmul.ac.uk; ; ; ; ;qmul.ac.uk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;0+1+2",
        "aff_unique_norm": "Queen Mary University of London;Alan Turing Institute;University of Warwick",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.qmul.ac.uk;https://www.turing.ac.uk;https://www.warwick.ac.uk",
        "aff_unique_abbr": "QMUL;ATI;Warwick",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;0;0;0;0+0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.306",
        "title": "Identifying the Human Values behind Arguments",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper studies the (often implicit) human values behind natural language arguments, such as to have freedom of thought or to be broadminded. Values are commonly accepted answers to why some option is desirable in the ethical sense and are thus essential both in real-world argumentation and theoretical argumentation frameworks. However, their large variety has been a major obstacle to modeling them in argument mining. To overcome this obstacle, we contribute an operationalization of human values, namely a multi-level taxonomy with 54 values that is in line with psychological research. Moreover, we provide a dataset of 5270 arguments from four geographical cultures, manually annotated for human values. First experiments with the automatic classification of human values are promising, with F1-scores up to 0.81 and 0.25 on average.",
        "author": "Johannes Kiesel; Milad Alshomary; Nicolas Handke; Xiaoni Cai; Henning Wachsmuth; Benno Stein",
        "authorids": "/j/johannes-kiesel/; /m/milad-alshomary/; /n/nicolas-handke/; /x/xiaoni-cai/; /h/henning-wachsmuth/; /b/benno-stein/",
        "bibtex": "@inproceedings{kiesel-etal-2022-identifying,\n    title = \"Identifying the Human Values behind Arguments\",\n    author = \"Kiesel, Johannes  and\n      Alshomary, Milad  and\n      Handke, Nicolas  and\n      Cai, Xiaoni  and\n      Wachsmuth, Henning  and\n      Stein, Benno\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.306/\",\n    doi = \"10.18653/v1/2022.acl-long.306\",\n    pages = \"4459--4471\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.306.pdf",
        "site": "https://aclanthology.org/2022.acl-long.306/",
        "pdf_size": 1060628,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9024852551422514547&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Bauhaus-Universit\u00e4t Weimar; Paderborn University; Universit\u00e4t Leipzig; Technische Universit\u00e4t M\u00fcnchen; Paderborn University; Bauhaus-Universit\u00e4t Weimar",
        "aff_domain": "uni-weimar.de;upb.de;gmx.de;in.tum.de;upb.de;uni-weimar.de",
        "email": "uni-weimar.de;upb.de;gmx.de;in.tum.de;upb.de;uni-weimar.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;1;0",
        "aff_unique_norm": "Bauhaus-Universit\u00e4t Weimar;Paderborn University;University of Leipzig;Technische Universit\u00e4t M\u00fcnchen",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.bauhaus-university.de;https://www.upb.de/;https://www.uni-leipzig.de;https://www.tum.de",
        "aff_unique_abbr": "Bauhaus-Uni Weimar;UPB;Uni Leipzig;TUM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Weimar;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.241",
        "title": "Image Retrieval from Contextual Descriptions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on ImageCoDe.Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.",
        "author": "Benno Krojer; Vaibhav Adlakha; Vibhav Vineet; Yash Goyal; Edoardo Ponti; Siva Reddy",
        "authorids": "/b/benno-krojer/; /v/vaibhav-adlakha/; /v/vibhav-vineet/; /y/yash-goyal/; /e/edoardo-ponti/; /s/siva-reddy/",
        "bibtex": "@inproceedings{krojer-etal-2022-image,\n    title = \"Image Retrieval from Contextual Descriptions\",\n    author = \"Krojer, Benno  and\n      Adlakha, Vaibhav  and\n      Vineet, Vibhav  and\n      Goyal, Yash  and\n      Ponti, Edoardo  and\n      Reddy, Siva\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.241/\",\n    doi = \"10.18653/v1/2022.acl-long.241\",\n    pages = \"3426--3440\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.241.pdf",
        "site": "https://aclanthology.org/2022.acl-long.241/",
        "pdf_size": 25469122,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13613253435861041467&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Mila/McGill University; Mila/McGill University; Microsoft Research; Samsung - SAIT AI Lab, Montreal; Mila/McGill University; Mila/McGill University + Facebook CIFAR AI Chair",
        "aff_domain": "mila.quebec; ; ; ; ;mila.quebec",
        "email": "mila.quebec; ; ; ; ;mila.quebec",
        "github": "https://github.com/McGill-NLP/imagecode",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;0;0+3",
        "aff_unique_norm": "McGill University;Microsoft;Samsung;Meta",
        "aff_unique_dep": "Mila;Microsoft Research;SAIT AI Lab;Facebook CIFAR AI",
        "aff_unique_url": "https://www.mcgill.ca;https://www.microsoft.com/en-us/research;https://www.samsung.com;https://www.facebook.com",
        "aff_unique_abbr": "McGill;MSR;Samsung;FB",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Montreal",
        "aff_country_unique_index": "0;0;1;0;0;0+1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.acl-long.339",
        "title": "Impact of Evaluation Methodologies on Code Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There has been a growing interest in developing machine learning (ML) models for code summarization tasks, e.g., comment generation and method naming. Despite substantial increase in the effectiveness of ML models, the evaluation methodologies, i.e., the way people split datasets into training, validation, and test sets, were not well studied. Specifically, no prior work on code summarization considered the timestamps of code and comments during evaluation. This may lead to evaluations that are inconsistent with the intended use cases. In this paper, we introduce the time-segmented evaluation methodology, which is novel to the code summarization research community, and compare it with the mixed-project and cross-project methodologies that have been commonly used. Each methodology can be mapped to some use cases, and the time-segmented methodology should be adopted in the evaluation of ML models for code summarization. To assess the impact of methodologies, we collect a dataset of (code, comment) pairs with timestamps to train and evaluate several recent ML models for code summarization. Our experiments show that different methodologies lead to conflicting evaluation results. We invite the community to expand the set of methodologies used in evaluations.",
        "author": "Pengyu Nie; Jiyang Zhang; Junyi Jessy Li; Ray Mooney; Milos Gligoric",
        "authorids": "/p/pengyu-nie/; /j/jiyang-zhang/; /j/junyi-jessy-li/; /r/ray-mooney/; /m/milos-gligoric/",
        "bibtex": "@inproceedings{nie-etal-2022-impact,\n    title = \"Impact of Evaluation Methodologies on Code Summarization\",\n    author = \"Nie, Pengyu  and\n      Zhang, Jiyang  and\n      Li, Junyi Jessy  and\n      Mooney, Ray  and\n      Gligoric, Milos\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.339/\",\n    doi = \"10.18653/v1/2022.acl-long.339\",\n    pages = \"4936--4960\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.339.pdf",
        "site": "https://aclanthology.org/2022.acl-long.339/",
        "pdf_size": 1299675,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9836454955714774239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;austin.utexas.edu;cs.utexas.edu;utexas.edu",
        "email": "utexas.edu;utexas.edu;austin.utexas.edu;cs.utexas.edu;utexas.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.312",
        "title": "Implicit Relation Linking for Question Answering over Knowledge Graph",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Relation linking (RL) is a vital module in knowledge-based question answering (KBQA) systems. It aims to link the relations expressed in natural language (NL) to the corresponding ones in knowledge graph (KG). Existing methods mainly rely on the textual similarities between NL and KG to build relation links. Due to the ambiguity of NL and the incompleteness of KG, many relations in NL are implicitly expressed, and may not link to a single relation in KG, which challenges the current methods. In this paper, we propose an implicit RL method called ImRL, which links relation phrases in NL to relation paths in KG. To find proper relation paths, we propose a novel path ranking model that aligns not only textual information in the word embedding space but also structural information in the KG embedding space between relation phrases in NL and relation paths in KG. Besides, we leverage a gated mechanism with attention to inject prior knowledge from external paraphrase dictionaries to address the relation phrases with vague meaning. Our experiments on two benchmark and a newly-created datasets show that ImRL significantly outperforms several state-of-the-art methods, especially for implicit RL.",
        "author": "Yao Zhao; Jiacheng Huang; Wei Hu; Qijin Chen; XiaoXia Qiu; Chengfu Huo; Weijun Ren",
        "authorids": "/y/yao-zhao/; /j/jiacheng-huang/; /w/wei-hu/; /q/qijin-chen/; /x/xiaoxia-qiu/; /c/chengfu-huo/; /w/weijun-ren/",
        "bibtex": "@inproceedings{zhao-etal-2022-implicit,\n    title = \"Implicit Relation Linking for Question Answering over Knowledge Graph\",\n    author = \"Zhao, Yao  and\n      Huang, Jiacheng  and\n      Hu, Wei  and\n      Chen, Qijin  and\n      Qiu, XiaoXia  and\n      Huo, Chengfu  and\n      Ren, Weijun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.312/\",\n    doi = \"10.18653/v1/2022.findings-acl.312\",\n    pages = \"3956--3968\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.312.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.312/",
        "pdf_size": 1732737,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13379794136894690235&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China + National Institute of Healthcare Data Science, Nanjing University, China; Alibaba Group, China; Alibaba Group, China; Alibaba Group, China; Alibaba Group, China",
        "aff_domain": "gmail.com;gmail.com;nju.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "gmail.com;gmail.com;nju.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0+0;1;1;1;1",
        "aff_unique_norm": "Nanjing University;Alibaba Group",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology;",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "Nanjing U;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.192",
        "title": "Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In document classification for, e.g., legal and biomedical text, we often deal with hundreds of classes, including very infrequent ones, as well as temporal concept drift caused by the influence of real world events, e.g., policy changes, conflicts, or pandemics. Class imbalance and drift can sometimes be mitigated by resampling the training data to simulate (or compensate for) a known target distribution, but what if the target distribution is determined by unknown future events? Instead of simply resampling uniformly to hedge our bets, we focus on the underlying optimization algorithms used to train such document classifiers and evaluate several group-robust optimization algorithms, initially proposed to mitigate group-level disparities. Reframing group-robust algorithms as adaptation algorithms under concept drift, we find that Invariant Risk Minimization and Spectral Decoupling outperform sampling-based approaches to class imbalance and concept drift, and lead to much better performance on minority classes. The effect is more pronounced the larger the label set.",
        "author": "Ilias Chalkidis; Anders S\u00f8gaard",
        "authorids": "/i/ilias-chalkidis/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{chalkidis-sogaard-2022-improved,\n    title = \"Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting\",\n    author = \"Chalkidis, Ilias  and\n      S{\\o}gaard, Anders\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.192/\",\n    doi = \"10.18653/v1/2022.findings-acl.192\",\n    pages = \"2441--2454\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.192.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.192/",
        "pdf_size": 921123,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10987098022216032235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Copenhagen, Denmark; Department of Computer Science, University of Copenhagen, Denmark",
        "aff_domain": "di.ku.dk;di.ku.dk",
        "email": "di.ku.dk;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Copenhagen",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ku.dk",
        "aff_unique_abbr": "UCPH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "2022.findings-acl.292",
        "title": "Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Entity linking (EL) is the task of linking entity mentions in a document to referent entities in a knowledge base (KB). Many previous studies focus on Wikipedia-derived KBs. There is little work on EL over Wikidata, even though it is the most extensive crowdsourced KB. The scale of Wikidata can open up many new real-world applications, but its massive number of entities also makes EL challenging. To effectively narrow down the search space, we propose a novel candidate retrieval paradigm based on entity profiling. Wikidata entities and their textual fields are first indexed into a text search engine (e.g., Elasticsearch). During inference, given a mention and its context, we use a sequence-to-sequence (seq2seq) model to generate the profile of the target entity, which consists of its title and description. We use the profile to query the indexed search engine to retrieve candidate entities. Our approach complements the traditional approach of using a Wikipedia anchor-text dictionary, enabling us to further design a highly effective hybrid method for candidate retrieval. Combined with a simple cross-attention reranker, our complete EL framework achieves state-of-the-art results on three Wikidata-based datasets and strong performance on TACKBP-2010.",
        "author": "Tuan Lai; Heng Ji; ChengXiang Zhai",
        "authorids": "/t/tuan-lai/; /h/heng-ji/; /c/chengxiang-zhai/",
        "bibtex": "@inproceedings{lai-etal-2022-improving,\n    title = \"Improving Candidate Retrieval with Entity Profile Generation for {W}ikidata Entity Linking\",\n    author = \"Lai, Tuan  and\n      Ji, Heng  and\n      Zhai, ChengXiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.292/\",\n    doi = \"10.18653/v1/2022.findings-acl.292\",\n    pages = \"3696--3711\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.292.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.292/",
        "pdf_size": 446576,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6200732449161256689&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "https://github.com/laituan245/EL-Dockers/",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.233",
        "title": "Improving Chinese Grammatical Error Detection via Data augmentation by Conditional Error Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Chinese Grammatical Error Detection(CGED) aims at detecting grammatical errors in Chinese texts. One of the main challenges for CGED is the lack of annotated data. To alleviate this problem, previous studies proposed various methods to automatically generate more training samples, which can be roughly categorized into rule-based methods and model-based methods. The rule-based methods construct erroneous sentences by directly introducing noises into original sentences. However, the introduced noises are usually context-independent, which are quite different from those made by humans. The model-based methods utilize generative models to imitate human errors. The generative model may bring too many changes to the original sentences and generate semantically ambiguous sentences, so it is difficult to detect grammatical errors in these generated sentences. In addition, generated sentences may be error-free and thus become noisy data. To handle these problems, we propose CNEG, a novel Conditional Non-Autoregressive Error Generation model for generating Chinese grammatical errors. Specifically, in order to generate a context-dependent error, we first mask a span in a correct text, then predict an erroneous span conditioned on both the masked text and the correct span. Furthermore, we filter out error-free spans by measuring their perplexities in the original sentences. Experimental results show that our proposed method achieves better performance than all compared data augmentation methods on the CGED-2018 and CGED-2020 benchmarks.",
        "author": "Tianchi Yue; Shulin Liu; Huihui Cai; Tao Yang; Shengkang Song; TingHao Yu",
        "authorids": "/t/tianchi-yue/; /s/shulin-liu/; /h/huihui-cai/; /t/tao-yang/; /s/shengkang-song/; /t/tinghao-yu/",
        "bibtex": "@inproceedings{yue-etal-2022-improving,\n    title = \"Improving {C}hinese Grammatical Error Detection via Data augmentation by Conditional Error Generation\",\n    author = \"Yue, Tianchi  and\n      Liu, Shulin  and\n      Cai, Huihui  and\n      Yang, Tao  and\n      Song, Shengkang  and\n      Yu, TingHao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.233/\",\n    doi = \"10.18653/v1/2022.findings-acl.233\",\n    pages = \"2966--2975\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.233.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.233/",
        "pdf_size": 690293,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14643678478342153857&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Tencent AI Platform Department, China; Tencent AI Platform Department, China; Tencent AI Platform Department, China; Tencent AI Platform Department, China; Tencent AI Platform Department, China; Tencent AI Platform Department, China",
        "aff_domain": "tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "email": "tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Tencent",
        "aff_unique_dep": "AI Platform Department",
        "aff_unique_url": "https://www.tencent.com",
        "aff_unique_abbr": "Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.289",
        "title": "Improving Compositional Generalization with Self-Training for Data-to-Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Data-to-text generation focuses on generating fluent natural language responses from structured meaning representations (MRs). Such representations are compositional and it is costly to collect responses for all possible combinations of atomic meaning schemata, thereby necessitating few-shot generalization to novel MRs. In this work, we systematically study the compositional generalization of the state-of-the-art T5 models in few-shot data-to-text tasks. We show that T5 models fail to generalize to unseen MRs, and we propose a template-based input representation that considerably improves the model\u2019s generalization capability. To further improve the model\u2019s performance, we propose an approach based on self-training using fine-tuned BLEURT for pseudo-response selection. On the commonly-used SGD and Weather benchmarks, the proposed self-training approach improves tree accuracy by 46%+ and reduces the slot error rates by 73%+ over the strong T5 baselines in few-shot settings.",
        "author": "Sanket Vaibhav Mehta; Jinfeng Rao; Yi Tay; Mihir Kale; Ankur Parikh; Emma Strubell",
        "authorids": "/s/sanket-vaibhav-mehta/; /j/jinfeng-rao/; /y/yi-tay/; /m/mihir-kale/; /a/ankur-parikh/; /e/emma-strubell/",
        "bibtex": "@inproceedings{mehta-etal-2022-improving,\n    title = \"Improving Compositional Generalization with Self-Training for Data-to-Text Generation\",\n    author = \"Mehta, Sanket Vaibhav  and\n      Rao, Jinfeng  and\n      Tay, Yi  and\n      Kale, Mihir  and\n      Parikh, Ankur  and\n      Strubell, Emma\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.289/\",\n    doi = \"10.18653/v1/2022.acl-long.289\",\n    pages = \"4205--4219\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.289.pdf",
        "site": "https://aclanthology.org/2022.acl-long.289/",
        "pdf_size": 450841,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12141821595727553604&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 5,
        "aff": "Carnegie Mellon University; Google; Google Research; Google; Google Research; Carnegie Mellon University+Google Research",
        "aff_domain": "cs.cmu.edu;google.com;google.com;google.com;google.com;cs.cmu.edu",
        "email": "cs.cmu.edu;google.com;google.com;google.com;google.com;cs.cmu.edu",
        "github": "github.com/google-research/google-research/tree/master/compgen_d2t",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;0+1",
        "aff_unique_norm": "Carnegie Mellon University;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.cmu.edu;https://www.google.com",
        "aff_unique_abbr": "CMU;Google",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.272",
        "title": "Improving Controllable Text Generation with Position-Aware Weighted Decoding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation. However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text. In this paper, we illustrate this trade-off is arisen by the controller imposing the target attribute on the LM at improper positions. And we propose a novel framework based on existing weighted decoding methods called CAT-PAW, which introduces a lightweight regulator to adjust bias signals from the controller at different decoding positions. Experiments on positive sentiment control, topic control, and language detoxification show the effectiveness of our CAT-PAW upon 4 SOTA models.",
        "author": "Yuxuan Gu; Xiaocheng Feng; Sicheng Ma; Jiaming Wu; Heng Gong; Bing Qin",
        "authorids": "/y/yuxuan-gu/; /x/xiaocheng-feng/; /s/sicheng-ma/; /j/jiaming-wu/; /h/heng-gong/; /b/bing-qin/",
        "bibtex": "@inproceedings{gu-etal-2022-improving,\n    title = \"Improving Controllable Text Generation with Position-Aware Weighted Decoding\",\n    author = \"Gu, Yuxuan  and\n      Feng, Xiaocheng  and\n      Ma, Sicheng  and\n      Wu, Jiaming  and\n      Gong, Heng  and\n      Qin, Bing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.272/\",\n    doi = \"10.18653/v1/2022.findings-acl.272\",\n    pages = \"3449--3467\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.272.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.272/",
        "pdf_size": 936380,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5893351554461997199&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology+Peng Cheng Laboratory; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology+Peng Cheng Laboratory",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "https://github.com/hit-scma/CAT-PAW",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+1;0;0;0;0+1",
        "aff_unique_norm": "Harbin Institute of Technology;Pengcheng Laboratory",
        "aff_unique_dep": ";Peng Cheng Laboratory",
        "aff_unique_url": "http://www.hit.edu.cn/;http://www.pcl.ac.cn",
        "aff_unique_abbr": "HIT;PCL",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0+0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.216",
        "title": "Improving Event Representation via Simultaneous Weakly Supervised Contrastive Learning and Clustering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Representations of events described in text are important for various tasks. In this work, we present SWCC: a Simultaneous Weakly supervised Contrastive learning and Clustering framework for event representation learning. SWCC learns event representations by making better use of co-occurrence information of events. Specifically, we introduce a weakly supervised contrastive learning method that allows us to consider multiple positives and multiple negatives, and a prototype-based clustering method that avoids semantically related events being pulled apart. For model training, SWCC learns representations by simultaneously performing weakly supervised contrastive learning and prototype-based clustering. Experimental results show that SWCC outperforms other baselines on Hard Similarity and Transitive Sentence Similarity tasks. In addition, a thorough analysis of the prototype-based clustering method demonstrates that the learned prototype vectors are able to implicitly capture various relations between events.",
        "author": "Jun Gao; Wei Wang; Changlong Yu; Huan Zhao; Wilfred Ng; Ruifeng Xu",
        "authorids": "/j/jun-gao/; /w/wei-wang/; /c/changlong-yu/; /h/huan-zhao/; /w/wilfred-ng/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{gao-etal-2022-improving,\n    title = \"Improving Event Representation via Simultaneous Weakly Supervised Contrastive Learning and Clustering\",\n    author = \"Gao, Jun  and\n      Wang, Wei  and\n      Yu, Changlong  and\n      Zhao, Huan  and\n      Ng, Wilfred  and\n      Xu, Ruifeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.216/\",\n    doi = \"10.18653/v1/2022.acl-long.216\",\n    pages = \"3036--3049\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.216.pdf",
        "site": "https://aclanthology.org/2022.acl-long.216/",
        "pdf_size": 919787,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=411808888731422286&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Harbin Institute of Technology (Shenzhen)+Peng Cheng Laboratory; Tsinghua University; HKUST, Hong Kong, China; Paradigm. Inc.+HKUST, Hong Kong, China; HKUST, Hong Kong, China; Harbin Institute of Technology (Shenzhen)",
        "aff_domain": "gmail.com;hit.edu.cn;163.com;cse.ust.hk;cse.ust.hk;4paradigm.com",
        "email": "gmail.com;hit.edu.cn;163.com;cse.ust.hk;cse.ust.hk;4paradigm.com",
        "github": "https://github.com/gaojun4ever/SWCC4Event",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;3;4+3;3;0",
        "aff_unique_norm": "Harbin Institute of Technology;Pengcheng Laboratory;Tsinghua University;Hong Kong University of Science and Technology;Paradigm, Inc.",
        "aff_unique_dep": ";Peng Cheng Laboratory;;;",
        "aff_unique_url": "http://en.hhit.edu.cn/;http://www.pcl.ac.cn;https://www.tsinghua.edu.cn;https://www.ust.hk;",
        "aff_unique_abbr": "HIT;PCL;THU;HKUST;",
        "aff_campus_unique_index": "0;2;2;2;0",
        "aff_campus_unique": "Shenzhen;;Hong Kong",
        "aff_country_unique_index": "0+0;0;0;1+0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.378",
        "title": "Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Robustness of machine learning models on ever-changing real-world data is critical, especially for applications affecting human well-being such as content moderation. New kinds of abusive language continually emerge in online discussions in response to current events (e.g., COVID-19), and the deployed abuse detection systems should be updated regularly to remain accurate. In this paper, we show that general abusive language classifiers tend to be fairly reliable in detecting out-of-domain explicitly abusive utterances but fail to detect new types of more subtle, implicit abuse. Next, we propose an interpretability technique, based on the Testing Concept Activation Vector (TCAV) method from computer vision, to quantify the sensitivity of a trained model to the human-defined concepts of explicit and implicit abusive language, and use that to explain the generalizability of the model on new data, in this case, COVID-related anti-Asian hate speech. Extending this technique, we introduce a novel metric, Degree of Explicitness, for a single instance and show that the new metric is beneficial in suggesting out-of-domain unlabeled examples to effectively enrich the training data with informative, implicitly abusive texts.",
        "author": "Isar Nejadgholi; Kathleen Fraser; Svetlana Kiritchenko",
        "authorids": "/i/isar-nejadgholi/; /k/kathleen-c-fraser/; /s/svetlana-kiritchenko/",
        "bibtex": "@inproceedings{nejadgholi-etal-2022-improving,\n    title = \"Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors\",\n    author = \"Nejadgholi, Isar  and\n      Fraser, Kathleen  and\n      Kiritchenko, Svetlana\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.378/\",\n    doi = \"10.18653/v1/2022.acl-long.378\",\n    pages = \"5517--5529\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.378.pdf",
        "site": "https://aclanthology.org/2022.acl-long.378/",
        "pdf_size": 413564,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15996719996894766575&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 8,
        "aff": "National Research Council Canada; National Research Council Canada; National Research Council Canada",
        "aff_domain": "nrc-cnrc.gc.ca;nrc-cnrc.gc.ca;nrc-cnrc.gc.ca",
        "email": "nrc-cnrc.gc.ca;nrc-cnrc.gc.ca;nrc-cnrc.gc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National Research Council Canada",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nrc-cnrc.gc.ca",
        "aff_unique_abbr": "NRC-CNRC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.598",
        "title": "Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "To perform well on a machine reading comprehension (MRC) task, machine readers usually require commonsense knowledge that is not explicitly mentioned in the given documents. This paper aims to extract a new kind of structured knowledge from scripts and use it to improve MRC. We focus on scripts as they contain rich verbal and nonverbal messages, and two relevant messages originally conveyed by different modalities during a short time period may serve as arguments of a piece of commonsense knowledge as they function together in daily communications. To save human efforts to name relations, we propose to represent relations implicitly by situating such an argument pair in a context and call it contextualized knowledge. To use the extracted knowledge to improve MRC, we compare several fine-tuning strategies to use the weakly-labeled MRC data constructed based on contextualized knowledge and further design a teacher-student paradigm with multiple teachers to facilitate the transfer of knowledge in weakly-labeled MRC data. Experimental results show that our paradigm outperforms other methods that use weakly-labeled data and improves a state-of-the-art baseline by 4.3% in accuracy on a Chinese multiple-choice MRC dataset C3, wherein most of the questions require unstated prior knowledge. We also seek to transfer the knowledge to other tasks by simply adapting the resulting student reader, yielding a 2.9% improvement in F1 on a relation extraction dataset DialogRE, demonstrating the potential usefulness of the knowledge for non-MRC tasks that require document comprehension.",
        "author": "Kai Sun; Dian Yu; Jianshu Chen; Dong Yu; Claire Cardie",
        "authorids": "/k/kai-sun/; /d/dian-yu/; /j/jianshu-chen/; /d/dong-yu/; /c/claire-cardie/",
        "bibtex": "@inproceedings{sun-etal-2022-improving,\n    title = \"Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge\",\n    author = \"Sun, Kai  and\n      Yu, Dian  and\n      Chen, Jianshu  and\n      Yu, Dong  and\n      Cardie, Claire\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.598/\",\n    doi = \"10.18653/v1/2022.acl-long.598\",\n    pages = \"8736--8747\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.598.pdf",
        "site": "https://aclanthology.org/2022.acl-long.598/",
        "pdf_size": 433566,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17313814091485238444&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Cornell University; Tencent AI Lab + Cornell University; Tencent AI Lab; Tencent AI Lab; Cornell University",
        "aff_domain": "cornell.edu;tencent.com;tencent.com;tencent.com;cs.cornell.edu",
        "email": "cornell.edu;tencent.com;tencent.com;tencent.com;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+0;1;1;0",
        "aff_unique_norm": "Cornell University;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.cornell.edu;https://ai.tencent.com",
        "aff_unique_abbr": "Cornell;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.44",
        "title": "Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Building models of natural language processing (NLP) is challenging in low-resource scenarios where limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model\u2019s reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of support sets stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.",
        "author": "Yingxiu Zhao; Zhiliang Tian; Huaxiu Yao; Yinhe Zheng; Dongkyu Lee; Yiping Song; Jian Sun; Nevin Zhang",
        "authorids": "/y/yingxiu-zhao/; /z/zhiliang-tian/; /h/huaxiu-yao/; /y/yinhe-zheng/; /d/dongkyu-lee/; /y/yiping-song/; /j/jian-sun/; /n/nevin-zhang/",
        "bibtex": "@inproceedings{zhao-etal-2022-improving,\n    title = \"Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation\",\n    author = \"Zhao, Yingxiu  and\n      Tian, Zhiliang  and\n      Yao, Huaxiu  and\n      Zheng, Yinhe  and\n      Lee, Dongkyu  and\n      Song, Yiping  and\n      Sun, Jian  and\n      Zhang, Nevin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.44/\",\n    doi = \"10.18653/v1/2022.acl-long.44\",\n    pages = \"583--595\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.44.pdf",
        "site": "https://aclanthology.org/2022.acl-long.44/",
        "pdf_size": 949597,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6483335637797488247&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The Hong Kong University of Science and Technology, Hong Kong SAR, China; The Hong Kong University of Science and Technology, Hong Kong SAR, China; Stanford University; Alibaba Group; The Hong Kong University of Science and Technology, Hong Kong SAR, China; Department of Computer Science, Peking University, Beijing, China; Alibaba Group; The Hong Kong University of Science and Technology, Hong Kong SAR, China",
        "aff_domain": "cse.ust.hk;cse.ust.hk;cs.stanford.edu;alibaba-inc.com;cse.ust.hk;pku.edu.cn;alibaba-inc.com;cse.ust.hk",
        "email": "cse.ust.hk;cse.ust.hk;cs.stanford.edu;alibaba-inc.com;cse.ust.hk;pku.edu.cn;alibaba-inc.com;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;2;0;3;2;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Stanford University;Alibaba Group;Peking University",
        "aff_unique_dep": ";;;Department of Computer Science",
        "aff_unique_url": "https://www.ust.hk;https://www.stanford.edu;https://www.alibaba.com;http://www.pku.edu.cn",
        "aff_unique_abbr": "HKUST;Stanford;Alibaba;Peking U",
        "aff_campus_unique_index": "0;0;1;0;3;0",
        "aff_campus_unique": "Hong Kong;Stanford;;Beijing",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.248",
        "title": "Improving Multi-label Malevolence Detection in Dialogues through Multi-faceted Label Correlation Enhancement",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A dialogue response is malevolent if it is grounded in negative emotions, inappropriate behavior, or an unethical value basis in terms of content and dialogue acts. The detection of malevolent dialogue responses is attracting growing interest. Current research on detecting dialogue malevolence has limitations in terms of datasets and methods. First, available dialogue datasets related to malevolence are labeled with a single category, but in practice assigning a single category to each utterance may not be appropriate as some malevolent utterances belong to multiple labels. Second, current methods for detecting dialogue malevolence neglect label correlation. Therefore, we propose the task of multi-label dialogue malevolence detection and crowdsource a multi-label dataset, multi-label dialogue malevolence detection (MDMD) for evaluation. We also propose a multi-label malevolence detection model, multi-faceted label correlation enhanced CRF (MCRF), with two label correlation mechanisms, label correlation in taxonomy (LCT) and label correlation in context (LCC). Experiments on MDMD show that our method outperforms the best performing baseline by a large margin, i.e., 16.1%, 11.9%, 12.0%, and 6.1% on precision, recall, F1, and Jaccard score, respectively.",
        "author": "Yangjun Zhang; Pengjie Ren; Wentao Deng; Zhumin Chen; Maarten de Rijke",
        "authorids": "/y/yangjun-zhang/; /p/pengjie-ren/; /w/wentao-deng/; /z/zhumin-chen/; /m/maarten-de-rijke/",
        "bibtex": "@inproceedings{zhang-etal-2022-improving-multi,\n    title = \"Improving Multi-label Malevolence Detection in Dialogues through Multi-faceted Label Correlation Enhancement\",\n    author = \"Zhang, Yangjun  and\n      Ren, Pengjie  and\n      Deng, Wentao  and\n      Chen, Zhumin  and\n      de Rijke, Maarten\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.248/\",\n    doi = \"10.18653/v1/2022.acl-long.248\",\n    pages = \"3543--3555\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.248.pdf",
        "site": "https://aclanthology.org/2022.acl-long.248/",
        "pdf_size": 1542767,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11624272008800757031&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "University of Amsterdam; Shandong University; Shandong University; Shandong University; University of Amsterdam",
        "aff_domain": "uva.nl;sdu.edu.cn;sdu.edu.cn;sdu.edu.cn;uva.nl",
        "email": "uva.nl;sdu.edu.cn;sdu.edu.cn;sdu.edu.cn;uva.nl",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Amsterdam;Shandong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uva.nl;http://www.sdu.edu.cn",
        "aff_unique_abbr": "UvA;SDU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "Netherlands;China"
    },
    {
        "id": "2022.findings-acl.186",
        "title": "Improving Neural Political Statement Classification with Class Hierarchical Information",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Many tasks in text-based computational social science (CSS) involve the classification of political statements into categories based on a domain-specific codebook. In order to be useful for CSS analysis, these categories must be fine-grained. The typically skewed distribution of fine-grained categories, however, results in a challenging classification problem on the NLP side. This paper proposes to make use of the hierarchical relations among categories typically present in such codebooks:e.g., markets and taxation are both subcategories of economy, while borders is a subcategory of security. We use these ontological relations as prior knowledge to establish additional constraints on the learned model, thusimproving performance overall and in particular for infrequent categories. We evaluate several lightweight variants of this intuition by extending state-of-the-art transformer-based textclassifiers on two datasets and multiple languages. We find the most consistent improvement for an approach based on regularization.",
        "author": "Erenay Dayanik; Andre Blessing; Nico Blokker; Sebastian Haunss; Jonas Kuhn; Gabriella Lapesa; Sebastian Pado",
        "authorids": "/e/erenay-dayanik/; /a/andre-blessing/; /n/nico-blokker/; /s/sebastian-haunss/; /j/jonas-kuhn/; /g/gabriella-lapesa/; /s/sebastian-pado/",
        "bibtex": "@inproceedings{dayanik-etal-2022-improving,\n    title = \"Improving Neural Political Statement Classification with Class Hierarchical Information\",\n    author = \"Dayanik, Erenay  and\n      Blessing, Andre  and\n      Blokker, Nico  and\n      Haunss, Sebastian  and\n      Kuhn, Jonas  and\n      Lapesa, Gabriella  and\n      Pado, Sebastian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.186/\",\n    doi = \"10.18653/v1/2022.findings-acl.186\",\n    pages = \"2367--2382\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.186.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.186/",
        "pdf_size": 400946,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16753174554511621050&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "IMS, University of Stuttgart, Germany; IMS, University of Stuttgart, Germany; SOCIUM, University of Bremen, Germany; SOCIUM, University of Bremen, Germany; IMS, University of Stuttgart, Germany; IMS, University of Stuttgart, Germany; IMS, University of Stuttgart, Germany",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;1;0;0;0",
        "aff_unique_norm": "University of Stuttgart;University of Bremen",
        "aff_unique_dep": "Institute for Modelling and Simulation (IMS);SOCIUM",
        "aff_unique_url": "https://www.ims.uni-stuttgart.de;https://www.uni-bremen.de",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.20",
        "title": "Improving Personalized Explanation Generation through Visualization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In modern recommender systems, there are usually comments or reviews from users that justify their ratings for different items. Trained on such textual corpus, explainable recommendation models learn to discover user interests and generate personalized explanations. Though able to provide plausible explanations, existing models tend to generate repeated sentences for different items or empty sentences with insufficient details. This begs an interesting question: can we immerse the models in a multimodal environment to gain proper awareness of real-world concepts and alleviate above shortcomings? To this end, we propose a visually-enhanced approach named METER with the help of visualization generation and text\u2013image matching discrimination: the explainable recommendation model is encouraged to visualize what it refers to while incurring a penalty if the visualization is incongruent with the textual explanation. Experimental results and a manual assessment demonstrate that our approach can improve not only the text quality but also the diversity and explainability of the generated explanations.",
        "author": "Shijie Geng; Zuohui Fu; Yingqiang Ge; Lei Li; Gerard de Melo; Yongfeng Zhang",
        "authorids": "/s/shijie-geng/; /z/zuohui-fu/; /y/yingqiang-ge/; /l/lei-li/; /g/gerard-de-melo/; /y/yongfeng-zhang/",
        "bibtex": "@inproceedings{geng-etal-2022-improving,\n    title = \"Improving Personalized Explanation Generation through Visualization\",\n    author = \"Geng, Shijie  and\n      Fu, Zuohui  and\n      Ge, Yingqiang  and\n      Li, Lei  and\n      de Melo, Gerard  and\n      Zhang, Yongfeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.20/\",\n    doi = \"10.18653/v1/2022.acl-long.20\",\n    pages = \"244--255\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.20.pdf",
        "site": "https://aclanthology.org/2022.acl-long.20/",
        "pdf_size": 6821372,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11934585031366631902&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "Rutgers University; Rutgers University; Rutgers University; Hong Kong Baptist University; HPI/University of Potsdam; Rutgers University",
        "aff_domain": "rutgers.edu;rutgers.edu;rutgers.edu;hkbuhk.edu;hpi.de;rutgers.edu",
        "email": "rutgers.edu;rutgers.edu;rutgers.edu;hkbuhk.edu;hpi.de;rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "Rutgers University;Hong Kong Baptist University;University of Potsdam",
        "aff_unique_dep": ";;Hasso Plattner Institute (HPI)",
        "aff_unique_url": "https://www.rutgers.edu;https://www.hkbu.edu.hk;https://www.hpi.de",
        "aff_unique_abbr": "Rutgers;HKBU;HPI",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Hong Kong SAR;Potsdam",
        "aff_country_unique_index": "0;0;0;1;2;0",
        "aff_country_unique": "United States;China;Germany"
    },
    {
        "id": "2022.findings-acl.147",
        "title": "Improving Relation Extraction through Syntax-induced Pre-training with Dependency Masking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Relation extraction (RE) is an important natural language processing task that predicts the relation between two given entities, where a good understanding of the contextual information is essential to achieve an outstanding model performance. Among different types of contextual information, the auto-generated syntactic information (namely, word dependencies) has shown its effectiveness for the task. However, most existing studies require modifications to the existing baseline architectures (e.g., adding new components, such as GCN, on the top of an encoder) to leverage the syntactic information. To offer an alternative solution, we propose to leverage syntactic information to improve RE by training a syntax-induced encoder on auto-parsed data through dependency masking. Specifically, the syntax-induced encoder is trained by recovering the masked dependency connections and types in first, second, and third orders, which significantly differs from existing studies that train language models or word embeddings by predicting the context words along the dependency paths. Experimental results on two English benchmark datasets, namely, ACE2005EN and SemEval 2010 Task 8 datasets, demonstrate the effectiveness of our approach for RE, where our approach outperforms strong baselines and achieve state-of-the-art results on both datasets.",
        "author": "Yuanhe Tian; Yan Song; Fei Xia",
        "authorids": "/y/yuanhe-tian/; /y/yan-song/; /f/fei-xia/",
        "bibtex": "@inproceedings{tian-etal-2022-improving,\n    title = \"Improving Relation Extraction through Syntax-induced Pre-training with Dependency Masking\",\n    author = \"Tian, Yuanhe  and\n      Song, Yan  and\n      Xia, Fei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.147/\",\n    doi = \"10.18653/v1/2022.findings-acl.147\",\n    pages = \"1875--1886\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.147.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.147/",
        "pdf_size": 1138159,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=753849414307390703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Washington; The Chinese University of Hong Kong (Shenzhen); University of Washington",
        "aff_domain": "uw.edu;cuhk.edu.cn;uw.edu",
        "email": "uw.edu;cuhk.edu.cn;uw.edu",
        "github": "https://github.com/synlp/RE-DMP",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Washington;Chinese University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.cuhk.edu.cn",
        "aff_unique_abbr": "UW;CUHK",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.findings-acl.246",
        "title": "Improving Robustness of Language Models from a Geometry-aware Perspective",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean data to the decision boundary but not crossing it does not degrade the test accuracy. Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data. On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps. Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps. In addition, we provide extensive empirical results and in-depth analyses on robustness to facilitate future studies.",
        "author": "Bin Zhu; Zhaoquan Gu; Le Wang; Jinyin Chen; Qi Xuan",
        "authorids": "/b/bin-zhu/; /z/zhaoquan-gu/; /l/le-wang/; /j/jinyin-chen/; /q/qi-xuan/",
        "bibtex": "@inproceedings{zhu-etal-2022-improving,\n    title = \"Improving Robustness of Language Models from a Geometry-aware Perspective\",\n    author = \"Zhu, Bin  and\n      Gu, Zhaoquan  and\n      Wang, Le  and\n      Chen, Jinyin  and\n      Xuan, Qi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.246/\",\n    doi = \"10.18653/v1/2022.findings-acl.246\",\n    pages = \"3115--3125\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.246.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.246/",
        "pdf_size": 385928,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10706044189740062528&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Cyberspace Institute of Advanced Technology (CIAT), Guangzhou University, Guangzhou 510006, China + Institute of Cyberspace Platform, Peng Cheng Laboratory, Shenzhen 999077, China; Cyberspace Institute of Advanced Technology (CIAT), Guangzhou University, Guangzhou 510006, China + Institute of Cyberspace Platform, Peng Cheng Laboratory, Shenzhen 999077, China; Cyberspace Institute of Advanced Technology (CIAT), Guangzhou University, Guangzhou 510006, China + Institute of Cyberspace Platform, Peng Cheng Laboratory, Shenzhen 999077, China; Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou 310023, China; Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou 310023, China",
        "aff_domain": "e.gzhu.edu.cn;gzhu.edu.cn;gzhu.edu.cn;zjut.edu.cn;zjut.edu.cn",
        "email": "e.gzhu.edu.cn;gzhu.edu.cn;gzhu.edu.cn;zjut.edu.cn;zjut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;2;2",
        "aff_unique_norm": "Guangzhou University;Pengcheng Laboratory;Zhejiang University of Technology",
        "aff_unique_dep": "Cyberspace Institute of Advanced Technology (CIAT);Institute of Cyberspace Platform;Institute of Cyberspace Security",
        "aff_unique_url": "http://www.gzhu.edu.cn;;",
        "aff_unique_abbr": "CIAT;;",
        "aff_campus_unique_index": "0+1;0+1;0+1;2;2",
        "aff_campus_unique": "Guangzhou;Shenzhen;Hangzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.552",
        "title": "Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Question answering over temporal knowledge graphs (KGs) efficiently uses facts contained in a temporal KG, which records entity relations and when they occur in time, to answer natural language questions (e.g., \u201cWho was the president of the US before Obama?\u201d). These questions often involve three time-related challenges that previous work fail to adequately address: 1) questions often do not specify exact timestamps of interest (e.g., \u201cObama\u201d instead of 2000); 2) subtle lexical differences in time relations (e.g., \u201cbefore\u201d vs \u201cafter\u201d); 3) off-the-shelf temporal KG embeddings that previous work builds on ignore the temporal order of timestamps, which is crucial for answering temporal-order related questions. In this paper, we propose a time-sensitive question answering (TSQA) framework to tackle these problems. TSQA features a timestamp estimation module to infer the unwritten timestamp from the question. We also employ a time-sensitive KG encoder to inject ordering information into the temporal KG embeddings that TSQA is based on. With the help of techniques to reduce the search space for potential answers, TSQA significantly outperforms the previous state of the art on a new benchmark for question answering over temporal KGs, especially achieving a 32% (absolute) error reduction on complex questions that require multiple steps of reasoning over facts in the temporal KG.",
        "author": "Chao Shang; Guangtao Wang; Peng Qi; Jing Huang",
        "authorids": "/c/chao-shang/; /g/guangtao-wang/; /p/peng-qi/; /j/jing-huang/",
        "bibtex": "@inproceedings{shang-etal-2022-improving,\n    title = \"Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs\",\n    author = \"Shang, Chao  and\n      Wang, Guangtao  and\n      Qi, Peng  and\n      Huang, Jing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.552/\",\n    doi = \"10.18653/v1/2022.acl-long.552\",\n    pages = \"8017--8026\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.552.pdf",
        "site": "https://aclanthology.org/2022.acl-long.552/",
        "pdf_size": 825476,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15189413646971158129&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "JD AI Research; JD AI Research; JD AI Research; Alexa AI, Amazon",
        "aff_domain": "jd.com;jd.com;jd.com;amazon.com",
        "email": "jd.com;jd.com;jd.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "JD;Amazon",
        "aff_unique_dep": "JD AI Research;Alexa AI",
        "aff_unique_url": "https://www.jd.com;https://www.amazon.com",
        "aff_unique_abbr": "JD AI;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.299",
        "title": "Improving Word Translation via Two-Stage Contrastive Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Word translation or bilingual lexicon induction (BLI) is a key cross-lingual task, aiming to bridge the lexical gap between different languages. In this work, we propose a robust and effective two-stage contrastive learning framework for the BLI task. At Stage C1, we propose to refine standard cross-lingual linear maps between static word embeddings (WEs) via a contrastive learning objective; we also show how to integrate it into the self-learning procedure for even more refined cross-lingual maps. In Stage C2, we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word translation capability. We also show that static WEs induced from the \u2018C2-tuned\u2019 mBERT complement static WEs from Stage C1. Comprehensive experiments on standard BLI datasets for diverse languages and different experimental setups demonstrate substantial gains achieved by our framework. While the BLI method from Stage C1 already yields substantial gains over all state-of-the-art BLI methods in our comparison, even stronger improvements are met with the full two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28 language pairs.",
        "author": "Yaoyiran Li; Fangyu Liu; Nigel Collier; Anna Korhonen; Ivan Vuli\u0107",
        "authorids": "/y/yaoyiran-li/; /f/fangyu-liu/; /n/nigel-collier/; /a/anna-korhonen/; /i/ivan-vulic/",
        "bibtex": "@inproceedings{li-etal-2022-improving,\n    title = \"Improving Word Translation via Two-Stage Contrastive Learning\",\n    author = \"Li, Yaoyiran  and\n      Liu, Fangyu  and\n      Collier, Nigel  and\n      Korhonen, Anna  and\n      Vuli{\\'c}, Ivan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.299/\",\n    doi = \"10.18653/v1/2022.acl-long.299\",\n    pages = \"4353--4374\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.299.pdf",
        "site": "https://aclanthology.org/2022.acl-long.299/",
        "pdf_size": 9738203,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2834935118471978273&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Language Technology Lab, TAL, University of Cambridge; Language Technology Lab, TAL, University of Cambridge; Language Technology Lab, TAL, University of Cambridge; Language Technology Lab, TAL, University of Cambridge; Language Technology Lab, TAL, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Language Technology Lab, TAL",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.321",
        "title": "Improving Zero-Shot Cross-lingual Transfer Between Closely Related Languages by Injecting Character-Level Noise",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Cross-lingual transfer between a high-resource language and its dialects or closely related language varieties should be facilitated by their similarity. However, current approaches that operate in the embedding space do not take surface similarity into account. This work presents a simple yet effective strategy to improve cross-lingual transfer between closely related varieties. We propose to augment the data of the high-resource source language with character-level noise to make the model more robust towards spelling variations. Our strategy shows consistent improvements over several languages and tasks: Zero-shot transfer of POS tagging and topic identification between language varieties from the Finnic, West and North Germanic, and Western Romance language branches. Our work provides evidence for the usefulness of simple surface-level noise in improving transfer between language varieties.",
        "author": "No\u00ebmi Aepli; Rico Sennrich",
        "authorids": "/n/noemi-aepli/; /r/rico-sennrich/",
        "bibtex": "@inproceedings{aepli-sennrich-2022-improving,\n    title = \"Improving Zero-Shot Cross-lingual Transfer Between Closely Related Languages by Injecting Character-Level Noise\",\n    author = {Aepli, No{\\\"e}mi  and\n      Sennrich, Rico},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.321/\",\n    doi = \"10.18653/v1/2022.findings-acl.321\",\n    pages = \"4074--4083\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.321.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.321/",
        "pdf_size": 1808703,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7259350104064448525&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computational Linguistics, University of Zurich + School of Informatics, University of Edinburgh; Department of Computational Linguistics, University of Zurich + School of Informatics, University of Edinburgh",
        "aff_domain": "cl.uzh.ch;cl.uzh.ch",
        "email": "cl.uzh.ch;cl.uzh.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Zurich;University of Edinburgh",
        "aff_unique_dep": "Department of Computational Linguistics;School of Informatics",
        "aff_unique_url": "https://www.unizh.ch;https://www.ed.ac.uk",
        "aff_unique_abbr": "UZH;Edinburgh",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Edinburgh",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "Switzerland;United Kingdom"
    },
    {
        "id": "2022.findings-acl.284",
        "title": "Improving the Adversarial Robustness of NLP Models by Information Bottleneck",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Existing studies have demonstrated that adversarial examples can be directly attributed to the presence of non-robust features, which are highly predictive, but can be easily manipulated by adversaries to fool NLP models. In this study, we explore the feasibility of capturing task-specific robust features, while eliminating the non-robust ones by using the information bottleneck theory. Through extensive experiments, we show that the models trained with our information bottleneck-based method are able to achieve a significant improvement in robust accuracy, exceeding performances of all the previously reported defense methods while suffering almost no performance drop in clean accuracy on SST-2, AGNEWS and IMDB datasets.",
        "author": "Cenyuan Zhang; Xiang Zhou; Yixin Wan; Xiaoqing Zheng; Kai-Wei Chang; Cho-Jui Hsieh",
        "authorids": "/c/cenyuan-zhang/; /x/xiang-zhou/; /y/yixin-wan/; /x/xiaoqing-zheng/; /k/kai-wei-chang/; /c/cho-jui-hsieh/",
        "bibtex": "@inproceedings{zhang-etal-2022-improving,\n    title = \"Improving the Adversarial Robustness of {NLP} Models by Information Bottleneck\",\n    author = \"Zhang, Cenyuan  and\n      Zhou, Xiang  and\n      Wan, Yixin  and\n      Zheng, Xiaoqing  and\n      Chang, Kai-Wei  and\n      Hsieh, Cho-Jui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.284/\",\n    doi = \"10.18653/v1/2022.findings-acl.284\",\n    pages = \"3588--3598\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.284.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.284/",
        "pdf_size": 1251542,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9592622309032973596&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing; School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing; Department of Computer Science, University of California, Los Angeles, USA; School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing; Department of Computer Science, University of California, Los Angeles, USA; Department of Computer Science, University of California, Los Angeles, USA",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;g.ucla.edu;fudan.edu.cn;cs.ucla.edu;cs.ucla.edu",
        "email": "fudan.edu.cn;fudan.edu.cn;g.ucla.edu;fudan.edu.cn;cs.ucla.edu;cs.ucla.edu",
        "github": "https://github.com/zhangcen456/IB.3588",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;2;0+1;2;2",
        "aff_unique_norm": "Fudan University;Shanghai Key Laboratory of Intelligent Information Processing;University of California, Los Angeles",
        "aff_unique_dep": "School of Computer Science;Intelligent Information Processing;Department of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn;;https://www.ucla.edu",
        "aff_unique_abbr": "Fudan;;UCLA",
        "aff_campus_unique_index": "0;0;2;0;2;2",
        "aff_campus_unique": "Shanghai;;Los Angeles",
        "aff_country_unique_index": "0+0;0+0;1;0+0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.578",
        "title": "Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automated methods have been widely used to identify and analyze mental health conditions (e.g., depression) from various sources of information, including social media. Yet, deployment of such models in real-world healthcare applications faces challenges including poor out-of-domain generalization and lack of trust in black box models. In this work, we propose approaches for depression detection that are constrained to different degrees by the presence of symptoms described in PHQ9, a questionnaire used by clinicians in the depression screening process. In dataset-transfer experiments on three social media datasets, we find that grounding the model in PHQ9\u2019s symptoms substantially improves its ability to generalize to out-of-distribution data compared to a standard BERT-based approach. Furthermore, this approach can still perform competitively on in-domain data. These results and our qualitative analyses suggest that grounding model predictions in clinically-relevant symptoms can improve generalizability while producing a model that is easier to inspect.",
        "author": "Thong Nguyen; Andrew Yates; Ayah Zirikly; Bart Desmet; Arman Cohan",
        "authorids": "/t/thong-nguyen/; /a/andrew-yates/; /a/ayah-zirikly/; /b/bart-desmet/; /a/arman-cohan/",
        "bibtex": "@inproceedings{nguyen-etal-2022-improving,\n    title = \"Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires\",\n    author = \"Nguyen, Thong  and\n      Yates, Andrew  and\n      Zirikly, Ayah  and\n      Desmet, Bart  and\n      Cohan, Arman\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.578/\",\n    doi = \"10.18653/v1/2022.acl-long.578\",\n    pages = \"8446--8459\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.578.pdf",
        "site": "https://aclanthology.org/2022.acl-long.578/",
        "pdf_size": 578355,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18045678427637291567&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff": "University of Amsterdam, Amsterdam, Netherlands + Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; University of Amsterdam, Amsterdam, Netherlands + Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Johns Hopkins University, Baltimore, Maryland; National Institutes of Health, Bethesda, Maryland; Allen Institute for AI, Seattle, WA",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;2;3;4",
        "aff_unique_norm": "University of Amsterdam;Max Planck Institute for Informatics;Johns Hopkins University;National Institutes of Health;Allen Institute for AI",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.uva.nl;https://mpi-inf.mpg.de;https://www.jhu.edu;https://www.nih.gov;https://allenai.org",
        "aff_unique_abbr": "UvA;MPII;JHU;NIH;AI2",
        "aff_campus_unique_index": "0+1;0+1;2;3;4",
        "aff_campus_unique": "Amsterdam;Saarbr\u00fccken;Baltimore;Bethesda;Seattle",
        "aff_country_unique_index": "0+1;0+1;2;2;2",
        "aff_country_unique": "Netherlands;Germany;United States"
    },
    {
        "id": "2022.acl-long.245",
        "title": "Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "State-of-the-art NLP systems represent inputs with word embeddings, but these are brittle when faced with Out-of-Vocabulary (OOV) words. To address this issue, we follow the principle of mimick-like models to generate vectors for unseen words, by learning the behavior of pre-trained embeddings using only the surface form of words. We present a simple contrastive learning framework, LOVE, which extends the word representation of an existing pre-trained language model (such as BERT) and makes it robust to OOV with few additional parameters. Extensive evaluations demonstrate that our lightweight model achieves similar or even better performances than prior competitors, both on original datasets and on corrupted variants. Moreover, it can be used in a plug-and-play fashion with FastText and BERT, where it significantly improves their robustness.",
        "author": "Lihu Chen; Gael Varoquaux; Fabian Suchanek",
        "authorids": "/l/lihu-chen/; /g/gael-varoquaux/; /f/fabian-suchanek/",
        "bibtex": "@inproceedings{chen-etal-2022-imputing,\n    title = \"Imputing Out-of-Vocabulary Embeddings with {LOVE} Makes {L}anguage{M}odels Robust with Little Cost\",\n    author = \"Chen, Lihu  and\n      Varoquaux, Gael  and\n      Suchanek, Fabian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.245/\",\n    doi = \"10.18653/v1/2022.acl-long.245\",\n    pages = \"3488--3504\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.245.pdf",
        "site": "https://aclanthology.org/2022.acl-long.245/",
        "pdf_size": 843177,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17843025893162123706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "LTCI & T\u00e9l\u00e9com Paris & Institut Polytechnique de Paris, France; Soda, Inria Saclay & CEA & Universit\u00e9 Paris-Saclay, France; LTCI & T\u00e9l\u00e9com Paris & Institut Polytechnique de Paris, France",
        "aff_domain": "telecom-paris.fr;inria.fr;telecom-paris.fr",
        "email": "telecom-paris.fr;inria.fr;telecom-paris.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Institut Polytechnique de Paris;INRIA Saclay",
        "aff_unique_dep": "LTCI;Soda",
        "aff_unique_url": "https://www.ipparis.fr;https://www.inria.fr/en/centre/saclay",
        "aff_unique_abbr": "IP Paris;Inria",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Saclay",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.findings-acl.285",
        "title": "Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding. Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.",
        "author": "Kai Zhang; Kun Zhang; Mengdi Zhang; Hongke Zhao; Qi Liu; Wei Wu; Enhong Chen",
        "authorids": "/k/kai-zhang/; /k/kun-zhang/; /m/mengdi-zhang/; /h/hongke-zhao/; /q/qi-liu/; /w/wei-wu/; /e/enhong-chen/",
        "bibtex": "@inproceedings{zhang-etal-2022-incorporating,\n    title = \"Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis\",\n    author = \"Zhang, Kai  and\n      Zhang, Kun  and\n      Zhang, Mengdi  and\n      Zhao, Hongke  and\n      Liu, Qi  and\n      Wu, Wei  and\n      Chen, Enhong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.285/\",\n    doi = \"10.18653/v1/2022.findings-acl.285\",\n    pages = \"3599--3610\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.285.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.285/",
        "pdf_size": 802057,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11047570700957758895&as_sdt=80005&sciodt=0,11&hl=en",
        "gs_version_total": 9,
        "aff": "School of Data Science, University of Science and Technology of China; School of Computer Science and Information Engineering, Hefei University of Technology; Meituan+College of Management and Economics, Tianjin University; College of Management and Economics, Tianjin University; School of Data Science, University of Science and Technology of China; Meituan; School of Data Science, University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;gmail.com;gmail.com;tju.edu.cn;ustc.edu.cn;gmail.com;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;gmail.com;gmail.com;tju.edu.cn;ustc.edu.cn;gmail.com;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2+3;3;0;2;0",
        "aff_unique_norm": "University of Science and Technology of China;Hefei University of Technology;Meituan;Tianjin University",
        "aff_unique_dep": "School of Data Science;School of Computer Science and Information Engineering;;College of Management and Economics",
        "aff_unique_url": "http://www.ustc.edu.cn;http://www.hfut.edu.cn/;https://www.meituan.com;http://www.tju.edu.cn",
        "aff_unique_abbr": "USTC;Hefei UTech;Meituan;Tianjin University",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Hefei",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.491",
        "title": "Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Hierarchical text classification is a challenging subtask of multi-label classification due to its complex label hierarchy. Existing methods encode text and label hierarchy separately and mix their representations for classification, where the hierarchy remains unchanged for all input text. Instead of modeling them separately, in this work, we propose Hierarchy-guided Contrastive Learning (HGCLR) to directly embed the hierarchy into a text encoder. During training, HGCLR constructs positive samples for input text under the guidance of the label hierarchy. By pulling together the input text and its positive sample, the text encoder can learn to generate the hierarchy-aware text representation independently. Therefore, after training, the HGCLR enhanced text encoder can dispense with the redundant hierarchy. Extensive experiments on three benchmark datasets verify the effectiveness of HGCLR.",
        "author": "Zihan Wang; Peiyi Wang; Lianzhe Huang; Xin Sun; Houfeng Wang",
        "authorids": "/z/zihan-wang/; /p/peiyi-wang/; /l/lianzhe-huang/; /x/xin-sun/; /h/houfeng-wang/",
        "bibtex": "@inproceedings{wang-etal-2022-incorporating,\n    title = \"Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification\",\n    author = \"Wang, Zihan  and\n      Wang, Peiyi  and\n      Huang, Lianzhe  and\n      Sun, Xin  and\n      Wang, Houfeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.491/\",\n    doi = \"10.18653/v1/2022.acl-long.491\",\n    pages = \"7109--7119\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.491.pdf",
        "site": "https://aclanthology.org/2022.acl-long.491/",
        "pdf_size": 352139,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5796774014751346014&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 6,
        "aff": "Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China",
        "aff_domain": "gmail.com;gmail.com;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "gmail.com;gmail.com;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.281",
        "title": "Incorporating Stock Market Signals for Twitter Stance Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Research in stance detection has so far focused on models which leverage purely textual input. In this paper, we investigate the integration of textual and financial signals for stance detection in the financial domain. Specifically, we propose a robust multi-task neural architecture that combines textual input with high-frequency intra-day time series from stock market prices. Moreover, we extend wt\u2013wt, an existing stance detection dataset which collects tweets discussing Mergers and Acquisitions operations, with the relevant financial signal. Importantly, the obtained dataset aligns with Stander, an existing news stance detection dataset, thus resulting in a unique multimodal, multi-genre stance detection resource. We show experimentally and through detailed result analysis that our stance detection system benefits from financial information, and achieves state-of-the-art results on the wt\u2013wt dataset: this demonstrates that the combination of multiple input signals is effective for cross-target stance detection, and opens interesting research directions for future work.",
        "author": "Costanza Conforti; Jakob Berndt; Mohammad Taher Pilehvar; Chryssi Giannitsarou; Flavio Toxvaerd; Nigel Collier",
        "authorids": "/c/costanza-conforti/; /j/jakob-berndt/; /m/mohammad-taher-pilehvar/; /c/chryssi-giannitsarou/; /f/flavio-toxvaerd/; /n/nigel-collier/",
        "bibtex": "@inproceedings{conforti-etal-2022-incorporating,\n    title = \"Incorporating Stock Market Signals for {T}witter Stance Detection\",\n    author = \"Conforti, Costanza  and\n      Berndt, Jakob  and\n      Pilehvar, Mohammad Taher  and\n      Giannitsarou, Chryssi  and\n      Toxvaerd, Flavio  and\n      Collier, Nigel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.281/\",\n    doi = \"10.18653/v1/2022.acl-long.281\",\n    pages = \"4074--4091\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.281.pdf",
        "site": "https://aclanthology.org/2022.acl-long.281/",
        "pdf_size": 1143210,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=541012720078092245&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 2,
        "aff": "Language Technology Lab, University of Cambridge; Faculty of Economics, University of Cambridge; Language Technology Lab, University of Cambridge+Tehran Institute for Advanced Studies, Khatam University, Iran; Faculty of Economics, University of Cambridge; Faculty of Economics, University of Cambridge; Language Technology Lab, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk; ; ; ; ",
        "email": "cam.ac.uk;cam.ac.uk; ; ; ; ",
        "github": "https://github.com/cambridge-wtwt/acl2022-wtwt-stocks4074",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0+1;0;0;0",
        "aff_unique_norm": "University of Cambridge;Tehran Institute for Advanced Studies",
        "aff_unique_dep": "Language Technology Lab;",
        "aff_unique_url": "https://www.cam.ac.uk;",
        "aff_unique_abbr": "Cambridge;",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0+1;0;0;0",
        "aff_country_unique": "United Kingdom;Iran"
    },
    {
        "id": "2022.findings-acl.280",
        "title": "Incremental Intent Detection for Medical Domain with Contrast Replay Networks",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Conventional approaches to medical intent detection require fixed pre-defined intent categories. However, due to the incessant emergence of new medical intents in the real world, such requirement is not practical. Considering that it is computationally expensive to store and re-train the whole data every time new data and intents come in, we propose to incrementally learn emerged intents while avoiding catastrophically forgetting old intents. We first formulate incremental learning for medical intent detection. Then, we employ a memory-based method to handle incremental learning. We further propose to enhance the method with contrast replay networks, which use multilevel distillation and contrast objective to address training data imbalance and medical rare words respectively. Experiments show that the proposed method outperforms the state-of-the-art model by 5.7% and 9.1% of accuracy on two benchmarks respectively.",
        "author": "Guirong Bai; Shizhu He; Kang Liu; Jun Zhao",
        "authorids": "/g/guirong-bai/; /s/shizhu-he/; /k/kang-liu/; /j/jun-zhao/",
        "bibtex": "@inproceedings{bai-etal-2022-incremental,\n    title = \"Incremental Intent Detection for Medical Domain with Contrast Replay Networks\",\n    author = \"Bai, Guirong  and\n      He, Shizhu  and\n      Liu, Kang  and\n      Zhao, Jun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.280/\",\n    doi = \"10.18653/v1/2022.findings-acl.280\",\n    pages = \"3549--3556\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.280.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.280/",
        "pdf_size": 376416,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9239470634538010220&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences + Beijing Academy of Artificial Intelligence; National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1+2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;https://www.baaic.cn",
        "aff_unique_abbr": "CAS;UCAS;BAAI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.145",
        "title": "IndicBART: A Pre-trained Model for Indic Natural Language Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this paper, we study pre-trained sequence-to-sequence models for a group of related languages, with a focus on Indic languages. We present IndicBART, a multilingual, sequence-to-sequence pre-trained model focusing on 11 Indic languages and English. IndicBART utilizes the orthographic similarity between Indic scripts to improve transfer learning between similar Indic languages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and extreme summarization. Our experiments on NMT and extreme summarization show that a model specific to related languages like IndicBART is competitive with large pre-trained models like mBART50 despite being significantly smaller. It also performs well on very low-resource translation scenarios where languages are not included in pre-training or fine-tuning. Script sharing, multilingual training, and better utilization of limited model capacity contribute to the good performance of the compact IndicBART model.",
        "author": "Raj Dabre; Himani Shrotriya; Anoop Kunchukuttan; Ratish Puduppully; Mitesh Khapra; Pratyush Kumar",
        "authorids": "/r/raj-dabre/; /h/himani-shrotriya/; /a/anoop-kunchukuttan/; /r/ratish-puduppully/; /m/mitesh-m-khapra/; /p/pratyush-kumar/",
        "bibtex": "@inproceedings{dabre-etal-2022-indicbart,\n    title = \"{I}ndic{BART}: A Pre-trained Model for Indic Natural Language Generation\",\n    author = \"Dabre, Raj  and\n      Shrotriya, Himani  and\n      Kunchukuttan, Anoop  and\n      Puduppully, Ratish  and\n      Khapra, Mitesh  and\n      Kumar, Pratyush\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.145/\",\n    doi = \"10.18653/v1/2022.findings-acl.145\",\n    pages = \"1849--1863\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.145.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.145/",
        "pdf_size": 234410,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14685317495199358072&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "National Institute of Information and Communications Technology; IIT Madras+Microsoft; Microsoft+University of Edinburgh; University of Edinburgh; IIT Madras; Microsoft",
        "aff_domain": "nict.go.jp;smail.iitm.ac.in;microsoft.com;sms.ed.ac.uk;cse.iitm.ac.in;microsoft.com",
        "email": "nict.go.jp;smail.iitm.ac.in;microsoft.com;sms.ed.ac.uk;cse.iitm.ac.in;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1+2;2+3;3;1;2",
        "aff_unique_norm": "National Institute of Information and Communications Technology;Indian Institute of Technology Madras;Microsoft;University of Edinburgh",
        "aff_unique_dep": ";;Microsoft Corporation;",
        "aff_unique_url": "https://www.nict.go.jp/;https://www.iitm.ac.in;https://www.microsoft.com;https://www.ed.ac.uk",
        "aff_unique_abbr": "NICT;IITM;Microsoft;Edinburgh",
        "aff_campus_unique_index": "1;;1",
        "aff_campus_unique": ";Madras",
        "aff_country_unique_index": "0;1+2;2+3;3;1;2",
        "aff_country_unique": "Japan;India;United States;United Kingdom"
    },
    {
        "id": "2022.acl-long.257",
        "title": "Inducing Positive Perspectives with Text Reframing",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, Positive Psychology Frames, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work.",
        "author": "Caleb Ziems; Minzhi Li; Anthony Zhang; Diyi Yang",
        "authorids": "/c/caleb-ziems/; /m/minzhi-li/; /a/anthony-zhang/; /d/diyi-yang/",
        "bibtex": "@inproceedings{ziems-etal-2022-inducing,\n    title = \"Inducing Positive Perspectives with Text Reframing\",\n    author = \"Ziems, Caleb  and\n      Li, Minzhi  and\n      Zhang, Anthony  and\n      Yang, Diyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.257/\",\n    doi = \"10.18653/v1/2022.acl-long.257\",\n    pages = \"3682--3700\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.257.pdf",
        "site": "https://aclanthology.org/2022.acl-long.257/",
        "pdf_size": 1032673,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2686470792999713267&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Georgia Institute of Technology\u2020; Georgia Institute of Technology\u2020; Georgia Institute of Technology\u2020; Georgia Institute of Technology\u2020",
        "aff_domain": "gatech.edu;u.nus.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;u.nus.edu;gatech.edu;gatech.edu",
        "github": "https://github.com/GT-SALT/positive-frames",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.585",
        "title": "Inferring Rewards from Language in Context",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In classic instruction following, language like \u201cI\u2019d like the JetBlue flight\u201d maps to actions (e.g., selecting that flight). However, language also conveys information about a user\u2019s underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences. On a new interactive flight\u2013booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that first maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning).",
        "author": "Jessy Lin; Daniel Fried; Dan Klein; Anca Dragan",
        "authorids": "/j/jessy-lin/; /d/daniel-fried/; /d/dan-klein/; /a/anca-dragan/",
        "bibtex": "@inproceedings{lin-etal-2022-inferring,\n    title = \"Inferring Rewards from Language in Context\",\n    author = \"Lin, Jessy  and\n      Fried, Daniel  and\n      Klein, Dan  and\n      Dragan, Anca\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.585/\",\n    doi = \"10.18653/v1/2022.acl-long.585\",\n    pages = \"8546--8560\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.585.pdf",
        "site": "https://aclanthology.org/2022.acl-long.585/",
        "pdf_size": 2136987,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8565592141847032134&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, Berkeley\u2666; Carnegie Mellon University\u2663; University of California, Berkeley\u2666; University of California, Berkeley\u2666",
        "aff_domain": "berkeley.edu;andrew.cmu.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;andrew.cmu.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, Berkeley;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UC Berkeley;CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.4",
        "title": "Input-specific Attention Subnetworks for Adversarial Detection",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Self-attention heads are characteristic of Transformer models and have been well studied for interpretability and pruning. In this work, we demonstrate an altogether different utility of attention heads, namely for adversarial detection. Specifically, we propose a method to construct input-specific attention subnetworks (IAS) from which we extract three features to discriminate between authentic and adversarial inputs. The resultant detector significantly improves (by over 7.5%) the state-of-the-art adversarial detection accuracy for the BERT encoder on 10 NLU datasets with 11 different adversarial attack types. We also demonstrate that our method (a) is more accurate for larger models which are likely to have more spurious correlations and thus vulnerable to adversarial attack, and (b) performs well even with modest training sets of adversarial examples.",
        "author": "Emil Biju; Anirudh Sriram; Pratyush Kumar; Mitesh Khapra",
        "authorids": "/e/emil-biju/; /a/anirudh-sriram/; /p/pratyush-kumar/; /m/mitesh-m-khapra/",
        "bibtex": "@inproceedings{biju-etal-2022-input,\n    title = \"Input-specific Attention Subnetworks for Adversarial Detection\",\n    author = \"Biju, Emil  and\n      Sriram, Anirudh  and\n      Kumar, Pratyush  and\n      Khapra, Mitesh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.4/\",\n    doi = \"10.18653/v1/2022.findings-acl.4\",\n    pages = \"31--44\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.4.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.4/",
        "pdf_size": 684380,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13089327638537041349&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Indian Institute of Technology Madras; Indian Institute of Technology Madras; Indian Institute of Technology Madras; Indian Institute of Technology Madras",
        "aff_domain": "alumni.iitm.ac.in;smail.iitm.ac.in;gmail.com;cse.iitm.ac.in",
        "email": "alumni.iitm.ac.in;smail.iitm.ac.in;gmail.com;cse.iitm.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Madras",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitm.ac.in",
        "aff_unique_abbr": "IIT Madras",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Madras",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.487",
        "title": "Integrating Vectorized Lexical Constraints for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Lexically constrained neural machine translation (NMT), which controls the generation of NMT models with pre-specified constraints, is important in many practical scenarios. Due to the representation gap between discrete constraints and continuous vectors in NMT models, most existing works choose to construct synthetic data or modify the decoding algorithm to impose lexical constraints, treating the NMT model as a black box. In this work, we propose to open this black box by directly integrating the constraints into NMT models. Specifically, we vectorize source and target constraints into continuous keys and values, which can be utilized by the attention modules of NMT models. The proposed integration method is based on the assumption that the correspondence between keys and values in attention modules is naturally suitable for modeling constraint pairs. Experimental results show that our method consistently outperforms several representative baselines on four language pairs, demonstrating the superiority of integrating vectorized lexical constraints.",
        "author": "Shuo Wang; Zhixing Tan; Yang Liu",
        "authorids": "/s/shuo-wang/; /z/zhixing-tan/; /y/yang-liu-ict/",
        "bibtex": "@inproceedings{wang-etal-2022-integrating,\n    title = \"Integrating Vectorized Lexical Constraints for Neural Machine Translation\",\n    author = \"Wang, Shuo  and\n      Tan, Zhixing  and\n      Liu, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.487/\",\n    doi = \"10.18653/v1/2022.acl-long.487\",\n    pages = \"7063--7073\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.487.pdf",
        "site": "https://aclanthology.org/2022.acl-long.487/",
        "pdf_size": 1467463,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18195648506883587303&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology; Institute for AI Industry Research, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Quan Cheng Laboratory+Institute for Guo Qiang, Tsinghua University, Beijing, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute for AI Industry Research, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Quan Cheng Laboratory+Institute for Guo Qiang, Tsinghua University, Beijing, China",
        "aff_domain": "gmail.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "gmail.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/shuo-git/VecConstNMT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+0+2+0;0+1+0+0+2+0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Quan Cheng Laboratory",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;",
        "aff_unique_abbr": "THU;;",
        "aff_campus_unique_index": "0;0+2+0;0+0+2+0",
        "aff_campus_unique": "Beijing;;Shanghai",
        "aff_country_unique_index": "0+0;0+0+0;0+0+0+0+0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2022.acl-long.232",
        "title": "Interactive Word Completion for Plains Cree",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The composition of richly-inflected words in morphologically complex languages can be a challenge for language learners developing literacy. Accordingly, Lane and Bird (2020) proposed a finite state approach which maps prefixes in a language to a set of possible completions up to the next morpheme boundary, for the incremental building of complex words. In this work, we develop an approach to morph-based auto-completion based on a finite state morphological analyzer of Plains Cree (n\u00eahiyaw\u00eawin), showing the portability of the concept to a much larger, more complete morphological transducer. Additionally, we propose and compare various novel ranking strategies on the morph auto-complete output. The best weighting scheme ranks the target completion in the top 10 results in 64.9% of queries, and in the top 50 in 73.9% of queries.",
        "author": "William Lane; Atticus Harrigan; Antti Arppe",
        "authorids": "/w/william-lane/; /a/atticus-harrigan/; /a/antti-arppe/",
        "bibtex": "@inproceedings{lane-etal-2022-interactive,\n    title = \"Interactive Word Completion for {P}lains {C}ree\",\n    author = \"Lane, William  and\n      Harrigan, Atticus  and\n      Arppe, Antti\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.232/\",\n    doi = \"10.18653/v1/2022.acl-long.232\",\n    pages = \"3284--3294\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.232.pdf",
        "site": "https://aclanthology.org/2022.acl-long.232/",
        "pdf_size": 483893,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7171669741485029788&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Northern Institute, Charles Darwin University; Alberta Language Technology Lab, University of Alberta; Alberta Language Technology Lab, University of Alberta",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Charles Darwin University;University of Alberta",
        "aff_unique_dep": ";Alberta Language Technology Lab",
        "aff_unique_url": "https://www.cdu.edu.au;https://www.ualberta.ca",
        "aff_unique_abbr": "CDU;UAlberta",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Northern Institute;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Australia;Canada"
    },
    {
        "id": "2022.acl-long.579",
        "title": "Internet-Augmented Dialogue Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
        "author": "Mojtaba Komeili; Kurt Shuster; Jason Weston",
        "authorids": "/m/mojtaba-komeili/; /k/kurt-shuster/; /j/jason-weston/",
        "bibtex": "@inproceedings{komeili-etal-2022-internet,\n    title = \"{I}nternet-Augmented Dialogue Generation\",\n    author = \"Komeili, Mojtaba  and\n      Shuster, Kurt  and\n      Weston, Jason\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.579/\",\n    doi = \"10.18653/v1/2022.acl-long.579\",\n    pages = \"8460--8478\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.579.pdf",
        "site": "https://aclanthology.org/2022.acl-long.579/",
        "pdf_size": 6354603,
        "gs_citation": 391,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15947258389458697933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Facebook AI Research Labs; Facebook AI Research Labs; Facebook AI Research Labs",
        "aff_domain": "fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Facebook AI Research Labs",
        "aff_unique_url": "https://research.facebook.com",
        "aff_unique_abbr": "FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.496",
        "title": "Interpretability for Language Learners Using Example-Based Grammatical Error Correction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Grammatical Error Correction (GEC) should not focus only on high accuracy of corrections but also on interpretability for language learning. However, existing neural-based GEC models mainly aim at improving accuracy, and their interpretability has not been explored.A promising approach for improving interpretability is an example-based method, which uses similar retrieved examples to generate corrections. In addition, examples are beneficial in language learning, helping learners understand the basis of grammatically incorrect/correct texts and improve their confidence in writing. Therefore, we hypothesize that incorporating an example-based method into GEC can improve interpretability as well as support language learners. In this study, we introduce an Example-Based GEC (EB-GEC) that presents examples to language learners as a basis for a correction result. The examples consist of pairs of correct and incorrect sentences similar to a given input and its predicted correction. Experiments demonstrate that the examples presented by EB-GEC help language learners decide to accept or refuse suggestions from the GEC output. Furthermore, the experiments also show that retrieved examples improve the accuracy of corrections.",
        "author": "Masahiro Kaneko; Sho Takase; Ayana Niwa; Naoaki Okazaki",
        "authorids": "/m/masahiro-kaneko/; /s/sho-takase/; /a/ayana-niwa/; /n/naoaki-okazaki/",
        "bibtex": "@inproceedings{kaneko-etal-2022-interpretability,\n    title = \"Interpretability for Language Learners Using Example-Based Grammatical Error Correction\",\n    author = \"Kaneko, Masahiro  and\n      Takase, Sho  and\n      Niwa, Ayana  and\n      Okazaki, Naoaki\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.496/\",\n    doi = \"10.18653/v1/2022.acl-long.496\",\n    pages = \"7176--7187\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.496.pdf",
        "site": "https://aclanthology.org/2022.acl-long.496/",
        "pdf_size": 634580,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5085992914929289415&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;c.titech.ac.jp",
        "email": "nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;c.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.findings-acl.305",
        "title": "Interpretable Research Replication Prediction via Variational Contextual Consistency Sentence Masking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Research Replication Prediction (RRP) is the task of predicting whether a published research result can be replicated or not. Building an interpretable neural text classifier for RRP promotes the understanding of why a research paper is predicted as replicable or non-replicable and therefore makes its real-world application more reliable and trustworthy. However, the prior works on model interpretation mainly focused on improving the model interpretability at the word/phrase level, which are insufficient especially for long research papers in RRP. Furthermore, the existing methods cannot utilize a large size of unlabeled dataset to further improve the model interpretability. To address these limitations, we aim to build an interpretable neural model which can provide sentence-level explanations and apply weakly supervised approach to further leverage the large corpus of unlabeled datasets to boost the interpretability in addition to improving prediction performance as existing works have done. In this work, we propose the Variational Contextual Consistency Sentence Masking (VCCSM) method to automatically extract key sentences based on the context in the classifier, using both labeled and unlabeled datasets. Results of our experiments on RRP along with European Convention of Human Rights (ECHR) datasets demonstrate that VCCSM is able to improve the model interpretability for the long document classification tasks using the area over the perturbation curve and post-hoc accuracy as evaluation metrics.",
        "author": "Tianyi Luo; Rui Meng; Xin Wang; Yang Liu",
        "authorids": "/t/tianyi-luo/; /r/rui-meng/; /x/xin-wang/; /y/yang-liu-umich/",
        "bibtex": "@inproceedings{luo-etal-2022-interpretable,\n    title = \"Interpretable Research Replication Prediction via Variational Contextual Consistency Sentence Masking\",\n    author = \"Luo, Tianyi  and\n      Meng, Rui  and\n      Wang, Xin  and\n      Liu, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.305/\",\n    doi = \"10.18653/v1/2022.findings-acl.305\",\n    pages = \"3864--3876\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.305.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.305/",
        "pdf_size": 573634,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4886634807685098685&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science and Engineering, University of California, Santa Cruz; Lawrence Berkeley National Laboratory, University of California, Berkeley; Computer Science and Engineering, University of California, Santa Cruz; Computer Science and Engineering, University of California, Santa Cruz",
        "aff_domain": "ucsc.edu;lbl.gov;ucsc.edu;ucsc.edu",
        "email": "ucsc.edu;lbl.gov;ucsc.edu;ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, Santa Cruz;Lawrence Berkeley National Laboratory",
        "aff_unique_dep": "Computer Science and Engineering;",
        "aff_unique_url": "https://www.ucsc.edu;https://www.lbl.gov",
        "aff_unique_abbr": "UCSC;LBL",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Santa Cruz;Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.470",
        "title": "Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Character-level information is included in many NLP models, but evaluating the information encoded in character representations is an open issue. We leverage perceptual representations in the form of shape, sound, and color embeddings and perform a representational similarity analysis to evaluate their correlation with textual representations in five languages. This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts. We further develop a set of probing classifiers to intrinsically evaluate what phonological information is encoded in character embeddings. Our results suggest that information on features such as voicing are embedded in both LSTM and transformer-based representations.",
        "author": "Sidsel Boldsen; Manex Agirrezabal; Nora Hollenstein",
        "authorids": "/s/sidsel-boldsen/; /m/manex-agirrezabal/; /n/nora-hollenstein/",
        "bibtex": "@inproceedings{boldsen-etal-2022-interpreting,\n    title = \"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color\",\n    author = \"Boldsen, Sidsel  and\n      Agirrezabal, Manex  and\n      Hollenstein, Nora\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.470/\",\n    doi = \"10.18653/v1/2022.acl-long.470\",\n    pages = \"6819--6836\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.470.pdf",
        "site": "https://aclanthology.org/2022.acl-long.470/",
        "pdf_size": 1057400,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12022328061514948052&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Centre for Language Technology+Department of Nordic Studies and Linguistics+University of Copenhagen; Centre for Language Technology+Department of Nordic Studies and Linguistics+University of Copenhagen; Centre for Language Technology+Department of Nordic Studies and Linguistics+University of Copenhagen",
        "aff_domain": "hum.ku.dk;hum.ku.dk;hum.ku.dk",
        "email": "hum.ku.dk;hum.ku.dk;hum.ku.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+1;0+1+1;0+1+1",
        "aff_unique_norm": "Centre for Language Technology;University of Copenhagen",
        "aff_unique_dep": "Language Technology;Department of Nordic Studies and Linguistics",
        "aff_unique_url": ";https://nordisk.sdu.dk",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1+1;1+1;1+1",
        "aff_country_unique": ";Denmark"
    },
    {
        "id": "2022.findings-acl.315",
        "title": "Interpreting the Robustness of Neural NLP Models to Textual Perturbations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data. However, it is still unclear why models are less robust to some perturbations than others. In this work, we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence). We further give a causal justification for the learnability metric. We conduct extensive experiments with four prominent NLP models \u2014 TextRNN, BERT, RoBERTa and XLNet \u2014 over eight types of textual perturbations on three datasets. We show that a model which is better at identifying a perturbation (higher learnability) becomes worse at ignoring such a perturbation at test time (lower robustness), providing empirical support for our hypothesis.",
        "author": "Yunxiang Zhang; Liangming Pan; Samson Tan; Min-Yen Kan",
        "authorids": "/y/yunxiang-zhang/; /l/liangming-pan/; /s/samson-tan/; /m/min-yen-kan/",
        "bibtex": "@inproceedings{zhang-etal-2022-interpreting,\n    title = \"Interpreting the Robustness of Neural {NLP} Models to Textual Perturbations\",\n    author = \"Zhang, Yunxiang  and\n      Pan, Liangming  and\n      Tan, Samson  and\n      Kan, Min-Yen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.315/\",\n    doi = \"10.18653/v1/2022.findings-acl.315\",\n    pages = \"3993--4007\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.315.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.315/",
        "pdf_size": 1130096,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5491007545034738416&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 11,
        "aff": "Wangxuan Institute of Computer Technology, Peking University; School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore",
        "aff_domain": "pku.edu.cn;u.nus.edu;comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "pku.edu.cn;u.nus.edu;comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Peking University;National University of Singapore",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;School of Computing",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "PKU;NUS",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Singapore",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.findings-acl.53",
        "title": "Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.",
        "author": "Yutai Hou; Cheng Chen; Xianzhen Luo; Bohan Li; Wanxiang Che",
        "authorids": "/y/yutai-hou/; /c/cheng-chen/; /x/xianzhen-luo/; /b/bohan-li/; /w/wanxiang-che/",
        "bibtex": "@inproceedings{hou-etal-2022-inverse,\n    title = \"Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging\",\n    author = \"Hou, Yutai  and\n      Chen, Cheng  and\n      Luo, Xianzhen  and\n      Li, Bohan  and\n      Che, Wanxiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.53/\",\n    doi = \"10.18653/v1/2022.findings-acl.53\",\n    pages = \"637--647\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.53.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.53/",
        "pdf_size": 738901,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16077353751931014994&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "Research Center for Social Computing and Information Retrieval",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Harbin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.14",
        "title": "Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Current practices in metric evaluation focus on one single dataset, e.g., Newstest dataset in each year\u2019s WMT Metrics Shared Task. However, in this paper, we qualitatively and quantitatively show that the performances of metrics are sensitive to data. The ranking of metrics varies when the evaluation is conducted on different datasets. Then this paper further investigates two potential hypotheses, i.e., insignificant data points and the deviation of i.i.d assumption, which may take responsibility for the issue of data variance. In conclusion, our findings suggest that when evaluating automatic translation metrics, researchers should take data variance into account and be cautious to report the results on unreliable datasets, because it may leads to inconsistent results with most of the other datasets.",
        "author": "Jiannan Xiang; Huayang Li; Yahui Liu; Lemao Liu; Guoping Huang; Defu Lian; Shuming Shi",
        "authorids": "/j/jiannan-xiang/; /h/huayang-li/; /y/yahui-liu/; /l/lemao-liu/; /g/guoping-huang/; /d/defu-lian/; /s/shuming-shi/",
        "bibtex": "@inproceedings{xiang-etal-2022-investigating,\n    title = \"Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics\",\n    author = \"Xiang, Jiannan  and\n      Li, Huayang  and\n      Liu, Yahui  and\n      Liu, Lemao  and\n      Huang, Guoping  and\n      Lian, Defu  and\n      Shi, Shuming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.14/\",\n    doi = \"10.18653/v1/2022.findings-acl.14\",\n    pages = \"150--157\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.14.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.14/",
        "pdf_size": 205957,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16893774775737057490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University; Nara Institute of Science and Technology; University of Trento, Italy; Tencent AI Lab; Tencent AI Lab; University of Science and Techology of China; Tencent AI Lab",
        "aff_domain": "cs.cmu.edu;is.naist.jp;unitn.it;gmail.com;tencent.com;tencent.com; ",
        "email": "cs.cmu.edu;is.naist.jp;unitn.it;gmail.com;tencent.com;tencent.com; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;3;4;3",
        "aff_unique_norm": "Carnegie Mellon University;Nara Institute of Science and Technology;University of Trento;Tencent;University of Science and Technology of China",
        "aff_unique_dep": ";;;Tencent AI Lab;",
        "aff_unique_url": "https://www.cmu.edu;https://www.nist.go.jp;https://www.unitn.it;https://ai.tencent.com;https://www.ustc.edu.cn",
        "aff_unique_abbr": "CMU;NIST;UniTN;Tencent AI Lab;USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;3;3;3;3",
        "aff_country_unique": "United States;Japan;Italy;China"
    },
    {
        "id": "2022.acl-long.243",
        "title": "Investigating Failures of Automatic Translationin the Case of Unambiguous Gender",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer-based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks. Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by current state-of-the-art NMT models with regards to translating from a language that doesn\u2019t mark gender on nouns into others that do. We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking, no tested model was able to accurately gender occupation nouns systematically. We release an evaluation scheme and dataset for measuring the ability of NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences. Our dataset translates from an English source into 20 languages from several different language families. With the availability of this dataset, our hope is that the NMT community can iterate on solutions for this class of especially egregious errors.",
        "author": "Adithya Renduchintala; Adina Williams",
        "authorids": "/a/adithya-renduchintala/; /a/adina-williams/",
        "bibtex": "@inproceedings{renduchintala-williams-2022-investigating,\n    title = \"Investigating Failures of Automatic Translationin the Case of Unambiguous Gender\",\n    author = \"Renduchintala, Adithya  and\n      Williams, Adina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.243/\",\n    doi = \"10.18653/v1/2022.acl-long.243\",\n    pages = \"3454--3469\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.243.pdf",
        "site": "https://aclanthology.org/2022.acl-long.243/",
        "pdf_size": 266625,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10703858156503226892&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Facebook AI; Facebook AI Research",
        "aff_domain": "fb.com;fb.com",
        "email": "fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Facebook AI",
        "aff_unique_url": "https://www.facebook.com",
        "aff_unique_abbr": "Facebook AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.146",
        "title": "Investigating Non-local Features for Neural Constituency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Thanks to the strong representation power of neural encoders, neural chart-based parsers have achieved highly competitive performance by using local features. Recently, it has been shown that non-local features in CRF structures lead to improvements. In this paper, we investigate injecting non-local features into the training process of a local span-based parser, by predicting constituent n-gram non-local patterns and ensuring consistency between non-local patterns and local constituents. Results show that our simple method gives better results than the self-attentive parser on both PTB and CTB. Besides, our method achieves state-of-the-art BERT-based performance on PTB (95.92 F1) and strong performance on CTB (92.31 F1). Our parser also outperforms the self-attentive parser in multi-lingual and zero-shot cross-domain settings.",
        "author": "Leyang Cui; Sen Yang; Yue Zhang",
        "authorids": "/l/leyang-cui/; /s/sen-yang/; /y/yue-zhang/",
        "bibtex": "@inproceedings{cui-etal-2022-investigating,\n    title = \"Investigating Non-local Features for Neural Constituency Parsing\",\n    author = \"Cui, Leyang  and\n      Yang, Sen  and\n      Zhang, Yue\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.146/\",\n    doi = \"10.18653/v1/2022.acl-long.146\",\n    pages = \"2065--2075\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.146.pdf",
        "site": "https://aclanthology.org/2022.acl-long.146/",
        "pdf_size": 595120,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16735239029414932622&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Zhejiang University + School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; School of Engineering, Westlake University",
        "aff_domain": "westlake.edu.cn;gmail.com;westlake.edu.cn",
        "email": "westlake.edu.cn;gmail.com;westlake.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;1+2;1",
        "aff_unique_norm": "Zhejiang University;Westlake University;Westlake Institute for Advanced Study",
        "aff_unique_dep": ";School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.westlake.edu.cn;http://www.wias.org.cn/",
        "aff_unique_abbr": "ZJU;;WIAS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.158",
        "title": "Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In order to equip NLP systems with \u2018selective prediction\u2019 capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.",
        "author": "Neeraj Varshney; Swaroop Mishra; Chitta Baral",
        "authorids": "/n/neeraj-varshney/; /s/swaroop-mishra/; /c/chitta-baral/",
        "bibtex": "@inproceedings{varshney-etal-2022-investigating,\n    title = \"Investigating Selective Prediction Approaches Across Several Tasks in {IID}, {OOD}, and Adversarial Settings\",\n    author = \"Varshney, Neeraj  and\n      Mishra, Swaroop  and\n      Baral, Chitta\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.158/\",\n    doi = \"10.18653/v1/2022.findings-acl.158\",\n    pages = \"1995--2002\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.158.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.158/",
        "pdf_size": 339422,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13886093287473714234&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.50",
        "title": "Investigating person-specific errors in chat-oriented dialogue systems",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Creating chatbots to behave like real people is important in terms of believability. Errors in general chatbots and chatbots that follow a rough persona have been studied, but those in chatbots that behave like real people have not been thoroughly investigated. We collected a large amount of user interactions of a generation-based chatbot trained from large-scale dialogue data of a specific character, i.e., target person, and analyzed errors related to that person. We found that person-specific errors can be divided into two types: errors in attributes and those in relations, each of which can be divided into two levels: self and other. The correspondence with an existing taxonomy of errors was also investigated, and person-specific errors that should be addressed in the future were clarified.",
        "author": "Koh Mitsuda; Ryuichiro Higashinaka; Tingxuan Li; Sen Yoshida",
        "authorids": "/k/koh-mitsuda/; /r/ryuichiro-higashinaka/; /t/tingxuan-li/; /s/sen-yoshida/",
        "bibtex": "@inproceedings{mitsuda-etal-2022-investigating,\n    title = \"Investigating person-specific errors in chat-oriented dialogue systems\",\n    author = \"Mitsuda, Koh  and\n      Higashinaka, Ryuichiro  and\n      Li, Tingxuan  and\n      Yoshida, Sen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.50/\",\n    doi = \"10.18653/v1/2022.acl-short.50\",\n    pages = \"464--469\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.50.pdf",
        "site": "https://aclanthology.org/2022.acl-short.50/",
        "pdf_size": 107009,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5304637124957518924&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "NTT Corporation, Japan; NTT Corporation, Japan; University of Tsukuba, Japan; NTT Corporation, Japan",
        "aff_domain": "hco.ntt.co.jp;hco.ntt.co.jp;s.tsukuba.ac.jp;hco.ntt.co.jp",
        "email": "hco.ntt.co.jp;hco.ntt.co.jp;s.tsukuba.ac.jp;hco.ntt.co.jp",
        "github": "https://github.com/nttcslab/japanese-dialog-transformers",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "NTT Corporation;University of Tsukuba",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntt.co.jp;https://www.tsukuba.ac.jp",
        "aff_unique_abbr": "NTT;UT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.acl-long.269",
        "title": "Is Attention Explanation? An Introduction to the Debate",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.",
        "author": "Adrien Bibal; R\u00e9mi Cardon; David Alfter; Rodrigo Wilkens; Xiaoou Wang; Thomas Fran\u00e7ois; Patrick Watrin",
        "authorids": "/a/adrien-bibal/; /r/remi-cardon/; /d/david-alfter/; /r/rodrigo-wilkens/; /x/xiaoou-wang/; /t/thomas-francois/; /p/patrick-watrin/",
        "bibtex": "@inproceedings{bibal-etal-2022-attention,\n    title = \"Is Attention Explanation? An Introduction to the Debate\",\n    author = \"Bibal, Adrien  and\n      Cardon, R{\\'e}mi  and\n      Alfter, David  and\n      Wilkens, Rodrigo  and\n      Wang, Xiaoou  and\n      Fran{\\c{c}}ois, Thomas  and\n      Watrin, Patrick\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.269/\",\n    doi = \"10.18653/v1/2022.acl-long.269\",\n    pages = \"3889--3900\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.269.pdf",
        "site": "https://aclanthology.org/2022.acl-long.269/",
        "pdf_size": 264307,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2687726811382191388&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "CENTAL, IL&C, University of Louvain, Belgium; CENTAL, IL&C, University of Louvain, Belgium; CENTAL, IL&C, University of Louvain, Belgium; CENTAL, IL&C, University of Louvain, Belgium; CENTAL, IL&C, University of Louvain, Belgium; CENTAL, IL&C, University of Louvain, Belgium; CENTAL, IL&C, University of Louvain, Belgium",
        "aff_domain": "uclouvain.be;uclouvain.be;uclouvain.be;uclouvain.be;uclouvain.be;uclouvain.be;uclouvain.be",
        "email": "uclouvain.be;uclouvain.be;uclouvain.be;uclouvain.be;uclouvain.be;uclouvain.be;uclouvain.be",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Louvain",
        "aff_unique_dep": "CENTAL, IL&C",
        "aff_unique_url": "https://www.uclouvain.be",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2022.acl-long.501",
        "title": "Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation. We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow\u2014such as redundancy, commonsense errors, and incoherence\u2014are identified through several rounds of crowd annotation experiments without a predefined ontology. We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.",
        "author": "Yao Dou; Maxwell Forbes; Rik Koncel-Kedziorski; Noah A. Smith; Yejin Choi",
        "authorids": "/y/yao-dou/; /m/maxwell-forbes/; /r/rik-koncel-kedziorski/; /n/noah-a-smith/; /y/yejin-choi/",
        "bibtex": "@inproceedings{dou-etal-2022-gpt,\n    title = \"Is {GPT}-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text\",\n    author = \"Dou, Yao  and\n      Forbes, Maxwell  and\n      Koncel-Kedziorski, Rik  and\n      Smith, Noah A.  and\n      Choi, Yejin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.501/\",\n    doi = \"10.18653/v1/2022.acl-long.501\",\n    pages = \"7250--7274\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.501.pdf",
        "site": "https://aclanthology.org/2022.acl-long.501/",
        "pdf_size": 3056236,
        "gs_citation": 183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12954889821320210284&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for AI; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for AI; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for AI; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for AI",
        "aff_domain": "cs.washington.edu;cs.washington.edu;uw.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;uw.edu;cs.washington.edu;cs.washington.edu",
        "github": "https://yao-dou.github.io/scarecrow/",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0;0+1;0+1",
        "aff_unique_norm": "University of Washington;Allen Institute for AI",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.262",
        "title": "IsoScore: Measuring the Uniformity of Embedding Space Utilization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The recent success of distributed word representations has led to an increased interest in analyzing the properties of their spatial distribution. Several studies have suggested that contextualized word embedding models do not isotropically project tokens into vector space. However, current methods designed to measure isotropy, such as average random cosine similarity and the partition score, have not been thoroughly analyzed and are not appropriate for measuring isotropy. We propose IsoScore: a novel tool that quantifies the degree to which a point cloud uniformly utilizes the ambient vector space. Using rigorously designed tests, we demonstrate that IsoScore is the only tool available in the literature that accurately measures how uniformly distributed variance is across dimensions in vector space. Additionally, we use IsoScore to challenge a number of recent conclusions in the NLP literature that have been derived using brittle metrics of isotropy. We caution future studies from using existing tools to measure isotropy in contextualized embedding space as resulting conclusions will be misleading or altogether inaccurate.",
        "author": "William Rudman; Nate Gillman; Taylor Rayne; Carsten Eickhoff",
        "authorids": "/w/william-rudman/; /n/nate-gillman/; /t/taylor-rayne/; /c/carsten-eickhoff/",
        "bibtex": "@inproceedings{rudman-etal-2022-isoscore,\n    title = \"{I}so{S}core: Measuring the Uniformity of Embedding Space Utilization\",\n    author = \"Rudman, William  and\n      Gillman, Nate  and\n      Rayne, Taylor  and\n      Eickhoff, Carsten\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.262/\",\n    doi = \"10.18653/v1/2022.findings-acl.262\",\n    pages = \"3325--3339\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.262.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.262/",
        "pdf_size": 1362207,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13351555704670970679&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Brown University\u2020; Department of Mathematics, Brown University\u2021; Quest University\u2217; Department of Computer Science, Brown University\u2020",
        "aff_domain": "brown.edu;brown.edu;questu.ca;brown.edu",
        "email": "brown.edu;brown.edu;questu.ca;brown.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Brown University;Quest University",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.brown.edu;https://www.questu.ca",
        "aff_unique_abbr": "Brown;QuestU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2022.acl-long.54",
        "title": "It is AI\u2019s Turn to Ask Humans a Question: Question-Answer Pair Generation for Children\u2019s Story Books",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing question answering (QA) techniques are created mainly to answer questions asked by humans. But in educational applications, teachers often need to decide what questions they should ask, in order to help students to improve their narrative understanding capabilities. We design an automated question-answer generation (QAG) system for this education scenario: given a story book at the kindergarten to eighth-grade level as input, our system can automatically generate QA pairs that are capable of testing a variety of dimensions of a student\u2019s comprehension skills. Our proposed QAG model architecture is demonstrated using a new expert-annotated FairytaleQA dataset, which has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and human evaluations show that our model outperforms state-of-the-art QAG baseline systems. On top of our QAG system, we also start to build an interactive story-telling application for the future real-world deployment in this educational scenario.",
        "author": "Bingsheng Yao; Dakuo Wang; Tongshuang Wu; Zheng Zhang; Toby Jia-Jun Li; Mo Yu; Ying Xu",
        "authorids": "/b/bingsheng-yao/; /d/dakuo-wang/; /t/tongshuang-wu/; /z/zheng-zhang/; /t/toby-jia-jun-li/; /m/mo-yu/; /y/ying-xu/",
        "bibtex": "@inproceedings{yao-etal-2022-ais,\n    title = \"It is {AI}`s Turn to Ask Humans a Question: Question-Answer Pair Generation for Children`s Story Books\",\n    author = \"Yao, Bingsheng  and\n      Wang, Dakuo  and\n      Wu, Tongshuang  and\n      Zhang, Zheng  and\n      Li, Toby Jia-Jun  and\n      Yu, Mo  and\n      Xu, Ying\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.54/\",\n    doi = \"10.18653/v1/2022.acl-long.54\",\n    pages = \"731--744\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.54.pdf",
        "site": "https://aclanthology.org/2022.acl-long.54/",
        "pdf_size": 1582649,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1186162844123660181&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Rensselaer Polytechnic Institute; IBM Research; University of Washington; University of Notre Dame; University of Notre Dame; WeChat AI, Tencent; University of California Irvine",
        "aff_domain": "rpi.edu;ibm.com; ; ; ; ; ",
        "email": "rpi.edu;ibm.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;3;4;5",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;IBM;University of Washington;University of Notre Dame;Tencent;University of California, Irvine",
        "aff_unique_dep": ";IBM Research;;;WeChat AI;",
        "aff_unique_url": "https://www.rpi.edu;https://www.ibm.com/research;https://www.washington.edu;https://www.nd.edu;https://www.tencent.com;https://www.uci.edu",
        "aff_unique_abbr": "RPI;IBM;UW;Notre Dame;Tencent;UCI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Irvine",
        "aff_country_unique_index": "0;0;0;0;0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.7",
        "title": "JointCL: A Joint Contrastive Learning Framework for Zero-Shot Stance Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Zero-shot stance detection (ZSSD) aims to detect the stance for an unseen target during the inference stage. In this paper, we propose a joint contrastive learning (JointCL) framework, which consists of stance contrastive learning and target-aware prototypical graph contrastive learning. Specifically, a stance contrastive learning strategy is employed to better generalize stance features for unseen targets. Further, we build a prototypical graph for each instance to learn the target-based representation, in which the prototypes are deployed as a bridge to share the graph structures between the known targets and the unseen ones. Then a novel target-aware prototypical graph contrastive learning strategy is devised to generalize the reasoning ability of target-based stance representations to the unseen targets. Extensive experiments on three benchmark datasets show that the proposed approach achieves state-of-the-art performance in the ZSSD task.",
        "author": "Bin Liang; Qinglin Zhu; Xiang Li; Min Yang; Lin Gui; Yulan He; Ruifeng Xu",
        "authorids": "/b/bin-liang/; /q/qinglin-zhu/; /x/xiang-li/; /m/min-yang/; /l/lin-gui/; /y/yulan-he/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{liang-etal-2022-jointcl,\n    title = \"{J}oint{CL}: A Joint Contrastive Learning Framework for Zero-Shot Stance Detection\",\n    author = \"Liang, Bin  and\n      Zhu, Qinglin  and\n      Li, Xiang  and\n      Yang, Min  and\n      Gui, Lin  and\n      He, Yulan  and\n      Xu, Ruifeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.7/\",\n    doi = \"10.18653/v1/2022.acl-long.7\",\n    pages = \"81--91\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.7.pdf",
        "site": "https://aclanthology.org/2022.acl-long.7/",
        "pdf_size": 3128318,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18120069937111060389&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China+Joint Lab of HITSZ and China Merchants Securities, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; SIAT, Chinese Academy of Sciences, Shenzhen, China; Department of Computer Science, University of Warwick, U.K+The Alan Turing Institute, UK; Department of Computer Science, University of Warwick, U.K+The Alan Turing Institute, UK+Peng Cheng Laboratory, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China+Peng Cheng Laboratory, Shenzhen, China",
        "aff_domain": "stu.hit.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;siat.ac.cn;warwick.ac.uk;warwick.ac.uk;hit.edu.cn",
        "email": "stu.hit.edu.cn;stu.hit.edu.cn;stu.hit.edu.cn;siat.ac.cn;warwick.ac.uk;warwick.ac.uk;hit.edu.cn",
        "github": "https://github.com/HITSZ-HLT/JointCL",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0;0;2;3+4;3+4+5;0+5",
        "aff_unique_norm": "Harbin Institute of Technology;Harbin Institute of Technology Shenzhen (HITSZ);Shenzhen Institute of Advanced Technology;University of Warwick;Alan Turing Institute;Pengcheng Laboratory",
        "aff_unique_dep": "School of Computer Science and Technology;Joint Lab;;Department of Computer Science;;Peng Cheng Laboratory",
        "aff_unique_url": "http://www.hit.edu.cn/;http://en.hitsz.edu.cn/;http://www.siat.ac.cn;https://warwick.ac.uk;https://www.turing.ac.uk;",
        "aff_unique_abbr": "HIT;HITSZ;SIAT;Warwick;ATI;",
        "aff_campus_unique_index": "0+0;0;0;0;;0;0+0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0;0;0;1+1;1+1+0;0+0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2022.acl-long.419",
        "title": "Just Rank: Rethinking Evaluation with Word and Sentence Similarities",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Word and sentence embeddings are useful feature representations in natural language processing. However, intrinsic evaluation for embeddings lags far behind, and there has been no significant update since the past decade. Word and sentence similarity tasks have become the de facto evaluation method. It leads models to overfit to such evaluations, negatively impacting embedding models\u2019 development. This paper first points out the problems using semantic similarity as the gold standard for word and sentence embedding evaluations. Further, we propose a new intrinsic evaluation method called EvalRank, which shows a much stronger correlation with downstream tasks. Extensive experiments are conducted based on 60+ models and popular datasets to certify our judgments. Finally, the practical evaluation toolkit is released for future benchmarking purposes.",
        "author": "Bin Wang; C.-C. Jay Kuo; Haizhou Li",
        "authorids": "/b/bin-wang/; /c/c-c-jay-kuo/; /h/haizhou-li/",
        "bibtex": "@inproceedings{wang-etal-2022-just,\n    title = \"Just Rank: Rethinking Evaluation with Word and Sentence Similarities\",\n    author = \"Wang, Bin  and\n      Kuo, C.-C. Jay  and\n      Li, Haizhou\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.419/\",\n    doi = \"10.18653/v1/2022.acl-long.419\",\n    pages = \"6060--6077\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.419.pdf",
        "site": "https://aclanthology.org/2022.acl-long.419/",
        "pdf_size": 396364,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3553286966489129709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "National University of Singapore, Singapore; University of Southern California, USA; The Chinese University of Hong Kong, Shenzhen, China + Kriston AI, China",
        "aff_domain": "gmail.com; ; ",
        "email": "gmail.com; ; ",
        "github": "https://github.com/BinWang28/EvalRank-Embedding-Evaluation",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "National University of Singapore;University of Southern California;Chinese University of Hong Kong;Kriston AI",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.usc.edu;https://www.cuhk.edu.cn;",
        "aff_unique_abbr": "NUS;USC;CUHK;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;1;2+2",
        "aff_country_unique": "Singapore;United States;China"
    },
    {
        "id": "2022.acl-long.340",
        "title": "KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current Open-Domain Question Answering (ODQA) models typically include a retrieving module and a reading module, where the retriever selects potentially relevant passages from open-source documents for a given question, and the reader produces an answer based on the retrieved passages. The recently proposed Fusion-in-Decoder (FiD) framework is a representative example, which is built on top of a dense passage retriever and a generative reader, achieving the state-of-the-art performance. In this paper we further improve the FiD approach by introducing a knowledge-enhanced version, namely KG-FiD. Our new model uses a knowledge graph to establish the structural relationship among the retrieved passages, and a graph neural network (GNN) to re-rank the passages and select only a top few for further processing. Our experiments on common ODQA benchmark datasets (Natural Questions and TriviaQA) demonstrate that KG-FiD can achieve comparable or better performance in answer prediction than FiD, with less than 40% of the computation cost.",
        "author": "Donghan Yu; Chenguang Zhu; Yuwei Fang; Wenhao Yu; Shuohang Wang; Yichong Xu; Xiang Ren; Yiming Yang; Michael Zeng",
        "authorids": "/d/donghan-yu/; /c/chenguang-zhu/; /y/yuwei-fang/; /w/wenhao-yu/; /s/shuohang-wang/; /y/yichong-xu/; /x/xiang-ren/; /y/yiming-yang/; /m/michael-zeng/",
        "bibtex": "@inproceedings{yu-etal-2022-kg,\n    title = \"{KG}-{F}i{D}: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering\",\n    author = \"Yu, Donghan  and\n      Zhu, Chenguang  and\n      Fang, Yuwei  and\n      Yu, Wenhao  and\n      Wang, Shuohang  and\n      Xu, Yichong  and\n      Ren, Xiang  and\n      Yang, Yiming  and\n      Zeng, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.340/\",\n    doi = \"10.18653/v1/2022.acl-long.340\",\n    pages = \"4961--4974\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.340.pdf",
        "site": "https://aclanthology.org/2022.acl-long.340/",
        "pdf_size": 644375,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2492411683198541373&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Carnegie Mellon University; Microsoft Cognitive Services Research Group; University of Notre Dame; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; University of Southern California; Carnegie Mellon University; Microsoft Cognitive Services Research Group",
        "aff_domain": "cs.cmu.edu;microsoft.com; ; ; ; ; ; ; ",
        "email": "cs.cmu.edu;microsoft.com; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;2;1;1;1;3;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Microsoft;University of Notre Dame;University of Southern California",
        "aff_unique_dep": ";Cognitive Services Research Group;;",
        "aff_unique_url": "https://www.cmu.edu;https://www.microsoft.com;https://www.nd.edu;https://www.usc.edu",
        "aff_unique_abbr": "CMU;Microsoft;Notre Dame;USC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.352",
        "title": "KNN-Contrastive Learning for Out-of-Domain Intent Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The Out-of-Domain (OOD) intent classification is a basic and challenging task for dialogue systems. Previous methods commonly restrict the region (in feature space) of In-domain (IND) intent features to be compact or simply-connected implicitly, which assumes no OOD intents reside, to learn discriminative semantic features. Then the distribution of the IND intent features is often assumed to obey a hypothetical distribution (Gaussian mostly) and samples outside this distribution are regarded as OOD samples. In this paper, we start from the nature of OOD intent classification and explore its optimization objective. We further propose a simple yet effective method, named KNN-contrastive learning. Our approach utilizes k-nearest neighbors (KNN) of IND intents to learn discriminative semantic features that are more conducive to OOD detection. Notably, the density-based novelty detection algorithm is so well-grounded in the essence of our method that it is reasonable to use it as the OOD detection algorithm without making any requirements for the feature distribution. Extensive experiments on four public datasets show that our approach can not only enhance the OOD detection performance substantially but also improve the IND intent classification while requiring no restrictions on feature distribution.",
        "author": "Yunhua Zhou; Peiju Liu; Xipeng Qiu",
        "authorids": "/y/yunhua-zhou/; /p/peiju-liu/; /x/xipeng-qiu/",
        "bibtex": "@inproceedings{zhou-etal-2022-knn,\n    title = \"{KNN}-Contrastive Learning for Out-of-Domain Intent Classification\",\n    author = \"Zhou, Yunhua  and\n      Liu, Peiju  and\n      Qiu, Xipeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.352/\",\n    doi = \"10.18653/v1/2022.acl-long.352\",\n    pages = \"5129--5141\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.352.pdf",
        "site": "https://aclanthology.org/2022.acl-long.352/",
        "pdf_size": 2294150,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=836183377042488989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science, Fudan University + Shanghai Collaborative Innovation Center of Intelligent Visual Computing; School of Computer Science, Fudan University + Shanghai Collaborative Innovation Center of Intelligent Visual Computing; School of Computer Science, Fudan University + Peng Cheng Laboratory",
        "aff_domain": "fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn",
        "github": "https://github.com/zyh190507/KnnContrastiveForOOD",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+2",
        "aff_unique_norm": "Fudan University;Shanghai Collaborative Innovation Center of Intelligent Visual Computing;Pengcheng Laboratory",
        "aff_unique_dep": "School of Computer Science;Intelligent Visual Computing;Peng Cheng Laboratory",
        "aff_unique_url": "https://www.fudan.edu.cn;;http://www.pcl.ac.cn",
        "aff_unique_abbr": "Fudan;;PCL",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.422",
        "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks. Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines.",
        "author": "Shulin Cao; Jiaxin Shi; Liangming Pan; Lunyiu Nie; Yutong Xiang; Lei Hou; Juanzi Li; Bin He; Hanwang Zhang",
        "authorids": "/s/shulin-cao/; /j/jiaxin-shi/; /l/liangming-pan/; /l/lunyiu-nie/; /y/yutong-xiang/; /l/lei-hou/; /j/juanzi-li/; /b/bin-he/; /h/hanwang-zhang/",
        "bibtex": "@inproceedings{cao-etal-2022-kqa,\n    title = \"{KQA} Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base\",\n    author = \"Cao, Shulin  and\n      Shi, Jiaxin  and\n      Pan, Liangming  and\n      Nie, Lunyiu  and\n      Xiang, Yutong  and\n      Hou, Lei  and\n      Li, Juanzi  and\n      He, Bin  and\n      Zhang, Hanwang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.422/\",\n    doi = \"10.18653/v1/2022.acl-long.422\",\n    pages = \"6101--6119\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.422.pdf",
        "site": "https://aclanthology.org/2022.acl-long.422/",
        "pdf_size": 1114843,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1263153016133998650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Technology, BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China; Cloud BU, Huawei Technologies; National University of Singapore; Department of Computer Science and Technology, BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China; ETH Z\u00fcrich; Department of Computer Science and Technology, BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China; Department of Computer Science and Technology, BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China; Noah\u2019s Ark Lab, Huawei Technologies; Nanyang Technological University",
        "aff_domain": "mails.tsinghua.edu.cn;gmail.com;u.nus.edu; ; ;tsinghua.edu.cn;tsinghua.edu.cn; ; ",
        "email": "mails.tsinghua.edu.cn;gmail.com;u.nus.edu; ; ;tsinghua.edu.cn;tsinghua.edu.cn; ; ",
        "github": "https://github.com/shijx12/KQAPro_Baselines",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;2;3;0+1;4;0+1;0+1;2;5",
        "aff_unique_norm": "BNRist;Tsinghua University;Huawei;National University of Singapore;ETH Zurich;Nanyang Technological University",
        "aff_unique_dep": "Department of Computer Science and Technology;Institute for Artificial Intelligence;Cloud BU;;;",
        "aff_unique_url": ";https://www.tsinghua.edu.cn;https://www.huawei.com;https://www.nus.edu.sg;https://www.ethz.ch;https://www.ntu.edu.sg",
        "aff_unique_abbr": ";THU;Huawei;NUS;ETHZ;NTU",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "1;1;2;1;3;1;1;1;2",
        "aff_country_unique": ";China;Singapore;Switzerland"
    },
    {
        "id": "2022.findings-acl.30",
        "title": "KSAM: Infusing Multi-Source Knowledge into Dialogue Generation via Knowledge Source Aware Multi-Head Decoding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge-enhanced methods have bridged the gap between human beings and machines in generating dialogue responses. However, most previous works solely seek knowledge from a single source, and thus they often fail to obtain available knowledge because of the insufficient coverage of a single knowledge source. To this end, infusing knowledge from multiple sources becomes a trend. This paper proposes a novel approach Knowledge Source Aware Multi-Head Decoding, KSAM, to infuse multi-source knowledge into dialogue generation more efficiently. Rather than following the traditional single decoder paradigm, KSAM uses multiple independent source-aware decoder heads to alleviate three challenging problems in infusing multi-source knowledge, namely, the diversity among different knowledge sources, the indefinite knowledge alignment issue, and the insufficient flexibility/scalability in knowledge usage. Experiments on a Chinese multi-source knowledge-aligned dataset demonstrate the superior performance of KSAM against various competitive approaches.",
        "author": "Sixing Wu; Ying Li; Dawei Zhang; Zhonghai Wu",
        "authorids": "/s/sixing-wu/; /y/ying-li/; /d/dawei-zhang/; /z/zhonghai-wu/",
        "bibtex": "@inproceedings{wu-etal-2022-ksam,\n    title = \"{KSAM}: Infusing Multi-Source Knowledge into Dialogue Generation via Knowledge Source Aware Multi-Head Decoding\",\n    author = \"Wu, Sixing  and\n      Li, Ying  and\n      Zhang, Dawei  and\n      Wu, Zhonghai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.30/\",\n    doi = \"10.18653/v1/2022.findings-acl.30\",\n    pages = \"353--363\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.30.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.30/",
        "pdf_size": 635566,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11175853705787374299&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science, Peking University, Beijing, China; National Research Center of Software Engineering, Peking University, Beijing, China + Key Lab of High Confidence Software Technologies (MOE), Peking University, Beijing, China; School of Computer Science, Peking University, Beijing, China; National Research Center of Software Engineering, Peking University, Beijing, China + Key Lab of High Confidence Software Technologies (MOE), Peking University, Beijing, China",
        "aff_domain": "pku.edu.cn;pku.edu.cn; ; ",
        "email": "pku.edu.cn;pku.edu.cn; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+0;0;0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "0;0+0;0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.35",
        "title": "KaFSP: Knowledge-Aware Fuzzy Semantic Parsing for Conversational Question Answering over a Large-Scale Knowledge Base",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we study two issues of semantic parsing approaches to conversational question answering over a large-scale knowledge base: (1) The actions defined in grammar are not sufficient to handle uncertain reasoning common in real-world scenarios. (2) Knowledge base information is not well exploited and incorporated into semantic parsing. To mitigate the two issues, we propose a knowledge-aware fuzzy semantic parsing framework (KaFSP). It defines fuzzy comparison operations in the grammar system for uncertain reasoning based on the fuzzy set theory. In order to enhance the interaction between semantic parsing and knowledge base, we incorporate entity triples from the knowledge base into a knowledge-aware entity disambiguation module. Additionally, we propose a multi-label classification framework to not only capture correlations between entity types and relations but also detect knowledge base information relevant to the current utterance. Both enhancements are based on pre-trained language models. Experiments on a large-scale conversational question answering benchmark demonstrate that the proposed KaFSP achieves significant improvements over previous state-of-the-art models, setting new SOTA results on 8 out of 10 question types, gaining improvements of over 10% F1 or accuracy on 3 question types, and improving overall F1 from 83.01% to 85.33%. The source code of KaFSP is available at https://github.com/tjunlp-lab/KaFSP.",
        "author": "Junzhuo Li; Deyi Xiong",
        "authorids": "/j/junzhuo-li/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{li-xiong-2022-kafsp,\n    title = \"{K}a{FSP}: Knowledge-Aware Fuzzy Semantic Parsing for Conversational Question Answering over a Large-Scale Knowledge Base\",\n    author = \"Li, Junzhuo  and\n      Xiong, Deyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.35/\",\n    doi = \"10.18653/v1/2022.acl-long.35\",\n    pages = \"461--473\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.35.pdf",
        "site": "https://aclanthology.org/2022.acl-long.35/",
        "pdf_size": 446917,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13982624526898656309&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 2,
        "aff": "School of New Media and Communication, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn",
        "github": "https://github.com/tjunlp-lab/KaFSP",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "School of New Media and Communication",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.210",
        "title": "KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Currently, Medical Subject Headings (MeSH) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information. With the rapid growth of the PubMed database, large-scale biomedical document indexing becomes increasingly important. MeSH indexing is a challenging task for machine learning, as it needs to assign multiple labels to each article from an extremely large hierachically organized collection. To address this challenge, we propose KenMeSH, an end-to-end model that combines new text features and a dynamic knowledge-enhanced mask attention that integrates document features with MeSH label hierarchy and journal correlation features to index MeSH terms. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.",
        "author": "Xindi Wang; Robert Mercer; Frank Rudzicz",
        "authorids": "/x/xindi-wang/; /r/robert-e-mercer/; /f/frank-rudzicz/",
        "bibtex": "@inproceedings{wang-etal-2022-kenmesh,\n    title = \"{K}en{M}e{SH}: Knowledge-enhanced End-to-end Biomedical Text Labelling\",\n    author = \"Wang, Xindi  and\n      Mercer, Robert  and\n      Rudzicz, Frank\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.210/\",\n    doi = \"10.18653/v1/2022.acl-long.210\",\n    pages = \"2941--2951\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.210.pdf",
        "site": "https://aclanthology.org/2022.acl-long.210/",
        "pdf_size": 750677,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18077487900034635957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Western Ontario, London, Ontario, Canada + Vector Institute for Arti\ufb01cial Intelligence, Toronto, Ontario, Canada; Department of Computer Science, University of Western Ontario, London, Ontario, Canada + Vector Institute for Arti\ufb01cial Intelligence, Toronto, Ontario, Canada; Department of Computer Science, University of Toronto, Toronto, Ontario, Canada + Vector Institute for Arti\ufb01cial Intelligence, Toronto, Ontario, Canada + Unity Health Toronto, Toronto, Ontario, Canada",
        "aff_domain": "uwo.ca;csd.uwo.ca;cs.toronto.edu",
        "email": "uwo.ca;csd.uwo.ca;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;2+1+3",
        "aff_unique_norm": "University of Western Ontario;Vector Institute for Artificial Intelligence;University of Toronto;Unity Health Toronto",
        "aff_unique_dep": "Department of Computer Science;Artificial Intelligence;Department of Computer Science;",
        "aff_unique_url": "https://www.uwo.ca;https://vectorinstitute.ai;https://www.utoronto.ca;https://www.unityhealth.to",
        "aff_unique_abbr": "UWO;Vector Institute;U of T;",
        "aff_campus_unique_index": "0+1;0+1;1+1+1",
        "aff_campus_unique": "London;Toronto",
        "aff_country_unique_index": "0+0;0+0;0+0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.304",
        "title": "Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Contrastive learning has achieved impressive success in generation tasks to militate the \u201cexposure bias\u201d problem and discriminatively exploit the different quality of references. Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the constrained mapping relationships. Hence, in this work, we propose a hierarchical contrastive learning mechanism, which can unify hybrid granularities semantic meaning in the input text. Concretely, we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations. Then, we construct intra-contrasts within instance-level and keyword-level, where we assume words are sampled nodes from a sentence distribution. Finally, to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem, we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution. Experiments demonstrate that our model outperforms competitive baselines on paraphrasing, dialogue generation, and storytelling tasks.",
        "author": "Mingzhe Li; XieXiong Lin; Xiuying Chen; Jinxiong Chang; Qishen Zhang; Feng Wang; Taifeng Wang; Zhongyi Liu; Wei Chu; Dongyan Zhao; Rui Yan",
        "authorids": "/m/mingzhe-li/; /x/xiexiong-lin/; /x/xiuying-chen/; /j/jinxiong-chang/; /q/qishen-zhang/; /f/feng-wang/; /t/taifeng-wang/; /z/zhongyi-liu/; /w/wei-chu/; /d/dongyan-zhao/; /r/rui-yan/",
        "bibtex": "@inproceedings{li-etal-2022-keywords,\n    title = \"Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation\",\n    author = \"Li, Mingzhe  and\n      Lin, XieXiong  and\n      Chen, Xiuying  and\n      Chang, Jinxiong  and\n      Zhang, Qishen  and\n      Wang, Feng  and\n      Wang, Taifeng  and\n      Liu, Zhongyi  and\n      Chu, Wei  and\n      Zhao, Dongyan  and\n      Yan, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.304/\",\n    doi = \"10.18653/v1/2022.acl-long.304\",\n    pages = \"4432--4441\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.304.pdf",
        "site": "https://aclanthology.org/2022.acl-long.304/",
        "pdf_size": 633814,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3503409655058462546&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Wangxuan Institute of Computer Technology, Peking University, Beijing, China+Center for Data Science, AAIS, Peking University, Beijing, China; Ant Group; Computational Bioscience Reseach Center, KAUST; Ant Group; Ant Group; Ant Group; Ant Group; Ant Group; Ant Group; Wangxuan Institute of Computer Technology, Peking University, Beijing, China+Center for Data Science, AAIS, Peking University, Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "pku.edu.cn;antfin.com; ; ; ; ; ; ; ;pku.edu.cn; ",
        "email": "pku.edu.cn;antfin.com; ; ; ; ; ; ; ;pku.edu.cn; ",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0+0;1;2;1;1;1;1;1;1;0+0;3",
        "aff_unique_norm": "Peking University;Ant Group;King Abdullah University of Science and Technology;Renmin University of China",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;;Computational Bioscience Research Center;Gaoling School of Artificial Intelligence",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.antgroup.com;https://www.kaust.edu.sa;http://www.ruc.edu.cn",
        "aff_unique_abbr": "PKU;Ant Group;KAUST;RUC",
        "aff_campus_unique_index": "0+0;0+0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;1;0;0;0;0;0;0;0+0;0",
        "aff_country_unique": "China;Saudi Arabia"
    },
    {
        "id": "2022.acl-long.367",
        "title": "KinyaBERT: a Morphology-aware Kinyarwanda Language Model",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks. However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities. We address these challenges by proposing a simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality.Despite the success of BERT, most of its evaluations have been conducted on high-resource languages, obscuring its applicability on low-resource languages. We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language, naming the proposed model architecture KinyaBERT. A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity recognition task and by 4.3% in average score of a machine-translated GLUE benchmark. KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise.",
        "author": "Antoine Nzeyimana; Andre Niyongabo Rubungo",
        "authorids": "/a/antoine-nzeyimana/; /a/andre-niyongabo-rubungo/",
        "bibtex": "@inproceedings{nzeyimana-niyongabo-rubungo-2022-kinyabert,\n    title = \"{K}inya{BERT}: a Morphology-aware {K}inyarwanda Language Model\",\n    author = \"Nzeyimana, Antoine  and\n      Niyongabo Rubungo, Andre\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.367/\",\n    doi = \"10.18653/v1/2022.acl-long.367\",\n    pages = \"5347--5363\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.367.pdf",
        "site": "https://aclanthology.org/2022.acl-long.367/",
        "pdf_size": 745693,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8055774620922520845&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "University of Massachusetts Amherst; Polytechnic University of Catalonia",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "https://github.com/anzeyimana/kinyabert-acl2022",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Polytechnic University of Catalonia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umass.edu;https://www.upc.edu",
        "aff_unique_abbr": "UMass Amherst;UPC",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Spain"
    },
    {
        "id": "2022.acl-long.221",
        "title": "Knowledge Enhanced Reflection Generation for Counseling Dialogues",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we study the effect of commonsense and domain knowledge while generating responses in counseling conversations using retrieval and generative methods for knowledge integration. We propose a pipeline that collects domain knowledge through web mining, and show that retrieval from both domain-specific and commonsense knowledge bases improves the quality of generated responses. We also present a model that incorporates knowledge generated by COMET using soft positional encoding and masked self-attention. We show that both retrieved and COMET-generated knowledge improve the system\u2019s performance as measured by automatic metrics and also by human evaluation. Lastly, we present a comparative study on the types of knowledge encoded by our system showing that causal and intentional relationships benefit the generation task more than other types of commonsense relations.",
        "author": "Siqi Shen; Veronica Perez-Rosas; Charles Welch; Soujanya Poria; Rada Mihalcea",
        "authorids": "/s/siqi-shen/; /v/veronica-perez-rosas/; /c/charles-welch/; /s/soujanya-poria/; /r/rada-mihalcea/",
        "bibtex": "@inproceedings{shen-etal-2022-knowledge,\n    title = \"Knowledge Enhanced Reflection Generation for Counseling Dialogues\",\n    author = \"Shen, Siqi  and\n      Perez-Rosas, Veronica  and\n      Welch, Charles  and\n      Poria, Soujanya  and\n      Mihalcea, Rada\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.221/\",\n    doi = \"10.18653/v1/2022.acl-long.221\",\n    pages = \"3096--3107\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.221.pdf",
        "site": "https://aclanthology.org/2022.acl-long.221/",
        "pdf_size": 753290,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17119665730288714493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Michigan, USA; University of Michigan, USA; University of Michigan, USA; DeCLaRe Lab, Singapore University of Technology and Design, Singapore; University of Michigan, USA + DeCLaRe + DeCLaRe Lab, Singapore University of Technology and Design, Singapore",
        "aff_domain": "umich.edu;umich.edu;umich.edu;sutd.edu.sg;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;sutd.edu.sg;umich.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0+2+1",
        "aff_unique_norm": "University of Michigan;Singapore University of Technology and Design;DeCLaRe",
        "aff_unique_dep": ";DeCLaRe Lab;",
        "aff_unique_url": "https://www.umich.edu;https://www.sutd.edu.sg;",
        "aff_unique_abbr": "UM;SUTD;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0+1",
        "aff_country_unique": "United States;Singapore;"
    },
    {
        "id": "2022.findings-acl.91",
        "title": "Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links in knowledge graphs. Designing a strong and effective loss framework is essential for knowledge graph embedding models to distinguish between correct and incorrect triplets. The classic margin-based ranking loss limits the scores of positive and negative triplets to have a suitable margin. The recently proposed Limit-based Scoring Loss independently limits the range of positive and negative triplet scores. However, these loss frameworks use equal or fixed penalty terms to reduce the scores of positive and negative sample pairs, which is inflexible in optimization. Our intuition is that if a triplet score deviates far from the optimum, it should be emphasized. To this end, we propose Adaptive Limit Scoring Loss, which simply re-weights each triplet to highlight the less-optimized triplet scores. We apply this loss framework to several knowledge graph embedding models such as TransE, TransH and ComplEx. The experimental results on link prediction and triplet classification show that our proposed method has achieved performance on par with the state of the art.",
        "author": "Jinfa Yang; Xianghua Ying; Yongjie Shi; Xin Tong; Ruibin Wang; Taiyan Chen; Bowei Xing",
        "authorids": "/j/jinfa-yang/; /x/xianghua-ying/; /y/yongjie-shi/; /x/xin-tong/; /r/ruibin-wang/; /t/taiyan-chen/; /b/bowei-xing/",
        "bibtex": "@inproceedings{yang-etal-2022-knowledge,\n    title = \"Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy\",\n    author = \"Yang, Jinfa  and\n      Ying, Xianghua  and\n      Shi, Yongjie  and\n      Tong, Xin  and\n      Wang, Ruibin  and\n      Chen, Taiyan  and\n      Xing, Bowei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.91/\",\n    doi = \"10.18653/v1/2022.findings-acl.91\",\n    pages = \"1153--1163\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.91.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.91/",
        "pdf_size": 1164616,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13130080683309354634&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Key Laboratory of Machine Perception (MOE) School of Artificial Intelligence, Peking University; Key Laboratory of Machine Perception (MOE) School of Artificial Intelligence, Peking University; Key Laboratory of Machine Perception (MOE) School of Artificial Intelligence, Peking University; Key Laboratory of Machine Perception (MOE) School of Artificial Intelligence, Peking University; Key Laboratory of Machine Perception (MOE) School of Artificial Intelligence, Peking University; Key Laboratory of Machine Perception (MOE) School of Artificial Intelligence, Peking University; Key Laboratory of Machine Perception (MOE) School of Artificial Intelligence, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Artificial Intelligence",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.581",
        "title": "Knowledge Neurons in Pretrained Transformers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.",
        "author": "Damai Dai; Li Dong; Yaru Hao; Zhifang Sui; Baobao Chang; Furu Wei",
        "authorids": "/d/damai-dai/; /l/li-dong/; /y/yaru-hao/; /z/zhifang-sui/; /b/baobao-chang/; /f/furu-wei/",
        "bibtex": "@inproceedings{dai-etal-2022-knowledge,\n    title = \"Knowledge Neurons in Pretrained Transformers\",\n    author = \"Dai, Damai  and\n      Dong, Li  and\n      Hao, Yaru  and\n      Sui, Zhifang  and\n      Chang, Baobao  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.581/\",\n    doi = \"10.18653/v1/2022.acl-long.581\",\n    pages = \"8493--8502\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.581.pdf",
        "site": "https://aclanthology.org/2022.acl-long.581/",
        "pdf_size": 472878,
        "gs_citation": 600,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2724150987199356816&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "MOE Key Lab of Computational Linguistics, Peking University\u2020\u2021\u2217; Microsoft Research\u2021; MOE Key Lab of Computational Linguistics, Peking University\u2020; MOE Key Lab of Computational Linguistics, Peking University\u2020; MOE Key Lab of Computational Linguistics, Peking University\u2020; Microsoft Research\u2021",
        "aff_domain": "pku.edu.cn;microsoft.com;microsoft.com;pku.edu.cn;pku.edu.cn;microsoft.com",
        "email": "pku.edu.cn;microsoft.com;microsoft.com;pku.edu.cn;pku.edu.cn;microsoft.com",
        "github": "https://github.com/Hunter-DDM/knowledge-neurons",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;1",
        "aff_unique_norm": "Peking University;Microsoft",
        "aff_unique_dep": "MOE Key Lab of Computational Linguistics;Microsoft Research",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "PKU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.158",
        "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompttuning (KPT), to improve and stabilize prompttuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.",
        "author": "Shengding Hu; Ning Ding; Huadong Wang; Zhiyuan Liu; Jingang Wang; Juanzi Li; Wei Wu; Maosong Sun",
        "authorids": "/s/shengding-hu/; /n/ning-ding/; /h/huadong-wang/; /z/zhiyuan-liu/; /j/jingang-wang/; /j/juanzi-li/; /w/wei-wu/; /m/maosong-sun/",
        "bibtex": "@inproceedings{hu-etal-2022-knowledgeable,\n    title = \"Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification\",\n    author = \"Hu, Shengding  and\n      Ding, Ning  and\n      Wang, Huadong  and\n      Liu, Zhiyuan  and\n      Wang, Jingang  and\n      Li, Juanzi  and\n      Wu, Wei  and\n      Sun, Maosong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.158/\",\n    doi = \"10.18653/v1/2022.acl-long.158\",\n    pages = \"2225--2240\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.158.pdf",
        "site": "https://aclanthology.org/2022.acl-long.158/",
        "pdf_size": 466491,
        "gs_citation": 410,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16615815545390444057&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Meituan, Beijing, China; Meituan, Beijing, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China; Meituan, Beijing, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;163.com;tsinghua.edu.cn; ; ; ; ",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;163.com;tsinghua.edu.cn; ; ; ; ",
        "github": "https://github.com/thunlp/KnowledgeablePromptTuning",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1+0+0;0+1+0+0;0+1+0+0;0+1+0+0+2;2;0+1+0+0;2;0+1+0+0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Meituan",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.meituan.com",
        "aff_unique_abbr": "THU;;Meituan",
        "aff_campus_unique_index": "0+0+2;0+0+2;0+0+2;0+0+2+0;0;0+0+2;0;0+0+2",
        "aff_campus_unique": "Beijing;;Shanghai",
        "aff_country_unique_index": "0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0+0;0;0+0+0+0;0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.24",
        "title": "Kronecker Decomposition for GPT Compression",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters. Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory. This problem can be mitigated using model compression techniques; however, compressing GPT models has not been investigated much in the literature. In this work, we use Kronecker decomposition to compress the linear mappings of the GPT-2 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre- training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is fine-tuned on downstream tasks using ILKD as well. We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.",
        "author": "Ali Edalati; Marzieh Tahaei; Ahmad Rashid; Vahid Nia; James Clark; Mehdi Rezagholizadeh",
        "authorids": "/a/ali-edalati/; /m/marzieh-tahaei/; /a/ahmad-rashid/; /v/vahid-nia/; /j/james-clark/; /m/mehdi-rezagholizadeh/",
        "bibtex": "@inproceedings{edalati-etal-2022-kronecker,\n    title = \"Kronecker Decomposition for {GPT} Compression\",\n    author = \"Edalati, Ali  and\n      Tahaei, Marzieh  and\n      Rashid, Ahmad  and\n      Nia, Vahid  and\n      Clark, James  and\n      Rezagholizadeh, Mehdi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.24/\",\n    doi = \"10.18653/v1/2022.acl-short.24\",\n    pages = \"219--226\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.24.pdf",
        "site": "https://aclanthology.org/2022.acl-short.24/",
        "pdf_size": 263681,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8684895408222204725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "McGill University; Huawei Noah Ark Lab; Huawei Noah Ark Lab; Huawei Noah Ark Lab; McGill University; Huawei Noah Ark Lab",
        "aff_domain": "mail.mcgill.ca;huawei.com;huawei.com;huawei.com;mcgill.ca;huawei.com",
        "email": "mail.mcgill.ca;huawei.com;huawei.com;huawei.com;mcgill.ca;huawei.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;0;1",
        "aff_unique_norm": "McGill University;Huawei",
        "aff_unique_dep": ";Huawei",
        "aff_unique_url": "https://www.mcgill.ca;https://www.huawei.com",
        "aff_unique_abbr": "McGill;Huawei",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Noah Ark Lab",
        "aff_country_unique_index": "0;1;1;1;0;1",
        "aff_country_unique": "Canada;China"
    },
    {
        "id": "2022.acl-long.233",
        "title": "LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Semantic parsing is the task of producing structured meaning representations for natural language sentences. Recent research has pointed out that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to generalize systematically, i.e. to handle examples that require recombining known knowledge in novel settings. In this work, we show that better systematic generalization can be achieved by producing the meaning representation directly as a graph and not as a sequence. To this end we propose LAGr (Label Aligned Graphs), a general framework to produce semantic parses by independently predicting node and edge labels for a complete multi-layer input-aligned graph. The strongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas weakly-supervised LAGr infers alignments for originally unaligned target graphs using approximate maximum-a-posteriori inference. Experiments demonstrate that LAGr achieves significant improvements in systematic generalization upon the baseline seq2seq parsers in both strongly- and weakly-supervised settings.",
        "author": "Dora Jambor; Dzmitry Bahdanau",
        "authorids": "/d/dora-jambor/; /d/dzmitry-bahdanau/",
        "bibtex": "@inproceedings{jambor-bahdanau-2022-lagr,\n    title = \"{LAG}r: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing\",\n    author = \"Jambor, Dora  and\n      Bahdanau, Dzmitry\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.233/\",\n    doi = \"10.18653/v1/2022.acl-long.233\",\n    pages = \"3295--3308\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.233.pdf",
        "site": "https://aclanthology.org/2022.acl-long.233/",
        "pdf_size": 406078,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7420963840468726873&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Mila - Quebec AI Institute + McGill University; ServiceNow Research + Mila - Quebec AI Institute + McGill University + Canada CIFAR AI Chair",
        "aff_domain": "mail.mcgill.ca; ",
        "email": "mail.mcgill.ca; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2+0+1+3",
        "aff_unique_norm": "Quebec AI Institute;McGill University;ServiceNow;Canadian Institute for Advanced Research",
        "aff_unique_dep": "AI Institute;;Research;AI Chair",
        "aff_unique_url": "https://mila.quebec;https://www.mcgill.ca;https://www.servicenow.com;https://www.cifar.ca",
        "aff_unique_abbr": "Mila;McGill;ServiceNow;CIFAR",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1+0+0+0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.findings-acl.17",
        "title": "LEVEN: A Large-Scale Chinese Legal Event Detection Dataset",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recognizing facts is the most fundamental step in making judgments, hence detecting events in the legal documents is important to legal case analysis tasks. However, existing Legal Event Detection (LED) datasets only concern incomprehensive event types and have limited annotated data, which restricts the development of LED methods and their downstream applications. To alleviate these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection dataset, with 8,116 legal documents and 150,977 human-annotated event mentions in 108 event types. Not only charge-related events, LEVEN also covers general events, which are critical for legal case understanding but neglected in existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and has dozens of times the data scale of others, which shall significantly promote the training and evaluation of LED methods. The results of extensive experiments indicate that LED is challenging and needs further effort. Moreover, we simply utilize legal events as side information to promote downstream applications. The method achieves improvements of average 2.2 points precision in low-resource judgment prediction, and 1.5 points mean average precision in unsupervised case retrieval, which suggests the fundamentality of LED. The source code and dataset can be obtained from https://github.com/thunlp/LEVEN.",
        "author": "Feng Yao; Chaojun Xiao; Xiaozhi Wang; Zhiyuan Liu; Lei Hou; Cunchao Tu; Juanzi Li; Yun Liu; Weixing Shen; Maosong Sun",
        "authorids": "/f/feng-yao/; /c/chaojun-xiao/; /x/xiaozhi-wang/; /z/zhiyuan-liu/; /l/lei-hou/; /c/cunchao-tu/; /j/juanzi-li/; /y/yun-liu/; /w/weixing-shen/; /m/maosong-sun/",
        "bibtex": "@inproceedings{yao-etal-2022-leven,\n    title = \"{LEVEN}: A Large-Scale {C}hinese Legal Event Detection Dataset\",\n    author = \"Yao, Feng  and\n      Xiao, Chaojun  and\n      Wang, Xiaozhi  and\n      Liu, Zhiyuan  and\n      Hou, Lei  and\n      Tu, Cunchao  and\n      Li, Juanzi  and\n      Liu, Yun  and\n      Shen, Weixing  and\n      Sun, Maosong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.17/\",\n    doi = \"10.18653/v1/2022.findings-acl.17\",\n    pages = \"183--201\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.17.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.17/",
        "pdf_size": 760763,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18335632661401545370&as_sdt=5,36&sciodt=0,36&hl=en",
        "gs_version_total": 6,
        "aff": "School of Law, Institute for AI and Law, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China; Beijing Powerlaw Intelligent Technology Co., Ltd., China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China; School of Law, Institute for AI and Law, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China; School of Law, Institute for AI and Law, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence, Beijing, China",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn; ; ; ; ; ;",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn; ; ; ; ; ;",
        "github": "https://github.com/thunlp/LEVEN",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1+0+2;0+1+0+2;0+1+0+2;0+1+0+2;0+1+0+2;3;0+1+0+2;0+1+0+2;0+1+0+2;0+1+0+2",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Beijing Academy of Artificial Intelligence;Beijing Powerlaw Intelligent Technology Co., Ltd.",
        "aff_unique_dep": "School of Law, Institute for AI and Law;;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.baaic.cn;",
        "aff_unique_abbr": "Tsinghua;;BAAI;",
        "aff_campus_unique_index": "0+2+0;0+2+0;0+2+0;0+2+0;0+2+0;0+2+0;0+2+0;0+2+0;0+2+0",
        "aff_campus_unique": "Beijing;;Shanghai",
        "aff_country_unique_index": "0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0;0;0+0+0+0;0+0+0+0;0+0+0+0;0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.34",
        "title": "LM-BFF-MS: Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "LM-BFF (CITATION) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example. To improve the approach of LM-BFF, this paper proposes LM-BFF-MS\u2014better few-shot fine-tuning of language models with multiple soft demonstrations by making its further extensions, which include 1) prompts with multiple demonstrations based on automatic generation of multiple label words; and 2) soft demonstration memory which consists of multiple sequences of globally shared word embeddings for a similar context. Experiments conducted on eight NLP tasks show that LM-BFF-MS leads to improvements over LM-BFF on five tasks, particularly achieving 94.0 and 90.4 on SST-2 and MRPC, respectively.",
        "author": "Eunhwan Park; Donghyeon Jeon; Seonhoon Kim; Inho Kang; Seung-Hoon Na",
        "authorids": "/e/eunhwan-park/; /d/donghyeon-jeon/; /s/seonhoon-kim/; /i/inho-kang/; /s/seung-hoon-na/",
        "bibtex": "@inproceedings{park-etal-2022-lm,\n    title = \"{LM}-{BFF}-{MS}: Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory\",\n    author = \"Park, Eunhwan  and\n      Jeon, Donghyeon  and\n      Kim, Seonhoon  and\n      Kang, Inho  and\n      Na, Seung-Hoon\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.34/\",\n    doi = \"10.18653/v1/2022.acl-short.34\",\n    pages = \"310--317\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.34.pdf",
        "site": "https://aclanthology.org/2022.acl-short.34/",
        "pdf_size": 513705,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13314935858002770919&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Jeonbuk National University+NA VER Corporation; NA VER Corporation; NA VER Corporation; NA VER Corporation; Jeonbuk National University+NA VER Corporation",
        "aff_domain": "jbnu.ac.kr;jbnu.ac.kr;navercorp.com;navercorp.com;navercorp.com",
        "email": "jbnu.ac.kr;jbnu.ac.kr;navercorp.com;navercorp.com;navercorp.com",
        "github": "https://github.com/judepark96/LM-BFF-MS",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;0+1",
        "aff_unique_norm": "Jeonbuk National University;NAVER Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jbnu.ac.kr;https://www.naver.com",
        "aff_unique_abbr": "JBNU;NAVER",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.findings-acl.281",
        "title": "LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks. Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.",
        "author": "Canwen Xu; Daya Guo; Nan Duan; Julian McAuley",
        "authorids": "/c/canwen-xu/; /d/daya-guo/; /n/nan-duan/; /j/julian-mcauley/",
        "bibtex": "@inproceedings{xu-etal-2022-laprador,\n    title = \"{L}a{P}ra{D}o{R}: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval\",\n    author = \"Xu, Canwen  and\n      Guo, Daya  and\n      Duan, Nan  and\n      McAuley, Julian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.281/\",\n    doi = \"10.18653/v1/2022.findings-acl.281\",\n    pages = \"3557--3569\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.281.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.281/",
        "pdf_size": 985482,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12216257895317174954&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, San Diego; Sun Yat-sen University; Microsoft Research Asia; University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;mail2.sysu.edu.cn;microsoft.com",
        "email": "ucsd.edu;ucsd.edu;mail2.sysu.edu.cn;microsoft.com",
        "github": "https://github.com/JetRunner/LaPraDoR",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of California, San Diego;Sun Yat-sen University;Microsoft",
        "aff_unique_dep": ";;Research",
        "aff_unique_url": "https://www.ucsd.edu;http://www.sysu.edu.cn/;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "UCSD;SYSU;MSR Asia",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "San Diego;;Asia",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.570",
        "title": "Label Semantic Aware Pre-training for Few-shot Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In text classification tasks, useful information is encoded in the label names. Label semantic aware systems have leveraged this information for improved text classification performance during fine-tuning and prediction. However, use of label-semantics during pre-training has not been extensively explored. We therefore propose Label Semantic Aware Pre-training (LSAP) to improve the generalization and data efficiency of text classification systems. LSAP incorporates label semantics into pre-trained generative models (T5 in our case) by performing secondary pre-training on labeled sentences from a variety of domains. As domain-general pre-training requires large amounts of data, we develop a filtering and labeling pipeline to automatically create sentence-label pairs from unlabeled text. We perform experiments on intent (ATIS, Snips, TOPv2) and topic classification (AG News, Yahoo! Answers). LSAP obtains significant accuracy improvements over state-of-the-art models for few-shot text classification while maintaining performance comparable to state of the art in high-resource settings.",
        "author": "Aaron Mueller; Jason Krone; Salvatore Romeo; Saab Mansour; Elman Mansimov; Yi Zhang; Dan Roth",
        "authorids": "/a/aaron-mueller/; /j/jason-krone/; /s/salvatore-romeo/; /s/saab-mansour/; /e/elman-mansimov/; /y/yi-zhang/; /d/dan-roth/",
        "bibtex": "@inproceedings{mueller-etal-2022-label,\n    title = \"Label Semantic Aware Pre-training for Few-shot Text Classification\",\n    author = \"Mueller, Aaron  and\n      Krone, Jason  and\n      Romeo, Salvatore  and\n      Mansour, Saab  and\n      Mansimov, Elman  and\n      Zhang, Yi  and\n      Roth, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.570/\",\n    doi = \"10.18653/v1/2022.acl-long.570\",\n    pages = \"8318--8334\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.570.pdf",
        "site": "https://aclanthology.org/2022.acl-long.570/",
        "pdf_size": 763141,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17494907606291608137&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Johns Hopkins University; Amazon Web Services AI Labs; Amazon Web Services AI Labs; Amazon Web Services AI Labs; Amazon Web Services AI Labs; Amazon Web Services AI Labs; Department of Computer & Information Science, University of Pennsylvania + Amazon Web Services AI Labs",
        "aff_domain": "jhu.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "jhu.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;2+1",
        "aff_unique_norm": "Johns Hopkins University;Amazon;University of Pennsylvania",
        "aff_unique_dep": "Department of Computer Science;AI Labs;Department of Computer & Information Science",
        "aff_unique_url": "https://www.jhu.edu;https://aws.amazon.com;https://www.upenn.edu",
        "aff_unique_abbr": "JHU;AWS;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.155",
        "title": "Label Semantics for Few Shot Named Entity Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We study the problem of few shot learning for named entity recognition. Specifically, we leverage the semantic information in the names of the labels as a way of giving the model additional signal and enriched priors. We propose a neural architecture that consists of two BERT encoders, one to encode the document and its tokens and another one to encode each of the labels in natural language format. Our model learns to match the representations of named entities computed by the first encoder with label representations computed by the second encoder. The label semantics signal is shown to support improved state-of-the-art results in multiple few shot NER benchmarks and on-par performance in standard benchmarks. Our model is especially effective in low resource settings.",
        "author": "Jie Ma; Miguel Ballesteros; Srikanth Doss; Rishita Anubhai; Sunil Mallya; Yaser Al-Onaizan; Dan Roth",
        "authorids": "/j/jie-ma/; /m/miguel-ballesteros/; /s/srikanth-doss/; /r/rishita-anubhai/; /s/sunil-mallya/; /y/yaser-al-onaizan/; /d/dan-roth/",
        "bibtex": "@inproceedings{ma-etal-2022-label,\n    title = \"Label Semantics for Few Shot Named Entity Recognition\",\n    author = \"Ma, Jie  and\n      Ballesteros, Miguel  and\n      Doss, Srikanth  and\n      Anubhai, Rishita  and\n      Mallya, Sunil  and\n      Al-Onaizan, Yaser  and\n      Roth, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.155/\",\n    doi = \"10.18653/v1/2022.findings-acl.155\",\n    pages = \"1956--1971\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.155.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.155/",
        "pdf_size": 539936,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10682542278342541949&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff": "AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs + Computer and Information Science, University of Pennsylvania",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;gmail.com;yahoo.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;gmail.com;yahoo.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0+1",
        "aff_unique_norm": "Amazon;University of Pennsylvania",
        "aff_unique_dep": "AWS AI Labs;Computer and Information Science",
        "aff_unique_url": "https://aws.amazon.com;https://www.upenn.edu",
        "aff_unique_abbr": "AWS;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.208",
        "title": "Lacking the Embedding of a Word? Look it up into a Traditional Dictionary",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Word embeddings are powerful dictionaries, which may easily capture language variations. However, these dictionaries fail to give sense to rare words, which are surprisingly often covered by traditional dictionaries. In this paper, we propose to use definitions retrieved in traditional dictionaries to produce word embeddings for rare words. For this purpose, we introduce two methods: Definition Neural Network (DefiNNet) and Define BERT (DefBERT). In our experiments, DefiNNet and DefBERT significantly outperform state-of-the-art as well as baseline methods devised for producing embeddings of unknown words. In fact, DefiNNet significantly outperforms FastText, which implements a method for the same task-based on n-grams, and DefBERT significantly outperforms the BERT method for OOV words. Then, definitions in traditional dictionaries are useful to build word embeddings for rare words.",
        "author": "Elena Sofia Ruzzetti; Leonardo Ranaldi; Michele Mastromattei; Francesca Fallucchi; Noemi Scarpato; Fabio Massimo Zanzotto",
        "authorids": "/e/elena-sofia-ruzzetti/; /l/leonardo-ranaldi/; /m/michele-mastromattei/; /f/francesca-fallucchi/; /n/noemi-scarpato/; /f/fabio-massimo-zanzotto/",
        "bibtex": "@inproceedings{ruzzetti-etal-2022-lacking,\n    title = \"Lacking the Embedding of a Word? Look it up into a Traditional Dictionary\",\n    author = \"Ruzzetti, Elena Sofia  and\n      Ranaldi, Leonardo  and\n      Mastromattei, Michele  and\n      Fallucchi, Francesca  and\n      Scarpato, Noemi  and\n      Zanzotto, Fabio Massimo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.208/\",\n    doi = \"10.18653/v1/2022.findings-acl.208\",\n    pages = \"2651--2662\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.208.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.208/",
        "pdf_size": 1505032,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=332500442276754470&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 11,
        "aff": "University of Rome Tor Vergata, Italy; Guglielmo Marconi University, Italy; Campus Bio-Medico University, Italy+University of Rome Tor Vergata, Italy; Guglielmo Marconi University, Italy; San Raffaele Roma Open University, Italy; University of Rome Tor Vergata, Italy",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2+0;1;3;0",
        "aff_unique_norm": "University of Rome Tor Vergata;Guglielmo Marconi University;Campus Bio-Medico University;San Raffaele Roma Open University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uniroma2.it;https://www.gmu.edu.it;https://www.unicampus.it;https://www.sroiu.eu",
        "aff_unique_abbr": "UniRoma2;GMU;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.acl-long.472",
        "title": "Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While neural text-to-speech systems perform remarkably well in high-resource scenarios, they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data. In this work, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations that hold across languages. In conjunction with language agnostic meta learning, this enables us to fine-tune a high-quality text-to-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker.",
        "author": "Florian Lux; Thang Vu",
        "authorids": "/f/florian-lux/; /t/thang-vu/",
        "bibtex": "@inproceedings{lux-vu-2022-language,\n    title = \"Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features\",\n    author = \"Lux, Florian  and\n      Vu, Thang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.472/\",\n    doi = \"10.18653/v1/2022.acl-long.472\",\n    pages = \"6858--6868\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.472.pdf",
        "site": "https://aclanthology.org/2022.acl-long.472/",
        "pdf_size": 605371,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7717846647538740239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Natural Language Processing, University of Stuttgart; Institute for Natural Language Processing, University of Stuttgart",
        "aff_domain": "ims.uni-stuttgart.de; ",
        "email": "ims.uni-stuttgart.de; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Stuttgart",
        "aff_unique_dep": "Institute for Natural Language Processing",
        "aff_unique_url": "https://www.uni-stuttgart.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.62",
        "title": "Language-agnostic BERT Sentence Embedding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%. Composing the best of these methods produces a model that achieves 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "author": "Fangxiaoyu Feng; Yinfei Yang; Daniel Cer; Naveen Arivazhagan; Wei Wang",
        "authorids": "/f/fangxiaoyu-feng/; /y/yinfei-yang/; /d/daniel-cer/; /n/naveen-arivazhagan/; /w/wei-wang/",
        "bibtex": "@inproceedings{feng-etal-2022-language,\n    title = \"Language-agnostic {BERT} Sentence Embedding\",\n    author = \"Feng, Fangxiaoyu  and\n      Yang, Yinfei  and\n      Cer, Daniel  and\n      Arivazhagan, Naveen  and\n      Wang, Wei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.62/\",\n    doi = \"10.18653/v1/2022.acl-long.62\",\n    pages = \"878--891\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.62.pdf",
        "site": "https://aclanthology.org/2022.acl-long.62/",
        "pdf_size": 607051,
        "gs_citation": 1081,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5496460748851269794&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Google AI; Google AI\u2020; Google AI; Google AI; Google AI\u2020",
        "aff_domain": "google.com;gmail.com;google.com;google.com;gmail.com",
        "email": "google.com;gmail.com;google.com;google.com;gmail.com",
        "github": "",
        "project": "https://tfhub.dev/google/LaBSE",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google AI",
        "aff_unique_url": "https://ai.google",
        "aff_unique_abbr": "Google AI",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.325",
        "title": "Large Scale Substitution-based Word Sense Induction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a word-sense induction method based on pre-trained masked language models (MLMs), which can cheaply scale to large vocabularies and large corpora. The result is a corpus which is sense-tagged according to a corpus-derived sense inventory and where each sense is associated with indicative words. Evaluation on English Wikipedia that was sense-tagged using our method shows that both the induced senses, and the per-instance sense assignment, are of high quality even compared to WSD methods, such as Babelfy. Furthermore, by training a static word embeddings algorithm on the sense-tagged corpus, we obtain high-quality static senseful embeddings. These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed. The data driven nature of the algorithm allows to induce corpora-specific senses, which may not appear in standard sense inventories, as we demonstrate using a case study on the scientific domain.",
        "author": "Matan Eyal; Shoval Sadde; Hillel Taub-Tabib; Yoav Goldberg",
        "authorids": "/m/matan-eyal/; /s/shoval-sadde/; /h/hillel-taub-tabib/; /y/yoav-goldberg/",
        "bibtex": "@inproceedings{eyal-etal-2022-large,\n    title = \"Large Scale Substitution-based Word Sense Induction\",\n    author = \"Eyal, Matan  and\n      Sadde, Shoval  and\n      Taub-Tabib, Hillel  and\n      Goldberg, Yoav\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.325/\",\n    doi = \"10.18653/v1/2022.acl-long.325\",\n    pages = \"4738--4752\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.325.pdf",
        "site": "https://aclanthology.org/2022.acl-long.325/",
        "pdf_size": 467309,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17512286409952763919&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Allen Institute for AI, Israel; Allen Institute for AI, Israel; Allen Institute for AI, Israel; Allen Institute for AI, Israel + Bar Ilan University, Ramat-Gan, Israel",
        "aff_domain": "allenai.org;allenai.org;allenai.org;allenai.org",
        "email": "allenai.org;allenai.org;allenai.org;allenai.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Allen Institute for AI;Bar-Ilan University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.allenai.org;https://www.biu.ac.il",
        "aff_unique_abbr": "AI2;BIU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ramat-Gan",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.findings-acl.179",
        "title": "Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Traditional methods for named entity recognition (NER) classify mentions into a fixed set of pre-defined entity types. However, in many real-world scenarios, new entity types are incrementally involved. To investigate this problem, continual learning is introduced for NER. However, the existing method depends on the relevance between tasks and is prone to inter-type confusion. In this paper, we propose a novel two-stage framework Learn-and-Review (L&R) for continual NER under the type-incremental setting to alleviate the above issues. Specifically, for the learning stage, we distill the old knowledge from teacher to a student on the current dataset. For the reviewing stage, we first generate synthetic samples of old types to augment the dataset. Then, we further distill new knowledge from the above student and old knowledge from the teacher to get an enhanced student on the augmented dataset. This stage has the following advantages: (1) The synthetic samples mitigate the gap between the old and new task and thus enhance the further distillation; (2) Different types of entities are jointly seen during training which alleviates the inter-type confusion. Experimental results show that L&R outperforms the state-of-the-art method on CoNLL-03 and OntoNotes-5.0.",
        "author": "Yu Xia; Quan Wang; Yajuan Lyu; Yong Zhu; Wenhao Wu; Sujian Li; Dai Dai",
        "authorids": "/y/yu-xia/; /q/quan-wang/; /y/yajuan-lyu/; /y/yong-zhu/; /w/wenhao-wu/; /s/sujian-li/; /d/dai-dai/",
        "bibtex": "@inproceedings{xia-etal-2022-learn,\n    title = \"Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples\",\n    author = \"Xia, Yu  and\n      Wang, Quan  and\n      Lyu, Yajuan  and\n      Zhu, Yong  and\n      Wu, Wenhao  and\n      Li, Sujian  and\n      Dai, Dai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.179/\",\n    doi = \"10.18653/v1/2022.findings-acl.179\",\n    pages = \"2291--2300\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.179.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.179/",
        "pdf_size": 570848,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11288930750361919606&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Key Laboratory of Computational Linguistics, Peking University, MOE, China+Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Baidu Inc., Beijing, China",
        "aff_domain": "pku.edu.cn;baidu.com;baidu.com;baidu.com;pku.edu.cn;pku.edu.cn;baidu.com",
        "email": "pku.edu.cn;baidu.com;baidu.com;baidu.com;pku.edu.cn;pku.edu.cn;baidu.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;1;0;0;1",
        "aff_unique_norm": "Peking University;Baidu",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics;Baidu Inc.",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.baidu.com",
        "aff_unique_abbr": "PKU;Baidu",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.39",
        "title": "Learn to Adapt for Generalized Zero-Shot Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generalized zero-shot text classification aims to classify textual instances from both previously seen classes and incrementally emerging unseen classes. Most existing methods generalize poorly since the learned parameters are only optimal for seen classes rather than for both classes, and the parameters keep stationary in predicting procedures. To address these challenges, we propose a novel Learn to Adapt (LTA) network using a variant meta-learning framework. Specifically, LTA trains an adaptive classifier by using both seen and virtual unseen classes to simulate a generalized zero-shot learning (GZSL) scenario in accordance with the test time, and simultaneously learns to calibrate the class prototypes and sample representations to make the learned parameters adaptive to incoming unseen classes. We claim that the proposed model is capable of representing all prototypes and samples from both classes to a more consistent distribution in a global space. Extensive experiments on five text classification datasets show that our model outperforms several competitive previous approaches by large margins. The code and the whole datasets are available at https://github.com/Quareia/LTA.",
        "author": "Yiwen Zhang; Caixia Yuan; Xiaojie Wang; Ziwei Bai; Yongbin Liu",
        "authorids": "/y/yiwen-zhang/; /c/caixia-yuan/; /x/xiaojie-wang/; /z/ziwei-bai/; /y/yongbin-liu/",
        "bibtex": "@inproceedings{zhang-etal-2022-learn,\n    title = \"Learn to Adapt for Generalized Zero-Shot Text Classification\",\n    author = \"Zhang, Yiwen  and\n      Yuan, Caixia  and\n      Wang, Xiaojie  and\n      Bai, Ziwei  and\n      Liu, Yongbin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.39/\",\n    doi = \"10.18653/v1/2022.acl-long.39\",\n    pages = \"517--527\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.39.pdf",
        "site": "https://aclanthology.org/2022.acl-long.39/",
        "pdf_size": 547793,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17748160605515759266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/Quareia/LTA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.220",
        "title": "Learned Incremental Representations for Parsing",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence, where the label is predicted using strictly incremental processing of a prefix of the sentence, and the sequence of labels for a sentence fully determines a parse tree. Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input, in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses. Our learned representations achieve 93.72 F1 on the Penn Treebank with as few as 5 bits per word, and at 8 bits per word they achieve 94.97 F1, which is comparable with other state of the art parsing models when using the same pre-trained embeddings. We also provide an analysis of the representations learned by our system, investigating properties such as the interpretable syntactic features captured by the system and mechanisms for deferred resolution of syntactic ambiguities.",
        "author": "Nikita Kitaev; Thomas Lu; Dan Klein",
        "authorids": "/n/nikita-kitaev/; /t/thomas-lu/; /d/dan-klein/",
        "bibtex": "@inproceedings{kitaev-etal-2022-learned,\n    title = \"Learned Incremental Representations for Parsing\",\n    author = \"Kitaev, Nikita  and\n      Lu, Thomas  and\n      Klein, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.220/\",\n    doi = \"10.18653/v1/2022.acl-long.220\",\n    pages = \"3086--3095\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.220.pdf",
        "site": "https://aclanthology.org/2022.acl-long.220/",
        "pdf_size": 240102,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12333829505822683149&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.74",
        "title": "Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present a comprehensive study of sparse attention patterns in Transformer models. We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model. Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages. We also demonstrate that a flexible approach to attention, with different patterns across different layers of the model, is beneficial for some tasks. Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns\u2014during fine-tuning\u2014different attention patterns for each Transformer layer depending on the downstream task. Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens\u2014for each task and model layer\u2014and focuses attention on those. It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training.",
        "author": "Zihan Wang; Jiuxiang Gu; Jason Kuen; Handong Zhao; Vlad Morariu; Ruiyi Zhang; Ani Nenkova; Tong Sun; Jingbo Shang",
        "authorids": "/z/zihan-wang/; /j/jiuxiang-gu/; /j/jason-kuen/; /h/handong-zhao/; /v/vlad-morariu/; /r/ruiyi-zhang/; /a/ani-nenkova/; /t/tong-sun/; /j/jingbo-shang/",
        "bibtex": "@inproceedings{wang-etal-2022-learning,\n    title = \"Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns\",\n    author = \"Wang, Zihan  and\n      Gu, Jiuxiang  and\n      Kuen, Jason  and\n      Zhao, Handong  and\n      Morariu, Vlad  and\n      Zhang, Ruiyi  and\n      Nenkova, Ani  and\n      Sun, Tong  and\n      Shang, Jingbo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.74/\",\n    doi = \"10.18653/v1/2022.findings-acl.74\",\n    pages = \"916--925\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.74.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.74/",
        "pdf_size": 400380,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2854401474138850614&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, San Diego; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of California, San Diego",
        "aff_domain": "ucsd.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;ucsd.edu",
        "email": "ucsd.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;1;1;1;0",
        "aff_unique_norm": "University of California, San Diego;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucsd.edu;https://research.adobe.com",
        "aff_unique_abbr": "UCSD;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.542",
        "title": "Learning Adaptive Segmentation Policy for End-to-End Simultaneous Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency. A typical simultaneous translation (ST) system consists of a speech translation model and a policy module, which determines when to wait and when to translate. Thus the policy is crucial to balance translation quality and latency. Conventional methods usually adopt fixed policies, e.g. segmenting the source speech with a fixed length and generating translation. However, this method ignores contextual information and suffers from low translation quality. This paper proposes an adaptive segmentation policy for end-to-end ST. Inspired by human interpreters, the policy learns to segment the source streaming speech into meaningful units by considering both acoustic features and translation history, maintaining consistency between the segmentation and translation. Experimental results on English-German and Chinese-English show that our method achieves a good accuracy-latency trade-off over recently proposed state-of-the-art methods.",
        "author": "Ruiqing Zhang; Zhongjun He; Hua Wu; Haifeng Wang",
        "authorids": "/r/ruiqing-zhang/; /z/zhongjun-he/; /h/hua-wu/; /h/haifeng-wang/",
        "bibtex": "@inproceedings{zhang-etal-2022-learning,\n    title = \"Learning Adaptive Segmentation Policy for End-to-End Simultaneous Translation\",\n    author = \"Zhang, Ruiqing  and\n      He, Zhongjun  and\n      Wu, Hua  and\n      Wang, Haifeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.542/\",\n    doi = \"10.18653/v1/2022.acl-long.542\",\n    pages = \"7862--7874\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.542.pdf",
        "site": "https://aclanthology.org/2022.acl-long.542/",
        "pdf_size": 1252797,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16508452721508301616&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China; Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China; Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China; Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Baidu Inc.",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.90",
        "title": "Learning Bias-reduced Word Embeddings Using Dictionary Definitions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pre-trained word embeddings, such as GloVe, have shown undesirable gender, racial, and religious biases. To address this problem, we propose DD-GloVe, a train-time debiasing algorithm to learn word embeddings by leveraging \u00a0\u0332dictionary \u00a0\u0332definitions. We introduce dictionary-guided loss functions that encourage word embeddings to be similar to their relatively neutral dictionary definition representations. Existing debiasing algorithms typically need a pre-compiled list of seed words to represent the bias direction, along which biased information gets removed. Producing this list involves subjective decisions and it might be difficult to obtain for some types of biases. We automate the process of finding seed words: our algorithm starts from a single pair of initial seed words and automatically finds more words whose definitions display similar attributes traits. We demonstrate the effectiveness of our approach with benchmark evaluations and empirical analyses. Our code is available at https://github.com/haozhe-an/DD-GloVe.",
        "author": "Haozhe An; Xiaojiang Liu; Donald Zhang",
        "authorids": "/h/haozhe-an/; /x/xiaojiang-liu/; /d/donald-zhang/",
        "bibtex": "@inproceedings{an-etal-2022-learning,\n    title = \"Learning Bias-reduced Word Embeddings Using Dictionary Definitions\",\n    author = \"An, Haozhe  and\n      Liu, Xiaojiang  and\n      Zhang, Donald\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.90/\",\n    doi = \"10.18653/v1/2022.findings-acl.90\",\n    pages = \"1139--1152\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.90.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.90/",
        "pdf_size": 1223011,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17085483813407920661&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 2,
        "aff": "University of Maryland, College Park; Apple; Apple",
        "aff_domain": "umd.edu;apple.com;apple.com",
        "email": "umd.edu;apple.com;apple.com",
        "github": "https://github.com/haozhe-an/DD-GloVe",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Maryland;Apple",
        "aff_unique_dep": ";Apple Inc.",
        "aff_unique_url": "https://www/umd.edu;https://www.apple.com",
        "aff_unique_abbr": "UMD;Apple",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.167",
        "title": "Learning Confidence for Transformer-based Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Confidence estimation aims to quantify the confidence of the model prediction, providing an expectation of success. A well-calibrated confidence estimate enables accurate failure prediction and proper risk measurement when given noisy samples and out-of-distribution data in real-world settings. However, this task remains a severe challenge for neural machine translation (NMT), where probabilities from softmax distribution fail to describe when the model is probably mistaken. To address this problem, we propose an unsupervised confidence estimate learning jointly with the training of the NMT model. We explain confidence as how many hints the NMT model needs to make a correct prediction, and more hints indicate low confidence. Specifically, the NMT model is given the option to ask for hints to improve translation accuracy at the cost of some slight penalty. Then, we approximate their level of confidence by counting the number of hints the model uses. We demonstrate that our learned confidence estimate achieves high accuracy on extensive sentence/word-level quality estimation tasks. Analytical results verify that our confidence estimate can correctly assess underlying risk in two real-world scenarios: (1) discovering noisy samples and (2) detecting out-of-domain data. We further propose a novel confidence-based instance-specific label smoothing approach based on our learned confidence estimate, which outperforms standard label smoothing.",
        "author": "Yu Lu; Jiali Zeng; Jiajun Zhang; Shuangzhi Wu; Mu Li",
        "authorids": "/y/yu-lu/; /j/jiali-zeng/; /j/jiajun-zhang/; /s/shuangzhi-wu/; /m/mu-li/",
        "bibtex": "@inproceedings{lu-etal-2022-learning,\n    title = \"Learning Confidence for Transformer-based Neural Machine Translation\",\n    author = \"Lu, Yu  and\n      Zeng, Jiali  and\n      Zhang, Jiajun  and\n      Wu, Shuangzhi  and\n      Li, Mu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.167/\",\n    doi = \"10.18653/v1/2022.acl-long.167\",\n    pages = \"2353--2364\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.167.pdf",
        "site": "https://aclanthology.org/2022.acl-long.167/",
        "pdf_size": 934933,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2069466512970541067&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Tencent Cloud Xiaowei, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Tencent Cloud Xiaowei, Beijing, China; Tencent Cloud Xiaowei, Beijing, China",
        "aff_domain": "nlpr.ia.ac.cn;tencent.com;nlpr.ia.ac.cn;tencent.com;tencent.com",
        "email": "nlpr.ia.ac.cn;tencent.com;nlpr.ia.ac.cn;tencent.com;tencent.com",
        "github": "https://github.com/yulu-dada/Learned-conf-NMT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;0+1;2;2",
        "aff_unique_norm": "National Laboratory of Pattern Recognition;University of Chinese Academy of Sciences;Tencent",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;Tencent Cloud Xiaowei",
        "aff_unique_url": ";http://www.ucas.ac.cn;https://cloud.tencent.com",
        "aff_unique_abbr": ";UCAS;Tencent Cloud",
        "aff_campus_unique_index": "0+0;0;0+0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.574",
        "title": "Learning Disentangled Representations of Negation and Uncertainty",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Negation and uncertainty modeling are long-standing tasks in natural language processing. Linguistic theory postulates that expressions of negation and uncertainty are semantically independent from each other and the content they modify. However, previous works on representation learning do not explicitly model this independence. We therefore attempt to disentangle the representations of negation, uncertainty, and content using a Variational Autoencoder. We find that simply supervising the latent representations results in good disentanglement, but auxiliary objectives based on adversarial learning and mutual information minimization can provide additional disentanglement gains.",
        "author": "Jake Vasilakes; Chrysoula Zerva; Makoto Miwa; Sophia Ananiadou",
        "authorids": "/j/jake-vasilakes/; /c/chrysoula-zerva/; /m/makoto-miwa/; /s/sophia-ananiadou/",
        "bibtex": "@inproceedings{vasilakes-etal-2022-learning,\n    title = \"Learning Disentangled Representations of Negation and Uncertainty\",\n    author = \"Vasilakes, Jake  and\n      Zerva, Chrysoula  and\n      Miwa, Makoto  and\n      Ananiadou, Sophia\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.574/\",\n    doi = \"10.18653/v1/2022.acl-long.574\",\n    pages = \"8380--8397\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.574.pdf",
        "site": "https://aclanthology.org/2022.acl-long.574/",
        "pdf_size": 926099,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2082695494932634820&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "National Centre for Text Mining, Department of Computer Science, The University of Manchester; Instituto Superior T\u00e9cnico+Instituto de Telecomunica\u00e7\u00f5es; Toyota Technological Institute; Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology",
        "aff_domain": "manchester.ac.uk;tecnico.ulisboa.pt;toyota-ti.ac.jp;manchester.ac.uk",
        "email": "manchester.ac.uk;tecnico.ulisboa.pt;toyota-ti.ac.jp;manchester.ac.uk",
        "github": "https://github.com/jvasilakes/disentanglement-vae",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;3;4",
        "aff_unique_norm": "University of Manchester;Instituto Superior T\u00e9cnico;Instituto de Telecomunica\u00e7\u00f5es;Toyota Technological Institute;National Institute of Advanced Industrial Science and Technology",
        "aff_unique_dep": "Department of Computer Science;;;;Artificial Intelligence Research Center",
        "aff_unique_url": "https://www.manchester.ac.uk;https://www.ist.utl.pt;https://www.it.pt;https://www.tti.ac.jp;https://www.aist.go.jp",
        "aff_unique_abbr": "UoM;IST;;TTI;AIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+1;2;2",
        "aff_country_unique": "United Kingdom;Portugal;Japan"
    },
    {
        "id": "2022.acl-long.70",
        "title": "Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual pre-trained models are able to zero-shot transfer knowledge from rich-resource to low-resource languages in machine reading comprehension (MRC). However, inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language. In this paper, we propose a novel multilingual MRC framework equipped with a Siamese Semantic Disentanglement Model (S2DM) to disassociate semantics from syntax in representations learned by multilingual pre-trained models. To explicitly transfer only semantic knowledge to the target language, we propose two groups of losses tailored for semantic and syntactic encoding and disentanglement. Experimental results on three multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the effectiveness of our proposed approach over models based on mBERT and XLM-100.",
        "author": "Linjuan Wu; Shaojuan Wu; Xiaowang Zhang; Deyi Xiong; Shizhan Chen; Zhiqiang Zhuang; Zhiyong Feng",
        "authorids": "/l/linjuan-wu/; /s/shaojuan-wu/; /x/xiaowang-zhang/; /d/deyi-xiong/; /s/shizhan-chen/; /z/zhiqiang-zhuang/; /z/zhiyong-feng/",
        "bibtex": "@inproceedings{wu-etal-2022-learning,\n    title = \"Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension\",\n    author = \"Wu, Linjuan  and\n      Wu, Shaojuan  and\n      Zhang, Xiaowang  and\n      Xiong, Deyi  and\n      Chen, Shizhan  and\n      Zhuang, Zhiqiang  and\n      Feng, Zhiyong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.70/\",\n    doi = \"10.18653/v1/2022.acl-long.70\",\n    pages = \"991--1000\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.70.pdf",
        "site": "https://aclanthology.org/2022.acl-long.70/",
        "pdf_size": 2089541,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11121802186747430237&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "1College of Intelligence and Computing, Tianjin University, Tianjin, China, 300350 + 2Tianjin University-Aishu Data Intelligence Joint Laboratory, Tianjin, China; 1College of Intelligence and Computing, Tianjin University, Tianjin, China, 300350 + 2Tianjin University-Aishu Data Intelligence Joint Laboratory, Tianjin, China; 1College of Intelligence and Computing, Tianjin University, Tianjin, China, 300350; 1College of Intelligence and Computing, Tianjin University, Tianjin, China, 300350; 1College of Intelligence and Computing, Tianjin University, Tianjin, China, 300350; 1College of Intelligence and Computing, Tianjin University, Tianjin, China, 300350; 1College of Intelligence and Computing, Tianjin University, Tianjin, China, 300350",
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn; ; ; ",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0;0;0;0;0",
        "aff_unique_norm": "Tianjin University;Tianjin University-Aishu Data Intelligence Joint Laboratory",
        "aff_unique_dep": "College of Intelligence and Computing;",
        "aff_unique_url": "http://www.tju.edu.cn;",
        "aff_unique_abbr": "Tianjin U;",
        "aff_campus_unique_index": "0+0;0+0;0;0;0;0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.187",
        "title": "Learning Disentangled Textual Representations via Statistical Measures of Similarity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "When working with textual data, a natural application of disentangled representations is the fair classification where the goal is to make predictions without being biased (or influenced) by sensible attributes that may be present in the data (e.g., age, gender or race). Dominant approaches to disentangle a sensitive attribute from textual representations rely on learning simultaneously a penalization term that involves either an adversary loss (e.g., a discriminator) or an information measure (e.g., mutual information). However, these methods require the training of a deep neural network with several parameter updates for each update of the representation model. As a matter of fact, the resulting nested optimization loop is both times consuming, adding complexity to the optimization dynamic, and requires a fine hyperparameter selection (e.g., learning rates, architecture). In this work, we introduce a family of regularizers for learning disentangled representations that do not require training. These regularizers are based on statistical measures of similarity between the conditional probability distributions with respect to the sensible attributes. Our novel regularizers do not require additional training, are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders.",
        "author": "Pierre Colombo; Guillaume Staerman; Nathan Noiry; Pablo Piantanida",
        "authorids": "/p/pierre-colombo/; /g/guillaume-staerman/; /n/nathan-noiry/; /p/pablo-piantanida/",
        "bibtex": "@inproceedings{colombo-etal-2022-learning,\n    title = \"Learning Disentangled Textual Representations via Statistical Measures of Similarity\",\n    author = \"Colombo, Pierre  and\n      Staerman, Guillaume  and\n      Noiry, Nathan  and\n      Piantanida, Pablo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.187/\",\n    doi = \"10.18653/v1/2022.acl-long.187\",\n    pages = \"2614--2630\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.187.pdf",
        "site": "https://aclanthology.org/2022.acl-long.187/",
        "pdf_size": 905927,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11712376975179064246&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 8,
        "aff": "L2S,CentraleSupelec CNRS Universite Paris-Saclay; LTCI, Telecom Paris, Institut Polytechnique de Paris + althiqa; althiqa + ILLS, Universit\u00e9 McGill - ETS - MILA - CNRS - Universit\u00e9 Paris-Saclay - CentraleSup\u00e9lec; ILLS, Universit\u00e9 McGill - ETS - MILA - CNRS - Universit\u00e9 Paris-Saclay - CentraleSup\u00e9lec",
        "aff_domain": "centralesupelec.fr; ; ; ",
        "email": "centralesupelec.fr; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;2+3;3",
        "aff_unique_norm": "CentraleSup\u00e9lec;Telecom Paris;Althiqa;Universit\u00e9 McGill",
        "aff_unique_dep": "L2S;LTCI;;",
        "aff_unique_url": "https://www.centrale-supelec.fr;https://www.telecom-paris.fr;;https://www.mcgill.ca",
        "aff_unique_abbr": "CS;Telecom Paris;;McGill",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;2;2",
        "aff_country_unique": "France;;Canada"
    },
    {
        "id": "2022.acl-long.342",
        "title": "Learning From Failure: Data Capture in an Australian Aboriginal Community",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most low resource language technology development is premised on the need to collect data for training statistical models. When we follow the typical process of recording and transcribing text for small Indigenous languages, we hit up against the so-called \u201ctranscription bottleneck.\u201d Therefore it is worth exploring new ways of engaging with speakers which generate data while avoiding the transcription bottleneck. We have deployed a prototype app for speakers to use for confirming system guesses in an approach to transcription based on word spotting. However, in the process of testing the app we encountered many new problems for engagement with speakers. This paper presents a close-up study of the process of deploying data capture technology on the ground in an Australian Aboriginal community. We reflect on our interactions with participants and draw lessons that apply to anyone seeking to develop methods for language data collection in an Indigenous community.",
        "author": "Eric Le Ferrand; Steven Bird; Laurent Besacier",
        "authorids": "/e/eric-le-ferrand/; /s/steven-bird/; /l/laurent-besacier/",
        "bibtex": "@inproceedings{le-ferrand-etal-2022-learning,\n    title = \"Learning From Failure: Data Capture in an {A}ustralian Aboriginal Community\",\n    author = \"Le Ferrand, Eric  and\n      Bird, Steven  and\n      Besacier, Laurent\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.342/\",\n    doi = \"10.18653/v1/2022.acl-long.342\",\n    pages = \"4988--4998\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.342.pdf",
        "site": "https://aclanthology.org/2022.acl-long.342/",
        "pdf_size": 3448344,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9251991119967545088&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Northern Institute, Charles Darwin University, Australia+Laboratoire Informatique de Grenoble, Universit\u00e9 Grenoble Alpes, France; Northern Institute, Charles Darwin University, Australia; Laboratoire Informatique de Grenoble, Universit\u00e9 Grenoble Alpes, France",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "Charles Darwin University;Universit\u00e9 Grenoble Alpes",
        "aff_unique_dep": ";Laboratoire Informatique de Grenoble",
        "aff_unique_url": "https://www.cdu.edu.au;https://www.univ-grenoble-alpes.fr",
        "aff_unique_abbr": "CDU;UGA",
        "aff_campus_unique_index": "0+1;0;1",
        "aff_campus_unique": "Northern Institute;Grenoble",
        "aff_country_unique_index": "0+1;0;1",
        "aff_country_unique": "Australia;France"
    },
    {
        "id": "2022.acl-long.275",
        "title": "Learning Functional Distributional Semantics with Visual Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Functional Distributional Semantics is a recently proposed framework for learning distributional semantics that provides linguistic interpretability. It models the meaning of a word as a binary classifier rather than a numerical vector. In this work, we propose a method to train a Functional Distributional Semantics model with grounded visual data. We train it on the Visual Genome dataset, which is closer to the kind of data encountered in human language acquisition than a large text corpus. On four external evaluation datasets, our model outperforms previous work on learning semantics from Visual Genome.",
        "author": "Yinhong Liu; Guy Emerson",
        "authorids": "/y/yinhong-liu/; /g/guy-emerson/",
        "bibtex": "@inproceedings{liu-emerson-2022-learning,\n    title = \"Learning Functional Distributional Semantics with Visual Data\",\n    author = \"Liu, Yinhong  and\n      Emerson, Guy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.275/\",\n    doi = \"10.18653/v1/2022.acl-long.275\",\n    pages = \"3976--3988\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.275.pdf",
        "site": "https://aclanthology.org/2022.acl-long.275/",
        "pdf_size": 2219517,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7028574433750413992&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Technology, University of Cambridge; Department of Computer Science and Technology, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "https://github.com/williamLyh/PixieVGModeltasks",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Computer Science and Technology",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.545",
        "title": "Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text summarization aims to generate a short summary for an input text. In this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS) approach, which does not require parallel data for training. Our NAUS first performs edit-based search towards a heuristically defined score, and generates a summary as pseudo-groundtruth. Then, we train an encoder-only non-autoregressive Transformer based on the search result. We also propose a dynamic programming approach for length-control decoding, which is important for the summarization task. Experiments on two datasets show that NAUS achieves state-of-the-art performance for unsupervised summarization, yet largely improving inference efficiency. Further, our algorithm is able to perform explicit length-transfer summary generation.",
        "author": "Puyuan Liu; Chenyang Huang; Lili Mou",
        "authorids": "/p/puyuan-liu/; /c/chenyang-huang/; /l/lili-mou/",
        "bibtex": "@inproceedings{liu-etal-2022-learning,\n    title = \"Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization\",\n    author = \"Liu, Puyuan  and\n      Huang, Chenyang  and\n      Mou, Lili\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.545/\",\n    doi = \"10.18653/v1/2022.acl-long.545\",\n    pages = \"7916--7929\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.545.pdf",
        "site": "https://aclanthology.org/2022.acl-long.545/",
        "pdf_size": 1417534,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=280342692188248582&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. Computing Science, Alberta Machine Intelligence Institute (Amii), University of Alberta, Canada; Dept. Computing Science, Alberta Machine Intelligence Institute (Amii), University of Alberta, Canada; Dept. Computing Science, Alberta Machine Intelligence Institute (Amii), University of Alberta, Canada",
        "aff_domain": "ualberta.ca;ualberta.ca;gmail.com",
        "email": "ualberta.ca;ualberta.ca;gmail.com",
        "github": "https://github.com/MANGA-UOFA/NAUS",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Dept. Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.findings-acl.129",
        "title": "Learning Reasoning Patterns for Relational Triple Extraction with Mutual Generation of Text and Graph",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Relational triple extraction is a critical task for constructing knowledge graphs. Existing methods focused on learning text patterns from explicit relational mentions. However, they usually suffered from ignoring relational reasoning patterns, thus failed to extract the implicitly implied triples. Fortunately, the graph structure of a sentence\u2019s relational triples can help find multi-hop reasoning paths. Moreover, the type inference logic through the paths can be captured with the sentence\u2019s supplementary relational expressions that represent the real-world conceptual meanings of the paths\u2019 composite relations. In this paper, we propose a unified framework to learn the relational reasoning patterns for this task. To identify multi-hop reasoning paths, we construct a relational graph from the sentence (text-to-graph generation) and apply multi-layer graph convolutions to it. To capture the relation type inference logic of the paths, we propose to understand the unlabeled conceptual expressions by reconstructing the sentence from the relational graph (graph-to-text generation) in a self-supervised manner. Experimental results on several benchmark datasets demonstrate the effectiveness of our method.",
        "author": "Yubo Chen; Yunqi Zhang; Yongfeng Huang",
        "authorids": "/y/yubo-chen/; /y/yunqi-zhang/; /y/yongfeng-huang/",
        "bibtex": "@inproceedings{chen-etal-2022-learning,\n    title = \"Learning Reasoning Patterns for Relational Triple Extraction with Mutual Generation of Text and Graph\",\n    author = \"Chen, Yubo  and\n      Zhang, Yunqi  and\n      Huang, Yongfeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.129/\",\n    doi = \"10.18653/v1/2022.findings-acl.129\",\n    pages = \"1638--1647\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.129.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.129/",
        "pdf_size": 808411,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3231546351732525706&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Electronic Engineering & BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering & BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering & BNRist, Tsinghua University, Beijing, China",
        "aff_domain": "gmail.com;mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "gmail.com;mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.50",
        "title": "Learning When to Translate for Streaming Speech",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "How to find proper moments to generate partial sentence translation given a streaming speech input? Existing approaches waiting-and-translating for a fixed duration often break the acoustic units in speech, since the boundaries between acoustic units in speech are not even. In this paper, we propose MoSST, a simple yet effective method for translating streaming speech content. Given a usually long speech sequence, we develop an efficient monotonic segmentation module inside an encoder-decoder model to accumulate acoustic information incrementally and detect proper speech unit boundaries for the input in speech translation task. Experiments on multiple translation directions of the MuST-C dataset show that outperforms existing methods and achieves the best trade-off between translation quality (BLEU) and latency. Our code is available at https://github.com/dqqcasia/mosst.",
        "author": "Qian Dong; Yaoming Zhu; Mingxuan Wang; Lei Li",
        "authorids": "/q/qian-dong/; /y/yaoming-zhu/; /m/mingxuan-wang/; /l/lei-li/",
        "bibtex": "@inproceedings{dong-etal-2022-learning,\n    title = \"Learning When to Translate for Streaming Speech\",\n    author = \"Dong, Qian  and\n      Zhu, Yaoming  and\n      Wang, Mingxuan  and\n      Li, Lei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.50/\",\n    doi = \"10.18653/v1/2022.acl-long.50\",\n    pages = \"680--694\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.50.pdf",
        "site": "https://aclanthology.org/2022.acl-long.50/",
        "pdf_size": 2502196,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9089723077572559709&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab; University of California, Santa Barbara",
        "aff_domain": "bytedance.com;bytedance.com;bytedance.com;cs.ucsb.edu",
        "email": "bytedance.com;bytedance.com;bytedance.com;cs.ucsb.edu",
        "github": "https://github.com/dqqcasia/mosst",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "ByteDance;University of California, Santa Barbara",
        "aff_unique_dep": "AI Lab;",
        "aff_unique_url": "https://www.bytedance.com;https://www.ucsb.edu",
        "aff_unique_abbr": "ByteDance;UCSB",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Santa Barbara",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.81",
        "title": "Learning and Evaluating Character Representations in Novels",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We address the problem of learning fixed-length vector representations of characters in novels. Recent advances in word embeddings have proven successful in learning entity representations from short texts, but fall short on longer documents because they do not capture full book-level information. To overcome the weakness of such text-based embeddings, we propose two novel methods for representing characters: (i) graph neural network-based embeddings from a full corpus-based character network; and (ii) low-dimensional embeddings constructed from the occurrence pattern of characters in each novel. We test the quality of these character embeddings using a new benchmark suite to evaluate character representations, encompassing 12 different tasks. We show that our representation techniques combined with text-based embeddings lead to the best character representations, outperforming text-based embeddings in four tasks. Our dataset and evaluation script will be made publicly available to stimulate additional work in this area.",
        "author": "Naoya Inoue; Charuta Pethe; Allen Kim; Steven Skiena",
        "authorids": "/n/naoya-inoue/; /c/charuta-pethe/; /a/allen-kim/; /s/steven-skiena/",
        "bibtex": "@inproceedings{inoue-etal-2022-learning,\n    title = \"Learning and Evaluating Character Representations in Novels\",\n    author = \"Inoue, Naoya  and\n      Pethe, Charuta  and\n      Kim, Allen  and\n      Skiena, Steven\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.81/\",\n    doi = \"10.18653/v1/2022.findings-acl.81\",\n    pages = \"1008--1019\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.81.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.81/",
        "pdf_size": 740118,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4718185812493623250&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Stony Brook University; Stony Brook University; Stony Brook University; Stony Brook University",
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.119",
        "title": "Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Commonsense inference poses a unique challenge to reason and generate the physical, social, and causal conditions of a given event. Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs. However, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quality. In this paper, we focus on addressing missing relations in commonsense knowledge graphs, and propose a novel contrastive learning framework called SOLAR. Our framework contrasts sets of semantically similar and dissimilar events, learning richer inferential knowledge compared to existing approaches. Empirical results demonstrate the efficacy of SOLAR in commonsense inference of diverse commonsense knowledge graphs. Specifically, SOLAR outperforms the state-of-the-art commonsense transformer on commonsense inference with ConceptNet by 1.84% on average among 8 automatic evaluation metrics. In-depth analysis of SOLAR sheds light on the effects of the missing relations utilized in learning commonsense knowledge graphs.",
        "author": "Yong-Ho Jung; Jun-Hyung Park; Joon-Young Choi; Mingyu Lee; Junho Kim; Kang-Min Kim; SangKeun Lee",
        "authorids": "/y/yong-ho-jung/; /j/jun-hyung-park/; /j/joon-young-choi/; /m/mingyu-lee/; /j/junho-kim/; /k/kang-min-kim/; /s/sangkeun-lee/",
        "bibtex": "@inproceedings{jung-etal-2022-learning,\n    title = \"Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference\",\n    author = \"Jung, Yong-Ho  and\n      Park, Jun-Hyung  and\n      Choi, Joon-Young  and\n      Lee, Mingyu  and\n      Kim, Junho  and\n      Kim, Kang-Min  and\n      Lee, SangKeun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.119/\",\n    doi = \"10.18653/v1/2022.findings-acl.119\",\n    pages = \"1514--1523\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.119.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.119/",
        "pdf_size": 717559,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4326813946961245134&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Engineering, Korea University; Department of Computer Science and Engineering, Korea University; Department of Artificial Intelligence, Korea University; Department of Artificial Intelligence, Korea University; Department of Artificial Intelligence, Korea University; Department of Data Science, The Catholic University of Korea; Department of Computer Science and Engineering+Department of Artificial Intelligence, Korea University",
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr;catholic.ac.kr;korea.ac.kr",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr;catholic.ac.kr;korea.ac.kr",
        "github": "https://github.com/yongho94/solar-framework_commonsense-inference",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;2+0",
        "aff_unique_norm": "Korea University;Catholic University of Korea;University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Data Science;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.korea.ac.kr;http://www.cukorea.ac.kr;https://cse.ucsd.edu",
        "aff_unique_abbr": "KU;CUK;UCSD CSE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;1+0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2022.acl-long.147",
        "title": "Learning from Sibling Mentions with Scalable Graph Inference in Fine-Grained Entity Typing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we firstly empirically find that existing models struggle to handle hard mentions due to their insufficient contexts, which consequently limits their overall typing performance. To this end, we propose to exploit sibling mentions for enhancing the mention representations. Specifically, we present two different metrics for sibling selection and employ an attentive graph neural network to aggregate information from sibling mentions. The proposed graph model is scalable in that unseen test mentions are allowed to be added as new nodes for inference. Exhaustive experiments demonstrate the effectiveness of our sibling learning strategy, where our model outperforms ten strong baselines. Moreover, our experiments indeed prove the superiority of sibling mentions in helping clarify the types for hard mentions.",
        "author": "Yi Chen; Jiayang Cheng; Haiyun Jiang; Lemao Liu; Haisong Zhang; Shuming Shi; Ruifeng Xu",
        "authorids": "/y/yi-chen/; /j/jiayang-cheng/; /h/haiyun-jiang/; /l/lemao-liu/; /h/haisong-zhang/; /s/shuming-shi/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{chen-etal-2022-learning-sibling,\n    title = \"Learning from Sibling Mentions with Scalable Graph Inference in Fine-Grained Entity Typing\",\n    author = \"Chen, Yi  and\n      Cheng, Jiayang  and\n      Jiang, Haiyun  and\n      Liu, Lemao  and\n      Zhang, Haisong  and\n      Shi, Shuming  and\n      Xu, Ruifeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.147/\",\n    doi = \"10.18653/v1/2022.acl-long.147\",\n    pages = \"2076--2087\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.147.pdf",
        "site": "https://aclanthology.org/2022.acl-long.147/",
        "pdf_size": 607524,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10580896482048251892&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China+Joint Lab of HITSZ and China Merchants Securities, Shenzhen, China; Department of Computer Science and Engineering, HKUST; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Huawei Technologies; Huawei Technologies; Huawei Technologies; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China+Peng Cheng Laboratory, Shenzhen, China",
        "aff_domain": "gmail.com;cse.ust.hk; ; ; ; ;hit.edu.cn",
        "email": "gmail.com;cse.ust.hk; ; ; ; ;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;0;3;3;3;0+4",
        "aff_unique_norm": "Harbin Institute of Technology;Harbin Institute of Technology Shenzhen (HITSZ);Hong Kong University of Science and Technology;Huawei;Pengcheng Laboratory",
        "aff_unique_dep": "School of Computer Science and Technology;Joint Lab;Department of Computer Science and Engineering;Huawei Technologies;Peng Cheng Laboratory",
        "aff_unique_url": "http://www.hit.edu.cn/;http://en.hitsz.edu.cn/;https://www.hkust.edu.hk;https://www.huawei.com;",
        "aff_unique_abbr": "HIT;HITSZ;HKUST;Huawei;",
        "aff_campus_unique_index": "0+0;1;0;0+0",
        "aff_campus_unique": "Shenzhen;Hong Kong SAR;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.549",
        "title": "Learning the Beauty in Songs: Neural Singing Voice Beautifier",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We are interested in a novel task, singing voice beautification (SVB). Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre. Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic quality. Hence, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone. In NSVB, we propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic Time Warping (SADTW), which ameliorates the robustness of existing time-warping approaches, to synchronize the amateur recording with the template pitch curve. Furthermore, we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one. To achieve this, we also propose a new dataset containing parallel singing recordings of both amateur and professional versions. Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics. Audio samples are available at https://neuralsvb.github.io. Codes: https://github.com/MoonInTheRiver/NeuralSVB.",
        "author": "Jinglin Liu; Chengxi Li; Yi Ren; Zhiying Zhu; Zhou Zhao",
        "authorids": "/j/jinglin-liu/; /c/chengxi-li/; /y/yi-ren/; /z/zhiying-zhu/; /z/zhou-zhao/",
        "bibtex": "@inproceedings{liu-etal-2022-learning-beauty,\n    title = \"Learning the Beauty in Songs: Neural Singing Voice Beautifier\",\n    author = \"Liu, Jinglin  and\n      Li, Chengxi  and\n      Ren, Yi  and\n      Zhu, Zhiying  and\n      Zhao, Zhou\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.549/\",\n    doi = \"10.18653/v1/2022.acl-long.549\",\n    pages = \"7970--7983\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.549.pdf",
        "site": "https://aclanthology.org/2022.acl-long.549/",
        "pdf_size": 5044512,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17442799639671484493&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "https://github.com/MoonInTheRiver/NeuralSVB",
        "project": "https://neuralsvb.github.io",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.231",
        "title": "Learning to Describe Solutions for Bug Reports Based on Developer Discussions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "When a software bug is reported, developers engage in a discussion to collaboratively resolve it. While the solution is likely formulated within the discussion, it is often buried in a large amount of text, making it difficult to comprehend and delaying its implementation. To expedite bug resolution, we propose generating a concise natural language description of the solution by synthesizing relevant content within the discussion, which encompasses both natural language and source code. We build a corpus for this task using a novel technique for obtaining noisy supervision from repository changes linked to bug reports, with which we establish benchmarks. We also design two systems for generating a description during an ongoing discussion by classifying when sufficient context for performing the task emerges in real-time. With automated and human evaluation, we find this task to form an ideal testbed for complex reasoning in long, bimodal dialogue context.",
        "author": "Sheena Panthaplackel; Junyi Jessy Li; Milos Gligoric; Ray Mooney",
        "authorids": "/s/sheena-panthaplackel/; /j/junyi-jessy-li/; /m/milos-gligoric/; /r/ray-mooney/",
        "bibtex": "@inproceedings{panthaplackel-etal-2022-learning,\n    title = \"Learning to Describe Solutions for Bug Reports Based on Developer Discussions\",\n    author = \"Panthaplackel, Sheena  and\n      Li, Junyi Jessy  and\n      Gligoric, Milos  and\n      Mooney, Ray\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.231/\",\n    doi = \"10.18653/v1/2022.findings-acl.231\",\n    pages = \"2935--2952\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.231.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.231/",
        "pdf_size": 947911,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16875034811928858752&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science; Department of Linguistics; Department of Electrical and Computer Engineering; Department of Computer Science",
        "aff_domain": "cs.utexas.edu;austin.utexas.edu;utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;austin.utexas.edu;utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Unknown Institution;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science;Department of Linguistics",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.acl-long.546",
        "title": "Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "The principal task in supervised neural machine translation (NMT) is to learn to generate target sentences conditioned on the source inputs from a set of parallel sentence pairs, and thus produce a model capable of generalizing to unseen instances. However, it is commonly observed that the generalization performance of the model is highly influenced by the amount of parallel data used in training. Although data augmentation is widely used to enrich the training data, conventional methods with discrete manipulations fail to generate diverse and faithful training samples. In this paper, we present a novel data augmentation paradigm termed Continuous Semantic Augmentation (CsaNMT), which augments each training instance with an adjacency semantic region that could cover adequate variants of literal expression under the same meaning. We conduct extensive experiments on both rich-resource and low-resource settings involving various language pairs, including WMT14 English\u2192{German,French}, NIST Chinese\u2192English and multiple low-resource IWSLT translation tasks. The provided empirical evidences show that CsaNMT sets a new level of performance among existing augmentation techniques, improving on the state-of-the-art by a large margin. The core codes are contained in Appendix E.",
        "author": "Xiangpeng Wei; Heng Yu; Yue Hu; Rongxiang Weng; Weihua Luo; Rong Jin",
        "authorids": "/x/xiangpeng-wei/; /h/heng-yu/; /y/yue-hu/; /r/rongxiang-weng/; /w/weihua-luo/; /r/rong-jin/",
        "bibtex": "@inproceedings{wei-etal-2022-learning,\n    title = \"Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation\",\n    author = \"Wei, Xiangpeng  and\n      Yu, Heng  and\n      Hu, Yue  and\n      Weng, Rongxiang  and\n      Luo, Weihua  and\n      Jin, Rong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.546/\",\n    doi = \"10.18653/v1/2022.acl-long.546\",\n    pages = \"7930--7944\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.546.pdf",
        "site": "https://aclanthology.org/2022.acl-long.546/",
        "pdf_size": 837272,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1731100944868826419&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Machine Intelligence Technology Lab, Alibaba DAMO Academy, Hangzhou, China; Machine Intelligence Technology Lab, Alibaba DAMO Academy, Hangzhou, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Machine Intelligence Technology Lab, Alibaba DAMO Academy, Hangzhou, China; Machine Intelligence Technology Lab, Alibaba DAMO Academy, Hangzhou, China; Machine Intelligence Technology Lab, Alibaba DAMO Academy, Hangzhou, China",
        "aff_domain": "gmail.com; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ",
        "github": "https://github.com/pemywei/csanmt",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1+2;0;0;0",
        "aff_unique_norm": "Alibaba DAMO Academy;Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Machine Intelligence Technology Lab;Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "https://damo.alibaba.com;http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "Alibaba DAMO;CAS;UCAS",
        "aff_campus_unique_index": "0;0;1+1;0;0;0",
        "aff_campus_unique": "Hangzhou;Beijing",
        "aff_country_unique_index": "0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.525",
        "title": "Learning to Generate Programs for Table Fact Verification via Structure-Aware Semantic Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Table fact verification aims to check the correctness of textual statements based on given semi-structured data. Most existing methods are devoted to better comprehending logical operations and tables, but they hardly study generating latent programs from statements, with which we can not only retrieve evidences efficiently but also explain reasons behind verifications naturally. However, it is challenging to get correct programs with existing weakly supervised semantic parsers due to the huge search space with lots of spurious programs. In this paper, we address the challenge by leveraging both lexical features and structure features for program generation. Through analyzing the connection between the program tree and the dependency tree, we define a unified concept, operation-oriented tree, to mine structure features, and introduce Structure-Aware Semantic Parsing to integrate structure features into program generation. Moreover, we design a refined objective function with lexical features and violation punishments to further avoid spurious programs. Experimental results show that our proposed method generates programs more accurately than existing semantic parsers, and achieves comparable performance to the SOTA on the large-scale benchmark TABFACT.",
        "author": "Suixin Ou; Yongmei Liu",
        "authorids": "/s/suixin-ou/; /y/yongmei-liu/",
        "bibtex": "@inproceedings{ou-liu-2022-learning,\n    title = \"Learning to Generate Programs for Table Fact Verification via Structure-Aware Semantic Parsing\",\n    author = \"Ou, Suixin  and\n      Liu, Yongmei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.525/\",\n    doi = \"10.18653/v1/2022.acl-long.525\",\n    pages = \"7624--7638\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.525.pdf",
        "site": "https://aclanthology.org/2022.acl-long.525/",
        "pdf_size": 704701,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8333599403780582328&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Dept. of Computer Science, Sun Yat-sen University, Guangzhou 510006, China; Dept. of Computer Science, Sun Yat-sen University, Guangzhou 510006, China",
        "aff_domain": "mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "http://www.sysu.edu.cn",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Guangzhou",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.5",
        "title": "Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural discrete reasoning (NDR) has shown remarkable progress in combining deep models with discrete reasoning. However, we find that existing NDR solution suffers from large performance drop on hypothetical questions, e.g. \u201cwhat the annualized rate of return would be if the revenue in 2020 was doubled\u201d. The key to hypothetical question answering (HQA) is counterfactual thinking, which is a natural ability of human reasoning but difficult for deep models. In this work, we devise a Learning to Imagine (L2I) module, which can be seamlessly incorporated into NDR models to perform the imagination of unseen counterfactual. In particular, we formulate counterfactual thinking into two steps: 1) identifying the fact to intervene, and 2) deriving the counterfactual from the fact and assumption, which are designed as neural networks. Based on TAT-QA, we construct a very challenging HQA dataset with 8,283 hypothetical questions. We apply the proposed L2I to TAGOP, the state-of-the-art solution on TAT-QA, validating the rationality and effectiveness of our approach.",
        "author": "Moxin Li; Fuli Feng; Hanwang Zhang; Xiangnan He; Fengbin Zhu; Tat-Seng Chua",
        "authorids": "/m/moxin-li/; /f/fuli-feng/; /h/hanwang-zhang/; /x/xiangnan-he/; /f/fengbin-zhu/; /t/tat-seng-chua/",
        "bibtex": "@inproceedings{li-etal-2022-learning,\n    title = \"Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning\",\n    author = \"Li, Moxin  and\n      Feng, Fuli  and\n      Zhang, Hanwang  and\n      He, Xiangnan  and\n      Zhu, Fengbin  and\n      Chua, Tat-Seng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.5/\",\n    doi = \"10.18653/v1/2022.acl-long.5\",\n    pages = \"57--69\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.5.pdf",
        "site": "https://aclanthology.org/2022.acl-long.5/",
        "pdf_size": 1237596,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1444174607267185842&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "National University of Singapore; University of Science and Technology of China; Nanyang Technological University; University of Science and Technology of China; National University of Singapore; National University of Singapore",
        "aff_domain": "u.nus.edu;gmail.com;ntu.edu.sg;gmail.com;gmail.com;nus.edu.sg",
        "email": "u.nus.edu;gmail.com;ntu.edu.sg;gmail.com;gmail.com;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;1;0;0",
        "aff_unique_norm": "National University of Singapore;University of Science and Technology of China;Nanyang Technological University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nus.edu.sg;http://www.ustc.edu.cn;https://www.ntu.edu.sg",
        "aff_unique_abbr": "NUS;USTC;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2022.acl-long.202",
        "title": "Learning to Mediate Disparities Towards Pragmatic Communication",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Human communication is a collaborative process. Speakers, on top of conveying their own intent, adjust the content and language expressions by taking the listeners into account, including their knowledge background, personalities, and physical capabilities. Towards building AI agents with similar abilities in language communication, we propose a novel rational reasoning framework, Pragmatic Rational Speaker (PRS), where the speaker attempts to learn the speaker-listener disparity and adjust the speech accordingly, by adding a light-weighted disparity adjustment layer into working memory on top of speaker\u2019s long-term memory system. By fixing the long-term memory, the PRS only needs to update its working memory to learn and adapt to different types of listeners. To validate our framework, we create a dataset that simulates different types of speaker-listener disparities in the context of referential games. Our empirical results demonstrate that the PRS is able to shift its output towards the language that listeners are able to understand, significantly improve the collaborative task outcome, and learn the disparity more efficiently than joint training.",
        "author": "Yuwei Bao; Sayan Ghosh; Joyce Chai",
        "authorids": "/y/yuwei-bao/; /s/sayan-ghosh/; /j/joyce-chai/",
        "bibtex": "@inproceedings{bao-etal-2022-learning,\n    title = \"Learning to Mediate Disparities Towards Pragmatic Communication\",\n    author = \"Bao, Yuwei  and\n      Ghosh, Sayan  and\n      Chai, Joyce\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.202/\",\n    doi = \"10.18653/v1/2022.acl-long.202\",\n    pages = \"2829--2842\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.202.pdf",
        "site": "https://aclanthology.org/2022.acl-long.202/",
        "pdf_size": 2068471,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8237728148551102858&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science and Engineering, University of Michigan; ASAPP; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Michigan;ASAPP",
        "aff_unique_dep": "Computer Science and Engineering;",
        "aff_unique_url": "https://www.umich.edu;https://www.asapp.com",
        "aff_unique_abbr": "UM;ASAPP",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.441",
        "title": "Learning to Rank Visual Stories From Human Ranking Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Visual storytelling (VIST) is a typical vision and language task that has seen extensive development in the natural language generation research domain. However, it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST. In this paper, we present the VHED (VIST Human Evaluation Data) dataset, which first re-purposes human evaluation results for automatic evaluation; hence we develop Vrank (VIST Ranker), a novel reference-free VIST metric for story evaluation. We first show that the results from commonly adopted automatic metrics for text generation have little correlation with those obtained from human evaluation, which motivates us to directly utilize human evaluation results to learn the automatic evaluation model. In the experiments, we evaluate the generated texts to predict story ranks using our model as well as other reference-based and reference-free metrics. Results show that Vrank prediction is significantly more aligned to human evaluation than other metrics with almost 30% higher accuracy when ranking story pairs. Moreover, we demonstrate that only Vrank shows human-like behavior in its strong ability to find better stories when the quality gap between two stories is high. Finally, we show the superiority of Vrank by its generalizability to pure textual stories, and conclude that this reuse of human evaluation results puts Vrank in a strong position for continued future advances.",
        "author": "Chi-Yang Hsu; Yun-Wei Chu; Vincent Chen; Kuan-Chieh Lo; Chacha Chen; Ting-Hao Huang; Lun-Wei Ku",
        "authorids": "/c/chi-yang-hsu/; /y/yun-wei-chu/; /v/vincent-chen/; /k/kuan-chieh-lo/; /c/chacha-chen/; /t/ting-hao-huang/; /l/lun-wei-ku/",
        "bibtex": "@inproceedings{hsu-etal-2022-learning,\n    title = \"Learning to Rank Visual Stories From Human Ranking Data\",\n    author = \"Hsu, Chi-Yang  and\n      Chu, Yun-Wei  and\n      Chen, Vincent  and\n      Lo, Kuan-Chieh  and\n      Chen, Chacha  and\n      Huang, Ting-Hao  and\n      Ku, Lun-Wei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.441/\",\n    doi = \"10.18653/v1/2022.acl-long.441\",\n    pages = \"6365--6378\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.441.pdf",
        "site": "https://aclanthology.org/2022.acl-long.441/",
        "pdf_size": 1061354,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15713635280401791541&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Pennsylvania State University1; Purdue University2; Institute of Information Science, Academia Sinica3; University of Chicago4; Institute of Information Science, Academia Sinica3; Pennsylvania State University1; Institute of Information Science, Academia Sinica3",
        "aff_domain": "psu.edu;psu.edu;purdue.edu;iis.sinica.edu.tw;iis.sinica.edu.tw;iis.sinica.edu.tw;uchicago.edu",
        "email": "psu.edu;psu.edu;purdue.edu;iis.sinica.edu.tw;iis.sinica.edu.tw;iis.sinica.edu.tw;uchicago.edu",
        "github": "https://github.com/AcademiaSinicaNLPLab/VHED.git",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;2;0;2",
        "aff_unique_norm": "Pennsylvania State University;Purdue University;Academia Sinica;University of Chicago",
        "aff_unique_dep": ";;Institute of Information Science;",
        "aff_unique_url": "https://www.psu.edu;https://www.purdue.edu;https://www.sinica.edu.tw;https://www.uchicago.edu",
        "aff_unique_abbr": "PSU;Purdue;AS;UChicago",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Taiwan",
        "aff_country_unique_index": "0;0;1;0;1;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.410",
        "title": "Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches typically do not provide explanations for the generated expressions. In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation. Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines. We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.",
        "author": "Zhanming Jie; Jierui Li; Wei Lu",
        "authorids": "/z/zhanming-jie/; /j/jierui-li/; /w/wei-lu/",
        "bibtex": "@inproceedings{jie-etal-2022-learning,\n    title = \"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction\",\n    author = \"Jie, Zhanming  and\n      Li, Jierui  and\n      Lu, Wei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.410/\",\n    doi = \"10.18653/v1/2022.acl-long.410\",\n    pages = \"5944--5955\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.410.pdf",
        "site": "https://aclanthology.org/2022.acl-long.410/",
        "pdf_size": 391565,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9663036460789971252&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "ByteDance AI Lab\u2666; University of Texas at Austin\u2666; StatNLP Research Group, Singapore University of Technology and Design\u2666",
        "aff_domain": "bytedance.com;cs.utexas.edu;sutd.edu.sg",
        "email": "bytedance.com;cs.utexas.edu;sutd.edu.sg",
        "github": "https://github.com/allanj/Deductive-MWP",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "ByteDance;University of Texas at Austin;Singapore University of Technology and Design",
        "aff_unique_dep": "AI Lab;;StatNLP Research Group",
        "aff_unique_url": "https://www.bytedance.com;https://www.utexas.edu;https://www.sutd.edu.sg",
        "aff_unique_abbr": "ByteDance;UT Austin;SUTD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "China;United States;Singapore"
    },
    {
        "id": "2022.findings-acl.94",
        "title": "Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtain. Although a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions (LFs). These LFs, in turn, have been used to generate a large amount of additional noisy labeled data in a paradigm that is now commonly referred to as data programming. Previous methods of generating LFs do not attempt to use the given labeled data further to train a model, thus missing opportunities for improving performance. Additionally, since the LFs are generated automatically, they are likely to be noisy, and naively aggregating these LFs can lead to suboptimal results. In this work, we propose an LF-based bi-level optimization framework WISDOM to solve these two critical limitations. WISDOM learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm. We show that WISDOM significantly outperforms prior approaches on several text classification datasets.",
        "author": "Ayush Maheshwari; Krishnateja Killamsetty; Ganesh Ramakrishnan; Rishabh Iyer; Marina Danilevsky; Lucian Popa",
        "authorids": "/a/ayush-maheshwari/; /k/krishnateja-killamsetty/; /g/ganesh-ramakrishnan/; /r/rishabh-iyer/; /m/marina-danilevsky/; /l/lucian-popa/",
        "bibtex": "@inproceedings{maheshwari-etal-2022-learning,\n    title = \"Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming\",\n    author = \"Maheshwari, Ayush  and\n      Killamsetty, Krishnateja  and\n      Ramakrishnan, Ganesh  and\n      Iyer, Rishabh  and\n      Danilevsky, Marina  and\n      Popa, Lucian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.94/\",\n    doi = \"10.18653/v1/2022.findings-acl.94\",\n    pages = \"1188--1202\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.94.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.94/",
        "pdf_size": 584414,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9439281891602412222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Indian Institute of Technology Bombay, India; The University of Texas at Dallas; Indian Institute of Technology Bombay, India; The University of Texas at Dallas; IBM Research \u2013 Almaden; IBM Research \u2013 Almaden",
        "aff_domain": "cse.iitb.ac.in;utdallas.edu;cse.iitb.ac.in;utdallas.edu;us.ibm.com;us.ibm.com",
        "email": "cse.iitb.ac.in;utdallas.edu;cse.iitb.ac.in;utdallas.edu;us.ibm.com;us.ibm.com",
        "github": "https://github.com/ayushbits/robust-aggregate-lfs",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;1;2;2",
        "aff_unique_norm": "Indian Institute of Technology Bombay;University of Texas at Dallas;IBM",
        "aff_unique_dep": ";;IBM Research",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.utdallas.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "IIT Bombay;UT Dallas;IBM",
        "aff_campus_unique_index": "0;1;0;1;2;2",
        "aff_campus_unique": "Bombay;Dallas;Almaden",
        "aff_country_unique_index": "0;1;0;1;1;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2022.acl-short.23",
        "title": "Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Comprehending a dialogue requires a model to capture diverse kinds of key information in the utterances, which are either scattered around or implicitly implied in different turns of conversations. Therefore, dialogue comprehension requires diverse capabilities such as paraphrasing, summarizing, and commonsense reasoning. Towards the objective of pre-training a zero-shot dialogue comprehension model, we develop a novel narrative-guided pre-training strategy that learns by narrating the key information from a dialogue input. However, the dialogue-narrative parallel corpus for such a pre-training strategy is currently unavailable. For this reason, we first construct a dialogue-narrative parallel corpus by automatically aligning movie subtitles and their synopses. We then pre-train a BART model on the data and evaluate its performance on four dialogue-based tasks that require comprehension. Experimental results show that our model not only achieves superior zero-shot performance but also exhibits stronger fine-grained dialogue comprehension capabilities. The data and code are available at https://github.com/zhaochaocs/Diana.",
        "author": "Chao Zhao; Wenlin Yao; Dian Yu; Kaiqiang Song; Dong Yu; Jianshu Chen",
        "authorids": "/c/chao-zhao/; /w/wenlin-yao/; /d/dian-yu/; /k/kaiqiang-song/; /d/dong-yu/; /j/jianshu-chen/",
        "bibtex": "@inproceedings{zhao-etal-2022-learning,\n    title = \"Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension\",\n    author = \"Zhao, Chao  and\n      Yao, Wenlin  and\n      Yu, Dian  and\n      Song, Kaiqiang  and\n      Yu, Dong  and\n      Chen, Jianshu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.23/\",\n    doi = \"10.18653/v1/2022.acl-short.23\",\n    pages = \"212--218\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.23.pdf",
        "site": "https://aclanthology.org/2022.acl-short.23/",
        "pdf_size": 2398958,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9327133776286028761&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "UNC Chapel Hill, Chapel Hill, NC+Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA",
        "aff_domain": "cs.unc.edu;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "email": "cs.unc.edu;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com",
        "github": "https://github.com/zhaochaocs/Diana",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Tencent",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www.unc.edu;https://ai.tencent.com",
        "aff_unique_abbr": "UNC Chapel Hill;Tencent AI Lab",
        "aff_campus_unique_index": "0+1;1;1;1;1;1",
        "aff_campus_unique": "Chapel Hill;Bellevue",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.48",
        "title": "Legal Judgment Prediction via Event Extraction with Constraints",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While significant progress has been made on the task of Legal Judgment Prediction (LJP) in recent years, the incorrect predictions made by SOTA LJP models can be attributed in part to their failure to (1) locate the key event information that determines the judgment, and (2) exploit the cross-task consistency constraints that exist among the subtasks of LJP. To address these weaknesses, we propose EPM, an Event-based Prediction Model with constraints, which surpasses existing SOTA models in performance on a standard LJP dataset.",
        "author": "Yi Feng; Chuanyi Li; Vincent Ng",
        "authorids": "/y/yi-feng/; /c/chuanyi-li/; /v/vincent-ng/",
        "bibtex": "@inproceedings{feng-etal-2022-legal,\n    title = \"Legal Judgment Prediction via Event Extraction with Constraints\",\n    author = \"Feng, Yi  and\n      Li, Chuanyi  and\n      Ng, Vincent\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.48/\",\n    doi = \"10.18653/v1/2022.acl-long.48\",\n    pages = \"648--664\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.48.pdf",
        "site": "https://aclanthology.org/2022.acl-long.48/",
        "pdf_size": 623311,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11158164314493061339&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Human Language Technology Research Institute, University of Texas at Dallas, USA",
        "aff_domain": "smail.nju.edu.cn;nju.edu.cn;hlt.utdallas.edu",
        "email": "smail.nju.edu.cn;nju.edu.cn;hlt.utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Nanjing University;University of Texas at Dallas",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology;Human Language Technology Research Institute",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.utdallas.edu",
        "aff_unique_abbr": "Nanjing U;UT Dallas",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dallas",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.474",
        "title": "Length Control in Abstractive Summarization by Pretraining Information Selection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous length-controllable summarization models mostly control lengths at the decoding stage, whereas the encoding or the selection of information from the source document is not sensitive to the designed length. They also tend to generate summaries as long as those in the training data. In this paper, we propose a length-aware attention mechanism (LAAM) to adapt the encoding of the source based on the desired length. Our approach works by training LAAM on a summary length balanced dataset built from the original training data, and then fine-tuning as usual. Results show that this approach is effective in generating high-quality summaries with desired lengths and even those short lengths never seen in the original training set.",
        "author": "Yizhu Liu; Qi Jia; Kenny Zhu",
        "authorids": "/y/yizhu-liu/; /q/qi-jia/; /k/kenny-zhu/",
        "bibtex": "@inproceedings{liu-etal-2022-length,\n    title = \"Length Control in Abstractive Summarization by Pretraining Information Selection\",\n    author = \"Liu, Yizhu  and\n      Jia, Qi  and\n      Zhu, Kenny\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.474/\",\n    doi = \"10.18653/v1/2022.acl-long.474\",\n    pages = \"6885--6895\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.474.pdf",
        "site": "https://aclanthology.org/2022.acl-long.474/",
        "pdf_size": 1206021,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14312781816568129733&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.463",
        "title": "Letters From the Past: Modeling Historical Sound Change Through Diachronic Character Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While a great deal of work has been done on NLP approaches to lexical semantic change detection, other aspects of language change have received less attention from the NLP community. In this paper, we address the detection of sound change through historical spelling. We propose that a sound change can be captured by comparing the relative distance through time between the distributions of the characters involved before and after the change has taken place. We model these distributions using PPMI character embeddings. We verify this hypothesis in synthetic data and then test the method\u2019s ability to trace the well-known historical change of lenition of plosives in Danish historical sources. We show that the models are able to identify several of the changes under consideration and to uncover meaningful contexts in which they appeared. The methodology has the potential to contribute to the study of open questions such as the relative chronology of sound shifts and their geographical distribution.",
        "author": "Sidsel Boldsen; Patrizia Paggio",
        "authorids": "/s/sidsel-boldsen/; /p/patrizia-paggio/",
        "bibtex": "@inproceedings{boldsen-paggio-2022-letters,\n    title = \"Letters From the Past: Modeling Historical Sound Change Through Diachronic Character Embeddings\",\n    author = \"Boldsen, Sidsel  and\n      Paggio, Patrizia\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.463/\",\n    doi = \"10.18653/v1/2022.acl-long.463\",\n    pages = \"6713--6722\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.463.pdf",
        "site": "https://aclanthology.org/2022.acl-long.463/",
        "pdf_size": 229039,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2640888776934230282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Nordic Studies and Linguistics, University of Copenhagen + Institute of Linguistics and Language Technology, University of Malta; Department of Nordic Studies and Linguistics, University of Copenhagen + Institute of Linguistics and Language Technology, University of Malta",
        "aff_domain": "hum.ku.dk;hum.ku.dk",
        "email": "hum.ku.dk;hum.ku.dk",
        "github": "",
        "project": "https://guides.lib.uchicago.edu/c.php?g=813534&p=5805534",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Copenhagen;University of Malta",
        "aff_unique_dep": "Department of Nordic Studies and Linguistics;Institute of Linguistics and Language Technology",
        "aff_unique_url": "https://www.ku.dk;https://www.um.edu.mt",
        "aff_unique_abbr": "UCPH;UoM",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "Denmark;Malta"
    },
    {
        "id": "2022.findings-acl.154",
        "title": "Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Named Entity Recognition (NER) systems often demonstrate great performance on in-distribution data, but perform poorly on examples drawn from a shifted distribution. One way to evaluate the generalization ability of NER models is to use adversarial examples, on which the specific variations associated with named entities are rarely considered. To this end, we propose leveraging expert-guided heuristics to change the entity tokens and their surrounding contexts thereby altering their entity types as adversarial attacks. Using expert-guided heuristics, we augmented the CoNLL 2003 test set and manually annotated it to construct a high-quality challenging set. We found that state-of-the-art NER systems trained on CoNLL 2003 training data drop performance dramatically on our challenging set. By training on adversarial augmented training examples and using mixup for regularization, we were able to significantly improve the performance on the challenging set as well as improve out-of-domain generalization which we evaluated by using OntoNotes data. We have publicly released our dataset and code at https://github.com/GT-SALT/Guided-Adversarial-Augmentation.",
        "author": "Aaron Reich; Jiaao Chen; Aastha Agrawal; Yanzhe Zhang; Diyi Yang",
        "authorids": "/a/aaron-reich/; /j/jiaao-chen/; /a/aastha-agrawal/; /y/yanzhe-zhang/; /d/diyi-yang/",
        "bibtex": "@inproceedings{reich-etal-2022-leveraging,\n    title = \"Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition\",\n    author = \"Reich, Aaron  and\n      Chen, Jiaao  and\n      Agrawal, Aastha  and\n      Zhang, Yanzhe  and\n      Yang, Diyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.154/\",\n    doi = \"10.18653/v1/2022.findings-acl.154\",\n    pages = \"1947--1955\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.154.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.154/",
        "pdf_size": 277884,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15023461625380583584&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Georgia Institute of Technology+Pionetechs, Inc.; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "github": "https://github.com/GT-SALT/Guided-Adversarial-Augmentation",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology;Pionetechs, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;",
        "aff_unique_abbr": "Georgia Tech;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.31",
        "title": "Leveraging Explicit Lexico-logical Alignments in Text-to-SQL Parsing",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Text-to-SQL aims to parse natural language questions into SQL queries, which is valuable in providing an easy interface to access large databases. Previous work has observed that leveraging lexico-logical alignments is very helpful to improve parsing performance. However, current attention-based approaches can only model such alignments at the token level and have unsatisfactory generalization capability. In this paper, we propose a new approach to leveraging explicit lexico-logical alignments. It first identifies possible phrase-level alignments and injects them as additional contexts to guide the parsing procedure. Experimental results on Squall show that our approach can make better use of such alignments and obtains an absolute improvement of 3.4% compared with the current state-of-the-art.",
        "author": "Runxin Sun; Shizhu He; Chong Zhu; Yaohan He; Jinlong Li; Jun Zhao; Kang Liu",
        "authorids": "/r/runxin-sun/; /s/shizhu-he/; /c/chong-zhu/; /y/yaohan-he/; /j/jinlong-li/; /j/jun-zhao/; /k/kang-liu/",
        "bibtex": "@inproceedings{sun-etal-2022-leveraging,\n    title = \"Leveraging Explicit Lexico-logical Alignments in Text-to-{SQL} Parsing\",\n    author = \"Sun, Runxin  and\n      He, Shizhu  and\n      Zhu, Chong  and\n      He, Yaohan  and\n      Li, Jinlong  and\n      Zhao, Jun  and\n      Liu, Kang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.31/\",\n    doi = \"10.18653/v1/2022.acl-short.31\",\n    pages = \"283--289\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.31.pdf",
        "site": "https://aclanthology.org/2022.acl-short.31/",
        "pdf_size": 541062,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3421743236048509575&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; AI Lab, China Merchant Bank, ShenZhen, China; AI Lab, China Merchant Bank, ShenZhen, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+Beijing Academy of Artificial Intelligence, Beijing, China",
        "aff_domain": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;cmbchina.com;cmbchina.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;cmbchina.com;cmbchina.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1;2;2;0+1;0+1+3",
        "aff_unique_norm": "National Laboratory of Pattern Recognition;University of Chinese Academy of Sciences;China Merchant Bank;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;AI Lab;",
        "aff_unique_url": ";http://www.ucas.ac.cn;https://www.cmbchina.com.cn;https://www.baaic.cn",
        "aff_unique_abbr": ";UCAS;CMB;BAAI",
        "aff_campus_unique_index": "0+0;0+0;0+0;1;1;0+0;0+0+0",
        "aff_campus_unique": "Beijing;ShenZhen",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.255",
        "title": "Leveraging Knowledge in Multilingual Commonsense Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Commonsense reasoning (CSR) requires models to be equipped with general world knowledge. While CSR is a language-agnostic process, most comprehensive knowledge sources are restricted to a small number of languages, especially English. Thus, it remains unclear how to effectively conduct multilingual commonsense reasoning (XCSR) for various languages. In this work, we propose to use English as a pivot language, utilizing English knowledge sources for our our commonsense reasoning framework via a translate-retrieve-translate (TRT) strategy. For multilingual commonsense questions and answer candidates, we collect related knowledge via translation and retrieval from the knowledge in the source language. The retrieved knowledge is then translated into the target language and integrated into a pre-trained multilingual language model via visible knowledge attention. Then we utilize a diverse of four English knowledge sources to provide more comprehensive coverage of knowledge in different formats. Extensive results on the XCSR benchmark demonstrate that TRT with external knowledge can significantly improve multilingual commonsense reasoning in both zero-shot and translate-train settings, consistently outperforming the state-of-the-art by more than 3% on the multilingual commonsense reasoning benchmark X-CSQA and X-CODAH.",
        "author": "Yuwei Fang; Shuohang Wang; Yichong Xu; Ruochen Xu; Siqi Sun; Chenguang Zhu; Michael Zeng",
        "authorids": "/y/yuwei-fang/; /s/shuohang-wang/; /y/yichong-xu/; /r/ruochen-xu/; /s/siqi-sun/; /c/chenguang-zhu/; /m/michael-zeng/",
        "bibtex": "@inproceedings{fang-etal-2022-leveraging,\n    title = \"Leveraging Knowledge in Multilingual Commonsense Reasoning\",\n    author = \"Fang, Yuwei  and\n      Wang, Shuohang  and\n      Xu, Yichong  and\n      Xu, Ruochen  and\n      Sun, Siqi  and\n      Zhu, Chenguang  and\n      Zeng, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.255/\",\n    doi = \"10.18653/v1/2022.findings-acl.255\",\n    pages = \"3237--3246\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.255.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.255/",
        "pdf_size": 495075,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1454408286618179559&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Cognitive Services Research Group",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.208",
        "title": "Leveraging Relaxed Equilibrium by Lazy Transition for Sequence Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In sequence modeling, certain tokens are usually less ambiguous than others, and representations of these tokens require fewer refinements for disambiguation. However, given the nature of attention-based models like Transformer and UT (universal transformer), all tokens are equally processed towards depth. Inspired by the equilibrium phenomenon, we present a lazy transition, a mechanism to adjust the significance of iterative refinements for each token representation. Our lazy transition is deployed on top of UT to build LT (lazy transformer), where all tokens are processed unequally towards depth. Eventually, LT is encouraged to oscillate around a relaxed equilibrium. Our experiments show that LT outperforms baseline models on several tasks of machine translation, pre-training, Learning to Execute, and LAMBADA.",
        "author": "Xi Ai; Bin Fang",
        "authorids": "/x/xi-ai/; /b/bin-fang/",
        "bibtex": "@inproceedings{ai-fang-2022-leveraging,\n    title = \"Leveraging Relaxed Equilibrium by Lazy Transition for Sequence Modeling\",\n    author = \"Ai, Xi  and\n      Fang, Bin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.208/\",\n    doi = \"10.18653/v1/2022.acl-long.208\",\n    pages = \"2904--2924\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.208.pdf",
        "site": "https://aclanthology.org/2022.acl-long.208/",
        "pdf_size": 2283135,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14849859251500595936&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "College of Computer Science, Chongqing University; College of Computer Science, Chongqing University",
        "aff_domain": "gmail.com;cqu.edu.cn",
        "email": "gmail.com;cqu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Chongqing University",
        "aff_unique_dep": "College of Computer Science",
        "aff_unique_url": "https://www.cqu.edu.cn",
        "aff_unique_abbr": "CQU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.122",
        "title": "Leveraging Similar Users for Personalized Language Modeling with Limited Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Personalized language models are designed and trained to capture language patterns specific to individual users. This makes them more accurate at predicting what a user will write. However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models. We propose a solution for this problem, using a model trained on users that are similar to a new user. In this paper, we explore strategies for finding the similarity between new users and existing ones and methods for using the data from existing users who are a good match. We further explore the trade-off between available data for new users and how well their language can be modeled.",
        "author": "Charles Welch; Chenxi Gu; Jonathan K. Kummerfeld; Veronica Perez-Rosas; Rada Mihalcea",
        "authorids": "/c/charles-welch/; /c/chenxi-gu/; /j/jonathan-k-kummerfeld/; /v/veronica-perez-rosas/; /r/rada-mihalcea/",
        "bibtex": "@inproceedings{welch-etal-2022-leveraging,\n    title = \"Leveraging Similar Users for Personalized Language Modeling with Limited Data\",\n    author = \"Welch, Charles  and\n      Gu, Chenxi  and\n      Kummerfeld, Jonathan K.  and\n      Perez-Rosas, Veronica  and\n      Mihalcea, Rada\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.122/\",\n    doi = \"10.18653/v1/2022.acl-long.122\",\n    pages = \"1742--1752\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.122.pdf",
        "site": "https://aclanthology.org/2022.acl-long.122/",
        "pdf_size": 426031,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17924782015433936622&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Conversational AI and Social Analytics (CAISA) Lab+Department of Mathematics and Computer Science, University of Marburg; Conversational AI and Social Analytics (CAISA) Lab+Department of Mathematics and Computer Science, University of Marburg; Language and Information Technologies Lab (LIT)+Department of Computer Science and Engineering, University of Michigan; Language and Information Technologies Lab (LIT)+Department of Computer Science and Engineering, University of Michigan; Language and Information Technologies Lab (LIT)+Department of Computer Science and Engineering, University of Michigan",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;2+3;2+3;2+3",
        "aff_unique_norm": "Conversational AI and Social Analytics (CAISA) Lab;University of Marburg;Language and Information Technologies Lab;University of Michigan",
        "aff_unique_dep": "Lab;Department of Mathematics and Computer Science;LIT;Department of Computer Science and Engineering",
        "aff_unique_url": ";https://www.uni-marburg.de;;https://www.umich.edu",
        "aff_unique_abbr": "CAISA Lab;;LIT;UM",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;2;2;2",
        "aff_country_unique": ";Germany;United States"
    },
    {
        "id": "2022.acl-long.461",
        "title": "Leveraging Task Transferability to Meta-learning for Clinical Section Classification with Limited Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Identifying sections is one of the critical components of understanding medical information from unstructured clinical notes and developing assistive technologies for clinical note-writing tasks. Most state-of-the-art text classification systems require thousands of in-domain text data to achieve high performance. However, collecting in-domain and recent clinical note data with section labels is challenging given the high level of privacy and sensitivity. The present paper proposes an algorithmic way to improve the task transferability of meta-learning-based text classification in order to address the issue of low-resource target data. Specifically, we explore how to make the best use of the source dataset and propose a unique task transferability measure named Normalized Negative Conditional Entropy (NNCE). Leveraging the NNCE, we develop strategies for selecting clinical categories and sections from source task data to boost cross-domain meta-learning accuracy. Experimental results show that our task selection strategies improve section classification accuracy significantly compared to meta-learning algorithms.",
        "author": "Zhuohao Chen; Jangwon Kim; Ram Bhakta; Mustafa Sir",
        "authorids": "/z/zhuohao-chen/; /j/jangwon-kim/; /r/ram-bhakta/; /m/mustafa-sir/",
        "bibtex": "@inproceedings{chen-etal-2022-leveraging,\n    title = \"Leveraging Task Transferability to Meta-learning for Clinical Section Classification with Limited Data\",\n    author = \"Chen, Zhuohao  and\n      Kim, Jangwon  and\n      Bhakta, Ram  and\n      Sir, Mustafa\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.461/\",\n    doi = \"10.18653/v1/2022.acl-long.461\",\n    pages = \"6690--6702\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.461.pdf",
        "site": "https://aclanthology.org/2022.acl-long.461/",
        "pdf_size": 557455,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4007047356680007529&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "University of Southern California, Los Angeles, USA + Amazon, Seattle, USA; Amazon, Seattle, USA; Amazon, Seattle, USA + Oath Care, Austin, USA; Amazon, Seattle, USA",
        "aff_domain": "usc.edu;amazon.com;gmail.com;amazon.com",
        "email": "usc.edu;amazon.com;gmail.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1+2;1",
        "aff_unique_norm": "University of Southern California;Amazon;Oath Care",
        "aff_unique_dep": ";Amazon.com, Inc.;",
        "aff_unique_url": "https://www.usc.edu;https://www.amazon.com;",
        "aff_unique_abbr": "USC;Amazon;",
        "aff_campus_unique_index": "0+1;1;1+2;1",
        "aff_campus_unique": "Los Angeles;Seattle;Austin",
        "aff_country_unique_index": "0+0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.308",
        "title": "Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Training Transformer-based models demands a large amount of data, while obtaining aligned and labelled data in multimodality is rather cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it makes a lot of sense to make use of unlabelled unimodal data. On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pre-trained models into a multimodal scenario remains underexplored. In this work, we successfully leverage unimodal self-supervised learning to promote the multimodal AVSR. In particular, audio and visual front-ends are trained on large-scale unimodal datasets, then we integrate components of both front-ends into a larger multimodal framework which learns to recognize parallel audio-visual data into characters through a combination of CTC and seq2seq decoding. We show that both components inherited from unimodal self-supervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning. Our model is experimentally validated on both word-level and sentence-level tasks. Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30%.",
        "author": "Xichen Pan; Peiyu Chen; Yichen Gong; Helong Zhou; Xinbing Wang; Zhouhan Lin",
        "authorids": "/x/xichen-pan/; /p/peiyu-chen/; /y/yichen-gong/; /h/helong-zhou/; /x/xinbing-wang/; /z/zhouhan-lin/",
        "bibtex": "@inproceedings{pan-etal-2022-leveraging,\n    title = \"Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition\",\n    author = \"Pan, Xichen  and\n      Chen, Peiyu  and\n      Gong, Yichen  and\n      Zhou, Helong  and\n      Wang, Xinbing  and\n      Lin, Zhouhan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.308/\",\n    doi = \"10.18653/v1/2022.acl-long.308\",\n    pages = \"4491--4503\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.308.pdf",
        "site": "https://aclanthology.org/2022.acl-long.308/",
        "pdf_size": 4353554,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17510645308625436243&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Horizon Robotics; Horizon Robotics; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;horizon.ai;horizon.ai;sjtu.edu.cn;gmail.com",
        "email": "sjtu.edu.cn;sjtu.edu.cn;horizon.ai;horizon.ai;sjtu.edu.cn;gmail.com",
        "github": "https://github.com/LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;Horizon Robotics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.horizon-robotics.com/",
        "aff_unique_abbr": "SJTU;Horizon Robotics",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.196",
        "title": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-Modal Knowledge Transfer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias. In this work, we study whether integrating visual knowledge into a language model can fill the gap. We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives.On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives.Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings.",
        "author": "Woojeong Jin; Dong-Ho Lee; Chenguang Zhu; Jay Pujara; Xiang Ren",
        "authorids": "/w/woojeong-jin/; /d/dong-ho-lee/; /c/chenguang-zhu/; /j/jay-pujara/; /x/xiang-ren/",
        "bibtex": "@inproceedings{jin-etal-2022-leveraging,\n    title = \"Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-Modal Knowledge Transfer\",\n    author = \"Jin, Woojeong  and\n      Lee, Dong-Ho  and\n      Zhu, Chenguang  and\n      Pujara, Jay  and\n      Ren, Xiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.196/\",\n    doi = \"10.18653/v1/2022.acl-long.196\",\n    pages = \"2750--2762\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.196.pdf",
        "site": "https://aclanthology.org/2022.acl-long.196/",
        "pdf_size": 1005793,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2731465490903071953&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Microsoft Cognitive Services Research Group; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California",
        "aff_domain": "usc.edu;usc.edu;microsoft.com;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;microsoft.com;usc.edu;usc.edu",
        "github": "https://github.com/INK-USC/CMKT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Southern California;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Cognitive Services Research Group",
        "aff_unique_url": "https://www.usc.edu;https://www.microsoft.com",
        "aff_unique_abbr": "USC;Microsoft",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.384",
        "title": "Leveraging Wikipedia article evolution for promotional tone detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Detecting biased language is useful for a variety of applications, such as identifying hyperpartisan news sources or flagging one-sided rhetoric. In this work we introduce WikiEvolve, a dataset for document-level promotional tone detection. Unlike previously proposed datasets, WikiEvolve contains seven versions of the same article from Wikipedia, from different points in its revision history; one with promotional tone, and six without it. This allows for obtaining more precise training signal for learning models from promotional tone detection. We adapt the previously proposed gradient reversal layer framework to encode two article versions simultaneously and thus leverage this additional training signal. In our experiments, our proposed adaptation of gradient reversal improves the accuracy of four different architectures on both in-domain and out-of-domain evaluation.",
        "author": "Christine De Kock; Andreas Vlachos",
        "authorids": "/c/christine-de-kock/; /a/andreas-vlachos/",
        "bibtex": "@inproceedings{de-kock-vlachos-2022-leveraging,\n    title = \"Leveraging {W}ikipedia article evolution for promotional tone detection\",\n    author = \"De Kock, Christine  and\n      Vlachos, Andreas\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.384/\",\n    doi = \"10.18653/v1/2022.acl-long.384\",\n    pages = \"5601--5613\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.384.pdf",
        "site": "https://aclanthology.org/2022.acl-long.384/",
        "pdf_size": 462039,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9573364499477458124&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Technology, University of Cambridge; Department of Computer Science and Technology, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "github.com/christinedekock11/wiki-evolve",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Computer Science and Technology",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.297",
        "title": "LexGLUE: A Benchmark Dataset for Legal Language Understanding in English",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.",
        "author": "Ilias Chalkidis; Abhik Jana; Dirk Hartung; Michael Bommarito; Ion Androutsopoulos; Daniel Katz; Nikolaos Aletras",
        "authorids": "/i/ilias-chalkidis/; /a/abhik-jana/; /d/dirk-hartung/; /m/michael-bommarito/; /i/ion-androutsopoulos/; /d/daniel-katz/; /n/nikolaos-aletras/",
        "bibtex": "@inproceedings{chalkidis-etal-2022-lexglue,\n    title = \"{L}ex{GLUE}: A Benchmark Dataset for Legal Language Understanding in {E}nglish\",\n    author = \"Chalkidis, Ilias  and\n      Jana, Abhik  and\n      Hartung, Dirk  and\n      Bommarito, Michael  and\n      Androutsopoulos, Ion  and\n      Katz, Daniel  and\n      Aletras, Nikolaos\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.297/\",\n    doi = \"10.18653/v1/2022.acl-long.297\",\n    pages = \"4310--4330\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.297.pdf",
        "site": "https://aclanthology.org/2022.acl-long.297/",
        "pdf_size": 1149614,
        "gs_citation": 280,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7197817094773190483&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Copenhagen, Denmark; Universit\u00e4t Hamburg, Germany; Bucerius Law School, Hamburg, Germany+CodeX, Stanford Law School, United States; Bucerius Law School, Hamburg, Germany+CodeX, Stanford Law School, United States; Athens University of Economics and Business, Greece; Bucerius Law School, Hamburg, Germany+CodeX, Stanford Law School, United States+Illinois Tech \u2013 Chicago Kent College of Law, United States; University of Sheffield, UK",
        "aff_domain": "di.ku.dk; ; ; ; ; ; ",
        "email": "di.ku.dk; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2+3;2+3;4;2+3+5;6",
        "aff_unique_norm": "University of Copenhagen;Universit\u00e4t Hamburg;Bucerius Law School;Stanford University;Athens University of Economics and Business;Illinois Institute of Technology;University of Sheffield",
        "aff_unique_dep": ";;Law School;Stanford Law School;;Chicago Kent College of Law;",
        "aff_unique_url": "https://www.ku.dk;https://www.uni-hamburg.de;https://www.bucerius.de;https://www.stanford.edu;https://www.aueb.gr;https://www.iit.edu;https://www.sheffield.ac.uk",
        "aff_unique_abbr": "UCPH;UHH;Bucerius;Stanford;AUEB;IIT;Sheffield",
        "aff_campus_unique_index": "1+2;1+2;1+2+3",
        "aff_campus_unique": ";Hamburg;Stanford;Chicago",
        "aff_country_unique_index": "0;1;1+2;1+2;3;1+2+2;4",
        "aff_country_unique": "Denmark;Germany;United States;Greece;United Kingdom"
    },
    {
        "id": "2022.acl-long.87",
        "title": "LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Lexical substitution is the task of generating meaningful substitutes for a word in a given textual context. Contextual word embedding models have achieved state-of-the-art results in the lexical substitution task by relying on contextual information extracted from the replaced word within the sentence. However, such models do not take into account structured knowledge that exists in external lexical databases. We introduce LexSubCon, an end-to-end lexical substitution framework based on contextual embedding models that can identify highly-accurate substitute candidates. This is achieved by combining contextual information with knowledge from structured lexical resources. Our approach involves: (i) introducing a novel mix-up embedding strategy to the target word\u2019s embedding through linearly interpolating the pair of the target input embedding and the average embedding of its probable synonyms; (ii) considering the similarity of the sentence-definition embeddings of the target word and its proposed candidates; and, (iii) calculating the effect of each substitution on the semantics of the sentence through a fine-tuned sentence similarity model. Our experiments show that LexSubCon outperforms previous state-of-the-art methods by at least 2% over all the official lexical substitution metrics on LS07 and CoInCo benchmark datasets that are widely used for lexical substitution tasks.",
        "author": "George Michalopoulos; Ian McKillop; Alexander Wong; Helen Chen",
        "authorids": "/g/george-michalopoulos/; /i/ian-mckillop/; /a/alexander-wong/; /h/helen-chen/",
        "bibtex": "@inproceedings{michalopoulos-etal-2022-lexsubcon,\n    title = \"{L}ex{S}ub{C}on: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution\",\n    author = \"Michalopoulos, George  and\n      McKillop, Ian  and\n      Wong, Alexander  and\n      Chen, Helen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.87/\",\n    doi = \"10.18653/v1/2022.acl-long.87\",\n    pages = \"1226--1236\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.87.pdf",
        "site": "https://aclanthology.org/2022.acl-long.87/",
        "pdf_size": 766980,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1119036451558208147&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada",
        "aff_domain": "uwaterloo.ca;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca",
        "email": "uwaterloo.ca;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca",
        "github": "https://github.com/gmichalo/LexSubCon1226",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Waterloo",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.547",
        "title": "Lexical Knowledge Internalization for Neural Dialog Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose knowledge internalization (KI), which aims to complement the lexical knowledge into neural dialog models. Instead of further conditioning the knowledge-grounded dialog (KGD) models on externally retrieved knowledge, we seek to integrate knowledge about each input token internally into the model\u2019s parameters. To tackle the challenge due to the large scale of lexical knowledge, we adopt the contrastive learning approach and create an effective token-level lexical knowledge retriever that requires only weak supervision mined from Wikipedia. We demonstrate the effectiveness and general applicability of our approach on various datasets and diversified model structures.",
        "author": "Zhiyong Wu; Wei Bi; Xiang Li; Lingpeng Kong; Ben Kao",
        "authorids": "/z/zhiyong-wu/; /w/wei-bi/; /x/xiang-li/; /l/lingpeng-kong/; /b/ben-kao/",
        "bibtex": "@inproceedings{wu-etal-2022-lexical,\n    title = \"Lexical Knowledge Internalization for Neural Dialog Generation\",\n    author = \"Wu, Zhiyong  and\n      Bi, Wei  and\n      Li, Xiang  and\n      Kong, Lingpeng  and\n      Kao, Ben\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.547/\",\n    doi = \"10.18653/v1/2022.acl-long.547\",\n    pages = \"7945--7958\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.547.pdf",
        "site": "https://aclanthology.org/2022.acl-long.547/",
        "pdf_size": 373387,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5334972746082780360&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Hong Kong + Shanghai AI Lab; Tencent AI Lab; East China Normal University; The University of Hong Kong; The University of Hong Kong",
        "aff_domain": "cs.hku.hk;cs.hku.hk;cs.hku.hk;tencent.com;dase.ecnu.edu.cn",
        "email": "cs.hku.hk;cs.hku.hk;cs.hku.hk;tencent.com;dase.ecnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;3;0;0",
        "aff_unique_norm": "University of Hong Kong;Shanghai AI Lab;Tencent;East China Normal University",
        "aff_unique_dep": ";;Tencent AI Lab;",
        "aff_unique_url": "https://www.hku.hk;https://www.shanghaiailab.com;https://ai.tencent.com;http://www.ecnu.edu.cn",
        "aff_unique_abbr": "HKU;SAIL;Tencent AI Lab;ECNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.534",
        "title": "LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT.",
        "author": "Jiapeng Wang; Lianwen Jin; Kai Ding",
        "authorids": "/j/jiapeng-wang/; /l/lianwen-jin/; /k/kai-ding/",
        "bibtex": "@inproceedings{wang-etal-2022-lilt,\n    title = \"{L}i{LT}: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding\",\n    author = \"Wang, Jiapeng  and\n      Jin, Lianwen  and\n      Ding, Kai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.534/\",\n    doi = \"10.18653/v1/2022.acl-long.534\",\n    pages = \"7747--7757\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.534.pdf",
        "site": "https://aclanthology.org/2022.acl-long.534/",
        "pdf_size": 1879437,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5777529341040128229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "South China University of Technology, Guangzhou, China + INTSIG-SCUT Joint Laboratory of Document Recognition and Understanding, China + Peng Cheng Laboratory, Shenzhen, China; South China University of Technology, Guangzhou, China + INTSIG-SCUT Joint Laboratory of Document Recognition and Understanding, China; IntSig Information Co., Ltd, Shanghai, China + INTSIG-SCUT Joint Laboratory of Document Recognition and Understanding, China",
        "aff_domain": "mail.scut.edu.cn;scut.edu.cn;intsig.net",
        "email": "mail.scut.edu.cn;scut.edu.cn;intsig.net",
        "github": "https://github.com/jpWang/LiLT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;0+1;3+1",
        "aff_unique_norm": "South China University of Technology;INTSIG-SCUT Joint Laboratory of Document Recognition and Understanding;Pengcheng Laboratory;IntSig Information Co., Ltd",
        "aff_unique_dep": ";Document Recognition and Understanding;Peng Cheng Laboratory;",
        "aff_unique_url": "http://www.scut.edu.cn;;;",
        "aff_unique_abbr": "SCUT;;;",
        "aff_campus_unique_index": "0+2;0;",
        "aff_campus_unique": "Guangzhou;;Shenzhen",
        "aff_country_unique_index": "0+0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.227",
        "title": "Life after BERT: What do Other Muppets Understand about Language?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model\u2019s linguistic capabilities.",
        "author": "Vladislav Lialin; Kevin Zhao; Namrata Shivagunde; Anna Rumshisky",
        "authorids": "/v/vladislav-lialin/; /k/kevin-zhao/; /n/namrata-shivagunde/; /a/anna-rumshisky/",
        "bibtex": "@inproceedings{lialin-etal-2022-life,\n    title = \"Life after {BERT}: What do Other Muppets Understand about Language?\",\n    author = \"Lialin, Vladislav  and\n      Zhao, Kevin  and\n      Shivagunde, Namrata  and\n      Rumshisky, Anna\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.227/\",\n    doi = \"10.18653/v1/2022.acl-long.227\",\n    pages = \"3180--3193\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.227.pdf",
        "site": "https://aclanthology.org/2022.acl-long.227/",
        "pdf_size": 351710,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8270567939785894491&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Massachusetts Lowell; Department of Computer Science, University of Massachusetts Lowell; Department of Computer Science, University of Massachusetts Lowell; Department of Computer Science, University of Massachusetts Lowell",
        "aff_domain": "cs.uml.edu;uml.edu;cs.uml.edu;cs.uml.edu",
        "email": "cs.uml.edu;uml.edu;cs.uml.edu;cs.uml.edu",
        "github": "github.com/kev-zhao/life-after-bert",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Massachusetts Lowell",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uml.edu",
        "aff_unique_abbr": "UMass Lowell",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lowell",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.551",
        "title": "LinkBERT: Pretraining Language Models with Document Links",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data.",
        "author": "Michihiro Yasunaga; Jure Leskovec; Percy Liang",
        "authorids": "/m/michihiro-yasunaga/; /j/jure-leskovec/; /p/percy-liang/",
        "bibtex": "@inproceedings{yasunaga-etal-2022-linkbert,\n    title = \"{L}ink{BERT}: Pretraining Language Models with Document Links\",\n    author = \"Yasunaga, Michihiro  and\n      Leskovec, Jure  and\n      Liang, Percy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.551/\",\n    doi = \"10.18653/v1/2022.acl-long.551\",\n    pages = \"8003--8016\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.551.pdf",
        "site": "https://aclanthology.org/2022.acl-long.551/",
        "pdf_size": 1777830,
        "gs_citation": 411,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16495764915143635064&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "https://github.com/michiyasunaga/LinkBERT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.87",
        "title": "Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and annotating the data \u2013 as opposed to giving companies and governments control over defining and combatting hate speech. This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm. Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT\u2019s predictions.",
        "author": "Antonis Maronikolakis; Axel Wisiorek; Leah Nann; Haris Jabbar; Sahana Udupa; Hinrich Schuetze",
        "authorids": "/a/antonis-maronikolakis/; /a/axel-wisiorek/; /l/leah-nann/; /h/haris-jabbar/; /s/sahana-udupa/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{maronikolakis-etal-2022-listening,\n    title = \"Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments\",\n    author = \"Maronikolakis, Antonis  and\n      Wisiorek, Axel  and\n      Nann, Leah  and\n      Jabbar, Haris  and\n      Udupa, Sahana  and\n      Schuetze, Hinrich\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.87/\",\n    doi = \"10.18653/v1/2022.findings-acl.87\",\n    pages = \"1089--1104\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.87.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.87/",
        "pdf_size": 629479,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8310749372596660072&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "CIS, Center for Information and Language Processing+LMU Munich; Center for Digital Humanities+LMU Munich; Institute of Social and Cultural Anthropology+LMU Munich; CIS, Center for Information and Language Processing+LMU Munich; Institute of Social and Cultural Anthropology+LMU Munich; CIS, Center for Information and Language Processing+LMU Munich",
        "aff_domain": "cis.lmu.de; ; ; ; ; ",
        "email": "cis.lmu.de; ; ; ; ; ",
        "github": "https://github.com/antmarakis/xtremespeech",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2+1;3+1;0+1;3+1;0+1",
        "aff_unique_norm": "Center for Information and Language Processing;Ludwig Maximilian University of Munich;Center for Digital Humanities;Institute of Social and Cultural Anthropology",
        "aff_unique_dep": "CIS;;Digital Humanities;Department of Social and Cultural Anthropology",
        "aff_unique_url": ";https://www.lmu.de;;",
        "aff_unique_abbr": ";LMU;;",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0+1;1;1;0+1;1;0+1",
        "aff_country_unique": "United States;Germany;"
    },
    {
        "id": "2022.acl-long.594",
        "title": "Lite Unified Modeling for Discriminative Reading Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As a broad and major category in machine reading comprehension (MRC), the generalized goal of discriminative MRC is answer prediction from the given materials. However, the focuses of various discriminative MRC tasks may be diverse enough: multi-choice MRC requires model to highlight and integrate all potential critical evidence globally; while extractive MRC focuses on higher local boundary preciseness for answer extraction. Among previous works, there lacks a unified design with pertinence for the overall discriminative MRC tasks. To fill in above gap, we propose a lightweight POS-Enhanced Iterative Co-Attention Network (POI-Net) as the first attempt of unified modeling with pertinence, to handle diverse discriminative MRC tasks synchronously. Nearly without introducing more parameters, our lite unified design brings model significant improvement with both encoder and decoder components. The evaluation results on four discriminative MRC benchmarks consistently indicate the general effectiveness and applicability of our model, and the code is available at https://github.com/Yilin1111/poi-net.",
        "author": "Yilin Zhao; Hai Zhao; Libin Shen; Yinggong Zhao",
        "authorids": "/y/yilin-zhao/; /h/hai-zhao/; /l/libin-shen/; /y/yinggong-zhao/",
        "bibtex": "@inproceedings{zhao-etal-2022-lite,\n    title = \"Lite Unified Modeling for Discriminative Reading Comprehension\",\n    author = \"Zhao, Yilin  and\n      Zhao, Hai  and\n      Shen, Libin  and\n      Zhao, Yinggong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.594/\",\n    doi = \"10.18653/v1/2022.acl-long.594\",\n    pages = \"8682--8695\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.594.pdf",
        "site": "https://aclanthology.org/2022.acl-long.594/",
        "pdf_size": 1269548,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17084472404424066345&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Leyan Tech, Shanghai, China; Leyan Tech, Shanghai, China",
        "aff_domain": "sjtu.edu.cn;cs.sjtu.edu.cn;leyantech.com;leyantech.com",
        "email": "sjtu.edu.cn;cs.sjtu.edu.cn;leyantech.com;leyantech.com",
        "github": "https://github.com/Yilin1111/poi-net",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Leyan Tech",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "SJTU;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.539",
        "title": "Local Languages, Third Spaces, and other High-Resource Scenarios",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "How can language technology address the diverse situations of the world\u2019s languages? In one view, languages exist on a resource continuum and the challenge is to scale existing solutions, bringing under-resourced languages into the high-resource world. In another view, presented here, the world\u2019s language ecology includes standardised languages, local languages, and contact languages. These are often subsumed under the label of \u201cunder-resourced languages\u201d even though they have distinct functions and prospects. I explore this position and propose some ecologically-aware language technology agendas.",
        "author": "Steven Bird",
        "authorids": "/s/steven-bird/",
        "bibtex": "@inproceedings{bird-2022-local,\n    title = \"Local Languages, Third Spaces, and other High-Resource Scenarios\",\n    author = \"Bird, Steven\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.539/\",\n    doi = \"10.18653/v1/2022.acl-long.539\",\n    pages = \"7817--7829\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.539.pdf",
        "site": "https://aclanthology.org/2022.acl-long.539/",
        "pdf_size": 1452616,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14138488833741608795&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 5,
        "aff": "Northern Institute, Charles Darwin University, Darwin, Australia",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Charles Darwin University",
        "aff_unique_dep": "Northern Institute",
        "aff_unique_url": "https://www.cdu.edu.au",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Darwin",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2022.findings-acl.293",
        "title": "Local Structure Matters Most: Perturbation Study in NLU",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent research analyzing the sensitivity of natural language understanding models to word-order perturbations has shown that neural models are surprisingly insensitive to the order of words. In this paper, we investigate this phenomenon by developing order-altering perturbations on the order of words, subwords, and characters to analyze their effect on neural models\u2019 performance on language understanding tasks. We experiment with measuring the impact of perturbations to the local neighborhood of characters and global position of characters in the perturbed texts and observe that perturbation functions found in prior literature only affect the global ordering while the local ordering remains relatively unperturbed. We empirically show that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.",
        "author": "Louis Clouatre; Prasanna Parthasarathi; Amal Zouaq; Sarath Chandar",
        "authorids": "/l/louis-clouatre/; /p/prasanna-parthasarathi/; /a/amal-zouaq/; /s/sarath-chandar/",
        "bibtex": "@inproceedings{clouatre-etal-2022-local,\n    title = \"Local Structure Matters Most: Perturbation Study in {NLU}\",\n    author = \"Clouatre, Louis  and\n      Parthasarathi, Prasanna  and\n      Zouaq, Amal  and\n      Chandar, Sarath\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.293/\",\n    doi = \"10.18653/v1/2022.findings-acl.293\",\n    pages = \"3712--3731\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.293.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.293/",
        "pdf_size": 775861,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10517718355476005985&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Polytechnique Montr\u00e9al+Quebec Artificial Intelligence Institute (Mila); School of Computer Science, McGill University+Quebec Artificial Intelligence Institute (Mila); Polytechnique Montr\u00e9al; Polytechnique Montr\u00e9al+Quebec Artificial Intelligence Institute (Mila)+Canada CIFAR AI Chair",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2+1;0;0+1+3",
        "aff_unique_norm": "Polytechnique Montr\u00e9al;Quebec Artificial Intelligence Institute;McGill University;Canadian Institute for Advanced Research",
        "aff_unique_dep": ";Artificial Intelligence;School of Computer Science;AI Chair",
        "aff_unique_url": "https://www.polymtl.ca;https://mila.quebec;https://www.mcgill.ca;https://www.cifar.ca",
        "aff_unique_abbr": "PolyMTL;Mila;McGill;CIFAR",
        "aff_campus_unique_index": "0;2;0;0",
        "aff_campus_unique": "Montr\u00e9al;;Montreal",
        "aff_country_unique_index": "0+0;0+0;0;0+0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.407",
        "title": "Logic Traps in Evaluating Attribution Scores",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modern deep learning models are notoriously opaque, which has motivated the development of methods for interpreting how deep models predict. This goal is usually approached with attribution method, which assesses the influence of features on model predictions. As an explanation method, the evaluation criteria of attribution methods is how accurately it reflects the actual reasoning process of the model (faithfulness). Meanwhile, since the reasoning process of deep models is inaccessible, researchers design various evaluation methods to demonstrate their arguments. However, some crucial logic traps in these evaluation methods are ignored in most works, causing inaccurate evaluation and unfair comparison. This paper systematically reviews existing methods for evaluating attribution scores and summarizes the logic traps in these methods. We further conduct experiments to demonstrate the existence of each logic trap. Through both theoretical and experimental analysis, we hope to increase attention on the inaccurate evaluation of attribution scores. Moreover, with this paper, we suggest stopping focusing on improving performance under unreliable evaluation systems and starting efforts on reducing the impact of proposed logic traps.",
        "author": "Yiming Ju; Yuanzhe Zhang; Zhao Yang; Zhongtao Jiang; Kang Liu; Jun Zhao",
        "authorids": "/y/yiming-ju/; /y/yuanzhe-zhang/; /z/zhao-yang/; /z/zhongtao-jiang/; /k/kang-liu/; /j/jun-zhao/",
        "bibtex": "@inproceedings{ju-etal-2022-logic,\n    title = \"Logic Traps in Evaluating Attribution Scores\",\n    author = \"Ju, Yiming  and\n      Zhang, Yuanzhe  and\n      Yang, Zhao  and\n      Jiang, Zhongtao  and\n      Liu, Kang  and\n      Zhao, Jun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.407/\",\n    doi = \"10.18653/v1/2022.acl-long.407\",\n    pages = \"5911--5922\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.407.pdf",
        "site": "https://aclanthology.org/2022.acl-long.407/",
        "pdf_size": 1098865,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4826796064067745382&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+Beijing Academy of Artificial Intelligence, Beijing, 100084, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1+2;0+1",
        "aff_unique_norm": "National Laboratory of Pattern Recognition;University of Chinese Academy of Sciences;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": ";http://www.ucas.ac.cn;https://www.baaic.cn",
        "aff_unique_abbr": ";UCAS;BAAI",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.127",
        "title": "Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset.",
        "author": "Siyuan Wang; Wanjun Zhong; Duyu Tang; Zhongyu Wei; Zhihao Fan; Daxin Jiang; Ming Zhou; Nan Duan",
        "authorids": "/s/siyuan-wang/; /w/wanjun-zhong/; /d/duyu-tang/; /z/zhongyu-wei/; /z/zhihao-fan/; /d/daxin-jiang/; /m/ming-zhou/; /n/nan-duan/",
        "bibtex": "@inproceedings{wang-etal-2022-logic,\n    title = \"Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text\",\n    author = \"Wang, Siyuan  and\n      Zhong, Wanjun  and\n      Tang, Duyu  and\n      Wei, Zhongyu  and\n      Fan, Zhihao  and\n      Jiang, Daxin  and\n      Zhou, Ming  and\n      Duan, Nan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.127/\",\n    doi = \"10.18653/v1/2022.findings-acl.127\",\n    pages = \"1619--1629\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.127.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.127/",
        "pdf_size": 1267006,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13542049561241013050&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 4,
        "aff": "School of Data Science, Fudan University, China; Sun Yat-Sen University, China; Microsoft, China; School of Data Science, Fudan University, China+Research Institute of Intelligent and Complex Systems, Fudan University, China; School of Data Science, Fudan University, China; Microsoft, China; Sinovation Ventures, China; Microsoft, China",
        "aff_domain": "fudan.edu.cn;mail2.sysu.edu.cn;microsoft.com;fudan.edu.cn;fudan.edu.cn;microsoft.com;chuangxin.com;microsoft.com",
        "email": "fudan.edu.cn;mail2.sysu.edu.cn;microsoft.com;fudan.edu.cn;fudan.edu.cn;microsoft.com;chuangxin.com;microsoft.com",
        "github": "https://github.com/WangsyGit/LReasoner",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;0+0;0;2;3;2",
        "aff_unique_norm": "Fudan University;Sun Yat-sen University;Microsoft;Sinovation Ventures",
        "aff_unique_dep": "School of Data Science;;Microsoft Corporation;",
        "aff_unique_url": "https://www.fudan.edu.cn;http://www.sysu.edu.cn;https://www.microsoft.com/en-us;https://www.sinovationventures.com",
        "aff_unique_abbr": "Fudan;SYSU;Microsoft;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.207",
        "title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM). This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.",
        "author": "Xinchao Xu; Zhibin Gou; Wenquan Wu; Zheng-Yu Niu; Hua Wu; Haifeng Wang; Shihang Wang",
        "authorids": "/x/xinchao-xu/; /z/zhibin-gou/; /w/wenquan-wu/; /z/zheng-yu-niu/; /h/hua-wu/; /h/haifeng-wang/; /s/shihang-wang/",
        "bibtex": "@inproceedings{xu-etal-2022-long,\n    title = \"Long Time No See! Open-Domain Conversation with Long-Term Persona Memory\",\n    author = \"Xu, Xinchao  and\n      Gou, Zhibin  and\n      Wu, Wenquan  and\n      Niu, Zheng-Yu  and\n      Wu, Hua  and\n      Wang, Haifeng  and\n      Wang, Shihang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.207/\",\n    doi = \"10.18653/v1/2022.findings-acl.207\",\n    pages = \"2639--2650\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.207.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.207/",
        "pdf_size": 482337,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10037879260760024720&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Baidu Inc., China; Baidu Inc., China + School of Computer Science, Beijing University of Posts and Telecommunications; Baidu Inc., China; Baidu Inc., China; Baidu Inc., China; Baidu Inc., China; Columbia University",
        "aff_domain": "baidu.com;gmail.com;baidu.com;baidu.com;baidu.com;baidu.com;columbia.edu",
        "email": "baidu.com;gmail.com;baidu.com;baidu.com;baidu.com;baidu.com;columbia.edu",
        "github": "https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2022-DuLeMon",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0+1;0;0;0;0;2",
        "aff_unique_norm": "Baidu;Beijing University of Posts and Telecommunications;Columbia University",
        "aff_unique_dep": "Baidu Inc.;School of Computer Science;",
        "aff_unique_url": "https://www.baidu.com;http://www.bupt.edu.cn/;https://www.columbia.edu",
        "aff_unique_abbr": "Baidu;BUPT;Columbia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0+0;0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.19",
        "title": "Long-range Sequence Modeling with Predictable Sparse Attention",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling, but it suffers from quadratic complexity in time and memory usage. Due to the sparsity of the attention matrix, much computation is redundant. Therefore, in this paper, we design an efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling. We provide a brand-new perspective for constructing sparse attention matrix, i.e. making the sparse attention matrix predictable. Two core sub-modules are: (1) A fast Fourier transform based hidden state cross module, which captures and pools L2 semantic combinations in \ud835\udcaa(Llog L) time complexity. (2) A sparse attention matrix estimation module, which predicts dominant elements of an attention matrix based on the output of the previous hidden state cross module. By reparameterization and gradient truncation, FSAT successfully learned the index of dominant elements. The overall complexity about the sequence length is reduced from \ud835\udcaa(L2) to \ud835\udcaa(Llog L). Extensive experiments (natural language, vision, and math) show that FSAT remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark.",
        "author": "Yimeng Zhuang; Jing Zhang; Mei Tu",
        "authorids": "/y/yimeng-zhuang/; /j/jing-zhang/; /m/mei-tu/",
        "bibtex": "@inproceedings{zhuang-etal-2022-long,\n    title = \"Long-range Sequence Modeling with Predictable Sparse Attention\",\n    author = \"Zhuang, Yimeng  and\n      Zhang, Jing  and\n      Tu, Mei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.19/\",\n    doi = \"10.18653/v1/2022.acl-long.19\",\n    pages = \"234--243\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.19.pdf",
        "site": "https://aclanthology.org/2022.acl-long.19/",
        "pdf_size": 716268,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14681681012787449115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Samsung Research China - Beijing (SRC-B); Samsung Research China - Beijing (SRC-B); Samsung Research China - Beijing (SRC-B)",
        "aff_domain": "samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Samsung",
        "aff_unique_dep": "Samsung Research China",
        "aff_unique_url": "https://www.samsung.com/cn/research/",
        "aff_unique_abbr": "SRC-B",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.465",
        "title": "Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Classifiers in natural language processing (NLP) often have a large number of output classes. For example, neural language models (LMs) and machine translation (MT) models both predict tokens from a vocabulary of thousands. The Softmax output layer of these models typically receives as input a dense feature representation, which has much lower dimensionality than the output. In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models (Demeter et al., 2020). In this paper we ask whether it can happen in practical large language models and translation models. To do so, we develop algorithms to detect such unargmaxable tokens in public models. We find that 13 out of 150 models do indeed have such tokens; however, they are very infrequent and unlikely to impact model quality. We release our algorithms and code to the public.",
        "author": "Andreas Grivas; Nikolay Bogoychev; Adam Lopez",
        "authorids": "/a/andreas-grivas/; /n/nikolay-bogoychev/; /a/adam-lopez/",
        "bibtex": "@inproceedings{grivas-etal-2022-low,\n    title = \"Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice\",\n    author = \"Grivas, Andreas  and\n      Bogoychev, Nikolay  and\n      Lopez, Adam\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.465/\",\n    doi = \"10.18653/v1/2022.acl-long.465\",\n    pages = \"6738--6758\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.465.pdf",
        "site": "https://aclanthology.org/2022.acl-long.465/",
        "pdf_size": 4090289,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3344853937392716855&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Language, Cognition, and Computation; Institute for Language, Cognition, and Computation; Institute for Language, Cognition, and Computation",
        "aff_domain": "ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "github": "https://github.com/andreasgrv/unargmaxable",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute for Language, Cognition, and Computation",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.391",
        "title": "M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different TV series, a total of 9,082 turns and 24,449 utterances. M3ED is annotated with 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral) at utterance level, and encompasses acoustic, visual, and textual modalities. To the best of our knowledge, M3ED is the first multimodal emotional dialogue dataset in Chinese.It is valuable for cross-culture emotion analysis and recognition. We apply several state-of-the-art methods on the M3ED dataset to verify the validity and quality of the dataset. We also propose a general Multimodal Dialogue-aware Interaction framework, MDI, to model the dialogue context for emotion recognition, which achieves comparable performance to the state-of-the-art methods on the M3ED. The full dataset and codes are available.",
        "author": "Jinming Zhao; Tenggan Zhang; Jingwen Hu; Yuchen Liu; Qin Jin; Xinchao Wang; Haizhou Li",
        "authorids": "/j/jinming-zhao/; /t/tenggan-zhang/; /j/jingwen-hu/; /y/yuchen-liu/; /q/qin-jin/; /x/xinchao-wang/; /h/haizhou-li/",
        "bibtex": "@inproceedings{zhao-etal-2022-m3ed,\n    title = \"{M}3{ED}: Multi-modal Multi-scene Multi-label Emotional Dialogue Database\",\n    author = \"Zhao, Jinming  and\n      Zhang, Tenggan  and\n      Hu, Jingwen  and\n      Liu, Yuchen  and\n      Jin, Qin  and\n      Wang, Xinchao  and\n      Li, Haizhou\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.391/\",\n    doi = \"10.18653/v1/2022.acl-long.391\",\n    pages = \"5699--5710\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.391.pdf",
        "site": "https://aclanthology.org/2022.acl-long.391/",
        "pdf_size": 522813,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4327429830464766506&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Information, Renmin University of China; School of Information, Renmin University of China; School of Information, Renmin University of China; School of Information, Renmin University of China; School of Information, Renmin University of China; Electrical and Computer Engineering, National University of Singapore; School of Data Science, The Chinese University of Hong Kong, Shenzhen, China+Electrical and Computer Engineering, National University of Singapore",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/AIM3-RUC/RUCM3ED",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;2+1",
        "aff_unique_norm": "Renmin University of China;National University of Singapore;Chinese University of Hong Kong",
        "aff_unique_dep": "School of Information;Electrical and Computer Engineering;School of Data Science",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.nus.edu.sg;https://www.cuhk.edu.cn",
        "aff_unique_abbr": "RUC;NUS;CUHK",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;1;0+1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.findings-acl.98",
        "title": "MDCSpell: A Multi-task Detector-Corrector Framework for Chinese Spelling Correction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Chinese Spelling Correction (CSC) is a task to detect and correct misspelled characters in Chinese texts. CSC is challenging since many Chinese characters are visually or phonologically similar but with quite different semantic meanings. Many recent works use BERT-based language models to directly correct each character of the input sentence. However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters. Some other works propose to use an error detector to guide the correction by masking the detected errors. Nevertheless, these methods dampen the visual or phonological features from the misspelled characters which could be critical for correction. In this work, we propose a novel general detector-corrector multi-task framework where the corrector uses BERT to capture the visual and phonological features from each character in the raw sentence and uses a late fusion strategy to fuse the hidden states of the corrector with that of the detector to minimize the negative impact from the misspelled characters. Comprehensive experiments on benchmarks demonstrate that our proposed method can significantly outperform the state-of-the-art methods in the CSC task.",
        "author": "Chenxi Zhu; Ziqiang Ying; Boyu Zhang; Feng Mao",
        "authorids": "/c/chenxi-zhu/; /z/ziqiang-ying/; /b/boyu-zhang/; /f/feng-mao/",
        "bibtex": "@inproceedings{zhu-etal-2022-mdcspell,\n    title = \"{MDCS}pell: A Multi-task Detector-Corrector Framework for {C}hinese Spelling Correction\",\n    author = \"Zhu, Chenxi  and\n      Ying, Ziqiang  and\n      Zhang, Boyu  and\n      Mao, Feng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.98/\",\n    doi = \"10.18653/v1/2022.findings-acl.98\",\n    pages = \"1244--1253\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.98.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.98/",
        "pdf_size": 866887,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14582335025393216045&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;163.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;163.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.34",
        "title": "MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content, which benefits downstream information retrieval and NLP tasks. Previous state-of-the-art methods select candidate keyphrases based on the similarity between learned representations of the candidates and the document. They suffer performance degradation on long documents due to discrepancy between sequence lengths which causes mismatch between representations of keyphrase candidates and the document. In this work, we propose a novel unsupervised embedding-based KPE approach, Masked Document Embedding Rank (MDERank), to address this problem by leveraging a mask strategy and ranking candidates by the similarity between embeddings of the source document and the masked document. We further develop a KPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised contrastive learning method, which is more compatible to MDERank than vanilla BERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the proposed MDERank outperforms state-of-the-art unsupervised KPE approach by average 1.80 F1@15 improvement. MDERank further benefits from KPEBERT and overall achieves average 3.53 F1@15 improvement over SIFRank.",
        "author": "Linhan Zhang; Qian Chen; Wen Wang; Chong Deng; ShiLiang Zhang; Bing Li; Wei Wang; Xin Cao",
        "authorids": "/l/linhan-zhang/; /q/qian-chen/; /w/wen-wang/; /c/chong-deng/; /s/shiliang-zhang/; /b/bing-li/; /w/wei-wang/; /x/xin-cao/",
        "bibtex": "@inproceedings{zhang-etal-2022-mderank,\n    title = \"{MDER}ank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction\",\n    author = \"Zhang, Linhan  and\n      Chen, Qian  and\n      Wang, Wen  and\n      Deng, Chong  and\n      Zhang, ShiLiang  and\n      Li, Bing  and\n      Wang, Wei  and\n      Cao, Xin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.34/\",\n    doi = \"10.18653/v1/2022.findings-acl.34\",\n    pages = \"396--409\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.34.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.34/",
        "pdf_size": 598469,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9393755272750361809&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science and Engineering, The University of New South Wales + Speech Lab, Alibaba Group, China; Speech Lab, Alibaba Group, China; Speech Lab, Alibaba Group, China; Speech Lab, Alibaba Group, China; Speech Lab, Alibaba Group, China; A*STAR Centre for Frontier AI Research (CFAR), Singapore; Hong Kong University of Science and Technology (Guangzhou), China; School of Computer Science and Engineering, The University of New South Wales",
        "aff_domain": "unsw.edu.au;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;ihpc.a-star.edu.sg;ust.hk;unsw.edu.au",
        "email": "unsw.edu.au;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;ihpc.a-star.edu.sg;ust.hk;unsw.edu.au",
        "github": "https://github.com/LinhanZ/mderank",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;1;1;1;2;3;0",
        "aff_unique_norm": "University of New South Wales;Alibaba Group;A*STAR Centre for Frontier AI Research;Hong Kong University of Science and Technology",
        "aff_unique_dep": "School of Computer Science and Engineering;Speech Lab;Centre for Frontier AI Research;",
        "aff_unique_url": "https://www.unsw.edu.au;https://www.alibaba.com;https://www.a-star.edu.sg;https://www.ust.hk",
        "aff_unique_abbr": "UNSW;Alibaba;A*STAR CFAR;HKUST",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Guangzhou",
        "aff_country_unique_index": "0+1;1;1;1;1;2;1;0",
        "aff_country_unique": "Australia;China;Singapore"
    },
    {
        "id": "2022.acl-long.160",
        "title": "MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Data augmentation is an effective solution to data scarcity in low-resource scenarios. However, when applied to token-level tasks such as NER, data augmentation methods often suffer from token-label misalignment, which leads to unsatsifactory performance. In this work, we propose Masked Entity Language Modeling (MELM) as a novel data augmentation framework for low-resource NER. To alleviate the token-label misalignment issue, we explicitly inject NER labels into sentence context, and thus the fine-tuned MELM is able to predict masked entity tokens by explicitly conditioning on their labels. Thereby, MELM generates high-quality augmented data with novel entities, which provides rich entity regularity knowledge and boosts NER performance. When training data from multiple languages are available, we also integrate MELM with code-mixing for further improvement. We demonstrate the effectiveness of MELM on monolingual, cross-lingual and multilingual NER across various low-resource levels. Experimental results show that our MELM consistently outperforms the baseline methods.",
        "author": "Ran Zhou; Xin Li; Ruidan He; Lidong Bing; Erik Cambria; Luo Si; Chunyan Miao",
        "authorids": "/r/ran-zhou/; /x/xin-li/; /r/ruidan-he/; /l/lidong-bing/; /e/erik-cambria/; /l/luo-si/; /c/chunyan-miao/",
        "bibtex": "@inproceedings{zhou-etal-2022-melm,\n    title = \"{MELM}: Data Augmentation with Masked Entity Language Modeling for Low-Resource {NER}\",\n    author = \"Zhou, Ran  and\n      Li, Xin  and\n      He, Ruidan  and\n      Bing, Lidong  and\n      Cambria, Erik  and\n      Si, Luo  and\n      Miao, Chunyan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.160/\",\n    doi = \"10.18653/v1/2022.acl-long.160\",\n    pages = \"2251--2262\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.160.pdf",
        "site": "https://aclanthology.org/2022.acl-long.160/",
        "pdf_size": 1489937,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16587206548164314214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "DAMO Academy, Alibaba Group+Joint Ph.D. Program between Alibaba and Nanyang Technological University; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; Nanyang Technological University, Singapore; DAMO Academy, Alibaba Group; Nanyang Technological University, Singapore",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;ntu.edu.sg;alibaba-inc.com;ntu.edu.sg",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;ntu.edu.sg;alibaba-inc.com;ntu.edu.sg",
        "github": "https://github.com/RandyZhouRan/MELM/",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0;0;0;1;0;1",
        "aff_unique_norm": "Alibaba Group;Nanyang Technological University",
        "aff_unique_dep": "DAMO Academy;Joint Ph.D. Program",
        "aff_unique_url": "https://www.alibaba-group.com;https://www.ntu.edu.sg",
        "aff_unique_abbr": "Alibaba;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;0;1;0;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.findings-acl.276",
        "title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from over-fitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform self-supervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements.",
        "author": "Fangkai Jiao; Yangyang Guo; Xuemeng Song; Liqiang Nie",
        "authorids": "/f/fangkai-jiao/; /y/yangyang-guo/; /x/xuemeng-song/; /l/liqiang-nie/",
        "bibtex": "@inproceedings{jiao-etal-2022-merit,\n    title = \"{MERI}t: {M}eta-{P}ath {G}uided {C}ontrastive {L}earning for {L}ogical {R}easoning\",\n    author = \"Jiao, Fangkai  and\n      Guo, Yangyang  and\n      Song, Xuemeng  and\n      Nie, Liqiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.276/\",\n    doi = \"10.18653/v1/2022.findings-acl.276\",\n    pages = \"3496--3509\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.276.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.276/",
        "pdf_size": 4524087,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4710528557018817840&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computing, National University of Singapore; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China",
        "aff_domain": "hotmail.com;gmail.com;gmail.com;gmail.com",
        "email": "hotmail.com;gmail.com;gmail.com;gmail.com",
        "github": "https://github.com/SparkJiao/MERIt",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Shandong University;National University of Singapore",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computing",
        "aff_unique_url": "http://www.sdu.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "SDU;NUS",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Qingdao;Singapore",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.acl-long.478",
        "title": "MILIE: Modular & Iterative Multilingual Open Information Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Open Information Extraction (OpenIE) is the task of extracting (subject, predicate, object) triples from natural language sentences. Current OpenIE systems extract all triple slots independently. In contrast, we explore the hypothesis that it may be beneficial to extract triple slots iteratively: first extract easy slots, followed by the difficult ones by conditioning on the easy slots, and therefore achieve a better overall extraction. Based on this hypothesis, we propose a neural OpenIE system, MILIE, that operates in an iterative fashion. Due to the iterative nature, the system is also modularit is possible to seamlessly integrate rule based extraction systems with a neural end-to-end system, thereby allowing rule based systems to supply extraction slots which MILIE can leverage for extracting the remaining slots. We confirm our hypothesis empirically: MILIE outperforms SOTA systems on multiple languages ranging from Chinese to Arabic. Additionally, we are the first to provide an OpenIE test dataset for Arabic and Galician.",
        "author": "Bhushan Kotnis; Kiril Gashteovski; Daniel Rubio; Ammar Shaker; Vanesa Rodriguez-Tembras; Makoto Takamoto; Mathias Niepert; Carolin Lawrence",
        "authorids": "/b/bhushan-kotnis/; /k/kiril-gashteovski/; /d/daniel-rubio/; /a/ammar-shaker/; /v/vanesa-rodriguez-tembras/; /m/makoto-takamoto/; /m/mathias-niepert/; /c/carolin-lawrence/",
        "bibtex": "@inproceedings{kotnis-etal-2022-milie,\n    title = \"{MILIE}: Modular {\\&} Iterative Multilingual Open Information Extraction\",\n    author = \"Kotnis, Bhushan  and\n      Gashteovski, Kiril  and\n      Rubio, Daniel  and\n      Shaker, Ammar  and\n      Rodriguez-Tembras, Vanesa  and\n      Takamoto, Makoto  and\n      Niepert, Mathias  and\n      Lawrence, Carolin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.478/\",\n    doi = \"10.18653/v1/2022.acl-long.478\",\n    pages = \"6939--6950\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.478.pdf",
        "site": "https://aclanthology.org/2022.acl-long.478/",
        "pdf_size": 1011937,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17332647347120129320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "NEC Laboratories Europe, Heidelberg, Germany; NEC Laboratories Europe, Heidelberg, Germany; NEC Laboratories Europe, Heidelberg, Germany; NEC Laboratories Europe, Heidelberg, Germany; Heidelberg University, Center for Iberoamerican Studies, Germany; NEC Laboratories Europe, Heidelberg, Germany; NEC Laboratories Europe, Heidelberg, Germany + University of Stuttgart, Germany; NEC Laboratories Europe, Heidelberg, Germany",
        "aff_domain": "neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu",
        "email": "neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu;neclab.eu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;1;0;0+2;0",
        "aff_unique_norm": "NEC Laboratories Europe;Heidelberg University;University of Stuttgart",
        "aff_unique_dep": ";Center for Iberoamerican Studies;",
        "aff_unique_url": "https://www.nec-labs.eu;https://www.uni-heidelberg.de;https://www.uni-stuttgart.de",
        "aff_unique_abbr": "NEC Europe;;USTuttgart",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Heidelberg;",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.findings-acl.63",
        "title": "MIMICause: Representation and automatic extraction of causal relation types from clinical notes",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. Extracted causal information from clinical notes can be combined with structured EHR data such as patients\u2019 demographics, diagnoses, and medications. This will enhance healthcare providers\u2019 ability to identify aspects of a patient\u2019s story communicated in the clinical notes and help make more informed decisions. In this work, we propose annotation guidelines, develop an annotated corpus and provide baseline scores to identify types and direction of causal relations between a pair of biomedical concepts in clinical notes; communicated implicitly or explicitly, identified either in a single sentence or across multiple sentences. We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2 shared task dataset and train four different language model based architectures. Annotation based on our guidelines achieved a high inter-annotator agreement i.e. Fleiss\u2019 kappa (\ud835\udf05) score of 0.72, and our model for identification of causal relations achieved a macro F1 score of 0.56 on the test data. The high inter-annotator agreement for clinical text shows the quality of our annotation guidelines while the provided baseline F1 score sets the direction for future research towards understanding narratives in clinical texts.",
        "author": "Vivek Khetan; Md Imbesat Rizvi; Jessica Huber; Paige Bartusiak; Bogdan Sacaleanu; Andrew Fano",
        "authorids": "/v/vivek-khetan/; /m/md-imbesat-rizvi/; /j/jessica-huber/; /p/paige-bartusiak/; /b/bogdan-sacaleanu/; /a/andrew-fano/",
        "bibtex": "@inproceedings{khetan-etal-2022-mimicause,\n    title = \"{MIMIC}ause: {R}epresentation and automatic extraction of causal relation types from clinical notes\",\n    author = \"Khetan, Vivek  and\n      Rizvi, Md Imbesat  and\n      Huber, Jessica  and\n      Bartusiak, Paige  and\n      Sacaleanu, Bogdan  and\n      Fano, Andrew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.63/\",\n    doi = \"10.18653/v1/2022.findings-acl.63\",\n    pages = \"764--773\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.63.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.63/",
        "pdf_size": 1551252,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10710824357029086133&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Accenture Labs, SF+Indian Institute of Science; Indian Institute of Science+Arizona State University; Arizona State University+Accenture Labs, SF; Duke University; Accenture Labs, SF; Accenture Labs, SF",
        "aff_domain": "accenture.com;iisc.ac.in;asu.edu;cs.duke.edu;accenture.com;accenture.com",
        "email": "accenture.com;iisc.ac.in;asu.edu;cs.duke.edu;accenture.com;accenture.com",
        "github": "",
        "project": "https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",
        "author_num": 6,
        "aff_unique_index": "0+1;1+2;2+0;3;0;0",
        "aff_unique_norm": "Accenture Labs;Indian Institute of Science;Arizona State University;Duke University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.accenture.com/us-en/labs;https://www.iisc.ac.in;https://www.asu.edu;https://www.duke.edu",
        "aff_unique_abbr": "Accenture Labs;IISc;ASU;Duke",
        "aff_campus_unique_index": "0;;0;0;0",
        "aff_campus_unique": "San Francisco;",
        "aff_country_unique_index": "0+1;1+0;0+0;0;0;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2022.acl-long.383",
        "title": "MINER: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "NER model has achieved promising performance on standard NER benchmarks. However, recent studies show that previous approaches may over-rely on entity mention information, resulting in poor performance on out-of-vocabulary(OOV) entity recognition. In this work, we propose MINER, a novel NER learning framework, to remedy this issue from an information-theoretic perspective. The proposed approach contains two mutual information based training objectives: i) generalizing information maximization, which enhances representation via deep understanding of context and entity surface forms; ii) superfluous information minimization, which discourages representation from rotate memorizing entity names or exploiting biased cues in data. Experiments on various settings and datasets demonstrate that it achieves better performance in predicting OOV entities.",
        "author": "Xiao Wang; Shihan Dou; Limao Xiong; Yicheng Zou; Qi Zhang; Tao Gui; Liang Qiao; Zhanzhan Cheng; Xuanjing Huang",
        "authorids": "/x/xiao-wang/; /s/shihan-dou/; /l/limao-xiong/; /y/yicheng-zou/; /q/qi-zhang/; /t/tao-gui/; /l/liang-qiao/; /z/zhanzhan-cheng/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{wang-etal-2022-miner,\n    title = \"{MINER}: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective\",\n    author = \"Wang, Xiao  and\n      Dou, Shihan  and\n      Xiong, Limao  and\n      Zou, Yicheng  and\n      Zhang, Qi  and\n      Gui, Tao  and\n      Qiao, Liang  and\n      Cheng, Zhanzhan  and\n      Huang, Xuanjing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.383/\",\n    doi = \"10.18653/v1/2022.acl-long.383\",\n    pages = \"5590--5600\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.383.pdf",
        "site": "https://aclanthology.org/2022.acl-long.383/",
        "pdf_size": 540627,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7263266435984981792&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China; Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China; Hikvision Research Institute, Hangzhou, China; Hikvision Research Institute, Hangzhou, China; School of Computer Science, Fudan University, Shanghai, China",
        "aff_domain": "fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn; ; ;fudan.edu.cn",
        "email": "fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn; ; ;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0+1;0;2;2;0",
        "aff_unique_norm": "Fudan University;Shanghai Key Laboratory of Intelligent Information Processing;Hikvision Research Institute",
        "aff_unique_dep": "School of Computer Science;;",
        "aff_unique_url": "https://www.fudan.edu.cn;;https://www.hikvision.com/cn/",
        "aff_unique_abbr": "Fudan;;HRI",
        "aff_campus_unique_index": "0;0;0;0;0;0;2;2;0",
        "aff_campus_unique": "Shanghai;;Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.29",
        "title": "MINER: Multi-Interest Matching Network for News Recommendation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Personalized news recommendation is an essential technique to help users find interested news. Accurately matching user\u2019s interests and candidate news is the key to news recommendation. Most existing methods learn a single user embedding from user\u2019s historical behaviors to represent the reading interest. However, user interest is usually diverse and may not be adequately modeled by a single user embedding. In this paper, we propose a poly attention scheme to learn multiple interest vectors for each user, which encodes the different aspects of user interest. We further propose a disagreement regularization to make the learned interests vectors more diverse. Moreover, we design a category-aware attention weighting strategy that incorporates the news category information as explicit interest signals into the attention mechanism. Extensive experiments on the MIND news recommendation benchmark demonstrate that our approach significantly outperforms existing state-of-the-art methods.",
        "author": "Jian Li; Jieming Zhu; Qiwei Bi; Guohao Cai; Lifeng Shang; Zhenhua Dong; Xin Jiang; Qun Liu",
        "authorids": "/j/jian-li/; /j/jieming-zhu/; /q/qiwei-bi/; /g/guohao-cai/; /l/lifeng-shang/; /z/zhenhua-dong/; /x/xin-jiang/; /q/qun-liu/",
        "bibtex": "@inproceedings{li-etal-2022-miner,\n    title = \"{MINER}: Multi-Interest Matching Network for News Recommendation\",\n    author = \"Li, Jian  and\n      Zhu, Jieming  and\n      Bi, Qiwei  and\n      Cai, Guohao  and\n      Shang, Lifeng  and\n      Dong, Zhenhua  and\n      Jiang, Xin  and\n      Liu, Qun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.29/\",\n    doi = \"10.18653/v1/2022.findings-acl.29\",\n    pages = \"343--352\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.29.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.29/",
        "pdf_size": 1063438,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15374214394580461796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; School of Statistics, Renmin University of China; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "huawei.com;huawei.com; ;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "email": "huawei.com;huawei.com; ;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;0;0;0;0;0",
        "aff_unique_norm": "Huawei;Renmin University of China",
        "aff_unique_dep": "Noah\u2019s Ark Lab;School of Statistics",
        "aff_unique_url": "https://www.huawei.com;http://www.ruc.edu.cn",
        "aff_unique_abbr": "Huawei;RUC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.25",
        "title": "MISC: A Mixed Strategy-Aware Model integrating COMET for Emotional Support Conversation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Applying existing methods to emotional support conversation\u2014which provides valuable assistance to people who are in need\u2014has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user\u2019s instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user\u2019s distress. To address the problems, we propose a novel model MISC, which firstly infers the user\u2019s fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling.",
        "author": "Quan Tu; Yanran Li; Jianwei Cui; Bin Wang; Ji-Rong Wen; Rui Yan",
        "authorids": "/q/quan-tu/; /y/yanran-li/; /j/jianwei-cui/; /b/bin-wang/; /j/ji-rong-wen/; /r/rui-yan/",
        "bibtex": "@inproceedings{tu-etal-2022-misc,\n    title = \"{MISC}: A Mixed Strategy-Aware Model integrating {COMET} for Emotional Support Conversation\",\n    author = \"Tu, Quan  and\n      Li, Yanran  and\n      Cui, Jianwei  and\n      Wang, Bin  and\n      Wen, Ji-Rong  and\n      Yan, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.25/\",\n    doi = \"10.18653/v1/2022.acl-long.25\",\n    pages = \"308--319\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.25.pdf",
        "site": "https://aclanthology.org/2022.acl-long.25/",
        "pdf_size": 1257170,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4380460151564443343&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Academy of Artificial Intelligence; Xiaomi AI Lab; Xiaomi AI Lab; Xiaomi AI Lab; Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Academy of Artificial Intelligence; Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Academy of Artificial Intelligence",
        "aff_domain": "ruc.edu.cn;xiaomi.com;xiaomi.com;xiaomi.com;ruc.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn;xiaomi.com;xiaomi.com;xiaomi.com;ruc.edu.cn;ruc.edu.cn",
        "github": "https://github.com/morecry/MISC",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;2;2;0+1;0+1",
        "aff_unique_norm": "Renmin University of China;Beijing Academy of Artificial Intelligence;Xiaomi Corporation",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;;Xiaomi AI Lab",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.baaic.cn;https://www.xiaomi.com",
        "aff_unique_abbr": "RUC;BAAI;Xiaomi",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.290",
        "title": "MMCoQA: Conversational Question Answering over Text, Tables, and Images",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The rapid development of conversational assistants accelerates the study on conversational question answering (QA). However, the existing conversational QA systems usually answer users\u2019 questions with a single knowledge source, e.g., paragraphs or a knowledge graph, but overlook the important visual cues, let alone multiple knowledge sources of different modalities. In this paper, we hence define a novel research task, i.e., multimodal conversational question answering (MMCoQA), aiming to answer users\u2019 questions with multimodal knowledge sources via multi-turn conversations. This new task brings a series of research challenges, including but not limited to priority, consistency, and complementarity of multimodal knowledge. To facilitate the data-driven approaches in this area, we construct the first multimodal conversational QA dataset, named MMConvQA. Questions are fully annotated with not only natural language answers but also the corresponding evidence and valuable decontextualized self-contained questions. Meanwhile, we introduce an end-to-end baseline model, which divides this complex research task into question understanding, multi-modal evidence retrieval, and answer extraction. Moreover, we report a set of benchmarking results, and the results indicate that there is ample room for improvement.",
        "author": "Yongqi Li; Wenjie Li; Liqiang Nie",
        "authorids": "/y/yongqi-li-hk/; /w/wenjie-li/; /l/liqiang-nie/",
        "bibtex": "@inproceedings{li-etal-2022-mmcoqa,\n    title = \"{MMC}o{QA}: Conversational Question Answering over Text, Tables, and Images\",\n    author = \"Li, Yongqi  and\n      Li, Wenjie  and\n      Nie, Liqiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.290/\",\n    doi = \"10.18653/v1/2022.acl-long.290\",\n    pages = \"4220--4231\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.290.pdf",
        "site": "https://aclanthology.org/2022.acl-long.290/",
        "pdf_size": 6076971,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1226769046761180183&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; Shandong University",
        "aff_domain": "gmail.com;comp.polyu.edu.hk;gmail.com",
        "email": "gmail.com;comp.polyu.edu.hk;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Hong Kong Polytechnic University;Shandong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.polyu.edu.hk;http://www.sdu.edu.cn",
        "aff_unique_abbr": "PolyU;SDU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.488",
        "title": "MPII: Multi-Level Mutual Promotion for Inference and Interpretation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In order to better understand the rationale behind model behavior, recent works have exploited providing interpretation to support the inference prediction. However, existing methods tend to provide human-unfriendly interpretation, and are prone to sub-optimal performance due to one-side promotion, i.e. either inference promotion with interpretation or vice versa. In this paper, we propose a multi-level Mutual Promotion mechanism for self-evolved Inference and sentence-level Interpretation (MPII). Specifically, from the model-level, we propose a Step-wise Integration Mechanism to jointly perform and deeply integrate inference and interpretation in an autoregressive manner. From the optimization-level, we propose an Adversarial Fidelity Regularization to improve the fidelity between inference and interpretation with the Adversarial Mutual Information training strategy. Extensive experiments on NLI and CQA tasks reveal that the proposed MPII approach can significantly outperform baseline models for both the inference performance and the interpretation quality.",
        "author": "Yan Liu; Sanyuan Chen; Yazheng Yang; Qi Dai",
        "authorids": "/y/yan-liu/; /s/sanyuan-chen/; /y/yazheng-yang/; /q/qi-dai/",
        "bibtex": "@inproceedings{liu-etal-2022-mpii,\n    title = \"{MPII}: Multi-Level Mutual Promotion for Inference and Interpretation\",\n    author = \"Liu, Yan  and\n      Chen, Sanyuan  and\n      Yang, Yazheng  and\n      Dai, Qi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.488/\",\n    doi = \"10.18653/v1/2022.acl-long.488\",\n    pages = \"7074--7084\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.488.pdf",
        "site": "https://aclanthology.org/2022.acl-long.488/",
        "pdf_size": 752915,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6323435763295257118&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, Zhejiang University, China; School of Computer Science and Technology, Harbin Institute of Technology, China+Microsoft Research, Beijing, China; School of Computer Science and Technology, Zhejiang University, China; Microsoft Research, Beijing, China",
        "aff_domain": "gmail.com;ir.hit.edu.cn;zju.edu.cn;microsoft.com",
        "email": "gmail.com;ir.hit.edu.cn;zju.edu.cn;microsoft.com",
        "github": "https://github.com/theNamek/MPII.git",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;0;2",
        "aff_unique_norm": "Zhejiang University;Harbin Institute of Technology;Microsoft",
        "aff_unique_dep": "School of Computer Science and Technology;School of Computer Science and Technology;Microsoft Research",
        "aff_unique_url": "http://www.zju.edu.cn;http://www.hit.edu.cn/;https://www.microsoft.com/en-us/research/group/microsoft-research-asia",
        "aff_unique_abbr": "ZJU;HIT;MSR",
        "aff_campus_unique_index": "1+2;2",
        "aff_campus_unique": ";Harbin;Beijing",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.25",
        "title": "MR-P: A Parallel Decoding Algorithm for Iterative Refinement Non-Autoregressive Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Non-autoregressive translation (NAT) predicts all the target tokens in parallel and significantly speeds up the inference process. The Conditional Masked Language Model (CMLM) is a strong baseline of NAT. It decodes with the Mask-Predict algorithm which iteratively refines the output. Most works about CMLM focus on the model structure and the training objective. However, the decoding algorithm is equally important. We propose a simple, effective, and easy-to-implement decoding algorithm that we call MaskRepeat-Predict (MR-P). The MR-P algorithm gives higher priority to consecutive repeated tokens when selecting tokens to mask for the next iteration and stops the iteration after target tokens converge. We conduct extensive experiments on six translation directions with varying data sizes. The results show that MR-P significantly improves the performance with the same model parameters. Specifically, we achieve a BLEU increase of 1.39 points in the WMT\u201914 En-De translation task.",
        "author": "Hao Cheng; Zhihua Zhang",
        "authorids": "/h/hao-cheng/; /z/zhihua-zhang/",
        "bibtex": "@inproceedings{cheng-zhang-2022-mr,\n    title = \"{MR}-{P}: A Parallel Decoding Algorithm for Iterative Refinement Non-Autoregressive Translation\",\n    author = \"Cheng, Hao  and\n      Zhang, Zhihua\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.25/\",\n    doi = \"10.18653/v1/2022.findings-acl.25\",\n    pages = \"285--296\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.25.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.25/",
        "pdf_size": 512015,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:g0pWI2MbgIUJ:scholar.google.com/&scioq=MR-P:+A+Parallel+Decoding+Algorithm+for+Iterative+Refinement+Non-Autoregressive+Translation&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Academy for Advanced Interdisciplinary Studies, Peking University; School of Mathematical Sciences, Peking University",
        "aff_domain": "pku.edu.cn;math.pku.edu.cn",
        "email": "pku.edu.cn;math.pku.edu.cn",
        "github": "https://github.com/chynphh/MaskRepeat-Predict",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Academy for Advanced Interdisciplinary Studies",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.198",
        "title": "MReD: A Meta-Review Dataset for Structure-Controllable Text Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.",
        "author": "Chenhui Shen; Liying Cheng; Ran Zhou; Lidong Bing; Yang You; Luo Si",
        "authorids": "/c/chenhui-shen/; /l/liying-cheng/; /r/ran-zhou/; /l/lidong-bing/; /y/yang-you/; /l/luo-si/",
        "bibtex": "@inproceedings{shen-etal-2022-mred,\n    title = \"{MR}e{D}: A Meta-Review Dataset for Structure-Controllable Text Generation\",\n    author = \"Shen, Chenhui  and\n      Cheng, Liying  and\n      Zhou, Ran  and\n      Bing, Lidong  and\n      You, Yang  and\n      Si, Luo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.198/\",\n    doi = \"10.18653/v1/2022.findings-acl.198\",\n    pages = \"2521--2535\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.198.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.198/",
        "pdf_size": 1390567,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9412573381579002931&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 4,
        "aff": "DAMO Academy, Alibaba Group+National University of Singapore; DAMO Academy, Alibaba Group+Singapore University of Technology and Design; DAMO Academy, Alibaba Group+Nanyang Technological University; DAMO Academy, Alibaba Group; National University of Singapore; DAMO Academy, Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;comp.nus.edu.sg;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;comp.nus.edu.sg;alibaba-inc.com",
        "github": "https://github.com/Shen-Chenhui/MReD",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+2;0+3;0;1;0",
        "aff_unique_norm": "Alibaba Group;National University of Singapore;Singapore University of Technology and Design;Nanyang Technological University",
        "aff_unique_dep": "DAMO Academy;;;",
        "aff_unique_url": "https://www.alibaba-group.com;https://www.nus.edu.sg;https://www.sutd.edu.sg;https://www.ntu.edu.sg",
        "aff_unique_abbr": "Alibaba;NUS;SUTD;NTU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0+1;0;1;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.acl-long.186",
        "title": "MSCTD: A Multimodal Sentiment Chat Translation Dataset",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal machine translation and textual chat translation have received considerable attention in recent years. Although the conversation in its natural form is usually multimodal, there still lacks work on multimodal machine translation in conversations. In this work, we introduce a new task named Multimodal Chat Translation (MCT), aiming to generate more accurate translations with the help of the associated dialogue history and visual context. To this end, we firstly construct a Multimodal Sentiment Chat Translation Dataset (MSCTD) containing 142,871 English-Chinese utterance pairs in 14,762 bilingual dialogues. Each utterance pair, corresponding to the visual context that reflects the current conversational scene, is annotated with a sentiment label. Then, we benchmark the task by establishing multiple baseline systems that incorporate multimodal and sentiment features for MCT. Preliminary experiments on two language directions (English-Chinese) verify the potential of contextual and multimodal information fusion and the positive impact of sentiment on the MCT task. Additionally, we provide a new benchmark on multimodal dialogue sentiment analysis with the constructed MSCTD. Our work can facilitate researches on both multimodal chat translation and multimodal dialogue sentiment analysis.",
        "author": "Yunlong Liang; Fandong Meng; Jinan Xu; Yufeng Chen; Jie Zhou",
        "authorids": "/y/yunlong-liang/; /f/fandong-meng/; /j/jinan-xu/; /y/yufeng-chen/; /j/jie-zhou/",
        "bibtex": "@inproceedings{liang-etal-2022-msctd,\n    title = \"{MSCTD}: A Multimodal Sentiment Chat Translation Dataset\",\n    author = \"Liang, Yunlong  and\n      Meng, Fandong  and\n      Xu, Jinan  and\n      Chen, Yufeng  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.186/\",\n    doi = \"10.18653/v1/2022.acl-long.186\",\n    pages = \"2601--2613\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.186.pdf",
        "site": "https://aclanthology.org/2022.acl-long.186/",
        "pdf_size": 6561105,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14060538125674441927&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China + Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "bjtu.edu.cn;tencent.com;bjtu.edu.cn;bjtu.edu.cn;tencent.com",
        "email": "bjtu.edu.cn;tencent.com;bjtu.edu.cn;bjtu.edu.cn;tencent.com",
        "github": "https://github.com/XL2248/MSCTD",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;0;0;1",
        "aff_unique_norm": "Beijing Jiao Tong University;Tencent",
        "aff_unique_dep": "Beijing Key Lab of Traffic Data Analysis and Mining;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "http://www.bjtu.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "BJTU;Tencent",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.424",
        "title": "MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prompting has recently been shown as a promising approach for applying pre-trained language models to perform downstream tasks. We present Multi-Stage Prompting, a simple and automatic approach for leveraging pre-trained language models to translation tasks. To better mitigate the discrepancy between pre-training and translation, MSP divides the translation process via pre-trained language models into three separate stages: the encoding stage, the re-encoding stage, and the decoding stage. During each stage, we independently apply different continuous prompts for allowing pre-trained language models better shift to translation tasks. We conduct extensive experiments on three translation tasks. Experiments show that our method can significantly improve the translation performance of pre-trained language models.",
        "author": "Zhixing Tan; Xiangwen Zhang; Shuo Wang; Yang Liu",
        "authorids": "/z/zhixing-tan/; /x/xiangwen-zhang/; /s/shuo-wang/; /y/yang-liu-ict/",
        "bibtex": "@inproceedings{tan-etal-2022-msp,\n    title = \"{MSP}: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators\",\n    author = \"Tan, Zhixing  and\n      Zhang, Xiangwen  and\n      Wang, Shuo  and\n      Liu, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.424/\",\n    doi = \"10.18653/v1/2022.acl-long.424\",\n    pages = \"6131--6142\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.424.pdf",
        "site": "https://aclanthology.org/2022.acl-long.424/",
        "pdf_size": 634948,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4682251571809396941&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Technology, Tsinghua University, Beijing, China+Institute for AI Industry Research, Tsinghua University, Beijing, China+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+International Innovation Center of Tsinghua University, Shanghai, China; Kuaishou Tech, Co.; Department of Computer Science and Technology, Tsinghua University, Beijing, China+Institute for AI Industry Research, Tsinghua University, Beijing, China+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+International Innovation Center of Tsinghua University, Shanghai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China+Institute for AI Industry Research, Tsinghua University, Beijing, China+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+International Innovation Center of Tsinghua University, Shanghai, China",
        "aff_domain": "tsinghua.edu.cn; ; ;tsinghua.edu.cn",
        "email": "tsinghua.edu.cn; ; ;tsinghua.edu.cn",
        "github": "https://github.com/THUNLP-MT/PLM4MT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+0+1+0;2;0+0+0+1+0;0+0+0+1+0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Kuaishou Technology Company",
        "aff_unique_dep": "Department of Computer Science and Technology;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.kuaishou.com",
        "aff_unique_abbr": "THU;;Kuaishou",
        "aff_campus_unique_index": "0+0+0+2;0+0+0+2;0+0+0+2",
        "aff_campus_unique": "Beijing;;Shanghai",
        "aff_country_unique_index": "0+0+0+0+0;0;0+0+0+0+0;0+0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.209",
        "title": "MTRec: Multi-Task Learning over BERT for News Recommendation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Existing news recommendation methods usually learn news representations solely based on news titles. To sufficiently utilize other fields of news information such as category and entities, some methods treat each field as an additional feature and combine different feature vectors with attentive pooling. With the adoption of large pre-trained models like BERT in news recommendation, the above way to incorporate multi-field information may encounter challenges: the shallow feature encoding to compress the category and entity information is not compatible with the deep BERT encoding. In this paper, we propose a multi-task method to incorporate the multi-field information into BERT, which improves its news encoding capability. Besides, we modify the gradients of auxiliary tasks based on their gradient conflicts with the main task, which further boosts the model performance. Extensive experiments on the MIND news recommendation benchmark show the effectiveness of our approach.",
        "author": "Qiwei Bi; Jian Li; Lifeng Shang; Xin Jiang; Qun Liu; Hanfang Yang",
        "authorids": "/q/qiwei-bi/; /j/jian-li/; /l/lifeng-shang/; /x/xin-jiang/; /q/qun-liu/; /h/hanfang-yang/",
        "bibtex": "@inproceedings{bi-etal-2022-mtrec,\n    title = \"{MTR}ec: Multi-Task Learning over {BERT} for News Recommendation\",\n    author = \"Bi, Qiwei  and\n      Li, Jian  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Yang, Hanfang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.209/\",\n    doi = \"10.18653/v1/2022.findings-acl.209\",\n    pages = \"2663--2669\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.209.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.209/",
        "pdf_size": 915444,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2777073647245938439&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Statistics, Renmin University of China + Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Center for Applied Statistics, Renmin University of China + School of Statistics, Renmin University of China",
        "aff_domain": "ruc.edu.cn;huawei.com;huawei.com;huawei.com;huawei.com;ruc.edu.cn",
        "email": "ruc.edu.cn;huawei.com;huawei.com;huawei.com;huawei.com;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;0+0",
        "aff_unique_norm": "Renmin University of China;Huawei",
        "aff_unique_dep": "School of Statistics;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": "RUC;Huawei",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.55",
        "title": "Machine Translation for Livonian: Catering to 20 Speakers",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Livonian is one of the most endangered languages in Europe with just a tiny handful of speakers and virtually no publicly available corpora. In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other \u2013 enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora. We rely on Livonian\u2019s linguistic similarity to Estonian and Latvian and collect parallel and monolingual data for the four languages for translation experiments. We combine different low-resource NMT techniques like zero-shot translation, cross-lingual transfer and synthetic data creation to reach the highest possible translation quality as well as to find which base languages are empirically more helpful for transfer to Livonian. The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released via OPUS and Huggingface repositories.",
        "author": "Mat\u012bss Rikters; Marili Tomingas; Tuuli Tuisk; Valts Ern\u0161treits; Mark Fishel",
        "authorids": "/m/matiss-rikters/; /m/marili-tomingas/; /t/tuuli-tuisk/; /v/valts-ernstreits/; /m/mark-fishel/",
        "bibtex": "@inproceedings{rikters-etal-2022-machine,\n    title = \"Machine Translation for {L}ivonian: Catering to 20 Speakers\",\n    author = \"Rikters, Mat{\\={i}}ss  and\n      Tomingas, Marili  and\n      Tuisk, Tuuli  and\n      Ern{\\v{s}}treits, Valts  and\n      Fishel, Mark\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.55/\",\n    doi = \"10.18653/v1/2022.acl-short.55\",\n    pages = \"508--514\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.55.pdf",
        "site": "https://aclanthology.org/2022.acl-short.55/",
        "pdf_size": 168178,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3182050876508012911&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Tartu; University of Tartu; University of Tartu + University of Copenhagen; University of Latvia; University of Tartu",
        "aff_domain": "ut.ee;ut.ee;ut.ee;lu.lv;ut.ee",
        "email": "ut.ee;ut.ee;ut.ee;lu.lv;ut.ee",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0+1;2;0",
        "aff_unique_norm": "University of Tartu;University of Copenhagen;University of Latvia",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ut.ee;https://www.ku.dk;https://www.lu.lv",
        "aff_unique_abbr": "UT;UCPH;UL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;2;0",
        "aff_country_unique": "Estonia;Denmark;Latvia"
    },
    {
        "id": "2022.acl-long.529",
        "title": "Make the Best of Cross-lingual Transfer: Evidence from POS Tagging with over 100 Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-lingual transfer learning with large multilingual pre-trained models can be an effective approach for low-resource languages with no labeled training data. Existing evaluations of zero-shot cross-lingual generalisability of large pre-trained models use datasets with English training data, and test data in a selection of target languages. We explore a more extensive transfer learning setup with 65 different source languages and 105 target languages for part-of-speech tagging. Through our analysis, we show that pre-training of both source and target language, as well as matching language families, writing systems, word order systems, and lexical-phonetic distance significantly impact cross-lingual performance. The findings described in this paper can be used as indicators of which factors are important for effective zero-shot cross-lingual transfer to zero- and low-resource languages.",
        "author": "Wietse de Vries; Martijn Wieling; Malvina Nissim",
        "authorids": "/w/wietse-de-vries/; /m/martijn-wieling/; /m/malvina-nissim/",
        "bibtex": "@inproceedings{de-vries-etal-2022-make,\n    title = \"Make the Best of Cross-lingual Transfer: Evidence from {POS} Tagging with over 100 Languages\",\n    author = \"de Vries, Wietse  and\n      Wieling, Martijn  and\n      Nissim, Malvina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.529/\",\n    doi = \"10.18653/v1/2022.acl-long.529\",\n    pages = \"7676--7685\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.529.pdf",
        "site": "https://aclanthology.org/2022.acl-long.529/",
        "pdf_size": 500181,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13378855858474450248&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Groningen; University of Groningen; University of Groningen",
        "aff_domain": "rug.nl;rug.nl;rug.nl",
        "email": "rug.nl;rug.nl;rug.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Groningen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rug.nl",
        "aff_unique_abbr": "RUG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.acl-long.251",
        "title": "Making Transformers Solve Compositional Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization. We identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in many compositional tasks. We achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).",
        "author": "Santiago Ontanon; Joshua Ainslie; Zachary Fisher; Vaclav Cvicek",
        "authorids": "/s/santiago-ontanon/; /j/joshua-ainslie/; /z/zachary-fisher/; /v/vaclav-cvicek/",
        "bibtex": "@inproceedings{ontanon-etal-2022-making,\n    title = \"Making Transformers Solve Compositional Tasks\",\n    author = \"Ontanon, Santiago  and\n      Ainslie, Joshua  and\n      Fisher, Zachary  and\n      Cvicek, Vaclav\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.251/\",\n    doi = \"10.18653/v1/2022.acl-long.251\",\n    pages = \"3591--3607\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.251.pdf",
        "site": "https://aclanthology.org/2022.acl-long.251/",
        "pdf_size": 532145,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2111862438578018510&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "github": "https://github.com/google-research/google-research/tree/master/compositional_transformers",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.420",
        "title": "MarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal pre-training with text, layout, and image has made significant progress for Visually Rich Document Understanding (VRDU), especially the fixed-layout documents such as scanned document images. While, there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks. The pre-trained model and code will be publicly available at https://aka.ms/markuplm.",
        "author": "Junlong Li; Yiheng Xu; Lei Cui; Furu Wei",
        "authorids": "/j/junlong-li/; /y/yiheng-xu/; /l/lei-cui/; /f/furu-wei/",
        "bibtex": "@inproceedings{li-etal-2022-markuplm,\n    title = \"{M}arkup{LM}: Pre-training of Text and Markup Language for Visually Rich Document Understanding\",\n    author = \"Li, Junlong  and\n      Xu, Yiheng  and\n      Cui, Lei  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.420/\",\n    doi = \"10.18653/v1/2022.acl-long.420\",\n    pages = \"6078--6087\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.420.pdf",
        "site": "https://aclanthology.org/2022.acl-long.420/",
        "pdf_size": 1539366,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9049750357822345594&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Shanghai Jiao Tong University; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia",
        "aff_domain": "sjtu.edu.cn;microsoft.com;microsoft.com;microsoft.com",
        "email": "sjtu.edu.cn;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "https://aka.ms/markuplm",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "SJTU;MSR Asia",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.106",
        "title": "Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pretrained multilingual models enable zero-shot learning even for unseen languages, and that performance can be further improved via adaptation prior to finetuning. However, it is unclear how the number of pretraining languages influences a model\u2019s zero-shot learning for languages unseen during pretraining. To fill this gap, we ask the following research questions: (1) How does the number of pretraining languages influence zero-shot performance on unseen target languages? (2) Does the answer to that question change with model adaptation? (3) Do the findings for our first question change if the languages used for pretraining are all related? Our experiments on pretraining with related languages indicate that choosing a diverse set of languages is crucial. Without model adaptation, surprisingly, increasing the number of pretraining languages yields better results up to adding related languages, after which performance plateaus. In contrast, with model adaptation via continued pretraining, pretraining on a larger number of languages often gives further improvement, suggesting that model adaptation is crucial to exploit additional pretraining languages.",
        "author": "Yoshinari Fujinuma; Jordan Boyd-Graber; Katharina Kann",
        "authorids": "/y/yoshinari-fujinuma/; /j/jordan-boyd-graber/; /k/katharina-von-der-wense/",
        "bibtex": "@inproceedings{fujinuma-etal-2022-match,\n    title = \"Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability\",\n    author = \"Fujinuma, Yoshinari  and\n      Boyd-Graber, Jordan  and\n      Kann, Katharina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.106/\",\n    doi = \"10.18653/v1/2022.acl-long.106\",\n    pages = \"1500--1512\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.106.pdf",
        "site": "https://aclanthology.org/2022.acl-long.106/",
        "pdf_size": 405307,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8107421632219118339&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "AWS AI Labs + University of Colorado Boulder; UMIACS, CS, LSC, iSchool, University of Maryland; Computer Science, University of Colorado Boulder",
        "aff_domain": "gmail.com;umiacs.umd.edu;colorado.edu",
        "email": "gmail.com;umiacs.umd.edu;colorado.edu",
        "github": "https://github.com/akkikiki/multilingual_zeroshot_analysis",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "Amazon;University of Colorado;University of Maryland;University of Colorado Boulder",
        "aff_unique_dep": "AWS AI Labs;;UMIACS, CS, LSC, iSchool;Computer Science",
        "aff_unique_url": "https://aws.amazon.com;https://www.colorado.edu;https://www.umd.edu;https://www.colorado.edu",
        "aff_unique_abbr": "AWS;CU;UMD;CU Boulder",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Boulder",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.401",
        "title": "Measuring Fairness of Text Classifiers via Prediction Sensitivity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions. Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system. In this work, we propose a new formulation \u2013 accumulated prediction sensitivity, which measures fairness in machine learning models based on the model\u2019s prediction sensitivity to perturbations in input features. The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group. We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness. It also correlates well with humans\u2019 perception of fairness. We conduct experiments on two text classification datasets \u2013 Jigsaw Toxicity, and Bias in Bios, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome. We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric.",
        "author": "Satyapriya Krishna; Rahul Gupta; Apurv Verma; Jwala Dhamala; Yada Pruksachatkun; Kai-Wei Chang",
        "authorids": "/s/satyapriya-krishna/; /r/rahul-gupta/; /a/apurv-verma/; /j/jwala-dhamala/; /y/yada-pruksachatkun/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{krishna-etal-2022-measuring,\n    title = \"Measuring Fairness of Text Classifiers via Prediction Sensitivity\",\n    author = \"Krishna, Satyapriya  and\n      Gupta, Rahul  and\n      Verma, Apurv  and\n      Dhamala, Jwala  and\n      Pruksachatkun, Yada  and\n      Chang, Kai-Wei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.401/\",\n    doi = \"10.18653/v1/2022.acl-long.401\",\n    pages = \"5830--5842\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.401.pdf",
        "site": "https://aclanthology.org/2022.acl-long.401/",
        "pdf_size": 453326,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2962151694321466075&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Harvard University; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; UCLA + Amazon Alexa",
        "aff_domain": "g.harvard.edu;amazon.com;amazon.com;gmail.com;amazon.com;cs.ucla.edu",
        "email": "g.harvard.edu;amazon.com;amazon.com;gmail.com;amazon.com;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;2+1",
        "aff_unique_norm": "Harvard University;Amazon;University of California, Los Angeles",
        "aff_unique_dep": ";Amazon Alexa AI;",
        "aff_unique_url": "https://www.harvard.edu;https://www.amazon.com;https://www.ucla.edu",
        "aff_unique_abbr": "Harvard;Amazon;UCLA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.184",
        "title": "Measuring and Mitigating Name Biases in Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural Machine Translation (NMT) systems exhibit problematic biases, such as stereotypical gender bias in the translation of occupation terms into languages with grammatical gender. In this paper we describe a new source of bias prevalent in NMT systems, relating to translations of sentences containing person names. To correctly translate such sentences, a NMT system needs to determine the gender of the name. We show that leading systems are particularly poor at this task, especially for female given names. This bias is deeper than given name gender: we show that the translation of terms with ambiguous sentiment can also be affected by person names, and the same holds true for proper nouns denoting race. To mitigate these biases we propose a simple but effective data augmentation method based on randomly switching entities during translation, which effectively eliminates the problem without any effect on translation quality.",
        "author": "Jun Wang; Benjamin Rubinstein; Trevor Cohn",
        "authorids": "/j/jun-wang/; /b/benjamin-rubinstein/; /t/trevor-cohn/",
        "bibtex": "@inproceedings{wang-etal-2022-measuring,\n    title = \"Measuring and Mitigating Name Biases in Neural Machine Translation\",\n    author = \"Wang, Jun  and\n      Rubinstein, Benjamin  and\n      Cohn, Trevor\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.184/\",\n    doi = \"10.18653/v1/2022.acl-long.184\",\n    pages = \"2576--2590\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.184.pdf",
        "site": "https://aclanthology.org/2022.acl-long.184/",
        "pdf_size": 500357,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2288152141758409767&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Melbourne, Australia; University of Melbourne, Australia; University of Melbourne, Australia",
        "aff_domain": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Melbourne",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2022.acl-long.362",
        "title": "Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There is a growing interest in the combined use of NLP and machine learning methods to predict gaze patterns during naturalistic reading. While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics. In this paper we report on experiments with two eye-tracking corpora of naturalistic reading and two language models (BERT and GPT-2). In all experiments, we test effects of a broad spectrum of features for predicting human reading behavior that fall into five categories (syntactic complexity, lexical richness, register-based multiword combinations, readability and psycholinguistic word properties). Our experiments show that both the features included and the architecture of the transformer-based language models play a role in predicting multiple eye-tracking measures during naturalistic reading. We also report the results of experiments aimed at determining the relative importance of features from different groups using SP-LIME.",
        "author": "Daniel Wiechmann; Yu Qiao; Elma Kerz; Justus Mattern",
        "authorids": "/d/daniel-wiechmann/; /y/yu-qiao/; /e/elma-kerz/; /j/justus-mattern/",
        "bibtex": "@inproceedings{wiechmann-kerz-2022-measuring,\n    title = \"Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns\",\n    author = \"Wiechmann, Daniel  and\n      Qiao, Yu  and\n      Kerz, Elma  and\n      Mattern, Justus\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.362/\",\n    doi = \"10.18653/v1/2022.acl-long.362\",\n    pages = \"5276--5290\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.362.pdf",
        "site": "https://aclanthology.org/2022.acl-long.362/",
        "pdf_size": 394295,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16759195349526389584&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 11,
        "aff": "University of Amsterdam; RWTH-Aachen University; RWTH-Aachen University; RWTH-Aachen University",
        "aff_domain": "uva.nl;rwth-aachen.de;ifaar.rwth-aachen.de;rwth-aachen.de",
        "email": "uva.nl;rwth-aachen.de;ifaar.rwth-aachen.de;rwth-aachen.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Amsterdam;RWTH Aachen University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uva.nl;https://www.rwth-aachen.de",
        "aff_unique_abbr": "UvA;RWTH",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Aachen",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Netherlands;Germany"
    },
    {
        "id": "2022.findings-acl.83",
        "title": "Measuring the Language of Self-Disclosure across Corpora",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Being able to reliably estimate self-disclosure \u2013 a key component of friendship and intimacy \u2013 from language is important for many psychology studies. We build single-task models on five self-disclosure corpora, but find that these models generalize poorly; the within-domain accuracy of predicted message-level self-disclosure of the best-performing model (mean Pearson\u2019s r=0.69) is much higher than the respective across data set accuracy (mean Pearson\u2019s r=0.32), due to both variations in the corpora (e.g., medical vs. general topics) and labeling instructions (target variables: self-disclosure, emotional disclosure, intimacy). However, some lexical features, such as expression of negative emotions and use of first person personal pronouns such as \u2018I\u2019 reliably predict self-disclosure across corpora. We develop a multi-task model that yields better results, with an average Pearson\u2019s r of 0.37 for out-of-corpora prediction.",
        "author": "Ann-Katrin Reuel; Sebastian Peralta; Jo\u00e3o Sedoc; Garrick Sherman; Lyle Ungar",
        "authorids": "/a/ann-katrin-reuel/; /s/sebastian-peralta/; /j/joao-sedoc/; /g/garrick-sherman/; /l/lyle-ungar/",
        "bibtex": "@inproceedings{reuel-etal-2022-measuring,\n    title = \"Measuring the Language of Self-Disclosure across Corpora\",\n    author = \"Reuel, Ann-Katrin  and\n      Peralta, Sebastian  and\n      Sedoc, Jo{\\~a}o  and\n      Sherman, Garrick  and\n      Ungar, Lyle\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.83/\",\n    doi = \"10.18653/v1/2022.findings-acl.83\",\n    pages = \"1035--1047\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.83.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.83/",
        "pdf_size": 533845,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9994892469980280612&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Pennsylvania, SEAS; University of Pennsylvania, SEAS; New York University Stern; University of Pennsylvania, SEAS; University of Pennsylvania, SEAS",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;stern.nyu.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;stern.nyu.edu;seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Pennsylvania;New York University",
        "aff_unique_dep": "School of Engineering and Applied Science;Stern School of Business",
        "aff_unique_url": "https://www.upenn.edu;https://www.stern.nyu.edu",
        "aff_unique_abbr": "UPenn;NYU Stern",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.450",
        "title": "MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum\u2019s awareness of extraction history.",
        "author": "Nianlong Gu; Elliott Ash; Richard Hahnloser",
        "authorids": "/n/nianlong-gu/; /e/elliott-ash/; /r/richard-hahnloser/",
        "bibtex": "@inproceedings{gu-etal-2022-memsum,\n    title = \"{M}em{S}um: Extractive Summarization of Long Documents Using Multi-Step Episodic {M}arkov Decision Processes\",\n    author = \"Gu, Nianlong  and\n      Ash, Elliott  and\n      Hahnloser, Richard\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.450/\",\n    doi = \"10.18653/v1/2022.acl-long.450\",\n    pages = \"6507--6522\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.450.pdf",
        "site": "https://aclanthology.org/2022.acl-long.450/",
        "pdf_size": 824707,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13955558557245021541&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Institute of Neuroinformatics, University of Zurich and ETH Zurich; Department of Humanities, Social and Political Sciences, ETH Zurich; Institute of Neuroinformatics, University of Zurich and ETH Zurich",
        "aff_domain": "ini.ethz.ch;ethz.ch;ini.ethz.ch",
        "email": "ini.ethz.ch;ethz.ch;ini.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Zurich;ETH Zurich",
        "aff_unique_dep": "Institute of Neuroinformatics;Department of Humanities, Social and Political Sciences",
        "aff_unique_url": "https://www.neuro.ethz.ch/;https://www.ethz.ch",
        "aff_unique_abbr": "UZH;ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2022.acl-long.521",
        "title": "Memorisation versus Generalisation in Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets. However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose an extension based on prototypical networks that improves performance in low-resource named entity recognition tasks.",
        "author": "Michael T\u00e4nzer; Sebastian Ruder; Marek Rei",
        "authorids": "/m/michael-tanzer/; /s/sebastian-ruder/; /m/marek-rei/",
        "bibtex": "@inproceedings{tanzer-etal-2022-memorisation,\n    title = \"Memorisation versus Generalisation in Pre-trained Language Models\",\n    author = {T{\\\"a}nzer, Michael  and\n      Ruder, Sebastian  and\n      Rei, Marek},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.521/\",\n    doi = \"10.18653/v1/2022.acl-long.521\",\n    pages = \"7564--7578\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.521.pdf",
        "site": "https://aclanthology.org/2022.acl-long.521/",
        "pdf_size": 1148303,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8502579702876031811&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Imperial College London; Google Research; Imperial College London",
        "aff_domain": "imperial.ac.uk;google.com;imperial.ac.uk",
        "email": "imperial.ac.uk;google.com;imperial.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Imperial College London;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.imperial.ac.uk;https://research.google",
        "aff_unique_abbr": "ICL;Google Research",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2022.acl-long.582",
        "title": "Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Meta-learning, or learning to learn, is a technique that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks. We apply model-agnostic meta-learning (MAML) to the task of cross-lingual dependency parsing. We train our model on a diverse set of languages to learn a parameter initialization that can adapt quickly to new languages. We find that meta-learning with pre-training can significantly improve upon the performance of language transfer and standard supervised learning baselines for a variety of unseen, typologically diverse, and low-resource languages, in a few-shot learning setup.",
        "author": "Anna Langedijk; Verna Dankers; Phillip Lippe; Sander Bos; Bryan Cardenas Guevara; Helen Yannakoudakis; Ekaterina Shutova",
        "authorids": "/a/anna-langedijk/; /v/verna-dankers/; /p/phillip-lippe/; /s/sander-bos/; /b/bryan-cardenas-guevara/; /h/helen-yannakoudakis/; /e/ekaterina-shutova/",
        "bibtex": "@inproceedings{langedijk-etal-2022-meta,\n    title = \"Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing\",\n    author = \"Langedijk, Anna  and\n      Dankers, Verna  and\n      Lippe, Phillip  and\n      Bos, Sander  and\n      Cardenas Guevara, Bryan  and\n      Yannakoudakis, Helen  and\n      Shutova, Ekaterina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.582/\",\n    doi = \"10.18653/v1/2022.acl-long.582\",\n    pages = \"8503--8520\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.582.pdf",
        "site": "https://aclanthology.org/2022.acl-long.582/",
        "pdf_size": 509466,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7125366974688262568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Institute for Logic, Language and Computation, University of Amsterdam; Institute for Logic, Language and Computation, University of Amsterdam+Institute of Language, Cognition and Computation, University of Edinburgh; Institute for Logic, Language and Computation, University of Amsterdam; Institute for Logic, Language and Computation, University of Amsterdam; Institute for Logic, Language and Computation, University of Amsterdam; Department of Informatics, King\u2019s College London; Institute for Logic, Language and Computation, University of Amsterdam",
        "aff_domain": "gmail.com; ; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0+1;0;0;0;2;0",
        "aff_unique_norm": "University of Amsterdam;University of Edinburgh;King\u2019s College London",
        "aff_unique_dep": "Institute for Logic, Language and Computation;Institute of Language, Cognition and Computation;Department of Informatics",
        "aff_unique_url": "https://www.uva.nl;https://www.ed.ac.uk;https://www.kcl.ac.uk",
        "aff_unique_abbr": "UvA;Edinburgh;KCL",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Edinburgh;London",
        "aff_country_unique_index": "0;0+1;0;0;0;1;0",
        "aff_country_unique": "Netherlands;United Kingdom"
    },
    {
        "id": "2022.findings-acl.24",
        "title": "Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-XNLG) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.",
        "author": "Kaushal Maurya; Maunendra Desarkar",
        "authorids": "/k/kaushal-maurya/; /m/maunendra-desarkar/",
        "bibtex": "@inproceedings{maurya-desarkar-2022-meta,\n    title = \"Meta-X$_{NLG}$: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation\",\n    author = \"Maurya, Kaushal  and\n      Desarkar, Maunendra\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.24/\",\n    doi = \"10.18653/v1/2022.findings-acl.24\",\n    pages = \"269--284\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.24.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.24/",
        "pdf_size": 675125,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2794684848376445815&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Indian Institute of Technology Hyderabad; Indian Institute of Technology Hyderabad",
        "aff_domain": "iith.ac.in;cse.iith.ac.in",
        "email": "iith.ac.in;cse.iith.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iith.ac.in",
        "aff_unique_abbr": "IIT Hyderabad",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.53",
        "title": "Meta-learning via Language Model In-context Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose in-context tuning (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x.",
        "author": "Yanda Chen; Ruiqi Zhong; Sheng Zha; George Karypis; He He",
        "authorids": "/y/yanda-chen/; /r/ruiqi-zhong/; /s/sheng-zha/; /g/george-karypis/; /h/he-he/",
        "bibtex": "@inproceedings{chen-etal-2022-meta,\n    title = \"Meta-learning via Language Model In-context Tuning\",\n    author = \"Chen, Yanda  and\n      Zhong, Ruiqi  and\n      Zha, Sheng  and\n      Karypis, George  and\n      He, He\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.53/\",\n    doi = \"10.18653/v1/2022.acl-long.53\",\n    pages = \"719--730\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.53.pdf",
        "site": "https://aclanthology.org/2022.acl-long.53/",
        "pdf_size": 1074713,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6086789713368518348&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Columbia University; University of California, Berkeley; AWS AI; AWS AI; New York University",
        "aff_domain": "columbia.edu;berkeley.edu;amazon.com;amazon.com;amazon.com",
        "email": "columbia.edu;berkeley.edu;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;2;3",
        "aff_unique_norm": "Columbia University;University of California, Berkeley;Amazon;New York University",
        "aff_unique_dep": ";;AWS AI;",
        "aff_unique_url": "https://www.columbia.edu;https://www.berkeley.edu;https://aws.amazon.com;https://www.nyu.edu",
        "aff_unique_abbr": "Columbia;UC Berkeley;AWS;NYU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.271",
        "title": "MetaWeighting: Learning to Weight Tasks in Multi-Task Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Task weighting, which assigns weights on the including tasks during training, significantly matters the performance of Multi-task Learning (MTL); thus, recently, there has been an explosive interest in it. However, existing task weighting methods assign weights only based on the training loss, while ignoring the gap between the training loss and generalization loss. It degenerates MTL\u2019s performance. To address this issue, the present paper proposes a novel task weighting algorithm, which automatically weights the tasks via a learning-to-learn paradigm, referred to as MetaWeighting. Extensive experiments are conducted to validate the superiority of our proposed method in multi-task text classification.",
        "author": "Yuren Mao; Zekai Wang; Weiwei Liu; Xuemin Lin; Pengtao Xie",
        "authorids": "/y/yuren-mao/; /z/zekai-wang/; /w/weiwei-liu/; /x/xuemin-lin/; /p/pengtao-xie/",
        "bibtex": "@inproceedings{mao-etal-2022-metaweighting,\n    title = \"{M}eta{W}eighting: Learning to Weight Tasks in Multi-Task Learning\",\n    author = \"Mao, Yuren  and\n      Wang, Zekai  and\n      Liu, Weiwei  and\n      Lin, Xuemin  and\n      Xie, Pengtao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.271/\",\n    doi = \"10.18653/v1/2022.findings-acl.271\",\n    pages = \"3436--3448\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.271.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.271/",
        "pdf_size": 1555311,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8123401210029106041&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Engineering, University of New South Wales; School of Computer Science, Wuhan University; School of Computer Science, Wuhan University; School of Computer Science and Engineering, University of New South Wales; Department of Electrical and Computer Engineering, University of California San Diego",
        "aff_domain": "unsw.edu.au;gmail.com;gmail.com;cse.unsw.edu.au;gmail.com",
        "email": "unsw.edu.au;gmail.com;gmail.com;cse.unsw.edu.au;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;2",
        "aff_unique_norm": "University of New South Wales;Wuhan University;University of California, San Diego",
        "aff_unique_dep": "School of Computer Science and Engineering;School of Computer Science;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.unsw.edu.au;http://www.whu.edu.cn;https://www.ucsd.edu",
        "aff_unique_abbr": "UNSW;WHU;UCSD",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Wuhan;San Diego",
        "aff_country_unique_index": "0;1;1;0;2",
        "aff_country_unique": "Australia;China;United States"
    },
    {
        "id": "2022.findings-acl.137",
        "title": "Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities. Since widely used systems such as search and personal-assistants must support the long tail of entities that users ask about, there has been significant effort towards enhancing these base LMs with factual knowledge. We observe proposed methods typically start with a base LM and data that has been annotated with entity metadata, then change the model, by modifying the architecture or introducing auxiliary loss terms to better capture entity knowledge. In this work, we question this typical process and ask to what extent can we match the quality of model modifications, with a simple alternative: using a base LM and only changing the data. We propose metadata shaping, a method which inserts substrings corresponding to the readily available entity metadata, e.g. types and descriptions, into examples at train and inference time based on mutual information. Despite its simplicity, metadata shaping is quite effective. On standard evaluation benchmarks for knowledge-enhanced LMs, the method exceeds the base-LM baseline by an average of 4.3 F1 points and achieves state-of-the-art results. We further show the gains are on average 4.4x larger for the slice of examples containing tail vs. popular entities.",
        "author": "Simran Arora; Sen Wu; Enci Liu; Christopher Re",
        "authorids": "/s/simran-arora/; /s/sen-wu/; /e/enci-liu/; /c/christopher-re/",
        "bibtex": "@inproceedings{arora-etal-2022-metadata,\n    title = \"Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models\",\n    author = \"Arora, Simran  and\n      Wu, Sen  and\n      Liu, Enci  and\n      Re, Christopher\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.137/\",\n    doi = \"10.18653/v1/2022.findings-acl.137\",\n    pages = \"1733--1745\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.137.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.137/",
        "pdf_size": 630579,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16524402918680211045&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.144",
        "title": "Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.",
        "author": "Ehsan Aghazadeh; Mohsen Fayyaz; Yadollah Yaghoobzadeh",
        "authorids": "/e/ehsan-aghazadeh/; /m/mohsen-fayyaz/; /y/yadollah-yaghoobzadeh/",
        "bibtex": "@inproceedings{aghazadeh-etal-2022-metaphors,\n    title = \"Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages\",\n    author = \"Aghazadeh, Ehsan  and\n      Fayyaz, Mohsen  and\n      Yaghoobzadeh, Yadollah\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.144/\",\n    doi = \"10.18653/v1/2022.acl-long.144\",\n    pages = \"2037--2050\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.144.pdf",
        "site": "https://aclanthology.org/2022.acl-long.144/",
        "pdf_size": 629775,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8364986332550099740&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Tehran, Iran",
        "aff_domain": "ut.ac.ir;ut.ac.ir;ut.ac.ir",
        "email": "ut.ac.ir;ut.ac.ir;ut.ac.ir",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tehran",
        "aff_unique_dep": "School of Electrical and Computer Engineering",
        "aff_unique_url": "https://en.ut.ac.ir",
        "aff_unique_abbr": "UT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tehran",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Iran"
    },
    {
        "id": "2022.acl-long.222",
        "title": "Misinfo Reaction Frames: Reasoning about Readers\u2019 Reactions to News Headlines",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer\u2019s intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends). Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news. We propose Misinfo Reaction Frames (MRF), a pragmatic formalism for modeling how readers might react to a news headline. In contrast to categorical schema, our free-text dimensions provide a more nuanced way of understanding intent beyond being benign or malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced dataset of reactions to over 25k news headlines focusing on global crises: the Covid-19 pandemic, climate change, and cancer. Empirical results confirm that it is indeed possible for neural models to predict the prominent patterns of readers\u2019 reactions to previously unseen news headlines. Additionally, our user study shows that displaying machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation. Our work demonstrates the feasibility and importance of pragmatic inferences on news headlines to help enhance AI-guided misinformation detection and mitigation.",
        "author": "Saadia Gabriel; Skyler Hallinan; Maarten Sap; Pemi Nguyen; Franziska Roesner; Eunsol Choi; Yejin Choi",
        "authorids": "/s/saadia-gabriel/; /s/skyler-hallinan/; /m/maarten-sap/; /p/pemi-nguyen/; /f/franziska-roesner/; /e/eunsol-choi/; /y/yejin-choi/",
        "bibtex": "@inproceedings{gabriel-etal-2022-misinfo,\n    title = \"Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines\",\n    author = \"Gabriel, Saadia  and\n      Hallinan, Skyler  and\n      Sap, Maarten  and\n      Nguyen, Pemi  and\n      Roesner, Franziska  and\n      Choi, Eunsol  and\n      Choi, Yejin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.222/\",\n    doi = \"10.18653/v1/2022.acl-long.222\",\n    pages = \"3108--3127\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.222.pdf",
        "site": "https://aclanthology.org/2022.acl-long.222/",
        "pdf_size": 475839,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17020416577915014867&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Allen Institute for Artificial Intelligence + Language Technologies Institute, Carnegie Mellon University; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Department of Computer Science, The University of Texas at Austin; Paul G. Allen School of Computer Science & Engineering, University of Washington + Allen Institute for Artificial Intelligence",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cmu.edu;cs.washington.edu;cs.washington.edu;utexas.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cmu.edu;cs.washington.edu;cs.washington.edu;utexas.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1+2;0;0;3;0+1",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence;Carnegie Mellon University;University of Texas at Austin",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;;Language Technologies Institute;Department of Computer Science",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org;https://www.cmu.edu;https://www.utexas.edu",
        "aff_unique_abbr": "UW;AI2;CMU;UT Austin",
        "aff_campus_unique_index": "0;0;2;0;0;3;0",
        "aff_campus_unique": "Seattle;;Pittsburgh;Austin",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.33",
        "title": "Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions. A belief state, one of the core pieces of information, refers to the subject and its specific content, and appears in the form of domain-slot-value. The trained model predicts \u201caccumulated\u201d belief states in every turn, and joint goal accuracy and slot accuracy are mainly used to evaluate the prediction; however, we specify that the current evaluation metrics have a critical limitation when evaluating belief states accumulated as the dialogue proceeds, especially in the most used MultiWOZ dataset. Additionally, we propose relative slot accuracy to complement existing metrics. Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialog. This study also encourages not solely the reporting of joint goal accuracy, but also various complementary metrics in DST tasks for the sake of a realistic evaluation.",
        "author": "Takyoung Kim; Hoonsang Yoon; Yukyung Lee; Pilsung Kang; Misuk Kim",
        "authorids": "/t/takyoung-kim/; /h/hoonsang-yoon/; /y/yukyung-lee/; /p/pilsung-kang/; /m/misuk-kim/",
        "bibtex": "@inproceedings{kim-etal-2022-mismatch,\n    title = \"Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking\",\n    author = \"Kim, Takyoung  and\n      Yoon, Hoonsang  and\n      Lee, Yukyung  and\n      Kang, Pilsung  and\n      Kim, Misuk\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.33/\",\n    doi = \"10.18653/v1/2022.acl-short.33\",\n    pages = \"297--309\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.33.pdf",
        "site": "https://aclanthology.org/2022.acl-short.33/",
        "pdf_size": 320958,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10751602595920049518&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff": "Korea University; Korea University; Korea University; Korea University; Sejong University",
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr;sejong.ac.kr",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr;sejong.ac.kr",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Korea University;Sejong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.korea.ac.kr;https://www.sejong.ac.kr",
        "aff_unique_abbr": "KU;Sejong",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.findings-acl.219",
        "title": "Mitigating Contradictions in Dialogue Based on Contrastive Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Chatbot models have achieved remarkable progress in recent years but tend to yield contradictory responses. In this paper, we exploit the advantage of contrastive learning technique to mitigate this issue. To endow the model with the ability of discriminating contradictory patterns, we minimize the similarity between the target response and contradiction related negative example. The negative example is generated with learnable latent noise, which receives contradiction related feedback from the pretrained critic. Experimental results show that our method helps to avoid contradictions in response generation while preserving response fluency, outperforming existing methods on both automatic and human evaluation.",
        "author": "Weizhao Li; Junsheng Kong; Ben Liao; Yi Cai",
        "authorids": "/w/weizhao-li/; /j/junsheng-kong/; /b/ben-liao/; /y/yi-cai/",
        "bibtex": "@inproceedings{li-etal-2022-mitigating,\n    title = \"Mitigating Contradictions in Dialogue Based on Contrastive Learning\",\n    author = \"Li, Weizhao  and\n      Kong, Junsheng  and\n      Liao, Ben  and\n      Cai, Yi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.219/\",\n    doi = \"10.18653/v1/2022.findings-acl.219\",\n    pages = \"2781--2788\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.219.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.219/",
        "pdf_size": 384491,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15079959335759648781&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Software Engineering, South China University of Technology+Key Laboratory of Big Data and Intelligent Robot (SCUT), MOE of China; School of Software Engineering, South China University of Technology+Key Laboratory of Big Data and Intelligent Robot (SCUT), MOE of China; Tencent Quantum Lab; School of Software Engineering, South China University of Technology+Key Laboratory of Big Data and Intelligent Robot (SCUT), MOE of China",
        "aff_domain": "mail.scut.edu.cn;mail.scut.edu.cn;tencent.com;scut.edu.cn",
        "email": "mail.scut.edu.cn;mail.scut.edu.cn;tencent.com;scut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;1;0+0",
        "aff_unique_norm": "South China University of Technology;Tencent",
        "aff_unique_dep": "School of Software Engineering;Quantum Lab",
        "aff_unique_url": "https://www.scut.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "SCUT;Tencent",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.55",
        "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model\u2019s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal\u2014modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT\u20132 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.",
        "author": "Umang Gupta; Jwala Dhamala; Varun Kumar; Apurv Verma; Yada Pruksachatkun; Satyapriya Krishna; Rahul Gupta; Kai-Wei Chang; Greg Ver Steeg; Aram Galstyan",
        "authorids": "/u/umang-gupta/; /j/jwala-dhamala/; /v/varun-kumar/; /a/apurv-verma/; /y/yada-pruksachatkun/; /s/satyapriya-krishna/; /r/rahul-gupta/; /k/kai-wei-chang/; /g/greg-ver-steeg/; /a/aram-galstyan/",
        "bibtex": "@inproceedings{gupta-etal-2022-mitigating,\n    title = \"Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal\",\n    author = \"Gupta, Umang  and\n      Dhamala, Jwala  and\n      Kumar, Varun  and\n      Verma, Apurv  and\n      Pruksachatkun, Yada  and\n      Krishna, Satyapriya  and\n      Gupta, Rahul  and\n      Chang, Kai-Wei  and\n      Ver Steeg, Greg  and\n      Galstyan, Aram\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.55/\",\n    doi = \"10.18653/v1/2022.findings-acl.55\",\n    pages = \"658--678\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.55.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.55/",
        "pdf_size": 451263,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14545264439387481682&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Information Sciences Institute, University of Southern California+Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Harvard University+Amazon Alexa; Amazon Alexa; University of California, Los Angeles+Amazon Alexa; Information Sciences Institute, University of Southern California+Amazon Alexa; Amazon Alexa",
        "aff_domain": "usc.edu; ; ; ; ; ;amazon.com; ; ; ",
        "email": "usc.edu; ; ; ; ; ;amazon.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;1;1;1;1;2+1;1;3+1;0+1;1",
        "aff_unique_norm": "University of Southern California;Amazon;Harvard University;University of California, Los Angeles",
        "aff_unique_dep": "Information Sciences Institute;Amazon Alexa;;",
        "aff_unique_url": "https://www.usc.edu;https://www.amazon.com/alexa;https://www.harvard.edu;https://www.ucla.edu",
        "aff_unique_abbr": "USC;Amazon Alexa;Harvard;UCLA",
        "aff_campus_unique_index": "0;;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0;0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.175",
        "title": "Mitigating the Inconsistency Between Word Saliency and Model Confidence with Pathological Contrastive Training",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Neural networks are widely used in various NLP tasks for their remarkable performance. However, the complexity makes them difficult to interpret, i.e., they are not guaranteed right for the right reason. Besides the complexity, we reveal that the model pathology - the inconsistency between word saliency and model confidence, further hurts the interpretability. We show that the pathological inconsistency is caused by the representation collapse issue, which means that the representation of the sentences with tokens in different saliency reduced is somehow collapsed, and thus the important words cannot be distinguished from unimportant words in terms of model confidence changing. In this paper, to mitigate the pathology and obtain more interpretable models, we propose Pathological Contrastive Training (PCT) framework, which adopts contrastive learning and saliency-based samples augmentation to calibrate the sentences representation. Combined with qualitative analysis, we also conduct extensive quantitative experiments and measure the interpretability with eight reasonable metrics. Experiments show that our method can mitigate the model pathology and generate more interpretable models while keeping the model performance. Ablation study also shows the effectiveness.",
        "author": "Pengwei Zhan; Yang Wu; Shaolei Zhou; Yunjian Zhang; Liming Wang",
        "authorids": "/p/pengwei-zhan/; /y/yang-wu/; /s/shaolei-zhou/; /y/yunjian-zhang/; /l/liming-wang/",
        "bibtex": "@inproceedings{zhan-etal-2022-mitigating,\n    title = \"Mitigating the Inconsistency Between Word Saliency and Model Confidence with Pathological Contrastive Training\",\n    author = \"Zhan, Pengwei  and\n      Wu, Yang  and\n      Zhou, Shaolei  and\n      Zhang, Yunjian  and\n      Wang, Liming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.175/\",\n    doi = \"10.18653/v1/2022.findings-acl.175\",\n    pages = \"2226--2244\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.175.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.175/",
        "pdf_size": 8055440,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3009666865947801877&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China + School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China + School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China + School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China + School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China + School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.31",
        "title": "Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",
        "author": "Fatemehsadat Mireshghallah; Kartik Goyal; Taylor Berg-Kirkpatrick",
        "authorids": "/f/fatemehsadat-mireshghallah/; /k/kartik-goyal/; /t/taylor-berg-kirkpatrick/",
        "bibtex": "@inproceedings{mireshghallah-etal-2022-mix,\n    title = \"Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models\",\n    author = \"Mireshghallah, Fatemehsadat  and\n      Goyal, Kartik  and\n      Berg-Kirkpatrick, Taylor\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.31/\",\n    doi = \"10.18653/v1/2022.acl-long.31\",\n    pages = \"401--415\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.31.pdf",
        "site": "https://aclanthology.org/2022.acl-long.31/",
        "pdf_size": 1175785,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8248841252347429619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of California San Diego; Toyota Technological Institute at Chicago (TTIC); University of California San Diego",
        "aff_domain": "ucsd.edu;ttic.edu;ucsd.edu",
        "email": "ucsd.edu;ttic.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://ucsd.edu;https://www.ttic.edu",
        "aff_unique_abbr": "UCSD;TTIC",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "San Diego;Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.71",
        "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.",
        "author": "Zhengyan Zhang; Yankai Lin; Zhiyuan Liu; Peng Li; Maosong Sun; Jie Zhou",
        "authorids": "/z/zhengyan-zhang/; /y/yankai-lin/; /z/zhiyuan-liu/; /p/peng-li/; /m/maosong-sun/; /j/jie-zhou/",
        "bibtex": "@inproceedings{zhang-etal-2022-moefication,\n    title = \"{M}o{E}fication: Transformer Feed-forward Layers are Mixtures of Experts\",\n    author = \"Zhang, Zhengyan  and\n      Lin, Yankai  and\n      Liu, Zhiyuan  and\n      Li, Peng  and\n      Sun, Maosong  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.71/\",\n    doi = \"10.18653/v1/2022.findings-acl.71\",\n    pages = \"877--890\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.71.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.71/",
        "pdf_size": 1400539,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6156740393572645160&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology; Pattern Recognition Center, WeChat AI, Tencent Inc; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence; Pattern Recognition Center, WeChat AI, Tencent Inc+Institute for AI Industry Research (AIR), Tsinghua University, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Arti\ufb01cial Intelligence+Jiangsu Collaborative Innovation Center for Language Ability, Xuzhou, China; Pattern Recognition Center, WeChat AI, Tencent Inc",
        "aff_domain": "mails.tsinghua.edu.cn; ;tsinghua.edu.cn; ;tsinghua.edu.cn; ",
        "email": "mails.tsinghua.edu.cn; ;tsinghua.edu.cn; ;tsinghua.edu.cn; ",
        "github": "https://github.com/thunlp/MoEfication",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;0+0+3;2+0;0+0+3+4;2",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Tencent;Beijing Academy of Artificial Intelligence;Jiangsu Collaborative Innovation Center for Language Ability",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;Pattern Recognition Center, WeChat AI;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.tencent.com;https://www.baaic.cn;",
        "aff_unique_abbr": "THU;;Tencent;BAAI;",
        "aff_campus_unique_index": "0;0+2;;0+2+3",
        "aff_campus_unique": "Beijing;;Shanghai;Xuzhou",
        "aff_country_unique_index": "0+0;0;0+0+0;0+0;0+0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.143",
        "title": "Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In multimodal machine learning, additive late-fusion is a straightforward approach to combine the feature representations from different modalities, in which the final prediction can be formulated as the sum of unimodal predictions. While it has been found that certain late-fusion models can achieve competitive performance with lower computational costs compared to complex multimodal interactive models, how to effectively search for a good late-fusion model is still an open question. Moreover, for different modalities, the best unimodal models may work under significantly different learning rates due to the nature of the modality and the computational flow of the model; thus, selecting a global learning rate for late-fusion models can result in a vanishing gradient for some modalities. To help address these issues, we propose a Modality-Specific Learning Rate (MSLR) method to effectively build late-fusion multimodal models from fine-tuned unimodal models. We investigate three different strategies to assign learning rates to different modalities. Our experiments show that MSLR outperforms global learning rates on multiple tasks and settings, and enables the models to effectively learn each modality.",
        "author": "Yiqun Yao; Rada Mihalcea",
        "authorids": "/y/yiqun-yao/; /r/rada-mihalcea/",
        "bibtex": "@inproceedings{yao-mihalcea-2022-modality,\n    title = \"Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion\",\n    author = \"Yao, Yiqun  and\n      Mihalcea, Rada\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.143/\",\n    doi = \"10.18653/v1/2022.findings-acl.143\",\n    pages = \"1824--1834\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.143.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.143/",
        "pdf_size": 346705,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1237961138660480183&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.176",
        "title": "Modeling Dual Read/Write Paths for Simultaneous Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Simultaneous machine translation (SiMT) outputs translation while reading source sentence and hence requires a policy to decide whether to wait for the next source word (READ) or generate a target word (WRITE), the actions of which form a read/write path. Although the read/write path is essential to SiMT performance, no direct supervision is given to the path in the existing methods. In this paper, we propose a method of dual-path SiMT which introduces duality constraints to direct the read/write path. According to duality constraints, the read/write path in source-to-target and target-to-source SiMT models can be mapped to each other. As a result, the two SiMT models can be optimized jointly by forcing their read/write paths to satisfy the mapping. Experiments on En-Vi and De-En tasks show that our method can outperform strong baselines under all latency.",
        "author": "Shaolei Zhang; Yang Feng",
        "authorids": "/s/shaolei-zhang/; /y/yang-feng/",
        "bibtex": "@inproceedings{zhang-feng-2022-modeling,\n    title = \"Modeling Dual Read/Write Paths for Simultaneous Machine Translation\",\n    author = \"Zhang, Shaolei  and\n      Feng, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.176/\",\n    doi = \"10.18653/v1/2022.acl-long.176\",\n    pages = \"2461--2477\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.176.pdf",
        "site": "https://aclanthology.org/2022.acl-long.176/",
        "pdf_size": 3553549,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2394983819871995916&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/ictnlp/Dual-Path",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.37",
        "title": "Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automatic code summarization, which aims to describe the source code in natural language, has become an essential task in software maintenance. Our fellow researchers have attempted to achieve such a purpose through various machine learning-based approaches. One key challenge keeping these approaches from being practical lies in the lacking of retaining the semantic structure of source code, which has unfortunately been overlooked by the state-of-the-art. Existing approaches resort to representing the syntax structure of code by modeling the Abstract Syntax Trees (ASTs). However, the hierarchical structures of ASTs have not been well explored. In this paper, we propose CODESCRIBE to model the hierarchical syntax structure of code by introducing a novel triplet position for code summarization. Specifically, CODESCRIBE leverages the graph neural network and Transformer to preserve the structural and sequential information of code, respectively. In addition, we propose a pointer-generator network that pays attention to both the structure and sequential tokens of code for a better summary generation. Experiments on two real-world datasets in Java and Python demonstrate the effectiveness of our proposed approach when compared with several state-of-the-art baselines.",
        "author": "Juncai Guo; Jin Liu; Yao Wan; Li Li; Pingyi Zhou",
        "authorids": "/j/juncai-guo/; /j/jin-liu/; /y/yao-wan/; /l/li-li/; /p/pingyi-zhou/",
        "bibtex": "@inproceedings{guo-etal-2022-modeling,\n    title = \"Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization\",\n    author = \"Guo, Juncai  and\n      Liu, Jin  and\n      Wan, Yao  and\n      Li, Li  and\n      Zhou, Pingyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.37/\",\n    doi = \"10.18653/v1/2022.acl-long.37\",\n    pages = \"486--500\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.37.pdf",
        "site": "https://aclanthology.org/2022.acl-long.37/",
        "pdf_size": 454726,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16507498370807847046&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Wuhan University, China; School of Computer Science, Wuhan University, China; School of Computer Sci. and Tech., Huazhong University of Science and Technology, China; Faculty of Information Technology, Monash University, Australia; Noah\u2019s Ark Lab, Huawei, China",
        "aff_domain": "whu.edu.cn;whu.edu.cn;hust.edu.cn;monash.edu;huawei.com",
        "email": "whu.edu.cn;whu.edu.cn;hust.edu.cn;monash.edu;huawei.com",
        "github": "https://github.com/GJCEXP/CODESCRIBE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;3",
        "aff_unique_norm": "Wuhan University;Huazhong University of Science and Technology;Monash University;Huawei",
        "aff_unique_dep": "School of Computer Science;School of Computer Sci. and Tech.;Faculty of Information Technology;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.whu.edu.cn;http://www.hust.edu.cn;https://www.monash.edu;https://www.huawei.com",
        "aff_unique_abbr": "WHU;HUST;Monash;Huawei",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Wuhan;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.findings-acl.228",
        "title": "Modeling Intensification for Sign Language Generation: A Computational Approach",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.",
        "author": "Mert Inan; Yang Zhong; Sabit Hassan; Lorna Quandt; Malihe Alikhani",
        "authorids": "/m/mert-inan/; /y/yang-zhong/; /s/sabit-hassan/; /l/lorna-quandt/; /m/malihe-alikhani/",
        "bibtex": "@inproceedings{inan-etal-2022-modeling,\n    title = \"Modeling Intensification for Sign Language Generation: A Computational Approach\",\n    author = \"Inan, Mert  and\n      Zhong, Yang  and\n      Hassan, Sabit  and\n      Quandt, Lorna  and\n      Alikhani, Malihe\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.228/\",\n    doi = \"10.18653/v1/2022.findings-acl.228\",\n    pages = \"2897--2911\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.228.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.228/",
        "pdf_size": 2637311,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9167300310385886776&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, School of Computing and Information, University of Pittsburgh, Pittsburgh, USA; Computer Science Department, School of Computing and Information, University of Pittsburgh, Pittsburgh, USA; Computer Science Department, School of Computing and Information, University of Pittsburgh, Pittsburgh, USA; Educational Neuroscience Program, Gallaudet University, Washington, D.C, USA; Computer Science Department, School of Computing and Information, University of Pittsburgh, Pittsburgh, USA",
        "aff_domain": "pitt.edu;pitt.edu;pitt.edu;gallaudet.edu;pitt.edu",
        "email": "pitt.edu;pitt.edu;pitt.edu;gallaudet.edu;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Pittsburgh;Gallaudet University",
        "aff_unique_dep": "Computer Science Department;Educational Neuroscience Program",
        "aff_unique_url": "https://www.pitt.edu;https://www.gallaudet.edu",
        "aff_unique_abbr": "Pitt;Gallaudet",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Pittsburgh;Washington, D.C",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.69",
        "title": "Modeling Multi-hop Question Answering as Single Sequence Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fusion-in-decoder (Fid) (Izacard and Grave, 2020) is a generative question answering (QA) model that leverages passage retrieval with a pre-trained transformer and pushed the state of the art on single-hop QA. However, the complexity of multi-hop QA hinders the effectiveness of the generative QA approach. In this work, we propose a simple generative approach (PathFid) that extends the task beyond just answer generation by explicitly modeling the reasoning process to resolve the answer for multi-hop questions. By linearizing the hierarchical reasoning path of supporting passages, their key sentences, and finally the factoid answer, we cast the problem as a single sequence prediction task. To facilitate complex reasoning with multiple clues, we further extend the unified flat representation of multiple input documents by encoding cross-passage interactions. Our extensive experiments demonstrate that PathFid leads to strong performance gains on two multi-hop QA datasets: HotpotQA and IIRC. Besides the performance gains, PathFid is more interpretable, which in turn yields answers that are more faithfully grounded to the supporting passages and facts compared to the baseline Fid model.",
        "author": "Semih Yavuz; Kazuma Hashimoto; Yingbo Zhou; Nitish Shirish Keskar; Caiming Xiong",
        "authorids": "/s/semih-yavuz/; /k/kazuma-hashimoto/; /y/yingbo-zhou/; /n/nitish-shirish-keskar/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{yavuz-etal-2022-modeling,\n    title = \"Modeling Multi-hop Question Answering as Single Sequence Prediction\",\n    author = \"Yavuz, Semih  and\n      Hashimoto, Kazuma  and\n      Zhou, Yingbo  and\n      Keskar, Nitish Shirish  and\n      Xiong, Caiming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.69/\",\n    doi = \"10.18653/v1/2022.acl-long.69\",\n    pages = \"974--990\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.69.pdf",
        "site": "https://aclanthology.org/2022.acl-long.69/",
        "pdf_size": 730599,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14953679640067853873&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Salesforce Research; Salesforce Research; Salesforce Research; Salesforce Research; Salesforce Research",
        "aff_domain": "salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "email": "salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Salesforce",
        "aff_unique_dep": "Salesforce Research",
        "aff_unique_url": "https://research.salesforce.com",
        "aff_unique_abbr": "Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.599",
        "title": "Modeling Persuasive Discourse to Adaptively Support Students\u2019 Argumentative Writing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce an argumentation annotation approach to model the structure of argumentative discourse in student-written business model pitches. Additionally, the annotation scheme captures a series of persuasiveness scores such as the specificity, strength, evidence, and relevance of the pitch and the individual components. Based on this scheme, we annotated a corpus of 200 business model pitches in German. Moreover, we trained predictive models to detect argumentative discourse structures and embedded them in an adaptive writing support system for students that provides them with individual argumentation feedback independent of an instructor, time, and location. We evaluated our tool in a real-world writing exercise and found promising results for the measured self-efficacy and perceived ease-of-use. Finally, we present our freely available corpus of persuasive business model pitches with 3,207 annotated sentences in German language and our annotation guidelines.",
        "author": "Thiemo Wambsganss; Christina Niklaus",
        "authorids": "/t/thiemo-wambsganss/; /c/christina-niklaus/",
        "bibtex": "@inproceedings{wambsganss-niklaus-2022-modeling,\n    title = \"Modeling Persuasive Discourse to Adaptively Support Students' Argumentative Writing\",\n    author = \"Wambsganss, Thiemo  and\n      Niklaus, Christina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.599/\",\n    doi = \"10.18653/v1/2022.acl-long.599\",\n    pages = \"8748--8760\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.599.pdf",
        "site": "https://aclanthology.org/2022.acl-long.599/",
        "pdf_size": 819937,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13743736005951651161&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 4,
        "aff": "University of St.Gallen/ CH+Carnegie Mellon University/ US; University of St.Gallen/ CH",
        "aff_domain": "unisg.ch;unisg.ch",
        "email": "unisg.ch;unisg.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "University of St.Gallen;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unisg.ch;https://www.cmu.edu",
        "aff_unique_abbr": "HSG;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "2022.acl-long.548",
        "title": "Modeling Syntactic-Semantic Dependency Correlations in Semantic Role Labeling Using Mixture Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we propose a mixture model-based end-to-end method to model the syntactic-semantic dependency correlation in Semantic Role Labeling (SRL). Semantic dependencies in SRL are modeled as a distribution over semantic dependency labels conditioned on a predicate and an argument word. The semantic label distribution varies depending on Shortest Syntactic Dependency Path (SSDP) hop patterns. We target the variation of semantic label distributions using a mixture model, separately estimating semantic label distributions for different hop patterns and probabilistically clustering hop patterns with similar semantic label distributions. Experiments show that the proposed method successfully learns a cluster assignment reflecting the variation of semantic label distributions. Modeling the variation improves performance in predicting short distance semantic dependencies, in addition to the improvement on long distance semantic dependencies that previous syntax-aware methods have achieved. The proposed method achieves a small but statistically significant improvement over baseline methods in English, German, and Spanish and obtains competitive performance with state-of-the-art methods in English.",
        "author": "Junjie Chen; Xiangheng He; Yusuke Miyao",
        "authorids": "/j/junjie-chen/; /x/xiangheng-he/; /y/yusuke-miyao/",
        "bibtex": "@inproceedings{chen-etal-2022-modeling,\n    title = \"Modeling Syntactic-Semantic Dependency Correlations in Semantic Role Labeling Using Mixture Models\",\n    author = \"Chen, Junjie  and\n      He, Xiangheng  and\n      Miyao, Yusuke\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.548/\",\n    doi = \"10.18653/v1/2022.acl-long.548\",\n    pages = \"7959--7969\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.548.pdf",
        "site": "https://aclanthology.org/2022.acl-long.548/",
        "pdf_size": 667618,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11684849825090324730&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, The University of Tokyo, Tokyo*; GLAM \u2013 Group on Language, Audio, & Music, Imperial College London, UK**; Department of Computer Science, The University of Tokyo, Tokyo*",
        "aff_domain": "is.s.u-tokyo.ac.jp;is.s.u-tokyo.ac.jp;imperial.ac.uk",
        "email": "is.s.u-tokyo.ac.jp;is.s.u-tokyo.ac.jp;imperial.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Tokyo;Imperial College London",
        "aff_unique_dep": "Department of Computer Science;Group on Language, Audio, & Music",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.imperial.ac.uk",
        "aff_unique_abbr": "UTokyo;ICL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Japan;United Kingdom"
    },
    {
        "id": "2022.acl-long.84",
        "title": "Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Procedural Multimodal Documents (PMDs) organize textual instructions and corresponding images step by step. Comprehending PMDs and inducing their representations for the downstream reasoning tasks is designated as Procedural MultiModal Machine Comprehension (M3C). In this study, we approach Procedural M3C at a fine-grained level (compared with existing explorations at a document or sentence level), that is, entity. With delicate consideration, we model entity both in its temporal and cross-modal relation and propose a novel Temporal-Modal Entity Graph (TMEG). Specifically, graph structure is formulated to capture textual and visual entities and trace their temporal-modal evolution. In addition, a graph aggregation module is introduced to conduct graph encoding and reasoning. Comprehensive experiments across three Procedural M3C tasks are conducted on a traditional dataset RecipeQA and our new dataset CraftQA, which can better evaluate the generalization of TMEG.",
        "author": "Huibin Zhang; Zhengkun Zhang; Yao Zhang; Jun Wang; Yufan Li; Ning Jiang; Xin Wei; Zhenglu Yang",
        "authorids": "/h/huibin-zhang/; /z/zhengkun-zhang/; /y/yao-zhang/; /j/jun-wang/; /y/yufan-li/; /n/ning-jiang/; /x/xin-wei/; /z/zhenglu-yang/",
        "bibtex": "@inproceedings{zhang-etal-2022-modeling,\n    title = \"Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension\",\n    author = \"Zhang, Huibin  and\n      Zhang, Zhengkun  and\n      Zhang, Yao  and\n      Wang, Jun  and\n      Li, Yufan  and\n      Jiang, Ning  and\n      Wei, Xin  and\n      Yang, Zhenglu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.84/\",\n    doi = \"10.18653/v1/2022.acl-long.84\",\n    pages = \"1179--1189\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.84.pdf",
        "site": "https://aclanthology.org/2022.acl-long.84/",
        "pdf_size": 3787860,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2657072240298159995&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "TKLNDST, CS, Nankai University, China+Ludong University, China; TKLNDST, CS, Nankai University, China+Ludong University, China; TKLNDST, CS, Nankai University, China; Ludong University, China; TKLNDST, CS, Nankai University, China; Mashang Consumer Finance Co., Ltd., China; Mashang Consumer Finance Co., Ltd., China; TKLNDST, CS, Nankai University, China+Ludong University, China",
        "aff_domain": "mail.nankai.edu.cn;mail.nankai.edu.cn;mail.nankai.edu.cn;mail.nankai.edu.cn;mail.nankai.edu.cn;msxf.com;msxf.com;nankai.edu.cn",
        "email": "mail.nankai.edu.cn;mail.nankai.edu.cn;mail.nankai.edu.cn;mail.nankai.edu.cn;mail.nankai.edu.cn;msxf.com;msxf.com;nankai.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;0;1;0;2;2;0+1",
        "aff_unique_norm": "Nankai University;Ludong University;Mashang Consumer Finance Co., Ltd.",
        "aff_unique_dep": "Computer Science;;",
        "aff_unique_url": ";http://www.ldu.edu.cn;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.22",
        "title": "Modeling U.S. State-Level Policies by Extracting Winners and Losers from Legislative Texts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Decisions on state-level policies have a deep effect on many aspects of our everyday life, such as health-care and education access. However, there is little understanding of how these policies and decisions are being formed in the legislative process. We take a data-driven approach by decoding the impact of legislation on relevant stakeholders (e.g., teachers in education bills) to understand legislators\u2019 decision-making process and votes. We build a new dataset for multiple US states that interconnects multiple sources of data including bills, stakeholders, legislators, and money donors. Next, we develop a textual graph-based model to embed and analyze state bills. Our model predicts winners/losers of bills and then utilizes them to better determine the legislative body\u2019s vote breakdown according to demographic/ideological criteria, e.g., gender.",
        "author": "Maryam Davoodi; Eric Waltenburg; Dan Goldwasser",
        "authorids": "/m/maryam-davoodi/; /e/eric-waltenburg/; /d/dan-goldwasser/",
        "bibtex": "@inproceedings{davoodi-etal-2022-modeling,\n    title = \"{M}odeling {U.S.} State-Level Policies by Extracting Winners and Losers from Legislative Texts\",\n    author = \"Davoodi, Maryam  and\n      Waltenburg, Eric  and\n      Goldwasser, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.22/\",\n    doi = \"10.18653/v1/2022.acl-long.22\",\n    pages = \"270--284\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.22.pdf",
        "site": "https://aclanthology.org/2022.acl-long.22/",
        "pdf_size": 620201,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17146727918349630060&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 2,
        "aff": "Purdue University; Purdue University; Purdue University",
        "aff_domain": "purdue.edu;purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.288",
        "title": "Modular Domain Adaptation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Off-the-shelf models are widely used by computational social science researchers to measure properties of text, such as sentiment. However, without access to source data it is difficult to account for domain shift, which represents a threat to validity. Here, we treat domain adaptation as a modular process that involves separate model producers and model consumers, and show how they can independently cooperate to facilitate more accurate measurements of text. We introduce two lightweight techniques for this scenario, and demonstrate that they reliably increase out-of-domain accuracy on four multi-domain text classification datasets when used with linear and contextual embedding models. We conclude with recommendations for model producers and consumers, and release models and replication code to accompany this paper.",
        "author": "Junshen Chen; Dallas Card; Dan Jurafsky",
        "authorids": "/j/junshen-chen/; /d/dallas-card/; /d/dan-jurafsky/",
        "bibtex": "@inproceedings{chen-etal-2022-modular,\n    title = \"Modular Domain Adaptation\",\n    author = \"Chen, Junshen  and\n      Card, Dallas  and\n      Jurafsky, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.288/\",\n    doi = \"10.18653/v1/2022.findings-acl.288\",\n    pages = \"3633--3655\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.288.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.288/",
        "pdf_size": 3879838,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13722654122961120770&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University; University of Michigan; Stanford University",
        "aff_domain": "gmail.com;umich.edu;stanford.edu",
        "email": "gmail.com;umich.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.umich.edu",
        "aff_unique_abbr": "Stanford;UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.234",
        "title": "Modular and Parameter-Efficient Multimodal Fusion with Prompting",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent research has made impressive progress in large-scale multimodal pre-training. In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. In this paper, we propose to use prompt vectors to align the modalities. Our method achieves comparable performance to several other multimodal fusion methods in low-resource settings. We further show that our method is modular and parameter-efficient for processing tasks involving two or more data modalities.",
        "author": "Sheng Liang; Mengjie Zhao; Hinrich Schuetze",
        "authorids": "/s/sheng-liang/; /m/mengjie-zhao/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{liang-etal-2022-modular,\n    title = \"Modular and Parameter-Efficient Multimodal Fusion with Prompting\",\n    author = \"Liang, Sheng  and\n      Zhao, Mengjie  and\n      Schuetze, Hinrich\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.234/\",\n    doi = \"10.18653/v1/2022.findings-acl.234\",\n    pages = \"2976--2985\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.234.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.234/",
        "pdf_size": 858582,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10471618851982461028&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Center for Information and Language Processing (CIS) LMU Munich, Germany; Center for Information and Language Processing (CIS) LMU Munich, Germany; Center for Information and Language Processing (CIS) LMU Munich, Germany",
        "aff_domain": "cis.lmu.de;cis.lmu.de; ",
        "email": "cis.lmu.de;cis.lmu.de; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "LMU Munich",
        "aff_unique_dep": "Center for Information and Language Processing (CIS)",
        "aff_unique_url": "https://www.lmu.de",
        "aff_unique_abbr": "LMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.findings-acl.212",
        "title": "More Than Words: Collocation Retokenization for Latent Dirichlet Allocation Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of retokenization based on chi-squared measures, t-statistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.",
        "author": "Jin Cheevaprawatdomrong; Alexandra Schofield; Attapol Rutherford",
        "authorids": "/j/jin-cheevaprawatdomrong/; /a/alexandra-schofield/; /a/attapol-rutherford/",
        "bibtex": "@inproceedings{cheevaprawatdomrong-etal-2022-words,\n    title = \"More Than Words: Collocation Retokenization for {L}atent {D}irichlet {A}llocation Models\",\n    author = \"Cheevaprawatdomrong, Jin  and\n      Schofield, Alexandra  and\n      Rutherford, Attapol\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.212/\",\n    doi = \"10.18653/v1/2022.findings-acl.212\",\n    pages = \"2696--2704\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.212.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.212/",
        "pdf_size": 993418,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13442912503454209615&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 4,
        "aff": "Chulalongkorn University; Harvey Mudd College; Chulalongkorn University",
        "aff_domain": "gmail.com;cs.hmc.edu;chula.ac.th",
        "email": "gmail.com;cs.hmc.edu;chula.ac.th",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Chulalongkorn University;Harvey Mudd College",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.chula.ac.th;https://www.hmc.edu",
        "aff_unique_abbr": "CU;HMC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Thailand;United States"
    },
    {
        "id": "2022.findings-acl.80",
        "title": "Morphological Processing of Low-Resource Languages: Where We Are and What\u2019s Next",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Automatic morphological processing can aid downstream natural language processing applications, especially for low-resource languages, and assist language documentation efforts for endangered languages. Having long been multilingual, the field of computational morphology is increasingly moving towards approaches suitable for languages with minimal or no annotated resources. First, we survey recent developments in computational morphology with a focus on low-resource languages. Second, we argue that the field is ready to tackle the logical next challenge: understanding a language\u2019s morphology from raw text alone. We perform an empirical study on a truly unsupervised version of the paradigm completion task and show that, while existing state-of-the-art models bridged by two newly proposed models we devise perform reasonably, there is still much room for improvement. The stakes are high: solving this task will increase the language coverage of morphological resources by a number of magnitudes.",
        "author": "Adam Wiemerslage; Miikka Silfverberg; Changbing Yang; Arya McCarthy; Garrett Nicolai; Eliana Colunga; Katharina Kann",
        "authorids": "/a/adam-wiemerslage/; /m/miikka-silfverberg/; /c/changbing-yang/; /a/arya-d-mccarthy/; /g/garrett-nicolai/; /e/eliana-colunga/; /k/katharina-von-der-wense/",
        "bibtex": "@inproceedings{wiemerslage-etal-2022-morphological,\n    title = \"Morphological Processing of Low-Resource Languages: Where We Are and What`s Next\",\n    author = \"Wiemerslage, Adam  and\n      Silfverberg, Miikka  and\n      Yang, Changbing  and\n      McCarthy, Arya  and\n      Nicolai, Garrett  and\n      Colunga, Eliana  and\n      Kann, Katharina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.80/\",\n    doi = \"10.18653/v1/2022.findings-acl.80\",\n    pages = \"988--1007\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.80.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.80/",
        "pdf_size": 1010840,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5975716518685246022&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Colorado Boulder\u266f; University of British Columbia\u266d; University of British Columbia\u266d; Johns Hopkins University\u266e; University of British Columbia\u266d; University of Colorado Boulder\u266f; University of Colorado Boulder\u266f",
        "aff_domain": "; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;2;1;0;0",
        "aff_unique_norm": "University of Colorado Boulder;University of British Columbia;Johns Hopkins University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.colorado.edu;https://www.ubc.ca;https://www.jhu.edu",
        "aff_unique_abbr": "CU Boulder;UBC;JHU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Boulder;",
        "aff_country_unique_index": "0;1;1;0;1;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2022.acl-short.21",
        "title": "Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In recent years, a flurry of morphological datasets had emerged, most notably UniMorph, aa multi-lingual repository of inflection tables. However, the flat structure of the current morphological annotation makes the treatment of some languages quirky, if not impossible, specifically in cases of polypersonal agreement. In this paper we propose a general solution for such cases and expand the UniMorph annotation schema to naturally address this phenomenon, in which verbs agree with multiple arguments using true affixes. We apply this extended schema to one such language, Georgian, and provide a human-verified, accurate and balanced morphological dataset for Georgian verbs. The dataset has 4 times more tables and 6 times more verb forms compared to the existing UniMorph dataset, covering all possible variants of argument marking, demonstrating the adequacy of our proposed scheme. Experiments on a reinflection task show that generalization is easy when the data is split at the form level, but extremely hard when splitting along lemma lines. Expanding the other languages in UniMorph according to this schema is expected to improve both the coverage, consistency and interpretability of this benchmark.",
        "author": "David Guriel; Omer Goldman; Reut Tsarfaty",
        "authorids": "/d/david-guriel/; /o/omer-goldman/; /r/reut-tsarfaty/",
        "bibtex": "@inproceedings{guriel-etal-2022-morphological,\n    title = \"Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a {G}eorgian Case Study\",\n    author = \"Guriel, David  and\n      Goldman, Omer  and\n      Tsarfaty, Reut\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.21/\",\n    doi = \"10.18653/v1/2022.acl-short.21\",\n    pages = \"196--202\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.21.pdf",
        "site": "https://aclanthology.org/2022.acl-short.21/",
        "pdf_size": 436006,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9809945880371496978&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Bar-Ilan University; Bar-Ilan University; Bar-Ilan University",
        "aff_domain": "gmail.com;gmail.com;biu.ac.il",
        "email": "gmail.com;gmail.com;biu.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Bar-Ilan University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.biu.ac.il",
        "aff_unique_abbr": "BIU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.findings-acl.135",
        "title": "Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6% absolute improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8% in Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario. Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect. Additionally, we show that high-quality morphological analyzers as external linguistic resources are beneficial especially in low-resource settings.",
        "author": "Go Inoue; Salam Khalifa; Nizar Habash",
        "authorids": "/g/go-inoue/; /s/salam-khalifa/; /n/nizar-habash/",
        "bibtex": "@inproceedings{inoue-etal-2022-morphosyntactic,\n    title = \"Morphosyntactic Tagging with Pre-trained Language Models for {A}rabic and its Dialects\",\n    author = \"Inoue, Go  and\n      Khalifa, Salam  and\n      Habash, Nizar\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.135/\",\n    doi = \"10.18653/v1/2022.findings-acl.135\",\n    pages = \"1708--1719\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.135.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.135/",
        "pdf_size": 627272,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2742689019907673433&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 6,
        "aff": "Computational Approaches to Modeling Language (CAMeL) Lab, New York University Abu Dhabi; Computational Approaches to Modeling Language (CAMeL) Lab, New York University Abu Dhabi + Stony Brook University; Computational Approaches to Modeling Language (CAMeL) Lab, New York University Abu Dhabi",
        "aff_domain": "nyu.edu;stonybrook.edu;nyu.edu",
        "email": "nyu.edu;stonybrook.edu;nyu.edu",
        "github": "https://github.com/CAMeL-Lab/CAMeLBERT_morphosyntactic_tagger",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "New York University Abu Dhabi;Stony Brook University",
        "aff_unique_dep": "Computational Approaches to Modeling Language (CAMeL) Lab;",
        "aff_unique_url": "https://nyu.edu;https://www.stonybrook.edu",
        "aff_unique_abbr": "NYU Abu Dhabi;SBU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Abu Dhabi;",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "United Arab Emirates;United States"
    },
    {
        "id": "2022.findings-acl.69",
        "title": "Mukayese: Turkish NLP Strikes Back",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Having sufficient resources for language X lifts it from the under-resourced languages class, but not necessarily from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. We demonstrate that languages such as Turkish are left behind the state-of-the-art in NLP applications. As a solution, we present Mukayese, a set of NLP benchmarks for the Turkish language that contains several NLP tasks. We work on one or more datasets for each benchmark and present two or more baselines. Moreover, we present four new benchmarking datasets in Turkish for language modeling, sentence segmentation, and spell checking. All datasets and baselines are available under: https://github.com/alisafaya/mukayese",
        "author": "Ali Safaya; Emirhan Kurtulu\u015f; Arda Goktogan; Deniz Yuret",
        "authorids": "/a/ali-safaya/; /e/emirhan-kurtulus/; /a/arda-goktogan/; /d/deniz-yuret/",
        "bibtex": "@inproceedings{safaya-etal-2022-mukayese,\n    title = \"Mukayese: {T}urkish {NLP} Strikes Back\",\n    author = \"Safaya, Ali  and\n      Kurtulu{\\c{s}}, Emirhan  and\n      Goktogan, Arda  and\n      Yuret, Deniz\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.69/\",\n    doi = \"10.18653/v1/2022.findings-acl.69\",\n    pages = \"846--863\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.69.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.69/",
        "pdf_size": 431721,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4531175762894471621&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "KUIS AI Center, Ko\u00e7 University\u2020Computer Engineering Department, Ko\u00e7 University; Ca\u011falo\u011flu Anadolu Lisesi, Istanbul; Computer Engineering Department, Bilkent University; KUIS AI Center, Ko\u00e7 University\u2020Computer Engineering Department, Ko\u00e7 University",
        "aff_domain": "ku.edu.tr; ; ; ",
        "email": "ku.edu.tr; ; ; ",
        "github": "https://github.com/alisafaya/mukayese",
        "project": "https://tdd.ai",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Ko\u00e7 University;Ca\u011falo\u011flu Anadolu Lisesi;Bilkent University",
        "aff_unique_dep": "Computer Engineering Department;;Computer Engineering Department",
        "aff_unique_url": "https://www.ku.edu.tr;;https://www.bilkent.edu.tr",
        "aff_unique_abbr": "KU;;Bilkent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "T\u00fcrkiye"
    },
    {
        "id": "2022.acl-long.374",
        "title": "Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning. In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem. We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model. Our approach also lends us the ability to perform a much more robust feature selection, and identify a common set of features that influence zero-shot performance across a variety of tasks.",
        "author": "Kabir Ahuja; Shanu Kumar; Sandipan Dandapat; Monojit Choudhury",
        "authorids": "/k/kabir-ahuja/; /s/shanu-kumar/; /s/sandipan-dandapat/; /m/monojit-choudhury/",
        "bibtex": "@inproceedings{ahuja-etal-2022-multi,\n    title = \"Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models\",\n    author = \"Ahuja, Kabir  and\n      Kumar, Shanu  and\n      Dandapat, Sandipan  and\n      Choudhury, Monojit\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.374/\",\n    doi = \"10.18653/v1/2022.acl-long.374\",\n    pages = \"5454--5467\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.374.pdf",
        "site": "https://aclanthology.org/2022.acl-long.374/",
        "pdf_size": 482421,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18243194348277202389&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Research, India; Microsoft R&D, Hyderabad, India; Microsoft R&D, Hyderabad, India; Microsoft Research, India",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/india.aspx",
        "aff_unique_abbr": "MSR India",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hyderabad",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.findings-acl.95",
        "title": "Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion Cause Pair Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The emotion cause pair extraction (ECPE) task aims to extract emotions and causes as pairs from documents. We observe that the relative distance distribution of emotions and causes is extremely imbalanced in the typical ECPE dataset. Existing methods have set a fixed size window to capture relations between neighboring clauses. However, they neglect the effective semantic connections between distant clauses, leading to poor generalization ability towards position-insensitive data. To alleviate the problem, we propose a novel Multi-Granularity Semantic Aware Graph model (MGSAG) to incorporate fine-grained and coarse-grained semantic features jointly, without regard to distance limitation. In particular, we first explore semantic dependencies between clauses and keywords extracted from the document that convey fine-grained semantic features, obtaining keywords enhanced clause representations. Besides, a clause graph is also established to model coarse-grained semantic relations between clauses. Experimental results indicate that MGSAG surpasses the existing state-of-the-art ECPE models. Especially, MGSAG outperforms other models significantly in the condition of position-insensitive data.",
        "author": "Yinan Bao; Qianwen Ma; Lingwei Wei; Wei Zhou; Songlin Hu",
        "authorids": "/y/yinan-bao/; /q/qianwen-ma/; /l/lingwei-wei/; /w/wei-zhou/; /s/songlin-hu/",
        "bibtex": "@inproceedings{bao-etal-2022-multi,\n    title = \"Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion Cause Pair Extraction\",\n    author = \"Bao, Yinan  and\n      Ma, Qianwen  and\n      Wei, Lingwei  and\n      Zhou, Wei  and\n      Hu, Songlin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.95/\",\n    doi = \"10.18653/v1/2022.findings-acl.95\",\n    pages = \"1203--1213\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.95.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.95/",
        "pdf_size": 1041771,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6828416363316775131&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.71",
        "title": "Multi-Granularity Structural Knowledge Distillation for Language Model Compression",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.",
        "author": "Chang Liu; Chongyang Tao; Jiazhan Feng; Dongyan Zhao",
        "authorids": "/c/chang-liu/; /c/chongyang-tao/; /j/jiazhan-feng/; /d/dongyan-zhao/",
        "bibtex": "@inproceedings{liu-etal-2022-multi-granularity,\n    title = \"Multi-Granularity Structural Knowledge Distillation for Language Model Compression\",\n    author = \"Liu, Chang  and\n      Tao, Chongyang  and\n      Feng, Jiazhan  and\n      Zhao, Dongyan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.71/\",\n    doi = \"10.18653/v1/2022.acl-long.71\",\n    pages = \"1001--1011\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.71.pdf",
        "site": "https://aclanthology.org/2022.acl-long.71/",
        "pdf_size": 392671,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1752518631225481170&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Wangxuan Institute of Computer Technology, Peking University+Center for Data Science, Peking University+Artificial Intelligence Institute of Peking University+State Key Laboratory of Media Convergence Production Technology and Systems; Microsoft Corporation; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University+Center for Data Science, Peking University+Artificial Intelligence Institute of Peking University+State Key Laboratory of Media Convergence Production Technology and Systems",
        "aff_domain": "pku.edu.cn;microsoft.com;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;microsoft.com;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+0+1;2;0;0+0+0+1",
        "aff_unique_norm": "Peking University;State Key Laboratory of Media Convergence Production Technology and Systems;Microsoft",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;;Microsoft Corporation",
        "aff_unique_url": "http://www.pku.edu.cn;;https://www.microsoft.com",
        "aff_unique_abbr": "PKU;;Microsoft",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0+0;1;0;0+0+0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.124",
        "title": "Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With the increasing popularity of posting multimodal messages online, many recent studies have been carried out utilizing both textual and visual information for multi-modal sarcasm detection. In this paper, we investigate multi-modal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities. Specifically, we first detect the objects paired with descriptions of the image modality, enabling the learning of important visual information. Then, the descriptions of the objects are served as a bridge to determine the importance of the association between the objects of image modality and the contextual words of text modality, so as to build a cross-modal graph for each multi-modal instance. Furthermore, we devise a cross-modal graph convolutional network to make sense of the incongruity relations between modalities for multi-modal sarcasm detection. Extensive experimental results and in-depth analysis show that our model achieves state-of-the-art performance in multi-modal sarcasm detection.",
        "author": "Bin Liang; Chenwei Lou; Xiang Li; Min Yang; Lin Gui; Yulan He; Wenjie Pei; Ruifeng Xu",
        "authorids": "/b/bin-liang/; /c/chenwei-lou/; /x/xiang-li/; /m/min-yang/; /l/lin-gui/; /y/yulan-he/; /w/wenjie-pei/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{liang-etal-2022-multi,\n    title = \"Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network\",\n    author = \"Liang, Bin  and\n      Lou, Chenwei  and\n      Li, Xiang  and\n      Yang, Min  and\n      Gui, Lin  and\n      He, Yulan  and\n      Pei, Wenjie  and\n      Xu, Ruifeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.124/\",\n    doi = \"10.18653/v1/2022.acl-long.124\",\n    pages = \"1767--1777\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.124.pdf",
        "site": "https://aclanthology.org/2022.acl-long.124/",
        "pdf_size": 10163482,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1075970496301416683&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China+Joint Lab of HITSZ and China Merchants Securities, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China+Joint Lab of HITSZ and China Merchants Securities, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China+Joint Lab of HITSZ and China Merchants Securities, Shenzhen, China; SIAT, Chinese Academy of Sciences, Shenzhen, China; Department of Computer Science, University of Warwick, UK; Department of Computer Science, University of Warwick, UK+The Alan Turing Institute, UK; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China+Peng Cheng Laboratory, Shenzhen, China",
        "aff_domain": "stu.hit.edu.cn;stu.hit.edu.cn;163.com;siat.ac.cn;warwick.ac.uk;warwick.ac.uk;outlook.com;hit.edu.cn",
        "email": "stu.hit.edu.cn;stu.hit.edu.cn;163.com;siat.ac.cn;warwick.ac.uk;warwick.ac.uk;outlook.com;hit.edu.cn",
        "github": "https://github.com/HITSZ-HLT/CMGCN",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;0+1;2;3;3+4;0;0+5",
        "aff_unique_norm": "Harbin Institute of Technology;Harbin Institute of Technology Shenzhen (HITSZ);Shenzhen Institute of Advanced Technology;University of Warwick;Alan Turing Institute;Pengcheng Laboratory",
        "aff_unique_dep": "School of Computer Science and Technology;Joint Lab;;Department of Computer Science;;Peng Cheng Laboratory",
        "aff_unique_url": "http://www.hit.edu.cn/;http://en.hitsz.edu.cn/;http://www.siat.ac.cn;https://warwick.ac.uk;https://www.turing.ac.uk;",
        "aff_unique_abbr": "HIT;HITSZ;SIAT;Warwick;ATI;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0;;0;0+0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0+0;0+0;0;1;1+1;0;0+0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2022.acl-long.24",
        "title": "Multi-Party Empathetic Dialogue Generation: A New Task for Dialog Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Empathetic dialogue assembles emotion understanding, feeling projection, and appropriate response generation. Existing work for empathetic dialogue generation concentrates on the two-party conversation scenario. Multi-party dialogues, however, are pervasive in reality. Furthermore, emotion and sensibility are typically confused; a refined empathy analysis is needed for comprehending fragile and nuanced human feelings. We address these issues by proposing a novel task called Multi-Party Empathetic Dialogue Generation in this study. Additionally, a Static-Dynamic model for Multi-Party Empathetic Dialogue Generation, SDMPED, is introduced as a baseline by exploring the static sensibility and dynamic emotion for the multi-party empathetic dialogue learning, the aspects that help SDMPED achieve the state-of-the-art performance.",
        "author": "Ling.Yu Zhu; Zhengkun Zhang; Jun Wang; Hongbin Wang; Haiying Wu; Zhenglu Yang",
        "authorids": "/l/ling-yu-zhu/; /z/zhengkun-zhang/; /j/jun-wang/; /h/hongbin-wang/; /h/haiying-wu/; /z/zhenglu-yang/",
        "bibtex": "@inproceedings{zhu-etal-2022-multi,\n    title = \"Multi-Party Empathetic Dialogue Generation: A New Task for Dialog Systems\",\n    author = \"Zhu, Ling.Yu  and\n      Zhang, Zhengkun  and\n      Wang, Jun  and\n      Wang, Hongbin  and\n      Wu, Haiying  and\n      Yang, Zhenglu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.24/\",\n    doi = \"10.18653/v1/2022.acl-long.24\",\n    pages = \"298--307\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.24.pdf",
        "site": "https://aclanthology.org/2022.acl-long.24/",
        "pdf_size": 1007069,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18072626609438060690&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "TKLNDST, CS, Nankai University; TKLNDST, CS, Nankai University; MS, Ludong University; Mashang Consumer Finance Co., Ltd., China; Mashang Consumer Finance Co., Ltd., China; TKLNDST, CS, Nankai University",
        "aff_domain": "mail.nankai.edu.cn;mail.nankai.edu.cn;mail.nankai.edu.cn;msxf.com;msxf.com;nankai.edu.cn",
        "email": "mail.nankai.edu.cn;mail.nankai.edu.cn;mail.nankai.edu.cn;msxf.com;msxf.com;nankai.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;2;0",
        "aff_unique_norm": "Nankai University;Ludong University;Mashang Consumer Finance Co., Ltd.",
        "aff_unique_dep": "Computer Science;MS;",
        "aff_unique_url": "http://www.nankai.edu.cn;http://www.ldu.edu.cn/;",
        "aff_unique_abbr": "Nankai U;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.7",
        "title": "Multi-Scale Distribution Deep Variational Autoencoder for Explanation Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Generating explanations for recommender systems is essential for improving their transparency, as users often wish to understand the reason for receiving a specified recommendation. Previous methods mainly focus on improving the generation quality, but often produce generic explanations that fail to incorporate user and item specific details. To resolve this problem, we present Multi-Scale Distribution Deep Variational Autoencoders (MVAE).These are deep hierarchical VAEs with a prior network that eliminates noise while retaining meaningful signals in the input, coupled with a recognition network serving as the source of information to guide the learning of the prior network. Further, the Multi-scale distribution Learning Framework (MLF) along with a Target Tracking Kullback-Leibler divergence (TKL) mechanism are proposed to employ multi KL divergences at different scales for more effective learning. Extensive empirical experiments demonstrate that our methods can generate explanations with concrete input-specific contents.",
        "author": "ZeFeng Cai; Linlin Wang; Gerard de Melo; Fei Sun; Liang He",
        "authorids": "/z/zefeng-cai/; /l/linlin-wang/; /g/gerard-de-melo/; /f/fei-sun/; /l/liang-he/",
        "bibtex": "@inproceedings{cai-etal-2022-multi,\n    title = \"Multi-Scale Distribution Deep Variational Autoencoder for Explanation Generation\",\n    author = \"Cai, ZeFeng  and\n      Wang, Linlin  and\n      de Melo, Gerard  and\n      Sun, Fei  and\n      He, Liang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.7/\",\n    doi = \"10.18653/v1/2022.findings-acl.7\",\n    pages = \"68--78\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.7.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.7/",
        "pdf_size": 829986,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7760448876199824974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "East China Normal University; East China Normal University; Hasso Plattner Institute, University of Potsdam; Alibaba Group; East China Normal University",
        "aff_domain": "stu.ecnu.edu.cn;cs.ecnu.edu.cn;demelo.org;gmail.com;cs.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;cs.ecnu.edu.cn;demelo.org;gmail.com;cs.ecnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "East China Normal University;Hasso Plattner Institute;Alibaba Group",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.ecnu.edu.cn;https://www.hpi.de;https://www.alibaba.com",
        "aff_unique_abbr": "ECNU;HPI;Alibaba",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Potsdam",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "China;Germany"
    },
    {
        "id": "2022.findings-acl.104",
        "title": "Multi-Stage Prompting for Knowledgeable Dialogue Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed. In this paper, we aim to address these limitations by leveraging the inherent knowledge stored in the pretrained LM as well as its powerful generation ability. We propose a multi-stage prompting approach to generate knowledgeable responses from a single pretrained LM. We first prompt the LM to generate knowledge based on the dialogue context. Then, we further prompt it to generate responses based on the dialogue context and the previously generated knowledge. Results show that our knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness. In addition, our multi-stage prompting outperforms the finetuning-based dialogue model in terms of response knowledgeability and engagement by up to 10% and 5%, respectively. Furthermore, we scale our model up to 530 billion parameters and demonstrate that larger LMs improve the generation correctness score by up to 10%, and response relevance, knowledgeability and engagement by up to 10%. Our code is available at: https://github.com/NVIDIA/Megatron-LM.",
        "author": "Zihan Liu; Mostofa Patwary; Ryan Prenger; Shrimai Prabhumoye; Wei Ping; Mohammad Shoeybi; Bryan Catanzaro",
        "authorids": "/z/zihan-liu/; /m/mostofa-patwary/; /r/ryan-prenger/; /s/shrimai-prabhumoye/; /w/wei-ping/; /m/mohammad-shoeybi/; /b/bryan-catanzaro/",
        "bibtex": "@inproceedings{liu-etal-2022-multi,\n    title = \"Multi-Stage Prompting for Knowledgeable Dialogue Generation\",\n    author = \"Liu, Zihan  and\n      Patwary, Mostofa  and\n      Prenger, Ryan  and\n      Prabhumoye, Shrimai  and\n      Ping, Wei  and\n      Shoeybi, Mohammad  and\n      Catanzaro, Bryan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.104/\",\n    doi = \"10.18653/v1/2022.findings-acl.104\",\n    pages = \"1317--1337\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.104.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.104/",
        "pdf_size": 1258848,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15448489159263937557&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 6,
        "aff": "The Hong Kong University of Science and Technology\u2021; NVIDIA\u00a7; NVIDIA\u00a7; NVIDIA\u00a7; NVIDIA\u00a7; NVIDIA\u00a7; NVIDIA\u00a7",
        "aff_domain": "connect.ust.hk;nvidia.com; ; ; ; ; ",
        "email": "connect.ust.hk;nvidia.com; ; ; ; ; ",
        "github": "https://github.com/NVIDIA/Megatron-LM",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;NVIDIA",
        "aff_unique_dep": ";NVIDIA Corporation",
        "aff_unique_url": "https://www.ust.hk;https://www.nvidia.com",
        "aff_unique_abbr": "HKUST;NVIDIA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;1;1;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.319",
        "title": "Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora. We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification. Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios. Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators.",
        "author": "Yixuan Su; Lei Shu; Elman Mansimov; Arshit Gupta; Deng Cai; Yi-An Lai; Yi Zhang",
        "authorids": "/y/yixuan-su/; /l/lei-shu/; /e/elman-mansimov/; /a/arshit-gupta/; /d/deng-cai/; /y/yi-an-lai/; /y/yi-zhang/",
        "bibtex": "@inproceedings{su-etal-2022-multi,\n    title = \"Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System\",\n    author = \"Su, Yixuan  and\n      Shu, Lei  and\n      Mansimov, Elman  and\n      Gupta, Arshit  and\n      Cai, Deng  and\n      Lai, Yi-An  and\n      Zhang, Yi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.319/\",\n    doi = \"10.18653/v1/2022.acl-long.319\",\n    pages = \"4661--4676\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.319.pdf",
        "site": "https://aclanthology.org/2022.acl-long.319/",
        "pdf_size": 716830,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9908876537688296457&as_sdt=8005&sciodt=0,7&hl=en",
        "gs_version_total": 6,
        "aff": "University of Cambridge; Amazon AWS AI; Amazon AWS AI; Amazon AWS AI; The Chinese University of Hong Kong; Amazon AWS AI; Amazon AWS AI",
        "aff_domain": "cam.ac.uk;amazon.com;amazon.com;amazon.com;gmail.com;amazon.com;amazon.com",
        "email": "cam.ac.uk;amazon.com;amazon.com;amazon.com;gmail.com;amazon.com;amazon.com",
        "github": "https://github.com/awslabs/pptod",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;2;1;1",
        "aff_unique_norm": "University of Cambridge;Amazon;Chinese University of Hong Kong",
        "aff_unique_dep": ";Amazon Web Services AI;",
        "aff_unique_url": "https://www.cam.ac.uk;https://aws.amazon.com;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "Cambridge;AWS;CUHK",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Cambridge;;Hong Kong SAR",
        "aff_country_unique_index": "0;1;1;1;2;1;1",
        "aff_country_unique": "United Kingdom;United States;China"
    },
    {
        "id": "2022.acl-long.414",
        "title": "Multi-View Document Representation Learning for Open-Domain Dense Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Dense retrieval has achieved impressive advances in first-stage retrieval from a large-scale document collection, which is built on bi-encoder architecture to produce single vector representation of query and document. However, a document can usually answer multiple potential queries from different views. So the single vector representation of a document is hard to match with multi-view queries, and faces a semantic mismatch problem. This paper proposes a multi-view document representation learning framework, aiming to produce multi-view embeddings to represent documents and enforce them to align with different queries. First, we propose a simple yet effective method of generating multiple embeddings through viewers. Second, to prevent multi-view embeddings from collapsing to the same one, we further propose a global-local loss with annealed temperature to encourage the multiple viewers to better align with different potential queries. Experiments show our method outperforms recent works and achieves state-of-the-art results.",
        "author": "Shunyu Zhang; Yaobo Liang; Ming Gong; Daxin Jiang; Nan Duan",
        "authorids": "/s/shunyu-zhang/; /y/yaobo-liang/; /m/ming-gong/; /d/daxin-jiang/; /n/nan-duan/",
        "bibtex": "@inproceedings{zhang-etal-2022-multi,\n    title = \"Multi-View Document Representation Learning for Open-Domain Dense Retrieval\",\n    author = \"Zhang, Shunyu  and\n      Liang, Yaobo  and\n      Gong, Ming  and\n      Jiang, Daxin  and\n      Duan, Nan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.414/\",\n    doi = \"10.18653/v1/2022.acl-long.414\",\n    pages = \"5990--6000\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.414.pdf",
        "site": "https://aclanthology.org/2022.acl-long.414/",
        "pdf_size": 473968,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13004098925644684070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Intelligent Computing and Machine Learning Lab, School of ASEE, Beihang University; Microsoft Research Asia; Microsoft STC Asia; Microsoft STC Asia; Microsoft Research Asia",
        "aff_domain": "buaa.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "buaa.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Beihang University;Microsoft",
        "aff_unique_dep": "School of ASEE;Research",
        "aff_unique_url": "http://www.buaa.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "Beihang;MSR Asia",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.97",
        "title": "Multi-task Learning for Paraphrase Generation With Keyword and Part-of-Speech Reconstruction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Paraphrase generation using deep learning has been a research hotspot of natural language processing in the past few years. While previous studies tackle the problem from different aspects, the essence of paraphrase generation is to retain the key semantics of the source sentence and rewrite the rest of the content. Inspired by this observation, we propose a novel two-stage model, PGKPR, for paraphrase generation with keyword and part-of-speech reconstruction. The rationale is to capture simultaneously the possible keywords of a source sentence and the relations between them to facilitate the rewriting. In the first stage, we identify the possible keywords using a prediction attribution technique, where the words obtaining higher attribution scores are more likely to be the keywords. In the second stage, we train a transformer-based model via multi-task learning for paraphrase generation. The novel learning task is the reconstruction of the keywords and part-of-speech tags, respectively, from a perturbed sequence of the source sentence. The learned encodings are then decoded to generate the paraphrase. We conduct the experiments on two commonly-used datasets, and demonstrate the superior performance of PGKPR over comparative models on multiple evaluation metrics.",
        "author": "Xuhang Xie; Xuesong Lu; Bei Chen",
        "authorids": "/x/xuhang-xie/; /x/xuesong-lu/; /b/bei-chen/",
        "bibtex": "@inproceedings{xie-etal-2022-multi,\n    title = \"Multi-task Learning for Paraphrase Generation With Keyword and Part-of-Speech Reconstruction\",\n    author = \"Xie, Xuhang  and\n      Lu, Xuesong  and\n      Chen, Bei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.97/\",\n    doi = \"10.18653/v1/2022.findings-acl.97\",\n    pages = \"1234--1243\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.97.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.97/",
        "pdf_size": 694299,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1880882257996956213&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Data Science and Engineering, East China Normal University; School of Data Science and Engineering, East China Normal University; School of Data Science and Engineering, East China Normal University",
        "aff_domain": "stu.ecnu.edu.cn;dase.ecnu.edu.cn;stu.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;dase.ecnu.edu.cn;stu.ecnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "East China Normal University",
        "aff_unique_dep": "School of Data Science and Engineering",
        "aff_unique_url": "http://www.ecnu.edu.cn",
        "aff_unique_abbr": "ECNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.454",
        "title": "MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Numerical reasoning over hybrid data containing both textual and tabular content (e.g., financial reports) has recently attracted much attention in the NLP community. However, existing question answering (QA) benchmarks over hybrid data only include a single flat table in each document and thus lack examples of multi-step numerical reasoning across multiple hierarchical tables. To facilitate data analytical progress, we construct a new large-scale benchmark, MultiHiertt, with QA pairs over Multi Hierarchical Tabular and Textual data. MultiHiertt is built from a wealth of financial reports and has the following unique characteristics: 1) each document contain multiple tables and longer unstructured texts; 2) most of tables contained are hierarchical; 3) the reasoning process required for each question is more complex and challenging than existing benchmarks; and 4) fine-grained annotations of reasoning processes and supporting facts are provided to reveal complex numerical reasoning. We further introduce a novel QA model termed MT2Net, which first applies facts retrieving to extract relevant supporting facts from both tables and text and then uses a reasoning module to perform symbolic reasoning over retrieved facts. We conduct comprehensive experiments on various baselines. The experimental results show that MultiHiertt presents a strong challenge for existing baselines whose results lag far behind the performance of human experts. The dataset and code are publicly available at https://github.com/psunlpgroup/MultiHiertt.",
        "author": "Yilun Zhao; Yunxiang Li; Chenying Li; Rui Zhang",
        "authorids": "/y/yilun-zhao/; /y/yunxiang-li/; /c/chenying-li/; /r/rui-zhang/",
        "bibtex": "@inproceedings{zhao-etal-2022-multihiertt,\n    title = \"{M}ulti{H}iertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data\",\n    author = \"Zhao, Yilun  and\n      Li, Yunxiang  and\n      Li, Chenying  and\n      Zhang, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.454/\",\n    doi = \"10.18653/v1/2022.acl-long.454\",\n    pages = \"6588--6600\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.454.pdf",
        "site": "https://aclanthology.org/2022.acl-long.454/",
        "pdf_size": 4862221,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=589567572554758854&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Yale University; The Chinese University of Hong Kong; Northeastern University; Penn State University",
        "aff_domain": "yale.edu;link.cuhk.edu.hk;northeastern.edu;psu.edu",
        "email": "yale.edu;link.cuhk.edu.hk;northeastern.edu;psu.edu",
        "github": "https://github.com/psunlpgroup/MultiHiertt",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Yale University;Chinese University of Hong Kong;Northeastern University;Penn State University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.yale.edu;https://www.cuhk.edu.hk;https://www.northeastern.edu;https://www.psu.edu",
        "aff_unique_abbr": "Yale;CUHK;NEU;PSU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.453",
        "title": "Multilingual Detection of Personal Employment Status on Twitter",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Detecting disclosures of individuals\u2019 employment status on social media can provide valuable information to match job seekers with suitable vacancies, offer social protection, or measure labor market flows. However, identifying such personal disclosures is a challenging task due to their rarity in a sea of social media content and the variety of linguistic forms used to describe them. Here, we examine three Active Learning (AL) strategies in real-world settings of extreme class imbalance, and identify five types of disclosures about individuals\u2019 employment status (e.g. job loss) in three languages using BERT-based classification models. Our findings show that, even under extreme imbalance settings, a small number of AL iterations is sufficient to obtain large and significant gains in precision, recall, and diversity of results compared to a supervised baseline with the same number of labels. We also find that no AL strategy consistently outperforms the rest. Qualitative analysis suggests that AL helps focus the attention mechanism of BERT on core terms and adjust the boundaries of semantic expansion, highlighting the importance of interpretable models to provide greater control and visibility into this dynamic learning process.",
        "author": "Manuel Tonneau; Dhaval Adjodah; Joao Palotti; Nir Grinberg; Samuel Fraiberger",
        "authorids": "/m/manuel-tonneau/; /d/dhaval-adjodah/; /j/joao-palotti/; /n/nir-grinberg/; /s/samuel-fraiberger/",
        "bibtex": "@inproceedings{tonneau-etal-2022-multilingual,\n    title = \"Multilingual Detection of Personal Employment Status on {T}witter\",\n    author = \"Tonneau, Manuel  and\n      Adjodah, Dhaval  and\n      Palotti, Joao  and\n      Grinberg, Nir  and\n      Fraiberger, Samuel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.453/\",\n    doi = \"10.18653/v1/2022.acl-long.453\",\n    pages = \"6564--6587\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.453.pdf",
        "site": "https://aclanthology.org/2022.acl-long.453/",
        "pdf_size": 941977,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6928218699110227574&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The World Bank+New York University+Centre Marc Bloch; The World Bank+New York University+Massachusetts Institute of Technology; Qatar Computing Research Institute; Ben-Gurion University of the Negev; The World Bank+New York University+Massachusetts Institute of Technology",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;0+1+3;4;5;0+1+3",
        "aff_unique_norm": "World Bank;New York University;Centre Marc Bloch;Massachusetts Institute of Technology;Qatar Computing Research Institute;Ben-Gurion University of the Negev",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.worldbank.org;https://www.nyu.edu;https://www.centre-marc-bloch.fr;https://web.mit.edu;https://www.qcri.org;https://www.bgu.ac.il",
        "aff_unique_abbr": "World Bank;NYU;;MIT;QCRI;BGU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+1;0+0+0;2;3;0+0+0",
        "aff_country_unique": "United States;France;Qatar;Israel"
    },
    {
        "id": "2022.acl-long.287",
        "title": "Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Document-level neural machine translation (DocNMT) achieves coherent translations by incorporating cross-sentence context. However, for most language pairs there\u2019s a shortage of parallel documents, although parallel sentences are readily available. In this paper, we study whether and how contextual modeling in DocNMT is transferable via multilingual modeling. We focus on the scenario of zero-shot transfer from teacher languages with document level data to student languages with no documents but sentence level data, and for the first time treat document-level translation as a transfer learning problem. Using simple concatenation-based DocNMT, we explore the effect of 3 factors on the transfer: the number of teacher languages with document level data, the balance between document and sentence level data at training, and the data condition of parallel documents (genuine vs. back-translated). Our experiments on Europarl-7 and IWSLT-10 show the feasibility of multilingual transfer for DocNMT, particularly on document-specific metrics. We observe that more teacher languages and adequate data balance both contribute to better transfer quality. Surprisingly, the transfer is less sensitive to the data condition, where multilingual DocNMT delivers decent performance with either back-translated or genuine document pairs.",
        "author": "Biao Zhang; Ankur Bapna; Melvin Johnson; Ali Dabirmoghaddam; Naveen Arivazhagan; Orhan Firat",
        "authorids": "/b/biao-zhang/; /a/ankur-bapna/; /m/melvin-johnson/; /a/ali-dabirmoghaddam/; /n/naveen-arivazhagan/; /o/orhan-firat/",
        "bibtex": "@inproceedings{zhang-etal-2022-multilingual,\n    title = \"Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents\",\n    author = \"Zhang, Biao  and\n      Bapna, Ankur  and\n      Johnson, Melvin  and\n      Dabirmoghaddam, Ali  and\n      Arivazhagan, Naveen  and\n      Firat, Orhan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.287/\",\n    doi = \"10.18653/v1/2022.acl-long.287\",\n    pages = \"4176--4192\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.287.pdf",
        "site": "https://aclanthology.org/2022.acl-long.287/",
        "pdf_size": 638594,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=599320003161134719&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "School of Informatics, University of Edinburgh+Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "ed.ac.uk;google.com;google.com;google.com;google.com;google.com",
        "email": "ed.ac.uk;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "University of Edinburgh;Google",
        "aff_unique_dep": "School of Informatics;Google Research",
        "aff_unique_url": "https://www.ed.ac.uk;https://research.google",
        "aff_unique_abbr": "Edinburgh;Google Research",
        "aff_campus_unique_index": "0+1;1;1;1;1;1",
        "aff_campus_unique": "Edinburgh;Mountain View",
        "aff_country_unique_index": "0+1;1;1;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2022.acl-long.317",
        "title": "Multilingual Generative Language Models for Zero-Shot Cross-Lingual Event Argument Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a study on leveraging multilingual pre-trained generative language models for zero-shot cross-lingual event argument extraction (EAE). By formulating EAE as a language generation task, our method effectively encodes event structures and captures the dependencies between arguments. We design language-agnostic templates to represent the event argument structures, which are compatible with any language, hence facilitating the cross-lingual transfer. Our proposed model finetunes multilingual pre-trained generative language models to generate sentences that fill in the language-agnostic template with arguments extracted from the input passage. The model is trained on source languages and is then directly applied to target languages for event argument extraction. Experiments demonstrate that the proposed model outperforms the current state-of-the-art models on zero-shot cross-lingual EAE. Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE.",
        "author": "Kuan-Hao Huang; I-Hung Hsu; Prem Natarajan; Kai-Wei Chang; Nanyun Peng",
        "authorids": "/k/kuan-hao-huang/; /i/i-hung-hsu/; /p/prem-natarajan/; /k/kai-wei-chang/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{huang-etal-2022-multilingual-generative,\n    title = \"Multilingual Generative Language Models for Zero-Shot Cross-Lingual Event Argument Extraction\",\n    author = \"Huang, Kuan-Hao  and\n      Hsu, I-Hung  and\n      Natarajan, Prem  and\n      Chang, Kai-Wei  and\n      Peng, Nanyun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.317/\",\n    doi = \"10.18653/v1/2022.acl-long.317\",\n    pages = \"4633--4646\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.317.pdf",
        "site": "https://aclanthology.org/2022.acl-long.317/",
        "pdf_size": 601075,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2888809928143045812&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, University of California, Los Angeles + Information Science Institute, University of Southern California; Computer Science Department, University of California, Los Angeles + Information Science Institute, University of Southern California; Information Science Institute, University of Southern California; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles + Information Science Institute, University of Southern California",
        "aff_domain": "cs.ucla.edu;isi.edu;isi.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;isi.edu;isi.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;1;0;0+1",
        "aff_unique_norm": "University of California, Los Angeles;University of Southern California",
        "aff_unique_dep": "Computer Science Department;Information Science Institute",
        "aff_unique_url": "https://www.ucla.edu;https://www.usc.edu",
        "aff_unique_abbr": "UCLA;USC",
        "aff_campus_unique_index": "0+0;0+0;0;0;0+0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.36",
        "title": "Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Predicting missing facts in a knowledge graph (KG) is crucial as modern KGs are far from complete. Due to labor-intensive human labeling, this phenomenon deteriorates when handling knowledge represented in various languages. In this paper, we explore multilingual KG completion, which leverages limited seed alignment as a bridge, to embrace the collective knowledge from multiple languages. However, language alignment used in prior works is still not fully exploited: (1) alignment pairs are treated equally to maximally push parallel entities to be close, which ignores KG capacity inconsistency; (2) seed alignment is scarce and new alignment identification is usually in a noisily unsupervised manner. To tackle these issues, we propose a novel self-supervised adaptive graph alignment (SS-AGA) method. Specifically, SS-AGA fuses all KGs as a whole graph by regarding alignment as a new edge type. As such, information propagation and noise influence across KGs can be adaptively controlled via relation-aware attention weights. Meanwhile, SS-AGA features a new pair generator that dynamically captures potential alignment pairs in a self-supervised paradigm. Extensive experiments on both the public multilingual DBPedia KG and newly-created industrial multilingual E-commerce KG empirically demonstrate the effectiveness of SS-AGA",
        "author": "Zijie Huang; Zheng Li; Haoming Jiang; Tianyu Cao; Hanqing Lu; Bing Yin; Karthik Subbian; Yizhou Sun; Wei Wang",
        "authorids": "/z/zijie-huang/; /z/zheng-li/; /h/haoming-jiang/; /t/tianyu-cao/; /h/hanqing-lu/; /b/bing-yin/; /k/karthik-subbian/; /y/yizhou-sun/; /w/wei-wang/",
        "bibtex": "@inproceedings{huang-etal-2022-multilingual,\n    title = \"Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment\",\n    author = \"Huang, Zijie  and\n      Li, Zheng  and\n      Jiang, Haoming  and\n      Cao, Tianyu  and\n      Lu, Hanqing  and\n      Yin, Bing  and\n      Subbian, Karthik  and\n      Sun, Yizhou  and\n      Wang, Wei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.36/\",\n    doi = \"10.18653/v1/2022.acl-long.36\",\n    pages = \"474--485\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.36.pdf",
        "site": "https://aclanthology.org/2022.acl-long.36/",
        "pdf_size": 550784,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7946924487972813600&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Los Angeles, CA, USA+Amazon.com Inc, CA, USA; Amazon.com Inc, CA, USA; Amazon.com Inc, CA, USA; Amazon.com Inc, CA, USA; Amazon.com Inc, CA, USA; Amazon.com Inc, CA, USA; Amazon.com Inc, CA, USA; University of California, Los Angeles, CA, USA+Amazon.com Inc, CA, USA; University of California, Los Angeles, CA, USA+Amazon.com Inc, CA, USA",
        "aff_domain": "cs.ucla.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;cs.ucla.edu;cs.ucla.edu",
        "github": "https://github.com/amzn/ss-aga-kgc",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;1;1;1;1;1;1;0+1;0+1",
        "aff_unique_norm": "University of California, Los Angeles;Amazon",
        "aff_unique_dep": ";Amazon.com Inc",
        "aff_unique_url": "https://www.ucla.edu;https://www.amazon.com",
        "aff_unique_abbr": "UCLA;Amazon",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.282",
        "title": "Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual neural machine translation models are trained to maximize the likelihood of a mix of examples drawn from multiple language pairs. The dominant inductive bias applied to these models is a shared vocabulary and a shared set of parameters across languages; the inputs and labels corresponding to examples drawn from different language pairs might still reside in distinct sub-spaces. In this paper, we introduce multilingual crossover encoder-decoder (mXEncDec) to fuse language pairs at an instance level. Our approach interpolates instances from different language pairs into joint \u2018crossover examples\u2019 in order to encourage sharing input and output spaces across languages. To ensure better fusion of examples in multilingual settings, we propose several techniques to improve example interpolation across dissimilar languages under heavy data imbalance. Experiments on a large-scale WMT multilingual dataset demonstrate that our approach significantly improves quality on English-to-Many, Many-to-English and zero-shot translation tasks (from +0.5 BLEU up to +5.5 BLEU points). Results on code-switching sets demonstrate the capability of our approach to improve model generalization to out-of-distribution multilingual examples. We also conduct qualitative and quantitative representation comparisons to analyze the advantages of our approach at the representation level.",
        "author": "Yong Cheng; Ankur Bapna; Orhan Firat; Yuan Cao; Pidong Wang; Wolfgang Macherey",
        "authorids": "/y/yong-cheng/; /a/ankur-bapna/; /o/orhan-firat/; /y/yuan-cao/; /p/pidong-wang/; /w/wolfgang-macherey/",
        "bibtex": "@inproceedings{cheng-etal-2022-multilingual,\n    title = \"Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation\",\n    author = \"Cheng, Yong  and\n      Bapna, Ankur  and\n      Firat, Orhan  and\n      Cao, Yuan  and\n      Wang, Pidong  and\n      Macherey, Wolfgang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.282/\",\n    doi = \"10.18653/v1/2022.acl-long.282\",\n    pages = \"4092--4102\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.282.pdf",
        "site": "https://aclanthology.org/2022.acl-long.282/",
        "pdf_size": 1064807,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10644122960130820797&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.242",
        "title": "Multilingual Molecular Representation Learning via Contrastive Pre-training",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Molecular representation learning plays an essential role in cheminformatics. Recently, language model-based approaches have gained popularity as an alternative to traditional expert-designed features to encode molecules. However, these approaches only utilize a single molecular language for representation learning. Motivated by the fact that a given molecule can be described using different languages such as Simplified Molecular Line Entry System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International Chemical Identifier (InChI), we propose a multilingual molecular embedding generation approach called MM-Deacon (multilingual molecular domain embedding analysis via contrastive learning). MM-Deacon is pre-trained using SMILES and IUPAC as two different languages on large-scale molecules. We evaluated the robustness of our method on seven molecular property prediction tasks from MoleculeNet benchmark, zero-shot cross-lingual retrieval, and a drug-drug interaction prediction task.",
        "author": "Zhihui Guo; Pramod Sharma; Andy Martinez; Liang Du; Robin Abraham",
        "authorids": "/z/zhihui-guo/; /p/pramod-sharma/; /a/andy-martinez/; /l/liang-du/; /r/robin-abraham/",
        "bibtex": "@inproceedings{guo-etal-2022-multilingual,\n    title = \"Multilingual Molecular Representation Learning via Contrastive Pre-training\",\n    author = \"Guo, Zhihui  and\n      Sharma, Pramod  and\n      Martinez, Andy  and\n      Du, Liang  and\n      Abraham, Robin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.242/\",\n    doi = \"10.18653/v1/2022.acl-long.242\",\n    pages = \"3441--3453\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.242.pdf",
        "site": "https://aclanthology.org/2022.acl-long.242/",
        "pdf_size": 952197,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11455599709528165505&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Corporation",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.29",
        "title": "Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We exploit the pre-trained seq2seq model mBART for multilingual text style transfer. Using machine translated data as well as gold aligned English sentences yields state-of-the-art results in the three target languages we consider. Besides, in view of the general scarcity of parallel data, we propose a modular approach for multilingual formality transfer, which consists of two training strategies that target adaptation to both language and task. Our approach achieves competitive performance without monolingual task-specific parallel data and can be applied to other style transfer tasks as well as to other languages.",
        "author": "Huiyuan Lai; Antonio Toral; Malvina Nissim",
        "authorids": "/h/huiyuan-lai/; /a/antonio-toral/; /m/malvina-nissim/",
        "bibtex": "@inproceedings{lai-etal-2022-multilingual,\n    title = \"Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer\",\n    author = \"Lai, Huiyuan  and\n      Toral, Antonio  and\n      Nissim, Malvina\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.29/\",\n    doi = \"10.18653/v1/2022.acl-short.29\",\n    pages = \"262--271\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.29.pdf",
        "site": "https://aclanthology.org/2022.acl-short.29/",
        "pdf_size": 761732,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4336608757021991657&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "CLCG, University of Groningen / The Netherlands; CLCG, University of Groningen / The Netherlands; CLCG, University of Groningen / The Netherlands",
        "aff_domain": "rug.nl;rug.nl;rug.nl",
        "email": "rug.nl;rug.nl;rug.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Groningen",
        "aff_unique_dep": "CLCG",
        "aff_unique_url": "https://www.rug.nl",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.acl-long.366",
        "title": "Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We show that unsupervised sequence-segmentation performance can be transferred to extremely low-resource languages by pre-training a Masked Segmental Language Model (Downey et al., 2021) multilingually. Further, we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar (but phylogenetically unrelated) to the target language. In our experiments, we transfer from a collection of 10 Indigenous American languages (AmericasNLP, Mager et al., 2021) to K\u2019iche\u2019, a Mayan language. We compare our multilingual model to a monolingual (from-scratch) baseline, as well as a model pre-trained on Quechua only. We show that the multilingual pre-trained approach yields consistent segmentation quality across target dataset sizes, exceeding the monolingual baseline in 6/10 experimental settings. Our model yields especially strong results at small target sizes, including a zero-shot performance of 20.6 F1. These results have promising implications for low-resource NLP pipelines involving human-like linguistic units, such as the sparse transcription framework proposed by Bird (2020).",
        "author": "C. Downey; Shannon Drizin; Levon Haroutunian; Shivin Thukral",
        "authorids": "/c/c-downey/; /s/shannon-drizin/; /l/levon-haroutunian/; /s/shivin-thukral/",
        "bibtex": "@inproceedings{downey-etal-2022-multilingual,\n    title = \"Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages\",\n    author = \"Downey, C.  and\n      Drizin, Shannon  and\n      Haroutunian, Levon  and\n      Thukral, Shivin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.366/\",\n    doi = \"10.18653/v1/2022.acl-long.366\",\n    pages = \"5331--5346\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.366.pdf",
        "site": "https://aclanthology.org/2022.acl-long.366/",
        "pdf_size": 667558,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12968148535625677678&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Linguistics, University of Washington; Department of Linguistics, University of Washington; Department of Linguistics, University of Washington; Department of Linguistics, University of Washington",
        "aff_domain": "uw.edu;uw.edu;uw.edu;uw.edu",
        "email": "uw.edu;uw.edu;uw.edu;uw.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.204",
        "title": "Multimodal Dialogue Response Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Responsing with image has been recognized as an important capability for an intelligent conversational agent. Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods, but neglecting generation methods. To fill in the gaps, we first present a new task: multimodal dialogue response generation (MDRG) - given the dialogue history, one model needs to generate a text sequence or an image as response. Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain. Motivated by the challenge in practice, we consider MDRG under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a novel conversational agent, Divter, in order to isolate parameters that depend on multimodal dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of text-only dialogues and text-image pairs respectively, then the whole parameters can be well fitted using the limited training examples. Extensive experiments demonstrate our method achieves state-of-the-art results in both automatic and human evaluation, and can generate informative text and high-resolution image responses.",
        "author": "Qingfeng Sun; Yujing Wang; Can Xu; Kai Zheng; Yaming Yang; Huang Hu; Fei Xu; Jessica Zhang; Xiubo Geng; Daxin Jiang",
        "authorids": "/q/qingfeng-sun/; /y/yujing-wang/; /c/can-xu/; /k/kai-zheng/; /y/yaming-yang/; /h/huang-hu/; /f/fei-xu/; /j/jessica-zhang/; /x/xiubo-geng/; /d/daxin-jiang/",
        "bibtex": "@inproceedings{sun-etal-2022-multimodal,\n    title = \"Multimodal Dialogue Response Generation\",\n    author = \"Sun, Qingfeng  and\n      Wang, Yujing  and\n      Xu, Can  and\n      Zheng, Kai  and\n      Yang, Yaming  and\n      Hu, Huang  and\n      Xu, Fei  and\n      Zhang, Jessica  and\n      Geng, Xiubo  and\n      Jiang, Daxin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.204/\",\n    doi = \"10.18653/v1/2022.acl-long.204\",\n    pages = \"2854--2866\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.204.pdf",
        "site": "https://aclanthology.org/2022.acl-long.204/",
        "pdf_size": 5065953,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10679951300997332764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft STC Asia; Microsoft Research Asia; Microsoft STC Asia; Microsoft STC Asia; Microsoft Research Asia; Microsoft STC Asia; Microsoft STC Asia; Microsoft STC Asia; Microsoft STC Asia; Microsoft STC Asia",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "STC",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "MS",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Asia",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.562",
        "title": "Multimodal Sarcasm Target Identification in Tweets",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sarcasm is important to sentiment analysis on social media. Sarcasm Target Identification (STI) deserves further study to understand sarcasm in depth. However, text lacking context or missing sarcasm target makes target identification very difficult. In this paper, we introduce multimodality to STI and present Multimodal Sarcasm Target Identification (MSTI) task. We propose a novel multi-scale cross-modality model that can simultaneously perform textual target labeling and visual target detection. In the model, we extract multi-scale visual features to enrich spatial information for different sized visual sarcasm targets. We design a set of convolution networks to unify multi-scale visual features with textual features for cross-modal attention learning, and correspondingly a set of transposed convolution networks to restore multi-scale visual information. The results show that visual clues can improve the performance of TSTI by a large margin, and VSTI achieves good accuracy.",
        "author": "Jiquan Wang; Lin Sun; Yi Liu; Meizhi Shao; Zengwei Zheng",
        "authorids": "/j/jiquan-wang/; /l/lin-sun/; /y/yi-liu/; /m/meizhi-shao/; /z/zengwei-zheng/",
        "bibtex": "@inproceedings{wang-etal-2022-multimodal,\n    title = \"Multimodal Sarcasm Target Identification in Tweets\",\n    author = \"Wang, Jiquan  and\n      Sun, Lin  and\n      Liu, Yi  and\n      Shao, Meizhi  and\n      Zheng, Zengwei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.562/\",\n    doi = \"10.18653/v1/2022.acl-long.562\",\n    pages = \"8164--8175\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.562.pdf",
        "site": "https://aclanthology.org/2022.acl-long.562/",
        "pdf_size": 3408479,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3531647034577134949&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Zhejiang University City College + College of Computer Science and Technology, Zhejiang University; Department of Computer Science, Zhejiang University City College; Department of Computer Science, Zhejiang University City College + College of Computer Science and Technology, Zhejiang University; Department of Computer Science, Zhejiang University City College; Department of Computer Science, Zhejiang University City College",
        "aff_domain": "zju.edu.cn;zucc.edu.cn;zju.edu.cn; ;zucc.edu.cn",
        "email": "zju.edu.cn;zucc.edu.cn;zju.edu.cn; ;zucc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0+1;0;0",
        "aff_unique_norm": "Zhejiang University City College;Zhejiang University",
        "aff_unique_dep": "Department of Computer Science;College of Computer Science and Technology",
        "aff_unique_url": ";http://www.zju.edu.cn",
        "aff_unique_abbr": ";ZJU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.83",
        "title": "Multimodal fusion via cortical network inspired losses",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Information integration from different modalities is an active area of research. Human beings and, in general, biological neural systems are quite adept at using a multitude of signals from different sensory perceptive fields to interact with the environment and each other. Recent work in deep fusion models via neural networks has led to substantial improvements over unimodal approaches in areas like speech recognition, emotion recognition and analysis, captioning and image description. However, such research has mostly focused on architectural changes allowing for fusion of different modalities while keeping the model complexity manageable. Inspired by neuroscientific ideas about multisensory integration and processing, we investigate the effect of introducing neural dependencies in the loss functions. Experiments on multimodal sentiment analysis tasks with different models show that our approach provides a consistent performance boost.",
        "author": "Shiv Shankar",
        "authorids": "/s/shiv-shankar/",
        "bibtex": "@inproceedings{shankar-2022-multimodal,\n    title = \"Multimodal fusion via cortical network inspired losses\",\n    author = \"Shankar, Shiv\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.83/\",\n    doi = \"10.18653/v1/2022.acl-long.83\",\n    pages = \"1167--1178\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.83.pdf",
        "site": "https://aclanthology.org/2022.acl-long.83/",
        "pdf_size": 450524,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16592890341585033852&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "University of Massachusetts",
        "aff_domain": "cics.umass.edu",
        "email": "cics.umass.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.409",
        "title": "Multitasking Framework for Unsupervised Simple Definition Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The definition generation task can help language learners by providing explanations for unfamiliar words. This task has attracted much attention in recent years. We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers. A significant challenge of this task is the lack of learner\u2019s dictionaries in many languages, and therefore the lack of data for supervised training. We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts. We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders. By jointly training these components, the framework can generate both complex and simple definitions simultaneously. We demonstrate that the framework can generate relevant, simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets. Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87%.",
        "author": "Cunliang Kong; Yun Chen; Hengyuan Zhang; Liner Yang; Erhong Yang",
        "authorids": "/c/cunliang-kong/; /y/yun-chen/; /h/hengyuan-zhang/; /l/liner-yang/; /e/erhong-yang/",
        "bibtex": "@inproceedings{kong-etal-2022-multitasking,\n    title = \"Multitasking Framework for Unsupervised Simple Definition Generation\",\n    author = \"Kong, Cunliang  and\n      Chen, Yun  and\n      Zhang, Hengyuan  and\n      Yang, Liner  and\n      Yang, Erhong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.409/\",\n    doi = \"10.18653/v1/2022.acl-long.409\",\n    pages = \"5934--5943\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.409.pdf",
        "site": "https://aclanthology.org/2022.acl-long.409/",
        "pdf_size": 2482235,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9369967308160963821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Information Science, Beijing Language and Culture University + National Language Resources Monitoring and Research Center Print Media Branch, Beijing Language and Culture University + Beijing Advanced Innovation Center for Language Resources, Beijing Language and Culture University; School of Information Management & Engineering, Shanghai University of Finance and Economics; School of Information Science, Beijing Language and Culture University; School of Information Science, Beijing Language and Culture University + National Language Resources Monitoring and Research Center Print Media Branch, Beijing Language and Culture University + Beijing Advanced Innovation Center for Language Resources, Beijing Language and Culture University; School of Information Science, Beijing Language and Culture University + National Language Resources Monitoring and Research Center Print Media Branch, Beijing Language and Culture University + Beijing Advanced Innovation Center for Language Resources, Beijing Language and Culture University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/blcuicall/SimpDefiner",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0+0;1;0;0+0+0;0+0+0",
        "aff_unique_norm": "Beijing Language and Culture University;Shanghai University of Finance and Economics",
        "aff_unique_dep": "School of Information Science;School of Information Management & Engineering",
        "aff_unique_url": "http://www.blcu.edu.cn;http://www.sufe.edu.cn",
        "aff_unique_abbr": ";SUFE",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0;0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.131",
        "title": "N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Augmentation of task-oriented dialogues has followed standard methods used for plain-text such as back-translation, word-level manipulation, and paraphrasing despite its richly annotated structure. In this work, we introduce an augmentation framework that utilizes belief state annotations to match turns from various dialogues and form new synthetic dialogues in a bottom-up manner. Unlike other augmentation strategies, it operates with as few as five examples. Our augmentation strategy yields significant improvements when both adapting a DST model to a new domain, and when adapting a language model to the DST task, on evaluations with TRADE and TOD-BERT models. Further analysis shows that our model performs better on seen values during training, and it is also more robust to unseen values. We conclude that exploiting belief state annotations enhances dialogue augmentation and results in improved models in n-shot training scenarios.",
        "author": "Taha Aksu; Zhengyuan Liu; Min-Yen Kan; Nancy Chen",
        "authorids": "/t/taha-aksu/; /z/zhengyuan-liu/; /m/min-yen-kan/; /n/nancy-chen/",
        "bibtex": "@inproceedings{aksu-etal-2022-n,\n    title = \"N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking\",\n    author = \"Aksu, Taha  and\n      Liu, Zhengyuan  and\n      Kan, Min-Yen  and\n      Chen, Nancy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.131/\",\n    doi = \"10.18653/v1/2022.findings-acl.131\",\n    pages = \"1659--1671\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.131.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.131/",
        "pdf_size": 897817,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6896930404969266055&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "National University of Singapore\u2021Institute for Infocomm Research, A*STAR; Institute for Infocomm Research, A*STAR\u00a7CNRS@CREATE; National University of Singapore; Institute for Infocomm Research, A*STAR",
        "aff_domain": "u.nus.edu; ; ; ",
        "email": "u.nus.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "National University of Singapore;Institute for Infocomm Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nus.edu.sg;https://i2r.a-star.edu.sg",
        "aff_unique_abbr": "NUS;I2R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.findings-acl.42",
        "title": "NEWTS: A Corpus for News Topic-Focused Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Text summarization models are approaching human levels of fidelity. Existing benchmarking corpora provide concordant pairs of full and abridged versions of Web, news or professional content. To date, all summarization datasets operate under a one-size-fits-all paradigm that may not reflect the full range of organic summarization needs. Several recently proposed models (e.g., plug and play language models) have the capacity to condition the generated summaries on a desired range of themes. These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization. This paper introduces the first topical summarization corpus NEWTS, based on the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing. Each source article is paired with two reference summaries, each focusing on a different theme of the source document. We evaluate a representative range of existing techniques and analyze the effectiveness of different prompting methods.",
        "author": "Seyed Ali Bahrainian; Sheridan Feucht; Carsten Eickhoff",
        "authorids": "/s/seyed-ali-bahrainian/; /s/sheridan-feucht/; /c/carsten-eickhoff/",
        "bibtex": "@inproceedings{bahrainian-etal-2022-newts,\n    title = \"{NEWTS}: A Corpus for News Topic-Focused Summarization\",\n    author = \"Bahrainian, Seyed Ali  and\n      Feucht, Sheridan  and\n      Eickhoff, Carsten\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.42/\",\n    doi = \"10.18653/v1/2022.findings-acl.42\",\n    pages = \"493--503\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.42.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.42/",
        "pdf_size": 631355,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11692820319124731313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Brown University, USA + Machine Learning and Optimization Lab, EPFL, Switzerland; Department of Computer Science, Brown University, USA; Department of Computer Science, Brown University, USA",
        "aff_domain": "brown.edu;brown.edu;brown.edu",
        "email": "brown.edu;brown.edu;brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Brown University;EPFL",
        "aff_unique_dep": "Department of Computer Science;Machine Learning and Optimization Lab",
        "aff_unique_url": "https://www.brown.edu;https://www.epfl.ch",
        "aff_unique_abbr": "Brown;EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "2022.acl-long.428",
        "title": "Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Nested named entity recognition (NER) has been receiving increasing attention. Recently, Fu et al. (2020) adapt a span-based constituency parser to tackle nested NER. They treat nested entities as partially-observed constituency trees and propose the masked inside algorithm for partial marginalization. However, their method cannot leverage entity heads, which have been shown useful in entity mention detection and entity typing. In this work, we resort to more expressive structures, lexicalized constituency trees in which constituents are annotated by headwords, to model nested entities. We leverage the Eisner-Satta algorithm to perform partial marginalization and inference efficiently. In addition, we propose to use (1) a two-stage strategy (2) a head regularization loss and (3) a head-aware labeling loss in order to enhance the performance. We make a thorough ablation study to investigate the functionality of each component. Experimentally, our method achieves the state-of-the-art performance on ACE2004, ACE2005 and NNE, and competitive performance on GENIA, and meanwhile has a fast inference speed.",
        "author": "Chao Lou; Songlin Yang; Kewei Tu",
        "authorids": "/c/chao-lou/; /s/songlin-yang/; /k/kewei-tu/",
        "bibtex": "@inproceedings{lou-etal-2022-nested,\n    title = \"Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing\",\n    author = \"Lou, Chao  and\n      Yang, Songlin  and\n      Tu, Kewei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.428/\",\n    doi = \"10.18653/v1/2022.acl-long.428\",\n    pages = \"6183--6198\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.428.pdf",
        "site": "https://aclanthology.org/2022.acl-long.428/",
        "pdf_size": 393870,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11745637378082353412&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Information Science and Technology, ShanghaiTech University; Shanghai Engineering Research Center of Intelligent Vision and Imaging; School of Information Science and Technology, ShanghaiTech University",
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn",
        "email": "shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn",
        "github": "github.com/LouChao98/nner_as_parsing",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "ShanghaiTech University;Shanghai Engineering Research Center of Intelligent Vision and Imaging",
        "aff_unique_dep": "School of Information Science and Technology;",
        "aff_unique_url": "https://www.shanghaitech.edu.cn;",
        "aff_unique_abbr": "ShanghaiTech;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.63",
        "title": "Nested Named Entity Recognition with Span-level Graphs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Span-based methods with the neural networks backbone have great potential for the nested named entity recognition (NER) problem. However, they face problems such as degenerating when positive instances and negative instances largely overlap. Besides, the generalization ability matters a lot in nested NER, as a large proportion of entities in the test set hardly appear in the training set. In this work, we try to improve the span representation by utilizing retrieval-based span-level graphs, connecting spans and entities in the training data based on n-gram features. Specifically, we build the entity-entity graph and span-entity graph globally based on n-gram similarity to integrate the information of similar neighbor entities into the span representation. To evaluate our method, we conduct experiments on three common nested NER datasets, ACE2004, ACE2005, and GENIA datasets. Experimental results show that our method achieves general improvements on all three benchmarks (+0.30 \u223c 0.85 micro-F1), and obtains special superiority on low frequency entities (+0.56 \u223c 2.08 recall).",
        "author": "Juncheng Wan; Dongyu Ru; Weinan Zhang; Yong Yu",
        "authorids": "/j/juncheng-wan/; /d/dongyu-ru/; /w/weinan-zhang/; /y/yong-yu/",
        "bibtex": "@inproceedings{wan-etal-2022-nested,\n    title = \"Nested Named Entity Recognition with Span-level Graphs\",\n    author = \"Wan, Juncheng  and\n      Ru, Dongyu  and\n      Zhang, Weinan  and\n      Yu, Yong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.63/\",\n    doi = \"10.18653/v1/2022.acl-long.63\",\n    pages = \"892--903\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.63.pdf",
        "site": "https://aclanthology.org/2022.acl-long.63/",
        "pdf_size": 773095,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16040864207695311855&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "aff_domain": "apex.sjtu.edu.cn;apex.sjtu.edu.cn;apex.sjtu.edu.cn;apex.sjtu.edu.cn",
        "email": "apex.sjtu.edu.cn;apex.sjtu.edu.cn;apex.sjtu.edu.cn;apex.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.42",
        "title": "Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In zero-shot multilingual extractive text summarization, a model is typically trained on English summarization dataset and then applied on summarization datasets of other languages. Given English gold summaries and documents, sentence-level labels for extractive summarization are usually generated using heuristics. However, these monolingual labels created on English datasets may not be optimal on datasets of other languages, for that there is the syntactic or semantic discrepancy between different languages. In this way, it is possible to translate the English dataset to other languages and obtain different sets of labels again using heuristics. To fully leverage the information of these different sets of labels, we propose NLSSum (Neural Label Search for Summarization), which jointly learns hierarchical weights for these different sets of labels together with our summarization model. We conduct multilingual zero-shot summarization experiments on MLSUM and WikiLingua datasets, and we achieve state-of-the-art results using both human and automatic evaluations across these two datasets.",
        "author": "Ruipeng Jia; Xingxing Zhang; Yanan Cao; Zheng Lin; Shi Wang; Furu Wei",
        "authorids": "/r/ruipeng-jia/; /x/xingxing-zhang/; /y/yanan-cao/; /z/zheng-lin/; /s/shi-wang/; /f/furu-wei/",
        "bibtex": "@inproceedings{jia-etal-2022-neural,\n    title = \"Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization\",\n    author = \"Jia, Ruipeng  and\n      Zhang, Xingxing  and\n      Cao, Yanan  and\n      Lin, Zheng  and\n      Wang, Shi  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.42/\",\n    doi = \"10.18653/v1/2022.acl-long.42\",\n    pages = \"561--570\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.42.pdf",
        "site": "https://aclanthology.org/2022.acl-long.42/",
        "pdf_size": 619776,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7957380181532022541&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Microsoft Research Asia; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, University of Chinese Academy of Sciences; Microsoft Research Asia",
        "aff_domain": "iie.ac.cn;microsoft.com;iie.ac.cn;ict.ac.cn;iie.ac.cn;microsoft.com",
        "email": "iie.ac.cn;microsoft.com;iie.ac.cn;ict.ac.cn;iie.ac.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;0+1;0;0+1;2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Microsoft",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;Research",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "CAS;UCAS;MSR Asia",
        "aff_campus_unique_index": ";1;;;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.390",
        "title": "Neural Machine Translation with Phrase-Level Universal Visual Representations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal machine translation (MMT) aims to improve neural machine translation (NMT) with additional visual information, but most existing MMT methods require paired input of source sentence and image, which makes them suffer from shortage of sentence-image pairs. In this paper, we propose a phrase-level retrieval-based method for MMT to get visual information for the source input from existing sentence-image data sets so that MMT can break the limitation of paired sentence-image input. Our method performs retrieval at the phrase level and hence learns visual information from pairs of source phrase and grounded region, which can mitigate data sparsity. Furthermore, our method employs the conditional variational auto-encoder to learn visual representations which can filter redundant visual information and only retain visual information related to the phrase. Experiments show that the proposed method significantly outperforms strong baselines on multiple MMT datasets, especially when the textual context is limited.",
        "author": "Qingkai Fang; Yang Feng",
        "authorids": "/q/qingkai-fang/; /y/yang-feng/",
        "bibtex": "@inproceedings{fang-feng-2022-neural,\n    title = \"Neural Machine Translation with Phrase-Level Universal Visual Representations\",\n    author = \"Fang, Qingkai  and\n      Feng, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.390/\",\n    doi = \"10.18653/v1/2022.acl-long.390\",\n    pages = \"5687--5698\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.390.pdf",
        "site": "https://aclanthology.org/2022.acl-long.390/",
        "pdf_size": 2839149,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4344661646450670845&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "1Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + 2University of Chinese Academy of Sciences, Beijing, China; 1Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + 2University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/ictnlp/PLUVR",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.ict.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.271",
        "title": "Neural Pipeline for Zero-Shot Data-to-Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise. We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs. Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on general-domain text-based operations: ordering, aggregation, and paragraph compression. We train PLMs for performing these operations on a synthetic corpus WikiFluent which we build from English Wikipedia. Our experiments on two major triple-to-text datasets\u2014WebNLG and E2E\u2014show that our approach enables D2T generation from RDF triples in zero-shot settings.",
        "author": "Zden\u011bk Kasner; Ondrej Dusek",
        "authorids": "/z/zdenek-kasner/; /o/ondrej-dusek/",
        "bibtex": "@inproceedings{kasner-dusek-2022-neural,\n    title = \"Neural Pipeline for Zero-Shot Data-to-Text Generation\",\n    author = \"Kasner, Zden{\\v{e}}k  and\n      Dusek, Ondrej\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.271/\",\n    doi = \"10.18653/v1/2022.acl-long.271\",\n    pages = \"3914--3932\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.271.pdf",
        "site": "https://aclanthology.org/2022.acl-long.271/",
        "pdf_size": 498127,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2482371161203081444&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Charles University, Faculty of Mathematics and Physics; Charles University, Faculty of Mathematics and Physics",
        "aff_domain": "ufal.mff.cuni.cz;ufal.mff.cuni.cz",
        "email": "ufal.mff.cuni.cz;ufal.mff.cuni.cz",
        "github": "https://github.com/kasnerz/zeroshot-d2t-pipeline",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Charles University",
        "aff_unique_dep": "Faculty of Mathematics and Physics",
        "aff_unique_url": "https://www.cuni.cz",
        "aff_unique_abbr": "Charles U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "id": "2022.acl-long.512",
        "title": "Neural reality of argument structure constructions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In lexicalist linguistic theories, argument structure is assumed to be predictable from the meaning of verbs. As a result, the verb is the primary determinant of the meaning of a clause. In contrast, construction grammarians propose that argument structure is encoded in constructions (or form-meaning pairs) that are distinct from verbs. Two decades of psycholinguistic research have produced substantial empirical evidence in favor of the construction view. Here we adapt several psycholinguistic studies to probe for the existence of argument structure constructions (ASCs) in Transformer-based language models (LMs). First, using a sentence sorting experiment, we find that sentences sharing the same construction are closer in embedding space than sentences sharing the same verb. Furthermore, LMs increasingly prefer grouping by construction with more input data, mirroring the behavior of non-native language learners. Second, in a \u201cJabberwocky\u201d priming-based experiment, we find that LMs associate ASCs with meaning, even in semantically nonsensical sentences. Our work offers the first evidence for ASCs in LMs and highlights the potential to devise novel probing methods grounded in psycholinguistic research.",
        "author": "Bai Li; Zining Zhu; Guillaume Thomas; Frank Rudzicz; Yang Xu",
        "authorids": "/b/bai-li/; /z/zining-zhu/; /g/guillaume-thomas/; /f/frank-rudzicz/; /y/yang-xu/",
        "bibtex": "@inproceedings{li-etal-2022-neural,\n    title = \"Neural reality of argument structure constructions\",\n    author = \"Li, Bai  and\n      Zhu, Zining  and\n      Thomas, Guillaume  and\n      Rudzicz, Frank  and\n      Xu, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.512/\",\n    doi = \"10.18653/v1/2022.acl-long.512\",\n    pages = \"7410--7423\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.512.pdf",
        "site": "https://aclanthology.org/2022.acl-long.512/",
        "pdf_size": 525853,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6611980013026655277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Toronto, Department of Computer Science + Vector Institute for Artificial Intelligence + Unity Health Toronto; University of Toronto, Department of Computer Science + Vector Institute for Artificial Intelligence + Unity Health Toronto; University of Toronto, Department of Linguistics; University of Toronto, Department of Computer Science + Vector Institute for Artificial Intelligence + Unity Health Toronto; University of Toronto, Cognitive Science Program + Vector Institute for Artificial Intelligence + Unity Health Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;utoronto.ca;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;utoronto.ca;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;0+1+2;0;0+1+2;0+1+2",
        "aff_unique_norm": "University of Toronto;Vector Institute for Artificial Intelligence;Unity Health Toronto",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": "https://www.utoronto.ca;https://vectorinstitute.ai/;https://www.unityhealth.to",
        "aff_unique_abbr": "U of T;Vector Institute;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0;0+0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.21",
        "title": "New Intent Discovery with Pre-training and Contrastive Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "New intent discovery aims to uncover novel intent categories from user utterances to expand the set of supported intent classes. It is a critical task for the development and service expansion of a practical dialogue system. Despite its importance, this problem remains under-explored in the literature. Existing approaches typically rely on a large amount of labeled utterances and employ pseudo-labeling methods for representation learning and clustering, which are label-intensive, inefficient, and inaccurate. In this paper, we provide new solutions to two important research questions for new intent discovery: (1) how to learn semantic utterance representations and (2) how to better cluster utterances. Particularly, we first propose a multi-task pre-training strategy to leverage rich unlabeled data along with external labeled data for representation learning. Then, we design a new contrastive loss to exploit self-supervisory signals in unlabeled data for clustering. Extensive experiments on three intent recognition benchmarks demonstrate the high effectiveness of our proposed method, which outperforms state-of-the-art methods by a large margin in both unsupervised and semi-supervised scenarios. The source code will be available at https://github.com/zhang-yu-wei/MTP-CLNN.",
        "author": "Yuwei Zhang; Haode Zhang; Li-Ming Zhan; Xiao-Ming Wu; Albert Lam",
        "authorids": "/y/yuwei-zhang/; /h/haode-zhang/; /l/li-ming-zhan/; /x/xiao-ming-wu/; /a/albert-lam/",
        "bibtex": "@inproceedings{zhang-etal-2022-new,\n    title = \"New Intent Discovery with Pre-training and Contrastive Learning\",\n    author = \"Zhang, Yuwei  and\n      Zhang, Haode  and\n      Zhan, Li-Ming  and\n      Wu, Xiao-Ming  and\n      Lam, Albert\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.21/\",\n    doi = \"10.18653/v1/2022.acl-long.21\",\n    pages = \"256--269\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.21.pdf",
        "site": "https://aclanthology.org/2022.acl-long.21/",
        "pdf_size": 3085946,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8033690563369740152&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, San Diego2; Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1+University of California, San Diego2; Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1; Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1\u2020; Fano Labs, Hong Kong S.A.R.3",
        "aff_domain": "gmail.com;connect.polyu.edu.hk;connect.polyu.edu.hk;comp.polyu.edu.hk;fano.ai",
        "email": "gmail.com;connect.polyu.edu.hk;connect.polyu.edu.hk;comp.polyu.edu.hk;fano.ai",
        "github": "https://github.com/zhang-yu-wei/MTP-CLNN",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+0;1;1;2",
        "aff_unique_norm": "University of California, San Diego;Hong Kong Polytechnic University;Fano Labs",
        "aff_unique_dep": ";Department of Computing;",
        "aff_unique_url": "https://ucsd.edu;https://www.polyu.edu.hk;https://www.fanolabs.com",
        "aff_unique_abbr": "UCSD;PolyU;",
        "aff_campus_unique_index": "0;1+0;1;1;1",
        "aff_campus_unique": "San Diego;Hong Kong SAR",
        "aff_country_unique_index": "0;1+0;1;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.324",
        "title": "Nibbling at the Hard Core of Word Sense Disambiguation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With state-of-the-art systems having finally attained estimated human performance, Word Sense Disambiguation (WSD) has now joined the array of Natural Language Processing tasks that have seemingly been solved, thanks to the vast amounts of knowledge encoded into Transformer-based pre-trained language models. And yet, if we look below the surface of raw figures, it is easy to realize that current approaches still make trivial mistakes that a human would never make. In this work, we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on traditional evaluation benchmarks. In addition, we produce and release a collection of test sets featuring (a) an amended version of the standard evaluation benchmark that fixes its lexical and semantic inaccuracies, (b) 42D, a challenge set devised to assess the resilience of systems with respect to least frequent word senses and senses not seen at training time, and (c) hardEN, a challenge set made up solely of instances which none of the investigated state-of-the-art systems can solve. We make all of the test sets and model predictions available to the research community at https://github.com/SapienzaNLP/wsd-hard-benchmark.",
        "author": "Marco Maru; Simone Conia; Michele Bevilacqua; Roberto Navigli",
        "authorids": "/m/marco-maru/; /s/simone-conia/; /m/michele-bevilacqua/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{maru-etal-2022-nibbling,\n    title = \"{N}ibbling at the Hard Core of {W}ord {S}ense {D}isambiguation\",\n    author = \"Maru, Marco  and\n      Conia, Simone  and\n      Bevilacqua, Michele  and\n      Navigli, Roberto\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.324/\",\n    doi = \"10.18653/v1/2022.acl-long.324\",\n    pages = \"4724--4737\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.324.pdf",
        "site": "https://aclanthology.org/2022.acl-long.324/",
        "pdf_size": 367061,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13714496555775145312&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science; Department of Computer Science; Department of Computer Science; Department of Computer, Control and Management Engineering",
        "aff_domain": "uniroma1.it;uniroma1.it;uniroma1.it;uniroma1.it",
        "email": "uniroma1.it;uniroma1.it;uniroma1.it;uniroma1.it",
        "github": "https://github.com/SapienzaNLP/wsd-hard-benchmark",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Unknown Institution;Politecnico di Torino",
        "aff_unique_dep": "Department of Computer Science;Department of Computer, Control and Management Engineering",
        "aff_unique_url": ";https://www.polito.it",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Italy"
    },
    {
        "id": "2022.acl-long.365",
        "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.",
        "author": "Sewon Min; Mike Lewis; Hannaneh Hajishirzi; Luke Zettlemoyer",
        "authorids": "/s/sewon-min/; /m/mike-lewis/; /h/hannaneh-hajishirzi/; /l/luke-zettlemoyer/",
        "bibtex": "@inproceedings{min-etal-2022-noisy,\n    title = \"Noisy Channel Language Model Prompting for Few-Shot Text Classification\",\n    author = \"Min, Sewon  and\n      Lewis, Mike  and\n      Hajishirzi, Hannaneh  and\n      Zettlemoyer, Luke\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.365/\",\n    doi = \"10.18653/v1/2022.acl-long.365\",\n    pages = \"5316--5330\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.365.pdf",
        "site": "https://aclanthology.org/2022.acl-long.365/",
        "pdf_size": 1444141,
        "gs_citation": 221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9624536526451945621&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Washington + Facebook AI Research; Facebook AI Research; University of Washington + Allen Institute for AI; University of Washington + Facebook AI Research",
        "aff_domain": "cs.washington.edu;fb.com;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;fb.com;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0+2;0+1",
        "aff_unique_norm": "University of Washington;Meta;Allen Institute for AI",
        "aff_unique_dep": ";Facebook AI Research;",
        "aff_unique_url": "https://www.washington.edu;https://research.facebook.com;https://allenai.org",
        "aff_unique_abbr": "UW;FAIR;AI2",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.76",
        "title": "NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance. In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before fine-tuning. More specifically, we propose a matrix-wise perturbing method which adds different uniform noises to different parameter matrices based on their standard deviations. In this way, the varied characteristics of different types of parameters in PLMs can be considered. Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks.",
        "author": "Chuhan Wu; Fangzhao Wu; Tao Qi; Yongfeng Huang",
        "authorids": "/c/chuhan-wu/; /f/fangzhao-wu/; /t/tao-qi/; /y/yongfeng-huang/",
        "bibtex": "@inproceedings{wu-etal-2022-noisytune,\n    title = \"{N}oisy{T}une: A Little Noise Can Help You Finetune Pretrained Language Models Better\",\n    author = \"Wu, Chuhan  and\n      Wu, Fangzhao  and\n      Qi, Tao  and\n      Huang, Yongfeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.76/\",\n    doi = \"10.18653/v1/2022.acl-short.76\",\n    pages = \"680--685\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.76.pdf",
        "site": "https://aclanthology.org/2022.acl-short.76/",
        "pdf_size": 1671743,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6960167032631291172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electronic Engineering, Tsinghua University, Beijing 100084, China+Microsoft Research Asia, Beijing 100080, China; Microsoft Research Asia, Beijing 100080, China; Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Department of Electronic Engineering, Tsinghua University, Beijing 100084, China",
        "aff_domain": "gmail.com;gmail.com;gmail.com;tsinghua.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "Tsinghua University;Microsoft",
        "aff_unique_dep": "Department of Electronic Engineering;Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "THU;MSRA",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.380",
        "title": "Non-neural Models Matter: a Re-evaluation of Neural Referring Expression Generation Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In recent years, neural models have often outperformed rule-based and classic Machine Learning approaches in NLG. These classic approaches are now often disregarded, for example when new neural models are evaluated. We argue that they should not be overlooked, since, for some tasks, well-designed non-neural approaches achieve better performance than neural ones. In this paper, the task of generating referring expressions in linguistic context is used as an example. We examined two very different English datasets (WEBNLG and WSJ), and evaluated each algorithm using both automatic and human evaluations. Overall, the results of these evaluations suggest that rule-based systems with simple rule sets achieve on-par or better performance on both datasets compared to state-of-the-art neural REG systems. In the case of the more realistic dataset, WSJ, a machine learning-based system with well-designed linguistic features performed best. We hope that our work can encourage researchers to consider non-neural models in future.",
        "author": "Fahime Same; Guanyi Chen; Kees Van Deemter",
        "authorids": "/f/fahime-same/; /g/guanyi-chen/; /k/kees-van-deemter/",
        "bibtex": "@inproceedings{same-etal-2022-non,\n    title = \"Non-neural Models Matter: a Re-evaluation of Neural Referring Expression Generation Systems\",\n    author = \"Same, Fahime  and\n      Chen, Guanyi  and\n      Van Deemter, Kees\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.380/\",\n    doi = \"10.18653/v1/2022.acl-long.380\",\n    pages = \"5554--5567\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.380.pdf",
        "site": "https://aclanthology.org/2022.acl-long.380/",
        "pdf_size": 304669,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=999667804271734633&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Linguistics, University of Cologne; Department of Information and Computing Sciences, Utrecht University; Department of Information and Computing Sciences, Utrecht University",
        "aff_domain": "uni-koeln.de;uu.nl;uu.nl",
        "email": "uni-koeln.de;uu.nl;uu.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Cologne;Utrecht University",
        "aff_unique_dep": "Department of Linguistics;Department of Information and Computing Sciences",
        "aff_unique_url": "https://www.uni-koeln.de/;https://www.uu.nl",
        "aff_unique_abbr": "UC;UU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Germany;Netherlands"
    },
    {
        "id": "2022.acl-long.272",
        "title": "Not always about you: Prioritizing community needs when developing endangered language technology",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Languages are classified as low-resource when they lack the quantity of data necessary for training statistical and machine learning tools and models. Causes of resource scarcity vary but can include poor access to technology for developing these resources, a relatively small population of speakers, or a lack of urgency for collecting such resources in bilingual populations where the second language is high-resource. As a result, the languages described as low-resource in the literature are as different as Finnish on the one hand, with millions of speakers using it in every imaginable domain, and Seneca, with only a small-handful of fluent speakers using the language primarily in a restricted domain. While issues stemming from the lack of resources necessary to train models unite this disparate group of languages, many other issues cut across the divide between widely-spoken low-resource languages and endangered languages. In this position paper, we discuss the unique technological, cultural, practical, and ethical challenges that researchers and indigenous speech community members face when working together to develop language technology to support endangered language documentation and revitalization. We report the perspectives of language teachers, Master Speakers and elders from indigenous communities, as well as the point of view of academics. We describe an ongoing fruitful collaboration and make recommendations for future partnerships between academic researchers and language community stakeholders.",
        "author": "Zoey Liu; Crystal Richardson; Richard Hatcher; Emily Prud\u2019hommeaux",
        "authorids": "/z/zoey-liu/; /c/crystal-richardson/; /r/richard-hatcher/; /e/emily-prudhommeaux/",
        "bibtex": "@inproceedings{liu-etal-2022-always,\n    title = \"Not always about you: Prioritizing community needs when developing endangered language technology\",\n    author = \"Liu, Zoey  and\n      Richardson, Crystal  and\n      Hatcher, Richard  and\n      Prud{'}hommeaux, Emily\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.272/\",\n    doi = \"10.18653/v1/2022.acl-long.272\",\n    pages = \"3933--3944\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.272.pdf",
        "site": "https://aclanthology.org/2022.acl-long.272/",
        "pdf_size": 196904,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14803418222952164923&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Boston College; University of California, Davis; University at Buffalo; Boston College",
        "aff_domain": "bc.edu;ucdavis.edu;buffalo.edu;bc.edu",
        "email": "bc.edu;ucdavis.edu;buffalo.edu;bc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Boston College;University of California, Davis;University at Buffalo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.bostoncollege.edu;https://www.ucdavis.edu;https://www.buffalo.edu",
        "aff_unique_abbr": "BC;UC Davis;UB",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Davis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.246",
        "title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.",
        "author": "Swaroop Mishra; Arindam Mitra; Neeraj Varshney; Bhavdeep Sachdeva; Peter Clark; Chitta Baral; Ashwin Kalyan",
        "authorids": "/s/swaroop-mishra/; /a/arindam-mitra/; /n/neeraj-varshney/; /b/bhavdeep-sachdeva/; /p/peter-clark/; /c/chitta-baral/; /a/ashwin-kalyan/",
        "bibtex": "@inproceedings{mishra-etal-2022-numglue,\n    title = \"{N}um{GLUE}: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks\",\n    author = \"Mishra, Swaroop  and\n      Mitra, Arindam  and\n      Varshney, Neeraj  and\n      Sachdeva, Bhavdeep  and\n      Clark, Peter  and\n      Baral, Chitta  and\n      Kalyan, Ashwin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.246/\",\n    doi = \"10.18653/v1/2022.acl-long.246\",\n    pages = \"3505--3523\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.246.pdf",
        "site": "https://aclanthology.org/2022.acl-long.246/",
        "pdf_size": 6960814,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11510575605447340660&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Arizona State University; Microsoft Research; Arizona State University; Arizona State University; Allen Institute for AI; Arizona State University; Allen Institute for AI",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;0;2;0;2",
        "aff_unique_norm": "Arizona State University;Microsoft;Allen Institute for AI",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "https://www.asu.edu;https://www.microsoft.com/en-us/research;https://allenai.org",
        "aff_unique_abbr": "ASU;MSR;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.92",
        "title": "OCR Improves Machine Translation for Low-Resource Languages",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We aim to investigate the performance of current OCR systems on low resource languages and low resource scripts. We introduce and make publicly available a novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with noise, for 60 low-resource languages in low resource scripts. We evaluate state-of-the-art OCR systems on our benchmark and analyse most common errors. We show that OCR monolingual data is a valuable resource that can increase performance of Machine Translation models, when used in backtranslation. We then perform an ablation study to investigate how OCR errors impact Machine Translation performance and determine what is the minimum level of OCR quality needed for the monolingual data to be useful for Machine Translation.",
        "author": "Oana Ignat; Jean Maillard; Vishrav Chaudhary; Francisco Guzm\u00e1n",
        "authorids": "/o/oana-ignat/; /j/jean-maillard/; /v/vishrav-chaudhary/; /f/francisco-guzman/",
        "bibtex": "@inproceedings{ignat-etal-2022-ocr,\n    title = \"{OCR} Improves Machine Translation for Low-Resource Languages\",\n    author = \"Ignat, Oana  and\n      Maillard, Jean  and\n      Chaudhary, Vishrav  and\n      Guzm{\\'a}n, Francisco\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.92/\",\n    doi = \"10.18653/v1/2022.findings-acl.92\",\n    pages = \"1164--1174\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.92.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.92/",
        "pdf_size": 825508,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16243804175009394093&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff": "University of Michigan1+Meta AI2; Meta AI2; Microsoft Turing3+Meta AI2; Meta AI2",
        "aff_domain": "umich.edu;maillard.it;microsoft.com;fb.com",
        "email": "umich.edu;maillard.it;microsoft.com;fb.com",
        "github": "",
        "project": "https://pustakalaya.org/en/1164",
        "author_num": 4,
        "aff_unique_index": "0+1;1;2+1;1",
        "aff_unique_norm": "University of Michigan;Meta;Microsoft",
        "aff_unique_dep": ";Meta AI;Turing",
        "aff_unique_url": "https://www.umich.edu;https://meta.ai;https://www.microsoft.com",
        "aff_unique_abbr": "UM;Meta AI;Microsoft",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.571",
        "title": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Residual networks are an Euler discretization of solutions to Ordinary Differential Equations (ODE). This paper explores a deeper relationship between Transformer and numerical ODE methods. We first show that a residual block of layers in Transformer can be described as a higher-order solution to ODE. Inspired by this, we design a new architecture, ODE Transformer, which is analogous to the Runge-Kutta method that is well motivated in ODE. As a natural extension to Transformer, ODE Transformer is easy to implement and efficient to use. Experimental results on the large-scale machine translation, abstractive summarization, and grammar error correction tasks demonstrate the high genericity of ODE Transformer. It can gain large improvements in model performance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the WMT\u201914 English-German and English-French benchmarks) at a slight cost in inference efficiency.",
        "author": "Bei Li; Quan Du; Tao Zhou; Yi Jing; Shuhan Zhou; Xin Zeng; Tong Xiao; JingBo Zhu; Xuebo Liu; Min Zhang",
        "authorids": "/b/bei-li/; /q/quan-du/; /t/tao-zhou/; /y/yi-jing/; /s/shuhan-zhou/; /x/xin-zeng/; /t/tong-xiao/; /j/jingbo-zhu/; /x/xuebo-liu/; /m/min-zhang/",
        "bibtex": "@inproceedings{li-etal-2022-ode,\n    title = \"{ODE} Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation\",\n    author = \"Li, Bei  and\n      Du, Quan  and\n      Zhou, Tao  and\n      Jing, Yi  and\n      Zhou, Shuhan  and\n      Zeng, Xin  and\n      Xiao, Tong  and\n      Zhu, JingBo  and\n      Liu, Xuebo  and\n      Zhang, Min\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.571/\",\n    doi = \"10.18653/v1/2022.acl-long.571\",\n    pages = \"8335--8351\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.571.pdf",
        "site": "https://aclanthology.org/2022.acl-long.571/",
        "pdf_size": 407429,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5377773736820028873&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China + NiuTrans Research, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China + NiuTrans Research, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China + NiuTrans Research, Shenyang, China; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China",
        "aff_domain": "outlook.com;mail.neu.edu.cn;mail.neu.edu.cn;hit.edu.cn;hit.edu.cn; ; ; ; ;",
        "email": "outlook.com;mail.neu.edu.cn;mail.neu.edu.cn;hit.edu.cn;hit.edu.cn; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0+1;0;0;0;0;0+1;0+1;2;2",
        "aff_unique_norm": "Northeastern University;NiuTrans Research;Harbin Institute of Technology",
        "aff_unique_dep": "School of Computer Science and Engineering;;Institute of Computing and Intelligence",
        "aff_unique_url": "http://www.neu.edu.cn/;;http://www.hhit.edu.cn",
        "aff_unique_abbr": "NEU;;HIT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;2;2",
        "aff_campus_unique": "Shenyang;;Shenzhen",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.430",
        "title": "OIE@OIA: an Adaptable and Efficient Open Information Extraction Framework",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Different Open Information Extraction (OIE) tasks require different types of information, so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements. This paper discusses the adaptability problem in existing OIE systems and designs a new adaptable and efficient OIE system - OIE@OIA as a solution. OIE@OIA follows the methodology of Open Information eXpression (OIX): parsing a sentence to an Open Information Annotation (OIA) Graph and then adapting the OIA graph to different OIE tasks with simple rules. As the core of our OIE@OIA system, we implement an end-to-end OIA generator by annotating a dataset (we make it open available) and designing an efficient learning algorithm for the complex OIA graph. We easily adapt the OIE@OIA system to accomplish three popular OIE tasks. The experimental show that our OIE@OIA achieves new SOTA performances on these tasks, showing the great adaptability of our OIE@OIA system. Furthermore, compared to other end-to-end OIE baselines that need millions of samples for training, our OIE@OIA needs much fewer training samples (12K), showing a significant advantage in terms of efficiency.",
        "author": "Xin Wang; Minlong Peng; Mingming Sun; Ping Li",
        "authorids": "/x/xin-wang/; /m/minlong-peng/; /m/mingming-sun/; /p/ping-li/",
        "bibtex": "@inproceedings{wang-etal-2022-oie,\n    title = \"{OIE}@{OIA}: an Adaptable and Efficient Open Information Extraction Framework\",\n    author = \"Wang, Xin  and\n      Peng, Minlong  and\n      Sun, Mingming  and\n      Li, Ping\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.430/\",\n    doi = \"10.18653/v1/2022.acl-long.430\",\n    pages = \"6213--6226\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.430.pdf",
        "site": "https://aclanthology.org/2022.acl-long.430/",
        "pdf_size": 366891,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2373999720000738963&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 2,
        "aff": "Cognitive Computing Lab, Baidu Research, No.10 Xibeiwang East Road, Beijing 100193, China; Cognitive Computing Lab, Baidu Research, No.10 Xibeiwang East Road, Beijing 100193, China; Cognitive Computing Lab, Baidu Research, No.10 Xibeiwang East Road, Beijing 100193, China; Cognitive Computing Lab, Baidu Research, No.10 Xibeiwang East Road, Beijing 100193, China",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Cognitive Computing Lab",
        "aff_unique_url": "https://research.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.223",
        "title": "On Continual Model Refinement in Out-of-Distribution Data Streams",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Real-world natural language processing (NLP) models need to be continually updated to fix the prediction errors in out-of-distribution (OOD) data streams while overcoming catastrophic forgetting. However, existing continual learning (CL) problem setups cannot cover such a realistic and complex scenario. In response to this, we propose a new CL problem formulation dubbed continual model refinement (CMR). Compared to prior CL settings, CMR is more practical and introduces unique challenges (boundary-agnostic and non-stationary distribution shift, diverse mixtures of multiple OOD data clusters, error-centric streams, etc.). We extend several existing CL approaches to the CMR setting and evaluate them extensively. For benchmarking and analysis, we propose a general sampling algorithm to obtain dynamic OOD data streams with controllable non-stationarity, as well as a suite of metrics measuring various aspects of online performance. Our experiments and detailed analysis reveal the promise and challenges of the CMR problem, supporting that studying CMR in dynamic OOD streams can benefit the longevity of deployed NLP models in production.",
        "author": "Bill Yuchen Lin; Sida Wang; Xi Lin; Robin Jia; Lin Xiao; Xiang Ren; Scott Yih",
        "authorids": "/b/bill-yuchen-lin/; /s/sida-i-wang/; /x/xi-lin/; /r/robin-jia/; /l/lin-xiao/; /x/xiang-ren/; /s/scott-yih/",
        "bibtex": "@inproceedings{lin-etal-2022-continual,\n    title = \"On Continual Model Refinement in Out-of-Distribution Data Streams\",\n    author = \"Lin, Bill Yuchen  and\n      Wang, Sida  and\n      Lin, Xi  and\n      Jia, Robin  and\n      Xiao, Lin  and\n      Ren, Xiang  and\n      Yih, Scott\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.223/\",\n    doi = \"10.18653/v1/2022.acl-long.223\",\n    pages = \"3128--3139\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.223.pdf",
        "site": "https://aclanthology.org/2022.acl-long.223/",
        "pdf_size": 1593803,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15159321471882785727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Southern California\u2020Facebook AI Research; Facebook AI Research; Facebook AI Research; University of Southern California\u2020Facebook AI Research; Facebook AI Research; University of Southern California\u2020Facebook AI Research; Facebook AI Research",
        "aff_domain": "usc.edu;fb.com;fb.com;usc.edu;fb.com;usc.edu;fb.com",
        "email": "usc.edu;fb.com;fb.com;usc.edu;fb.com;usc.edu;fb.com",
        "github": "",
        "project": "https://cmr-nlp.github.io/",
        "author_num": 7,
        "aff_unique_index": "0;1;1;0;1;0;1",
        "aff_unique_norm": "University of Southern California;Meta",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.usc.edu;https://research.facebook.com",
        "aff_unique_abbr": "USC;FAIR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.204",
        "title": "On Controlling Fallback Responses for Grounded Dialogue Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Dialogue agents can leverage external textual knowledge to generate responses of a higher quality. To our best knowledge, most existing works on knowledge grounded dialogue settings assume that the user intention is always answerable. Unfortunately, this is impractical as there is no guarantee that the knowledge retrievers could always retrieve the desired knowledge. Therefore, this is crucial to incorporate fallback responses to respond to unanswerable contexts appropriately while responding to the answerable contexts in an informative manner. We propose a novel framework that automatically generates a control token with the generator to bias the succeeding response towards informativeness for answerable contexts and fallback for unanswerable contexts in an end-to-end manner. Since no existing knowledge grounded dialogue dataset considers this aim, we augment the existing dataset with unanswerable contexts to conduct our experiments. Automatic and human evaluation results indicate that naively incorporating fallback responses with controlled text generation still hurts informativeness for answerable context. In contrast, our proposed framework effectively mitigates this problem while still appropriately presenting fallback responses to unanswerable contexts. Such a framework also reduces the extra burden of the additional classifier and the overheads introduced in the previous works, which operates in a pipeline manner.",
        "author": "Hongyuan Lu; Wai Lam; Hong Cheng; Helen Meng",
        "authorids": "/h/hongyuan-lu/; /w/wai-lam/; /h/hong-cheng/; /h/helen-meng/",
        "bibtex": "@inproceedings{lu-etal-2022-controlling,\n    title = \"On Controlling Fallback Responses for Grounded Dialogue Generation\",\n    author = \"Lu, Hongyuan  and\n      Lam, Wai  and\n      Cheng, Hong  and\n      Meng, Helen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.204/\",\n    doi = \"10.18653/v1/2022.findings-acl.204\",\n    pages = \"2591--2601\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.204.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.204/",
        "pdf_size": 482046,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15446572005716889273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "aff_domain": "se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk",
        "email": "se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk",
        "github": "https://github.com/HongyuanLuke/OCFR",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.9",
        "title": "On Efficiently Acquiring Annotations for Multilingual Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "When tasked with supporting multiple languages for a given problem, two approaches have arisen: training a model for each language with the annotation budget divided equally among them, and training on a high-resource language followed by zero-shot transfer to the remaining languages. In this work, we show that the strategy of joint learning across multiple languages using a single model performs substantially better than the aforementioned alternatives. We also demonstrate that active learning provides additional, complementary benefits. We show that this simple approach enables the model to be data efficient by allowing it to arbitrate its annotation budget to query languages it is less certain on. We illustrate the effectiveness of our proposed method on a diverse set of tasks: a classification task with 4 languages, a sequence tagging task with 4 languages and a dependency parsing task with 5 languages. Our proposed method, whilst simple, substantially outperforms the other viable alternatives for building a model in a multilingual setting under constrained budgets.",
        "author": "Joel Ruben Antony Moniz; Barun Patra; Matthew Gormley",
        "authorids": "/j/joel-ruben-antony-moniz/; /b/barun-patra/; /m/matthew-r-gormley/",
        "bibtex": "@inproceedings{moniz-etal-2022-efficiently,\n    title = \"On Efficiently Acquiring Annotations for Multilingual Models\",\n    author = \"Moniz, Joel Ruben Antony  and\n      Patra, Barun  and\n      Gormley, Matthew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.9/\",\n    doi = \"10.18653/v1/2022.acl-short.9\",\n    pages = \"69--85\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.9.pdf",
        "site": "https://aclanthology.org/2022.acl-short.9/",
        "pdf_size": 4668622,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10500153376009459164&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Equal Contribution; Equal Contribution; Carnegie Mellon University",
        "aff_domain": "gmail.com;gmail.com;cs.cmu.edu",
        "email": "gmail.com;gmail.com;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "1",
        "aff_unique_norm": ";Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2022.findings-acl.330",
        "title": "On Length Divergence Bias in Textual Matching Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Despite the remarkable success deep models have achieved in Textual Matching (TM) tasks, it still remains unclear whether they truly understand language or measure the semantic similarity of texts by exploiting statistical bias in datasets. In this work, we provide a new perspective to study this issue \u2014 via the length divergence bias. We find the length divergence heuristic widely exists in prevalent TM datasets, providing direct cues for prediction. To determine whether TM models have adopted such heuristic, we introduce an adversarial evaluation scheme which invalidates the heuristic. In this adversarial setting, all TM models perform worse, indicating they have indeed adopted this heuristic. Through a well-designed probing experiment, we empirically validate that the bias of TM models can be attributed in part to extracting the text length information during training. To alleviate the length divergence bias, we propose an adversarial training method. The results demonstrate we successfully improve the robustness and generalization ability of models at the same time.",
        "author": "Lan Jiang; Tianshu Lyu; Yankai Lin; Meng Chong; Xiaoyong Lyu; Dawei Yin",
        "authorids": "/l/lan-jiang/; /t/tianshu-lyu/; /y/yankai-lin/; /m/meng-chong/; /x/xiaoyong-lyu/; /d/dawei-yin/",
        "bibtex": "@inproceedings{jiang-etal-2022-length,\n    title = \"On Length Divergence Bias in Textual Matching Models\",\n    author = \"Jiang, Lan  and\n      Lyu, Tianshu  and\n      Lin, Yankai  and\n      Chong, Meng  and\n      Lyu, Xiaoyong  and\n      Yin, Dawei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.330/\",\n    doi = \"10.18653/v1/2022.findings-acl.330\",\n    pages = \"4187--4193\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.330.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.330/",
        "pdf_size": 295395,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12624752238812750581&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Automation, Tsinghua University; Baidu Inc., Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc., China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China",
        "aff_domain": "mails.thu.edu;baidu.com;tencent.com;baidu.com;baidu.com;acm.org",
        "email": "mails.thu.edu;baidu.com;tencent.com;baidu.com;baidu.com;acm.org",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;1;1;1",
        "aff_unique_norm": "Tsinghua University;Baidu;Tencent",
        "aff_unique_dep": "Department of Automation;Baidu Inc.;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baidu.com;https://www.tencent.com",
        "aff_unique_abbr": "THU;Baidu;Tencent",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.103",
        "title": "On The Ingredients of an Effective Zero-shot Semantic Parser",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Semantic parsers map natural language utterances into meaning representations (e.g., programs). Such models are typically bottlenecked by the paucity of training data due to the required laborious annotation efforts. Recent studies have performed zero-shot learning by synthesizing training examples of canonical utterances and programs from a grammar, and further paraphrasing these utterances to improve linguistic diversity. However, such synthetic examples cannot fully capture patterns in real data. In this paper we analyze zero-shot parsers through the lenses of the language and logical gaps (Herzig and Berant, 2019), which quantify the discrepancy of language and programmatic patterns between the canonical examples and real-world user-issued ones. We propose bridging these gaps using improved grammars, stronger paraphrasers, and efficient learning methods using canonical examples that most likely reflect real user intents. Our model achieves strong performance on two semantic parsing benchmarks (Scholar, Geo) with zero labeled data.",
        "author": "Pengcheng Yin; John Wieting; Avirup Sil; Graham Neubig",
        "authorids": "/p/pengcheng-yin/; /j/john-wieting/; /a/avirup-sil/; /g/graham-neubig/",
        "bibtex": "@inproceedings{yin-etal-2022-ingredients,\n    title = \"On The Ingredients of an Effective Zero-shot Semantic Parser\",\n    author = \"Yin, Pengcheng  and\n      Wieting, John  and\n      Sil, Avirup  and\n      Neubig, Graham\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.103/\",\n    doi = \"10.18653/v1/2022.acl-long.103\",\n    pages = \"1455--1474\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.103.pdf",
        "site": "https://aclanthology.org/2022.acl-long.103/",
        "pdf_size": 959042,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16177826513121639354&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Carnegie Mellon University\u2660; Google Research\u2663; IBM Research AI\u2666; Carnegie Mellon University\u2660",
        "aff_domain": "cs.cmu.edu;google.com;us.ibm.com;cs.cmu.edu",
        "email": "cs.cmu.edu;google.com;us.ibm.com;cs.cmu.edu",
        "github": "",
        "project": "https://pcyin.me/zeroshot_parser",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Carnegie Mellon University;Google;IBM",
        "aff_unique_dep": ";Google Research;AI",
        "aff_unique_url": "https://www.cmu.edu;https://research.google;https://www.ibm.com/research",
        "aff_unique_abbr": "CMU;Google Research;IBM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.438",
        "title": "On Vision Features in Multimodal Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models. In this work, we investigate the impact of vision models on MMT. Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models (such as Vision Transformer) and enhanced features (such as object-detection and image captioning). We develop a selective attention model to study the patch-level contribution of an image in MMT. On detailed probing tasks, we find that stronger vision models are helpful for learning translation from the visual modality. Our results also suggest the need of carefully examining MMT models, especially when current benchmarks are small-scale and biased.",
        "author": "Bei Li; Chuanhao Lv; Zefan Zhou; Tao Zhou; Tong Xiao; Anxiang Ma; JingBo Zhu",
        "authorids": "/b/bei-li/; /c/chuanhao-lv/; /z/zefan-zhou/; /t/tao-zhou/; /t/tong-xiao/; /a/anxiang-ma/; /j/jingbo-zhu/",
        "bibtex": "@inproceedings{li-etal-2022-vision,\n    title = \"On Vision Features in Multimodal Machine Translation\",\n    author = \"Li, Bei  and\n      Lv, Chuanhao  and\n      Zhou, Zefan  and\n      Zhou, Tao  and\n      Xiao, Tong  and\n      Ma, Anxiang  and\n      Zhu, JingBo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.438/\",\n    doi = \"10.18653/v1/2022.acl-long.438\",\n    pages = \"6327--6337\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.438.pdf",
        "site": "https://aclanthology.org/2022.acl-long.438/",
        "pdf_size": 2182135,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15198534572424801209&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China + NiuTrans Research, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China + NiuTrans Research, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China + NiuTrans Research, Shenyang, China",
        "aff_domain": "outlook.com;outlook.com;outlook.com;outlook.com;mail.neu.edu.cn;mail.neu.edu.cn;mail.neu.edu.cn",
        "email": "outlook.com;outlook.com;outlook.com;outlook.com;mail.neu.edu.cn;mail.neu.edu.cn;mail.neu.edu.cn",
        "github": "https://github.com/libeineu/fairseq_mmt",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0+1;0+1;0+1",
        "aff_unique_norm": "Northeastern University;NiuTrans Research",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": "http://www.neu.edu.cn/;",
        "aff_unique_abbr": "NEU;",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Shenyang;",
        "aff_country_unique_index": "0;0;0;0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.368",
        "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
        "author": "Seo Yeon Park; Cornelia Caragea",
        "authorids": "/s/seo-yeon-park/; /c/cornelia-caragea/",
        "bibtex": "@inproceedings{park-caragea-2022-calibration,\n    title = \"On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency\",\n    author = \"Park, Seo Yeon  and\n      Caragea, Cornelia\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.368/\",\n    doi = \"10.18653/v1/2022.acl-long.368\",\n    pages = \"5364--5374\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.368.pdf",
        "site": "https://aclanthology.org/2022.acl-long.368/",
        "pdf_size": 304886,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=145399636100948701&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science, University of Illinois Chicago; Computer Science, University of Illinois Chicago",
        "aff_domain": "uic.edu;uic.edu",
        "email": "uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois Chicago",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.78",
        "title": "On the Effect of Isotropy on VAE Representations of Text",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Injecting desired geometric properties into text representations has attracted a lot of attention. A property that has been argued for, due to its better utilisation of representation space, is isotropy. In parallel, VAEs have been successful in areas of NLP, but are known for their sub-optimal utilisation of the representation space. To address an aspect of this, we investigate the impact of injecting isotropy during training of VAEs. We achieve this by using an isotropic Gaussian posterior (IGP) instead of the ellipsoidal Gaussian posterior. We illustrate that IGP effectively encourages isotropy in the representations, inducing a more discriminative latent space. Compared to vanilla VAE, this translates into a much better classification performance, robustness to input perturbation, and generative behavior. Additionally, we offer insights about the representational properties encouraged by IGP.",
        "author": "Lan Zhang; Wray Buntine; Ehsan Shareghi",
        "authorids": "/l/lan-zhang/; /w/wray-buntine/; /e/ehsan-shareghi/",
        "bibtex": "@inproceedings{zhang-etal-2022-effect,\n    title = \"On the Effect of Isotropy on {VAE} Representations of Text\",\n    author = \"Zhang, Lan  and\n      Buntine, Wray  and\n      Shareghi, Ehsan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.78/\",\n    doi = \"10.18653/v1/2022.acl-short.78\",\n    pages = \"694--701\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.78.pdf",
        "site": "https://aclanthology.org/2022.acl-short.78/",
        "pdf_size": 1030782,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4440636161796955023&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Data Science & AI, Monash University; Department of Data Science & AI, Monash University + College of Eng. and Comp. Sc., VinUniversity; Department of Data Science & AI, Monash University + Language Technology Lab, University of Cambridge",
        "aff_domain": "monash.edu;monash.edu;monash.edu",
        "email": "monash.edu;monash.edu;monash.edu",
        "github": "https://github.com/lanzhang128/IGPVAE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0+2",
        "aff_unique_norm": "Monash University;VinUniversity;University of Cambridge",
        "aff_unique_dep": "Department of Data Science & AI;College of Engineering and Computer Science;Language Technology Lab",
        "aff_unique_url": "https://www.monash.edu;https://vinuni.edu.vn;https://www.cam.ac.uk",
        "aff_unique_abbr": "Monash;VinUni;Cambridge",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0+1;0+2",
        "aff_country_unique": "Australia;Vietnam;United Kingdom"
    },
    {
        "id": "2022.findings-acl.20",
        "title": "On the Importance of Data Size in Probing Fine-tuned Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Several studies have investigated the reasons behind the effectiveness of fine-tuning, usually through the lens of probing. However, these studies often neglect the role of the size of the dataset on which the model is fine-tuned. In this paper, we highlight the importance of this factor and its undeniable role in probing performance. We show that the extent of encoded linguistic knowledge depends on the number of fine-tuning samples. The analysis also reveals that larger training data mainly affects higher layers, and that the extent of this change is a factor of the number of iterations updating the model during fine-tuning rather than the diversity of the training samples. Finally, we show through a set of experiments that fine-tuning data size affects the recoverability of the changes made to the model\u2019s linguistic knowledge.",
        "author": "Houman Mehrafarin; Sara Rajaee; Mohammad Taher Pilehvar",
        "authorids": "/h/houman-mehrafarin/; /s/sara-rajaee/; /m/mohammad-taher-pilehvar/",
        "bibtex": "@inproceedings{mehrafarin-etal-2022-importance,\n    title = \"On the Importance of Data Size in Probing Fine-tuned Models\",\n    author = \"Mehrafarin, Houman  and\n      Rajaee, Sara  and\n      Pilehvar, Mohammad Taher\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.20/\",\n    doi = \"10.18653/v1/2022.findings-acl.20\",\n    pages = \"228--238\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.20.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.20/",
        "pdf_size": 2631025,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14568874396567833077&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "\u2662Iran University of Science and Technology, Tehran, Iran; \u2662Iran University of Science and Technology, Tehran, Iran; \u2660Tehran Institute for Advanced Studies, Khatam University, Iran",
        "aff_domain": "comp.iust.ac.ir;comp.iust.ac.ir;cam.ac.uk",
        "email": "comp.iust.ac.ir;comp.iust.ac.ir;cam.ac.uk",
        "github": "https://github.com/hmehrafarin/data-size-analysis",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Iran University of Science and Technology;Tehran Institute for Advanced Studies",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iust.ac.ir;",
        "aff_unique_abbr": "IUST;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tehran;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Iran"
    },
    {
        "id": "2022.acl-short.93",
        "title": "On the Importance of Effectively Adapting Pretrained Language Models for Active Learning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Recent active learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.",
        "author": "Katerina Margatina; Loic Barrault; Nikolaos Aletras",
        "authorids": "/k/katerina-margatina/; /l/loic-barrault/; /n/nikolaos-aletras/",
        "bibtex": "@inproceedings{margatina-etal-2022-importance,\n    title = \"On the Importance of Effectively Adapting Pretrained Language Models for Active Learning\",\n    author = \"Margatina, Katerina  and\n      Barrault, Loic  and\n      Aletras, Nikolaos\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.93/\",\n    doi = \"10.18653/v1/2022.acl-short.93\",\n    pages = \"825--836\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.93.pdf",
        "site": "https://aclanthology.org/2022.acl-short.93/",
        "pdf_size": 890507,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11999062651974862561&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Sheffield; University of Le Mans; University of Sheffield",
        "aff_domain": "sheffield.ac.uk;univ-lemans.fr;sheffield.ac.uk",
        "email": "sheffield.ac.uk;univ-lemans.fr;sheffield.ac.uk",
        "github": "https://github.com/mourga/contrastive-active-learning",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Sheffield;University of Le Mans",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.univ-lemans.fr",
        "aff_unique_abbr": "Sheffield;ULM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "id": "2022.acl-short.62",
        "title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.",
        "author": "Yang Trista Cao; Yada Pruksachatkun; Kai-Wei Chang; Rahul Gupta; Varun Kumar; Jwala Dhamala; Aram Galstyan",
        "authorids": "/y/yang-trista-cao/; /y/yada-pruksachatkun/; /k/kai-wei-chang/; /r/rahul-gupta/; /v/varun-kumar/; /j/jwala-dhamala/; /a/aram-galstyan/",
        "bibtex": "@inproceedings{cao-etal-2022-intrinsic,\n    title = \"On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations\",\n    author = \"Cao, Yang Trista  and\n      Pruksachatkun, Yada  and\n      Chang, Kai-Wei  and\n      Gupta, Rahul  and\n      Kumar, Varun  and\n      Dhamala, Jwala  and\n      Galstyan, Aram\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.62/\",\n    doi = \"10.18653/v1/2022.acl-short.62\",\n    pages = \"561--570\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.62.pdf",
        "site": "https://aclanthology.org/2022.acl-short.62/",
        "pdf_size": 1905311,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10889616789968042884&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Maryland, College Park; Amazon Alexa AI-NU; Amazon Alexa AI-NU + University of California, Los Angeles; Amazon Alexa AI-NU; Amazon Alexa AI-NU; Amazon Alexa AI-NU; Information Sciences Institute, University of Southern California",
        "aff_domain": "umd.edu;gmail.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "umd.edu;gmail.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1+2;1;1;1;3",
        "aff_unique_norm": "University of Maryland;Amazon;University of California, Los Angeles;University of Southern California",
        "aff_unique_dep": ";Amazon Alexa AI;;Information Sciences Institute",
        "aff_unique_url": "https://www/umd.edu;https://www.amazon.com;https://www.ucla.edu;https://www.usc.edu",
        "aff_unique_abbr": "UMD;Amazon;UCLA;USC",
        "aff_campus_unique_index": "0;2;2",
        "aff_campus_unique": "College Park;;Los Angeles",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.513",
        "title": "On the Robustness of Offensive Language Classifiers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Social media platforms are deploying machine learning based offensive language classification systems to combat hateful, racist, and other forms of offensive speech at scale. However, despite their real-world deployment, we do not yet comprehensively understand the extent to which offensive language classifiers are robust against adversarial attacks. Prior work in this space is limited to studying robustness of offensive language classifiers against primitive attacks such as misspellings and extraneous spaces. To address this gap, we systematically analyze the robustness of state-of-the-art offensive language classifiers against more crafty adversarial attacks that leverage greedy- and attention-based word selection and context-aware embeddings for word replacement. Our results on multiple datasets show that these crafty adversarial attacks can degrade the accuracy of offensive language classifiers by more than 50% while also being able to preserve the readability and meaning of the modified text.",
        "author": "Jonathan Rusert; Zubair Shafiq; Padmini Srinivasan",
        "authorids": "/j/jonathan-rusert/; /z/zubair-shafiq/; /p/padmini-srinivasan/",
        "bibtex": "@inproceedings{rusert-etal-2022-robustness,\n    title = \"On the Robustness of Offensive Language Classifiers\",\n    author = \"Rusert, Jonathan  and\n      Shafiq, Zubair  and\n      Srinivasan, Padmini\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.513/\",\n    doi = \"10.18653/v1/2022.acl-long.513\",\n    pages = \"7424--7438\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.513.pdf",
        "site": "https://aclanthology.org/2022.acl-long.513/",
        "pdf_size": 298934,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6569736074543550257&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Iowa; University of California, Davis; University of Iowa",
        "aff_domain": "uiowa.edu;ucdavis.edu;uiowa.edu",
        "email": "uiowa.edu;ucdavis.edu;uiowa.edu",
        "github": "https://github.com/JonRusert/RobustnessOfOffensiveClassifiers",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Iowa;University of California, Davis",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uiowa.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "UIowa;UC Davis",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Davis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.149",
        "title": "On the Robustness of Question Rewriting Systems to Questions of Varying Hardness",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In conversational question answering (CQA), the task of question rewriting (QR) in context aims to rewrite a context-dependent question into an equivalent self-contained question that gives the same answer. In this paper, we are interested in the robustness of a QR system to questions varying in rewriting hardness or difficulty. Since there is a lack of questions classified based on their rewriting hardness, we first propose a heuristic method to automatically classify questions into subsets of varying hardness, by measuring the discrepancy between a question and its rewrite. To find out what makes questions hard or easy for rewriting, we then conduct a human evaluation to annotate the rewriting hardness of questions. Finally, to enhance the robustness of QR systems to questions of varying hardness, we propose a novel learning framework for QR that first trains a QR model independently on each subset of questions of a certain level of hardness, then combines these QR models as one joint model for inference. Experimental results on two datasets show that our framework improves the overall performance compared to the baselines.",
        "author": "Hai Ye; Hwee Tou Ng; Wenjuan Han",
        "authorids": "/h/hai-ye/; /h/hwee-tou-ng/; /w/wenjuan-han/",
        "bibtex": "@inproceedings{ye-etal-2022-robustness,\n    title = \"On the Robustness of Question Rewriting Systems to Questions of Varying Hardness\",\n    author = \"Ye, Hai  and\n      Ng, Hwee Tou  and\n      Han, Wenjuan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.149/\",\n    doi = \"10.18653/v1/2022.acl-long.149\",\n    pages = \"2100--2113\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.149.pdf",
        "site": "https://aclanthology.org/2022.acl-long.149/",
        "pdf_size": 587425,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8498664313422394698&as_sdt=5,40&sciodt=0,40&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore; Beijing Institute for General Artificial Intelligence (BIGAI), Beijing, China",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg;bigai.ai",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg;bigai.ai",
        "github": "https://github.com/nusnlp/DiffQRe",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "National University of Singapore;Beijing Institute for General Artificial Intelligence",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.nus.edu.sg;http://www.bigmodel.cn/",
        "aff_unique_abbr": "NUS;BIGAI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2022.findings-acl.308",
        "title": "On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Dialogue safety problems severely limit the real-world deployment of neural conversational models and have attracted great research interests recently. However, dialogue safety problems remain under-defined and the corresponding dataset is scarce. We propose a taxonomy for dialogue safety specifically designed to capture unsafe behaviors in human-bot dialogue settings, with focuses on context-sensitive unsafety, which is under-explored in prior works. To spur research in this direction, we compile DiaSafety, a dataset with rich context-sensitive unsafe examples. Experiments show that existing safety guarding tools fail severely on our dataset. As a remedy, we train a dialogue safety classifier to provide a strong baseline for context-sensitive dialogue unsafety detection. With our classifier, we perform safety evaluations on popular conversational models and show that existing dialogue systems still exhibit concerning context-sensitive safety problems.",
        "author": "Hao Sun; Guangxuan Xu; Jiawen Deng; Jiale Cheng; Chujie Zheng; Hao Zhou; Nanyun Peng; Xiaoyan Zhu; Minlie Huang",
        "authorids": "/h/hao-sun/; /g/guangxuan-xu/; /j/jiawen-deng/; /j/jiale-cheng/; /c/chujie-zheng/; /h/hao-zhou/; /n/nanyun-peng/; /x/xiaoyan-zhu/; /m/minlie-huang/",
        "bibtex": "@inproceedings{sun-etal-2022-safety,\n    title = \"On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark\",\n    author = \"Sun, Hao  and\n      Xu, Guangxuan  and\n      Deng, Jiawen  and\n      Cheng, Jiale  and\n      Zheng, Chujie  and\n      Zhou, Hao  and\n      Peng, Nanyun  and\n      Zhu, Xiaoyan  and\n      Huang, Minlie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.308/\",\n    doi = \"10.18653/v1/2022.findings-acl.308\",\n    pages = \"3906--3923\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.308.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.308/",
        "pdf_size": 622089,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17024566038570300255&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; University of California Los Angeles; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; University of California Los Angeles; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China",
        "aff_domain": "mails.tsinghua.edu.cn;cs.ucla.edu; ; ; ; ;cs.ucla.edu; ;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;cs.ucla.edu; ; ; ; ;cs.ucla.edu; ;tsinghua.edu.cn",
        "github": "https://github.com/thu-coai/DiaSafety",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;2;0+1;0+1;0+1;3;2;0+1;0+1",
        "aff_unique_norm": "Institute for Artificial Intelligence;Tsinghua University;University of California, Los Angeles;Tencent",
        "aff_unique_dep": "CoAI group, DCST, State Key Lab of Intelligent Technology and Systems;Beijing National Research Center for Information Science and Technology;;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": ";https://www.tsinghua.edu.cn;https://www.ucla.edu;https://www.tencent.com",
        "aff_unique_abbr": ";THU;UCLA;Tencent",
        "aff_campus_unique_index": "1;2;1;1;1;2;1;1",
        "aff_campus_unique": ";Beijing;Los Angeles",
        "aff_country_unique_index": "0+0;1;0+0;0+0;0+0;0;1;0+0;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.188",
        "title": "On the Sensitivity and Stability of Model Interpretations in NLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent years have witnessed the emergence of a variety of post-hoc interpretations that aim to uncover how natural language processing (NLP) models make predictions. Despite the surge of new interpretation methods, it remains an open problem how to define and quantitatively measure the faithfulness of interpretations, i.e., to what extent interpretations reflect the reasoning process by a model. We propose two new criteria, sensitivity and stability, that provide complementary notions of faithfulness to the existed removal-based criteria. Our results show that the conclusion for how faithful interpretations are could vary substantially based on different notions. Motivated by the desiderata of sensitivity and stability, we introduce a new class of interpretation methods that adopt techniques from adversarial robustness. Empirical results show that our proposed methods are effective under the new criteria and overcome limitations of gradient-based methods on removal-based criteria. Besides text classification, we also apply interpretation methods and metrics to dependency parsing. Our results shed light on understanding the diverse set of interpretations.",
        "author": "Fan Yin; Zhouxing Shi; Cho-Jui Hsieh; Kai-Wei Chang",
        "authorids": "/f/fan-yin/; /z/zhouxing-shi/; /c/cho-jui-hsieh/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{yin-etal-2022-sensitivity,\n    title = \"On the Sensitivity and Stability of Model Interpretations in {NLP}\",\n    author = \"Yin, Fan  and\n      Shi, Zhouxing  and\n      Hsieh, Cho-Jui  and\n      Chang, Kai-Wei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.188/\",\n    doi = \"10.18653/v1/2022.acl-long.188\",\n    pages = \"2631--2647\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.188.pdf",
        "site": "https://aclanthology.org/2022.acl-long.188/",
        "pdf_size": 396930,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11702185641617560925&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.326",
        "title": "On the data requirements of probing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them. There are many papers with conclusions of the form \u201cobservation X is found in model Y\u201d, using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models.",
        "author": "Zining Zhu; Jixuan Wang; Bai Li; Frank Rudzicz",
        "authorids": "/z/zining-zhu/; /j/jixuan-wang/; /b/bai-li/; /f/frank-rudzicz/",
        "bibtex": "@inproceedings{zhu-etal-2022-data,\n    title = \"On the data requirements of probing\",\n    author = \"Zhu, Zining  and\n      Wang, Jixuan  and\n      Li, Bai  and\n      Rudzicz, Frank\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.326/\",\n    doi = \"10.18653/v1/2022.findings-acl.326\",\n    pages = \"4132--4147\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.326.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.326/",
        "pdf_size": 971756,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12458550178159613648&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 4,
        "aff": "University of Toronto+Vector Institute for Artificial Intelligence; University of Toronto+Vector Institute for Artificial Intelligence; University of Toronto+Vector Institute for Artificial Intelligence; University of Toronto+Vector Institute for Artificial Intelligence+Unity Health Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1+2",
        "aff_unique_norm": "University of Toronto;Vector Institute for Artificial Intelligence;Unity Health Toronto",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;https://vectorinstitute.ai/;https://www.unityhealth.to",
        "aff_unique_abbr": "U of T;Vector Institute;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-short.5",
        "title": "On the probability\u2013quality paradox in language generation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text\u2014covering multiple tasks and common decoding strategies\u2014suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.",
        "author": "Clara Meister; Gian Wiher; Tiago Pimentel; Ryan Cotterell",
        "authorids": "/c/clara-meister/; /g/gian-wiher/; /t/tiago-pimentel/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{meister-etal-2022-high,\n    title = \"On the probability{--}quality paradox in language generation\",\n    author = \"Meister, Clara  and\n      Wiher, Gian  and\n      Pimentel, Tiago  and\n      Cotterell, Ryan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.5/\",\n    doi = \"10.18653/v1/2022.acl-short.5\",\n    pages = \"36--45\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.5.pdf",
        "site": "https://aclanthology.org/2022.acl-short.5/",
        "pdf_size": 2077547,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3307393598806786243&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "ETH Z\u00fcrich; ETH Z\u00fcrich; University of Cambridge; ETH Z\u00fcrich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;cam.ac.uk;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;cam.ac.uk;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "ETH Zurich;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.cam.ac.uk",
        "aff_unique_abbr": "ETHZ;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Switzerland;United Kingdom"
    },
    {
        "id": "2022.findings-acl.257",
        "title": "One Agent To Rule Them All: Towards Multi-agent Conversational AI",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The increasing volume of commercially available conversational agents (CAs) on the market has resulted in users being burdened with learning and adopting multiple agents to accomplish their tasks. Though prior work has explored supporting a multitude of domains within the design of a single agent, the interaction experience suffers due to the large action space of desired capabilities. To address these problems, we introduce a new task BBAI: Black-Box Agent Integration, focusing on combining the capabilities of multiple black-box CAs at scale. We explore two techniques: question agent pairing and question response pairing aimed at resolving this task. Leveraging these techniques, we design One For All (OFA), a scalable system that provides a unified interface to interact with multiple CAs. Additionally, we introduce MARS: Multi-Agent Response Selection, a new encoder model for question response pairing that jointly encodes user question and agent response pairs. We demonstrate that OFA is able to automatically and accurately integrate an ensemble of commercially available CAs spanning disparate domains. Specifically, using the MARS encoder we achieve the highest accuracy on our BBAI task, outperforming strong baselines.",
        "author": "Christopher Clarke; Joseph Peper; Karthik Krishnamurthy; Walter Talamonti; Kevin Leach; Walter Lasecki; Yiping Kang; Lingjia Tang; Jason Mars",
        "authorids": "/c/christopher-clarke/; /j/joseph-j-peper/; /k/karthik-krishnamurthy/; /w/walter-talamonti/; /k/kevin-leach/; /w/walter-lasecki/; /y/yiping-kang/; /l/lingjia-tang/; /j/jason-mars/",
        "bibtex": "@inproceedings{clarke-etal-2022-one,\n    title = \"One Agent To Rule Them All: Towards Multi-agent Conversational {AI}\",\n    author = \"Clarke, Christopher  and\n      Peper, Joseph  and\n      Krishnamurthy, Karthik  and\n      Talamonti, Walter  and\n      Leach, Kevin  and\n      Lasecki, Walter  and\n      Kang, Yiping  and\n      Tang, Lingjia  and\n      Mars, Jason\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.257/\",\n    doi = \"10.18653/v1/2022.findings-acl.257\",\n    pages = \"3258--3267\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.257.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.257/",
        "pdf_size": 822012,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12842253776831752747&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "University of Michigan, Ann Arbor, MI; Ford Motor Company, Dearborn, MI; Vanderbilt University, Nashville, TN; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu;ford.com;ford.com;vanderbilt.edu;gmail.com",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu;ford.com;ford.com;vanderbilt.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;2;0;0;0;0;0;0",
        "aff_unique_norm": "University of Michigan;Ford Motor Company;Vanderbilt University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.umich.edu;https://www.ford.com;https://www.vanderbilt.edu",
        "aff_unique_abbr": "UM;Ford;Vanderbilt",
        "aff_campus_unique_index": "0;1;2;0;0;0;0;0;0",
        "aff_campus_unique": "Ann Arbor;Dearborn;Nashville",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.500",
        "title": "One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects. Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia\u2019s 700+ languages. We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems. Finally, we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.",
        "author": "Alham Fikri Aji; Genta Indra Winata; Fajri Koto; Samuel Cahyawijaya; Ade Romadhony; Rahmad Mahendra; Kemal Kurniawan; David Moeljadi; Radityo Eko Prasojo; Timothy Baldwin; Jey Han Lau; Sebastian Ruder",
        "authorids": "/a/alham-fikri-aji/; /g/genta-indra-winata/; /f/fajri-koto/; /s/samuel-cahyawijaya/; /a/ade-romadhony/; /r/rahmad-mahendra/; /k/kemal-kurniawan/; /d/david-moeljadi/; /r/radityo-eko-prasojo/; /t/timothy-baldwin/; /j/jey-han-lau/; /s/sebastian-ruder/",
        "bibtex": "@inproceedings{aji-etal-2022-one,\n    title = \"One Country, 700+ Languages: {NLP} Challenges for Underrepresented Languages and Dialects in {I}ndonesia\",\n    author = \"Aji, Alham Fikri  and\n      Winata, Genta Indra  and\n      Koto, Fajri  and\n      Cahyawijaya, Samuel  and\n      Romadhony, Ade  and\n      Mahendra, Rahmad  and\n      Kurniawan, Kemal  and\n      Moeljadi, David  and\n      Prasojo, Radityo Eko  and\n      Baldwin, Timothy  and\n      Lau, Jey Han  and\n      Ruder, Sebastian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.500/\",\n    doi = \"10.18653/v1/2022.acl-long.500\",\n    pages = \"7226--7249\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.500.pdf",
        "site": "https://aclanthology.org/2022.acl-long.500/",
        "pdf_size": 654802,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12629024988235239625&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Amazon; Bloomberg; The University of Melbourne; HKUST; Telkom University+INACL; Universitas Indonesia+INACL; The University of Melbourne+INACL; Kanda University of International Studies; Kata.ai; MBZUAI; The University of Melbourne; Google Research",
        "aff_domain": "amazon.co.uk;bloomberg.net;gmail.com; ;cs.ui.ac.id; ; ; ; ; ; ; ",
        "email": "amazon.co.uk;bloomberg.net;gmail.com; ;cs.ui.ac.id; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;1;2;3;4+5;6+5;2+5;7;8;9;2;10",
        "aff_unique_norm": "Amazon;Bloomberg;University of Melbourne;Hong Kong University of Science and Technology;Telkom University;International Conference on Availability, Reliability, and Security;Universitas Indonesia;Kanda University of International Studies;Kata.ai;Mohamed bin Zayed University of Artificial Intelligence;Google",
        "aff_unique_dep": "Amazon.com, Inc.;;;;;;;;;;Google Research",
        "aff_unique_url": "https://www.amazon.com;https://www.bloomberg.com;https://www.unimelb.edu.au;https://www.ust.hk;https://www.telkomuniversity.ac.id;;https://www.ui.ac.id;https://www.kuis.ac.jp;https://www.kata.ai;https://www.mbzuai.ac.ae;https://research.google",
        "aff_unique_abbr": "Amazon;Bloomberg;UniMelb;HKUST;Telkom U;INACL;UI;KUIS;Kata.ai;MBZUAI;Google Research",
        "aff_campus_unique_index": "1;;;;2",
        "aff_campus_unique": ";Hong Kong SAR;Mountain View",
        "aff_country_unique_index": "0;0;1;2;3+4;3+4;1+4;5;3;6;1;0",
        "aff_country_unique": "United States;Australia;China;Indonesia;Unknown;Japan;United Arab Emirates"
    },
    {
        "id": "2022.findings-acl.226",
        "title": "OneAligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Aligning parallel sentences in multilingual corpora is essential to curating data for downstream applications such as Machine Translation. In this work, we present OneAligner, an alignment model specially designed for sentence retrieval tasks. This model is able to train on only one language pair and transfers, in a cross-lingual fashion, to low-resource language pairs with negligible degradation in performance. When trained with all language pairs of a large-scale parallel multilingual corpus (OPUS-100), this model achieves the state-of-the-art result on the Tateoba dataset, outperforming an equally-sized previous model by 8.0 points in accuracy while using less than 0.6% of their parallel data. When finetuned on a single rich-resource language pair, be it English-centered or not, our model is able to match the performance of the ones finetuned on all language pairs under the same data budget with less than 2.0 points decrease in accuracy. Furthermore, with the same setup, scaling up the number of rich-resource language pairs monotonically improves the performance, reaching a minimum of 0.4 points discrepancy in accuracy, making it less mandatory to collect any low-resource parallel data. Finally, we conclude through empirical results and analyses that the performance of the sentence alignment task depends mostly on the monolingual and parallel data size, up to a certain size threshold, rather than on what language pairs are used for training or evaluation.",
        "author": "Tong Niu; Kazuma Hashimoto; Yingbo Zhou; Caiming Xiong",
        "authorids": "/t/tong-niu/; /k/kazuma-hashimoto/; /y/yingbo-zhou/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{niu-etal-2022-onealigner,\n    title = \"{O}ne{A}ligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval\",\n    author = \"Niu, Tong  and\n      Hashimoto, Kazuma  and\n      Zhou, Yingbo  and\n      Xiong, Caiming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.226/\",\n    doi = \"10.18653/v1/2022.findings-acl.226\",\n    pages = \"2869--2882\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.226.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.226/",
        "pdf_size": 366424,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14064305488479783731&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Salesforce Research; Salesforce Research; Salesforce Research; Salesforce Research",
        "aff_domain": "salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "email": "salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Salesforce",
        "aff_unique_dep": "Salesforce Research",
        "aff_unique_url": "https://research.salesforce.com",
        "aff_unique_abbr": "Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.110",
        "title": "Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Standard conversational semantic parsing maps a complete user utterance into an executable program, after which the program is executed to respond to the user. This could be slow when the program contains expensive function calls. We investigate the opportunity to reduce latency by predicting and executing function calls while the user is still speaking. We introduce the task of online semantic parsing for this purpose, with a formal latency reduction metric inspired by simultaneous machine translation. We propose a general framework with first a learned prefix-to-program prediction module, and then a simple yet effective thresholding heuristic for subprogram selection for early execution. Experiments on the SMCalFlow and TreeDST datasets show our approach achieves large latency reduction with good parsing quality, with a 30%\u201365% latency reduction depending on function execution time and allowed cost.",
        "author": "Jiawei Zhou; Jason Eisner; Michael Newman; Emmanouil Antonios Platanios; Sam Thomson",
        "authorids": "/j/jiawei-zhou/; /j/jason-eisner/; /m/michael-newman/; /e/emmanouil-antonios-platanios/; /s/sam-thomson/",
        "bibtex": "@inproceedings{zhou-etal-2022-online,\n    title = \"Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue\",\n    author = \"Zhou, Jiawei  and\n      Eisner, Jason  and\n      Newman, Michael  and\n      Platanios, Emmanouil Antonios  and\n      Thomson, Sam\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.110/\",\n    doi = \"10.18653/v1/2022.acl-long.110\",\n    pages = \"1554--1576\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.110.pdf",
        "site": "https://aclanthology.org/2022.acl-long.110/",
        "pdf_size": 2847278,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10831629968676711293&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Harvard University + Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines",
        "aff_domain": "g.harvard.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "g.harvard.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "Harvard University;Microsoft",
        "aff_unique_dep": ";Semantic Machines",
        "aff_unique_url": "https://www.harvard.edu;https://www.microsoft.com",
        "aff_unique_abbr": "Harvard;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.113",
        "title": "Open Domain Question Answering with A Unified Knowledge Interface",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The retriever-reader framework is popular for open-domain question answering (ODQA) due to its ability to use explicit knowledge. Although prior work has sought to increase the knowledge coverage by incorporating structured knowledge beyond text, accessing heterogeneous knowledge sources through a unified interface remains an open question. While data-to-text generation has the potential to serve as a universal interface for data and text, its feasibility for downstream tasks remains largely unknown. In this work, we bridge this gap and use the data-to-text method as a means for encoding structured knowledge for open-domain question answering. Specifically, we propose a verbalizer-retriever-reader framework for ODQA over data and text where verbalized tables from Wikipedia and graphs from Wikidata are used as augmented knowledge sources. We show that our Unified Data and Text QA, UDT-QA, can effectively benefit from the expanded knowledge index, leading to large gains over text-only baselines. Notably, our approach sets the single-model state-of-the-art on Natural Questions. Furthermore, our analyses indicate that verbalized knowledge is preferred for answer reasoning for both adapted and hot-swap settings.",
        "author": "Kaixin Ma; Hao Cheng; Xiaodong Liu; Eric Nyberg; Jianfeng Gao",
        "authorids": "/k/kaixin-ma/; /h/hao-cheng/; /x/xiaodong-liu/; /e/eric-nyberg/; /j/jianfeng-gao/",
        "bibtex": "@inproceedings{ma-etal-2022-open,\n    title = \"Open Domain Question Answering with A Unified Knowledge Interface\",\n    author = \"Ma, Kaixin  and\n      Cheng, Hao  and\n      Liu, Xiaodong  and\n      Nyberg, Eric  and\n      Gao, Jianfeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.113/\",\n    doi = \"10.18653/v1/2022.acl-long.113\",\n    pages = \"1605--1620\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.113.pdf",
        "site": "https://aclanthology.org/2022.acl-long.113/",
        "pdf_size": 891722,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14310381031104909949&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Carnegie Mellon University+Microsoft Research; Microsoft Research; Microsoft Research; Carnegie Mellon University; Microsoft Research",
        "aff_domain": "cs.cmu.edu;microsoft.com;microsoft.com;cs.cmu.edu;microsoft.com",
        "email": "cs.cmu.edu;microsoft.com;microsoft.com;cs.cmu.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.cmu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CMU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.26",
        "title": "Open Relation Modeling: Learning to Define Relations between Entities",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Relations between entities can be represented by different instances, e.g., a sentence containing both entities or a fact in a Knowledge Graph (KG). However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source. In this paper, we introduce the Open Relation Modeling problem - given two entities, generate a coherent sentence describing the relation between them. To solve this problem, we propose to teach machines to generate definition-like relation descriptions by letting them learn from defining entities. Specifically, we fine-tune Pre-trained Language Models (PLMs) to produce definitions conditioned on extracted entity pairs. To help PLMs reason between entities and provide additional relational knowledge to PLMs for open relation modeling, we incorporate reasoning paths in KGs and include a reasoning path selection mechanism. Experimental results show that our model can generate concise but informative relation descriptions that capture the representative characteristics of entities.",
        "author": "Jie Huang; Kevin Chang; Jinjun Xiong; Wen-mei Hwu",
        "authorids": "/j/jie-huang/; /k/kevin-chang/; /j/jinjun-xiong/; /w/wen-mei-hwu/",
        "bibtex": "@inproceedings{huang-etal-2022-open,\n    title = \"Open Relation Modeling: Learning to Define Relations between Entities\",\n    author = \"Huang, Jie  and\n      Chang, Kevin  and\n      Xiong, Jinjun  and\n      Hwu, Wen-mei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.26/\",\n    doi = \"10.18653/v1/2022.findings-acl.26\",\n    pages = \"297--308\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.26.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.26/",
        "pdf_size": 453086,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13229177805344007402&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; University at Buffalo, USA; University of Illinois at Urbana-Champaign, USA+NVIDIA, USA",
        "aff_domain": "illinois.edu;illinois.edu;buffalo.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;buffalo.edu;illinois.edu",
        "github": "https://github.com/jeffhj/open-relation-modeling",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0+2",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;University at Buffalo;NVIDIA",
        "aff_unique_dep": ";;NVIDIA",
        "aff_unique_url": "https://illinois.edu;https://www.buffalo.edu;https://www.nvidia.com",
        "aff_unique_abbr": "UIUC;UB;NV",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.123",
        "title": "Open Vocabulary Extreme Classification Using Generative Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The extreme multi-label classification (XMC) task aims at tagging content with a subset of labels from an extremely large label set. The label vocabulary is typically defined in advance by domain experts and assumed to capture all necessary tags. However in real world scenarios this label set, although large, is often incomplete and experts frequently need to refine it. To develop systems that simplify this process, we introduce the task of open vocabulary XMC (OXMC): given a piece of content, predict a set of labels, some of which may be outside of the known tag set. Hence, in addition to not having training data for some labels\u2013as is the case in zero-shot classification\u2013models need to invent some labels on-thefly. We propose GROOV, a fine-tuned seq2seq model for OXMC that generates the set of labels as a flat sequence and is trained using a novel loss independent of predicted label order. We show the efficacy of the approach, experimenting with popular XMC datasets for which GROOV is able to predict meaningful labels outside the given vocabulary while performing on par with state-of-the-art solutions for known labels.",
        "author": "Daniel Simig; Fabio Petroni; Pouya Yanki; Kashyap Popat; Christina Du; Sebastian Riedel; Majid Yazdani",
        "authorids": "/d/daniel-simig/; /f/fabio-petroni/; /p/pouya-yanki/; /k/kashyap-popat/; /c/christina-du/; /s/sebastian-riedel/; /m/majid-yazdani/",
        "bibtex": "@inproceedings{simig-etal-2022-open,\n    title = \"Open Vocabulary Extreme Classification Using Generative Models\",\n    author = \"Simig, Daniel  and\n      Petroni, Fabio  and\n      Yanki, Pouya  and\n      Popat, Kashyap  and\n      Du, Christina  and\n      Riedel, Sebastian  and\n      Yazdani, Majid\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.123/\",\n    doi = \"10.18653/v1/2022.findings-acl.123\",\n    pages = \"1561--1583\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.123.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.123/",
        "pdf_size": 1108452,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8191627099070265154&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2022.acl-long.150",
        "title": "OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "AI technologies for Natural Languages have made tremendous progress recently. However, commensurate progress has not been made on Sign Languages, in particular, in recognizing signs as individual words or as complete sentences. We introduce OpenHands, a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign languages for word-level recognition. First, we propose using pose extracted through pretrained models as the standard modality of data in this work to reduce training time and enable efficient inference, and we release standardized pose datasets for different existing sign language datasets. Second, we train and release checkpoints of 4 pose-based isolated sign language recognition models across 6 languages (American, Argentinian, Chinese, Greek, Indian, and Turkish), providing baselines and ready checkpoints for deployment. Third, to address the lack of labelled data, we propose self-supervised pretraining on unlabelled data. We curate and release the largest pose-based pretraining dataset on Indian Sign Language (Indian-SL). Fourth, we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating (a) improved fine-tuning performance especially in low-resource settings, and (b) high crosslingual transfer from Indian-SL to few other sign languages. We open-source all models and datasets in OpenHands with a hope that it makes research in sign languages reproducible and more accessible.",
        "author": "Prem Selvaraj; Gokul Nc; Pratyush Kumar; Mitesh Khapra",
        "authorids": "/p/prem-selvaraj/; /g/gokul-nc/; /p/pratyush-kumar/; /m/mitesh-m-khapra/",
        "bibtex": "@inproceedings{selvaraj-etal-2022-openhands,\n    title = \"{O}pen{H}ands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages\",\n    author = \"Selvaraj, Prem  and\n      Nc, Gokul  and\n      Kumar, Pratyush  and\n      Khapra, Mitesh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.150/\",\n    doi = \"10.18653/v1/2022.acl-long.150\",\n    pages = \"2114--2133\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.150.pdf",
        "site": "https://aclanthology.org/2022.acl-long.150/",
        "pdf_size": 935672,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7118639019938163607&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "AI4Bharat+IIT-Madras; AI4Bharat+IIT-Madras+Microsoft Research; IIT-Madras+Microsoft Research; AI4Bharat+IIT-Madras",
        "aff_domain": "ai4bharat.org;ai4bharat.org;cse.iitm.ac.in;cse.iitm.ac.in",
        "email": "ai4bharat.org;ai4bharat.org;cse.iitm.ac.in;cse.iitm.ac.in",
        "github": "https://github.com/AI4Bharat/OpenHands",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1+2;1+2;0+1",
        "aff_unique_norm": "AI4Bharat;Indian Institute of Technology Madras;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": ";https://www.iitm.ac.in;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": ";IIT-M;MSR",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Chennai",
        "aff_country_unique_index": "0+0;0+0+1;0+1;0+0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2022.acl-long.182",
        "title": "Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Role-oriented dialogue summarization is to generate summaries for different roles in the dialogue, e.g., merchants and consumers. Existing methods handle this task by summarizing each role\u2019s content separately and thus are prone to ignore the information from other roles. However, we believe that other roles\u2019 content could benefit the quality of summaries, such as the omitted information mentioned by other roles. Therefore, we propose a novel role interaction enhanced method for role-oriented dialogue summarization. It adopts cross attention and decoder self-attention interactions to interactively acquire other roles\u2019 critical information. The cross attention interaction aims to select other roles\u2019 critical dialogue utterances, while the decoder self-attention interaction aims to obtain key information from other roles\u2019 summaries. Experimental results have shown that our proposed method significantly outperforms strong baselines on two public role-oriented dialogue summarization datasets. Extensive analyses have demonstrated that other roles\u2019 content could help generate summaries with more complete semantics and correct topic structures.",
        "author": "Haitao Lin; Junnan Zhu; Lu Xiang; Yu Zhou; Jiajun Zhang; Chengqing Zong",
        "authorids": "/h/haitao-lin/; /j/junnan-zhu/; /l/lu-xiang/; /y/yu-zhou/; /j/jiajun-zhang/; /c/chengqing-zong/",
        "bibtex": "@inproceedings{lin-etal-2022-roles,\n    title = \"Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions\",\n    author = \"Lin, Haitao  and\n      Zhu, Junnan  and\n      Xiang, Lu  and\n      Zhou, Yu  and\n      Zhang, Jiajun  and\n      Zong, Chengqing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.182/\",\n    doi = \"10.18653/v1/2022.acl-long.182\",\n    pages = \"2545--2558\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.182.pdf",
        "site": "https://aclanthology.org/2022.acl-long.182/",
        "pdf_size": 911280,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7948963289718661732&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 6,
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+Fanyu AI Laboratory, Zhongke Fanyu Technology Co., Ltd, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "https://github.com/xiaolinAndy/RODS",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;0+1+2;0+1;0+1",
        "aff_unique_norm": "National Laboratory of Pattern Recognition;University of Chinese Academy of Sciences;Zhongke Fanyu Technology Co., Ltd",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;Fanyu AI Laboratory",
        "aff_unique_url": ";http://www.ucas.ac.cn;",
        "aff_unique_abbr": ";UCAS;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.143",
        "title": "Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural networks tend to gradually forget the previously learned knowledge when learning multiple tasks sequentially from dynamic data distributions. This problem is called catastrophic forgetting, which is a fundamental challenge in the continual learning of neural networks. In this work, we observe that catastrophic forgetting not only occurs in continual learning but also affects the traditional static training. Neural networks, especially neural machine translation models, suffer from catastrophic forgetting even if they learn from a static training set. To be specific, the final model pays imbalanced attention to training samples, where recently exposed samples attract more attention than earlier samples. The underlying cause is that training samples do not get balanced training in each model update, so we name this problem imbalanced training. To alleviate this problem, we propose Complementary Online Knowledge Distillation (COKD), which uses dynamically updated teacher models trained on specific data orders to iteratively provide complementary knowledge to the student model. Experimental results on multiple machine translation tasks show that our method successfully alleviates the problem of imbalanced training and achieves substantial improvements over strong baseline systems.",
        "author": "Chenze Shao; Yang Feng",
        "authorids": "/c/chenze-shao/; /y/yang-feng/",
        "bibtex": "@inproceedings{shao-feng-2022-overcoming,\n    title = \"Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation\",\n    author = \"Shao, Chenze  and\n      Feng, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.143/\",\n    doi = \"10.18653/v1/2022.acl-long.143\",\n    pages = \"2023--2036\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.143.pdf",
        "site": "https://aclanthology.org/2022.acl-long.143/",
        "pdf_size": 526537,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7957224281223333001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/ictnlp/COKD",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.527",
        "title": "Overcoming a Theoretical Limitation of Self-Attention",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer\u2019s classification decisions get closer and closer to random guessing (that is, a cross-entropy of 1) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation implied by Hahn\u2019s lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.",
        "author": "David Chiang; Peter Cholak",
        "authorids": "/d/david-chiang/; /p/peter-cholak/",
        "bibtex": "@inproceedings{chiang-cholak-2022-overcoming,\n    title = \"Overcoming a Theoretical Limitation of Self-Attention\",\n    author = \"Chiang, David  and\n      Cholak, Peter\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.527/\",\n    doi = \"10.18653/v1/2022.acl-long.527\",\n    pages = \"7654--7664\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.527.pdf",
        "site": "https://aclanthology.org/2022.acl-long.527/",
        "pdf_size": 521305,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16077772535963093271&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Notre Dame; University of Notre Dame",
        "aff_domain": "nd.edu;nd.edu",
        "email": "nd.edu;nd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Notre Dame",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nd.edu",
        "aff_unique_abbr": "Notre Dame",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.18",
        "title": "Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained multilingual language models such as mBERT and XLM-R have demonstrated great potential for zero-shot cross-lingual transfer to low web-resource languages (LRL). However, due to limited model capacity, the large difference in the sizes of available monolingual corpora between high web-resource languages (HRL) and LRLs does not provide enough scope of co-embedding the LRL with the HRL, thereby affecting the downstream task performance of LRLs. In this paper, we argue that relatedness among languages in a language family along the dimension of lexical overlap may be leveraged to overcome some of the corpora limitations of LRLs. We propose Overlap BPE (OBPE), a simple yet effective modification to the BPE vocabulary generation algorithm which enhances overlap across related languages. Through extensive experiments on multiple NLP tasks and datasets, we observe that OBPE generates a vocabulary that increases the representation of LRLs via tokens shared with HRLs. This results in improved zero-shot transfer from related HRLs to LRLs without reducing HRL representation and accuracy. Unlike previous studies that dismissed the importance of token-overlap, we show that in the low-resource related language setting, token overlap matters. Synthetically reducing the overlap to zero can cause as much as a four-fold drop in zero-shot transfer accuracy.",
        "author": "Vaidehi Patil; Partha Talukdar; Sunita Sarawagi",
        "authorids": "/v/vaidehi-patil/; /p/partha-talukdar/; /s/sunita-sarawagi/",
        "bibtex": "@inproceedings{patil-etal-2022-overlap,\n    title = \"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages\",\n    author = \"Patil, Vaidehi  and\n      Talukdar, Partha  and\n      Sarawagi, Sunita\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.18/\",\n    doi = \"10.18653/v1/2022.acl-long.18\",\n    pages = \"219--233\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.18.pdf",
        "site": "https://aclanthology.org/2022.acl-long.18/",
        "pdf_size": 455915,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17220641198505543468&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Indian Institute of Technology Bombay, India; Google Research, India; Indian Institute of Technology Bombay, India",
        "aff_domain": "gmail.com;google.com;iitb.ac.in",
        "email": "gmail.com;google.com;iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.iitb.ac.in;https://research.google",
        "aff_unique_abbr": "IIT Bombay;Google Research",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Bombay;India",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-short.8",
        "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
        "author": "Xiao Liu; Kaixuan Ji; Yicheng Fu; Weng Tam; Zhengxiao Du; Zhilin Yang; Jie Tang",
        "authorids": "/x/xiao-liu/; /k/kaixuan-ji/; /y/yicheng-fu/; /w/weng-tam/; /z/zhengxiao-du/; /z/zhilin-yang/; /j/jie-tang/",
        "bibtex": "@inproceedings{liu-etal-2022-p,\n    title = \"{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks\",\n    author = \"Liu, Xiao  and\n      Ji, Kaixuan  and\n      Fu, Yicheng  and\n      Tam, Weng  and\n      Du, Zhengxiao  and\n      Yang, Zhilin  and\n      Tang, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.8/\",\n    doi = \"10.18653/v1/2022.acl-short.8\",\n    pages = \"61--68\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.8.pdf",
        "site": "https://aclanthology.org/2022.acl-short.8/",
        "pdf_size": 664873,
        "gs_citation": 1517,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2013484515801163267&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 8,
        "aff": "Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University+Beijing Academy of Artificial Intelligence (BAAI); Tsinghua University+Shanghai Qi Zhi Institute; Tsinghua University+Beijing Academy of Artificial Intelligence (BAAI)",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/THUDM/P-tuning-v2",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0+1;0+2;0+1",
        "aff_unique_norm": "Tsinghua University;Beijing Academy of Artificial Intelligence;Shanghai Qi Zhi Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baai.ac.cn;https://www.qz.io",
        "aff_unique_abbr": "THU;BAAI;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.38",
        "title": "PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Neural models for distantly supervised relation extraction (DS-RE) encode each sentence in an entity-pair bag separately. These are then aggregated for bag-level relation prediction. Since, at encoding time, these approaches do not allow information to flow from other sentences in the bag, we believe that they do not utilize the available bag data to the fullest. In response, we explore a simple baseline approach (PARE) in which all sentences of a bag are concatenated into a passage of sentences, and encoded jointly using BERT. The contextual embeddings of tokens are aggregated using attention with the candidate relation as query \u2013 this summary of whole passage predicts the candidate relation. We find that our simple baseline solution outperforms existing state-of-the-art DS-RE models in both monolingual and multilingual DS-RE datasets.",
        "author": "Vipul Rathore; Kartikeya Badola; Parag Singla; Mausam",
        "authorids": "/v/vipul-rathore/; /k/kartikeya-badola/; /p/parag-singla/; /m/mausam/",
        "bibtex": "@inproceedings{rathore-etal-2022-pare,\n    title = \"{PARE}: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction\",\n    author = \"Rathore, Vipul  and\n      Badola, Kartikeya  and\n      Singla, Parag  and\n      Mausam\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.38/\",\n    doi = \"10.18653/v1/2022.acl-short.38\",\n    pages = \"340--354\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.38.pdf",
        "site": "https://aclanthology.org/2022.acl-short.38/",
        "pdf_size": 690761,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3562393893459704090&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Indian Institute of Technology New Delhi, India; Indian Institute of Technology New Delhi, India; Indian Institute of Technology New Delhi, India; Indian Institute of Technology New Delhi, India",
        "aff_domain": "gmail.com;gmail.com;cse.iitd.ac.in;cse.iitd.ac.in",
        "email": "gmail.com;gmail.com;cse.iitd.ac.in;cse.iitd.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitdelhi.ac.in",
        "aff_unique_abbr": "IIT Delhi",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "New Delhi",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.163",
        "title": "PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow. In this work, we propose PLANET, a novel generation framework leveraging autoregressive self-attention mechanism to conduct content planning and surface realization dynamically. To guide the generation of output sentences, our framework enriches the Transformer decoder with latent representations to maintain sentence-level semantic plans grounded by bag-of-words. Moreover, we introduce a new coherence-based contrastive learning objective to further improve the coherence of output. Extensive experiments are conducted on two challenging long-form text generation tasks including counterargument generation and opinion article generation. Both automatic and human evaluations show that our method significantly outperforms strong baselines and generates more coherent texts with richer contents.",
        "author": "Zhe Hu; Hou Pong Chan; Jiachen Liu; Xinyan Xiao; Hua Wu; Lifu Huang",
        "authorids": "/z/zhe-hu/; /h/hou-pong-chan/; /j/jiachen-liu/; /x/xinyan-xiao/; /h/hua-wu/; /l/lifu-huang/",
        "bibtex": "@inproceedings{hu-etal-2022-planet,\n    title = \"{PLANET}: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation\",\n    author = \"Hu, Zhe  and\n      Chan, Hou Pong  and\n      Liu, Jiachen  and\n      Xiao, Xinyan  and\n      Wu, Hua  and\n      Huang, Lifu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.163/\",\n    doi = \"10.18653/v1/2022.acl-long.163\",\n    pages = \"2288--2305\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.163.pdf",
        "site": "https://aclanthology.org/2022.acl-long.163/",
        "pdf_size": 1401470,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17094949477172138096&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Baidu Inc; Faculty of Science and Technology, University of Macau; Baidu Inc; Baidu Inc; Baidu Inc; Virginia Tech",
        "aff_domain": "baidu.com;um.edu.mo;baidu.com;baidu.com;baidu.com;vt.edu",
        "email": "baidu.com;um.edu.mo;baidu.com;baidu.com;baidu.com;vt.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;2",
        "aff_unique_norm": "Baidu;University of Macau;Virginia Tech",
        "aff_unique_dep": "Baidu;Faculty of Science and Technology;",
        "aff_unique_url": "https://www.baidu.com;https://www.um.edu.mo;https://www.vt.edu",
        "aff_unique_abbr": "Baidu;UM;VT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Macau SAR",
        "aff_country_unique_index": "0;0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.576",
        "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework \u201cPPT\u201d. To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.",
        "author": "Yuxian Gu; Xu Han; Zhiyuan Liu; Minlie Huang",
        "authorids": "/y/yuxian-gu/; /x/xu-han/; /z/zhiyuan-liu/; /m/minlie-huang/",
        "bibtex": "@inproceedings{gu-etal-2022-ppt,\n    title = \"{PPT}: Pre-trained Prompt Tuning for Few-shot Learning\",\n    author = \"Gu, Yuxian  and\n      Han, Xu  and\n      Liu, Zhiyuan  and\n      Huang, Minlie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.576/\",\n    doi = \"10.18653/v1/2022.acl-long.576\",\n    pages = \"8410--8423\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.576.pdf",
        "site": "https://aclanthology.org/2022.acl-long.576/",
        "pdf_size": 569902,
        "gs_citation": 503,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1462935225176605600&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The CoAI group, Tsinghua University, Beijing, China + Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China + Beijing Academy of Artificial Intelligence, BAAI, Beijing, China; The THUNLP group, Tsinghua University, Beijing, China + Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China + Beijing Academy of Artificial Intelligence, BAAI, Beijing, China; The THUNLP group, Tsinghua University, Beijing, China + Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China + Beijing Academy of Artificial Intelligence, BAAI, Beijing, China; The CoAI group, Tsinghua University, Beijing, China + Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China + Beijing Academy of Artificial Intelligence, BAAI, Beijing, China",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/thu-coai/PPT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+1;0+0+1;0+0+1;0+0+1",
        "aff_unique_norm": "Tsinghua University;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "The CoAI group;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baaic.cn",
        "aff_unique_abbr": "THU;BAAI",
        "aff_campus_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.360",
        "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.",
        "author": "Wen Xiao; Iz Beltagy; Giuseppe Carenini; Arman Cohan",
        "authorids": "/w/wen-xiao/; /i/iz-beltagy/; /g/giuseppe-carenini/; /a/arman-cohan/",
        "bibtex": "@inproceedings{xiao-etal-2022-primera,\n    title = \"{PRIMERA}: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\",\n    author = \"Xiao, Wen  and\n      Beltagy, Iz  and\n      Carenini, Giuseppe  and\n      Cohan, Arman\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.360/\",\n    doi = \"10.18653/v1/2022.acl-long.360\",\n    pages = \"5245--5263\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.360.pdf",
        "site": "https://aclanthology.org/2022.acl-long.360/",
        "pdf_size": 780213,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15187856408603230348&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "University of British Columbia; Allen Institute for AI; University of British Columbia; Allen Institute for AI+Paul G. Allen School of Computer Science & Engineering, University of Washington",
        "aff_domain": "cs.ubc.ca;allenai.org;cs.ubc.ca;allenai.org",
        "email": "cs.ubc.ca;allenai.org;cs.ubc.ca;allenai.org",
        "github": "https://github.com/allenai/PRIMER",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1+2",
        "aff_unique_norm": "University of British Columbia;Allen Institute for AI;University of Washington",
        "aff_unique_dep": ";;Paul G. Allen School of Computer Science & Engineering",
        "aff_unique_url": "https://www.ubc.ca;https://allenai.org;https://www.washington.edu",
        "aff_unique_abbr": "UBC;AI2;UW",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;1;0;1+1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.acl-long.337",
        "title": "Packed Levitated Marker for Entity and Relation Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent entity and relation extraction works focus on investigating how to obtain a better span representation from the pre-trained encoder. However, a major limitation of existing works is that they ignore the interrelation between spans (pairs). In this work, we propose a novel span representation approach, named Packed Levitated Markers (PL-Marker), to consider the interrelation between the spans (pairs) by strategically packing the markers in the encoder. In particular, we propose a neighborhood-oriented packing strategy, which considers the neighbor spans integrally to better model the entity boundary information. Furthermore, for those more complicated span pair classification tasks, we design a subject-oriented packing strategy, which packs each subject and all its objects to model the interrelation between the same-subject span pairs. The experimental results show that, with the enhanced marker feature, our model advances baselines on six NER benchmarks, and obtains a 4.1%-4.3% strict relation F1 improvement with higher speed over previous state-of-the-art models on ACE04 and ACE05. Our code and models are publicly available at https://github.com/thunlp/PL-Marker",
        "author": "Deming Ye; Yankai Lin; Peng Li; Maosong Sun",
        "authorids": "/d/deming-ye/; /y/yankai-lin/; /p/peng-li/; /m/maosong-sun/",
        "bibtex": "@inproceedings{ye-etal-2022-packed,\n    title = \"Packed Levitated Marker for Entity and Relation Extraction\",\n    author = \"Ye, Deming  and\n      Lin, Yankai  and\n      Li, Peng  and\n      Sun, Maosong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.337/\",\n    doi = \"10.18653/v1/2022.acl-long.337\",\n    pages = \"4904--4917\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.337.pdf",
        "site": "https://aclanthology.org/2022.acl-long.337/",
        "pdf_size": 396355,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5937060709249853761&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+International Innovation Center of Tsinghua University, Shanghai, China+Jiangsu Collaborative Innovation Center for Language Ability, Xuzhou, China+Institute Guo Qiang, Tsinghua University; Pattern Recognition Center, WeChat AI; Institute for AI Industry Research (AIR), Tsinghua University; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+International Innovation Center of Tsinghua University, Shanghai, China+Jiangsu Collaborative Innovation Center for Language Ability, Xuzhou, China+Institute Guo Qiang, Tsinghua University",
        "aff_domain": "163.com; ; ;tsinghua.edu.cn",
        "email": "163.com; ; ;tsinghua.edu.cn",
        "github": "https://github.com/thunlp/PL-Marker",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+0+2+0;3;0;0+1+0+2+0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Jiangsu Collaborative Innovation Center for Language Ability;WeChat AI",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;;Pattern Recognition Center",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;;https://wwwwechat.com",
        "aff_unique_abbr": "THU;;;WeChat AI",
        "aff_campus_unique_index": "0+2+3;0+2+3",
        "aff_campus_unique": "Beijing;;Shanghai;Xuzhou",
        "aff_country_unique_index": "0+0+0+0+0;0;0;0+0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.469",
        "title": "ParaDetox: Detoxification with Parallel Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources. We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.",
        "author": "Varvara Logacheva; Daryna Dementieva; Sergey Ustyantsev; Daniil Moskovskiy; David Dale; Irina Krotova; Nikita Semenov; Alexander Panchenko",
        "authorids": "/v/varvara-logacheva/; /d/daryna-dementieva/; /s/sergey-ustyantsev/; /d/daniil-moskovskiy/; /d/david-dale/; /i/irina-krotova/; /n/nikita-semenov/; /a/alexander-panchenko/",
        "bibtex": "@inproceedings{logacheva-etal-2022-paradetox,\n    title = \"{P}ara{D}etox: Detoxification with Parallel Data\",\n    author = \"Logacheva, Varvara  and\n      Dementieva, Daryna  and\n      Ustyantsev, Sergey  and\n      Moskovskiy, Daniil  and\n      Dale, David  and\n      Krotova, Irina  and\n      Semenov, Nikita  and\n      Panchenko, Alexander\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.469/\",\n    doi = \"10.18653/v1/2022.acl-long.469\",\n    pages = \"6804--6818\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.469.pdf",
        "site": "https://aclanthology.org/2022.acl-long.469/",
        "pdf_size": 476478,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1885408924581629581&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia + Data and Web Science Group, University of Mannheim, Germany; Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia; Mobile TeleSystems (MTS), Russia; Mobile TeleSystems (MTS), Russia; Skolkovo Institute of Science and Technology, Russia",
        "aff_domain": "skoltech.ru;skoltech.ru;skoltech.ru;skoltech.ru;skoltech.ru;mts.ai;mts.ru;skoltech.ru",
        "email": "skoltech.ru;skoltech.ru;skoltech.ru;skoltech.ru;skoltech.ru;mts.ai;mts.ru;skoltech.ru",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0+1;0;0;0;2;2;0",
        "aff_unique_norm": "Skolkovo Institute of Science and Technology;University of Mannheim;Mobile TeleSystems",
        "aff_unique_dep": ";Data and Web Science Group;",
        "aff_unique_url": "https://www.skoltech.ru;https://www.uni-mannheim.de;",
        "aff_unique_abbr": "Skoltech;;MTS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0;0;0;0;0",
        "aff_country_unique": "Russian Federation;Germany"
    },
    {
        "id": "2022.acl-long.67",
        "title": "Parallel Instance Query Network for Named Entity Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Named entity recognition (NER) is a fundamental task in natural language processing. Recent works treat named entity recognition as a reading comprehension task, constructing type-specific queries manually to extract entities. This paradigm suffers from three issues. First, type-specific queries can only extract one type of entities per inference, which is inefficient. Second, the extraction for different types of entities is isolated, ignoring the dependencies between them. Third, query construction relies on external knowledge and is difficult to apply to realistic scenarios with hundreds of entity types. To deal with them, we propose Parallel Instance Query Network (PIQN), which sets up global and learnable instance queries to extract entities from a sentence in a parallel manner. Each instance query predicts one entity, and by feeding all instance queries simultaneously, we can query all entities in parallel. Instead of being constructed from external knowledge, instance queries can learn their different query semantics during training. For training the model, we treat label assignment as a one-to-many Linear Assignment Problem (LAP) and dynamically assign gold entities to instance queries with minimal assignment cost. Experiments on both nested and flat NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.",
        "author": "Yongliang Shen; Xiaobin Wang; Zeqi Tan; Guangwei Xu; Pengjun Xie; Fei Huang; Weiming Lu; Yueting Zhuang",
        "authorids": "/y/yongliang-shen/; /x/xiaobin-wang/; /z/zeqi-tan/; /g/guangwei-xu/; /p/pengjun-xie/; /f/fei-huang/; /w/weiming-lu/; /y/yueting-zhuang/",
        "bibtex": "@inproceedings{shen-etal-2022-parallel,\n    title = \"Parallel Instance Query Network for Named Entity Recognition\",\n    author = \"Shen, Yongliang  and\n      Wang, Xiaobin  and\n      Tan, Zeqi  and\n      Xu, Guangwei  and\n      Xie, Pengjun  and\n      Huang, Fei  and\n      Lu, Weiming  and\n      Zhuang, Yueting\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.67/\",\n    doi = \"10.18653/v1/2022.acl-long.67\",\n    pages = \"947--961\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.67.pdf",
        "site": "https://aclanthology.org/2022.acl-long.67/",
        "pdf_size": 1362434,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10437009892940793492&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "College of Computer Science and Technology, Zhejiang University+DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; College of Computer Science and Technology, Zhejiang University; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University",
        "aff_domain": "zju.edu.cn;alibaba-inc.com; ; ; ; ;zju.edu.cn; ",
        "email": "zju.edu.cn;alibaba-inc.com; ; ; ; ;zju.edu.cn; ",
        "github": "https://github.com/tricktreat/piqn",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;0;1;1;1;0;0",
        "aff_unique_norm": "Zhejiang University;Alibaba Group",
        "aff_unique_dep": "College of Computer Science and Technology;DAMO Academy",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.alibaba-group.com",
        "aff_unique_abbr": "ZJU;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.347",
        "title": "Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Selecting an appropriate pre-trained model (PTM) for a specific downstream task typically requires significant efforts of fine-tuning. To accelerate this process, researchers propose feature-based model selection (FMS) methods, which assess PTMs\u2019 transferability to a specific task in a fast way without fine-tuning. In this work, we argue that current FMS methods are vulnerable, as the assessment mainly relies on the static features extracted from PTMs. However, such features are derived without training PTMs on downstream tasks, and are not necessarily reliable indicators for the PTM\u2019s transferability. To validate our viewpoints, we design two methods to evaluate the robustness of FMS: (1) model disguise attack, which post-trains an inferior PTM with a contrastive objective, and (2) evaluation data selection, which selects a subset of the data points for FMS evaluation based on K-means clustering. Experimental results prove that both methods can successfully make FMS mistakenly judge the transferability of PTMs. Moreover, we find that these two methods can further be combined with the backdoor attack to misguide the FMS to select poisoned models. To the best of our knowledge, this is the first work to demonstrate the defects of current FMS algorithms and evaluate their potential security risks. By identifying previously unseen risks of FMS, our study indicates new directions for improving the robustness of FMS.",
        "author": "Biru Zhu; Yujia Qin; Fanchao Qi; Yangdong Deng; Zhiyuan Liu; Maosong Sun; Ming Gu",
        "authorids": "/b/biru-zhu/; /y/yujia-qin/; /f/fanchao-qi/; /y/yangdong-deng/; /z/zhiyuan-liu/; /m/maosong-sun/; /m/ming-gu/",
        "bibtex": "@inproceedings{zhu-etal-2022-pass,\n    title = \"Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models\",\n    author = \"Zhu, Biru  and\n      Qin, Yujia  and\n      Qi, Fanchao  and\n      Deng, Yangdong  and\n      Liu, Zhiyuan  and\n      Sun, Maosong  and\n      Gu, Ming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.347/\",\n    doi = \"10.18653/v1/2022.acl-long.347\",\n    pages = \"5060--5072\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.347.pdf",
        "site": "https://aclanthology.org/2022.acl-long.347/",
        "pdf_size": 708608,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12782300613003790093&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 5,
        "aff": "School of Software, Tsinghua University; Department of Computer Science and Technology, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University; School of Software, Tsinghua University; Department of Computer Science and Technology, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University+Beijing Academy of Artificial Intelligence; Department of Computer Science and Technology, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University+Beijing Academy of Artificial Intelligence; School of Software, Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0+1+0;0+1+0;0;0+1+0+0+0+2;0+1+0+0+0+2;0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "School of Software;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.baaic.cn",
        "aff_unique_abbr": "THU;;BAAI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0+0;0+0+0;0;0+0+0+0+0+0;0+0+0+0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.41",
        "title": "Perceiving the World: Question-guided Reinforcement Learning for Text-based Games",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text-based games provide an interactive way to study natural language processing. While deep reinforcement learning has shown effectiveness in developing the game playing agent, the low sample efficiency and the large action space remain to be the two major challenges that hinder the DRL from being applied in the real world. In this paper, we address the challenges by introducing world-perceiving modules, which automatically decompose tasks and prune actions by answering questions about the environment. We then propose a two-phase training framework to decouple language learning from reinforcement learning, which further improves the sample efficiency. The experimental results show that the proposed method significantly improves the performance and sample efficiency. Besides, it shows robustness against compound error and limited pre-training data.",
        "author": "Yunqiu Xu; Meng Fang; Ling Chen; Yali Du; Joey Zhou; Chengqi Zhang",
        "authorids": "/y/yunqiu-xu/; /m/meng-fang/; /l/ling-chen/; /y/yali-du/; /j/joey-zhou/; /c/chengqi-zhang/",
        "bibtex": "@inproceedings{xu-etal-2022-perceiving,\n    title = \"Perceiving the World: Question-guided Reinforcement Learning for Text-based Games\",\n    author = \"Xu, Yunqiu  and\n      Fang, Meng  and\n      Chen, Ling  and\n      Du, Yali  and\n      Zhou, Joey  and\n      Zhang, Chengqi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.41/\",\n    doi = \"10.18653/v1/2022.acl-long.41\",\n    pages = \"538--560\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.41.pdf",
        "site": "https://aclanthology.org/2022.acl-long.41/",
        "pdf_size": 12731249,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2094944165730679436&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 7,
        "aff": "University of Technology Sydney, Sydney, Australia; Eindhoven University of Technology, Eindhoven, the Netherlands; University of Technology Sydney, Sydney, Australia; King\u2019s College London, London, United Kingdom; A*STAR Centre for Frontier AI Research (CFAR), Singapore; University of Technology Sydney, Sydney, Australia",
        "aff_domain": "uts.edu.au;tue.nl;uts.edu.au;kcl.ac.uk;ihpc.a-star.edu.sg;uts.edu.au",
        "email": "uts.edu.au;tue.nl;uts.edu.au;kcl.ac.uk;ihpc.a-star.edu.sg;uts.edu.au",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;2;3;0",
        "aff_unique_norm": "University of Technology Sydney;Eindhoven University of Technology;King's College London;A*STAR Centre for Frontier AI Research",
        "aff_unique_dep": ";;;Centre for Frontier AI Research",
        "aff_unique_url": "https://www.uts.edu.au;https://www.tue.nl/;https://www.kcl.ac.uk;https://www.a-star.edu.sg",
        "aff_unique_abbr": "UTS;TU/e;KCL;A*STAR CFAR",
        "aff_campus_unique_index": "0;1;0;2;0",
        "aff_campus_unique": "Sydney;Eindhoven;London;",
        "aff_country_unique_index": "0;1;0;2;3;0",
        "aff_country_unique": "Australia;Netherlands;United Kingdom;Singapore"
    },
    {
        "id": "2022.findings-acl.232",
        "title": "Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K human-written text perturbations in the wild and leverages them for realistic adversarial attack. Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts. We find that adversarial texts generated by ANTHRO achieve the best trade-off between (1) attack success rate, (2) semantic preservation of the original text, and (3) stealthiness\u2013i.e. indistinguishable from human writings hence harder to be flagged as suspicious. Specifically, our attacks accomplished around 83% and 91% attack success rates on BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger baseline with an increase of 50% and 40% in terms of semantic preservation and stealthiness when evaluated by both layperson and professional human workers. ANTHRO can further enhance a BERT classifier\u2019s performance in understanding different variations of human-written toxic texts via adversarial training when compared to the Perspective API.",
        "author": "Thai Le; Jooyoung Lee; Kevin Yen; Yifan Hu; Dongwon Lee",
        "authorids": "/t/thai-le/; /j/jooyoung-lee/; /k/kevin-yen/; /y/yifan-hu/; /d/dongwon-lee/",
        "bibtex": "@inproceedings{le-etal-2022-perturbations,\n    title = \"Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense\",\n    author = \"Le, Thai  and\n      Lee, Jooyoung  and\n      Yen, Kevin  and\n      Hu, Yifan  and\n      Lee, Dongwon\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.232/\",\n    doi = \"10.18653/v1/2022.findings-acl.232\",\n    pages = \"2953--2965\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.232.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.232/",
        "pdf_size": 3897591,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7412922575317219196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Penn State University; Penn State University; Yahoo Research; Yahoo Research; Penn State University",
        "aff_domain": "psu.edu;psu.edu;yahooinc.com;yahooinc.com;psu.edu",
        "email": "psu.edu;psu.edu;yahooinc.com;yahooinc.com;psu.edu",
        "github": "github.com/lethaiq/perturbations-in-the-wild",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Penn State University;Yahoo",
        "aff_unique_dep": ";Yahoo Research",
        "aff_unique_url": "https://www.psu.edu;https://research.yahoo.com",
        "aff_unique_abbr": "PSU;Yahoo Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.364",
        "title": "Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world\u2019s languages. In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language. Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch. Preprocessing and training code will be uploaded to https://github.com/sil-ai/phone-it-in.",
        "author": "Colin Leong; Daniel Whitenack",
        "authorids": "/c/colin-leong/; /d/daniel-whitenack/",
        "bibtex": "@inproceedings{leong-whitenack-2022-phone,\n    title = \"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data\",\n    author = \"Leong, Colin  and\n      Whitenack, Daniel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.364/\",\n    doi = \"10.18653/v1/2022.acl-long.364\",\n    pages = \"5306--5315\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.364.pdf",
        "site": "https://aclanthology.org/2022.acl-long.364/",
        "pdf_size": 316962,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16254236028163318947&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "University of Dayton; SIL International",
        "aff_domain": "udayton.edu;sil.org",
        "email": "udayton.edu;sil.org",
        "github": "https://github.com/sil-ai/phone-it-in",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Dayton;SIL International",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.udayton.edu;https://www.sil.org",
        "aff_unique_abbr": "UD;SIL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.180",
        "title": "Phoneme transcription of endangered languages: an evaluation of recent ASR architectures in the single speaker scenario",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers. In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. However, when a single speaker is involved, several studies have reported encouraging results for phonetic transcription even with small amounts of training. Here we expand this body of work on speaker-dependent transcription by comparing four ASR approaches, notably recent transformer and pretrained multilingual models, on a common dataset of 11 languages. To automate data preparation, training and evaluation steps, we also developed a phoneme recognition setup which handles morphologically complex languages and writing systems for which no pronunciation dictionary exists. We find that fine-tuning a multilingual pretrained model yields an average phoneme error rate (PER) of 15% for 6 languages with 99 minutes or less of transcribed data for training. For the 5 languages with between 100 and 192 minutes of training, we achieved a PER of 8.4% or less. These results on a number of varied languages suggest that ASR can now significantly reduce transcription efforts in the speaker-dependent situation common in endangered language work.",
        "author": "Gilles Boulianne",
        "authorids": "/g/gilles-boulianne/",
        "bibtex": "@inproceedings{boulianne-2022-phoneme,\n    title = \"Phoneme transcription of endangered languages: an evaluation of recent {ASR} architectures in the single speaker scenario\",\n    author = \"Boulianne, Gilles\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.180/\",\n    doi = \"10.18653/v1/2022.findings-acl.180\",\n    pages = \"2301--2308\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.180.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.180/",
        "pdf_size": 286801,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4994120179285210695&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "CRIM (Centre de recherche informatique de Montr\u00e9al)",
        "aff_domain": "crim.ca",
        "email": "crim.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Centre de Recherche Informatique de Montr\u00e9al",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.crim.ca",
        "aff_unique_abbr": "CRIM",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.444",
        "title": "Phrase-aware Unsupervised Constituency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies have achieved inspiring success in unsupervised grammar induction using masked language modeling (MLM) as the proxy task. Despite their high accuracy in identifying low-level structures, prior arts tend to struggle in capturing high-level structures like clauses, since the MLM task usually only requires information from local context. In this work, we revisit LM-based constituency parsing from a phrase-centered perspective. Inspired by the natural reading process of human, we propose to regularize the parser with phrases extracted by an unsupervised phrase tagger to help the LM model quickly manage low-level structures. For a better understanding of high-level structures, we propose a phrase-guided masking strategy for LM to emphasize more on reconstructing non-phrase words. We show that the initial phrase regularization serves as an effective bootstrap, and phrase-guided masking improves the identification of high-level structures. Experiments on the public benchmark with two different backbone models demonstrate the effectiveness and generality of our method.",
        "author": "Xiaotao Gu; Yikang Shen; Jiaming Shen; Jingbo Shang; Jiawei Han",
        "authorids": "/x/xiaotao-gu/; /y/yikang-shen/; /j/jiaming-shen/; /j/jingbo-shang/; /j/jiawei-han/",
        "bibtex": "@inproceedings{gu-etal-2022-phrase,\n    title = \"Phrase-aware Unsupervised Constituency Parsing\",\n    author = \"Gu, Xiaotao  and\n      Shen, Yikang  and\n      Shen, Jiaming  and\n      Shang, Jingbo  and\n      Han, Jiawei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.444/\",\n    doi = \"10.18653/v1/2022.acl-long.444\",\n    pages = \"6406--6415\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.444.pdf",
        "site": "https://aclanthology.org/2022.acl-long.444/",
        "pdf_size": 2442832,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4548247154704977722&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois at Urbana-Champaign; Mila / Universit\u00e9 de Montr\u00e9al + Pattern Recognition Center, WeChat AI, Tencent Inc; Google Research; University of California San Diego; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;gmail.com;google.com;ucsd.edu;illinois.edu",
        "email": "illinois.edu;gmail.com;google.com;ucsd.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+2;3;4;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Universit\u00e9 de Montr\u00e9al;Tencent;Google;University of California, San Diego",
        "aff_unique_dep": ";Mila;Pattern Recognition Center, WeChat AI;Google Research;",
        "aff_unique_url": "https://illinois.edu;https://www.umontreal.ca;https://www.tencent.com;https://research.google;https://ucsd.edu",
        "aff_unique_abbr": "UIUC;UdeM;Tencent;Google Research;UCSD",
        "aff_campus_unique_index": "0;1;3;4;0",
        "aff_campus_unique": "Urbana-Champaign;Montr\u00e9al;;Mountain View;San Diego",
        "aff_country_unique_index": "0;1+2;0;0;0",
        "aff_country_unique": "United States;Canada;China"
    },
    {
        "id": "2022.acl-short.13",
        "title": "Pixie: Preference in Implicit and Explicit Comparisons",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We present Pixie, a manually annotated dataset for preference classification comprising 8,890 sentences drawn from app reviews. Unlike previous studies on preference classification, Pixie contains implicit (omitting an entity being compared) and indirect (lacking comparative linguistic cues) comparisons. We find that transformer-based pretrained models, finetuned on Pixie, achieve a weighted average F1 score of 83.34% and outperform the existing state-of-the-art preference classification model (73.99%).",
        "author": "Amanul Haque; Vaibhav Garg; Hui Guo; Munindar Singh",
        "authorids": "/a/amanul-haque/; /v/vaibhav-garg/; /h/hui-guo/; /m/munindar-p-singh/",
        "bibtex": "@inproceedings{haque-etal-2022-pixie,\n    title = \"Pixie: Preference in Implicit and Explicit Comparisons\",\n    author = \"Haque, Amanul  and\n      Garg, Vaibhav  and\n      Guo, Hui  and\n      Singh, Munindar\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.13/\",\n    doi = \"10.18653/v1/2022.acl-short.13\",\n    pages = \"106--112\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.13.pdf",
        "site": "https://aclanthology.org/2022.acl-short.13/",
        "pdf_size": 201434,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2107486365124786086&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University",
        "aff_domain": "ncsu.edu;ncsu.edu;ncsu.edu;ncsu.edu",
        "email": "ncsu.edu;ncsu.edu;ncsu.edu;ncsu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "North Carolina State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ncsu.edu",
        "aff_unique_abbr": "NCSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.290",
        "title": "Platt-Bin: Efficient Posterior Calibrated Training for NLP Classifiers",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Modern NLP classifiers are known to return uncalibrated estimations of class posteriors. Existing methods for posterior calibration rescale the predicted probabilities but often have an adverse impact on final classification accuracy, thus leading to poorer generalization. We propose an end-to-end trained calibrator, Platt-Binning, that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities. Our method leverages the sample efficiency of Platt scaling and the verification guarantees of histogram binning, thus not only reducing the calibration error but also improving task performance. In contrast to existing calibrators, we perform this efficient calibration during training. Empirical evaluation of benchmark NLP classification tasks echoes the efficacy of our proposal.",
        "author": "Rishabh Singh; Shirin Goshtasbpour",
        "authorids": "/r/rishabh-singh/; /s/shirin-goshtasbpour/",
        "bibtex": "@inproceedings{singh-goshtasbpour-2022-platt,\n    title = \"{P}latt-Bin: Efficient Posterior Calibrated Training for {NLP} Classifiers\",\n    author = \"Singh, Rishabh  and\n      Goshtasbpour, Shirin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.290/\",\n    doi = \"10.18653/v1/2022.findings-acl.290\",\n    pages = \"3673--3684\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.290.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.290/",
        "pdf_size": 1895268,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:C4ueXV8J7ZkJ:scholar.google.com/&scioq=Platt-Bin:+Efficient+Posterior+Calibrated+Training+for+NLP+Classifiers&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2022.findings-acl.37",
        "title": "Plug-and-Play Adaptation for Continuously-updated QA",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs\u2019 efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task\u2014Continuously-updated QA (CuQA)\u2014in which multiple large-scale updates are made to LMs, and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. We then present LMs with plug-in modules that effectively handle the updates. Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches. We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.",
        "author": "Kyungjae Lee; Wookje Han; Seung-won Hwang; Hwaran Lee; Joonsuk Park; Sang-Woo Lee",
        "authorids": "/k/kyungjae-lee/; /w/wookje-han/; /s/seung-won-hwang/; /h/hwaran-lee/; /j/joonsuk-park/; /s/sang-woo-lee/",
        "bibtex": "@inproceedings{lee-etal-2022-plug,\n    title = \"Plug-and-Play Adaptation for Continuously-updated {QA}\",\n    author = \"Lee, Kyungjae  and\n      Han, Wookje  and\n      Hwang, Seung-won  and\n      Lee, Hwaran  and\n      Park, Joonsuk  and\n      Lee, Sang-Woo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.37/\",\n    doi = \"10.18653/v1/2022.findings-acl.37\",\n    pages = \"438--447\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.37.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.37/",
        "pdf_size": 2772597,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9562597901557655369&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "LG AI Research; Seoul National University; Seoul National University; NA VER AI Lab; NA VER AI Lab+University of Richmond; NA VER AI Lab+NA VER CLOV A",
        "aff_domain": "lge.com;snu.ac.kr;snu.ac.kr;navercorp.com;navercorp.com;navercorp.com",
        "email": "lge.com;snu.ac.kr;snu.ac.kr;navercorp.com;navercorp.com;navercorp.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;2;2+3;2",
        "aff_unique_norm": "LG;Seoul National University;NAVER Corporation;University of Richmond;",
        "aff_unique_dep": "LG AI Research;;AI Lab;;",
        "aff_unique_url": "https://www.lgaires.com;https://www.snu.ac.kr;https://www.naver.com;https://www.richmond.edu;",
        "aff_unique_abbr": "LG AI;SNU;NAVER;UR;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+1;0",
        "aff_country_unique": "South Korea;United States;"
    },
    {
        "id": "2022.findings-acl.6",
        "title": "Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "What can pre-trained multilingual sequence-to-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning data, (3) the amount of pre-training data in the model, (4) the impact of domain mismatch, and (5) language typology. In addition to yielding several heuristics, the experiments form a framework for evaluating the data sensitivities of machine translation systems. While mBART is robust to domain differences, its translations for unseen and typologically distant languages remain below 3.0 BLEU. In answer to our title\u2019s question, mBART is not a low-resource panacea; we therefore encourage shifting the emphasis from new models to new data.",
        "author": "En-Shiun Annie Lee; Sarubi Thillainathan; Shravan Nayak; Surangika Ranathunga; David Ifeoluwa Adelani; Ruisi Su; Arya D. McCarthy",
        "authorids": "/e/en-shiun-annie-lee/; /s/sarubi-thillainathan/; /s/shravan-nayak/; /s/surangika-ranathunga/; /d/david-ifeoluwa-adelani/; /r/ruisi-su/; /a/arya-d-mccarthy/",
        "bibtex": "@inproceedings{lee-etal-2022-pre,\n    title = \"Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?\",\n    author = \"Lee, En-Shiun Annie  and\n      Thillainathan, Sarubi  and\n      Nayak, Shravan  and\n      Ranathunga, Surangika  and\n      Adelani, David Ifeoluwa  and\n      Su, Ruisi  and\n      McCarthy, Arya D.\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.6/\",\n    doi = \"10.18653/v1/2022.findings-acl.6\",\n    pages = \"58--67\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.6.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.6/",
        "pdf_size": 333115,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3682949146677518655&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Toronto; University of Moratuwa; IIT(BHU) Varanasi; University of Moratuwa; Masakhane NLP+Saarland University; Sway AI; John Hopkins University",
        "aff_domain": "cs.toronto.edu; ; ; ; ; ; ",
        "email": "cs.toronto.edu; ; ; ; ; ; ",
        "github": "https://github.com/LRLNMT/LRLNMT",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;1;3+4;5;6",
        "aff_unique_norm": "University of Toronto;University of Moratuwa;Indian Institute of Technology (BHU);Masakhane;Saarland University;Sway AI;Johns Hopkins University",
        "aff_unique_dep": ";;;NLP;;;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.mrt.ac.lk;https://www.iitbhu.ac.in;;https://www.uni-saarland.de;https://www.swayai.com;https://www.jhu.edu",
        "aff_unique_abbr": "U of T;UoM;IIT(BHU);;UdS;Sway AI;JHU",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Varanasi",
        "aff_country_unique_index": "0;1;2;1;3+4;5;5",
        "aff_country_unique": "Canada;Sri Lanka;India;South Africa;Germany;United States"
    },
    {
        "id": "2022.acl-long.413",
        "title": "Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings (PWEs) or pre-trained language models (PLMs) into neural topic modeling. However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead. In this paper, we propose a novel strategy to incorporate external knowledge into neural topic modeling where the neural topic model is pre-trained on a large corpus and then fine-tuned on the target dataset. Experiments have been conducted on three datasets and results show that the proposed approach significantly outperforms both current state-of-the-art neural topic models and some topic modeling approaches enhanced with PWEs or PLMs. Moreover, further study shows that the proposed approach greatly reduces the need for the huge size of training data.",
        "author": "Linhai Zhang; Xuemeng Hu; Boyu Wang; Deyu Zhou; Qian-Wen Zhang; Yunbo Cao",
        "authorids": "/l/linhai-zhang/; /x/xuemeng-hu/; /b/boyu-wang/; /d/deyu-zhou/; /q/qian-wen-zhang/; /y/yunbo-cao/",
        "bibtex": "@inproceedings{zhang-etal-2022-pre,\n    title = \"Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge\",\n    author = \"Zhang, Linhai  and\n      Hu, Xuemeng  and\n      Wang, Boyu  and\n      Zhou, Deyu  and\n      Zhang, Qian-Wen  and\n      Cao, Yunbo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.413/\",\n    doi = \"10.18653/v1/2022.acl-long.413\",\n    pages = \"5980--5989\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.413.pdf",
        "site": "https://aclanthology.org/2022.acl-long.413/",
        "pdf_size": 360726,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10243653915661639687&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China; Tencent Could Xiaowei, Beijing, China; Tencent Could Xiaowei, Beijing, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;tencent.com;tencent.com",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn;seu.edu.cn;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "Southeast University;Tencent",
        "aff_unique_dep": "School of Computer Science and Engineering;Xiaowei",
        "aff_unique_url": "https://www.seu.edu.cn/;https://cloud.tencent.com",
        "aff_unique_abbr": "SEU;Tencent",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.397",
        "title": "Pre-training to Match for Unified Low-shot Relation Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Low-shot relation extraction (RE) aims to recognize novel relations with very few or even no samples, which is critical in real scenario application. Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem to be with similar target but require totally different underlying abilities. In this paper, we propose Multi-Choice Matching Networks to unify low-shot relation extraction. To fill in the gap between zero-shot and few-shot RE, we propose the triplet-paraphrase meta-training, which leverages triplet paraphrase to pre-train zero-shot label matching ability and uses meta-learning paradigm to learn few-shot instance summarizing ability. Experimental results on three different low-shot RE tasks show that the proposed method outperforms strong baselines by a large margin, and achieve the best performance on few-shot RE leaderboard.",
        "author": "Fangchao Liu; Hongyu Lin; Xianpei Han; Boxi Cao; Le Sun",
        "authorids": "/f/fangchao-liu/; /h/hongyu-lin/; /x/xianpei-han/; /b/boxi-cao/; /l/le-sun/",
        "bibtex": "@inproceedings{liu-etal-2022-pre,\n    title = \"Pre-training to Match for Unified Low-shot Relation Extraction\",\n    author = \"Liu, Fangchao  and\n      Lin, Hongyu  and\n      Han, Xianpei  and\n      Cao, Boxi  and\n      Sun, Le\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.397/\",\n    doi = \"10.18653/v1/2022.acl-long.397\",\n    pages = \"5785--5795\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.397.pdf",
        "site": "https://aclanthology.org/2022.acl-long.397/",
        "pdf_size": 534761,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13389385681943018817&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Chinese Information Processing Laboratory+University of Chinese Academy of Sciences; Chinese Information Processing Laboratory; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Beijing Academy of Artificial Intelligence; Chinese Information Processing Laboratory+University of Chinese Academy of Sciences; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science",
        "aff_domain": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "email": "iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn",
        "github": "",
        "project": "https://thunlp.github.io/2/fewrel2_nota.html",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0+2+3;0+1;0+2",
        "aff_unique_norm": "Chinese Information Processing Laboratory;University of Chinese Academy of Sciences;State Key Laboratory of Computer Science;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Information Processing;;;",
        "aff_unique_url": ";http://www.ucas.ac.cn;;https://www.baaic.cn",
        "aff_unique_abbr": ";UCAS;;BAAI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.382",
        "title": "Predicate-Argument Based Bi-Encoder for Paraphrase Identification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Paraphrase identification involves identifying whether a pair of sentences express the same or similar meanings. While cross-encoders have achieved high performances across several benchmarks, bi-encoders such as SBERT have been widely applied to sentence pair tasks. They exhibit substantially lower computation complexity and are better suited to symmetric tasks. In this work, we adopt a bi-encoder approach to the paraphrase identification task, and investigate the impact of explicitly incorporating predicate-argument information into SBERT through weighted aggregation. Experiments on six paraphrase identification datasets demonstrate that, with a minimal increase in parameters, the proposed model is able to outperform SBERT/SRoBERTa significantly. Further, ablation studies reveal that the predicate-argument based component plays a significant role in the performance gain.",
        "author": "Qiwei Peng; David Weir; Julie Weeds; Yekun Chai",
        "authorids": "/q/qiwei-peng/; /d/david-weir/; /j/julie-weeds/; /y/yekun-chai/",
        "bibtex": "@inproceedings{peng-etal-2022-predicate,\n    title = \"Predicate-Argument Based Bi-Encoder for Paraphrase Identification\",\n    author = \"Peng, Qiwei  and\n      Weir, David  and\n      Weeds, Julie  and\n      Chai, Yekun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.382/\",\n    doi = \"10.18653/v1/2022.acl-long.382\",\n    pages = \"5579--5589\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.382.pdf",
        "site": "https://aclanthology.org/2022.acl-long.382/",
        "pdf_size": 391701,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14379708227701590858&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Sussex; University of Sussex; University of Sussex; Baidu, Inc.",
        "aff_domain": "sussex.ac.uk;sussex.ac.uk;sussex.ac.uk;baidu.com",
        "email": "sussex.ac.uk;sussex.ac.uk;sussex.ac.uk;baidu.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Sussex;Baidu",
        "aff_unique_dep": ";Baidu, Inc.",
        "aff_unique_url": "https://www.sussex.ac.uk;https://www.baidu.com",
        "aff_unique_abbr": "Sussex;Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "2022.acl-short.15",
        "title": "Predicting Difficulty and Discrimination of Natural Language Questions",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Item Response Theory (IRT) has been extensively used to numerically characterize question difficulty and discrimination for human subjects in domains including cognitive psychology and education (Primi et al., 2014; Downing, 2003). More recently, IRT has been used to similarly characterize item difficulty and discrimination for natural language models across various datasets (Lalor et al., 2019; Vania et al., 2021; Rodriguez et al., 2021). In this work, we explore predictive models for directly estimating and explaining these traits for natural language questions in a question-answering context. We use HotpotQA for illustration. Our experiments show that it is possible to predict both difficulty and discrimination parameters for new questions, and these traits are correlated with features of questions, answers, and associated contexts. Our findings can have significant implications for the creation of new datasets and tests on the one hand and strategies such as active learning and curriculum learning on the other.",
        "author": "Matthew Byrd; Shashank Srivastava",
        "authorids": "/m/matthew-byrd/; /s/shashank-srivastava/",
        "bibtex": "@inproceedings{byrd-srivastava-2022-predicting,\n    title = \"Predicting Difficulty and Discrimination of Natural Language Questions\",\n    author = \"Byrd, Matthew  and\n      Srivastava, Shashank\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.15/\",\n    doi = \"10.18653/v1/2022.acl-short.15\",\n    pages = \"119--130\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.15.pdf",
        "site": "https://aclanthology.org/2022.acl-short.15/",
        "pdf_size": 3582102,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=878361923728655292&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill",
        "aff_domain": "outlook.com;cs.unc.edu",
        "email": "outlook.com;cs.unc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of North Carolina",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.137",
        "title": "Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Clinical trials offer a fundamental opportunity to discover new treatments and advance the medical knowledge. However, the uncertainty of the outcome of a trial can lead to unforeseen costs and setbacks. In this study, we propose a new method to predict the effectiveness of an intervention in a clinical trial. Our method relies on generating an informative summary from multiple documents available in the literature about the intervention under study. Specifically, our method first gathers all the abstracts of PubMed articles related to the intervention. Then, an evidence sentence, which conveys information about the effectiveness of the intervention, is extracted automatically from each abstract. Based on the set of evidence sentences extracted from the abstracts, a short summary about the intervention is constructed. Finally, the produced summaries are used to train a BERT-based classifier, in order to infer the effectiveness of an intervention. To evaluate our proposed method, we introduce a new dataset which is a collection of clinical trials together with their associated PubMed articles. Our experiments, demonstrate the effectiveness of producing short informative summaries and using them to predict the effectiveness of an intervention.",
        "author": "Georgios Katsimpras; Georgios Paliouras",
        "authorids": "/g/georgios-katsimpras/; /g/georgios-paliouras/",
        "bibtex": "@inproceedings{katsimpras-paliouras-2022-predicting,\n    title = \"Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization\",\n    author = \"Katsimpras, Georgios  and\n      Paliouras, Georgios\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.137/\",\n    doi = \"10.18653/v1/2022.acl-long.137\",\n    pages = \"1947--1957\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.137.pdf",
        "site": "https://aclanthology.org/2022.acl-long.137/",
        "pdf_size": 308914,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1658678185846272845&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "NCSR Demokritos, Athens, Greece; NCSR Demokritos, Athens, Greece",
        "aff_domain": "iit.demokritos.gr;iit.demokritos.gr",
        "email": "iit.demokritos.gr;iit.demokritos.gr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Centre for Scientific Research 'Demokritos'",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.demokritos.gr",
        "aff_unique_abbr": "NCSR Demokritos",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Athens",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Greece"
    },
    {
        "id": "2022.acl-short.28",
        "title": "Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Document-level text simplification often deletes some sentences besides performing lexical, grammatical or structural simplification to reduce text complexity. In this work, we focus on sentence deletions for text simplification and use a news genre-specific functional discourse structure, which categorizes sentences based on their contents and their function roles in telling a news story, for predicting sentence deletion. We incorporate sentence categories into a neural net model in two ways for predicting sentence deletions, either as additional features or by jointly predicting sentence deletions and sentence categories. Experimental results using human-annotated data show that incorporating the functional structure improves the recall of sentence deletion prediction by 6.5% and 10.7% respectively using the two methods, and improves the overall F1-score by 3.6% and 4.3% respectively.",
        "author": "Bohan Zhang; Prafulla Kumar Choubey; Ruihong Huang",
        "authorids": "/b/bohan-zhang/; /p/prafulla-kumar-choubey/; /r/ruihong-huang/",
        "bibtex": "@inproceedings{zhang-etal-2022-predicting,\n    title = \"Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure\",\n    author = \"Zhang, Bohan  and\n      Choubey, Prafulla Kumar  and\n      Huang, Ruihong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.28/\",\n    doi = \"10.18653/v1/2022.acl-short.28\",\n    pages = \"255--261\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.28.pdf",
        "site": "https://aclanthology.org/2022.acl-short.28/",
        "pdf_size": 498899,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4817255497208368480&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "University of Michigan; Salesforce Research; Texas A&M University",
        "aff_domain": "umich.edu;salesforce.com;tamu.edu",
        "email": "umich.edu;salesforce.com;tamu.edu",
        "github": "https://github.com/prafulla77/Discourse_Profiling",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Michigan;Salesforce;Texas A&M University",
        "aff_unique_dep": ";Salesforce Research;",
        "aff_unique_url": "https://www.umich.edu;https://research.salesforce.com;https://www.tamu.edu",
        "aff_unique_abbr": "UM;Salesforce;TAMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.528",
        "title": "Prediction Difference Regularization against Perturbation for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Regularization methods applying input perturbation have drawn considerable attention and have been frequently explored for NMT tasks in recent years. Despite their simplicity and effectiveness, we argue that these methods are limited by the under-fitting of training data. In this paper, we utilize prediction difference for ground-truth tokens to analyze the fitting of token-level samples and find that under-fitting is almost as common as over-fitting. We introduce prediction difference regularization (PD-R), a simple and effective method that can reduce over-fitting and under-fitting at the same time. For all token-level samples, PD-R minimizes the prediction difference between the original pass and the input-perturbed pass, making the model less sensitive to small input changes, thus more robust to both perturbations and under-fitted training data. Experiments on three widely used WMT translation tasks show that our approach can significantly improve over existing perturbation regularization methods. On WMT16 En-De task, our model achieves 1.80 SacreBLEU improvement over vanilla transformer.",
        "author": "Dengji Guo; Zhengrui Ma; Min Zhang; Yang Feng",
        "authorids": "/d/dengji-guo/; /z/zhengrui-ma/; /m/min-zhang/; /y/yang-feng/",
        "bibtex": "@inproceedings{guo-etal-2022-prediction,\n    title = \"Prediction Difference Regularization against Perturbation for Neural Machine Translation\",\n    author = \"Guo, Dengji  and\n      Ma, Zhengrui  and\n      Zhang, Min  and\n      Feng, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.528/\",\n    doi = \"10.18653/v1/2022.acl-long.528\",\n    pages = \"7665--7675\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.528.pdf",
        "site": "https://aclanthology.org/2022.acl-long.528/",
        "pdf_size": 519718,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=811450194213897724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China; Harbin Institute of Technology, Shenzhen, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ict.ac.cn;ict.ac.cn;hit.edu.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;hit.edu.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Harbin Institute of Technology",
        "aff_unique_dep": "Institute of Computing Technology;;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;http://en.hhit.edu.cn/",
        "aff_unique_abbr": "CAS;UCAS;HIT",
        "aff_campus_unique_index": "1;1;2;1",
        "aff_campus_unique": ";Beijing;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.66",
        "title": "Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "It is a common practice for recent works in vision language cross-modal reasoning to adopt a binary or multi-choice classification formulation taking as input a set of source image(s) and textual query. In this work, we take a sober look at such an \u201cunconditional\u201d formulation in the sense that no prior knowledge is specified with respect to the source image(s). Inspired by the designs of both visual commonsense reasoning and natural language inference tasks, we propose a new task termed \u201cPremise-based Multi-modal Reasoning\u201d (PMR) where a textual premise is the background presumption on each source image. The PMR dataset contains 15,360 manually annotated samples which are created by a multi-phase crowd-sourcing process. With selected high-quality movie screenshots and human-curated premise templates from 6 pre-defined categories, we ask crowd-source workers to write one true hypothesis and three distractors (4 choices) given the premise and image through a cross-check procedure.",
        "author": "Qingxiu Dong; Ziwei Qin; Heming Xia; Tian Feng; Shoujie Tong; Haoran Meng; Lin Xu; Zhongyu Wei; Weidong Zhan; Baobao Chang; Sujian Li; Tianyu Liu; Zhifang Sui",
        "authorids": "/q/qingxiu-dong/; /z/ziwei-qin/; /h/heming-xia/; /t/tian-feng/; /s/shoujie-tong/; /h/haoran-meng/; /l/lin-xu/; /z/zhongyu-wei/; /w/weidong-zhan/; /b/baobao-chang/; /s/sujian-li/; /t/tianyu-liu/; /z/zhifang-sui/",
        "bibtex": "@inproceedings{dong-etal-2022-premise,\n    title = \"Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues\",\n    author = \"Dong, Qingxiu  and\n      Qin, Ziwei  and\n      Xia, Heming  and\n      Feng, Tian  and\n      Tong, Shoujie  and\n      Meng, Haoran  and\n      Xu, Lin  and\n      Wei, Zhongyu  and\n      Zhan, Weidong  and\n      Chang, Baobao  and\n      Li, Sujian  and\n      Liu, Tianyu  and\n      Sui, Zhifang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.66/\",\n    doi = \"10.18653/v1/2022.acl-long.66\",\n    pages = \"932--946\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.66.pdf",
        "site": "https://aclanthology.org/2022.acl-long.66/",
        "pdf_size": 1489647,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16085761398638387674&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; School of Data Science, Fudan University; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Key Laboratory of Computational Linguistics, Peking University, MOE, China; Tencent Cloud Xiaowei; Key Laboratory of Computational Linguistics, Peking University, MOE, China",
        "aff_domain": "stu.pku.edu.cn;stu.pku.edu.cn; ; ; ; ; ; ; ; ; ; ; ",
        "email": "stu.pku.edu.cn;stu.pku.edu.cn; ; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://2030nlp.github.io/PMR/",
        "author_num": 13,
        "aff_unique_index": "0;0;0;0;0;0;0;1;0;0;0;2;0",
        "aff_unique_norm": "Peking University;Fudan University;Tencent",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics;School of Data Science;Tencent Cloud Xiaowei",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.fudan.edu.cn;https://cloud.tencent.com",
        "aff_unique_abbr": "PKU;Fudan;Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.504",
        "title": "Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language. We design artificial languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language.Our experimental results show that pretraining with an artificial language with a nesting dependency structure provides some knowledge transferable to natural language.A follow-up probing analysis indicates that its success in the transfer is related to the amount of encoded contextual information and what is transferred is the knowledge of position-aware context dependence of language.Our results provide insights into how neural network encoders process human languages and the source of cross-lingual transferability of recent multilingual language models.",
        "author": "Ryokan Ri; Yoshimasa Tsuruoka",
        "authorids": "/r/ryokan-ri/; /y/yoshimasa-tsuruoka/",
        "bibtex": "@inproceedings{ri-tsuruoka-2022-pretraining,\n    title = \"Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models\",\n    author = \"Ri, Ryokan  and\n      Tsuruoka, Yoshimasa\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.504/\",\n    doi = \"10.18653/v1/2022.acl-long.504\",\n    pages = \"7302--7315\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.504.pdf",
        "site": "https://aclanthology.org/2022.acl-long.504/",
        "pdf_size": 651356,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16931860729398593362&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Tokyo; The University of Tokyo",
        "aff_domain": "logos.t.u-tokyo.ac.jp;logos.t.u-tokyo.ac.jp",
        "email": "logos.t.u-tokyo.ac.jp;logos.t.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.acl-short.65",
        "title": "PriMock57: A Dataset Of Primary Care Mock Consultations",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Recent advances in Automatic Speech Recognition (ASR) have made it possible to reliably produce automatic transcripts of clinician-patient conversations. However, access to clinical datasets is heavily restricted due to patient privacy, thus slowing down normal research practices. We detail the development of a public access, high quality dataset comprising of 57 mocked primary care consultations, including audio recordings, their manual utterance-level transcriptions, and the associated consultation notes. Our work illustrates how the dataset can be used as a benchmark for conversational medical ASR as well as consultation note generation from transcripts.",
        "author": "Alex Papadopoulos Korfiatis; Francesco Moramarco; Radmila Sarac; Aleksandar Savkov",
        "authorids": "/a/alex-papadopoulos-korfiatis/; /f/francesco-moramarco/; /r/radmila-sarac/; /a/aleksandar-savkov/",
        "bibtex": "@inproceedings{papadopoulos-korfiatis-etal-2022-primock57,\n    title = \"{P}ri{M}ock57: A Dataset Of Primary Care Mock Consultations\",\n    author = \"Papadopoulos Korfiatis, Alex  and\n      Moramarco, Francesco  and\n      Sarac, Radmila  and\n      Savkov, Aleksandar\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.65/\",\n    doi = \"10.18653/v1/2022.acl-short.65\",\n    pages = \"588--598\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.65.pdf",
        "site": "https://aclanthology.org/2022.acl-short.65/",
        "pdf_size": 311594,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1498963350151171610&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 3,
        "aff": "Babylon; Babylon+University of Aberdeen; ; Babylon",
        "aff_domain": "babylonhealth.co.uk;babylonhealth.co.uk;gmail.com;babylonhealth.co.uk",
        "email": "babylonhealth.co.uk;babylonhealth.co.uk;gmail.com;babylonhealth.co.uk",
        "github": "https://github.com/babylonhealth/primock57",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Babylon;University of Aberdeen",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.abdn.ac.uk",
        "aff_unique_abbr": ";Aberdeen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "Iraq;United Kingdom"
    },
    {
        "id": "2022.acl-short.82",
        "title": "Primum Non Nocere: Before working with Indigenous data, the ACL must confront ongoing colonialism",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In this paper, we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature. While the vast majority of NLP research focuses on a very small number of very high resource languages (English, Chinese, etc), some work has begun to engage with Indigenous languages. No research involving Indigenous language data can be considered ethical without first acknowledging that Indigenous languages are not merely very low resource languages. The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers. To this end, we propose that the ACL draft and adopt an ethical framework for NLP researchers and computational linguists wishing to engage in research involving Indigenous languages.",
        "author": "Lane Schwartz",
        "authorids": "/l/lane-schwartz/",
        "bibtex": "@inproceedings{schwartz-2022-primum,\n    title = \"{P}rimum {N}on {N}ocere: {B}efore working with {I}ndigenous data, the {ACL} must confront ongoing colonialism\",\n    author = \"Schwartz, Lane\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.82/\",\n    doi = \"10.18653/v1/2022.acl-short.82\",\n    pages = \"724--731\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.82.pdf",
        "site": "https://aclanthology.org/2022.acl-short.82/",
        "pdf_size": 155978,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15509228291650935079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Linguistics, University of Illinois + Institute of Northern Engineering, University of Alaska",
        "aff_domain": "illinois.edu",
        "email": "illinois.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0+1",
        "aff_unique_norm": "University of Illinois;University of Alaska",
        "aff_unique_dep": "Department of Linguistics;Institute of Northern Engineering",
        "aff_unique_url": "https://wwwillinois.edu;https://www.alaska.edu",
        "aff_unique_abbr": "UIUC;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.114",
        "title": "Principled Paraphrase Generation with Parallel Corpora",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Round-trip Machine Translation (MT) is a popular choice for paraphrase generation, which leverages readily available parallel corpora for supervision. In this paper, we formalize the implicit similarity function induced by this approach, and show that it is susceptible to non-paraphrase pairs sharing a single ambiguous translation. Based on these insights, we design an alternative similarity metric that mitigates this issue by requiring the entire translation distribution to match, and implement a relaxation of it through the Information Bottleneck method. Our approach incorporates an adversarial term into MT training in order to learn representations that encode as much information about the reference translation as possible, while keeping as little information about the input as possible. Paraphrases can be generated by decoding back to the source from this representation, without having to generate pivot translations. In addition to being more principled and efficient than round-trip MT, our approach offers an adjustable parameter to control the fidelity-diversity trade-off, and obtains better results in our experiments.",
        "author": "Aitor Ormazabal; Mikel Artetxe; Aitor Soroa; Gorka Labaka; Eneko Agirre",
        "authorids": "/a/aitor-ormazabal/; /m/mikel-artetxe/; /a/aitor-soroa/; /g/gorka-labaka/; /e/eneko-agirre/",
        "bibtex": "@inproceedings{ormazabal-etal-2022-principled,\n    title = \"Principled Paraphrase Generation with Parallel Corpora\",\n    author = \"Ormazabal, Aitor  and\n      Artetxe, Mikel  and\n      Soroa, Aitor  and\n      Labaka, Gorka  and\n      Agirre, Eneko\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.114/\",\n    doi = \"10.18653/v1/2022.acl-long.114\",\n    pages = \"1621--1638\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.114.pdf",
        "site": "https://aclanthology.org/2022.acl-long.114/",
        "pdf_size": 509942,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7478617750466730564&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "HiTZ Center, University of the Basque Country (UPV/EHU); Meta AI; HiTZ Center, University of the Basque Country (UPV/EHU); HiTZ Center, University of the Basque Country (UPV/EHU); HiTZ Center, University of the Basque Country (UPV/EHU)",
        "aff_domain": "ehu.eus;fb.com;ehu.eus;ehu.eus;ehu.eus",
        "email": "ehu.eus;fb.com;ehu.eus;ehu.eus;ehu.eus",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "University of the Basque Country;Meta",
        "aff_unique_dep": "HiTZ Center;Meta AI",
        "aff_unique_url": "https://www.ehu.eus/en;https://meta.com",
        "aff_unique_abbr": "UPV/EHU;Meta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "Spain;United States"
    },
    {
        "id": "2022.findings-acl.297",
        "title": "Prior Knowledge and Memory Enriched Transformer for Sign Language Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper attacks the challenging problem of sign language translation (SLT), which involves not only visual and textual understanding but also additional prior knowledge learning (i.e. performing style, syntax). However, the majority of existing methods with vanilla encoder-decoder structures fail to sufficiently explore all of them. Based on this concern, we propose a novel method called Prior knowledge and memory Enriched Transformer (PET) for SLT, which incorporates the auxiliary information into vanilla transformer. Concretely, we develop gated interactive multi-head attention which associates the multimodal representation and global signing style with adaptive gated functions. One Part-of-Speech (POS) sequence generator relies on the associated information to predict the global syntactic structure, which is thereafter leveraged to guide the sentence generation. Besides, considering that the visual-textual context information, and additional auxiliary knowledge of a word may appear in more than one video, we design a multi-stream memory structure to obtain higher-quality translations, which stores the detailed correspondence between a word and its various relevant information, leading to a more comprehensive understanding for each word. We conduct extensive empirical studies on RWTH-PHOENIX-Weather-2014 dataset with both signer-dependent and signer-independent conditions. The quantitative and qualitative experimental results comprehensively reveal the effectiveness of PET.",
        "author": "Tao Jin; Zhou Zhao; Meng Zhang; Xingshan Zeng",
        "authorids": "/t/tao-jin/; /z/zhou-zhao/; /m/meng-zhang/; /x/xingshan-zeng/",
        "bibtex": "@inproceedings{jin-etal-2022-prior,\n    title = \"Prior Knowledge and Memory Enriched Transformer for Sign Language Translation\",\n    author = \"Jin, Tao  and\n      Zhao, Zhou  and\n      Zhang, Meng  and\n      Zeng, Xingshan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.297/\",\n    doi = \"10.18653/v1/2022.findings-acl.297\",\n    pages = \"3766--3775\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.297.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.297/",
        "pdf_size": 1279696,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16409372617546589652&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Zhejiang University; Zhejiang University; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "zju.edu.cn;zju.edu.cn;huawei.com;gmail.com",
        "email": "zju.edu.cn;zju.edu.cn;huawei.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Zhejiang University;Huawei",
        "aff_unique_dep": ";Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": "ZJU;Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.371",
        "title": "Prix-LM: Pretraining for Multilingual Knowledge Base Construction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge bases (KBs) contain plenty of structured world and commonsense knowledge. As such, they often complement distributional text-based information and facilitate various downstream tasks. Since their manual construction is resource- and time-intensive, recent efforts have tried leveraging large pretrained language models (PLMs) to generate additional monolingual knowledge facts for KBs. However, such methods have not been attempted for building and enriching multilingual KBs. Besides wider application, such multilingual KBs can provide richer combined knowledge than monolingual (e.g., English) KBs. Knowledge expressed in different languages may be complementary and unequally distributed: this implies that the knowledge available in high-resource languages can be transferred to low-resource ones. To achieve this, it is crucial to represent multilingual knowledge in a shared/unified space. To this end, we propose a unified representation model, Prix-LM, for multilingual KB construction and completion. We leverage two types of knowledge, monolingual triples and cross-lingual links, extracted from existing multilingual KBs, and tune a multilingual language encoder XLM-R via a causal language modeling objective. Prix-LM integrates useful multilingual and KB-based factual knowledge into a single model. Experiments on standard entity-related tasks, such as link prediction in multiple languages, cross-lingual entity linking and bilingual lexicon induction, demonstrate its effectiveness, with gains reported over strong task-specialised baselines.",
        "author": "Wenxuan Zhou; Fangyu Liu; Ivan Vuli\u0107; Nigel Collier; Muhao Chen",
        "authorids": "/w/wenxuan-zhou/; /f/fangyu-liu/; /i/ivan-vulic/; /n/nigel-collier/; /m/muhao-chen/",
        "bibtex": "@inproceedings{zhou-etal-2022-prix,\n    title = \"Prix-{LM}: Pretraining for Multilingual Knowledge Base Construction\",\n    author = \"Zhou, Wenxuan  and\n      Liu, Fangyu  and\n      Vuli{\\'c}, Ivan  and\n      Collier, Nigel  and\n      Chen, Muhao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.371/\",\n    doi = \"10.18653/v1/2022.acl-long.371\",\n    pages = \"5412--5424\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.371.pdf",
        "site": "https://aclanthology.org/2022.acl-long.371/",
        "pdf_size": 389553,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14111454507132157535&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "LUKA Lab, University of Southern California, USA; Language Technology Lab, TAL, University of Cambridge, UK; Language Technology Lab, TAL, University of Cambridge, UK; Language Technology Lab, TAL, University of Cambridge, UK; LUKA Lab, University of Southern California, USA",
        "aff_domain": "usc.edu;cam.ac.uk;cam.ac.uk;cam.ac.uk;usc.edu",
        "email": "usc.edu;cam.ac.uk;cam.ac.uk;cam.ac.uk;usc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Southern California;University of Cambridge",
        "aff_unique_dep": "LUKA Lab;Language Technology Lab, TAL",
        "aff_unique_url": "https://www.usc.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": "USC;Cambridge",
        "aff_campus_unique_index": "0;1;1;1;0",
        "aff_campus_unique": "Los Angeles;Cambridge",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2022.findings-acl.314",
        "title": "Probing BERT\u2019s priors with serial reproduction chains",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Sampling is a promising bottom-up method for exposing what generative models have learned about language, but it remains unclear how to generate representative samples from popular masked language models (MLMs) like BERT. The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches. Drawing from theories of iterated learning in cognitive science, we explore the use of serial reproduction chains to sample from BERT\u2019s priors. In particular, we observe that a unique and consistent estimator of the ground-truth joint distribution is given by a Generative Stochastic Network (GSN) sampler, which randomly selects which token to mask and reconstruct on each step. We show that the lexical and syntactic statistics of sentences from GSN chains closely match the ground-truth corpus distribution and perform better than other methods in a large corpus of naturalness judgments. Our findings establish a firmer theoretical foundation for bottom-up probing and highlight richer deviations from human priors.",
        "author": "Takateru Yamakoshi; Thomas Griffiths; Robert Hawkins",
        "authorids": "/t/takateru-yamakoshi/; /t/thomas-l-griffiths/; /r/robert-hawkins/",
        "bibtex": "@inproceedings{yamakoshi-etal-2022-probing,\n    title = \"Probing {BERT}`s priors with serial reproduction chains\",\n    author = \"Yamakoshi, Takateru  and\n      Griffiths, Thomas  and\n      Hawkins, Robert\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.314/\",\n    doi = \"10.18653/v1/2022.findings-acl.314\",\n    pages = \"3977--3992\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.314.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.314/",
        "pdf_size": 3574135,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6283512898037085049&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Princeton University + The University of Tokyo; Princeton University; Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu",
        "github": "https://github.com/taka-yamakoshi/TelephoneGame",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Princeton University;University of Tokyo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.princeton.edu;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "Princeton;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "2022.findings-acl.294",
        "title": "Probing Factually Grounded Content Transfer with Factual Ablation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Despite recent success, large neural models often generate factually incorrect text. Compounding this is the lack of a standard automatic evaluation for factuality\u2013it cannot be meaningfully improved if it cannot be measured. Grounded generation promises a path to solving both of these problems: models draw on a reliable external document (grounding) for factual information, simplifying the challenge of factuality. Measuring factuality is also simplified\u2013to factual consistency, testing whether the generation agrees with the grounding, rather than all facts. Yet, without a standard automatic metric for factual consistency, factually grounded generation remains an open problem. We study this problem for content transfer, in which generations extend a prompt, using information from factual grounding. Particularly, this domain allows us to introduce the notion of factual ablation for automatically measuring factual consistency: this captures the intuition that the model should be less likely to produce an output given a less relevant grounding document. In practice, we measure this by presenting a model with two grounding documents, and the model should prefer to use the more factually relevant one. We contribute two evaluation sets to measure this. Applying our new evaluation, we propose multiple novel methods improving over strong baselines.",
        "author": "Peter West; Chris Quirk; Michel Galley; Yejin Choi",
        "authorids": "/p/peter-west/; /c/chris-quirk/; /m/michel-galley/; /y/yejin-choi/",
        "bibtex": "@inproceedings{west-etal-2022-probing,\n    title = \"Probing Factually Grounded Content Transfer with Factual Ablation\",\n    author = \"West, Peter  and\n      Quirk, Chris  and\n      Galley, Michel  and\n      Choi, Yejin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.294/\",\n    doi = \"10.18653/v1/2022.findings-acl.294\",\n    pages = \"3732--3746\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.294.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.294/",
        "pdf_size": 956485,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5171809948214842334&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for Artificial Intelligence",
        "aff_domain": "cs.washington.edu;microsoft.com;microsoft.com;cs.washington.edu",
        "email": "cs.washington.edu;microsoft.com;microsoft.com;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0+2",
        "aff_unique_norm": "University of Washington;Microsoft;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;Microsoft Research;",
        "aff_unique_url": "https://www.washington.edu;https://www.microsoft.com/en-us/research;https://allenai.org",
        "aff_unique_abbr": "UW;MSR;AI2",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Seattle;Redmond;",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.299",
        "title": "Probing Multilingual Cognate Prediction Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Character-based neural machine translation models have become the reference models for cognate prediction, a historical linguistics task. So far, all linguistic interpretations about latent information captured by such models have been based on external analysis (accuracy, raw results, errors). In this paper, we investigate what probing can tell us about both models and previous interpretations, and learn that though our models store linguistic and diachronic information, they do not achieve it in previously assumed ways.",
        "author": "Cl\u00e9mentine Fourrier; Beno\u00eet Sagot",
        "authorids": "/c/clementine-fourrier/; /b/benoit-sagot/",
        "bibtex": "@inproceedings{fourrier-sagot-2022-probing,\n    title = \"Probing Multilingual Cognate Prediction Models\",\n    author = \"Fourrier, Cl{\\'e}mentine  and\n      Sagot, Beno{\\^i}t\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.299/\",\n    doi = \"10.18653/v1/2022.findings-acl.299\",\n    pages = \"3786--3801\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.299.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.299/",
        "pdf_size": 947513,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2298020801921328190&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Inria, France; Inria, France",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "github.com/clefourrier/CopperMT",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.acl-long.404",
        "title": "Probing Simile Knowledge from Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Simile interpretation (SI) and simile generation (SG) are challenging tasks for NLP because models require adequate world knowledge to produce predictions. Previous works have employed many hand-crafted resources to bring knowledge-related into models, which is time-consuming and labor-intensive. In recent years, pre-trained language models (PLMs) based approaches have become the de-facto standard in NLP since they learn generic knowledge from a large corpus. The knowledge embedded in PLMs may be useful for SI and SG tasks. Nevertheless, there are few works to explore it. In this paper, we probe simile knowledge from PLMs to solve the SI and SG tasks in the unified framework of simile triple completion for the first time. The backbone of our framework is to construct masked sentences with manual patterns and then predict the candidate words in the masked position. In this framework, we adopt a secondary training process (Adjective-Noun mask Training) with the masked language model (MLM) loss to enhance the prediction diversity of candidate words in the masked position. Moreover, pattern ensemble (PE) and pattern search (PS) are applied to improve the quality of predicted words. Finally, automatic and human evaluations demonstrate the effectiveness of our framework in both SI and SG tasks.",
        "author": "Weijie Chen; Yongzhu Chang; Rongsheng Zhang; Jiashu Pu; Guandan Chen; Le Zhang; Yadong Xi; Yijiang Chen; Chang Su",
        "authorids": "/w/weijie-chen/; /y/yongzhu-chang/; /r/rongsheng-zhang/; /j/jiashu-pu/; /g/guandan-chen/; /l/le-zhang/; /y/yadong-xi/; /y/yijiang-chen/; /c/chang-su/",
        "bibtex": "@inproceedings{chen-etal-2022-probing,\n    title = \"Probing Simile Knowledge from Pre-trained Language Models\",\n    author = \"Chen, Weijie  and\n      Chang, Yongzhu  and\n      Zhang, Rongsheng  and\n      Pu, Jiashu  and\n      Chen, Guandan  and\n      Zhang, Le  and\n      Xi, Yadong  and\n      Chen, Yijiang  and\n      Su, Chang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.404/\",\n    doi = \"10.18653/v1/2022.acl-long.404\",\n    pages = \"5875--5887\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.404.pdf",
        "site": "https://aclanthology.org/2022.acl-long.404/",
        "pdf_size": 362183,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15692453659816319741&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Informatics, Xiamen University, Xiamen, China+Fuxi AI Lab, NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China",
        "aff_domain": "stu.xmu.edu.cn;corp.netease.com;corp.netease.com; ; ; ; ;stu.xmu.edu.cn;xmu.edu.cn",
        "email": "stu.xmu.edu.cn;corp.netease.com;corp.netease.com; ; ; ; ;stu.xmu.edu.cn;xmu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;1;1;1;1;1;1;0;0",
        "aff_unique_norm": "Xiamen University;NetEase Inc.",
        "aff_unique_dep": "School of Informatics;Fuxi AI Lab",
        "aff_unique_url": "https://www.xmu.edu.cn;https://www.163.com",
        "aff_unique_abbr": "XMU;NetEase",
        "aff_campus_unique_index": "0+1;1;1;1;1;1;1;0;0",
        "aff_campus_unique": "Xiamen;Hangzhou",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.130",
        "title": "Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Structured pruning has been extensively studied on monolingual pre-trained language models and is yet to be fully evaluated on their multilingual counterparts. This work investigates three aspects of structured pruning on multilingual pre-trained language models: settings, algorithms, and efficiency. Experiments on nine downstream tasks show several counter-intuitive phenomena: for settings, individually pruning for each language does not induce a better result; for algorithms, the simplest method performs the best; for efficiency, a fast model does not imply that it is also small. To facilitate the comparison on all sparsity levels, we present Dynamic Sparsification, a simple approach that allows training the model once and adapting to different model sizes at inference. We hope this work fills the gap in the study of structured pruning on multilingual pre-trained models and sheds light on future research.",
        "author": "Yanyang Li; Fuli Luo; Runxin Xu; Songfang Huang; Fei Huang; Liwei Wang",
        "authorids": "/y/yanyang-li/; /f/fuli-luo/; /r/runxin-xu/; /s/songfang-huang/; /f/fei-huang/; /l/liwei-wang/",
        "bibtex": "@inproceedings{li-etal-2022-probing,\n    title = \"Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency\",\n    author = \"Li, Yanyang  and\n      Luo, Fuli  and\n      Xu, Runxin  and\n      Huang, Songfang  and\n      Huang, Fei  and\n      Wang, Liwei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.130/\",\n    doi = \"10.18653/v1/2022.acl-long.130\",\n    pages = \"1852--1865\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.130.pdf",
        "site": "https://aclanthology.org/2022.acl-long.130/",
        "pdf_size": 543552,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5041872468187248751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, The Chinese University of Hong Kong; Alibaba Group; Key Laboratory of Computational Linguistics, Peking University, MOE, China + Alibaba Group; Alibaba Group; Alibaba Group; Department of Computer Science and Engineering, The Chinese University of Hong Kong",
        "aff_domain": "cse.cuhk.edu.hk;alibaba-inc.com;gmail.com;alibaba-inc.com;alibaba-inc.com;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;alibaba-inc.com;gmail.com;alibaba-inc.com;alibaba-inc.com;cse.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2+1;1;1;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Alibaba Group;Peking University",
        "aff_unique_dep": "Department of Computer Science and Engineering;;Key Laboratory of Computational Linguistics",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.alibaba.com;http://www.pku.edu.cn",
        "aff_unique_abbr": "CUHK;Alibaba;PKU",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.129",
        "title": "Probing as Quantifying Inductive Bias",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained contextual representations have led to dramatic performance improvements on a range of downstream tasks. Such performance improvements have motivated researchers to quantify and understand the linguistic information encoded in these representations. In general, researchers quantify the amount of linguistic information through probing, an endeavor which consists of training a supervised model to predict a linguistic property directly from the contextual representations. Unfortunately, this definition of probing has been subject to extensive criticism in the literature, and has been observed to lead to paradoxical and counter-intuitive results. In the theoretical portion of this paper, we take the position that the goal of probing ought to be measuring the amount of inductive bias that the representations encode on a specific task. We further describe a Bayesian framework that operationalizes this goal and allows us to quantify the representations\u2019 inductive bias. In the empirical portion of the paper, we apply our framework to a variety of NLP tasks. Our results suggest that our proposed framework alleviates many previous problems found in probing. Moreover, we are able to offer concrete evidence that\u2014for some tasks\u2014fastText can offer a better inductive bias than BERT.",
        "author": "Alexander Immer; Lucas Torroba Hennigen; Vincent Fortuin; Ryan Cotterell",
        "authorids": "/a/alexander-immer/; /l/lucas-torroba-hennigen/; /v/vincent-fortuin/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{immer-etal-2022-probing,\n    title = \"Probing as Quantifying Inductive Bias\",\n    author = \"Immer, Alexander  and\n      Torroba Hennigen, Lucas  and\n      Fortuin, Vincent  and\n      Cotterell, Ryan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.129/\",\n    doi = \"10.18653/v1/2022.acl-long.129\",\n    pages = \"1839--1851\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.129.pdf",
        "site": "https://aclanthology.org/2022.acl-long.129/",
        "pdf_size": 687797,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2176239217012062576&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ETH Z\u00fcrich; MIT; ETH Z\u00fcrich; ETH Z\u00fcrich",
        "aff_domain": "inf.ethz.ch;mit.edu;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;mit.edu;inf.ethz.ch;inf.ethz.ch",
        "github": "https://github.com/rycolab/evidence-probing",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "ETH Zurich;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://web.mit.edu",
        "aff_unique_abbr": "ETHZ;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "2022.acl-long.532",
        "title": "Probing for Labeled Dependency Trees",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Probing has become an important tool for analyzing representations in Natural Language Processing (NLP). For graphical NLP tasks such as dependency parsing, linear probes are currently limited to extracting undirected or unlabeled parse trees which do not capture the full task. This work introduces DepProbe, a linear probe which can extract labeled and directed dependency parse trees from embeddings while using fewer parameters and compute than prior methods. Leveraging its full task coverage and lightweight parametrization, we investigate its predictive power for selecting the best transfer language for training a full biaffine attention parser. Across 13 languages, our proposed method identifies the best source treebank 94% of the time, outperforming competitive baselines and prior work. Finally, we analyze the informativeness of task-specific subspaces in contextual embeddings as well as which benefits a full parser\u2019s non-linear parametrization provides.",
        "author": "Max M\u00fcller-Eberstein; Rob van der Goot; Barbara Plank",
        "authorids": "/m/max-muller-eberstein/; /r/rob-van-der-goot/; /b/barbara-plank/",
        "bibtex": "@inproceedings{muller-eberstein-etal-2022-probing,\n    title = \"Probing for Labeled Dependency Trees\",\n    author = {M{\\\"u}ller-Eberstein, Max  and\n      van der Goot, Rob  and\n      Plank, Barbara},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.532/\",\n    doi = \"10.18653/v1/2022.acl-long.532\",\n    pages = \"7711--7726\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.532.pdf",
        "site": "https://aclanthology.org/2022.acl-long.532/",
        "pdf_size": 829355,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9552096964562407548&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, IT University of Copenhagen, Denmark; Department of Computer Science, IT University of Copenhagen, Denmark; Department of Computer Science, IT University of Copenhagen, Denmark",
        "aff_domain": "itu.dk;itu.dk;itu.dk",
        "email": "itu.dk;itu.dk;itu.dk",
        "github": "",
        "project": "https://personads.me/x/acl-2022-code.7711",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "IT University of Copenhagen",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://itu.dk",
        "aff_unique_abbr": "ITU Copenhagen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "2022.acl-long.316",
        "title": "Probing for Predicate Argument Structures in Pretrained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Thanks to the effectiveness and wide availability of modern pretrained language models (PLMs), recently proposed approaches have achieved remarkable results in dependency- and span-based, multilingual and cross-lingual Semantic Role Labeling (SRL). These results have prompted researchers to investigate the inner workings of modern PLMs with the aim of understanding how, where, and to what extent they encode information about SRL. In this paper, we follow this line of research and probe for predicate argument structures in PLMs. Our study shows that PLMs do encode semantic structures directly into the contextualized representation of a predicate, and also provides insights into the correlation between predicate senses and their structures, the degree of transferability between nominal and verbal structures, and how such structures are encoded across languages. Finally, we look at the practical implications of such insights and demonstrate the benefits of embedding predicate argument structure information into an SRL model.",
        "author": "Simone Conia; Roberto Navigli",
        "authorids": "/s/simone-conia/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{conia-navigli-2022-probing,\n    title = \"Probing for Predicate Argument Structures in Pretrained Language Models\",\n    author = \"Conia, Simone  and\n      Navigli, Roberto\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.316/\",\n    doi = \"10.18653/v1/2022.acl-long.316\",\n    pages = \"4622--4632\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.316.pdf",
        "site": "https://aclanthology.org/2022.acl-long.316/",
        "pdf_size": 937476,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2675730066874390522&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science; Department of Computer, Control and Management Engineering",
        "aff_domain": "di.uniroma1.it;diag.uniroma1.it",
        "email": "di.uniroma1.it;diag.uniroma1.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Unknown Institution;Politecnico di Torino",
        "aff_unique_dep": "Department of Computer Science;Department of Computer, Control and Management Engineering",
        "aff_unique_url": ";https://www.polito.it",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Italy"
    },
    {
        "id": "2022.acl-long.603",
        "title": "Probing for the Usage of Grammatical Number",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious\u2014i.e., the model might not rely on it when making predictions. In this paper, we try to find an encoding that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model\u2019s representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Finally, we identify in which layers information about grammatical number is transferred from a noun to its head verb.",
        "author": "Karim Lasri; Tiago Pimentel; Alessandro Lenci; Thierry Poibeau; Ryan Cotterell",
        "authorids": "/k/karim-lasri/; /t/tiago-pimentel/; /a/alessandro-lenci/; /t/thierry-poibeau/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{lasri-etal-2022-probing,\n    title = \"Probing for the Usage of Grammatical Number\",\n    author = \"Lasri, Karim  and\n      Pimentel, Tiago  and\n      Lenci, Alessandro  and\n      Poibeau, Thierry  and\n      Cotterell, Ryan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.603/\",\n    doi = \"10.18653/v1/2022.acl-long.603\",\n    pages = \"8818--8831\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.603.pdf",
        "site": "https://aclanthology.org/2022.acl-long.603/",
        "pdf_size": 747708,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12885548448665678132&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Lattice (\u00c9cole Normale Sup\u00e9rieure-PSL, CNRS, U. Sorbonne Nouvelle); University of Pisa; University of Cambridge; Lattice (\u00c9cole Normale Sup\u00e9rieure-PSL, CNRS, U. Sorbonne Nouvelle); ETH Z\u00fcrich",
        "aff_domain": "ens.psl.eu;cam.ac.uk;unipi.it;ens.psl.eu;inf.ethz.ch",
        "email": "ens.psl.eu;cam.ac.uk;unipi.it;ens.psl.eu;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;3",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure-PSL;University of Pisa;University of Cambridge;ETH Zurich",
        "aff_unique_dep": "Lattice;;;",
        "aff_unique_url": "https://www.ens.psl.eu;https://www.unipi.it;https://www.cam.ac.uk;https://www.ethz.ch",
        "aff_unique_abbr": "ENS-PSL;UNIP;Cambridge;ETHZ",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;2;0;3",
        "aff_country_unique": "France;Italy;United Kingdom;Switzerland"
    },
    {
        "id": "2022.acl-short.85",
        "title": "Probing the Robustness of Trained Metrics for Conversational Dialogue Systems",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "This paper introduces an adversarial method to stress-test trained metrics for the evaluation of conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to giving high scores to responses generated by rather simple and obviously flawed strategies that our method converges on. For instance, simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans.",
        "author": "Jan Deriu; Don Tuggener; Pius Von D\u00e4niken; Mark Cieliebak",
        "authorids": "/j/jan-milan-deriu/; /d/don-tuggener/; /p/pius-von-daniken/; /m/mark-cieliebak/",
        "bibtex": "@inproceedings{deriu-etal-2022-probing,\n    title = \"Probing the Robustness of Trained Metrics for Conversational Dialogue Systems\",\n    author = {Deriu, Jan  and\n      Tuggener, Don  and\n      Von D{\\\"a}niken, Pius  and\n      Cieliebak, Mark},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.85/\",\n    doi = \"10.18653/v1/2022.acl-short.85\",\n    pages = \"750--761\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.85.pdf",
        "site": "https://aclanthology.org/2022.acl-short.85/",
        "pdf_size": 399363,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3507839148868718827&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 8,
        "aff": "Zurich University of Applied Sciences (ZHAW), Winterthur, Switzerland; Zurich University of Applied Sciences (ZHAW), Winterthur, Switzerland; Zurich University of Applied Sciences (ZHAW), Winterthur, Switzerland; Zurich University of Applied Sciences (ZHAW), Winterthur, Switzerland",
        "aff_domain": "zhaw.ch; ; ; ",
        "email": "zhaw.ch; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Zurich University of Applied Sciences",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.zhaw.ch",
        "aff_unique_abbr": "ZHAW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Winterthur",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2022.acl-short.45",
        "title": "Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case.",
        "author": "Kaitlyn Zhou; Kawin Ethayarajh; Dallas Card; Dan Jurafsky",
        "authorids": "/k/kaitlyn-zhou/; /k/kawin-ethayarajh/; /d/dallas-card/; /d/dan-jurafsky/",
        "bibtex": "@inproceedings{zhou-etal-2022-problems,\n    title = \"Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words\",\n    author = \"Zhou, Kaitlyn  and\n      Ethayarajh, Kawin  and\n      Card, Dallas  and\n      Jurafsky, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.45/\",\n    doi = \"10.18653/v1/2022.acl-short.45\",\n    pages = \"401--423\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.45.pdf",
        "site": "https://aclanthology.org/2022.acl-short.45/",
        "pdf_size": 1550215,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15077579648852741229&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University; Stanford University; University of Michigan; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;umich.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;umich.edu;stanford.edu",
        "github": "https://github.com/katezhou/cosine_and_frequency",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Stanford University;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.umich.edu",
        "aff_unique_abbr": "Stanford;UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.559",
        "title": "Program Transfer for Answering Complex Questions over Knowledge Bases",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Program induction for answering complex questions over knowledge bases (KBs) aims to decompose a question into a multi-step program, whose execution against the KB produces the final answer. Learning to induce programs relies on a large number of parallel question-program pairs for the given KB. However, for most KBs, the gold program annotations are usually lacking, making learning difficult. In this paper, we propose the approach of program transfer, which aims to leverage the valuable program annotations on the rich-resourced KBs as external supervision signals to aid program induction for the low-resourced KBs that lack program annotations. For program transfer, we design a novel two-stage parsing framework with an efficient ontology-guided pruning strategy. First, a sketch parser translates the question into a high-level program sketch, which is the composition of functions. Second, given the question and sketch, an argument parser searches the detailed arguments from the KB for functions. During the searching, we incorporate the KB ontology to prune the search space. The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly, demonstrating the effectiveness of program transfer and our framework. Our codes and datasets can be obtained from https://github.com/THU-KEG/ProgramTransfer.",
        "author": "Shulin Cao; Jiaxin Shi; Zijun Yao; Xin Lv; Jifan Yu; Lei Hou; Juanzi Li; Zhiyuan Liu; Jinghui Xiao",
        "authorids": "/s/shulin-cao/; /j/jiaxin-shi/; /z/zijun-yao/; /x/xin-lv/; /j/jifan-yu/; /l/lei-hou/; /j/juanzi-li/; /z/zhiyuan-liu/; /j/jinghui-xiao/",
        "bibtex": "@inproceedings{cao-etal-2022-program,\n    title = \"Program Transfer for Answering Complex Questions over Knowledge Bases\",\n    author = \"Cao, Shulin  and\n      Shi, Jiaxin  and\n      Yao, Zijun  and\n      Lv, Xin  and\n      Yu, Jifan  and\n      Hou, Lei  and\n      Li, Juanzi  and\n      Liu, Zhiyuan  and\n      Xiao, Jinghui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.559/\",\n    doi = \"10.18653/v1/2022.acl-long.559\",\n    pages = \"8128--8140\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.559.pdf",
        "site": "https://aclanthology.org/2022.acl-long.559/",
        "pdf_size": 615986,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12782623048570697096&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Technology+BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology+BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University+Cloud BU, Huawei Technologies; Department of Computer Science and Technology+BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology+BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology+BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology+BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology+BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology+BNRist+KIRC, Institute for Artificial Intelligence, Tsinghua University; Noah\u2019s Ark Lab, Huawei Technologies",
        "aff_domain": "mails.tsinghua.edu.cn;gmail.com;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ;tsinghua.edu.cn;tsinghua.edu.cn; ; ",
        "email": "mails.tsinghua.edu.cn;gmail.com;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ;tsinghua.edu.cn;tsinghua.edu.cn; ; ",
        "github": "https://github.com/THU-KEG/ProgramTransfer",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1+2;0+1+2+3;0+1+2;0+1+2;0+1+2;0+1+2;0+1+2;0+1+2;3",
        "aff_unique_norm": "University of Cambridge;BNRist;Tsinghua University;Huawei",
        "aff_unique_dep": "Department of Computer Science and Technology;;Institute for Artificial Intelligence;Cloud BU",
        "aff_unique_url": "https://www.cam.ac.uk;;https://www.tsinghua.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": "Cambridge;;Tsinghua;Huawei",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+2;0+2+2;0+2;0+2;0+2;0+2;0+2;0+2;2",
        "aff_country_unique": "United Kingdom;;China"
    },
    {
        "id": "2022.acl-long.292",
        "title": "PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks. We propose Prompt-based Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human effort in collecting unlabeled in-domain data and maintains the quality of generated synthetic data. In addition, PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models. Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised model using unlabeled in-domain data. The synthetic data from PromDA are also complementary with unlabeled in-domain data. The NLU models can be further improved when they are combined for training.",
        "author": "Yufei Wang; Can Xu; Qingfeng Sun; Huang Hu; Chongyang Tao; Xiubo Geng; Daxin Jiang",
        "authorids": "/y/yufei-wang/; /c/can-xu/; /q/qingfeng-sun/; /h/huang-hu/; /c/chongyang-tao/; /x/xiubo-geng/; /d/daxin-jiang/",
        "bibtex": "@inproceedings{wang-etal-2022-promda,\n    title = \"{P}rom{DA}: Prompt-based Data Augmentation for Low-Resource {NLU} Tasks\",\n    author = \"Wang, Yufei  and\n      Xu, Can  and\n      Sun, Qingfeng  and\n      Hu, Huang  and\n      Tao, Chongyang  and\n      Geng, Xiubo  and\n      Jiang, Daxin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.292/\",\n    doi = \"10.18653/v1/2022.acl-long.292\",\n    pages = \"4242--4255\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.292.pdf",
        "site": "https://aclanthology.org/2022.acl-long.292/",
        "pdf_size": 950667,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6181542434430287988&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Macquarie University, Sydney, Australia; Microsoft Corporation, Beijing, China; Microsoft Corporation, Beijing, China; Microsoft Corporation, Beijing, China; Microsoft Corporation, Beijing, China; Microsoft Corporation, Beijing, China; Microsoft Corporation, Beijing, China",
        "aff_domain": "students.mq.edu.au;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "students.mq.edu.au;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "Macquarie University;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://www.mq.edu.au;https://www.microsoft.com",
        "aff_unique_abbr": "MQ;Microsoft",
        "aff_campus_unique_index": "0;1;1;1;1;1;1",
        "aff_campus_unique": "Sydney;Beijing",
        "aff_country_unique_index": "0;1;1;1;1;1;1",
        "aff_country_unique": "Australia;China"
    },
    {
        "id": "2022.findings-acl.273",
        "title": "Prompt Tuning for Discriminative Pre-trained Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent works have shown promising results of prompt tuning in stimulating pre-trained language models (PLMs) for natural language processing (NLP) tasks. However, to the best of our knowledge, existing works focus on prompt-tuning generative PLMs that are pre-trained to generate target tokens, such as BERT. It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned. In this work, we present DPT, the first prompt tuning framework for discriminative PLMs, which reformulates NLP tasks into a discriminative language modeling problem. Comprehensive experiments on text classification and question answering show that, compared with vanilla fine-tuning, DPT achieves significantly higher performance, and also prevents the unstable problem in tuning large PLMs in both full-set and low-resource settings.",
        "author": "Yuan Yao; Bowen Dong; Ao Zhang; Zhengyan Zhang; Ruobing Xie; Zhiyuan Liu; Leyu Lin; Maosong Sun; Jianyong Wang",
        "authorids": "/y/yuan-yao/; /b/bowen-dong/; /a/ao-zhang/; /z/zhengyan-zhang/; /r/ruobing-xie/; /z/zhiyuan-liu/; /l/leyu-lin/; /m/maosong-sun/; /j/jianyong-wang/",
        "bibtex": "@inproceedings{yao-etal-2022-prompt,\n    title = \"Prompt Tuning for Discriminative Pre-trained Language Models\",\n    author = \"Yao, Yuan  and\n      Dong, Bowen  and\n      Zhang, Ao  and\n      Zhang, Zhengyan  and\n      Xie, Ruobing  and\n      Liu, Zhiyuan  and\n      Lin, Leyu  and\n      Sun, Maosong  and\n      Wang, Jianyong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.273/\",\n    doi = \"10.18653/v1/2022.findings-acl.273\",\n    pages = \"3468--3473\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.273.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.273/",
        "pdf_size": 387016,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10331556617210864493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology; Department of Computer Science, National University of Singapore, Singapore; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology; WeChat Search Application Department, Tencent, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China; WeChat Search Application Department, Tencent, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology",
        "aff_domain": "163.com;mails.tsinghua.edu.cn; ; ; ;tsinghua.edu.cn; ;tsinghua.edu.cn; ",
        "email": "163.com;mails.tsinghua.edu.cn; ; ; ;tsinghua.edu.cn; ;tsinghua.edu.cn; ",
        "github": "https://github.com/thunlp/DPT",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;0+1;2;0+1;3;0+1+0+0+0;3;0+1+0+0+0;0+1",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;National University of Singapore;Tencent",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;Department of Computer Science;WeChat Search Application Department",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.nus.edu.sg;https://www.tencent.com",
        "aff_unique_abbr": "THU;;NUS;Tencent",
        "aff_campus_unique_index": "0;0;0;0+0+0+2;0+0+0+2;0",
        "aff_campus_unique": "Beijing;;Shanghai",
        "aff_country_unique_index": "0+0;0+0;1;0+0;0;0+0+0+0+0;0;0+0+0+0+0;0+0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.acl-long.466",
        "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
        "author": "Yubo Ma; Zehao Wang; Yixin Cao; Mukai Li; Meiqi Chen; Kun Wang; Jing Shao",
        "authorids": "/y/yubo-ma/; /z/zehao-wang/; /y/yixin-cao/; /m/mukai-li/; /m/meiqi-chen/; /k/kun-wang/; /j/jing-shao/",
        "bibtex": "@inproceedings{ma-etal-2022-prompt,\n    title = \"{P}rompt for Extraction? {PAIE}: {P}rompting Argument Interaction for Event Argument Extraction\",\n    author = \"Ma, Yubo  and\n      Wang, Zehao  and\n      Cao, Yixin  and\n      Li, Mukai  and\n      Chen, Meiqi  and\n      Wang, Kun  and\n      Shao, Jing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.466/\",\n    doi = \"10.18653/v1/2022.acl-long.466\",\n    pages = \"6759--6774\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.466.pdf",
        "site": "https://aclanthology.org/2022.acl-long.466/",
        "pdf_size": 517444,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4552161224153548371&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff": "S-Lab, Nanyang Technological University+SenseTime Research; KU Leuven+SenseTime Research; Singapore Management University; SenseTime Research; Peking University+SenseTime Research; SenseTime Research; SenseTime Research",
        "aff_domain": "e.ntu.edu.sg;esat.kuleuven.be; ; ; ; ; ",
        "email": "e.ntu.edu.sg;esat.kuleuven.be; ; ; ; ; ",
        "github": "https://github.com/mayubo2333/PAIE",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2+1;3;1;4+1;1;1",
        "aff_unique_norm": "Nanyang Technological University;SenseTime;Katholieke Universiteit Leuven;Singapore Management University;Peking University",
        "aff_unique_dep": "S-Lab;SenseTime Research;;;",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.sensetime.com;https://www.kuleuven.be;https://www.smu.edu.sg;http://www.pku.edu.cn",
        "aff_unique_abbr": "NTU;SenseTime;KU Leuven;SMU;Peking U",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2+1;0;1;1+1;1;1",
        "aff_country_unique": "Singapore;China;Belgium"
    },
    {
        "id": "2022.acl-long.55",
        "title": "Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Weakly-supervised learning (WSL) has shown promising results in addressing label scarcity on many NLP tasks, but manually designing a comprehensive, high-quality labeling rule set is tedious and difficult. We study interactive weakly-supervised learning\u2014the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL model. Our proposed model, named PRBoost, achieves this goal via iterative prompt-based rule discovery and model boosting. It uses boosting to identify large-error instances and discovers candidate rules from them by prompting pre-trained LMs with rule templates. The candidate rules are judged by human experts, and the accepted rules are used to generate complementary weak labels and strengthen the current model. Experiments on four tasks show PRBoost outperforms state-of-the-art WSL baselines up to 7.1%, and bridges the gaps with fully supervised models.",
        "author": "Rongzhi Zhang; Yue Yu; Pranav Shetty; Le Song; Chao Zhang",
        "authorids": "/r/rongzhi-zhang/; /y/yue-yu/; /p/pranav-shetty/; /l/le-song/; /c/chao-zhang-tu/",
        "bibtex": "@inproceedings{zhang-etal-2022-prompt,\n    title = \"Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning\",\n    author = \"Zhang, Rongzhi  and\n      Yu, Yue  and\n      Shetty, Pranav  and\n      Song, Le  and\n      Zhang, Chao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.55/\",\n    doi = \"10.18653/v1/2022.acl-long.55\",\n    pages = \"745--758\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.55.pdf",
        "site": "https://aclanthology.org/2022.acl-long.55/",
        "pdf_size": 853629,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8301694954231298011&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Georgia Tech; Georgia Tech; Georgia Tech; MBZUAI; Georgia Tech",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;mbzuai.ac.ae;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;mbzuai.ac.ae;gatech.edu",
        "github": "https://github.com/rz-zhang/PRBoost",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "Georgia Tech;MBZUAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "id": "2022.findings-acl.203",
        "title": "Prompt-Driven Neural Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Neural machine translation (NMT) has obtained significant performance improvement over the recent years. However, NMT models still face various challenges including fragility and lack of style flexibility. Moreover, current methods for instance-level constraints are limited in that they are either constraint-specific or model-specific. To this end, we propose prompt-driven neural machine translation to incorporate prompts for enhancing translation control and enriching flexibility. Empirical results demonstrate the effectiveness of our method in both prompt responding and translation quality. Through human evaluation, we further show the flexibility of prompt control and the efficiency in human-in-the-loop translation.",
        "author": "Yafu Li; Yongjing Yin; Jing Li; Yue Zhang",
        "authorids": "/y/yafu-li/; /y/yongjing-yin/; /j/jing-li/; /y/yue-zhang/",
        "bibtex": "@inproceedings{li-etal-2022-prompt,\n    title = \"Prompt-Driven Neural Machine Translation\",\n    author = \"Li, Yafu  and\n      Yin, Yongjing  and\n      Li, Jing  and\n      Zhang, Yue\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.203/\",\n    doi = \"10.18653/v1/2022.findings-acl.203\",\n    pages = \"2579--2590\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.203.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.203/",
        "pdf_size": 736300,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10593813387385979044&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Zhejiang University+School of Engineering, Westlake University; Zhejiang University+School of Engineering, Westlake University; Sichuan Lan-bridge Information Technology Co., Ltd.; School of Engineering, Westlake University+Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "gmail.com;westlake.edu.cn;gmail.com;wias.org.cn",
        "email": "gmail.com;westlake.edu.cn;gmail.com;wias.org.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;2;1+3",
        "aff_unique_norm": "Zhejiang University;Westlake University;Sichuan Lan-bridge Information Technology Co., Ltd.;Westlake Institute for Advanced Study",
        "aff_unique_dep": ";School of Engineering;;Institute of Advanced Technology",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.westlake.edu.cn;;http://www.wias.org.cn/",
        "aff_unique_abbr": "ZJU;;;WIAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.254",
        "title": "Prompt-free and Efficient Few-shot Learning with Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose Perfect, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points. Perfect makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively. Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding. These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference. Experiments on a wide range of few shot NLP tasks demonstrate that Perfect, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods. Our code is publicly available at https://github.com/rabeehk/perfect.",
        "author": "Rabeeh Karimi Mahabadi; Luke Zettlemoyer; James Henderson; Lambert Mathias; Marzieh Saeidi; Veselin Stoyanov; Majid Yazdani",
        "authorids": "/r/rabeeh-karimi-mahabadi/; /l/luke-zettlemoyer/; /j/james-henderson/; /l/lambert-mathias/; /m/marzieh-saeidi/; /v/veselin-stoyanov/; /m/majid-yazdani/",
        "bibtex": "@inproceedings{karimi-mahabadi-etal-2022-prompt,\n    title = \"Prompt-free and Efficient Few-shot Learning with Language Models\",\n    author = \"Karimi Mahabadi, Rabeeh  and\n      Zettlemoyer, Luke  and\n      Henderson, James  and\n      Mathias, Lambert  and\n      Saeidi, Marzieh  and\n      Stoyanov, Veselin  and\n      Yazdani, Majid\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.254/\",\n    doi = \"10.18653/v1/2022.acl-long.254\",\n    pages = \"3638--3652\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.254.pdf",
        "site": "https://aclanthology.org/2022.acl-long.254/",
        "pdf_size": 537558,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1008012024505772775&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Meta AI+University of Washington+EPFL+Idiap Research Institute; Meta AI+University of Washington; Idiap Research Institute; Meta AI; Meta AI; Meta AI; Meta AI",
        "aff_domain": "idiap.ch;cs.washington.edu;idiap.ch;fb.com;fb.com;fb.com;fb.com",
        "email": "idiap.ch;cs.washington.edu;idiap.ch;fb.com;fb.com;fb.com;fb.com",
        "github": "https://github.com/facebookresearch/perfect.git",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1+2+3;0+1;3;0;0;0;0",
        "aff_unique_norm": "Meta;University of Washington;EPFL;Idiap Research Institute",
        "aff_unique_dep": "Meta AI;;;",
        "aff_unique_url": "https://meta.com;https://www.washington.edu;https://www.epfl.ch;https://www.idiap.ch",
        "aff_unique_abbr": "Meta;UW;EPFL;Idiap",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+1+1;0+0;1;0;0;0;0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "2022.acl-long.68",
        "title": "ProphetChat: Enhancing Dialogue Generation with Simulation of Future Conversation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Typical generative dialogue models utilize the dialogue history to generate the response. However, since one dialogue utterance can often be appropriately answered by multiple distinct responses, generating a desired response solely based on the historical information is not easy. Intuitively, if the chatbot can foresee in advance what the user would talk about (i.e., the dialogue future) after receiving its response, it could possibly provide a more informative response. Accordingly, we propose a novel dialogue generation framework named ProphetChat that utilizes the simulated dialogue futures in the inference phase to enhance response generation. To enable the chatbot to foresee the dialogue future, we design a beam-search-like roll-out strategy for dialogue future simulation using a typical dialogue generation model and a dialogue selector. With the simulated futures, we then utilize the ensemble of a history-to-response generator and a future-to-response generator to jointly generate a more informative response. Experiments on two popular open-domain dialogue datasets demonstrate that ProphetChat can generate better responses over strong baselines, which validates the advantages of incorporating the simulated dialogue futures.",
        "author": "Chang Liu; Xu Tan; Chongyang Tao; Zhenxin Fu; Dongyan Zhao; Tie-Yan Liu; Rui Yan",
        "authorids": "/c/chang-liu/; /x/xu-tan/; /c/chongyang-tao/; /z/zhenxin-fu/; /d/dongyan-zhao/; /t/tie-yan-liu/; /r/rui-yan/",
        "bibtex": "@inproceedings{liu-etal-2022-prophetchat,\n    title = \"{P}rophet{C}hat: Enhancing Dialogue Generation with Simulation of Future Conversation\",\n    author = \"Liu, Chang  and\n      Tan, Xu  and\n      Tao, Chongyang  and\n      Fu, Zhenxin  and\n      Zhao, Dongyan  and\n      Liu, Tie-Yan  and\n      Yan, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.68/\",\n    doi = \"10.18653/v1/2022.acl-long.68\",\n    pages = \"962--973\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.68.pdf",
        "site": "https://aclanthology.org/2022.acl-long.68/",
        "pdf_size": 682547,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4783215732205184238&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "Wangxuan Institute of Computer Technology, Peking University+Center for Data Science, Peking University; Microsoft Research Asia; Microsoft Corporation; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University+Center for Data Science, Peking University+Artificial Intelligence Institute of Peking University+State Key Laboratory of Media Convergence Production Technology and Systems; Microsoft Research Asia; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "pku.edu.cn;microsoft.com;microsoft.com;pku.edu.cn;pku.edu.cn;microsoft.com;ruc.edu.cn",
        "email": "pku.edu.cn;microsoft.com;microsoft.com;pku.edu.cn;pku.edu.cn;microsoft.com;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+0;1;1;0;0+0+0+2;1;3",
        "aff_unique_norm": "Peking University;Microsoft;State Key Laboratory of Media Convergence Production Technology and Systems;Renmin University of China",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;Research;;Gaoling School of Artificial Intelligence",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.microsoft.com/en-us/research/group/asia;;http://www.ruc.edu.cn",
        "aff_unique_abbr": "PKU;MSR Asia;;RUC",
        "aff_campus_unique_index": "1;2;1;2;1",
        "aff_campus_unique": ";Beijing;Asia",
        "aff_country_unique_index": "0+0;0;1;0;0+0+0+0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.213",
        "title": "ProtoTEx: Explaining Model Decisions with Prototype Tensors",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present ProtoTEx, a novel white-box NLP classification architecture based on prototype networks (Li et al., 2018). ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples. At inference time, classification decisions are based on the distances between the input text and the prototype tensors, explained via the training examples most similar to the most influential prototypes. We also describe a novel interleaved training algorithm that effectively handles classes characterized by ProtoTEx indicative features. On a propaganda detection task, ProtoTEx accuracy matches BART-large and exceeds BERTlarge with the added benefit of providing faithful explanations. A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news.",
        "author": "Anubrata Das; Chitrank Gupta; Venelin Kovatchev; Matthew Lease; Junyi Jessy Li",
        "authorids": "/a/anubrata-das/; /c/chitrank-gupta/; /v/venelin-kovatchev/; /m/matthew-lease/; /j/junyi-jessy-li/",
        "bibtex": "@inproceedings{das-etal-2022-prototex,\n    title = \"{P}roto{TE}x: Explaining Model Decisions with Prototype Tensors\",\n    author = \"Das, Anubrata  and\n      Gupta, Chitrank  and\n      Kovatchev, Venelin  and\n      Lease, Matthew  and\n      Li, Junyi Jessy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.213/\",\n    doi = \"10.18653/v1/2022.acl-long.213\",\n    pages = \"2986--2997\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.213.pdf",
        "site": "https://aclanthology.org/2022.acl-long.213/",
        "pdf_size": 649704,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15566917651859717009&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 8,
        "aff": "School of Information and Dept. of Linguistics, The University of Texas at Austin; Dept. of Computer Science, Indian Institute of Technology Bombay; School of Information and Dept. of Linguistics, The University of Texas at Austin; School of Information and Dept. of Linguistics, The University of Texas at Austin; School of Information and Dept. of Linguistics, The University of Texas at Austin",
        "aff_domain": "utexas.edu;gmail.com;utexas.edu;utexas.edu;utexas.edu",
        "email": "utexas.edu;gmail.com;utexas.edu;utexas.edu;utexas.edu",
        "github": "https://github.com/anubrata/ProtoTEx/",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "University of Texas at Austin;Indian Institute of Technology Bombay",
        "aff_unique_dep": "School of Information;Dept. of Computer Science",
        "aff_unique_url": "https://www.utexas.edu;https://www.iitb.ac.in",
        "aff_unique_abbr": "UT Austin;IIT Bombay",
        "aff_campus_unique_index": "0;1;0;0;0",
        "aff_campus_unique": "Austin;Bombay",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2022.acl-long.483",
        "title": "Prototypical Verbalizer for Prompt-based Few-shot Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging. In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https://github.com/thunlp/OpenPrompt.",
        "author": "Ganqu Cui; Shengding Hu; Ning Ding; Longtao Huang; Zhiyuan Liu",
        "authorids": "/g/ganqu-cui/; /s/shengding-hu/; /n/ning-ding/; /l/longtao-huang/; /z/zhiyuan-liu/",
        "bibtex": "@inproceedings{cui-etal-2022-prototypical,\n    title = \"Prototypical Verbalizer for Prompt-based Few-shot Tuning\",\n    author = \"Cui, Ganqu  and\n      Hu, Shengding  and\n      Ding, Ning  and\n      Huang, Longtao  and\n      Liu, Zhiyuan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.483/\",\n    doi = \"10.18653/v1/2022.acl-long.483\",\n    pages = \"7014--7024\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.483.pdf",
        "site": "https://aclanthology.org/2022.acl-long.483/",
        "pdf_size": 568224,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1356007579914205212&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Artificial Intelligence; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Artificial Intelligence; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Artificial Intelligence; Alibaba Group; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China+Beijing Academy of Artificial Intelligence",
        "aff_domain": "mails.tsinghua.edu.cn; ; ; ;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn; ; ; ;tsinghua.edu.cn",
        "github": "https://github.com/thunlp/OpenPrompt",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+0+0+2;0+1+0+0+2;0+1+0+0+2;3;0+1+0+0+2",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Beijing Academy of Artificial Intelligence;Alibaba Group",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.baaic.cn;https://www.alibaba.com",
        "aff_unique_abbr": "THU;;BAAI;Alibaba",
        "aff_campus_unique_index": "0+0+2;0+0+2;0+0+2;0+0+2",
        "aff_campus_unique": "Beijing;;Shanghai",
        "aff_country_unique_index": "0+0+0+0+0;0+0+0+0+0;0+0+0+0+0;0;0+0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.602",
        "title": "Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer-based language models such as BERT (CITATION) have achieved the state-of-the-art performance on various NLP tasks, but are computationally prohibitive. A recent line of works use various heuristics to successively shorten sequence length while transforming tokens through encoders, in tasks such as classification and ranking that require a single token embedding for prediction. We present a novel solution to this problem, called Pyramid-BERT where we replace previously used heuristics with a core-set based token selection method justified by theoretical results. The core-set based token selection technique allows us to avoid expensive pre-training, gives a space-efficient fine tuning, and thus makes it suitable to handle longer sequence lengths. We provide extensive experiments establishing advantages of pyramid BERT over several baselines and existing works on the GLUE benchmarks and Long Range Arena (CITATION) datasets.",
        "author": "Xin Huang; Ashish Khetan; Rene Bidart; Zohar Karnin",
        "authorids": "/x/xin-huang/; /a/ashish-khetan/; /r/rene-bidart/; /z/zohar-karnin/",
        "bibtex": "@inproceedings{huang-etal-2022-pyramid,\n    title = \"Pyramid-{BERT}: Reducing Complexity via Successive Core-set based Token Selection\",\n    author = \"Huang, Xin  and\n      Khetan, Ashish  and\n      Bidart, Rene  and\n      Karnin, Zohar\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.602/\",\n    doi = \"10.18653/v1/2022.acl-long.602\",\n    pages = \"8798--8817\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.602.pdf",
        "site": "https://aclanthology.org/2022.acl-long.602/",
        "pdf_size": 1986836,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16035534666193984847&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Amazon AWS, USA; Amazon AWS, USA; University of Waterloo, Canada; Amazon AWS, Israel",
        "aff_domain": "amazon.com;amazon.com;uwaterloo.ca;amazon.com",
        "email": "amazon.com;amazon.com;uwaterloo.ca;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Amazon;University of Waterloo",
        "aff_unique_dep": "Amazon AWS;",
        "aff_unique_url": "https://aws.amazon.com;https://uwaterloo.ca",
        "aff_unique_abbr": "AWS;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2",
        "aff_country_unique": "United States;Canada;Israel"
    },
    {
        "id": "2022.acl-long.370",
        "title": "QAConv: Question Answering on Informative Conversations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper introduces QAConv, a new question answering (QA) dataset that uses conversations as a knowledge source. We focus on informative conversations, including business emails, panel discussions, and work channels. Unlike open-domain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain knowledge. In total, we collect 34,608 QA pairs from 10,259 selected conversations with both human-written and machine-generated questions. We use a question generator and a dialogue summarizer as auxiliary tools to collect and recommend questions. The dataset has two testing scenarios: chunk mode and full mode, depending on whether the grounded partial conversation is provided or retrieved. Experimental results show that state-of-the-art pretrained QA systems have limited zero-shot performance and tend to predict our questions as unanswerable. Our dataset provides a new training and evaluation testbed to facilitate QA on conversations research.",
        "author": "Chien-Sheng Wu; Andrea Madotto; Wenhao Liu; Pascale Fung; Caiming Xiong",
        "authorids": "/c/chien-sheng-wu/; /a/andrea-madotto/; /w/wenhao-liu/; /p/pascale-fung/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{wu-etal-2022-qaconv,\n    title = \"{QAC}onv: Question Answering on Informative Conversations\",\n    author = \"Wu, Chien-Sheng  and\n      Madotto, Andrea  and\n      Liu, Wenhao  and\n      Fung, Pascale  and\n      Xiong, Caiming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.370/\",\n    doi = \"10.18653/v1/2022.acl-long.370\",\n    pages = \"5389--5411\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.370.pdf",
        "site": "https://aclanthology.org/2022.acl-long.370/",
        "pdf_size": 4652149,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2511209580894980213&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Salesforce AI Research; The Hong Kong University of Science and Technology; Salesforce AI Research; The Hong Kong University of Science and Technology; Salesforce AI Research",
        "aff_domain": "salesforce.com;connect.ust.hk;salesforce.com;ece.ust.hk;salesforce.com",
        "email": "salesforce.com;connect.ust.hk;salesforce.com;ece.ust.hk;salesforce.com",
        "github": "https://github.com/salesforce/QAConv",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "Salesforce;Hong Kong University of Science and Technology",
        "aff_unique_dep": "Salesforce AI Research;",
        "aff_unique_url": "https://www.salesforce.com;https://www.ust.hk",
        "aff_unique_abbr": "Salesforce AI;HKUST",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.45",
        "title": "Quality Controlled Paraphrase Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Paraphrase generation has been widely used in various downstream tasks. Most tasks benefit mainly from high quality paraphrases, namely those that are semantically similar to, yet linguistically diverse from, the original sentence. Generating high-quality paraphrases is challenging as it becomes increasingly hard to preserve meaning as linguistic diversity increases. Recent works achieve nice results by controlling specific aspects of the paraphrase, such as its syntactic tree. However, they do not allow to directly control the quality of the generated paraphrase, and suffer from low flexibility and scalability. Here we propose QCPG, a quality-guided controlled paraphrase generation model, that allows directly controlling the quality dimensions. Furthermore, we suggest a method that given a sentence, identifies points in the quality control space that are expected to yield optimal generated paraphrases. We show that our method is able to generate paraphrases which maintain the original meaning while achieving higher diversity than the uncontrolled baseline. The models, the code, and the data can be found in https://github.com/IBM/quality-controlled-paraphrase-generation.",
        "author": "Elron Bandel; Ranit Aharonov; Michal Shmueli-Scheuer; Ilya Shnayderman; Noam Slonim; Liat Ein-Dor",
        "authorids": "/e/elron-bandel/; /r/ranit-aharonov/; /m/michal-shmueli-scheuer/; /i/ilya-shnayderman/; /n/noam-slonim/; /l/liat-ein-dor/",
        "bibtex": "@inproceedings{bandel-etal-2022-quality,\n    title = \"Quality Controlled Paraphrase Generation\",\n    author = \"Bandel, Elron  and\n      Aharonov, Ranit  and\n      Shmueli-Scheuer, Michal  and\n      Shnayderman, Ilya  and\n      Slonim, Noam  and\n      Ein-Dor, Liat\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.45/\",\n    doi = \"10.18653/v1/2022.acl-long.45\",\n    pages = \"596--609\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.45.pdf",
        "site": "https://aclanthology.org/2022.acl-long.45/",
        "pdf_size": 7430670,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15208637053057362337&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "IBM Research + Computer Science Department, Bar Ilan University; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research",
        "aff_domain": "ibm.com;ibm.com;il.ibm.com;il.ibm.com;il.ibm.com;il.ibm.com",
        "email": "ibm.com;ibm.com;il.ibm.com;il.ibm.com;il.ibm.com;il.ibm.com",
        "github": "https://github.com/IBM/quality-controlled-paraphrase-generation",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0;0;0;0",
        "aff_unique_norm": "IBM;Bar-Ilan University",
        "aff_unique_dep": "IBM Research;Computer Science Department",
        "aff_unique_url": "https://www.ibm.com/research;https://www.biu.ac.il",
        "aff_unique_abbr": "IBM;BIU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;0;0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "2022.acl-long.2",
        "title": "Quantified Reproducibility Assessment of NLP Results",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 different system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but also of different, original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and as a result, allows conclusions to be drawn about what aspects of system and/or evaluation design need to be changed in order to improve reproducibility.",
        "author": "Anya Belz; Maja Popovic; Simon Mille",
        "authorids": "/a/anja-belz/; /m/maja-popovic/; /s/simon-mille/",
        "bibtex": "@inproceedings{belz-etal-2022-quantified,\n    title = \"Quantified Reproducibility Assessment of {NLP} Results\",\n    author = \"Belz, Anya  and\n      Popovic, Maja  and\n      Mille, Simon\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.2/\",\n    doi = \"10.18653/v1/2022.acl-long.2\",\n    pages = \"16--28\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.2.pdf",
        "site": "https://aclanthology.org/2022.acl-long.2/",
        "pdf_size": 363968,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=361776292194020079&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 9,
        "aff": "ADAPT Research Centre, Dublin City University, Ireland; ADAPT Research Centre, Dublin City University, Ireland; Universitat Pompeu Fabra, Barcelona, Spain",
        "aff_domain": "adaptcentre.ie;adaptcentre.ie;upf.edu",
        "email": "adaptcentre.ie;adaptcentre.ie;upf.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Dublin City University;Universitat Pompeu Fabra",
        "aff_unique_dep": "ADAPT Research Centre;",
        "aff_unique_url": "https://www.dcu.ie;https://www.upf.edu/",
        "aff_unique_abbr": "DCU;UPF",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Barcelona",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Ireland;Spain"
    },
    {
        "id": "2022.findings-acl.16",
        "title": "Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Event extraction is typically modeled as a multi-class classification problem where event types and argument roles are treated as atomic symbols. These approaches are usually limited to a set of pre-defined types. We propose a novel event extraction framework that uses event types and argument roles as natural language queries to extract candidate triggers and arguments from the input text. With the rich semantics in the queries, our framework benefits from the attention mechanisms to better capture the semantic correlation between the event types or argument roles and the input text. Furthermore, the query-and-extract formulation allows our approach to leverage all available event annotations from various ontologies as a unified model. Experiments on ACE and ERE demonstrate that our approach achieves state-of-the-art performance on each dataset and significantly outperforms existing methods on zero-shot event extraction.",
        "author": "Sijia Wang; Mo Yu; Shiyu Chang; Lichao Sun; Lifu Huang",
        "authorids": "/s/sijia-wang/; /m/mo-yu/; /s/shiyu-chang/; /l/lichao-sun/; /l/lifu-huang/",
        "bibtex": "@inproceedings{wang-etal-2022-query,\n    title = \"Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding\",\n    author = \"Wang, Sijia  and\n      Yu, Mo  and\n      Chang, Shiyu  and\n      Sun, Lichao  and\n      Huang, Lifu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.16/\",\n    doi = \"10.18653/v1/2022.findings-acl.16\",\n    pages = \"169--182\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.16.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.16/",
        "pdf_size": 758092,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=779793444827584006&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Virginia Tech; WeChat AI; University of California Santa Barbara; Lehigh University; Virginia Tech",
        "aff_domain": "vt.edu;vt.edu;tencent.com;ucsb.edu;lehigh.edu",
        "email": "vt.edu;vt.edu;tencent.com;ucsb.edu;lehigh.edu",
        "github": "https://github.com/VT-NLP/Event_Query_Extract",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "Virginia Tech;WeChat;University of California, Santa Barbara;Lehigh University",
        "aff_unique_dep": ";WeChat AI;;",
        "aff_unique_url": "https://www.vt.edu;https://www.wechat.com;https://www.ucsb.edu;https://www.lehigh.edu",
        "aff_unique_abbr": "VT;WeChat AI;UCSB;Lehigh",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Santa Barbara",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.findings-acl.59",
        "title": "Question Answering Infused Pre-training of General-Purpose Contextualized Representations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. To this end, we train a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder\u2019s token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-the-art results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.",
        "author": "Robin Jia; Mike Lewis; Luke Zettlemoyer",
        "authorids": "/r/robin-jia/; /m/mike-lewis/; /l/luke-zettlemoyer/",
        "bibtex": "@inproceedings{jia-etal-2022-question,\n    title = \"Question Answering Infused Pre-training of General-Purpose Contextualized Representations\",\n    author = \"Jia, Robin  and\n      Lewis, Mike  and\n      Zettlemoyer, Luke\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.59/\",\n    doi = \"10.18653/v1/2022.findings-acl.59\",\n    pages = \"711--728\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.59.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.59/",
        "pdf_size": 396094,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11292906330338875755&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Southern California + Facebook AI Research; Facebook AI Research; Facebook AI Research",
        "aff_domain": "usc.edu;fb.com;fb.com",
        "email": "usc.edu;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "University of Southern California;Meta",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.usc.edu;https://research.facebook.com",
        "aff_unique_abbr": "USC;FAIR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.168",
        "title": "Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Reading is integral to everyday life, and yet learning to read is a struggle for many young learners. During lessons, teachers can use comprehension questions to increase engagement, test reading skills, and improve retention. Historically such questions were written by skilled teachers, but recently language models have been used to generate comprehension questions. However, many existing Question Generation (QG) systems focus on generating extractive questions from the text, and have no way to control the type of the generated question. In this paper, we study QG for reading comprehension where inferential questions are critical and extractive techniques cannot be used. We propose a two-step model (HTA-WTA) that takes advantage of previous datasets, and can generate questions for a specific targeted comprehension skill. We propose a new reading comprehension dataset that contains questions annotated with story-based reading comprehension skills (SBRCS), allowing for a more complete reader assessment. Across several experiments, our results show that HTA-WTA outperforms multiple strong baselines on this new dataset. We show that the HTA-WTA model tests for strong SCRS by asking deep inferential questions.",
        "author": "Bilal Ghanem; Lauren Lutz Coleman; Julia Rivard Dexter; Spencer von der Ohe; Alona Fyshe",
        "authorids": "/b/bilal-ghanem/; /l/lauren-lutz-coleman/; /j/julia-rivard-dexter/; /s/spencer-von-der-ohe/; /a/alona-fyshe/",
        "bibtex": "@inproceedings{ghanem-etal-2022-question,\n    title = \"Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask\",\n    author = \"Ghanem, Bilal  and\n      Lutz Coleman, Lauren  and\n      Rivard Dexter, Julia  and\n      von der Ohe, Spencer  and\n      Fyshe, Alona\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.168/\",\n    doi = \"10.18653/v1/2022.findings-acl.168\",\n    pages = \"2131--2146\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.168.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.168/",
        "pdf_size": 370107,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=791402657940421688&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Alberta, Canada; EyeRead, Canada; EyeRead, Canada; University of Alberta, Canada; University of Alberta, Canada",
        "aff_domain": "ualberta.ca;eyeread.co;eyeread.co;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;eyeread.co;eyeread.co;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "University of Alberta;EyeRead",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ualberta.ca;",
        "aff_unique_abbr": "UAlberta;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.27",
        "title": "QuoteR: A Benchmark of Quote Recommendation for Writing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "It is very common to use quotations (quotes) to make our writings more elegant or convincing. To help people find appropriate quotes efficiently, the task of quote recommendation is presented, aiming to recommend quotes that fit the current context of writing. There have been various quote recommendation approaches, but they are evaluated on different unpublished datasets. To facilitate the research on this task, we build a large and fully open quote recommendation dataset called QuoteR, which comprises three parts including English, standard Chinese and classical Chinese. Any part of it is larger than previous unpublished counterparts. We conduct an extensive evaluation of existing quote recommendation methods on QuoteR. Furthermore, we propose a new quote recommendation model that significantly outperforms previous methods on all three parts of QuoteR. All the code and data of this paper can be obtained at https://github.com/thunlp/QuoteR.",
        "author": "Fanchao Qi; Yanhui Yang; Jing Yi; Zhili Cheng; Zhiyuan Liu; Maosong Sun",
        "authorids": "/f/fanchao-qi/; /y/yanhui-yang/; /j/jing-yi/; /z/zhili-cheng/; /z/zhiyuan-liu/; /m/maosong-sun/",
        "bibtex": "@inproceedings{qi-etal-2022-quoter,\n    title = \"{Q}uote{R}: A Benchmark of Quote Recommendation for Writing\",\n    author = \"Qi, Fanchao  and\n      Yang, Yanhui  and\n      Yi, Jing  and\n      Cheng, Zhili  and\n      Liu, Zhiyuan  and\n      Sun, Maosong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.27/\",\n    doi = \"10.18653/v1/2022.acl-long.27\",\n    pages = \"336--348\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.27.pdf",
        "site": "https://aclanthology.org/2022.acl-long.27/",
        "pdf_size": 694118,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9340026086498655951&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Technology, Tsinghua University+Institute for Artificial Intelligence, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University; Beijing National Research Center for Information Science and Technology; Department of Computer Science and Technology, Tsinghua University+Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology, Tsinghua University+Institute for Artificial Intelligence, Tsinghua University; Department of Computer Science and Technology, Tsinghua University+Institute for Artificial Intelligence, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University; Department of Computer Science and Technology, Tsinghua University+Institute for Artificial Intelligence, Tsinghua University+Beijing National Research Center for Information Science and Technology+Institute Guo Qiang, Tsinghua University+International Innovation Center of Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;gmail.com; ; ; ;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;gmail.com; ; ; ;tsinghua.edu.cn",
        "github": "https://github.com/thunlp/QuoteR",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0+1+0+0;1;0+0;0+0;0+0+1+0+0;0+0+1+0+0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Technology;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "THU;",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0+0+0;0;0+0;0+0;0+0+0+0+0;0+0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.517",
        "title": "RELiC: Retrieving Evidence for Literary Claims",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Humanities scholars commonly provide evidence for claims that they make about a work of literature (e.g., a novel) in the form of quotations from the work. We collect a large-scale dataset (RELiC) of 78K literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval, in which models are given an excerpt of literary analysis surrounding a masked quotation and asked to retrieve the quoted passage from the set of all passages in the work. Solving this retrieval task requires a deep understanding of complex literary and linguistic phenomena, which proves challenging to methods that overwhelmingly rely on lexical and semantic similarity matching. We implement a RoBERTa-based dense passage retriever for this task that outperforms existing pretrained information retrieval baselines; however, experiments and analysis by human domain experts indicate that there is substantial room for improvement.",
        "author": "Katherine Thai; Yapei Chang; Kalpesh Krishna; Mohit Iyyer",
        "authorids": "/k/katherine-thai/; /y/yapei-chang/; /k/kalpesh-krishna/; /m/mohit-iyyer/",
        "bibtex": "@inproceedings{thai-etal-2022-relic,\n    title = \"{REL}i{C}: Retrieving Evidence for Literary Claims\",\n    author = \"Thai, Katherine  and\n      Chang, Yapei  and\n      Krishna, Kalpesh  and\n      Iyyer, Mohit\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.517/\",\n    doi = \"10.18653/v1/2022.acl-long.517\",\n    pages = \"7500--7518\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.517.pdf",
        "site": "https://aclanthology.org/2022.acl-long.517/",
        "pdf_size": 434972,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1288540985006595304&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Massachusetts Amherst; Smith College; University of Massachusetts Amherst; University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;smith.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;smith.edu;cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "https://relic.cs.umass.edu",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Massachusetts Amherst;Smith College",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umass.edu;https://www.smith.edu",
        "aff_unique_abbr": "UMass Amherst;Smith",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.417",
        "title": "RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on GrailQA and WebQSP datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard. In addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization.",
        "author": "Xi Ye; Semih Yavuz; Kazuma Hashimoto; Yingbo Zhou; Caiming Xiong",
        "authorids": "/x/xi-ye/; /s/semih-yavuz/; /k/kazuma-hashimoto/; /y/yingbo-zhou/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{ye-etal-2022-rng,\n    title = \"{RNG}-{KBQA}: Generation Augmented Iterative Ranking for Knowledge Base Question Answering\",\n    author = \"Ye, Xi  and\n      Yavuz, Semih  and\n      Hashimoto, Kazuma  and\n      Zhou, Yingbo  and\n      Xiong, Caiming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.417/\",\n    doi = \"10.18653/v1/2022.acl-long.417\",\n    pages = \"6032--6043\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.417.pdf",
        "site": "https://aclanthology.org/2022.acl-long.417/",
        "pdf_size": 519630,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11294476293159392873&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, The University of Texas at Austin+Salesforce Research; Salesforce Research; Salesforce Research; Salesforce Research; Salesforce Research",
        "aff_domain": "cs.utexas.edu;salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "email": "cs.utexas.edu;salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "github": "https://github.com/salesforce/rng-kbqa",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "University of Texas at Austin;Salesforce",
        "aff_unique_dep": "Department of Computer Science;Salesforce Research",
        "aff_unique_url": "https://www.utexas.edu;https://research.salesforce.com",
        "aff_unique_abbr": "UT Austin;Salesforce",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.597",
        "title": "RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development. However, it still remains challenging to generate release notes automatically. In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub. Then, we propose classwise extractive-then-abstractive/abstractive summarization approaches to this task, which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints. The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines. We also observe that there is a significant gap in the coverage of essential information when compared to human references. Our dataset and the code are publicly available.",
        "author": "Hisashi Kamezawa; Noriki Nishida; Nobuyuki Shimizu; Takashi Miyazaki; Hideki Nakayama",
        "authorids": "/h/hisashi-kamezawa/; /n/noriki-nishida/; /n/nobuyuki-shimizu/; /t/takashi-miyazaki/; /h/hideki-nakayama/",
        "bibtex": "@inproceedings{kamezawa-etal-2022-rnsum,\n    title = \"{RNS}um: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization\",\n    author = \"Kamezawa, Hisashi  and\n      Nishida, Noriki  and\n      Shimizu, Nobuyuki  and\n      Miyazaki, Takashi  and\n      Nakayama, Hideki\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.597/\",\n    doi = \"10.18653/v1/2022.acl-long.597\",\n    pages = \"8718--8735\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.597.pdf",
        "site": "https://aclanthology.org/2022.acl-long.597/",
        "pdf_size": 553213,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11282568134552104379&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 2,
        "aff": "The University of Tokyo; RIKEN Center for Advanced Intelligence Project; Yahoo Japan Corporation + The University of Tokyo; Yahoo Japan Corporation; The University of Tokyo",
        "aff_domain": "nlab.ci.i.u-tokyo.ac.jp;riken.jp;yahoo-corp.jp;yahoo-corp.jp;ci.i.u-tokyo.ac.jp",
        "email": "nlab.ci.i.u-tokyo.ac.jp;riken.jp;yahoo-corp.jp;yahoo-corp.jp;ci.i.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2+0;2;0",
        "aff_unique_norm": "University of Tokyo;RIKEN;Yahoo Japan Corporation",
        "aff_unique_dep": ";Center for Advanced Intelligence Project;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.riken.jp/en/;https://www.yahoo.co.jp",
        "aff_unique_abbr": "UTokyo;RIKEN;Yahoo Japan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.acl-long.294",
        "title": "RST Discourse Parsing with Second-Stage EDU-Level Pre-training",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) have shown great potentials in natural language processing (NLP) including rhetorical structure theory (RST) discourse parsing. Current PLMs are obtained by sentence-level pre-training, which is different from the basic processing unit, i.e. element discourse unit (EDU).To this end, we propose a second-stage EDU-level pre-training approach in this work, which presents two novel tasks to learn effective EDU representations continually based on well pre-trained language models. Concretely, the two tasks are (1) next EDU prediction (NEP) and (2) discourse marker prediction (DMP).We take a state-of-the-art transition-based neural parser as baseline, and adopt it with a light bi-gram EDU modification to effectively explore the EDU-level pre-trained EDU representation. Experimental results on a benckmark dataset show that our method is highly effective,leading a 2.1-point improvement in F1-score. All codes and pre-trained models will be released publicly to facilitate future studies.",
        "author": "Nan Yu; Meishan Zhang; Guohong Fu; Min Zhang",
        "authorids": "/n/nan-yu/; /m/meishan-zhang/; /g/guohong-fu/; /m/min-zhang/",
        "bibtex": "@inproceedings{yu-etal-2022-rst,\n    title = \"{RST} Discourse Parsing with Second-Stage {EDU}-Level Pre-training\",\n    author = \"Yu, Nan  and\n      Zhang, Meishan  and\n      Fu, Guohong  and\n      Zhang, Min\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.294/\",\n    doi = \"10.18653/v1/2022.acl-long.294\",\n    pages = \"4269--4280\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.294.pdf",
        "site": "https://aclanthology.org/2022.acl-long.294/",
        "pdf_size": 510982,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15366092593988555783&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, Soochow University, China; Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China; School of Computer Science and Technology, Soochow University, China + Institute of Artificial Intelligence, Soochow University, China; Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China",
        "aff_domain": "stu.suda.edu.cn;gmail.com;suda.edu.cn;suda.edu.cn",
        "email": "stu.suda.edu.cn;gmail.com;suda.edu.cn;suda.edu.cn",
        "github": "http://github.com/yunan4nlp/E-NNRSTParser",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0+0;1",
        "aff_unique_norm": "Soochow University;Harbin Institute of Technology",
        "aff_unique_dep": "School of Computer Science and Technology;Institute of Computing and Intelligence",
        "aff_unique_url": "https://eng.suda.edu.cn/;http://www.hhit.edu.cn",
        "aff_unique_abbr": "Soochow U;HIT",
        "aff_campus_unique_index": "1;;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.161",
        "title": "Ranking-Constrained Learning with Rationales for Text Classification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data. We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints. We evaluate our proposed rationale-augmented learning approach on three human-annotated datasets, and show that our approach provides significant improvements over classification approaches that do not utilize rationales as well as other state-of-the-art rationale-augmented baselines.",
        "author": "Juanyan Wang; Manali Sharma; Mustafa Bilgic",
        "authorids": "/j/juanyan-wang/; /m/manali-sharma/; /m/mustafa-bilgic/",
        "bibtex": "@inproceedings{wang-etal-2022-ranking,\n    title = \"Ranking-Constrained Learning with Rationales for Text Classification\",\n    author = \"Wang, Juanyan  and\n      Sharma, Manali  and\n      Bilgic, Mustafa\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.161/\",\n    doi = \"10.18653/v1/2022.findings-acl.161\",\n    pages = \"2034--2046\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.161.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.161/",
        "pdf_size": 827519,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=212931568487407747&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "Illinois Institute of Technology; Samsung Semiconductor, Inc; Illinois Institute of Technology",
        "aff_domain": "hawk.iit.edu;samsung.com;iit.edu",
        "email": "hawk.iit.edu;samsung.com;iit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Illinois Institute of Technology;Samsung",
        "aff_unique_dep": ";Samsung Semiconductor, Inc",
        "aff_unique_url": "https://www.iit.edu;https://www.samsung.com/us/business semiconductors/",
        "aff_unique_abbr": "IIT;SSI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.3",
        "title": "Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models. Although the existing methods that address the degeneration problem based on observations of the phenomenon triggered by the problem improves the performance of the text generation, the training dynamics of token embeddings behind the degeneration problem are still not explored. In this study, we analyze the training dynamics of the token embeddings focusing on rare token embedding. We demonstrate that the specific part of the gradient for rare token embeddings is the key cause of the degeneration problem for all tokens during training stage. Based on the analysis, we propose a novel method called, adaptive gradient gating(AGG). AGG addresses the degeneration problem by gating the specific part of the gradient for rare token embeddings. Experimental results from language modeling, word similarity, and machine translation tasks quantitatively and qualitatively verify the effectiveness of AGG.",
        "author": "Sangwon Yu; Jongyoon Song; Heeseung Kim; Seongmin Lee; Woo-Jong Ryu; Sungroh Yoon",
        "authorids": "/s/sangwon-yu/; /j/jongyoon-song/; /h/heeseung-kim/; /s/seongmin-lee/; /w/woo-jong-ryu/; /s/sungroh-yoon/",
        "bibtex": "@inproceedings{yu-etal-2022-rare,\n    title = \"Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings\",\n    author = \"Yu, Sangwon  and\n      Song, Jongyoon  and\n      Kim, Heeseung  and\n      Lee, Seongmin  and\n      Ryu, Woo-Jong  and\n      Yoon, Sungroh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.3/\",\n    doi = \"10.18653/v1/2022.acl-long.3\",\n    pages = \"29--45\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.3.pdf",
        "site": "https://aclanthology.org/2022.acl-long.3/",
        "pdf_size": 1429239,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13232852962785546014&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Data Science & AI Laboratory, Seoul National University, Korea; Data Science & AI Laboratory, Seoul National University, Korea; Data Science & AI Laboratory, Seoul National University, Korea; AIRS Company, Hyundai Motor Group, Korea; AIRS Company, Hyundai Motor Group, Korea; Data Science & AI Laboratory, Seoul National University, Korea + ASRI, ECE, GSAI, and INMC, Seoul National University, Korea",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;hyundai.com;hyundai.com",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;hyundai.com;hyundai.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;1;0+0",
        "aff_unique_norm": "Seoul National University;Hyundai Motor Group",
        "aff_unique_dep": "Data Science & AI Laboratory;AIRS Company",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.hyundai.com",
        "aff_unique_abbr": "SNU;HMG",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2022.acl-long.323",
        "title": "Rare and Zero-shot Word Sense Disambiguation using Z-Reweighting",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Word sense disambiguation (WSD) is a crucial problem in the natural language processing (NLP) community. Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models. However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses. There are more training instances and senses for words with top frequency ranks than those with low frequency ranks in the training dataset. We investigate the statistical relation between word frequency rank and word sense number distribution. Based on the relation, we propose a Z-reweighting method on the word level to adjust the training on the imbalanced dataset. The experiments show that the Z-reweighting strategy achieves performance gain on the standard English all words WSD benchmark. Moreover, the strategy can help models generalize better on rare and zero-shot senses.",
        "author": "Ying Su; Hongming Zhang; Yangqiu Song; Tong Zhang",
        "authorids": "/y/ying-su/; /h/hongming-zhang/; /y/yangqiu-song/; /t/tong-zhang/",
        "bibtex": "@inproceedings{su-etal-2022-rare,\n    title = \"Rare and Zero-shot Word Sense Disambiguation using {Z}-Reweighting\",\n    author = \"Su, Ying  and\n      Zhang, Hongming  and\n      Song, Yangqiu  and\n      Zhang, Tong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.323/\",\n    doi = \"10.18653/v1/2022.acl-long.323\",\n    pages = \"4713--4723\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.323.pdf",
        "site": "https://aclanthology.org/2022.acl-long.323/",
        "pdf_size": 336178,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2488344868244700821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "HKUST; HKUST + Tencent AI lab, Seattle; HKUST; HKUST",
        "aff_domain": "connect.ust.hk;cse.ust.hk;cse.ust.hk;ust.hk",
        "email": "connect.ust.hk;cse.ust.hk;cse.ust.hk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Tencent",
        "aff_unique_dep": ";AI lab",
        "aff_unique_url": "https://www.ust.hk;https://ai.tencent.com",
        "aff_unique_abbr": "HKUST;Tencent AI lab",
        "aff_campus_unique_index": "0;0+1;0;0",
        "aff_campus_unique": "Hong Kong SAR;Seattle",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.431",
        "title": "ReACC: A Retrieval-Augmented Code Completion Framework",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing \u201dexternal\u201d context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark.",
        "author": "Shuai Lu; Nan Duan; Hojae Han; Daya Guo; Seung-won Hwang; Alexey Svyatkovskiy",
        "authorids": "/s/shuai-lu/; /n/nan-duan/; /h/hojae-han/; /d/daya-guo/; /s/seung-won-hwang/; /a/alexey-svyatkovskiy/",
        "bibtex": "@inproceedings{lu-etal-2022-reacc,\n    title = \"{R}e{ACC}: A Retrieval-Augmented Code Completion Framework\",\n    author = \"Lu, Shuai  and\n      Duan, Nan  and\n      Han, Hojae  and\n      Guo, Daya  and\n      Hwang, Seung-won  and\n      Svyatkovskiy, Alexey\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.431/\",\n    doi = \"10.18653/v1/2022.acl-long.431\",\n    pages = \"6227--6240\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.431.pdf",
        "site": "https://aclanthology.org/2022.acl-long.431/",
        "pdf_size": 883456,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12215645593138891096&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Microsoft Research Asia; Microsoft Research Asia; Seoul National University; Sun Yat-sen University; Seoul National University; Microsoft Devdiv",
        "aff_domain": "microsoft.com;microsoft.com;snu.ac.kr;mail2.sysu.edu.cn;snu.ac.kr;microsoft.com",
        "email": "microsoft.com;microsoft.com;snu.ac.kr;mail2.sysu.edu.cn;snu.ac.kr;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;1;0",
        "aff_unique_norm": "Microsoft;Seoul National University;Sun Yat-sen University",
        "aff_unique_dep": "Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia;https://www.snu.ac.kr;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "MSR Asia;SNU;SYSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Asia;",
        "aff_country_unique_index": "0;0;1;0;1;2",
        "aff_country_unique": "China;South Korea;United States"
    },
    {
        "id": "2022.acl-long.357",
        "title": "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP\u2019s contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP\u2019s relative improvement over supervised ReC models trained on real images is 8%.",
        "author": "Sanjay Subramanian; William Merrill; Trevor Darrell; Matt Gardner; Sameer Singh; Anna Rohrbach",
        "authorids": "/s/sanjay-subramanian/; /w/william-merrill/; /t/trevor-darrell/; /m/matt-gardner/; /s/sameer-singh/; /a/anna-rohrbach/",
        "bibtex": "@inproceedings{subramanian-etal-2022-reclip,\n    title = \"{R}e{CLIP}: A Strong Zero-Shot Baseline for Referring Expression Comprehension\",\n    author = \"Subramanian, Sanjay  and\n      Merrill, William  and\n      Darrell, Trevor  and\n      Gardner, Matt  and\n      Singh, Sameer  and\n      Rohrbach, Anna\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.357/\",\n    doi = \"10.18653/v1/2022.acl-long.357\",\n    pages = \"5198--5215\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.357.pdf",
        "site": "https://aclanthology.org/2022.acl-long.357/",
        "pdf_size": 12140100,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2030961720375041388&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "UC Berkeley+AI2; New York University+AI2; UC Berkeley; Microsoft Semantic Machines+AI2; UC Irvine+AI2; UC Berkeley",
        "aff_domain": "berkeley.edu;nyu.edu;berkeley.edu;microsoft.com;uci.edu;berkeley.edu",
        "email": "berkeley.edu;nyu.edu;berkeley.edu;microsoft.com;uci.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2+1;0;3+1;4+1;0",
        "aff_unique_norm": "University of California, Berkeley;AI2;New York University;Microsoft;University of California, Irvine",
        "aff_unique_dep": ";;;Semantic Machines;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ai2.edu;https://www.nyu.edu;https://www.microsoft.com;https://www.uci.edu",
        "aff_unique_abbr": "UC Berkeley;AI2;NYU;Microsoft;UCI",
        "aff_campus_unique_index": "0;;0;;2;0",
        "aff_campus_unique": "Berkeley;;Irvine",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.51",
        "title": "Read Top News First: A Document Reordering Approach for Multi-Document News Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "A common method for extractive multi-document news summarization is to re-formulate it as a single-document summarization problem by concatenating all documents as a single meta-document. However, this method neglects the relative importance of documents. We propose a simple approach to reorder the documents according to their relative importance before concatenating and summarizing them. The reordering makes the salient content easier to learn by the summarization model. Experiments show that our approach outperforms previous state-of-the-art methods with more complex architectures.",
        "author": "Chao Zhao; Tenghao Huang; Somnath Basu Roy Chowdhury; Muthu Kumar Chandrasekaran; Kathleen McKeown; Snigdha Chaturvedi",
        "authorids": "/c/chao-zhao/; /t/tenghao-huang/; /s/somnath-basu-roy-chowdhury/; /m/muthu-kumar-chandrasekaran/; /k/kathleen-mckeown/; /s/snigdha-chaturvedi/",
        "bibtex": "@inproceedings{zhao-etal-2022-read,\n    title = \"Read Top News First: A Document Reordering Approach for Multi-Document News Summarization\",\n    author = \"Zhao, Chao  and\n      Huang, Tenghao  and\n      Basu Roy Chowdhury, Somnath  and\n      Chandrasekaran, Muthu Kumar  and\n      McKeown, Kathleen  and\n      Chaturvedi, Snigdha\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.51/\",\n    doi = \"10.18653/v1/2022.findings-acl.51\",\n    pages = \"613--621\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.51.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.51/",
        "pdf_size": 294435,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18011734052151639345&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill + Columbia University; Columbia University; UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu;gmail.com;cs.columbia.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu;gmail.com;cs.columbia.edu;cs.unc.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0+1;1;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unc.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UNC;Columbia",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Chapel Hill;",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.61",
        "title": "Read before Generate! Faithful Long Form Question Answering with Machine Reading",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Long-form question answering (LFQA) aims to generate a paragraph-length answer for a given question. While current work on LFQA using large pre-trained model for generation are effective at producing fluent and somewhat relevant content, one primary challenge lies in how to generate a faithful answer that has less hallucinated content. We propose a new end-to-end framework that jointly models answer generation and machine reading. The key idea is to augment the generation model with fine-grained, answer-related salient information which can be viewed as an emphasis on faithful facts. State-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate the effectiveness of our method, in comparison with strong baselines on automatic and human evaluation metrics. A detailed analysis further proves the competency of our methods in generating fluent, relevant, and more faithful answers.",
        "author": "Dan Su; Xiaoguang Li; Jindi Zhang; Lifeng Shang; Xin Jiang; Qun Liu; Pascale Fung",
        "authorids": "/d/dan-su/; /x/xiaoguang-li/; /j/jindi-zhang/; /l/lifeng-shang/; /x/xin-jiang/; /q/qun-liu/; /p/pascale-fung/",
        "bibtex": "@inproceedings{su-etal-2022-read,\n    title = \"Read before Generate! Faithful Long Form Question Answering with Machine Reading\",\n    author = \"Su, Dan  and\n      Li, Xiaoguang  and\n      Zhang, Jindi  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Fung, Pascale\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.61/\",\n    doi = \"10.18653/v1/2022.findings-acl.61\",\n    pages = \"744--756\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.61.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.61/",
        "pdf_size": 716851,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12572890319470511451&as_sdt=5,34&sciodt=0,34&hl=en",
        "gs_version_total": 5,
        "aff": "Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology; Huawei Noah\u2019s Ark Lab+City University of Hong Kong; City University of Hong Kong; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology",
        "aff_domain": "connect.ust.hk;huawei.com; ; ; ; ; ",
        "email": "connect.ust.hk;huawei.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1+2;2;1;1;1;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Huawei;City University of Hong Kong",
        "aff_unique_dep": "Center for Artificial Intelligence Research (CAiRE);Noah\u2019s Ark Lab;",
        "aff_unique_url": "https://www.ust.hk;https://www.huawei.com;https://www.cityu.edu.hk",
        "aff_unique_abbr": "HKUST;Huawei;CityU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.172",
        "title": "Redistributing Low-Frequency Words: Making the Most of Monolingual Data in Non-Autoregressive Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge distillation (KD) is the preliminary step for training non-autoregressive translation (NAT) models, which eases the training of NAT models at the cost of losing important information for translating low-frequency words. In this work, we provide an appealing alternative for NAT \u2013 monolingual KD, which trains NAT student on external monolingual data with AT teacher trained on the original bilingual data. Monolingual KD is able to transfer both the knowledge of the original bilingual data (implicitly encoded in the trained AT teacher model) and that of the new monolingual data to the NAT student model. Extensive experiments on eight WMT benchmarks over two advanced NAT models show that monolingual KD consistently outperforms the standard KD by improving low-frequency word translation, without introducing any computational cost. Monolingual KD enjoys desirable expandability, which can be further enhanced (when given more computational budget) by combining with the standard KD, a reverse monolingual KD, or enlarging the scale of monolingual data. Extensive analyses demonstrate that these techniques can be used together profitably to further recall the useful information lost in the standard KD. Encouragingly, combining with standard KD, our approach achieves 30.4 and 34.1 BLEU points on the WMT14 English-German and German-English datasets, respectively. Our code and trained models are freely available at https://github.com/alphadl/RLFW-NAT.mono.",
        "author": "Liang Ding; Longyue Wang; Shuming Shi; Dacheng Tao; Zhaopeng Tu",
        "authorids": "/l/liang-ding/; /l/longyue-wang/; /s/shuming-shi/; /d/dacheng-tao/; /z/zhaopeng-tu/",
        "bibtex": "@inproceedings{ding-etal-2022-redistributing,\n    title = \"Redistributing Low-Frequency Words: Making the Most of Monolingual Data in Non-Autoregressive Translation\",\n    author = \"Ding, Liang  and\n      Wang, Longyue  and\n      Shi, Shuming  and\n      Tao, Dacheng  and\n      Tu, Zhaopeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.172/\",\n    doi = \"10.18653/v1/2022.acl-long.172\",\n    pages = \"2417--2426\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.172.pdf",
        "site": "https://aclanthology.org/2022.acl-long.172/",
        "pdf_size": 373278,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15206890511214054710&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 2,
        "aff": "The University of Sydney+Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; The University of Sydney+JD Explore Academy; Tencent AI Lab",
        "aff_domain": "sydney.edu.au;tencent.com;tencent.com;gmail.com;tencent.com",
        "email": "sydney.edu.au;tencent.com;tencent.com;gmail.com;tencent.com",
        "github": "https://github.com/alphadl/RLFW-NAT.mono",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;0+2;1",
        "aff_unique_norm": "University of Sydney;Tencent;JD",
        "aff_unique_dep": ";Tencent AI Lab;JD Explore Academy",
        "aff_unique_url": "https://www.sydney.edu.au;https://ai.tencent.com;",
        "aff_unique_abbr": "USYD;Tencent AI Lab;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;0;1",
        "aff_country_unique": "Australia;China;"
    },
    {
        "id": "2022.acl-long.467",
        "title": "Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Simultaneous machine translation (SiMT) starts translating while receiving the streaming source inputs, and hence the source sentence is always incomplete during translating. Different from the full-sentence MT using the conventional seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture, which forces each target word to only align with a partial source prefix to adapt to the incomplete source in streaming inputs. However, the source words in the front positions are always illusoryly considered more important since they appear in more prefixes, resulting in position bias, which makes the model pay more attention on the front source positions in testing. In this paper, we first analyze the phenomenon of position bias in SiMT, and develop a Length-Aware Framework to reduce the position bias by bridging the structural gap between SiMT and full-sentence MT. Specifically, given the streaming inputs, we first predict the full-sentence length and then fill the future source position with positional encoding, thereby turning the streaming inputs into a pseudo full-sentence. The proposed framework can be integrated into most existing SiMT methods to further improve performance. Experiments on two representative SiMT methods, including the state-of-the-art adaptive policy, show that our method successfully reduces the position bias and thereby achieves better SiMT performance.",
        "author": "Shaolei Zhang; Yang Feng",
        "authorids": "/s/shaolei-zhang/; /y/yang-feng/",
        "bibtex": "@inproceedings{zhang-feng-2022-reducing,\n    title = \"Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework\",\n    author = \"Zhang, Shaolei  and\n      Feng, Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.467/\",\n    doi = \"10.18653/v1/2022.acl-long.467\",\n    pages = \"6775--6788\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.467.pdf",
        "site": "https://aclanthology.org/2022.acl-long.467/",
        "pdf_size": 1039010,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4285585417308471980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.50",
        "title": "Reframing Instructional Prompts to GPTk\u2019s Language",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.",
        "author": "Swaroop Mishra; Daniel Khashabi; Chitta Baral; Yejin Choi; Hannaneh Hajishirzi",
        "authorids": "/s/swaroop-mishra/; /d/daniel-khashabi/; /c/chitta-baral/; /y/yejin-choi/; /h/hannaneh-hajishirzi/",
        "bibtex": "@inproceedings{mishra-etal-2022-reframing,\n    title = \"Reframing Instructional Prompts to {GPT}k`s Language\",\n    author = \"Mishra, Swaroop  and\n      Khashabi, Daniel  and\n      Baral, Chitta  and\n      Choi, Yejin  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.50/\",\n    doi = \"10.18653/v1/2022.findings-acl.50\",\n    pages = \"589--612\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.50.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.50/",
        "pdf_size": 753000,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5254224900306592837&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 6,
        "aff": "Arizona State University; Allen Institute for AI + University of Washington; Arizona State University; Allen Institute for AI + University of Washington; Allen Institute for AI + University of Washington",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+2;0;1+2;1+2",
        "aff_unique_norm": "Arizona State University;Allen Institute for AI;University of Washington",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.asu.edu;https://allenai.org;https://www.washington.edu",
        "aff_unique_abbr": "ASU;AI2;UW",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.59",
        "title": "Region-dependent temperature scaling for certainty calibration and application to class-imbalanced token classification",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Certainty calibration is an important goal on the path to interpretability and trustworthy AI. Particularly in the context of human-in-the-loop systems, high-quality low to mid-range certainty estimates are essential. In the presence of a dominant high-certainty class, for instance the non-entity class in NER problems, existing calibration error measures are completely insensitive to potentially large errors in this certainty region of interest. We introduce a region-balanced calibration error metric that weights all certainty regions equally. When low and mid certainty estimates are taken into account, calibration error is typically larger than previously reported. We introduce a simple extension of temperature scaling, requiring no additional computation, that can reduce both traditional and region-balanced notions of calibration error over existing baselines.",
        "author": "Hillary Dawkins; Isar Nejadgholi",
        "authorids": "/h/hillary-dawkins/; /i/isar-nejadgholi/",
        "bibtex": "@inproceedings{dawkins-nejadgholi-2022-region,\n    title = \"Region-dependent temperature scaling for certainty calibration and application to class-imbalanced token classification\",\n    author = \"Dawkins, Hillary  and\n      Nejadgholi, Isar\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.59/\",\n    doi = \"10.18653/v1/2022.acl-short.59\",\n    pages = \"538--544\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.59.pdf",
        "site": "https://aclanthology.org/2022.acl-short.59/",
        "pdf_size": 307771,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18137730861060065672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of Guelph, Canada+Vector Institute, Toronto, Canada; National Research Council Canada, Ottawa, Canada",
        "aff_domain": "uoguelph.ca;nrc-cnrc.gc.ca",
        "email": "uoguelph.ca;nrc-cnrc.gc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "University of Guelph;Vector Institute;National Research Council Canada",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uoguelph.ca;https://vectorinstitute.ai;https://www.nrc-cnrc.gc.ca",
        "aff_unique_abbr": "U of G;Vector Institute;NRC-CNRC",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Toronto;Ottawa",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.findings-acl.38",
        "title": "Reinforced Cross-modal Alignment for Radiology Report Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Medical images are widely used in clinical decision-making, where writing radiology reports is a potential application that can be enhanced by automatic solutions to alleviate physicians\u2019 workload. In general, radiology report generation is an image-text task, where cross-modal mappings between images and texts play an important role in generating high-quality reports. Although previous studies attempt to facilitate the alignment via the co-attention mechanism under supervised settings, they suffer from lacking valid and accurate correspondences due to no annotation of such alignment. In this paper, we propose an approach with reinforcement learning (RL) over a cross-modal memory (CMM) to better align visual and textual features for radiology report generation. In detail, a shared memory is used to record the mappings between visual and textual information, and the proposed reinforced algorithm is performed to learn the signal from the reports to guide the cross-modal alignment even though such reports are not directly related to how images and texts are mapped. Experimental results on two English radiology report datasets, i.e., IU X-Ray and MIMIC-CXR, show the effectiveness of our approach, where the state-of-the-art results are achieved. We further conduct human evaluation and case study which confirm the validity of the reinforced algorithm in our approach.",
        "author": "Han Qin; Yan Song",
        "authorids": "/h/han-qin/; /y/yan-song/",
        "bibtex": "@inproceedings{qin-song-2022-reinforced,\n    title = \"Reinforced Cross-modal Alignment for Radiology Report Generation\",\n    author = \"Qin, Han  and\n      Song, Yan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.38/\",\n    doi = \"10.18653/v1/2022.findings-acl.38\",\n    pages = \"448--458\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.38.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.38/",
        "pdf_size": 2331809,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12848329936612831775&as_sdt=10005&sciodt=0,8&hl=en",
        "gs_version_total": 2,
        "aff": "The Chinese University of Hong Kong (Shenzhen); The Chinese University of Hong Kong (Shenzhen)",
        "aff_domain": "link.cuhk.edu.cn;cuhk.edu.cn",
        "email": "link.cuhk.edu.cn;cuhk.edu.cn",
        "github": "https://github.com/cuhksz-nlp/R2GenRL",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cuhk.edu.cn",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shenzhen",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.462",
        "title": "Reinforcement Guided Multi-Task Learning Framework for Low-Resource Stereotype Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As large Pre-trained Language Models (PLMs) trained on large amounts of data in an unsupervised manner become more ubiquitous, identifying various types of bias in the text has come into sharp focus. Existing \u2018Stereotype Detection\u2019 datasets mainly adopt a diagnostic approach toward large PLMs. Blodgett et. al. (2021) show that there are significant reliability issues with the existing benchmark datasets. Annotating a reliable dataset requires a precise understanding of the subtle nuances of how stereotypes manifest in text. In this paper, we annotate a focused evaluation set for \u2018Stereotype Detection\u2019 that addresses those pitfalls by de-constructing various ways in which stereotypes manifest in text. Further, we present a multi-task model that leverages the abundance of data-rich neighboring tasks such as hate speech detection, offensive language detection, misogyny detection, etc., to improve the empirical performance on \u2018Stereotype Detection\u2019. We then propose a reinforcement-learning agent that guides the multi-task learning model by learning to identify the training examples from the neighboring tasks that help the target task the most. We show that the proposed models achieve significant empirical gains over existing baselines on all the tasks.",
        "author": "Rajkumar Pujari; Erik Oveson; Priyanka Kulkarni; Elnaz Nouri",
        "authorids": "/r/rajkumar-pujari/; /e/erik-oveson/; /p/priyanka-kulkarni/; /e/elnaz-nouri/",
        "bibtex": "@inproceedings{pujari-etal-2022-reinforcement,\n    title = \"Reinforcement Guided Multi-Task Learning Framework for Low-Resource Stereotype Detection\",\n    author = \"Pujari, Rajkumar  and\n      Oveson, Erik  and\n      Kulkarni, Priyanka  and\n      Nouri, Elnaz\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.462/\",\n    doi = \"10.18653/v1/2022.acl-long.462\",\n    pages = \"6703--6712\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.462.pdf",
        "site": "https://aclanthology.org/2022.acl-long.462/",
        "pdf_size": 316130,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16962909291968756100&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Purdue University; Microsoft, Redmond+Microsoft Research, Redmond; Microsoft, Redmond+Microsoft Research, Redmond; Microsoft Research, Redmond",
        "aff_domain": "purdue.edu;microsoft.com;microsoft.com;microsoft.com",
        "email": "purdue.edu;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+1;1+1;1",
        "aff_unique_norm": "Purdue University;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://www.purdue.edu;https://www.microsoft.com",
        "aff_unique_abbr": "Purdue;Microsoft",
        "aff_campus_unique_index": "1+1;1+1;1",
        "aff_campus_unique": ";Redmond",
        "aff_country_unique_index": "0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.5",
        "title": "RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.",
        "author": "Yew Ken Chia; Lidong Bing; Soujanya Poria; Luo Si",
        "authorids": "/y/yew-ken-chia/; /l/lidong-bing/; /s/soujanya-poria/; /l/luo-si/",
        "bibtex": "@inproceedings{chia-etal-2022-relationprompt,\n    title = \"{R}elation{P}rompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction\",\n    author = \"Chia, Yew Ken  and\n      Bing, Lidong  and\n      Poria, Soujanya  and\n      Si, Luo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.5/\",\n    doi = \"10.18653/v1/2022.findings-acl.5\",\n    pages = \"45--57\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.5.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.5/",
        "pdf_size": 890844,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=119984045643216975&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "DAMO Academy, Alibaba Group + Singapore University of Technology and Design; DAMO Academy, Alibaba Group + Singapore University of Technology and Design; Singapore University of Technology and Design; DAMO Academy, Alibaba Group + Singapore University of Technology and Design",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;sutd.edu.sg;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;sutd.edu.sg;alibaba-inc.com",
        "github": "github.com/declare-lab/RelationPrompt",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;1;0+1",
        "aff_unique_norm": "Alibaba Group;Singapore University of Technology and Design",
        "aff_unique_dep": "DAMO Academy;",
        "aff_unique_url": "https://www.alibaba-group.com;https://www.sutd.edu.sg",
        "aff_unique_abbr": "Alibaba;SUTD",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;1;0+1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.findings-acl.152",
        "title": "Relevant CommonSense Subgraphs for \u201cWhat if...\u201d Procedural Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We study the challenge of learning causal reasoning over procedural text to answer \u201cWhat if...\u201d questions when external commonsense knowledge is required. We propose a novel multi-hop graph reasoning model to 1) efficiently extract a commonsense subgraph with the most relevant information from a large knowledge graph; 2) predict the causal answer by reasoning over the representations obtained from the commonsense subgraph and the contextual interactions between the questions and context. We evaluate our model on WIQA benchmark and achieve state-of-the-art performance compared to the recent models.",
        "author": "Chen Zheng; Parisa Kordjamshidi",
        "authorids": "/c/chen-zheng/; /p/parisa-kordjamshidi/",
        "bibtex": "@inproceedings{zheng-kordjamshidi-2022-relevant,\n    title = \"Relevant {C}ommon{S}ense Subgraphs for {\\textquotedblleft}What if...{\\textquotedblright} Procedural Reasoning\",\n    author = \"Zheng, Chen  and\n      Kordjamshidi, Parisa\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.152/\",\n    doi = \"10.18653/v1/2022.findings-acl.152\",\n    pages = \"1927--1933\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.152.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.152/",
        "pdf_size": 500689,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12459605554169224988&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Michigan State University; Michigan State University",
        "aff_domain": "msu.edu;msu.edu",
        "email": "msu.edu;msu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.379",
        "title": "Reports of personal experiences and stories in argumentation: datasets and analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Reports of personal experiences or stories can play a crucial role in argumentation, as they represent an immediate and (often) relatable way to back up one\u2019s position with respect to a given topic. They are easy to understand and increase empathy: this makes them powerful in argumentation. The impact of personal reports and stories in argumentation has been studied in the Social Sciences, but it is still largely underexplored in NLP. Our work is the first step towards filling this gap: our goal is to develop robust classifiers to identify documents containing personal experiences and reports. The main challenge is the scarcity of annotated data: our solution is to leverage existing annotations to be able to scale-up the analysis. Our contribution is two-fold. First, we conduct a set of in-domain and cross-domain experiments involving three datasets (two from Argument Mining, one from the Social Sciences), modeling architectures, training setups and fine-tuning options tailored to the involved domains. We show that despite the differences among datasets and annotations, robust cross-domain classification is possible. Second, we employ linear regression for performance mining, identifying performance trends both for overall classification performance and individual classifier predictions.",
        "author": "Neele Falk; Gabriella Lapesa",
        "authorids": "/n/neele-falk/; /g/gabriella-lapesa/",
        "bibtex": "@inproceedings{falk-lapesa-2022-reports,\n    title = \"Reports of personal experiences and stories in argumentation: datasets and analysis\",\n    author = \"Falk, Neele  and\n      Lapesa, Gabriella\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.379/\",\n    doi = \"10.18653/v1/2022.acl-long.379\",\n    pages = \"5530--5553\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.379.pdf",
        "site": "https://aclanthology.org/2022.acl-long.379/",
        "pdf_size": 838855,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6756149358563312658&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Institute for Natural Language Processing, University of Stuttgart; Institute for Natural Language Processing, University of Stuttgart",
        "aff_domain": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "email": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Stuttgart",
        "aff_unique_dep": "Institute for Natural Language Processing",
        "aff_unique_url": "https://www.uni-stuttgart.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.507",
        "title": "Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization. By building speech synthesis systems for three Indigenous languages spoken in Canada, Kanien\u2019k\u00e9ha, Gitksan & SEN\u0106O\u0166EN, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models. For example, preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data. Finally, we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization.",
        "author": "Aidan Pine; Dan Wells; Nathan Brinklow; Patrick Littell; Korin Richmond",
        "authorids": "/a/aidan-pine/; /d/dan-wells/; /n/nathan-brinklow/; /p/patrick-littell/; /k/korin-richmond/",
        "bibtex": "@inproceedings{pine-etal-2022-requirements,\n    title = \"Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization\",\n    author = \"Pine, Aidan  and\n      Wells, Dan  and\n      Brinklow, Nathan  and\n      Littell, Patrick  and\n      Richmond, Korin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.507/\",\n    doi = \"10.18653/v1/2022.acl-long.507\",\n    pages = \"7346--7359\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.507.pdf",
        "site": "https://aclanthology.org/2022.acl-long.507/",
        "pdf_size": 367226,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13334702134333716035&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "National Research Council Canada; University of Edinburgh; Queen\u2019s University; National Research Council Canada; University of Edinburgh",
        "aff_domain": "nrc.ca;ed.ac.uk;queensu.ca;nrc.ca;ed.ac.uk",
        "email": "nrc.ca;ed.ac.uk;queensu.ca;nrc.ca;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "National Research Council Canada;University of Edinburgh;Queen's University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nrc-cnrc.gc.ca;https://www.ed.ac.uk;https://www.queensu.ca",
        "aff_unique_abbr": "NRC-CNRC;Edinburgh;Queen's U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "Canada;United Kingdom"
    },
    {
        "id": "2022.findings-acl.279",
        "title": "Rethinking Document-level Neural Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper does not aim at introducing a novel model for document-level neural machine translation. Instead, we head back to the original Transformer model and hope to answer the following question: Is the capacity of current models strong enough for document-level translation? Interestingly, we observe that the original Transformer with appropriate training techniques can achieve strong results for document translation, even with a length of 2000 words. We evaluate this model and several recent approaches on nine document-level datasets and two sentence-level datasets across six languages. Experiments show that document-level Transformer models outperforms sentence-level ones and many previous methods in a comprehensive set of metrics, including BLEU, four lexical indices, three newly proposed assistant linguistic indicators, and human evaluation.",
        "author": "Zewei Sun; Mingxuan Wang; Hao Zhou; Chengqi Zhao; Shujian Huang; Jiajun Chen; Lei Li",
        "authorids": "/z/zewei-sun/; /m/mingxuan-wang/; /h/hao-zhou/; /c/chengqi-zhao/; /s/shujian-huang/; /j/jiajun-chen/; /l/lei-li/",
        "bibtex": "@inproceedings{sun-etal-2022-rethinking,\n    title = \"Rethinking Document-level Neural Machine Translation\",\n    author = \"Sun, Zewei  and\n      Wang, Mingxuan  and\n      Zhou, Hao  and\n      Zhao, Chengqi  and\n      Huang, Shujian  and\n      Chen, Jiajun  and\n      Li, Lei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.279/\",\n    doi = \"10.18653/v1/2022.findings-acl.279\",\n    pages = \"3537--3548\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.279.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.279/",
        "pdf_size": 522836,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17730480417567913601&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University+ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab; State Key Laboratory for Novel Software Technology, Nanjing University+Peng Cheng Laboratory, Shenzhen; State Key Laboratory for Novel Software Technology, Nanjing University; University of California, Santa Barbara",
        "aff_domain": "bytedance.com;bytedance.com;bytedance.com;bytedance.com;nju.edu.cn;nju.edu.cn;cs.ucsb.edu",
        "email": "bytedance.com;bytedance.com;bytedance.com;bytedance.com;nju.edu.cn;nju.edu.cn;cs.ucsb.edu",
        "github": "https://github.com/sunzewei2715/Doc2Doc_NMT",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;1;0+2;0;3",
        "aff_unique_norm": "Nanjing University;ByteDance;Pengcheng Laboratory;University of California, Santa Barbara",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology;AI Lab;Peng Cheng Laboratory;",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.bytedance.com;;https://www.ucsb.edu",
        "aff_unique_abbr": "Nanjing University;ByteDance;;UCSB",
        "aff_campus_unique_index": ";1;2",
        "aff_campus_unique": ";Shenzhen;Santa Barbara",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.497",
        "title": "Rethinking Negative Sampling for Handling Missing Entity Annotations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Negative sampling is highly effective in handling missing annotations for named entity recognition (NER). One of our contributions is an analysis on how it makes sense through introducing two insightful concepts: missampling and uncertainty. Empirical studies show low missampling rate and high uncertainty are both essential for achieving promising performances with negative sampling. Based on the sparsity of named entities, we also theoretically derive a lower bound for the probability of zero missampling rate, which is only relevant to sentence length. The other contribution is an adaptive and weighted sampling distribution that further improves negative sampling via our former analysis. Experiments on synthetic datasets and well-annotated datasets (e.g., CoNLL-2003) show that our proposed approach benefits negative sampling in terms of F1 score and loss convergence. Besides, models with improved negative sampling have achieved new state-of-the-art results on real-world datasets (e.g., EC).",
        "author": "Yangming Li; Lemao Liu; Shuming Shi",
        "authorids": "/y/yangming-li/; /l/lemao-liu/; /s/shuming-shi/",
        "bibtex": "@inproceedings{li-etal-2022-rethinking,\n    title = \"Rethinking Negative Sampling for Handling Missing Entity Annotations\",\n    author = \"Li, Yangming  and\n      Liu, Lemao  and\n      Shi, Shuming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.497/\",\n    doi = \"10.18653/v1/2022.acl-long.497\",\n    pages = \"7188--7197\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.497.pdf",
        "site": "https://aclanthology.org/2022.acl-long.497/",
        "pdf_size": 477231,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8648471000001573630&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tencent AI Lab; Tencent AI Lab; Tencent AI Lab",
        "aff_domain": "tencent.com;tencent.com;tencent.com",
        "email": "tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tencent",
        "aff_unique_dep": "Tencent AI Lab",
        "aff_unique_url": "https://ai.tencent.com",
        "aff_unique_abbr": "Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.307",
        "title": "Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We introduce the task of implicit offensive text detection in dialogues, where a statement may have either an offensive or non-offensive interpretation, depending on the listener and context. We argue that reasoning is crucial for understanding this broader class of offensive utterances, and release SLIGHT, a dataset to support research on this task. Experiments using the data show that state-of-the-art methods of offense detection perform poorly when asked to detect implicitly offensive statements, achieving only \u223c 11% accuracy. In contrast to existing offensive text detection datasets, SLIGHT features human-annotated chains of reasoning which describe the mental process by which an offensive interpretation can be reached from each ambiguous statement. We explore the potential for a multi-hop reasoning approach by utilizing existing entailment models to score the probability of these chains, and show that even naive reasoning models can yield improved performance in most situations. Analysis of the chains provides insight into the human interpretation process and emphasizes the importance of incorporating additional commonsense knowledge.",
        "author": "Qiang Zhang; Jason Naradowsky; Yusuke Miyao",
        "authorids": "/q/qiang-zhang/; /j/jason-naradowsky/; /y/yusuke-miyao/",
        "bibtex": "@inproceedings{zhang-etal-2022-rethinking,\n    title = \"Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem\",\n    author = \"Zhang, Qiang  and\n      Naradowsky, Jason  and\n      Miyao, Yusuke\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.307/\",\n    doi = \"10.18653/v1/2022.findings-acl.307\",\n    pages = \"3888--3905\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.307.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.307/",
        "pdf_size": 783594,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5225532407357244382&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, The University of Tokyo, Tokyo; Department of Computer Science, The University of Tokyo, Tokyo; Department of Computer Science, The University of Tokyo, Tokyo",
        "aff_domain": "g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;is.s.u-tokyo.ac.jp",
        "email": "g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;is.s.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.acl-long.418",
        "title": "Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Given the claims of improved text generation quality across various pre-trained neural models, we consider the coherence evaluation of machine generated text to be one of the principal applications of coherence models that needs to be investigated. Prior work in neural coherence modeling has primarily focused on devising new architectures for solving the permuted document task. We instead use a basic model architecture and show significant improvements over state of the art within the same training regime. We then design a harder self-supervision objective by increasing the ratio of negative samples within a contrastive learning setup, and enhance the model further through automatic hard negative mining coupled with a large global negative queue encoded by a momentum encoder. We show empirically that increasing the density of negative samples improves the basic model, and using a global negative queue further improves and stabilizes the model while training with hard negative samples. We evaluate the coherence model on task-independent test sets that resemble real-world applications and show significant improvements in coherence evaluations of downstream tasks.",
        "author": "Prathyusha Jwalapuram; Shafiq Joty; Xiang Lin",
        "authorids": "/p/prathyusha-jwalapuram/; /s/shafiq-joty/; /x/xiang-lin/",
        "bibtex": "@inproceedings{jwalapuram-etal-2022-rethinking,\n    title = \"Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling\",\n    author = \"Jwalapuram, Prathyusha  and\n      Joty, Shafiq  and\n      Lin, Xiang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.418/\",\n    doi = \"10.18653/v1/2022.acl-long.418\",\n    pages = \"6044--6059\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.418.pdf",
        "site": "https://aclanthology.org/2022.acl-long.418/",
        "pdf_size": 967685,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=287106064780165053&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Nanyang Technological University, Singapore+Salesforce Research Asia, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "github": "",
        "project": "https://ntunlpsg.github.io/project/coherence-paradigm",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Nanyang Technological University;Salesforce Research Asia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntu.edu.sg;https://research.salesforce.com",
        "aff_unique_abbr": "NTU;SRA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.acl-short.86",
        "title": "Rethinking and Refining the Distinct Metric",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Distinct is a widely used automatic metric for evaluating diversity in language generation tasks. However, we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations. We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score. Our experiments show that our proposed metric, Expectation-Adjusted Distinct (EAD), correlates better with human judgment in evaluating response diversity.To assist future research, we provide an example implementation at https://github.com/lsy641/Expectation-Adjusted-Distinct.",
        "author": "Siyang Liu; Sahand Sabour; Yinhe Zheng; Pei Ke; Xiaoyan Zhu; Minlie Huang",
        "authorids": "/s/siyang-liu/; /s/sahand-sabour/; /y/yinhe-zheng/; /p/pei-ke/; /x/xiaoyan-zhu/; /m/minlie-huang/",
        "bibtex": "@inproceedings{liu-etal-2022-rethinking,\n    title = \"Rethinking and Refining the Distinct Metric\",\n    author = \"Liu, Siyang  and\n      Sabour, Sahand  and\n      Zheng, Yinhe  and\n      Ke, Pei  and\n      Zhu, Xiaoyan  and\n      Huang, Minlie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.86/\",\n    doi = \"10.18653/v1/2022.acl-short.86\",\n    pages = \"762--770\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.86.pdf",
        "site": "https://aclanthology.org/2022.acl-short.86/",
        "pdf_size": 511127,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11978783977109858344&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China+Kuaishou, Beijing, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China+Lingxin AI, Beijing, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China",
        "aff_domain": "gmail.com;gmail.com;163.com;outlook.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "gmail.com;gmail.com;163.com;outlook.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "https://github.com/lsy641/Expectation-Adjusted-Distinct",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0+2;0;0;0",
        "aff_unique_norm": "Tsinghua University;Kuaishou;Lingxin AI",
        "aff_unique_dep": "Institute for Artificial Intelligence;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.kuaishou.com;",
        "aff_unique_abbr": "Tsinghua;;",
        "aff_campus_unique_index": "0+0;0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.117",
        "title": "Retrieval-guided Counterfactual Generation for QA",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals \u2014 i.e. minimally perturbed inputs \u2014 can help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements in a model\u2019s robustness to local perturbations.",
        "author": "Bhargavi Paranjape; Matthew Lamm; Ian Tenney",
        "authorids": "/b/bhargavi-paranjape/; /m/matthew-lamm/; /i/ian-tenney/",
        "bibtex": "@inproceedings{paranjape-etal-2022-retrieval,\n    title = \"Retrieval-guided Counterfactual Generation for {QA}\",\n    author = \"Paranjape, Bhargavi  and\n      Lamm, Matthew  and\n      Tenney, Ian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.117/\",\n    doi = \"10.18653/v1/2022.acl-long.117\",\n    pages = \"1670--1686\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.117.pdf",
        "site": "https://aclanthology.org/2022.acl-long.117/",
        "pdf_size": 574783,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7124780552252040356&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington; Google Research; Google Research",
        "aff_domain": "cs.washington.edu;google.com;google.com",
        "email": "cs.washington.edu;google.com;google.com",
        "github": "https://github.com/google-research/language/tree/master/language/qa_counterfactuals",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Washington;Google",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;Google Research",
        "aff_unique_url": "https://www.washington.edu;https://research.google",
        "aff_unique_abbr": "UW;Google Research",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Seattle;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.122",
        "title": "Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "It has been the norm for a long time to evaluate automated summarization tasks using the popular ROUGE metric. Although several studies in the past have highlighted the limitations of ROUGE, researchers have struggled to reach a consensus on a better alternative until today. One major limitation of the traditional ROUGE metric is the lack of semantic understanding (relies on direct overlap of n-grams). In this paper, we exclusively focus on the extractive summarization task and propose a semantic-aware nCG (normalized cumulative gain)-based evaluation metric (called Sem-nCG) for evaluating this task. One fundamental contribution of the paper is that it demonstrates how we can generate more reliable semantic-aware ground truths for evaluating extractive summarization tasks without any additional human intervention. To the best of our knowledge, this work is the first of its kind. We have conducted extensive experiments with this new metric using the widely used CNN/DailyMail dataset. Experimental results show that the new Sem-nCG metric is indeed semantic-aware, shows higher correlation with human judgement (more reliable) and yields a large number of disagreements with the original ROUGE metric (suggesting that ROUGE often leads to inaccurate conclusions also verified by humans).",
        "author": "Mousumi Akter; Naman Bansal; Shubhra Kanti Karmaker",
        "authorids": "/m/mousumi-akter/; /n/naman-bansal/; /s/shubhra-kanti-karmaker/",
        "bibtex": "@inproceedings{akter-etal-2022-revisiting,\n    title = \"Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than {ROUGE}?\",\n    author = \"Akter, Mousumi  and\n      Bansal, Naman  and\n      Karmaker, Shubhra Kanti\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.122/\",\n    doi = \"10.18653/v1/2022.findings-acl.122\",\n    pages = \"1547--1560\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.122.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.122/",
        "pdf_size": 360334,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16310612872720335939&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "BDI Lab, Auburn University, Alabama, USA; BDI Lab, Auburn University, Alabama, USA; BDI Lab, Auburn University, Alabama, USA",
        "aff_domain": "auburn.edu;auburn.edu;auburn.edu",
        "email": "auburn.edu;auburn.edu;auburn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Auburn University",
        "aff_unique_dep": "BDI Lab",
        "aff_unique_url": "https://www.auburn.edu",
        "aff_unique_abbr": "Auburn",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Alabama",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.564",
        "title": "Revisiting Over-Smoothness in Text to Speech",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Non-autoregressive text to speech (NAR-TTS) models have attracted much attention from both academia and industry due to their fast generation speed. One limitation of NAR-TTS models is that they ignore the correlation in time and frequency domains while generating speech mel-spectrograms, and thus cause blurry and over-smoothed results. In this work, we revisit this over-smoothing problem from a novel perspective: the degree of over-smoothness is determined by the gap between the complexity of data distributions and the capability of modeling methods. Both simplifying data distributions and improving modeling methods can alleviate the problem. Accordingly, we first study methods reducing the complexity of data distributions. Then we conduct a comprehensive study on NAR-TTS models that use some advanced modeling methods. Based on these studies, we find that 1) methods that provide additional condition inputs reduce the complexity of data distributions to model, thus alleviating the over-smoothing problem and achieving better voice quality. 2) Among advanced modeling methods, Laplacian mixture loss performs well at modeling multimodal distributions and enjoys its simplicity, while GAN and Glow achieve the best voice quality while suffering from increased training or model complexity. 3) The two categories of methods can be combined to further alleviate the over-smoothness and improve the voice quality. 4) Our experiments on the multi-speaker dataset lead to similar conclusions as above and providing more variance information can reduce the difficulty of modeling the target data distribution and alleviate the requirements for model capacity.",
        "author": "Yi Ren; Xu Tan; Tao Qin; Zhou Zhao; Tie-Yan Liu",
        "authorids": "/y/yi-ren/; /x/xu-tan/; /t/tao-qin/; /z/zhou-zhao/; /t/tie-yan-liu/",
        "bibtex": "@inproceedings{ren-etal-2022-revisiting,\n    title = \"Revisiting Over-Smoothness in Text to Speech\",\n    author = \"Ren, Yi  and\n      Tan, Xu  and\n      Qin, Tao  and\n      Zhao, Zhou  and\n      Liu, Tie-Yan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.564/\",\n    doi = \"10.18653/v1/2022.acl-long.564\",\n    pages = \"8197--8213\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.564.pdf",
        "site": "https://aclanthology.org/2022.acl-long.564/",
        "pdf_size": 4379897,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11633660030649739510&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University; Microsoft Research; Microsoft Research; Zhejiang University; Microsoft Research",
        "aff_domain": "zju.edu.cn;microsoft.com;microsoft.com;zju.edu.cn;microsoft.com",
        "email": "zju.edu.cn;microsoft.com;microsoft.com;zju.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "Zhejiang University;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "ZJU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.172",
        "title": "Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (\u201ctransformers\u201d) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.",
        "author": "Christopher Schr\u00f6der; Andreas Niekler; Martin Potthast",
        "authorids": "/c/christopher-schroder/; /a/andreas-niekler/; /m/martin-potthast/",
        "bibtex": "@inproceedings{schroder-etal-2022-revisiting,\n    title = \"Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers\",\n    author = {Schr{\\\"o}der, Christopher  and\n      Niekler, Andreas  and\n      Potthast, Martin},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.172/\",\n    doi = \"10.18653/v1/2022.findings-acl.172\",\n    pages = \"2194--2203\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.172.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.172/",
        "pdf_size": 723226,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13317604667995542955&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Leipzig University; Leipzig University; Leipzig University",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Leipzig University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-leipzig.de",
        "aff_unique_abbr": "Uni Leipzig",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-short.46",
        "title": "Revisiting the Compositional Generalization Abilities of Neural Sequence Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future.",
        "author": "Arkil Patel; Satwik Bhattamishra; Phil Blunsom; Navin Goyal",
        "authorids": "/a/arkil-patel/; /s/satwik-bhattamishra/; /p/phil-blunsom/; /n/navin-goyal/",
        "bibtex": "@inproceedings{patel-etal-2022-revisiting,\n    title = \"Revisiting the Compositional Generalization Abilities of Neural Sequence Models\",\n    author = \"Patel, Arkil  and\n      Bhattamishra, Satwik  and\n      Blunsom, Phil  and\n      Goyal, Navin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.46/\",\n    doi = \"10.18653/v1/2022.acl-short.46\",\n    pages = \"424--434\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.46.pdf",
        "site": "https://aclanthology.org/2022.acl-short.46/",
        "pdf_size": 760764,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8692141667317782214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Research India; University of Oxford; University of Oxford; Microsoft Research India",
        "aff_domain": "gmail.com;cs.ox.ac.uk;cs.ox.ac.uk;microsoft.com",
        "email": "gmail.com;cs.ox.ac.uk;cs.ox.ac.uk;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Microsoft;University of Oxford",
        "aff_unique_dep": "Microsoft Research India;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india;https://www.ox.ac.uk",
        "aff_unique_abbr": "MSR India;Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "India;United Kingdom"
    },
    {
        "id": "2022.findings-acl.230",
        "title": "Revisiting the Effects of Leakage on Dependency Parsing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent work by S\u00f8gaard (2020) showed that, treebank size aside, overlap between training and test graphs (termed leakage) explains more of the observed variation in dependency parsing performance than other explanations. In this work we revisit this claim, testing it on more models and languages. We find that it only holds for zero-shot cross-lingual settings. We then propose a more fine-grained measure of such leakage which, unlike the original measure, not only explains but also correlates with observed performance variation. Code and data are available here: https://github.com/miriamwanner/reu-nlp-project",
        "author": "Nathaniel Krasner; Miriam Wanner; Antonios Anastasopoulos",
        "authorids": "/n/nathaniel-krasner/; /m/miriam-wanner/; /a/antonios-anastasopoulos/",
        "bibtex": "@inproceedings{krasner-etal-2022-revisiting,\n    title = \"Revisiting the Effects of Leakage on Dependency Parsing\",\n    author = \"Krasner, Nathaniel  and\n      Wanner, Miriam  and\n      Anastasopoulos, Antonios\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.230/\",\n    doi = \"10.18653/v1/2022.findings-acl.230\",\n    pages = \"2925--2934\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.230.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.230/",
        "pdf_size": 259536,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=668302353158777750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, George Mason University; Department of Computer Science, University of Virginia; Department of Computer Science, George Mason University",
        "aff_domain": "gmail.com;virginia.edu;gmu.edu",
        "email": "gmail.com;virginia.edu;gmu.edu",
        "github": "https://github.com/miriamwanner/reu-nlp-project",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "George Mason University;University of Virginia",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.gmu.edu;https://www.virginia.edu",
        "aff_unique_abbr": "GMU;UVA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.80",
        "title": "Rewarding Semantic Similarity under Optimized Alignments for AMR-to-Text Generation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards. However, metrics such as BERTScore greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference. Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting. We address these issues by proposing metrics that replace the greedy alignments in BERTScore with optimized ones. We compute them on a model\u2019s trained token embeddings to prevent domain mismatch. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. In addition, we find that this approach enjoys stable training compared to a non-RL setting.",
        "author": "Lisa Jin; Daniel Gildea",
        "authorids": "/l/lisa-jin/; /d/daniel-gildea/",
        "bibtex": "@inproceedings{jin-gildea-2022-rewarding,\n    title = \"Rewarding Semantic Similarity under Optimized Alignments for {AMR}-to-Text Generation\",\n    author = \"Jin, Lisa  and\n      Gildea, Daniel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.80/\",\n    doi = \"10.18653/v1/2022.acl-short.80\",\n    pages = \"710--715\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.80.pdf",
        "site": "https://aclanthology.org/2022.acl-short.80/",
        "pdf_size": 257382,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1460237374720472749&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Rochester; Department of Computer Science, University of Rochester",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.329",
        "title": "Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models (PLMs). Despite the growing progress of probing knowledge for PLMs in the general domain, specialised areas such as the biomedical domain are vastly under-explored. To facilitate this, we release a well-curated biomedical knowledge probing benchmark, MedLAMA, constructed based on the Unified Medical Language System (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10. While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks. To achieve this, we propose Contrastive-Probe, a novel self-supervised contrastive probing approach, that adjusts the underlying PLMs without using any probing data. While Contrastive-Probe pushes the acc@10 to 28%, the performance gap still remains notable. Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is still under-estimated as UMLS still does not include the full spectrum of factual knowledge. We hope MedLAMA and Contrastive-Probe facilitate further developments of more suited probing techniques for this domain. Our code and dataset are publicly available at https://github.com/cambridgeltl/medlama.",
        "author": "Zaiqiao Meng; Fangyu Liu; Ehsan Shareghi; Yixuan Su; Charlotte Collins; Nigel Collier",
        "authorids": "/z/zaiqiao-meng/; /f/fangyu-liu/; /e/ehsan-shareghi/; /y/yixuan-su/; /c/charlotte-collins/; /n/nigel-collier/",
        "bibtex": "@inproceedings{meng-etal-2022-rewire,\n    title = \"Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models\",\n    author = \"Meng, Zaiqiao  and\n      Liu, Fangyu  and\n      Shareghi, Ehsan  and\n      Su, Yixuan  and\n      Collins, Charlotte  and\n      Collier, Nigel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.329/\",\n    doi = \"10.18653/v1/2022.acl-long.329\",\n    pages = \"4798--4810\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.329.pdf",
        "site": "https://aclanthology.org/2022.acl-long.329/",
        "pdf_size": 418968,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17339082555159064330&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Language Technology Lab, University of Cambridge+Department of Computing Science, University of Glasgow; Language Technology Lab, University of Cambridge; Department of Data Science and AI, Monash University+Language Technology Lab, University of Cambridge; Language Technology Lab, University of Cambridge; Language Technology Lab, University of Cambridge; Language Technology Lab, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;monash.edu;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;monash.edu;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "https://github.com/cambridgeltl/medlama",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;2+0;0;0;0",
        "aff_unique_norm": "University of Cambridge;University of Glasgow;Monash University",
        "aff_unique_dep": "Language Technology Lab;Department of Computing Science;Department of Data Science and AI",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.gla.ac.uk;https://www.monash.edu",
        "aff_unique_abbr": "Cambridge;UofG;Monash",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+0;0;1+0;0;0;0",
        "aff_country_unique": "United Kingdom;Australia"
    },
    {
        "id": "2022.findings-acl.164",
        "title": "Richer Countries and Richer Representations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We examine whether some countries are more richly represented in embedding space than others. We find that countries whose names occur with low frequency in training corpora are more likely to be tokenized into subwords, are less semantically distinct in embedding space, and are less likely to be correctly predicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted for, \u201cThe country producing the most cocoa is [MASK].\u201d. Although these performance discrepancies and representational harms are due to frequency, we find that frequency is highly correlated with a country\u2019s GDP; thus perpetuating historic power and wealth inequalities. We analyze the effectiveness of mitigation strategies; recommend that researchers report training word frequencies; and recommend future work for the community to define and design representational guarantees.",
        "author": "Kaitlyn Zhou; Kawin Ethayarajh; Dan Jurafsky",
        "authorids": "/k/kaitlyn-zhou/; /k/kawin-ethayarajh/; /d/dan-jurafsky/",
        "bibtex": "@inproceedings{zhou-etal-2022-richer,\n    title = \"Richer Countries and Richer Representations\",\n    author = \"Zhou, Kaitlyn  and\n      Ethayarajh, Kawin  and\n      Jurafsky, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.164/\",\n    doi = \"10.18653/v1/2022.findings-acl.164\",\n    pages = \"2074--2085\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.164.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.164/",
        "pdf_size": 264907,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=993941883185743767&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.231",
        "title": "Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "When pre-trained contextualized embedding-based models developed for unstructured data are adapted for structured tabular data, they perform admirably. However, recent probing studies show that these models use spurious correlations, and often predict inference labels by focusing on false evidence or ignoring it altogether. To study this issue, we introduce the task of Trustworthy Tabular Reasoning, where a model needs to extract evidence to be used for reasoning, in addition to predicting the label. As a case study, we propose a two-stage sequential prediction approach, which includes an evidence extraction and an inference stage. First, we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for InfoTabS, a tabular NLI benchmark. Our evidence extraction strategy outperforms earlier baselines. On the downstream tabular inference task, using only the automatically extracted evidence as the premise, our approach outperforms prior benchmarks.",
        "author": "Vivek Gupta; Shuo Zhang; Alakananda Vempala; Yujie He; Temma Choji; Vivek Srikumar",
        "authorids": "/v/vivek-gupta/; /s/shuo-zhang/; /a/alakananda-vempala/; /y/yujie-he/; /t/temma-choji/; /v/vivek-srikumar/",
        "bibtex": "@inproceedings{gupta-etal-2022-right,\n    title = \"Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning\",\n    author = \"Gupta, Vivek  and\n      Zhang, Shuo  and\n      Vempala, Alakananda  and\n      He, Yujie  and\n      Choji, Temma  and\n      Srikumar, Vivek\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.231/\",\n    doi = \"10.18653/v1/2022.acl-long.231\",\n    pages = \"3268--3283\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.231.pdf",
        "site": "https://aclanthology.org/2022.acl-long.231/",
        "pdf_size": 465667,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18142475926477391268&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Utah; Bloomberg; Bloomberg; Bloomberg; Bloomberg; University of Utah",
        "aff_domain": "cs.utah.edu;bloomberg.net;bloomberg.net;bloomberg.net;bloomberg.net;cs.utah.edu",
        "email": "cs.utah.edu;bloomberg.net;bloomberg.net;bloomberg.net;bloomberg.net;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of Utah;Bloomberg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utah.edu;https://www.bloomberg.com",
        "aff_unique_abbr": "Utah;Bloomberg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.65",
        "title": "RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large-scale pretrained language models have achieved SOTA results on NLP tasks. However, they have been shown vulnerable to adversarial attacks especially for logographic languages like Chinese. In this work, we propose RoCBert: a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation, synonyms, typos, etc. It is pretrained with the contrastive learning objective which maximizes the label consistency under different synthesized adversarial examples. The model takes as input multimodal information including the semantic, phonetic and visual features. We show all these features areimportant to the model robustness since the attack can be performed in all the three forms. Across 5 Chinese NLU tasks, RoCBert outperforms strong baselines under three blackbox adversarial algorithms without sacrificing the performance on clean testset. It also performs the best in the toxic content detection task under human-made attacks.",
        "author": "Hui Su; Weiwei Shi; Xiaoyu Shen; Zhou Xiao; Tuo Ji; Jiarui Fang; Jie Zhou",
        "authorids": "/h/hui-su/; /w/weiwei-shi/; /x/xiaoyu-shen/; /z/zhou-xiao/; /t/tuo-ji/; /j/jiarui-fang/; /j/jie-zhou/",
        "bibtex": "@inproceedings{su-etal-2022-rocbert,\n    title = \"{R}o{CB}ert: Robust {C}hinese Bert with Multimodal Contrastive Pretraining\",\n    author = \"Su, Hui  and\n      Shi, Weiwei  and\n      Shen, Xiaoyu  and\n      Xiao, Zhou  and\n      Ji, Tuo  and\n      Fang, Jiarui  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.65/\",\n    doi = \"10.18653/v1/2022.acl-long.65\",\n    pages = \"921--931\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.65.pdf",
        "site": "https://aclanthology.org/2022.acl-long.65/",
        "pdf_size": 453220,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18247250212048655107&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Pattern Recognition Center, Wechat AI, Tencent Inc, China; Pattern Recognition Center, Wechat AI, Tencent Inc, China; Saarland Informatics Campus; Pattern Recognition Center, Wechat AI, Tencent Inc, China; Pattern Recognition Center, Wechat AI, Tencent Inc, China; Pattern Recognition Center, Wechat AI, Tencent Inc, China; Pattern Recognition Center, Wechat AI, Tencent Inc, China",
        "aff_domain": "tencent.com; ; ; ; ; ; ",
        "email": "tencent.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;0;0",
        "aff_unique_norm": "Tencent;Saarland University",
        "aff_unique_dep": "Pattern Recognition Center, Wechat AI;Department of Computer Science",
        "aff_unique_url": "https://www.tencent.com;https://www.uni-saarland.de",
        "aff_unique_abbr": "Tencent;Uni Saar",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Saarbr\u00fccken",
        "aff_country_unique_index": "0;0;1;0;0;0;0",
        "aff_country_unique": "China;Germany"
    },
    {
        "id": "2022.acl-long.387",
        "title": "RoMe: A Robust Metric for Evaluating Natural Language Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Evaluating Natural Language Generation (NLG) systems is a challenging task. Firstly, the metric should ensure that the generated hypothesis reflects the reference\u2019s semantics. Secondly, it should consider the grammatical quality of the generated sentence. Thirdly, it should be robust enough to handle various surface forms of the generated sentence. Thus, an effective evaluation metric has to be multifaceted. In this paper, we propose an automatic evaluation metric incorporating several core aspects of natural language understanding (language competence, syntactic and semantic variation). Our proposed metric, RoMe, is trained on language features such as semantic similarity combined with tree edit distance and grammatical acceptability, using a self-supervised neural network to assess the overall quality of the generated sentence. Moreover, we perform an extensive robustness analysis of the state-of-the-art methods and RoMe. Empirical results suggest that RoMe has a stronger correlation to human judgment over state-of-the-art metrics in evaluating system-generated sentences across several NLG tasks.",
        "author": "Md Rashad Al Hasan Rony; Liubov Kovriguina; Debanjan Chaudhuri; Ricardo Usbeck; Jens Lehmann",
        "authorids": "/m/md-rashad-al-hasan-rony/; /l/liubov-kovriguina/; /d/debanjan-chaudhuri/; /r/ricardo-usbeck/; /j/jens-lehmann/",
        "bibtex": "@inproceedings{rony-etal-2022-rome,\n    title = \"{R}o{M}e: A Robust Metric for Evaluating Natural Language Generation\",\n    author = \"Rony, Md Rashad Al Hasan  and\n      Kovriguina, Liubov  and\n      Chaudhuri, Debanjan  and\n      Usbeck, Ricardo  and\n      Lehmann, Jens\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.387/\",\n    doi = \"10.18653/v1/2022.acl-long.387\",\n    pages = \"5645--5657\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.387.pdf",
        "site": "https://aclanthology.org/2022.acl-long.387/",
        "pdf_size": 1537245,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3924560274343480923&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Bonn+Fraunhofer IAIS Dresden; Fraunhofer IAIS Dresden; University of Bonn+Fraunhofer IAIS Dresden; University of Hamburg; University of Bonn+Fraunhofer IAIS Dresden",
        "aff_domain": "iais.fraunhofer.de;iais.fraunhofer.de;uni-bonn.de;uni-hamburg.de;iais.fraunhofer.de",
        "email": "iais.fraunhofer.de;iais.fraunhofer.de;uni-bonn.de;uni-hamburg.de;iais.fraunhofer.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;0+1;2;0+1",
        "aff_unique_norm": "University of Bonn;Fraunhofer Institute for Applied Information Technology;University of Hamburg",
        "aff_unique_dep": ";IAIS;",
        "aff_unique_url": "https://www.uni-bonn.de/;https://www.iais.fraunhofer.de;https://www.uni-hamburg.de",
        "aff_unique_abbr": "UBonn;Fraunhofer IAIS;UHH",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Dresden",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.157",
        "title": "Robust Lottery Tickets for Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization. Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.",
        "author": "Rui Zheng; Bao Rong; Yuhao Zhou; Di Liang; Sirui Wang; Wei Wu; Tao Gui; Qi Zhang; Xuanjing Huang",
        "authorids": "/r/rui-zheng/; /b/bao-rong/; /y/yuhao-zhou/; /d/di-liang/; /s/sirui-wang/; /w/wei-wu/; /t/tao-gui/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{zheng-etal-2022-robust,\n    title = \"Robust Lottery Tickets for Pre-trained Language Models\",\n    author = \"Zheng, Rui  and\n      Rong, Bao  and\n      Zhou, Yuhao  and\n      Liang, Di  and\n      Wang, Sirui  and\n      Wu, Wei  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.157/\",\n    doi = \"10.18653/v1/2022.acl-long.157\",\n    pages = \"2211--2224\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.157.pdf",
        "site": "https://aclanthology.org/2022.acl-long.157/",
        "pdf_size": 669850,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9753256032570043124&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Meituan Inc., Beijing, China; Meituan Inc., Beijing, China; Meituan Inc., Beijing, China; Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China+Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Fudan University; School of Computer Science, Fudan University, Shanghai, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn; ; ; ;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn; ; ; ;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;1;1;1;0;0+0;0",
        "aff_unique_norm": "Fudan University;Meituan Inc.",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.meituan.com",
        "aff_unique_abbr": "Fudan;Meituan",
        "aff_campus_unique_index": "0;0;0;1;1;1;0;0+0;0",
        "aff_campus_unique": "Shanghai;Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.402",
        "title": "RotateQVS: Representing Temporal Information as Rotations in Quaternion Vector Space for Temporal Knowledge Graph Completion",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Temporal factors are tied to the growth of facts in realistic applications, such as the progress of diseases and the development of political situation, therefore, research on Temporal Knowledge Graph (TKG) attracks much attention. In TKG, relation patterns inherent with temporality are required to be studied for representation learning and reasoning across temporal facts. However, existing methods can hardly model temporal relation patterns, nor can capture the intrinsic connections between relations when evolving over time, lacking of interpretability. In this paper, we propose a novel temporal modeling method which represents temporal entities as Rotations in Quaternion Vector Space (RotateQVS) and relations as complex vectors in Hamilton\u2019s quaternion space. We demonstrate our method can model key patterns of relations in TKG, such as symmetry, asymmetry, inverse, and can capture time-evolved relations by theory. And empirically, we show that our method can boost the performance of link prediction tasks over four temporal knowledge graph benchmarks.",
        "author": "Kai Chen; Ye Wang; Yitong Li; Aiping Li",
        "authorids": "/k/kai-chen/; /y/ye-wang/; /y/yitong-li/; /a/aiping-li/",
        "bibtex": "@inproceedings{chen-etal-2022-rotateqvs,\n    title = \"{R}otate{QVS}: Representing Temporal Information as Rotations in Quaternion Vector Space for Temporal Knowledge Graph Completion\",\n    author = \"Chen, Kai  and\n      Wang, Ye  and\n      Li, Yitong  and\n      Li, Aiping\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.402/\",\n    doi = \"10.18653/v1/2022.acl-long.402\",\n    pages = \"5843--5857\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.402.pdf",
        "site": "https://aclanthology.org/2022.acl-long.402/",
        "pdf_size": 1035993,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=410968317731521052&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "National University of Defense Technology; National University of Defense Technology + Pengcheng Laboratory; Huawei Technologies Co., Ltd.; National University of Defense Technology",
        "aff_domain": "nudt.edu.cn;nudt.edu.cn;huawei.com;nudt.edu.cn",
        "email": "nudt.edu.cn;nudt.edu.cn;huawei.com;nudt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;2;0",
        "aff_unique_norm": "National University of Defense Technology;Pengcheng Laboratory;Huawei",
        "aff_unique_dep": ";;Huawei Technologies",
        "aff_unique_url": "http://www.nudt.edu.cn/;;https://www.huawei.com",
        "aff_unique_abbr": "NUDT;;Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.21",
        "title": "RuCCoN: Clinical Concept Normalization in Russian",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present RuCCoN, a new dataset for clinical concept normalization in Russian manually annotated by medical professionals. It contains over 16,028 entity mentions manually linked to over 2,409 unique concepts from the Russian language part of the UMLS ontology. We provide train/test splits for different settings (stratified, zero-shot, and CUI-less) and present strong baselines obtained with state-of-the-art models such as SapBERT. At present, Russian medical NLP is lacking in both datasets and trained models, and we view this work as an important step towards filling this gap. Our dataset and annotation guidelines are available at https://github.com/AIRI-Institute/RuCCoN.",
        "author": "Alexandr Nesterov; Galina Zubkova; Zulfat Miftahutdinov; Vladimir Kokh; Elena Tutubalina; Artem Shelmanov; Anton Alekseev; Manvel Avetisian; Andrey Chertok; Sergey Nikolenko",
        "authorids": "/a/alexandr-nesterov/; /g/galina-zubkova/; /z/zulfat-miftahutdinov/; /v/vladimir-kokh/; /e/elena-tutubalina/; /a/artem-shelmanov/; /a/anton-alekseev/; /m/manvel-avetisian/; /a/andrey-chertok/; /s/sergey-nikolenko/",
        "bibtex": "@inproceedings{nesterov-etal-2022-ruccon,\n    title = \"{R}u{CC}o{N}: Clinical Concept Normalization in {R}ussian\",\n    author = \"Nesterov, Alexandr  and\n      Zubkova, Galina  and\n      Miftahutdinov, Zulfat  and\n      Kokh, Vladimir  and\n      Tutubalina, Elena  and\n      Shelmanov, Artem  and\n      Alekseev, Anton  and\n      Avetisian, Manvel  and\n      Chertok, Andrey  and\n      Nikolenko, Sergey\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.21/\",\n    doi = \"10.18653/v1/2022.findings-acl.21\",\n    pages = \"239--245\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.21.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.21/",
        "pdf_size": 191336,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=185690252271586979&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Sber AI Lab, Moscow, Russia; Sber AI Lab, Moscow, Russia; Kazan Federal University, Kazan, Russia; Sber AI Lab, Moscow, Russia; Kazan Federal University, Kazan, Russia+HSE University, Moscow, Russia+Sber AI, Moscow, Russia; AIRI, Moscow, Russia; St. Petersburg Department of Steklov Mathematical Institute, St. Petersburg, Russia; Sber AI Lab, Moscow, Russia; Sber AI, Moscow, Russia+AIRI, Moscow, Russia; St. Petersburg Department of Steklov Mathematical Institute, St. Petersburg, Russia+ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia+Neuromation OU, Tallinn, Estonia",
        "aff_domain": "sberbank.ru;sberbank.ru; ; ; ; ; ; ; ; ",
        "email": "sberbank.ru;sberbank.ru; ; ; ; ; ; ; ; ",
        "github": "https://github.com/sberbank-ai-lab/RuCCoN",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;1;0;1+2+0;3;4;0;0+3;4+5+6",
        "aff_unique_norm": "Sberbank;Kazan Federal University;HSE University;Advanced Institute for Research and Innovation;Steklov Mathematical Institute;ISP RAS Research Center for Trusted Artificial Intelligence;Neuromation OU",
        "aff_unique_dep": "Sber AI Lab;;;;St. Petersburg Department;;",
        "aff_unique_url": "https://sberbank.ru;https://kpfu.ru;https://hse.ru;;http://www.mathsoc.spb.ru;;",
        "aff_unique_abbr": "Sber;KFU;HSE;AIRI;SMI;;",
        "aff_campus_unique_index": "0;0;1;0;1+0+0;0;2;0;0+0;2",
        "aff_campus_unique": "Moscow;Kazan;St. Petersburg;",
        "aff_country_unique_index": "0;0;0;0;0+0+0;0;0;0;0+0;0+0+1",
        "aff_country_unique": "Russian Federation;Estonia"
    },
    {
        "id": "2022.findings-acl.99",
        "title": "S2SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing. The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well. In this paper, we propose S2SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectively leverages the syntactic dependency information of questions in text-to-SQL to improve the performance. We also employ the decoupling constraint to induce diverse relational edge embedding, which further improves the network\u2019s performance. Experiments on the Spider and robustness setting Spider-Syn demonstrate that the proposed approach outperforms all existing methods when pre-training models are used, resulting in a performance ranks first on the Spider leaderboard.",
        "author": "Binyuan Hui; Ruiying Geng; Lihan Wang; Bowen Qin; Yanyang Li; Bowen Li; Jian Sun; Yongbin Li",
        "authorids": "/b/binyuan-hui/; /r/ruiying-geng/; /l/lihan-wang/; /b/bowen-qin/; /y/yanyang-li/; /b/bowen-li/; /j/jian-sun/; /y/yongbin-li/",
        "bibtex": "@inproceedings{hui-etal-2022-s2sql,\n    title = \"{S}$^2${SQL}: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-{SQL} Parsers\",\n    author = \"Hui, Binyuan  and\n      Geng, Ruiying  and\n      Wang, Lihan  and\n      Qin, Bowen  and\n      Li, Yanyang  and\n      Li, Bowen  and\n      Sun, Jian  and\n      Li, Yongbin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.99/\",\n    doi = \"10.18653/v1/2022.findings-acl.99\",\n    pages = \"1254--1262\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.99.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.99/",
        "pdf_size": 524849,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12685144676668038879&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Alibaba Group; Alibaba Group; Alibaba Group + University of Chinese Academy of Sciences + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Alibaba Group + University of Chinese Academy of Sciences + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; The Chinese University of Hong Kong; Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0+1+2;0+1+2;3;0;0;0",
        "aff_unique_norm": "Alibaba Group;University of Chinese Academy of Sciences;Chinese Academy of Sciences;Chinese University of Hong Kong",
        "aff_unique_dep": ";;Shenzhen Institutes of Advanced Technology;",
        "aff_unique_url": "https://www.alibaba.com;http://www.ucas.ac.cn;http://www.siat.cas.cn;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "Alibaba;UCAS;SIAT;CUHK",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Shenzhen;Hong Kong SAR",
        "aff_country_unique_index": "0;0;0+0+0;0+0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.58",
        "title": "S4-Tuning: A Simple Cross-lingual Sub-network Tuning Method",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "The emergence of multilingual pre-trained language models makes it possible to adapt to target languages with only few labeled examples. However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios. To address two problems elegantly, we propose S4-Tuning, a Simple Cross-lingual Sub-network Tuning method. S4-Tuning first detects the most essential sub-network for each target language, and only updates it during fine-tuning.In this way, the language sub-networks lower the scale of trainable parameters, and hence better suit the low-resource scenarios.Meanwhile, the commonality and characteristics across languages are modeled by the overlapping and non-overlapping parts to ease the interference among languages.Simple but effective, S4-Tuning gains consistent improvements over vanilla fine-tuning on three multi-lingual tasks involving 37 different languages in total (XNLI, PAWS-X, and Tatoeba).",
        "author": "Runxin Xu; Fuli Luo; Baobao Chang; Songfang Huang; Fei Huang",
        "authorids": "/r/runxin-xu/; /f/fuli-luo/; /b/baobao-chang/; /s/songfang-huang/; /f/fei-huang/",
        "bibtex": "@inproceedings{xu-etal-2022-s4,\n    title = \"S$^4$-Tuning: A Simple Cross-lingual Sub-network Tuning Method\",\n    author = \"Xu, Runxin  and\n      Luo, Fuli  and\n      Chang, Baobao  and\n      Huang, Songfang  and\n      Huang, Fei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.58/\",\n    doi = \"10.18653/v1/2022.acl-short.58\",\n    pages = \"530--537\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.58.pdf",
        "site": "https://aclanthology.org/2022.acl-short.58/",
        "pdf_size": 363888,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6024209297909430373&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Key Laboratory of Computational Linguistics, Peking University, MOE, China + Alibaba Group; Alibaba Group; Key Laboratory of Computational Linguistics, Peking University, MOE, China + Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "gmail.com;alibaba-inc.com;pku.edu.cn;alibaba-inc.com;alibaba-inc.com",
        "email": "gmail.com;alibaba-inc.com;pku.edu.cn;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;0+1;1;1",
        "aff_unique_norm": "Peking University;Alibaba Group",
        "aff_unique_dep": "Key Laboratory of Computational Linguistics;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "PKU;Alibaba",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.44",
        "title": "SCD: Self-Contrastive Decorrelation of Sentence Embeddings",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In this paper, we propose Self-Contrastive Decorrelation (SCD), a self-supervised approach. Given an input sentence, it optimizes a joint self-contrastive and decorrelation objective. Learning a representation is facilitated by leveraging the contrast arising from the instantiation of standard dropout at different rates. The proposed method is conceptually simple yet empirically powerful. It achieves comparable results with state-of-the-art methods on multiple benchmarks without using contrastive pairs. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods.",
        "author": "Tassilo Klein; Moin Nabi",
        "authorids": "/t/tassilo-klein/; /m/moin-nabi/",
        "bibtex": "@inproceedings{klein-nabi-2022-scd,\n    title = \"{SCD}: Self-Contrastive Decorrelation of Sentence Embeddings\",\n    author = \"Klein, Tassilo  and\n      Nabi, Moin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.44/\",\n    doi = \"10.18653/v1/2022.acl-short.44\",\n    pages = \"394--400\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.44.pdf",
        "site": "https://aclanthology.org/2022.acl-short.44/",
        "pdf_size": 354065,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13104579487307670369&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "SAP AI Research; SAP AI Research",
        "aff_domain": "sap.com;sap.com",
        "email": "sap.com;sap.com",
        "github": "https://github.com/SAP-samples/acl2022-self-contrastive-decorrelation/",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "SAP",
        "aff_unique_dep": "SAP AI Research",
        "aff_unique_url": "https://www.sap.com",
        "aff_unique_abbr": "SAP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.457",
        "title": "SDR: Efficient Neural Re-ranking using Succinct Document Representation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "BERT based ranking models have achieved superior performance on various information retrieval tasks. However, the large number of parameters and complex self-attention operations come at a significant latency overhead. To remedy this, recent works propose late-interaction architectures, which allow pre-computation of intermediate document representations, thus reducing latency. Nonetheless, having solved the immediate latency issue, these methods now introduce storage costs and network fetching latency, which limit their adoption in real-life production systems. In this work, we propose the Succinct Document Representation (SDR) scheme that computes highly compressed intermediate document representations, mitigating the storage/network issue. Our approach first reduces the dimension of token representations by encoding them using a novel autoencoder architecture that uses the document\u2019s textual content in both the encoding and decoding phases. After this token encoding step, we further reduce the size of the document representations using modern quantization techniques. Evaluation on MSMARCO\u2019s passage re-reranking task show that compared to existing approaches using compressed document representations, our method is highly efficient, achieving 4x\u201311.6x higher compression rates for the same ranking quality. Similarly, on the TREC CAR dataset, we achieve 7.7x higher compression rate for the same ranking quality.",
        "author": "Nachshon Cohen; Amit Portnoy; Besnik Fetahu; Amir Ingber",
        "authorids": "/n/nachshon-cohen/; /a/amit-portnoy/; /b/besnik-fetahu/; /a/amir-ingber/",
        "bibtex": "@inproceedings{cohen-etal-2022-sdr,\n    title = \"{SDR}: Efficient Neural Re-ranking using Succinct Document Representation\",\n    author = \"Cohen, Nachshon  and\n      Portnoy, Amit  and\n      Fetahu, Besnik  and\n      Ingber, Amir\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.457/\",\n    doi = \"10.18653/v1/2022.acl-long.457\",\n    pages = \"6624--6637\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.457.pdf",
        "site": "https://aclanthology.org/2022.acl-long.457/",
        "pdf_size": 517713,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6432589177928942417&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Amazon; Ben-Gurion University\u2020; Amazon; Pinecone Systems\u2020",
        "aff_domain": "amazon.com;post.bgu.ac.il;amazon.com;pinecone.io",
        "email": "amazon.com;post.bgu.ac.il;amazon.com;pinecone.io",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Amazon;Ben-Gurion University of the Negev;Pinecone Systems",
        "aff_unique_dep": "Amazon.com, Inc.;;",
        "aff_unique_url": "https://www.amazon.com;https://www.bgu.ac.il;",
        "aff_unique_abbr": "Amazon;BGU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Israel;"
    },
    {
        "id": "2022.acl-long.459",
        "title": "SHIELD: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Even though several methods have proposed to defend textual neural network (NN) models against black-box adversarial attacks, they often defend against a specific text perturbation strategy and/or require re-training the models from scratch. This leads to a lack of generalization in practice and redundant computation. In particular, the state-of-the-art transformer models (e.g., BERT, RoBERTa) require great time and computation resources. By borrowing an idea from software engineering, in order to address these limitations, we propose a novel algorithm, SHIELD, which modifies and re-trains only the last layer of a textual NN, and thus it \u201cpatches\u201d and \u201ctransforms\u201d the NN into a stochastic weighted ensemble of multi-expert prediction heads. Considering that most of current black-box attacks rely on iterative search mechanisms to optimize their adversarial perturbations, SHIELD confuses the attackers by automatically utilizing different weighted ensembles of predictors depending on the input. In other words, SHIELD breaks a fundamental assumption of the attack, which is a victim NN model remains constant during an attack. By conducting comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and RoBERTa-based textual NNs, once patched by SHIELD, exhibit a relative enhancement of 15%\u201370% in accuracy on average against 14 different black-box attacks, outperforming 6 defensive baselines across 3 public datasets. All codes are to be released.",
        "author": "Thai Le; Noseong Park; Dongwon Lee",
        "authorids": "/t/thai-le/; /n/noseong-park/; /d/dongwon-lee/",
        "bibtex": "@inproceedings{le-etal-2022-shield,\n    title = \"{SHIELD}: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher\",\n    author = \"Le, Thai  and\n      Park, Noseong  and\n      Lee, Dongwon\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.459/\",\n    doi = \"10.18653/v1/2022.acl-long.459\",\n    pages = \"6661--6674\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.459.pdf",
        "site": "https://aclanthology.org/2022.acl-long.459/",
        "pdf_size": 728854,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8309037796338959786&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Penn State University; Yonsei University; Penn State University",
        "aff_domain": "psu.edu;yonsei.ac.kr;psu.edu",
        "email": "psu.edu;yonsei.ac.kr;psu.edu",
        "github": "github.com/lethaiq/shield-defend-adversarial-texts",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Penn State University;Yonsei University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.psu.edu;https://www.yonsei.ac.kr",
        "aff_unique_abbr": "PSU;Yonsei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2022.acl-long.346",
        "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks. More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000\u00d7 fewer task-specific parameters. To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer. Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.",
        "author": "Tu Vu; Brian Lester; Noah Constant; Rami Al-Rfou\u2019; Daniel Cer",
        "authorids": "/t/tu-vu/; /b/brian-lester/; /n/noah-constant/; /r/rami-al-rfou/; /d/daniel-cer/",
        "bibtex": "@inproceedings{vu-etal-2022-spot,\n    title = \"{SP}o{T}: Better Frozen Model Adaptation through Soft Prompt Transfer\",\n    author = \"Vu, Tu  and\n      Lester, Brian  and\n      Constant, Noah  and\n      Al-Rfou{'}, Rami  and\n      Cer, Daniel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.346/\",\n    doi = \"10.18653/v1/2022.acl-long.346\",\n    pages = \"5039--5059\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.346.pdf",
        "site": "https://aclanthology.org/2022.acl-long.346/",
        "pdf_size": 996278,
        "gs_citation": 308,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12539188987613138387&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Massachusetts Amherst; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "cs.umass.edu;google.com;google.com;google.com;google.com",
        "email": "cs.umass.edu;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.umass.edu;https://research.google",
        "aff_unique_abbr": "UMass Amherst;Google Research",
        "aff_campus_unique_index": "0;1;1;1;1",
        "aff_campus_unique": "Amherst;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.314",
        "title": "SRL4E \u2013 Semantic Role Labeling for Emotions: A Unified Evaluation Framework",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents. However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area. In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme. We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area.",
        "author": "Cesare Campagnano; Simone Conia; Roberto Navigli",
        "authorids": "/c/cesare-campagnano/; /s/simone-conia/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{campagnano-etal-2022-srl4e,\n    title = \"{SRL4E} {--} {S}emantic {R}ole {L}abeling for {E}motions: {A} Unified Evaluation Framework\",\n    author = \"Campagnano, Cesare  and\n      Conia, Simone  and\n      Navigli, Roberto\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.314/\",\n    doi = \"10.18653/v1/2022.acl-long.314\",\n    pages = \"4586--4601\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.314.pdf",
        "site": "https://aclanthology.org/2022.acl-long.314/",
        "pdf_size": 693207,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5952323673642510901&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science; Department of Computer Science; Department of Computer, Control and Management Engineering",
        "aff_domain": "di.uniroma1.it;di.uniroma1.it;diag.uniroma1.it",
        "email": "di.uniroma1.it;di.uniroma1.it;diag.uniroma1.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Unknown Institution;Politecnico di Torino",
        "aff_unique_dep": "Department of Computer Science;Department of Computer, Control and Management Engineering",
        "aff_unique_url": ";https://www.polito.it",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Italy"
    },
    {
        "id": "2022.acl-long.486",
        "title": "STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "How to learn a better speech representation for end-to-end speech-to-text translation (ST) with limited labeled data? Existing techniques often attempt to transfer powerful machine translation (MT) capabilities to ST, but neglect the representation discrepancy across modalities. In this paper, we propose the Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy. Specifically, we mix up the representation sequences of different modalities, and take both unimodal speech sequences and multimodal mixed sequences as input to the translation model in parallel, and regularize their output predictions with a self-learning framework. Experiments on MuST-C speech translation benchmark and further analysis show that our method effectively alleviates the cross-modal representation discrepancy, and achieves significant improvements over a strong baseline on eight translation directions.",
        "author": "Qingkai Fang; Rong Ye; Lei Li; Yang Feng; Mingxuan Wang",
        "authorids": "/q/qingkai-fang/; /r/rong-ye/; /l/lei-li/; /y/yang-feng/; /m/mingxuan-wang/",
        "bibtex": "@inproceedings{fang-etal-2022-stemm,\n    title = \"{STEMM}: Self-learning with Speech-text Manifold Mixup for Speech Translation\",\n    author = \"Fang, Qingkai  and\n      Ye, Rong  and\n      Li, Lei  and\n      Feng, Yang  and\n      Wang, Mingxuan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.486/\",\n    doi = \"10.18653/v1/2022.acl-long.486\",\n    pages = \"7050--7062\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.486.pdf",
        "site": "https://aclanthology.org/2022.acl-long.486/",
        "pdf_size": 681359,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3249385593384746024&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China; ByteDance AI Lab; University of California, Santa Barbara; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences, Beijing, China; ByteDance AI Lab",
        "aff_domain": "ict.ac.cn;bytedance.com;cs.ucsb.edu;ict.ac.cn;bytedance.com",
        "email": "ict.ac.cn;bytedance.com;cs.ucsb.edu;ict.ac.cn;bytedance.com",
        "github": "https://github.com/ictnlp/STEMM",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;3;0+1;2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;ByteDance;University of California, Santa Barbara",
        "aff_unique_dep": "Institute of Computing Technology;;AI Lab;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.bytedance.com;https://www.ucsb.edu",
        "aff_unique_abbr": "CAS;UCAS;ByteDance;UCSB",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Beijing;Santa Barbara",
        "aff_country_unique_index": "0+0;0;1;0+0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.580",
        "title": "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models. SUPERB was a step towards introducing a common benchmark to evaluate pre-trained models across various speech tasks. In this paper, we introduce SUPERB-SG, a new benchmark focusing on evaluating the semantic and generative capabilities of pre-trained models by increasing task diversity and difficulty over SUPERB. We use a lightweight methodology to test the robustness of representations learned by pre-trained models under shifts in data domain and quality across different types of tasks. It entails freezing pre-trained model parameters, only using simple task-specific trainable heads. The goal is to be inclusive of all researchers, and encourage efficient use of computational resources. We also show that the task diversity of SUPERB-SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation.",
        "author": "Hsiang-Sheng Tsai; Heng-Jui Chang; Wen-Chin Huang; Zili Huang; Kushal Lakhotia; Shu-wen Yang; Shuyan Dong; Andy Liu; Cheng-I Lai; Jiatong Shi; Xuankai Chang; Phil Hall; Hsuan-Jui Chen; Shang-Wen Li; Shinji Watanabe; Abdelrahman Mohamed; Hung-yi Lee",
        "authorids": "/h/hsiang-sheng-tsai/; /h/heng-jui-chang/; /w/wen-chin-huang/; /z/zili-huang/; /k/kushal-lakhotia/; /s/shu-wen-yang/; /s/shuyan-dong/; /a/andy-liu/; /c/cheng-i-lai/; /j/jiatong-shi/; /x/xuankai-chang/; /p/phil-hall/; /h/hsuan-jui-chen/; /s/shang-wen-li/; /s/shinji-watanabe/; /a/abdelrahman-mohamed/; /h/hung-yi-lee/",
        "bibtex": "@inproceedings{tsai-etal-2022-superb,\n    title = \"{SUPERB}-{SG}: Enhanced Speech processing Universal {PER}formance Benchmark for Semantic and Generative Capabilities\",\n    author = \"Tsai, Hsiang-Sheng  and\n      Chang, Heng-Jui  and\n      Huang, Wen-Chin  and\n      Huang, Zili  and\n      Lakhotia, Kushal  and\n      Yang, Shu-wen  and\n      Dong, Shuyan  and\n      Liu, Andy  and\n      Lai, Cheng-I  and\n      Shi, Jiatong  and\n      Chang, Xuankai  and\n      Hall, Phil  and\n      Chen, Hsuan-Jui  and\n      Li, Shang-Wen  and\n      Watanabe, Shinji  and\n      Mohamed, Abdelrahman  and\n      Lee, Hung-yi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.580/\",\n    doi = \"10.18653/v1/2022.acl-long.580\",\n    pages = \"8479--8492\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.580.pdf",
        "site": "https://aclanthology.org/2022.acl-long.580/",
        "pdf_size": 304321,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10826307053486010524&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 11,
        "aff": "National Taiwan University; National Taiwan University; Nagoya University; Johns Hopkins University; Meta AI; National Taiwan University; Meta AI; National Taiwan University; Massachusetts Institute of Technology; Carnegie Mellon University; Carnegie Mellon University; LXT; National Taiwan University; Meta AI; Carnegie Mellon University; Meta AI; National Taiwan University",
        "aff_domain": "ntu.edu.tw;ntu.edu.tw;g.sp.m.is.nagoya-u.ac.jp;jhu.edu;fb.com;fb.com;ieee.org;fb.com;ntu.edu.tw; ; ; ; ; ; ; ;",
        "email": "ntu.edu.tw;ntu.edu.tw;g.sp.m.is.nagoya-u.ac.jp;jhu.edu;fb.com;fb.com;ieee.org;fb.com;ntu.edu.tw; ; ; ; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 17,
        "aff_unique_index": "0;0;1;2;3;0;3;0;4;5;5;6;0;3;5;3;0",
        "aff_unique_norm": "National Taiwan University;Nagoya University;Johns Hopkins University;Meta;Massachusetts Institute of Technology;Carnegie Mellon University;LXT",
        "aff_unique_dep": ";;;Meta AI;;;",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.nagoya-u.ac.jp;https://www.jhu.edu;https://meta.com;https://web.mit.edu;https://www.cmu.edu;",
        "aff_unique_abbr": "NTU;Nagoya U;JHU;Meta;MIT;CMU;",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Taiwan;",
        "aff_country_unique_index": "0;0;1;2;2;0;2;0;2;2;2;0;2;2;2;0",
        "aff_country_unique": "China;Japan;United States;"
    },
    {
        "id": "2022.acl-long.447",
        "title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current open-domain conversational models can easily be made to talk in inadequate ways. Online learning from conversational feedback given by the conversation partner is a promising avenue for a model to improve and adapt, so as to generate fewer of these safety failures. However, current state-of-the-art models tend to react to feedback with defensive or oblivious responses. This makes for an unpleasant experience and may discourage conversation partners from giving feedback in the future. This work proposes SaFeRDialogues, a task and dataset of graceful responses to conversational feedback about safety failures. We collect a dataset of 8k dialogues demonstrating safety failures, feedback signaling them, and a response acknowledging the feedback. We show how fine-tuning on this dataset results in conversations that human raters deem considerably more likely to lead to a civil conversation, without sacrificing engagingness or general conversational ability.",
        "author": "Megan Ung; Jing Xu; Y-Lan Boureau",
        "authorids": "/m/megan-ung/; /j/jing-xu/; /y/y-lan-boureau/",
        "bibtex": "@inproceedings{ung-etal-2022-saferdialogues,\n    title = \"{S}a{F}e{RD}ialogues: Taking Feedback Gracefully after Conversational Safety Failures\",\n    author = \"Ung, Megan  and\n      Xu, Jing  and\n      Boureau, Y-Lan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.447/\",\n    doi = \"10.18653/v1/2022.acl-long.447\",\n    pages = \"6462--6481\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.447.pdf",
        "site": "https://aclanthology.org/2022.acl-long.447/",
        "pdf_size": 2040264,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18124539658688150048&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Facebook AI Research; Facebook AI Research; Facebook AI Research",
        "aff_domain": "fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Facebook AI Research",
        "aff_unique_url": "https://research.facebook.com",
        "aff_unique_abbr": "FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.284",
        "title": "SafetyKit: First Aid for Measuring Safety in Open-domain Conversational Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The social impact of natural language processing and its applications has received increasing attention. In this position paper, we focus on the problem of safety for end-to-end conversational AI. We survey the problem landscape therein, introducing a taxonomy of three observed phenomena: the Instigator, Yea-Sayer, and Impostor effects. We then empirically assess the extent to which current tools can measure these effects and current systems display them. We release these tools as part of a \u201cfirst aid kit\u201d (SafetyKit) to quickly assess apparent safety concerns. Our results show that, while current tools are able to provide an estimate of the relative safety of systems in various settings, they still have several shortcomings. We suggest several future directions and discuss ethical considerations.",
        "author": "Emily Dinan; Gavin Abercrombie; A. Bergman; Shannon Spruit; Dirk Hovy; Y-Lan Boureau; Verena Rieser",
        "authorids": "/e/emily-dinan/; /g/gavin-abercrombie/; /a/a-bergman/; /s/shannon-l-spruit/; /d/dirk-hovy/; /y/y-lan-boureau/; /v/verena-rieser/",
        "bibtex": "@inproceedings{dinan-etal-2022-safetykit,\n    title = \"{S}afety{K}it: First Aid for Measuring Safety in Open-domain Conversational Systems\",\n    author = \"Dinan, Emily  and\n      Abercrombie, Gavin  and\n      Bergman, A.  and\n      Spruit, Shannon  and\n      Hovy, Dirk  and\n      Boureau, Y-Lan  and\n      Rieser, Verena\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.284/\",\n    doi = \"10.18653/v1/2022.acl-long.284\",\n    pages = \"4113--4133\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.284.pdf",
        "site": "https://aclanthology.org/2022.acl-long.284/",
        "pdf_size": 564599,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16861917725957457397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Facebook AI Research; Heriot-Watt University; Responsible AI, Facebook; Independent Ethics Advisor at Populytics, Netherlands; Bocconi University; Facebook AI Research; Heriot-Watt University + Alana AI",
        "aff_domain": "fb.com;hw.ac.uk;fb.com;populytics.nl;unibocconi.it;fb.com;hw.ac.uk",
        "email": "fb.com;hw.ac.uk;fb.com;populytics.nl;unibocconi.it;fb.com;hw.ac.uk",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;2;3;0;1+4",
        "aff_unique_norm": "Meta;Heriot-Watt University;Populytics;Bocconi University;Alana AI",
        "aff_unique_dep": "Facebook AI Research;;Independent Ethics Advisor;;",
        "aff_unique_url": "https://research.facebook.com;https://www.hw.ac.uk;;https://www.bocconi.edu;https://www.alana.ai",
        "aff_unique_abbr": "FAIR;HWU;;Bocconi;Alana AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;3;0;1+0",
        "aff_country_unique": "United States;United Kingdom;Netherlands;Italy"
    },
    {
        "id": "2022.acl-long.425",
        "title": "SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Dialogue systems are usually categorized into two types, open-domain and task-oriented. The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue. The other one focuses on a specific task instead of casual talks, e.g., finding a movie on Friday night, playing a song. These two directions have been studied separately due to their different purposes. However, how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities, and there is no any public data focusing on such scenarios. Hence, this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to task-oriented purposes, and releases a large-scale dataset with detailed annotations for encouraging this research direction. To achieve this goal, this paper proposes a framework to automatically generate many dialogues without human involvement, in which any powerful open-domain dialogue generation model can be easily leveraged. The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality, showing that our released data has a great potential of guiding future research directions and commercial activities. Furthermore, the released models allow researchers to automatically generate unlimited dialogues in the target scenarios, which can greatly benefit semi-supervised and unsupervised approaches.",
        "author": "Ssu Chiu; Maolin Li; Yen-Ting Lin; Yun-Nung Chen",
        "authorids": "/s/ssu-chiu/; /m/maolin-li/; /y/yen-ting-lin/; /y/yun-nung-chen/",
        "bibtex": "@inproceedings{chiu-etal-2022-salesbot,\n    title = \"{S}ales{B}ot: Transitioning from Chit-Chat to Task-Oriented Dialogues\",\n    author = \"Chiu, Ssu  and\n      Li, Maolin  and\n      Lin, Yen-Ting  and\n      Chen, Yun-Nung\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.425/\",\n    doi = \"10.18653/v1/2022.acl-long.425\",\n    pages = \"6143--6158\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.425.pdf",
        "site": "https://aclanthology.org/2022.acl-long.425/",
        "pdf_size": 375741,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15106529394048625562&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "National Taiwan University, Taipei, Taiwan; MediaTek Research, Cambridge, UK; National Taiwan University, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan",
        "aff_domain": "csie.ntu.edu.tw;mtkresearch.com;csie.ntu.edu.tw;ieee.org",
        "email": "csie.ntu.edu.tw;mtkresearch.com;csie.ntu.edu.tw;ieee.org",
        "github": "https://github.com/MiuLab/SalesBot",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "National Taiwan University;MediaTek Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.mediatek.com",
        "aff_unique_abbr": "NTU;",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Taiwan;Cambridge",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2022.acl-long.313",
        "title": "Saliency as Evidence: Event Detection with Trigger Saliency Attribution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Event detection (ED) is a critical subtask of event extraction that seeks to identify event triggers of certain types in texts. Despite significant advances in ED, existing methods typically follow a \u201cone model fits all types\u201d approach, which sees no differences between event types and often results in a quite skewed performance. Finding the causes of skewed performance is crucial for the robustness of an ED model, but to date there has been little exploration of this problem. This research examines the issue in depth and presents a new concept termed trigger salience attribution, which can explicitly quantify the underlying patterns of events. On this foundation, we develop a new training mechanism for ED, which can distinguish between trigger-dependent and context-dependent types and achieve promising performance on two benchmarks. Finally, by highlighting many distinct characteristics of trigger-dependent and context-dependent types, our work may promote more research into this problem.",
        "author": "Jian Liu; Yufeng Chen; Jinan Xu",
        "authorids": "/j/jian-liu/; /y/yufeng-chen/; /j/jinan-xu/",
        "bibtex": "@inproceedings{liu-etal-2022-saliency,\n    title = \"Saliency as Evidence: Event Detection with Trigger Saliency Attribution\",\n    author = \"Liu, Jian  and\n      Chen, Yufeng  and\n      Xu, Jinan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.313/\",\n    doi = \"10.18653/v1/2022.acl-long.313\",\n    pages = \"4573--4585\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.313.pdf",
        "site": "https://aclanthology.org/2022.acl-long.313/",
        "pdf_size": 3756426,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16313925110789613621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Beijing Jiaotong University, School of Computer and Information Technology, China; Beijing Jiaotong University, School of Computer and Information Technology, China; Beijing Jiaotong University, School of Computer and Information Technology, China",
        "aff_domain": "bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn",
        "email": "bjtu.edu.cn;bjtu.edu.cn;bjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Beijing Jiao Tong University",
        "aff_unique_dep": "School of Computer and Information Technology",
        "aff_unique_url": "http://www.bjtu.edu.cn",
        "aff_unique_abbr": "BJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.27",
        "title": "Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language. Such data are notoriously scarce, making synthetic data augmentation by back-translation or knowledge distillation a necessary ingredient of end-to-end training. In this paper, we present a novel approach to data augmentation that leverages audio alignments, linguistic properties, and translation. First, we augment a transcription by sampling from a suffix memory that stores text and audio data. Second, we translate the augmented transcript. Finally, we recombine concatenated audio segments and the generated translation. Our method delivers consistent improvements of up to 0.9 and 1.1 BLEU points on top of augmentation with knowledge distillation on five language pairs on CoVoST 2 and on two language pairs on Europarl-ST, respectively.",
        "author": "Tsz Kin Lam; Shigehiko Schamoni; Stefan Riezler",
        "authorids": "/t/tsz-kin-lam/; /s/shigehiko-schamoni/; /s/stefan-riezler/",
        "bibtex": "@inproceedings{lam-etal-2022-sample,\n    title = \"Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation\",\n    author = \"Lam, Tsz Kin  and\n      Schamoni, Shigehiko  and\n      Riezler, Stefan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.27/\",\n    doi = \"10.18653/v1/2022.acl-short.27\",\n    pages = \"245--254\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.27.pdf",
        "site": "https://aclanthology.org/2022.acl-short.27/",
        "pdf_size": 289375,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14411638791293626137&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computational Linguistics, Heidelberg University + Interdisciplinary Center for Scientific Computing (IWR), Heidelberg University; Department of Computational Linguistics, Heidelberg University + Interdisciplinary Center for Scientific Computing (IWR), Heidelberg University; Department of Computational Linguistics, Heidelberg University + Interdisciplinary Center for Scientific Computing (IWR), Heidelberg University",
        "aff_domain": "cl.uni-heidelberg.de;cl.uni-heidelberg.de;cl.uni-heidelberg.de",
        "email": "cl.uni-heidelberg.de;cl.uni-heidelberg.de;cl.uni-heidelberg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Heidelberg University",
        "aff_unique_dep": "Department of Computational Linguistics",
        "aff_unique_url": "https://www.uni-heidelberg.de",
        "aff_unique_abbr": "Uni Heidelberg",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Heidelberg",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.300",
        "title": "Scheduled Multi-task Learning for Neural Chat Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural Chat Translation (NCT) aims to translate conversational text into different languages. Existing methods mainly focus on modeling the bilingual dialogue characteristics (e.g., coherence) to improve chat translation via multi-task learning on small-scale chat translation data. Although the NCT models have achieved impressive success, it is still far from satisfactory due to insufficient chat translation data and simple joint training manners. To address the above issues, we propose a scheduled multi-task learning framework for NCT. Specifically, we devise a three-stage training framework to incorporate the large-scale in-domain chat translation data into training by adding a second pre-training stage between the original pre-training and fine-tuning stages. Further, we investigate where and how to schedule the dialogue-related auxiliary tasks in multiple training stages to effectively enhance the main chat translation task. Extensive experiments on four language directions (English-Chinese and English-German) verify the effectiveness and superiority of the proposed approach. Additionally, we will make the large-scale in-domain paired bilingual dialogue dataset publicly available for the research community.",
        "author": "Yunlong Liang; Fandong Meng; Jinan Xu; Yufeng Chen; Jie Zhou",
        "authorids": "/y/yunlong-liang/; /f/fandong-meng/; /j/jinan-xu/; /y/yufeng-chen/; /j/jie-zhou/",
        "bibtex": "@inproceedings{liang-etal-2022-scheduled,\n    title = \"Scheduled Multi-task Learning for Neural Chat Translation\",\n    author = \"Liang, Yunlong  and\n      Meng, Fandong  and\n      Xu, Jinan  and\n      Chen, Yufeng  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.300/\",\n    doi = \"10.18653/v1/2022.acl-long.300\",\n    pages = \"4375--4388\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.300.pdf",
        "site": "https://aclanthology.org/2022.acl-long.300/",
        "pdf_size": 2449615,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=618251279031646684&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 4,
        "aff": "Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China + Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "bjtu.edu.cn;tencent.com;bjtu.edu.cn;bjtu.edu.cn;tencent.com",
        "email": "bjtu.edu.cn;tencent.com;bjtu.edu.cn;bjtu.edu.cn;tencent.com",
        "github": "https://github.com/XL2248/SML",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;0;0;1",
        "aff_unique_norm": "Beijing Jiao Tong University;Tencent",
        "aff_unique_dep": "Beijing Key Lab of Traffic Data Analysis and Mining;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "http://www.bjtu.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "BJTU;Tencent",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.511",
        "title": "SciNLI: A Corpus for Natural Language Inference on Scientific Text",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing Natural Language Inference (NLI) datasets, while being instrumental in the advancement of Natural Language Understanding (NLU) research, are not related to scientific text. In this paper, we introduce SciNLI, a large dataset for NLI that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on NLP and computational linguistics. Given that the text used in scientific literature differs vastly from the text used in everyday language both in terms of vocabulary and sentence structure, our dataset is well suited to serve as a benchmark for the evaluation of scientific NLU models. Our experiments show that SciNLI is harder to classify than the existing NLI datasets. Our best performing model with XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23% showing that there is substantial room for improvement.",
        "author": "Mobashir Sadat; Cornelia Caragea",
        "authorids": "/m/mobashir-sadat/; /c/cornelia-caragea/",
        "bibtex": "@inproceedings{sadat-caragea-2022-scinli,\n    title = \"{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text\",\n    author = \"Sadat, Mobashir  and\n      Caragea, Cornelia\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.511/\",\n    doi = \"10.18653/v1/2022.acl-long.511\",\n    pages = \"7399--7409\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.511.pdf",
        "site": "https://aclanthology.org/2022.acl-long.511/",
        "pdf_size": 274336,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12217234694047195984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science, University of Illinois Chicago; Computer Science, University of Illinois Chicago",
        "aff_domain": "uic.edu;uic.edu",
        "email": "uic.edu;uic.edu",
        "github": "https://github.com/msadat3/SciNLI7399",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois Chicago",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.119",
        "title": "Searching for fingerspelled content in American Sign Language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing for sign language video\u2014including tasks like recognition, translation, and search\u2014is crucial for making artificial intelligence technologies accessible to deaf individuals, and is gaining research interest in recent years. In this paper, we address the problem of searching for fingerspelled keywords or key phrases in raw sign language videos. This is an important task since significant content in sign language is often conveyed via fingerspelling, and to our knowledge the task has not been studied before. We propose an end-to-end model for this task, FSS-Net, that jointly detects fingerspelling and matches it to a text sequence. Our experiments, done on a large public dataset of ASL fingerspelling in the wild, show the importance of fingerspelling detection as a component of a search and retrieval model. Our model significantly outperforms baseline methods adapted from prior work on related tasks.",
        "author": "Bowen Shi; Diane Brentari; Greg Shakhnarovich; Karen Livescu",
        "authorids": "/b/bowen-shi/; /d/diane-brentari/; /g/greg-shakhnarovich/; /k/karen-livescu/",
        "bibtex": "@inproceedings{shi-etal-2022-searching,\n    title = \"Searching for fingerspelled content in {A}merican {S}ign {L}anguage\",\n    author = \"Shi, Bowen  and\n      Brentari, Diane  and\n      Shakhnarovich, Greg  and\n      Livescu, Karen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.119/\",\n    doi = \"10.18653/v1/2022.acl-long.119\",\n    pages = \"1699--1712\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.119.pdf",
        "site": "https://aclanthology.org/2022.acl-long.119/",
        "pdf_size": 14097501,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1896379907310490255&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "TTI-Chicago; University of Chicago; TTI-Chicago; TTI-Chicago",
        "aff_domain": "ttic.edu;uchicago.edu;ttic.edu;ttic.edu",
        "email": "ttic.edu;uchicago.edu;ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.uchicago.edu",
        "aff_unique_abbr": "TTI;UChicago",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.195",
        "title": "Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Math Word Problem (MWP) solving needs to discover the quantitative relationships over natural language narratives. Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs. In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns. We first investigate how a neural network understands patterns only from semantics, and observe that, if the prototype equations are the same, most problems get closer representations and those representations apart from them or close to other prototypes tend to produce wrong solutions. Inspired by it, we propose a contrastive learning approach, where the neural network perceives the divergence of patterns. We collect contrastive examples by converting the prototype equation into a tree and seeking similar tree structures. The solving model is trained with an auxiliary objective on the collected examples, resulting in the representations of problems with similar prototypes being pulled closer. We conduct experiments on the Chinese dataset Math23k and the English dataset MathQA. Our method greatly improves the performance in monolingual and multilingual settings.",
        "author": "Zhongli Li; Wenxuan Zhang; Chao Yan; Qingyu Zhou; Chao Li; Hongzhi Liu; Yunbo Cao",
        "authorids": "/z/zhongli-li/; /w/wenxuan-zhang/; /c/chao-yan/; /q/qingyu-zhou/; /c/chao-li/; /h/hongzhi-liu/; /y/yunbo-cao/",
        "bibtex": "@inproceedings{li-etal-2022-seeking,\n    title = \"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems\",\n    author = \"Li, Zhongli  and\n      Zhang, Wenxuan  and\n      Yan, Chao  and\n      Zhou, Qingyu  and\n      Li, Chao  and\n      Liu, Hongzhi  and\n      Cao, Yunbo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.195/\",\n    doi = \"10.18653/v1/2022.findings-acl.195\",\n    pages = \"2486--2496\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.195.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.195/",
        "pdf_size": 5811037,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2842706355981378849&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Tencent Cloud Xiaowei+Peking University; Peking University+Tencent Cloud Xiaowei; Peking University; Tencent Cloud Xiaowei; Tencent Cloud Xiaowei; Peking University; Tencent Cloud Xiaowei",
        "aff_domain": "tencent.com;stu.pku.edu.cn;stu.pku.edu.cn;tencent.com;tencent.com;ss.pku.edu.cn;tencent.com",
        "email": "tencent.com;stu.pku.edu.cn;stu.pku.edu.cn;tencent.com;tencent.com;ss.pku.edu.cn;tencent.com",
        "github": "https://github.com/zwx980624/mwp-cl",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1+0;1;0;0;1;0",
        "aff_unique_norm": "Tencent;Peking University",
        "aff_unique_dep": "Tencent Cloud Xiaowei;",
        "aff_unique_url": "https://cloud.tencent.com;http://www.pku.edu.cn",
        "aff_unique_abbr": "Tencent;Peking U",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.241",
        "title": "Selecting Stickers in Open-Domain Dialogue through Multitask Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "With the increasing popularity of online chatting, stickers are becoming important in our online communication. Selecting appropriate stickers in open-domain dialogue requires a comprehensive understanding of both dialogues and stickers, as well as the relationship between the two types of modalities. To tackle these challenges, we propose a multitask learning method comprised of three auxiliary tasks to enhance the understanding of dialogue history, emotion and semantic meaning of stickers. Extensive experiments conducted on a recent challenging dataset show that our model can better combine the multimodal information and achieve significantly higher accuracy over strong baselines. Ablation study further verifies the effectiveness of each auxiliary task. Our code is available at https://github.com/nonstopfor/Sticker-Selection.",
        "author": "Zhexin Zhang; Yeshuang Zhu; Zhengcong Fei; Jinchao Zhang; Jie Zhou",
        "authorids": "/z/zhexin-zhang/; /y/yeshuang-zhu/; /z/zhengcong-fei/; /j/jinchao-zhang/; /j/jie-zhou/",
        "bibtex": "@inproceedings{zhang-etal-2022-selecting,\n    title = \"Selecting Stickers in Open-Domain Dialogue through Multitask Learning\",\n    author = \"Zhang, Zhexin  and\n      Zhu, Yeshuang  and\n      Fei, Zhengcong  and\n      Zhang, Jinchao  and\n      Zhou, Jie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.241/\",\n    doi = \"10.18653/v1/2022.findings-acl.241\",\n    pages = \"3053--3060\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.241.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.241/",
        "pdf_size": 1403264,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3483102863932039348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The CoAI group, DCST+Institute for Artificial Intelligence+State Key Lab of Intelligent Technology and Systems+Beijing National Research Center for Information Science and Technology+Tsinghua University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "mails.tsinghua.edu.cn;tencent.com;ict.ac.cn;tencent.com;tencent.com",
        "email": "mails.tsinghua.edu.cn;tencent.com;ict.ac.cn;tencent.com;tencent.com",
        "github": "https://github.com/nonstopfor/Sticker-Selection",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2+3+4;5;5;5;5",
        "aff_unique_norm": "DCST;Institute for Artificial Intelligence;State Key Lab of Intelligent Technology and Systems;Beijing National Research Center for Information Science and Technology;Tsinghua University;Tencent",
        "aff_unique_dep": "The CoAI group;Artificial Intelligence;;;;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": ";;;;https://www.tsinghua.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": ";;;;THU;Tencent",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "1+2+2+2;2;2;2;2",
        "aff_country_unique": ";United States;China"
    },
    {
        "id": "2022.acl-long.553",
        "title": "Self-supervised Semantic-driven Phoneme Discovery for Zero-resource Speech Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Phonemes are defined by their relationship to words: changing a phoneme changes the word. Learning a phoneme inventory with little supervision has been a longstanding challenge with important applications to under-resourced speech technology. In this paper, we bridge the gap between the linguistic and statistical definition of phonemes and propose a novel neural discrete representation learning model for self-supervised learning of phoneme inventory with raw speech and word labels. Under mild assumptions, we prove that the phoneme inventory learned by our approach converges to the true one with an exponentially low error rate. Moreover, in experiments on TIMIT and Mboshi benchmarks, our approach consistently learns a better phoneme-level representation and achieves a lower error rate in a zero-resource phoneme recognition task than previous state-of-the-art self-supervised representation learning algorithms.",
        "author": "Liming Wang; Siyuan Feng; Mark Hasegawa-Johnson; Chang Yoo",
        "authorids": "/l/liming-wang/; /s/siyuan-feng/; /m/mark-hasegawa-johnson/; /c/chang-yoo/",
        "bibtex": "@inproceedings{wang-etal-2022-self,\n    title = \"Self-supervised Semantic-driven Phoneme Discovery for Zero-resource Speech Recognition\",\n    author = \"Wang, Liming  and\n      Feng, Siyuan  and\n      Hasegawa-Johnson, Mark  and\n      Yoo, Chang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.553/\",\n    doi = \"10.18653/v1/2022.acl-long.553\",\n    pages = \"8027--8047\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.553.pdf",
        "site": "https://aclanthology.org/2022.acl-long.553/",
        "pdf_size": 4754265,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1018922979094726411&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Multimedia Computing Group, Delft University of Technology; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Artificial Intelligence and Machine Learning Lab, KAIST",
        "aff_domain": "illinois.edu;tudelft.nl;illinois.edu;kaist.ac.kr",
        "email": "illinois.edu;tudelft.nl;illinois.edu;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Delft University of Technology;KAIST",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Multimedia Computing Group;Artificial Intelligence and Machine Learning Lab",
        "aff_unique_url": "https://illinois.edu;https://www.tudelft.nl;https://www.kaist.edu",
        "aff_unique_abbr": "UIUC;TUDelft;KAIST",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Urbana-Champaign;Delft;",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "United States;Netherlands;South Korea"
    },
    {
        "id": "2022.acl-long.372",
        "title": "Semantic Composition with PSHRG for Derivation Tree Reconstruction from Graph-Based Meaning Representations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce a data-driven approach to generating derivation trees from meaning representation graphs with probabilistic synchronous hyperedge replacement grammar (PSHRG). SHRG has been used to produce meaning representation graphs from texts and syntax trees, but little is known about its viability on the reverse. In particular, we experiment on Dependency Minimal Recursion Semantics (DMRS) and adapt PSHRG as a formalism that approximates the semantic composition of DMRS graphs and simultaneously recovers the derivations that license the DMRS graphs. Consistent results are obtained as evaluated on a collection of annotated corpora. This work reveals the ability of PSHRG in formalizing a syntax\u2013semantics interface, modelling compositional graph-to-tree translations, and channelling explainability to surface realization.",
        "author": "Chun Hei Lo; Wai Lam; Hong Cheng",
        "authorids": "/c/chun-hei-lo/; /w/wai-lam/; /h/hong-cheng/",
        "bibtex": "@inproceedings{lo-etal-2022-semantic,\n    title = \"Semantic Composition with {PSHRG} for Derivation Tree Reconstruction from Graph-Based Meaning Representations\",\n    author = \"Lo, Chun Hei  and\n      Lam, Wai  and\n      Cheng, Hong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.372/\",\n    doi = \"10.18653/v1/2022.acl-long.372\",\n    pages = \"5425--5439\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.372.pdf",
        "site": "https://aclanthology.org/2022.acl-long.372/",
        "pdf_size": 726000,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13289555474350134395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong",
        "aff_domain": "se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk",
        "email": "se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Systems Engineering and Engineering Management",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.118",
        "title": "Semantically Distributed Robust Optimization for Vision-and-Language Inference",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms. While data augmentation techniques have been designed to mitigate against these failure modes, methods that can integrate this knowledge into the training pipeline remain under-explored. In this paper, we present SDRO, a model-agnostic method that utilizes a set linguistic transformations in a distributed robust optimization setting, along with an ensembling technique to leverage these transformations during inference.Experiments on benchmark datasets with images (NLVR2) and video (VIOLIN) demonstrate performance improvements as well as robustness to adversarial attacks.Experiments on binary VQA explore the generalizability of this method to other V&L tasks.",
        "author": "Tejas Gokhale; Abhishek Chaudhary; Pratyay Banerjee; Chitta Baral; Yezhou Yang",
        "authorids": "/t/tejas-gokhale/; /a/abhishek-chaudhary/; /p/pratyay-banerjee/; /c/chitta-baral/; /y/yezhou-yang/",
        "bibtex": "@inproceedings{gokhale-etal-2022-semantically,\n    title = \"Semantically Distributed Robust Optimization for Vision-and-Language Inference\",\n    author = \"Gokhale, Tejas  and\n      Chaudhary, Abhishek  and\n      Banerjee, Pratyay  and\n      Baral, Chitta  and\n      Yang, Yezhou\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.118/\",\n    doi = \"10.18653/v1/2022.findings-acl.118\",\n    pages = \"1493--1513\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.118.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.118/",
        "pdf_size": 3090524,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8040251357101141791&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Arizona State University; Arizona State University; Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;asu.edu;asu.edu",
        "github": "https://github.com/ASU-APG/VLI_SDRO",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.15",
        "title": "Sememe Prediction for BabelNet Synsets using Multilingual and Multimodal Information",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In linguistics, a sememe is defined as the minimum semantic unit of languages. Sememe knowledge bases (KBs), which are built by manually annotating words with sememes, have been successfully applied to various NLP tasks. However, existing sememe KBs only cover a few languages, which hinders the wide utilization of sememes. To address this issue, the task of sememe prediction for BabelNet synsets (SPBS) is presented, aiming to build a multilingual sememe KB based on BabelNet, a multilingual encyclopedia dictionary. By automatically predicting sememes for a BabelNet synset, the words in many languages in the synset would obtain sememe annotations simultaneously. However, previous SPBS methods have not taken full advantage of the abundant information in BabelNet. In this paper, we utilize the multilingual synonyms, multilingual glosses and images in BabelNet for SPBS. We design a multimodal information fusion model to encode and combine this information for sememe prediction. Experimental results show the substantial outperformance of our model over previous methods (about 10 MAP and F1 scores). All the code and data of this paper can be obtained at https://github.com/thunlp/MSGI.",
        "author": "Fanchao Qi; Chuancheng Lv; Zhiyuan Liu; Xiaojun Meng; Maosong Sun; Hai-Tao Zheng",
        "authorids": "/f/fanchao-qi/; /c/chuancheng-lv/; /z/zhiyuan-liu/; /x/xiaojun-meng/; /m/maosong-sun/; /h/hai-tao-zheng/",
        "bibtex": "@inproceedings{qi-etal-2022-sememe,\n    title = \"Sememe Prediction for {B}abel{N}et Synsets using Multilingual and Multimodal Information\",\n    author = \"Qi, Fanchao  and\n      Lv, Chuancheng  and\n      Liu, Zhiyuan  and\n      Meng, Xiaojun  and\n      Sun, Maosong  and\n      Zheng, Hai-Tao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.15/\",\n    doi = \"10.18653/v1/2022.findings-acl.15\",\n    pages = \"158--168\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.15.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.15/",
        "pdf_size": 1034882,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13286653896669038526&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology; Shenzhen International Graduate School, Tsinghua University, China+Peng Cheng Laboratory; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China; Huawei Noah\u2019s Ark Lab; Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China+Institute Guo Qiang, Tsinghua University, Beijing, China+International Innovation Center of Tsinghua University, Shanghai, China; Shenzhen International Graduate School, Tsinghua University, China+Peng Cheng Laboratory",
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ;tsinghua.edu.cn; ",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn; ; ;tsinghua.edu.cn; ",
        "github": "https://github.com/thunlp/MSGI",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+2;0+0+0;3;0+0+0;0+2",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Pengcheng Laboratory;Huawei",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.;;Peng Cheng Laboratory;Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;http://www.pcl.ac.cn;https://www.huawei.com",
        "aff_unique_abbr": "THU;;PCL;Huawei",
        "aff_campus_unique_index": "0;2;0+0+3;0+0+3;2",
        "aff_campus_unique": "Beijing;;Shenzhen;Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0+0;0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.321",
        "title": "Semi-Supervised Formality Style Transfer with Consistency Training",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Formality style transfer (FST) is a task that involves paraphrasing an informal sentence into a formal one without altering its meaning. To address the data-scarcity problem of existing parallel datasets, previous studies tend to adopt a cycle-reconstruction scheme to utilize additional unlabeled data, where the FST model mainly benefits from target-side unlabeled sentences. In this work, we propose a simple yet effective semi-supervised framework to better utilize source-side unlabeled sentences based on consistency training. Specifically, our approach augments pseudo-parallel data obtained from a source-side informal sentence by enforcing the model to generate similar outputs for its perturbed version. Moreover, we empirically examined the effects of various data perturbation methods and propose effective data filtering strategies to improve our framework. Experimental results on the GYAFC benchmark demonstrate that our approach can achieve state-of-the-art results, even with less than 40% of the parallel data.",
        "author": "Ao Liu; An Wang; Naoaki Okazaki",
        "authorids": "/a/ao-liu/; /a/an-wang/; /n/naoaki-okazaki/",
        "bibtex": "@inproceedings{liu-etal-2022-semi,\n    title = \"Semi-Supervised Formality Style Transfer with Consistency Training\",\n    author = \"Liu, Ao  and\n      Wang, An  and\n      Okazaki, Naoaki\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.321/\",\n    doi = \"10.18653/v1/2022.acl-long.321\",\n    pages = \"4689--4701\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.321.pdf",
        "site": "https://aclanthology.org/2022.acl-long.321/",
        "pdf_size": 413692,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11771586300163863484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "nlp.c.titech.ac.jp;de.cs.titech.ac.jp;c.titech.ac.jp",
        "email": "nlp.c.titech.ac.jp;de.cs.titech.ac.jp;c.titech.ac.jp",
        "github": "https://github.com/Aolius/semi-fst",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.acl-long.74",
        "title": "Semi-supervised Domain Adaptation for Dependency Parsing with Dynamic Matching Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Supervised parsing models have achieved impressive results on in-domain texts. However, their performances drop drastically on out-of-domain texts due to the data distribution shift. The shared-private model has shown its promising advantages for alleviating this problem via feature separation, whereas prior works pay more attention to enhance shared features but neglect the in-depth relevance of specific ones. To address this issue, we for the first time apply a dynamic matching network on the shared-private model for semi-supervised cross-domain dependency parsing. Meanwhile, considering the scarcity of target-domain labeled data, we leverage unlabeled data from two aspects, i.e., designing a new training strategy to improve the capability of the dynamic matching network and fine-tuning BERT to obtain domain-related contextualized representations. Experiments on benchmark datasets show that our proposed model consistently outperforms various baselines, leading to new state-of-the-art results on all domains. Detailed analysis on different matching strategies demonstrates that it is essential to learn suitable matching weights to emphasize useful features and ignore useless or even harmful ones. Besides, our proposed model can be directly extended to multi-source domain adaptation and achieves best performances among various baselines, further verifying the effectiveness and robustness.",
        "author": "Ying Li; Shuaike Li; Min Zhang",
        "authorids": "/y/ying-li/; /s/shuaike-li/; /m/min-zhang/",
        "bibtex": "@inproceedings{li-etal-2022-semi,\n    title = \"Semi-supervised Domain Adaptation for Dependency Parsing with Dynamic Matching Network\",\n    author = \"Li, Ying  and\n      Li, Shuaike  and\n      Zhang, Min\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.74/\",\n    doi = \"10.18653/v1/2022.acl-long.74\",\n    pages = \"1035--1045\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.74.pdf",
        "site": "https://aclanthology.org/2022.acl-long.74/",
        "pdf_size": 456391,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14505185692575480103&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "foxmail.com;stu.suda.edu.cn;suda.edu.cn",
        "email": "foxmail.com;stu.suda.edu.cn;suda.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "Institute of Artificial Intelligence, School of Computer Science and Technology",
        "aff_unique_url": "http://www.soochow.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.135",
        "title": "Sense Embeddings are also Biased \u2013 Evaluating Social Biases in Static and Contextualised Sense Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sense embedding learning methods learn different embeddings for the different senses of an ambiguous word. One sense of an ambiguous word might be socially biased while its other senses remain unbiased. In comparison to the numerous prior work evaluating the social biases in pretrained word embeddings, the biases in sense embeddings have been relatively understudied. We create a benchmark dataset for evaluating the social biases in sense embeddings and propose novel sense-specific bias evaluation measures. We conduct an extensive evaluation of multiple static and contextualised sense embeddings for various types of social biases using the proposed measures. Our experimental results show that even in cases where no biases are found at word-level, there still exist worrying levels of social biases at sense-level, which are often ignored by the word-level bias evaluation measures.",
        "author": "Yi Zhou; Masahiro Kaneko; Danushka Bollegala",
        "authorids": "/y/yi-zhou/; /m/masahiro-kaneko/; /d/danushka-bollegala/",
        "bibtex": "@inproceedings{zhou-etal-2022-sense,\n    title = \"Sense Embeddings are also Biased {--} Evaluating Social Biases in Static and Contextualised Sense Embeddings\",\n    author = \"Zhou, Yi  and\n      Kaneko, Masahiro  and\n      Bollegala, Danushka\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.135/\",\n    doi = \"10.18653/v1/2022.acl-long.135\",\n    pages = \"1924--1935\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.135.pdf",
        "site": "https://aclanthology.org/2022.acl-long.135/",
        "pdf_size": 926000,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13990668396795684107&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Liverpool1; Tokyo Institute of Technology2; University of Liverpool1+Amazon3",
        "aff_domain": "liverpool.ac.uk;nlp.c.titech.ac.jp;liverpool.ac.uk",
        "email": "liverpool.ac.uk;nlp.c.titech.ac.jp;liverpool.ac.uk",
        "github": "github.com/LivNLP/bias-sense",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "University of Liverpool;Tokyo Institute of Technology;Amazon",
        "aff_unique_dep": ";;Amazon",
        "aff_unique_url": "https://www.liverpool.ac.uk;https://www.titech.ac.jp;https://www.amazon.com",
        "aff_unique_abbr": "Liv Uni;Titech;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+2",
        "aff_country_unique": "United Kingdom;Japan;United States"
    },
    {
        "id": "2022.findings-acl.146",
        "title": "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder. We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark. Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up ST5 from millions to billions of parameters shown to consistently improve performance. Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings.",
        "author": "Jianmo Ni; Gustavo Hernandez Abrego; Noah Constant; Ji Ma; Keith Hall; Daniel Cer; Yinfei Yang",
        "authorids": "/j/jianmo-ni/; /g/gustavo-hernandez-abrego/; /n/noah-constant/; /j/ji-ma/; /k/keith-hall/; /d/daniel-cer/; /y/yinfei-yang/",
        "bibtex": "@inproceedings{ni-etal-2022-sentence,\n    title = \"Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models\",\n    author = \"Ni, Jianmo  and\n      Hernandez Abrego, Gustavo  and\n      Constant, Noah  and\n      Ma, Ji  and\n      Hall, Keith  and\n      Cer, Daniel  and\n      Yang, Yinfei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.146/\",\n    doi = \"10.18653/v1/2022.findings-acl.146\",\n    pages = \"1864--1874\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.146.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.146/",
        "pdf_size": 3019950,
        "gs_citation": 553,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17073143488692290439&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.76",
        "title": "Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Training dense passage representations via contrastive learning has been shown effective for Open-Domain Passage Retrieval (ODPR). Existing studies focus on further optimizing by improving negative sampling strategy or extra pretraining. However, these studies keep unknown in capturing passage with internal representation conflicts from improper modeling granularity. Specifically, under our observation that a passage can be organized by multiple semantically different sentences, modeling such a passage as a unified dense vector is not optimal. This work thus presents a refined model on the basis of a smaller granularity, contextual sentences, to alleviate the concerned conflicts. In detail, we introduce an in-passage negative sampling strategy to encourage a diverse generation of sentence representations within the same passage. Experiments on three benchmark datasets verify the efficacy of our method, especially on datasets where conflicts are severe. Extensive experiments further present good transferability of our method across datasets.",
        "author": "Bohong Wu; Zhuosheng Zhang; Jinyuan Wang; Hai Zhao",
        "authorids": "/b/bohong-wu/; /z/zhuosheng-zhang/; /j/jinyuan-wang/; /h/hai-zhao/",
        "bibtex": "@inproceedings{hong-etal-2022-sentence,\n    title = \"Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval\",\n    author = \"Wu, Bohong  and\n      Zhang, Zhuosheng  and\n      Wang, Jinyuan  and\n      Zhao, Hai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.76/\",\n    doi = \"10.18653/v1/2022.acl-long.76\",\n    pages = \"1062--1074\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.76.pdf",
        "site": "https://aclanthology.org/2022.acl-long.76/",
        "pdf_size": 608335,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1304639250454919765&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.238",
        "title": "Sentence-level Privacy for Document Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "User language data can contain highly sensitive personal content. As such, it is imperative to offer users a strong and interpretable privacy guarantee when learning from their data. In this work we propose SentDP, pure local differential privacy at the sentence level for a single user document. We propose a novel technique, DeepCandidate, that combines concepts from robust statistics and language modeling to produce high (768) dimensional, general \ud835\udf16-SentDP document embeddings. This guarantees that any single sentence in a document can be substituted with any other sentence while keeping the embedding \ud835\udf16-indistinguishable. Our experiments indicate that these private document embeddings are useful for downstream tasks like sentiment analysis and topic classification and even outperform baseline methods with weaker guarantees like word-level Metric DP.",
        "author": "Casey Meehan; Khalil Mrini; Kamalika Chaudhuri",
        "authorids": "/c/casey-meehan/; /k/khalil-mrini/; /k/kamalika-chaudhuri/",
        "bibtex": "@inproceedings{meehan-etal-2022-sentence,\n    title = \"Sentence-level Privacy for Document Embeddings\",\n    author = \"Meehan, Casey  and\n      Mrini, Khalil  and\n      Chaudhuri, Kamalika\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.238/\",\n    doi = \"10.18653/v1/2022.acl-long.238\",\n    pages = \"3367--3380\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.238.pdf",
        "site": "https://aclanthology.org/2022.acl-long.238/",
        "pdf_size": 977270,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6562103350711004770&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "UC San Diego; UC San Diego; UC San Diego",
        "aff_domain": "eng.ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "email": "eng.ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.109",
        "title": "Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed. However, the performance of the state-of-the-art models decreases sharply when they are deployed in the real world. We find that the main reason is that real-world applications can only access the text outputs by the automatic speech recognition (ASR) models, which may be with errors because of the limitation of model capacity. Through further analysis of the ASR outputs, we find that in some cases the sentiment words, the key sentiment elements in the textual modality, are recognized as other words, which makes the sentiment of the text change and hurts the performance of multimodal sentiment analysis models directly. To address this problem, we propose the sentiment word aware multimodal refinement model (SWRM), which can dynamically refine the erroneous sentiment words by leveraging multimodal sentiment clues. Specifically, we first use the sentiment word position detection module to obtain the most possible position of the sentiment word in the text and then utilize the multimodal sentiment word refinement module to dynamically refine the sentiment word embeddings. The refined embeddings are taken as the textual inputs of the multimodal feature fusion module to predict the sentiment labels. We conduct extensive experiments on the real-world datasets including MOSI-Speechbrain, MOSI-IBM, and MOSI-iFlytek and the results demonstrate the effectiveness of our model, which surpasses the current state-of-the-art models on three datasets. Furthermore, our approach can be adapted for other multimodal feature fusion models easily.",
        "author": "Yang Wu; Yanyan Zhao; Hao Yang; Song Chen; Bing Qin; Xiaohuan Cao; Wenting Zhao",
        "authorids": "/y/yang-wu/; /y/yanyan-zhao/; /h/hao-yang/; /s/song-chen/; /b/bing-qin/; /x/xiaohuan-cao/; /w/wenting-zhao/",
        "bibtex": "@inproceedings{wu-etal-2022-sentiment,\n    title = \"Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with {ASR} Errors\",\n    author = \"Wu, Yang  and\n      Zhao, Yanyan  and\n      Yang, Hao  and\n      Chen, Song  and\n      Qin, Bing  and\n      Cao, Xiaohuan  and\n      Zhao, Wenting\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.109/\",\n    doi = \"10.18653/v1/2022.findings-acl.109\",\n    pages = \"1397--1406\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.109.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.109/",
        "pdf_size": 6679390,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14047011755769636815&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; AI Lab of China Merchants Bank; AI Lab of China Merchants Bank",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;cmbchina.com;cmbchina.com",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;cmbchina.com;cmbchina.com",
        "github": "https://github.com/albertwy/SWRM",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;1",
        "aff_unique_norm": "Harbin Institute of Technology;China Merchants Bank",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.cmbchina.com.cn",
        "aff_unique_abbr": "HIT;CMB",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.174",
        "title": "Seq2Path: Generating Sentiment Tuples as Paths of a Tree",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) tasks aim to extract sentiment tuples from a sentence. Recent generative methods such as Seq2Seq models have achieved good performance by formulating the output as a sequence of sentiment tuples. However, the orders between the sentiment tuples do not naturally exist and the generation of the current tuple should not condition on the previous ones. In this paper, we propose Seq2Path to generate sentiment tuples as paths of a tree. A tree can represent \u201c1-to-n\u201d relations (e.g., an aspect term may correspond to multiple opinion terms) and the paths of a tree are independent and do not have orders. For training, we treat each path as an independent target, and we calculate the average loss of the ordinary Seq2Seq model over paths. For inference, we apply beam search with constrained decoding. By introducing an additional discriminative token and applying a data augmentation technique, valid paths can be automatically selected. We conduct experiments on five tasks including AOPE, ASTE, TASD, UABSA, ACOS. We evaluate our method on four common benchmark datasets including Laptop14, Rest14, Rest15, Rest16. Our proposed method achieves state-of-the-art results in almost all cases.",
        "author": "Yue Mao; Yi Shen; Jingchao Yang; Xiaoying Zhu; Longjun Cai",
        "authorids": "/y/yue-mao/; /y/yi-shen/; /j/jingchao-yang/; /x/xiaoying-zhu/; /l/longjun-cai/",
        "bibtex": "@inproceedings{mao-etal-2022-seq2path,\n    title = \"{S}eq2{P}ath: Generating Sentiment Tuples as Paths of a Tree\",\n    author = \"Mao, Yue  and\n      Shen, Yi  and\n      Yang, Jingchao  and\n      Zhu, Xiaoying  and\n      Cai, Longjun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.174/\",\n    doi = \"10.18653/v1/2022.findings-acl.174\",\n    pages = \"2215--2225\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.174.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.174/",
        "pdf_size": 560570,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13447495278237028401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.201",
        "title": "Sequence-to-Sequence Knowledge Graph Completion and Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors. These methods have recently been applied to KG link prediction and question answering over incomplete KGs (KGQA). KGEs typically create an embedding for each entity in the graph, which results in large model sizes on real-world graphs with millions of entities. For downstream tasks these atomic entity representations often need to be integrated into a multi stage pipeline, limiting their utility. We show that an off-the-shelf encoder-decoder Transformer model can serve as a scalable and versatile KGE model obtaining state-of-the-art results for KG link prediction and incomplete KG question answering. We achieve this by posing KG link prediction as a sequence-to-sequence task and exchange the triple scoring approach taken by prior KGE methods with autoregressive decoding. Such a simple but powerful method reduces the model size up to 98% compared to conventional KGE models while keeping inference time tractable. After finetuning this model on the task of KGQA over incomplete KGs, our approach outperforms baselines on multiple large-scale datasets without extensive hyperparameter tuning.",
        "author": "Apoorv Saxena; Adrian Kochsiek; Rainer Gemulla",
        "authorids": "/a/apoorv-saxena/; /a/adrian-kochsiek/; /r/rainer-gemulla/",
        "bibtex": "@inproceedings{saxena-etal-2022-sequence,\n    title = \"Sequence-to-Sequence Knowledge Graph Completion and Question Answering\",\n    author = \"Saxena, Apoorv  and\n      Kochsiek, Adrian  and\n      Gemulla, Rainer\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.201/\",\n    doi = \"10.18653/v1/2022.acl-long.201\",\n    pages = \"2814--2828\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.201.pdf",
        "site": "https://aclanthology.org/2022.acl-long.201/",
        "pdf_size": 685214,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9244723078170514522&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Indian Institute of Science, Bangalore; University of Mannheim, Germany; University of Mannheim, Germany",
        "aff_domain": "iisc.ac.in;informatik.uni-mannheim.de;uni-mannheim.de",
        "email": "iisc.ac.in;informatik.uni-mannheim.de;uni-mannheim.de",
        "github": "https://github.com/apoorvumang/kgt5",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Indian Institute of Science;University of Mannheim",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.uni-mannheim.de",
        "aff_unique_abbr": "IISc;UM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Bangalore;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "India;Germany"
    },
    {
        "id": "2022.acl-short.63",
        "title": "Sequence-to-sequence AMR Parsing with Ancestor Information",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "AMR parsing is the task that maps a sentence to an AMR semantic graph automatically. The difficulty comes from generating the complex graph structure. The previous state-of-the-art method translates the AMR graph into a sequence, then directly fine-tunes a pretrained sequence-to-sequence Transformer model (BART). However, purely treating the graph as a sequence does not take advantage of structural information about the graph. In this paper, we design several strategies to add the important ancestor information into the Transformer Decoder. Our experiments show that we can improve the performance for both AMR 2.0 and AMR 3.0 dataset and achieve new state-of-the-art results.",
        "author": "Chen Yu; Daniel Gildea",
        "authorids": "/c/chen-yu/; /d/daniel-gildea/",
        "bibtex": "@inproceedings{yu-gildea-2022-sequence,\n    title = \"Sequence-to-sequence {AMR} Parsing with Ancestor Information\",\n    author = \"Yu, Chen  and\n      Gildea, Daniel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.63/\",\n    doi = \"10.18653/v1/2022.acl-short.63\",\n    pages = \"571--577\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.63.pdf",
        "site": "https://aclanthology.org/2022.acl-short.63/",
        "pdf_size": 316646,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13983922110985974509&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Rochester; Department of Computer Science, University of Rochester",
        "aff_domain": "; ",
        "email": "; ",
        "github": "https://github.com/lukecyu/amr-parser-s2s-ancestor",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.508",
        "title": "Sharpness-Aware Minimization Improves Language Model Generalization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The allure of superhuman-level capabilities has led to considerable interest in language models like GPT-3 and T5, wherein the research has, by and large, revolved around new model architectures, training tasks, and loss objectives, along with substantial engineering efforts to scale up model capacity and dataset size. Comparatively little work has been done to improve the generalization of these models through better optimization. In this work, we show that Sharpness-Aware Minimization (SAM), a recently proposed optimization procedure that encourages convergence to flatter minima, can substantially improve the generalization of language models without much computational overhead. We show that SAM is able to boost performance on SuperGLUE, GLUE, Web Questions, Natural Questions, Trivia QA, and TyDiQA, with particularly large gains when training data for these tasks is limited.",
        "author": "Dara Bahri; Hossein Mobahi; Yi Tay",
        "authorids": "/d/dara-bahri/; /h/hossein-mobahi/; /y/yi-tay/",
        "bibtex": "@inproceedings{bahri-etal-2022-sharpness,\n    title = \"Sharpness-Aware Minimization Improves Language Model Generalization\",\n    author = \"Bahri, Dara  and\n      Mobahi, Hossein  and\n      Tay, Yi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.508/\",\n    doi = \"10.18653/v1/2022.acl-long.508\",\n    pages = \"7360--7371\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.508.pdf",
        "site": "https://aclanthology.org/2022.acl-long.508/",
        "pdf_size": 2161943,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13869143131235194992&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.325",
        "title": "Should We Trust This Summary? Bayesian Abstractive Summarization to The Rescue",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We explore the notion of uncertainty in the context of modern abstractive summarization models, using the tools of Bayesian Deep Learning. Our approach approximates Bayesian inference by first extending state-of-the-art summarization models with Monte Carlo dropout and then using them to perform multiple stochastic forward passes. Based on Bayesian inference we are able to effectively quantify uncertainty at prediction time. Having a reliable uncertainty measure, we can improve the experience of the end user by filtering out generated summaries of high uncertainty. Furthermore, uncertainty estimation could be used as a criterion for selecting samples for annotation, and can be paired nicely with active learning and human-in-the-loop approaches. Finally, Bayesian inference enables us to find a Bayesian summary which performs better than a deterministic one and is more robust to uncertainty. In practice, we show that our Variational Bayesian equivalents of BART and PEGASUS can outperform their deterministic counterparts on multiple benchmark datasets.",
        "author": "Alexios Gidiotis; Grigorios Tsoumakas",
        "authorids": "/a/alexios-gidiotis/; /g/grigorios-tsoumakas/",
        "bibtex": "@inproceedings{gidiotis-tsoumakas-2022-trust,\n    title = \"Should We Trust This Summary? {B}ayesian Abstractive Summarization to The Rescue\",\n    author = \"Gidiotis, Alexios  and\n      Tsoumakas, Grigorios\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.325/\",\n    doi = \"10.18653/v1/2022.findings-acl.325\",\n    pages = \"4119--4131\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.325.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.325/",
        "pdf_size": 688110,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13135738712521172364&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Aristotle University of Thessaloniki; Aristotle University of Thessaloniki",
        "aff_domain": "csd.auth.gr;csd.auth.gr",
        "email": "csd.auth.gr;csd.auth.gr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Aristotle University of Thessaloniki",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.auth.gr",
        "aff_unique_abbr": "AUTH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Thessaloniki",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Greece"
    },
    {
        "id": "2022.acl-long.530",
        "title": "Should a Chatbot be Sarcastic? Understanding User Preferences Towards Sarcasm Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous sarcasm generation research has focused on how to generate text that people perceive as sarcastic to create more human-like interactions. In this paper, we argue that we should first turn our attention to the question of when sarcasm should be generated, finding that humans consider sarcastic responses inappropriate to many input utterances. Next, we use a theory-driven framework for generating sarcastic responses, which allows us to control the linguistic devices included during generation. For each device, we investigate how much humans associate it with sarcasm, finding that pragmatic insincerity and emotional markers are devices crucial for making sarcasm recognisable.",
        "author": "Silviu Vlad Oprea; Steven Wilson; Walid Magdy",
        "authorids": "/s/silviu-vlad-oprea/; /s/steven-wilson/; /w/walid-magdy/",
        "bibtex": "@inproceedings{oprea-etal-2022-chatbot,\n    title = \"Should a Chatbot be Sarcastic? Understanding User Preferences Towards Sarcasm Generation\",\n    author = \"Oprea, Silviu Vlad  and\n      Wilson, Steven  and\n      Magdy, Walid\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.530/\",\n    doi = \"10.18653/v1/2022.acl-long.530\",\n    pages = \"7686--7700\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.530.pdf",
        "site": "https://aclanthology.org/2022.acl-long.530/",
        "pdf_size": 362896,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6450557176943410111&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Informatics, University of Edinburgh, Edinburgh, United Kingdom + The Alan Turing Institute, London, United Kingdom; School of Engineering and Computer Science, Oakland University, Rochester, MI, USA; School of Informatics, University of Edinburgh, Edinburgh, United Kingdom + The Alan Turing Institute, London, United Kingdom",
        "aff_domain": "ed.ac.uk;oakland.edu;inf.ed.ac.uk",
        "email": "ed.ac.uk;oakland.edu;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "University of Edinburgh;Alan Turing Institute;Oakland University",
        "aff_unique_dep": "School of Informatics;;School of Engineering and Computer Science",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.turing.ac.uk;https://www.oakland.edu",
        "aff_unique_abbr": "Edinburgh;ATI;OU",
        "aff_campus_unique_index": "0+1;2;0+1",
        "aff_campus_unique": "Edinburgh;London;Rochester",
        "aff_country_unique_index": "0+0;1;0+0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2022.acl-long.214",
        "title": "Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Procedures are inherently hierarchical. To \u201cmake videos\u201d, one may need to \u201cpurchase a camera\u201d, which in turn may require one to \u201cset a budget\u201d. While such hierarchical knowledge is critical for reasoning about complex procedures, most existing work has treated procedures as shallow structures without modeling the parent-child relation. In this work, we attempt to construct an open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a website containing more than 110k instructional articles, each documenting the steps to carry out a complex procedure. To this end, we develop a simple and efficient method that links steps (e.g., \u201cpurchase a camera\u201d) in an article to other articles with similar goals (e.g., \u201chow to choose a camera\u201d), recursively constructing the KB. Our method significantly outperforms several strong baselines according to automatic evaluation, human judgment, and application to downstream tasks such as instructional video retrieval.",
        "author": "Shuyan Zhou; Li Zhang; Yue Yang; Qing Lyu; Pengcheng Yin; Chris Callison-Burch; Graham Neubig",
        "authorids": "/s/shuyan-zhou/; /l/li-zhang-upenn/; /y/yue-yang/; /q/qing-lyu/; /p/pengcheng-yin/; /c/chris-callison-burch/; /g/graham-neubig/",
        "bibtex": "@inproceedings{zhou-etal-2022-show,\n    title = \"Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data\",\n    author = \"Zhou, Shuyan  and\n      Zhang, Li  and\n      Yang, Yue  and\n      Lyu, Qing  and\n      Yin, Pengcheng  and\n      Callison-Burch, Chris  and\n      Neubig, Graham\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.214/\",\n    doi = \"10.18653/v1/2022.acl-long.214\",\n    pages = \"2998--3012\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.214.pdf",
        "site": "https://aclanthology.org/2022.acl-long.214/",
        "pdf_size": 2005722,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6794040691961291334&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Carnegie Mellon University; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; Carnegie Mellon University; University of Pennsylvania; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;cs.cmu.edu;seas.upenn.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;cs.cmu.edu;seas.upenn.edu;cs.cmu.edu",
        "github": "https://github.com/shuyanzhou/wikihow_hierarchy",
        "project": "https://wikihow-hierarchy.github.io/",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.upenn.edu",
        "aff_unique_abbr": "CMU;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.140",
        "title": "Sibylvariant Transformations for Robust Text Classification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The vast majority of text transformation techniques in NLP are inherently limited in their ability to expand input space coverage due to an implicit constraint to preserve the original class label. In this work, we propose the notion of sibylvariance (SIB) to describe the broader set of transforms that relax the label-preserving constraint, knowably vary the expected class, and lead to significantly more diverse input distributions. We offer a unified framework to organize all data transformations, including two types of SIB: (1) Transmutations convert one discrete kind into another, (2) Mixture Mutations blend two or more classes together. To explore the role of sibylvariance within NLP, we implemented 41 text transformations, including several novel techniques like Concept2Sentence and SentMix. Sibylvariance also enables a unique form of adaptive training that generates new input mixtures for the most confused class pairs, challenging the learner to differentiate with greater nuance. Our experiments on six benchmark datasets strongly support the efficacy of sibylvariance for generalization performance, defect detection, and adversarial robustness.",
        "author": "Fabrice Harel-Canada; Muhammad Ali Gulzar; Nanyun Peng; Miryung Kim",
        "authorids": "/f/fabrice-harel-canada/; /m/muhammad-ali-gulzar/; /n/nanyun-peng/; /m/miryung-kim/",
        "bibtex": "@inproceedings{harel-canada-etal-2022-sibylvariant,\n    title = \"Sibylvariant Transformations for Robust Text Classification\",\n    author = \"Harel-Canada, Fabrice  and\n      Gulzar, Muhammad Ali  and\n      Peng, Nanyun  and\n      Kim, Miryung\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.140/\",\n    doi = \"10.18653/v1/2022.findings-acl.140\",\n    pages = \"1771--1788\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.140.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.140/",
        "pdf_size": 2370609,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3503656243746490751&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science Department, University of California, Los Angeles; Computer Science Department, Virginia Tech; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.vt.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.vt.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, Los Angeles;Virginia Tech",
        "aff_unique_dep": "Computer Science Department;Computer Science Department",
        "aff_unique_url": "https://www.ucla.edu;https://www.vt.edu",
        "aff_unique_abbr": "UCLA;VT",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Los Angeles;Blacksburg",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.492",
        "title": "Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing models learn word representations based on the distributional hypothesis, which asserts that word context (e.g., co-occurrence) correlates with meaning. We propose that n-grams composed of random character sequences, or garble, provide a novel context for studying word meaning both within and beyond extant language. In particular, randomly generated character n-grams lack meaning but contain primitive information based on the distribution of characters they contain. By studying the embeddings of a large corpus of garble, extant language, and pseudowords using CharacterBERT, we identify an axis in the model\u2019s high-dimensional embedding space that separates these classes of n-grams. Furthermore, we show that this axis relates to structure within extant language, including word part-of-speech, morphology, and concept concreteness. Thus, in contrast to studies that are mainly limited to extant language, our work reveals that meaning and primitive information are intrinsically linked.",
        "author": "Mark Chu; Bhargav Srinivasa Desikan; Ethan Nadler; Donald Ruggiero Lo Sardo; Elise Darragh-Ford; Douglas Guilbeault",
        "authorids": "/m/mark-chu/; /b/bhargav-srinivasa-desikan/; /e/ethan-nadler/; /d/donald-ruggiero-lo-sardo/; /e/elise-darragh-ford/; /d/douglas-guilbeault/",
        "bibtex": "@inproceedings{chu-etal-2022-signal,\n    title = \"Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models\",\n    author = \"Chu, Mark  and\n      Srinivasa Desikan, Bhargav  and\n      Nadler, Ethan  and\n      Lo Sardo, Donald Ruggiero  and\n      Darragh-Ford, Elise  and\n      Guilbeault, Douglas\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.492/\",\n    doi = \"10.18653/v1/2022.acl-long.492\",\n    pages = \"7120--7134\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.492.pdf",
        "site": "https://aclanthology.org/2022.acl-long.492/",
        "pdf_size": 9167834,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6438303375791159014&as_sdt=10005&sciodt=0,8&hl=en",
        "gs_version_total": 6,
        "aff": "Columbia University; \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne; Carnegie Observatories + University of Southern California; Sapienza University of Rome; Stanford University (KIPAC & Department of Physics); University of California, Berkeley (Haas Business School)",
        "aff_domain": "columbia.edu;epfl.ch;carnegiescience.edu;gmail.com;stanford.edu;berkeley.edu",
        "email": "columbia.edu;epfl.ch;carnegiescience.edu;gmail.com;stanford.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2+3;4;5;6",
        "aff_unique_norm": "Columbia University;EPFL;Carnegie Observatories;University of Southern California;Sapienza University of Rome;Stanford University;University of California, Berkeley",
        "aff_unique_dep": ";;;;;Department of Physics;Haas Business School",
        "aff_unique_url": "https://www.columbia.edu;https://www.epfl.ch;https://www.ociw.edu;https://www.usc.edu;https://www.uniroma1.it;https://www.stanford.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Columbia;EPFL;OCIW;USC;Sapienza;Stanford;UC Berkeley",
        "aff_campus_unique_index": "1;2;3;4",
        "aff_campus_unique": ";Los Angeles;Rome;Stanford;Berkeley",
        "aff_country_unique_index": "0;1;0+0;2;0;0",
        "aff_country_unique": "United States;Switzerland;Italy"
    },
    {
        "id": "2022.acl-long.295",
        "title": "SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19% on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the Wikidata5M inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available at https://github.com/intfloat/SimKGC .",
        "author": "Liang Wang; Wei Zhao; Zhuoyu Wei; Jingming Liu",
        "authorids": "/l/liang-wang/; /w/wei-zhao/; /z/zhuoyu-wei/; /j/jingming-liu/",
        "bibtex": "@inproceedings{wang-etal-2022-simkgc,\n    title = \"{S}im{KGC}: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models\",\n    author = \"Wang, Liang  and\n      Zhao, Wei  and\n      Wei, Zhuoyu  and\n      Liu, Jingming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.295/\",\n    doi = \"10.18653/v1/2022.acl-long.295\",\n    pages = \"4281--4294\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.295.pdf",
        "site": "https://aclanthology.org/2022.acl-long.295/",
        "pdf_size": 358039,
        "gs_citation": 256,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15212314932443788809&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Research Asia; Yuanfudao AI Lab, Beijing, China; Yuanfudao AI Lab, Beijing, China; Yuanfudao AI Lab, Beijing, China",
        "aff_domain": "microsoft.com;yuanfudao.com;yuanfudao.com;yuanfudao.com",
        "email": "microsoft.com;yuanfudao.com;yuanfudao.com;yuanfudao.com",
        "github": "https://github.com/intfloat/SimKGC",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Microsoft;Yuanfudao",
        "aff_unique_dep": "Research;AI Lab",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia;https://www.yuanfudao.com",
        "aff_unique_abbr": "MSR Asia;",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Asia;Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.25",
        "title": "Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "A key challenge in attribute value extraction (AVE) from e-commerce sites is how to handle a large number of attributes for diverse products. Although this challenge is partially addressed by a question answering (QA) approach which finds a value in product data for a given query (attribute), it does not work effectively for rare and ambiguous queries. We thus propose simple knowledge-driven query expansion based on possible answers (values) of a query (attribute) for QA-based AVE. We retrieve values of a query (attribute) from the training data to expand the query. We train a model with two tricks, knowledge dropout and knowledge token mixing, which mimic the imperfection of the value knowledge in testing. Experimental results on our cleaned version of AliExpress dataset show that our method improves the performance of AVE (+6.08 macro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro F1, respectively).",
        "author": "Keiji Shinzato; Naoki Yoshinaga; Yandi Xia; Wei-Te Chen",
        "authorids": "/k/keiji-shinzato/; /n/naoki-yoshinaga/; /y/yandi-xia/; /w/wei-te-chen/",
        "bibtex": "@inproceedings{shinzato-etal-2022-simple,\n    title = \"Simple and Effective Knowledge-Driven Query Expansion for {QA}-Based Product Attribute Extraction\",\n    author = \"Shinzato, Keiji  and\n      Yoshinaga, Naoki  and\n      Xia, Yandi  and\n      Chen, Wei-Te\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.25/\",\n    doi = \"10.18653/v1/2022.acl-short.25\",\n    pages = \"227--234\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.25.pdf",
        "site": "https://aclanthology.org/2022.acl-short.25/",
        "pdf_size": 361755,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3575489178095038035&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Rakuten Institute of Technology, Rakuten Group Inc.; Institute of Industrial Science, the University of Tokyo; Rakuten Institute of Technology, Rakuten Group Inc.; Rakuten Institute of Technology, Rakuten Group Inc.",
        "aff_domain": "rakuten.com;iis.u-tokyo.ac.jp;rakuten.com;rakuten.com",
        "email": "rakuten.com;iis.u-tokyo.ac.jp;rakuten.com;rakuten.com",
        "github": "https://github.com/lanmanok/ACL19_Scaling_Up_Open_Tagging227",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Rakuten Institute of Technology;University of Tokyo",
        "aff_unique_dep": ";Institute of Industrial Science",
        "aff_unique_url": "https://rit.rakuten.com;https://www.iis.u-tokyo.ac.jp",
        "aff_unique_abbr": "RIT;UTokyo",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.acl-long.355",
        "title": "Simulating Bandit Learning from User Feedback for Extractive Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation effort, but instead improving the system on-the-fly via user feedback.",
        "author": "Ge Gao; Eunsol Choi; Yoav Artzi",
        "authorids": "/g/ge-gao/; /e/eunsol-choi/; /y/yoav-artzi/",
        "bibtex": "@inproceedings{gao-etal-2022-simulating,\n    title = \"Simulating Bandit Learning from User Feedback for Extractive Question Answering\",\n    author = \"Gao, Ge  and\n      Choi, Eunsol  and\n      Artzi, Yoav\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.355/\",\n    doi = \"10.18653/v1/2022.acl-long.355\",\n    pages = \"5167--5179\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.355.pdf",
        "site": "https://aclanthology.org/2022.acl-long.355/",
        "pdf_size": 597165,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3856226606460016509&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Cornell Tech, Cornell University\u2666; Department of Computer Science, The University of Texas at Austin\u2663; Department of Computer Science and Cornell Tech, Cornell University\u2666",
        "aff_domain": "cs.cornell.edu;utexas.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;utexas.edu;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Cornell University;University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu;https://www.utexas.edu",
        "aff_unique_abbr": "Cornell;UT Austin",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cornell Tech;Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.199",
        "title": "Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Subword regularizations use multiple subword segmentations during training to improve the robustness of neural machine translation models. In previous subword regularizations, we use multiple segmentations in the training process but use only one segmentation in the inference. In this study, we propose an inference strategy to address this discrepancy. The proposed strategy approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations. Because the proposed strategy aggregates predictions from several segmentations, we can regard it as a single model ensemble that does not require any additional cost for training. Experimental results show that the proposed strategy improves the performance of models trained with subword regularization in low-resource machine translation tasks.",
        "author": "Sho Takase; Tatsuya Hiraoka; Naoaki Okazaki",
        "authorids": "/s/sho-takase/; /t/tatsuya-hiraoka/; /n/naoaki-okazaki/",
        "bibtex": "@inproceedings{takase-etal-2022-single,\n    title = \"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation\",\n    author = \"Takase, Sho  and\n      Hiraoka, Tatsuya  and\n      Okazaki, Naoaki\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.199/\",\n    doi = \"10.18653/v1/2022.findings-acl.199\",\n    pages = \"2536--2541\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.199.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.199/",
        "pdf_size": 265754,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13811535107140125322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;c.titech.ac.jp",
        "email": "nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;c.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.acl-long.557",
        "title": "Situated Dialogue Learning through Procedural Environment Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al. 2019)\u2014a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution\u2014an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests.",
        "author": "Prithviraj Ammanabrolu; Renee Jia; Mark Riedl",
        "authorids": "/p/prithviraj-ammanabrolu/; /r/renee-jia/; /m/mark-riedl/",
        "bibtex": "@inproceedings{ammanabrolu-etal-2022-situated,\n    title = \"Situated Dialogue Learning through Procedural Environment Generation\",\n    author = \"Ammanabrolu, Prithviraj  and\n      Jia, Renee  and\n      Riedl, Mark\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.557/\",\n    doi = \"10.18653/v1/2022.acl-long.557\",\n    pages = \"8099--8116\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.557.pdf",
        "site": "https://aclanthology.org/2022.acl-long.557/",
        "pdf_size": 2280095,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8178074087942964229&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Georgia Institute of Technology+Allen Institute for AI; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "allenai.org; ; ",
        "email": "allenai.org; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Georgia Institute of Technology;Allen Institute for AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://allenai.org",
        "aff_unique_abbr": "Georgia Tech;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.120",
        "title": "Skill Induction and Planning with Latent Language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a framework for learning hierarchical policies from demonstrations, using sparse natural language annotations to guide the discovery of reusable skills for autonomous decision-making. We formulate a generative model of action sequences in which goals generate sequences of high-level subtask descriptions, and these descriptions generate sequences of low-level actions. We describe how to train this model using primarily unannotated demonstrations by parsing demonstrations into sequences of named high-level sub-tasks, using only a small number of seed annotations to ground language in action. In trained models, natural language commands index a combinatorial library of skills; agents can use these skills to plan by generating high-level instruction sequences tailored to novel goals. We evaluate this approach in the ALFRED household simulation environment, providing natural language annotations for only 10% of demonstrations. It achieves performance comparable state-of-the-art models on ALFRED success rate, outperforming several recent methods with access to ground-truth plans during training and evaluation.",
        "author": "Pratyusha Sharma; Antonio Torralba; Jacob Andreas",
        "authorids": "/p/pratyusha-sharma/; /a/antonio-torralba/; /j/jacob-andreas/",
        "bibtex": "@inproceedings{sharma-etal-2022-skill,\n    title = \"Skill Induction and Planning with Latent Language\",\n    author = \"Sharma, Pratyusha  and\n      Torralba, Antonio  and\n      Andreas, Jacob\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.120/\",\n    doi = \"10.18653/v1/2022.acl-long.120\",\n    pages = \"1713--1726\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.120.pdf",
        "site": "https://aclanthology.org/2022.acl-long.120/",
        "pdf_size": 5226723,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11941975366872840883&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "https://sites.google.com/view/skill-induction-latent-lang/",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.503",
        "title": "SkipBERT: Efficient Inference with Shallow Layer Skipping",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we propose SkipBERT to accelerate BERT inference by skipping the computation of shallow layers. To achieve this, our approach encodes small text chunks into independent representations, which are then materialized to approximate the shallow representation of BERT. Since the use of such approximation is inexpensive compared with transformer calculations, we leverage it to replace the shallow layers of BERT to skip their runtime overhead. With off-the-shelf early exit mechanisms, we also skip redundant computation from the highest few layers to further improve inference efficiency. Results on GLUE show that our approach can reduce latency by 65% without sacrificing performance. By using only two-layer transformer calculations, we can still maintain 95% accuracy of BERT.",
        "author": "Jue Wang; Ke Chen; Gang Chen; Lidan Shou; Julian McAuley",
        "authorids": "/j/jue-wang/; /k/ke-chen/; /g/gang-chen/; /l/lidan-shou/; /j/julian-mcauley/",
        "bibtex": "@inproceedings{wang-etal-2022-skipbert,\n    title = \"{S}kip{BERT}: Efficient Inference with Shallow Layer Skipping\",\n    author = \"Wang, Jue  and\n      Chen, Ke  and\n      Chen, Gang  and\n      Shou, Lidan  and\n      McAuley, Julian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.503/\",\n    doi = \"10.18653/v1/2022.acl-long.503\",\n    pages = \"7287--7301\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.503.pdf",
        "site": "https://aclanthology.org/2022.acl-long.503/",
        "pdf_size": 1366249,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5138970953120713348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University + State Key Laboratory of CAD&CG, Zhejiang University; College of Computer Science and Technology, Zhejiang University + State Key Laboratory of CAD&CG, Zhejiang University; University of California, San Diego",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;ucsd.edu",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;ucsd.edu",
        "github": "https://github.com/LorrinWWW/SkipBERT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0+0;0+0;1",
        "aff_unique_norm": "Zhejiang University;University of California, San Diego",
        "aff_unique_dep": "College of Computer Science and Technology;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.ucsd.edu",
        "aff_unique_abbr": "ZJU;UCSD",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0+0;0+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.101",
        "title": "Slangvolution: A Causal Analysis of Semantic Change and Frequency Dynamics in Slang",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Languages are continuously undergoing changes, and the mechanisms that underlie these changes are still a matter of debate. In this work, we approach language evolution through the lens of causality in order to model not only how various distributional factors associate with language change, but how they causally affect it. In particular, we study slang, which is an informal language that is typically restricted to a specific group or social setting. We analyze the semantic change and frequency shift of slang words and compare them to those of standard, nonslang words. With causal discovery and causal inference techniques, we measure the effect that word type (slang/nonslang) has on both semantic change and frequency shift, as well as its relationship to frequency, polysemy and part of speech. Our analysis provides some new insights in the study of language change, e.g., we show that slang words undergo less semantic change but tend to have larger frequency shifts over time.",
        "author": "Daphna Keidar; Andreas Opedal; Zhijing Jin; Mrinmaya Sachan",
        "authorids": "/d/daphna-keidar/; /a/andreas-opedal/; /z/zhijing-jin/; /m/mrinmaya-sachan/",
        "bibtex": "@inproceedings{keidar-etal-2022-slangvolution,\n    title = \"Slangvolution: {A} Causal Analysis of Semantic Change and Frequency Dynamics in Slang\",\n    author = \"Keidar, Daphna  and\n      Opedal, Andreas  and\n      Jin, Zhijing  and\n      Sachan, Mrinmaya\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.101/\",\n    doi = \"10.18653/v1/2022.acl-long.101\",\n    pages = \"1422--1442\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.101.pdf",
        "site": "https://aclanthology.org/2022.acl-long.101/",
        "pdf_size": 2228809,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5973728618726163653&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "ETH Z\u00fcrich; ETH Z\u00fcrich + Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; ETH Z\u00fcrich",
        "aff_domain": "ethz.ch;inf.ethz.ch;tue.mpg.de;inf.ethz.ch",
        "email": "ethz.ch;inf.ethz.ch;tue.mpg.de;inf.ethz.ch",
        "github": "https://github.com/andreasopedal/slangvolution",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;1;0",
        "aff_unique_norm": "ETH Zurich;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "ETHZ;MPI-IS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "0;0+1;1;0",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "id": "2022.acl-long.32",
        "title": "So Different Yet So Alike! Constrained Unsupervised Text Style Transfer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automatic transfer of text between domains has become popular in recent times. One of its aims is to preserve the semantic content while adapting to the target domain. However, it does not explicitly maintain other attributes between the source and translated text: e.g., text length and descriptiveness. Maintaining constraints in transfer has several downstream applications, including data augmentation and debiasing. We introduce a method for such constrained unsupervised text style transfer by introducing two complementary losses to the generative adversarial network (GAN) family of models. Unlike the competing losses used in GANs, we introduce cooperative losses where the discriminator and the generator cooperate and reduce the same loss. The first is a contrastive loss and the second is a classification loss \u2014 aiming to regularize the latent space further and bring similar sentences closer together. We demonstrate that such training retains lexical, syntactic and domain-specific constraints between domains for multiple benchmark datasets, including ones where more than one attribute change. We show that the complementary cooperative losses improve text quality, according to both automated and human evaluation measures.",
        "author": "Abhinav Ramesh Kashyap; Devamanyu Hazarika; Min-Yen Kan; Roger Zimmermann; Soujanya Poria",
        "authorids": "/a/abhinav-ramesh-kashyap/; /d/devamanyu-hazarika/; /m/min-yen-kan/; /r/roger-zimmermann/; /s/soujanya-poria/",
        "bibtex": "@inproceedings{ramesh-kashyap-etal-2022-different,\n    title = \"So Different Yet So Alike! Constrained Unsupervised Text Style Transfer\",\n    author = \"Ramesh Kashyap, Abhinav  and\n      Hazarika, Devamanyu  and\n      Kan, Min-Yen  and\n      Zimmermann, Roger  and\n      Poria, Soujanya\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.32/\",\n    doi = \"10.18653/v1/2022.acl-long.32\",\n    pages = \"416--431\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.32.pdf",
        "site": "https://aclanthology.org/2022.acl-long.32/",
        "pdf_size": 1527642,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore; DeCLaRe Lab, Singapore University of Technology and Design, Singapore",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg;sutd.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg;sutd.edu.sg",
        "github": "https://github.com/abhinavkashyap/dct",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "National University of Singapore;Singapore University of Technology and Design",
        "aff_unique_dep": ";DeCLaRe Lab",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.sutd.edu.sg",
        "aff_unique_abbr": "NUS;SUTD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.acl-long.554",
        "title": "Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.",
        "author": "Haw-Shiuan Chang; Andrew McCallum",
        "authorids": "/h/haw-shiuan-chang/; /a/andrew-mccallum/",
        "bibtex": "@inproceedings{chang-mccallum-2022-softmax,\n    title = \"Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions\",\n    author = \"Chang, Haw-Shiuan  and\n      McCallum, Andrew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.554/\",\n    doi = \"10.18653/v1/2022.acl-long.554\",\n    pages = \"8048--8073\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.554.pdf",
        "site": "https://aclanthology.org/2022.acl-long.554/",
        "pdf_size": 670991,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1749959363561731127&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "CICS, University of Massachusetts, Amherst; CICS, University of Massachusetts, Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "Computer and Information Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.16",
        "title": "Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Conventional wisdom in pruning Transformer-based language models is that pruning reduces the model expressiveness and thus is more likely to underfit rather than overfit. However, under the trending pretrain-and-finetune paradigm, we postulate a counter-traditional hypothesis, that is: pruning increases the risk of overfitting when performed at the fine-tuning phase. In this paper, we aim to address the overfitting problem and improve pruning performance via progressive knowledge distillation with error-bound properties. We show for the first time that reducing the risk of overfitting can help the effectiveness of pruning under the pretrain-and-finetune paradigm. Ablation studies and experiments on the GLUE benchmark show that our method outperforms the leading competitors across different tasks.",
        "author": "Shaoyi Huang; Dongkuan Xu; Ian Yen; Yijue Wang; Sung-En Chang; Bingbing Li; Shiyang Chen; Mimi Xie; Sanguthevar Rajasekaran; Hang Liu; Caiwen Ding",
        "authorids": "/s/shaoyi-huang/; /d/dongkuan-xu/; /i/ian-yen/; /y/yijue-wang/; /s/sung-en-chang/; /b/bingbing-li/; /s/shiyang-chen/; /m/mimi-xie/; /s/sanguthevar-rajasekaran/; /h/hang-liu/; /c/caiwen-ding/",
        "bibtex": "@inproceedings{huang-etal-2022-sparse,\n    title = \"Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm\",\n    author = \"Huang, Shaoyi  and\n      Xu, Dongkuan  and\n      Yen, Ian  and\n      Wang, Yijue  and\n      Chang, Sung-En  and\n      Li, Bingbing  and\n      Chen, Shiyang  and\n      Xie, Mimi  and\n      Rajasekaran, Sanguthevar  and\n      Liu, Hang  and\n      Ding, Caiwen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.16/\",\n    doi = \"10.18653/v1/2022.acl-long.16\",\n    pages = \"190--200\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.16.pdf",
        "site": "https://aclanthology.org/2022.acl-long.16/",
        "pdf_size": 1002526,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8109552695725065390&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Connecticut; Penn State University; Moffett AI; University of Connecticut; Northeastern University; University of Connecticut; Stevens Institute of Technology; University of Texas at San Antonio; University of Connecticut; Stevens Institute of Technology; University of Connecticut",
        "aff_domain": "uconn.edu;psu.edu;moffett.ai;uconn.edu; ;stevens.edu; ;utsa.edu;uconn.edu; ;uconn.edu",
        "email": "uconn.edu;psu.edu;moffett.ai;uconn.edu; ;stevens.edu; ;utsa.edu;uconn.edu; ;uconn.edu",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;1;2;0;3;0;4;5;0;4;0",
        "aff_unique_norm": "University of Connecticut;Penn State University;Moffett AI;Northeastern University;Stevens Institute of Technology;University of Texas at San Antonio",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.uconn.edu;https://www.psu.edu;;https://www.northeastern.edu;https://www.stevens.edu;https://www.utsa.edu",
        "aff_unique_abbr": "UConn;PSU;;NEU;SIT;UTSA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Antonio",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.590",
        "title": "Sparsifying Transformer Models with Trainable Representation Pooling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on the task-specific parts of an input. A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-k operator.Our experiments on a challenging long document summarization task show that even our simple baseline performs comparably to the current SOTA, and with trainable pooling we can retain its top quality, while being 1.8\u00d7 faster during training, 4.5\u00d7 faster during inference, and up to 13\u00d7 more computationally efficient in the decoder.",
        "author": "Micha\u0142 Pietruszka; \u0141ukasz Borchmann; \u0141ukasz Garncarek",
        "authorids": "/m/michal-pietruszka/; /l/lukasz-borchmann/; /l/lukasz-garncarek/",
        "bibtex": "@inproceedings{pietruszka-etal-2022-sparsifying,\n    title = \"Sparsifying Transformer Models with Trainable Representation Pooling\",\n    author = \"Pietruszka, Micha{\\l}  and\n      Borchmann, {\\L}ukasz  and\n      Garncarek, {\\L}ukasz\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.590/\",\n    doi = \"10.18653/v1/2022.acl-long.590\",\n    pages = \"8616--8633\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.590.pdf",
        "site": "https://aclanthology.org/2022.acl-long.590/",
        "pdf_size": 597052,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13093542722814073189&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Applica.ai+Jagiellonian University; Applica.ai+Poznan University of Technology; Applica.ai",
        "aff_domain": "applica.ai;applica.ai;applica.ai",
        "email": "applica.ai;applica.ai;applica.ai",
        "github": "https://github.com/applicaai/pyramidions",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+2;0",
        "aff_unique_norm": "Applica;Jagiellonian University;Poznan University of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.applica.ai;https://www.uj.edu.pl;https://www.put.poznan.pl/",
        "aff_unique_abbr": "Applica;UJ;PUT",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0",
        "aff_country_unique": "United States;Poland"
    },
    {
        "id": "2022.acl-long.267",
        "title": "Speaker Information Can Guide Models to Better Inductive Biases: A Case Study On Predicting Code-Switching",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing (NLP) models trained on people-generated data can be unreliable because, without any constraints, they can learn from spurious correlations that are not relevant to the task. We hypothesize that enriching models with speaker information in a controlled, educated way can guide them to pick up on relevant inductive biases. For the speaker-driven task of predicting code-switching points in English\u2013Spanish bilingual dialogues, we show that adding sociolinguistically-grounded speaker features as prepended prompts significantly improves accuracy. We find that by adding influential phrases to the input, speaker-informed models learn useful and explainable linguistic information. To our knowledge, we are the first to incorporate speaker characteristics in a neural model for code-switching, and more generally, take a step towards developing transparent, personalized models that use speaker information in a controlled way.",
        "author": "Alissa Ostapenko; Shuly Wintner; Melinda Fricke; Yulia Tsvetkov",
        "authorids": "/a/alissa-ostapenko/; /s/shuly-wintner/; /m/melinda-fricke/; /y/yulia-tsvetkov/",
        "bibtex": "@inproceedings{ostapenko-etal-2022-speaker,\n    title = \"Speaker Information Can Guide Models to Better Inductive Biases: A Case Study On Predicting Code-Switching\",\n    author = \"Ostapenko, Alissa  and\n      Wintner, Shuly  and\n      Fricke, Melinda  and\n      Tsvetkov, Yulia\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.267/\",\n    doi = \"10.18653/v1/2022.acl-long.267\",\n    pages = \"3853--3867\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.267.pdf",
        "site": "https://aclanthology.org/2022.acl-long.267/",
        "pdf_size": 666367,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10066916912138754659&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Department of Computer Science, University of Haifa; Department of Linguistics, University of Pittsburgh; Paul G. Allen School of Computer Science & Engineering, University of Washington",
        "aff_domain": "andrew.cmu.edu;cs.haifa.ac.il;pitt.edu;cs.washington.edu",
        "email": "andrew.cmu.edu;cs.haifa.ac.il;pitt.edu;cs.washington.edu",
        "github": "https://github.com/ostapen/Switch-and-Explain",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Carnegie Mellon University;University of Haifa;University of Pittsburgh;University of Washington",
        "aff_unique_dep": "Language Technologies Institute;Department of Computer Science;Department of Linguistics;Paul G. Allen School of Computer Science & Engineering",
        "aff_unique_url": "https://www.cmu.edu;https://www.haifa.ac.il;https://www.pitt.edu;https://www.washington.edu",
        "aff_unique_abbr": "CMU;UoH;Pitt;UW",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Pittsburgh;;Seattle",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "2022.acl-long.393",
        "title": "SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.",
        "author": "Junyi Ao; Rui Wang; Long Zhou; Chengyi Wang; Shuo Ren; Yu Wu; Shujie Liu; Tom Ko; Qing Li; Yu Zhang; Zhihua Wei; Yao Qian; Jinyu Li; Furu Wei",
        "authorids": "/j/junyi-ao/; /r/rui-wang/; /l/long-zhou/; /c/chengyi-wang/; /s/shuo-ren/; /y/yu-wu/; /s/shujie-liu/; /t/tom-ko/; /q/qing-li/; /y/yu-zhang/; /z/zhihua-wei/; /y/yao-qian/; /j/jinyu-li/; /f/furu-wei/",
        "bibtex": "@inproceedings{ao-etal-2022-speecht5,\n    title = \"{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing\",\n    author = \"Ao, Junyi  and\n      Wang, Rui  and\n      Zhou, Long  and\n      Wang, Chengyi  and\n      Ren, Shuo  and\n      Wu, Yu  and\n      Liu, Shujie  and\n      Ko, Tom  and\n      Li, Qing  and\n      Zhang, Yu  and\n      Wei, Zhihua  and\n      Qian, Yao  and\n      Li, Jinyu  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.393/\",\n    doi = \"10.18653/v1/2022.acl-long.393\",\n    pages = \"5723--5738\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.393.pdf",
        "site": "https://aclanthology.org/2022.acl-long.393/",
        "pdf_size": 1609812,
        "gs_citation": 271,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5341736616533210721&as_sdt=5,48&sciodt=0,48&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Engineering, Southern University of Science and Technology+Microsoft; Department of Computing, The Hong Kong Polytechnic University; Department of Computer Science and Technology, Tongji University; Microsoft; Microsoft; Microsoft; Microsoft; Department of Computer Science and Engineering, Southern University of Science and Technology; Department of Computing, The Hong Kong Polytechnic University; Department of Computer Science and Engineering, Southern University of Science and Technology+Peng Cheng Laboratory; Department of Computer Science and Technology, Tongji University; Microsoft; Microsoft; Microsoft",
        "aff_domain": "sustech.edu.cn;tongji.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;sustech.edu.cn;comp.polyu.edu.hk;pcl.ac.cn;tongji.edu.cn;microsoft.com;microsoft.com;microsoft.com",
        "email": "sustech.edu.cn;tongji.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;sustech.edu.cn;comp.polyu.edu.hk;pcl.ac.cn;tongji.edu.cn;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/microsoft/SpeechT5",
        "project": "",
        "author_num": 14,
        "aff_unique_index": "0+1;2;3;1;1;1;1;0;2;0+4;3;1;1;1",
        "aff_unique_norm": "Southern University of Science and Technology;Microsoft;Hong Kong Polytechnic University;Tongji University;Pengcheng Laboratory",
        "aff_unique_dep": "Department of Computer Science and Engineering;Microsoft Corporation;Department of Computing;Department of Computer Science and Technology;Peng Cheng Laboratory",
        "aff_unique_url": "https://www.sustech.edu.cn;https://www.microsoft.com;https://www.polyu.edu.hk;https://www.tongji.edu.cn;http://www.pcl.ac.cn",
        "aff_unique_abbr": "SUSTech;Microsoft;PolyU;Tongji;PCL",
        "aff_campus_unique_index": ";1;1;",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+1;0;0;1;1;1;1;0;0;0+0;0;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.102",
        "title": "Spurious Correlations in Reference-Free Evaluation of Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Model-based, reference-free evaluation metricshave been proposed as a fast and cost-effectiveapproach to evaluate Natural Language Generation(NLG) systems. Despite promising recentresults, we find evidence that reference-freeevaluation metrics of summarization and dialoggeneration may be relying on spuriouscorrelations with measures such as word overlap,perplexity, and length. We further observethat for text summarization, these metrics havehigh error rates when ranking current state-ofthe-art abstractive summarization systems. Wedemonstrate that these errors can be mitigatedby explicitly designing evaluation metrics toavoid spurious features in reference-free evaluation.",
        "author": "Esin Durmus; Faisal Ladhak; Tatsunori Hashimoto",
        "authorids": "/e/esin-durmus/; /f/faisal-ladhak/; /t/tatsunori-b-hashimoto/",
        "bibtex": "@inproceedings{durmus-etal-2022-spurious,\n    title = \"Spurious Correlations in Reference-Free Evaluation of Text Generation\",\n    author = \"Durmus, Esin  and\n      Ladhak, Faisal  and\n      Hashimoto, Tatsunori\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.102/\",\n    doi = \"10.18653/v1/2022.acl-long.102\",\n    pages = \"1443--1454\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.102.pdf",
        "site": "https://aclanthology.org/2022.acl-long.102/",
        "pdf_size": 358853,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5591940997238644124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University; Columbia University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.columbia.edu;stanford.edu",
        "email": "cs.stanford.edu;cs.columbia.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.columbia.edu",
        "aff_unique_abbr": "Stanford;Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.184",
        "title": "Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The prototypical NLP experiment trains a standard architecture on labeled English data and optimizes for accuracy, without accounting for other dimensions such as fairness, interpretability, or computational efficiency. We show through a manual classification of recent NLP research papers that this is indeed the case and refer to it as the square one experimental setup. We observe that NLP research often goes beyond the square one setup, e.g, focusing not only on accuracy, but also on fairness or interpretability, but typically only along a single dimension. Most work targeting multilinguality, for example, considers only accuracy; most work on fairness or interpretability considers only English; and so on. Such one-dimensionality of most research means we are only exploring a fraction of the NLP research search space. We provide historical and recent examples of how the square one bias has led researchers to draw false conclusions or make unwise choices, point to promising yet unexplored directions on the research manifold, and make practical recommendations to enable more multi-dimensional research. We open-source the results of our annotations to enable further analysis.",
        "author": "Sebastian Ruder; Ivan Vuli\u0107; Anders S\u00f8gaard",
        "authorids": "/s/sebastian-ruder/; /i/ivan-vulic/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{ruder-etal-2022-square,\n    title = \"Square One Bias in {NLP}: Towards a Multi-Dimensional Exploration of the Research Manifold\",\n    author = \"Ruder, Sebastian  and\n      Vuli{\\'c}, Ivan  and\n      S{\\o}gaard, Anders\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.184/\",\n    doi = \"10.18653/v1/2022.findings-acl.184\",\n    pages = \"2340--2354\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.184.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.184/",
        "pdf_size": 288066,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=407416479678296379&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Google Research; University of Cambridge; University of Copenhagen",
        "aff_domain": "google.com;cam.ac.uk;di.ku.dk",
        "email": "google.com;cam.ac.uk;di.ku.dk",
        "github": "github.com/google-research/url-nlp",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;University of Cambridge;University of Copenhagen",
        "aff_unique_dep": "Google Research;;",
        "aff_unique_url": "https://research.google;https://www.cam.ac.uk;https://www.ku.dk",
        "aff_unique_abbr": "Google Research;Cambridge;UCPH",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Mountain View;Cambridge;",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "United States;United Kingdom;Denmark"
    },
    {
        "id": "2022.acl-long.489",
        "title": "StableMoE: Stable Routing Strategy for Mixture of Experts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The Mixture-of-Experts (MoE) technique can scale up the model size of Transformers with an affordable computational overhead. We point out that existing learning-to-route MoE methods suffer from the routing fluctuation issue, i.e., the target expert of the same input may change along with training, but only one expert will be activated for the input during inference. The routing fluctuation tends to harm sample efficiency because the same input updates different experts but only one is finally used. In this paper, we propose StableMoE with two training stages to address the routing fluctuation problem. In the first training stage, we learn a balanced and cohesive routing strategy and distill it into a lightweight router decoupled from the backbone model. In the second training stage, we utilize the distilled router to determine the token-to-expert assignment and freeze it for a stable routing strategy. We validate our method on language modeling and multilingual machine translation. The results show that StableMoE outperforms existing MoE methods in terms of both convergence speed and performance.",
        "author": "Damai Dai; Li Dong; Shuming Ma; Bo Zheng; Zhifang Sui; Baobao Chang; Furu Wei",
        "authorids": "/d/damai-dai/; /l/li-dong/; /s/shuming-ma/; /b/bo-zheng/; /z/zhifang-sui/; /b/baobao-chang/; /f/furu-wei/",
        "bibtex": "@inproceedings{dai-etal-2022-stablemoe,\n    title = \"{S}table{M}o{E}: Stable Routing Strategy for Mixture of Experts\",\n    author = \"Dai, Damai  and\n      Dong, Li  and\n      Ma, Shuming  and\n      Zheng, Bo  and\n      Sui, Zhifang  and\n      Chang, Baobao  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.489/\",\n    doi = \"10.18653/v1/2022.acl-long.489\",\n    pages = \"7085--7095\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.489.pdf",
        "site": "https://aclanthology.org/2022.acl-long.489/",
        "pdf_size": 2021342,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1650342554888394995&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "MOE Key Lab of Computational Linguistics, Peking University+Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; MOE Key Lab of Computational Linguistics, Peking University; MOE Key Lab of Computational Linguistics, Peking University; Microsoft Research",
        "aff_domain": "pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;pku.edu.cn;pku.edu.cn;microsoft.com",
        "email": "pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;pku.edu.cn;pku.edu.cn;microsoft.com",
        "github": "https://github.com/Hunter-DDM/stablemoe",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;1;0;0;1",
        "aff_unique_norm": "Peking University;Microsoft",
        "aff_unique_dep": "MOE Key Lab of Computational Linguistics;Microsoft Research",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "PKU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.148",
        "title": "Striking a Balance: Alleviating Inconsistency in Pre-trained Models for Symmetric Classification Tasks",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While fine-tuning pre-trained models for downstream classification is the conventional paradigm in NLP, often task-specific nuances may not get captured in the resultant models. Specifically, for tasks that take two inputs and require the output to be invariant of the order of the inputs, inconsistency is often observed in the predicted labels or confidence scores. We highlight this model shortcoming and apply a consistency loss function to alleviate inconsistency in symmetric classification. Our results show an improved consistency in predictions for three paraphrase detection datasets without a significant drop in the accuracy scores. We examine the classification performance of six datasets (both symmetric and non-symmetric) to showcase the strengths and limitations of our approach.",
        "author": "Ashutosh Kumar; Aditya Joshi",
        "authorids": "/a/ashutosh-kumar/; /a/aditya-joshi/",
        "bibtex": "@inproceedings{kumar-joshi-2022-striking,\n    title = \"Striking a Balance: Alleviating Inconsistency in Pre-trained Models for Symmetric Classification Tasks\",\n    author = \"Kumar, Ashutosh  and\n      Joshi, Aditya\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.148/\",\n    doi = \"10.18653/v1/2022.findings-acl.148\",\n    pages = \"1887--1895\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.148.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.148/",
        "pdf_size": 1209497,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=363107772514401881&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Indian Institute of Science, India; SEEK, Australia",
        "aff_domain": "iisc.ac.in;gmail.com",
        "email": "iisc.ac.in;gmail.com",
        "github": "https://github.com/ashutoshml/alleviating-inconsistency1887",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Indian Institute of Science;SEEK",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.seek.com.au",
        "aff_unique_abbr": "IISc;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "India;Australia"
    },
    {
        "id": "2022.acl-long.23",
        "title": "Structural Characterization for Dialogue Disentanglement",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Tangled multi-party dialogue contexts lead to challenges for dialogue reading comprehension, where multiple dialogue threads flow simultaneously within a common dialogue record, increasing difficulties in understanding the dialogue history for both human and machine. Previous studies mainly focus on utterance encoding methods with carefully designed features but pay inadequate attention to characteristic features of the structure of dialogues. We specially take structure factors into account and design a novel model for dialogue disentangling. Based on the fact that dialogues are constructed on successive participation and interactions between speakers, we model structural information of dialogues in two aspects: 1)speaker property that indicates whom a message is from, and 2) reference dependency that shows whom a message may refer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC benchmark dataset and contributes to dialogue-related comprehension.",
        "author": "Xinbei Ma; Zhuosheng Zhang; Hai Zhao",
        "authorids": "/x/xinbei-ma/; /z/zhuosheng-zhang/; /h/hai-zhao/",
        "bibtex": "@inproceedings{ma-etal-2022-structural,\n    title = \"Structural Characterization for Dialogue Disentanglement\",\n    author = \"Ma, Xinbei  and\n      Zhang, Zhuosheng  and\n      Zhao, Hai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.23/\",\n    doi = \"10.18653/v1/2022.acl-long.23\",\n    pages = \"285--297\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.23.pdf",
        "site": "https://aclanthology.org/2022.acl-long.23/",
        "pdf_size": 451787,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2691805248954798123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.322",
        "title": "Structural Supervision for Word Alignment and Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation. Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences. In this work, we propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multi-task learning. Particularly, we won\u2019t leverage any annotated syntactic graph of the target side during training, so we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially and simultaneously generate the target tokens and the corresponding syntactic graphs, and further guide the word alignment. On this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs. Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality.",
        "author": "Lei Li; Kai Fan; Hongjia Li; Chun Yuan",
        "authorids": "/l/lei-li/; /k/kai-fan/; /h/hongjia-li/; /c/chun-yuan/",
        "bibtex": "@inproceedings{li-etal-2022-structural,\n    title = \"Structural Supervision for Word Alignment and Machine Translation\",\n    author = \"Li, Lei  and\n      Fan, Kai  and\n      Li, Hongjia  and\n      Yuan, Chun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.322/\",\n    doi = \"10.18653/v1/2022.findings-acl.322\",\n    pages = \"4084--4094\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.322.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.322/",
        "pdf_size": 915085,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4324638743136010781&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Technology, Tsinghua University, Beijing, China+Alibaba DAMO Academy, Alibaba Group Inc.+Tsinghua Shenzhen International Graduate School, Peng Cheng Lab; Alibaba DAMO Academy, Alibaba Group Inc.; Department of Computer Science and Technology, Tsinghua University, Beijing, China+Tsinghua Shenzhen International Graduate School, Peng Cheng Lab; Tsinghua Shenzhen International Graduate School, Peng Cheng Lab",
        "aff_domain": "tsinghua.edu.cn;alibaba-inc.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "tsinghua.edu.cn;alibaba-inc.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+0;1;0+0;0",
        "aff_unique_norm": "Tsinghua University;Alibaba Group Inc.",
        "aff_unique_dep": "Department of Computer Science and Technology;Alibaba DAMO Academy",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.alibaba-group.com",
        "aff_unique_abbr": "THU;Alibaba",
        "aff_campus_unique_index": "0+2;0+2;2",
        "aff_campus_unique": "Beijing;;Shenzhen",
        "aff_country_unique_index": "0+0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.107",
        "title": "Structured Pruning Learns Compact and Accurate Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches.",
        "author": "Mengzhou Xia; Zexuan Zhong; Danqi Chen",
        "authorids": "/m/mengzhou-xia/; /z/zexuan-zhong/; /d/danqi-chen/",
        "bibtex": "@inproceedings{xia-etal-2022-structured,\n    title = \"Structured Pruning Learns Compact and Accurate Models\",\n    author = \"Xia, Mengzhou  and\n      Zhong, Zexuan  and\n      Chen, Danqi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.107/\",\n    doi = \"10.18653/v1/2022.acl-long.107\",\n    pages = \"1513--1528\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.107.pdf",
        "site": "https://aclanthology.org/2022.acl-long.107/",
        "pdf_size": 2892808,
        "gs_citation": 253,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14913415226188690622&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "Princeton University; Princeton University; Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "github": "https://github.com/princeton-nlp/CoFiPruning",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.68",
        "title": "Sub-Word Alignment is Still Useful: A Vest-Pocket Method for Enhancing Low-Resource Machine Translation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We leverage embedding duplication between aligned sub-words to extend the Parent-Child transfer learning method, so as to improve low-resource machine translation. We conduct experiments on benchmark datasets of My-En, Id-En and Tr-En translation scenarios. The test results show that our method produces substantial improvements, achieving the BLEU scores of 22.5, 28.0 and 18.1 respectively. In addition, the method is computationally efficient which reduces the consumption of training time by 63.8%, reaching the duration of 1.6 hours when training on a Tesla 16GB P100 GPU. All the models and source codes in the experiments will be made publicly available to support reproducible research.",
        "author": "Minhan Xu; Yu Hong",
        "authorids": "/m/minhan-xu/; /y/yu-hong/",
        "bibtex": "@inproceedings{xu-hong-2022-sub,\n    title = \"Sub-Word Alignment is Still Useful: A Vest-Pocket Method for Enhancing Low-Resource Machine Translation\",\n    author = \"Xu, Minhan  and\n      Hong, Yu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.68/\",\n    doi = \"10.18653/v1/2022.acl-short.68\",\n    pages = \"613--619\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.68.pdf",
        "site": "https://aclanthology.org/2022.acl-short.68/",
        "pdf_size": 221095,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17014279125180907263&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, Soochow University, China",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Soochow University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "https://eng.suda.edu.cn/",
        "aff_unique_abbr": "Soochow U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.396",
        "title": "Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent works on knowledge base question answering (KBQA) retrieve subgraphs for easier reasoning. The desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises. However, the existing retrieval is either heuristic or interwoven with the reasoning, causing reasoning on the partial subgraphs, which increases the reasoning bias when the intermediate supervision is missing. This paper proposes a trainable subgraph retriever (SR) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive experiments demonstrate SR achieves significantly better retrieval and QA performance than existing retrieval methods. Via weakly supervised pre-training as well as the end-to-end fine-tuning, SR achieves new state-of-the-art performance when combined with NSM (He et al., 2021), a subgraph-oriented reasoner, for embedding-based KBQA methods. Codes and datasets are available online (https://github.com/RUCKBReasoning/SubgraphRetrievalKBQA)",
        "author": "Jing Zhang; Xiaokang Zhang; Jifan Yu; Jian Tang; Jie Tang; Cuiping Li; Hong Chen",
        "authorids": "/j/jing-zhang/; /x/xiaokang-zhang/; /j/jifan-yu/; /j/jian-tang/; /j/jie-tang/; /c/cuiping-li/; /h/hong-chen/",
        "bibtex": "@inproceedings{zhang-etal-2022-subgraph,\n    title = \"Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering\",\n    author = \"Zhang, Jing  and\n      Zhang, Xiaokang  and\n      Yu, Jifan  and\n      Tang, Jian  and\n      Tang, Jie  and\n      Li, Cuiping  and\n      Chen, Hong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.396/\",\n    doi = \"10.18653/v1/2022.acl-long.396\",\n    pages = \"5773--5784\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.396.pdf",
        "site": "https://aclanthology.org/2022.acl-long.396/",
        "pdf_size": 6512602,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4218622362084491883&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Information, Renmin University of China; School of Information, Renmin University of China; Department of Computer Science and Technology, Tsinghua University; Mila - Quebec AI Institute; Department of Computer Science and Technology, Tsinghua University; School of Information, Renmin University of China; School of Information, Renmin University of China",
        "aff_domain": "ruc.edu.cn;ruc.edu.cn;mails.tsinghua.edu.cn;hec.ca;tsinghua.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn;ruc.edu.cn;mails.tsinghua.edu.cn;hec.ca;tsinghua.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "github": "https://github.com/RUCKBReasoning/SubgraphRetrievalKBQA",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;2;1;0;0",
        "aff_unique_norm": "Renmin University of China;Tsinghua University;Quebec AI Institute",
        "aff_unique_dep": "School of Information;Department of Computer Science and Technology;AI Institute",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.tsinghua.edu.cn;https://mila.quebec",
        "aff_unique_abbr": "RUC;THU;Mila",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0;0",
        "aff_country_unique": "China;Canada"
    },
    {
        "id": "2022.acl-long.452",
        "title": "Substructure Distribution Projection for Zero-Shot Cross-Lingual Dependency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present substructure distribution projection (SubDP), a technique that projects a distribution over structures in one domain to another, by projecting substructure distributions separately. Models for the target domain can then be trained, using the projected distributions as soft silver labels. We evaluate SubDP on zero shot cross-lingual dependency parsing, taking dependency arcs as substructures: we project the predicted dependency arc distributions in the source language(s) to target language(s), and train a target language parser on the resulting distributions. Given an English tree bank as the only source of human supervision, SubDP achieves better unlabeled attachment score than all prior work on the Universal Dependencies v2.2 (Nivre et al., 2020) test set across eight diverse target languages, as well as the best labeled attachment score on six languages. In addition, SubDP improves zero shot cross-lingual dependency parsing with very few (e.g., 50) supervised bitext pairs, across a broader range of target languages.",
        "author": "Freda Shi; Kevin Gimpel; Karen Livescu",
        "authorids": "/f/freda-shi/; /k/kevin-gimpel/; /k/karen-livescu/",
        "bibtex": "@inproceedings{shi-etal-2022-substructure,\n    title = \"Substructure Distribution Projection for Zero-Shot Cross-Lingual Dependency Parsing\",\n    author = \"Shi, Freda  and\n      Gimpel, Kevin  and\n      Livescu, Karen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.452/\",\n    doi = \"10.18653/v1/2022.acl-long.452\",\n    pages = \"6547--6563\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.452.pdf",
        "site": "https://aclanthology.org/2022.acl-long.452/",
        "pdf_size": 747588,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14390931876335147557&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "ttic.edu;ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI Chicago",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.112",
        "title": "SummN: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose SummN, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. SummN first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed. Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge, SummN is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that SummN outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport. Our data and code are available at https://github.com/psunlpgroup/Summ-N.",
        "author": "Yusen Zhang; Ansong Ni; Ziming Mao; Chen Henry Wu; Chenguang Zhu; Budhaditya Deb; Ahmed Awadallah; Dragomir Radev; Rui Zhang",
        "authorids": "/y/yusen-zhang/; /a/ansong-ni/; /z/ziming-mao/; /c/chen-henry-wu/; /c/chenguang-zhu/; /b/budhaditya-deb/; /a/ahmed-awadallah/; /d/dragomir-radev/; /r/rui-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2022-summn,\n    title = \"{S}umm$^N$: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents\",\n    author = \"Zhang, Yusen  and\n      Ni, Ansong  and\n      Mao, Ziming  and\n      Wu, Chen Henry  and\n      Zhu, Chenguang  and\n      Deb, Budhaditya  and\n      Awadallah, Ahmed  and\n      Radev, Dragomir  and\n      Zhang, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.112/\",\n    doi = \"10.18653/v1/2022.acl-long.112\",\n    pages = \"1592--1604\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.112.pdf",
        "site": "https://aclanthology.org/2022.acl-long.112/",
        "pdf_size": 434393,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16356199590386024228&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Penn State University\u2663; Yale University\u2020; Carnegie Mellon University\u2021; Microsoft Research\u2666; Microsoft Research\u2666; Microsoft Research\u2666; Microsoft Research\u2666; Yale University\u2020; Penn State University\u2663",
        "aff_domain": "psu.edu;psu.edu;yale.edu;yale.edu; ; ; ; ;",
        "email": "psu.edu;psu.edu;yale.edu;yale.edu; ; ; ; ;",
        "github": "https://github.com/psunlpgroup/Summ-N",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;2;3;3;3;3;1;0",
        "aff_unique_norm": "Penn State University;Yale University;Carnegie Mellon University;Microsoft",
        "aff_unique_dep": ";;;Microsoft Research",
        "aff_unique_url": "https://www.psu.edu;https://www.yale.edu;https://www.cmu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "PSU;Yale;CMU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.589",
        "title": "SummScreen: A Dataset for Abstractive Screenplay Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce SummScreen, a summarization dataset comprised of pairs of TV series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be found and integrated to form the succinct plot descriptions in the recaps. Also, TV scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief. This information is rarely contained in recaps. Since characters are fundamental to TV series, we also propose two entity-centric evaluation metrics. Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors. An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts. Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors. Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions.",
        "author": "Mingda Chen; Zewei Chu; Sam Wiseman; Kevin Gimpel",
        "authorids": "/m/mingda-chen/; /z/zewei-chu/; /s/sam-wiseman/; /k/kevin-gimpel/",
        "bibtex": "@inproceedings{chen-etal-2022-summscreen,\n    title = \"{S}umm{S}creen: A Dataset for Abstractive Screenplay Summarization\",\n    author = \"Chen, Mingda  and\n      Chu, Zewei  and\n      Wiseman, Sam  and\n      Gimpel, Kevin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.589/\",\n    doi = \"10.18653/v1/2022.acl-long.589\",\n    pages = \"8602--8615\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.589.pdf",
        "site": "https://aclanthology.org/2022.acl-long.589/",
        "pdf_size": 429337,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13561423579695636817&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Toyota Technological Institute at Chicago, IL, USA+University of Chicago; Toyota Technological Institute at Chicago, IL, USA+Duke University, NC, USA; Duke University, NC, USA+Toyota Technological Institute at Chicago, IL, USA; Toyota Technological Institute at Chicago, IL, USA",
        "aff_domain": "ttic.edu;gmail.com;cs.duke.edu;ttic.edu",
        "email": "ttic.edu;gmail.com;cs.duke.edu;ttic.edu",
        "github": "https://github.com/mingdachen/SummScreen",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+2;2+0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Chicago;Duke University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.uchicago.edu;https://www.duke.edu",
        "aff_unique_abbr": "TTI Chicago;UChicago;Duke",
        "aff_campus_unique_index": "0;0+2;2+0;0",
        "aff_campus_unique": "Chicago;;Durham",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.309",
        "title": "SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PEGASUS, we push ROUGE scores by 5.44% on CNN- DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at https://github.com/ntunlp/SummaReranker.",
        "author": "Mathieu Ravaut; Shafiq Joty; Nancy Chen",
        "authorids": "/m/mathieu-ravaut/; /s/shafiq-joty/; /n/nancy-chen/",
        "bibtex": "@inproceedings{ravaut-etal-2022-summareranker,\n    title = \"{S}umma{R}eranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization\",\n    author = \"Ravaut, Mathieu  and\n      Joty, Shafiq  and\n      Chen, Nancy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.309/\",\n    doi = \"10.18653/v1/2022.acl-long.309\",\n    pages = \"4504--4524\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.309.pdf",
        "site": "https://aclanthology.org/2022.acl-long.309/",
        "pdf_size": 1810201,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16827900334865836040&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Nanyang Technological University, Singapore + Institute of Infocomm Research (I2R), Singapore + Salesforce Research Asia, Singapore; Nanyang Technological University, Singapore + Institute of Infocomm Research (I2R), Singapore; Nanyang Technological University, Singapore + Institute of Infocomm Research (I2R), Singapore",
        "aff_domain": "e.ntu.edu.sg;ntu.edu.sg;i2r.a-star.edu.sg",
        "email": "e.ntu.edu.sg;ntu.edu.sg;i2r.a-star.edu.sg",
        "github": "https://github.com/ntunlp/SummaReranker",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;0+1;0+1",
        "aff_unique_norm": "Nanyang Technological University;Institute of Infocomm Research;Salesforce Research Asia",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.i2r.a-star.edu.sg;https://research.salesforce.com",
        "aff_unique_abbr": "NTU;I2R;SRA",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0+0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.findings-acl.227",
        "title": "Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prior research has discussed and illustrated the need to consider linguistic norms at the community level when studying taboo (hateful/offensive/toxic etc.) language. However, a methodology for doing so, that is firmly founded on community language norms is still largely absent. This can lead both to biases in taboo text classification and limitations in our understanding of the causes of bias. We propose a method to study bias in taboo classification and annotation where a community perspective is front and center. This is accomplished by using special classifiers tuned for each community\u2019s language. In essence, these classifiers represent community level language norms. We use these to study bias and find, for example, biases are largest against African Americans (7/10 datasets and all 3 classifiers examined). In contrast to previous papers we also study other communities and find, for example, strong biases against South Asians. In a small scale user study we illustrate our key idea which is that common utterances, i.e., those with high alignment scores with a community (community classifier confidence scores) are unlikely to be regarded taboo. Annotators who are community members contradict taboo classification decisions and annotations in a majority of instances. This paper is a significant step toward reducing false positive taboo decisions that over time harm minority communities.",
        "author": "Osama Khalid; Jonathan Rusert; Padmini Srinivasan",
        "authorids": "/o/osama-khalid/; /j/jonathan-rusert/; /p/padmini-srinivasan/",
        "bibtex": "@inproceedings{khalid-etal-2022-suum,\n    title = \"Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective\",\n    author = \"Khalid, Osama  and\n      Rusert, Jonathan  and\n      Srinivasan, Padmini\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.227/\",\n    doi = \"10.18653/v1/2022.findings-acl.227\",\n    pages = \"2883--2896\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.227.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.227/",
        "pdf_size": 549024,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13265347982421674660&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Iowa; University of Iowa; University of Iowa",
        "aff_domain": "uiowa.edu;uiowa.edu;uiowa.edu",
        "email": "uiowa.edu;uiowa.edu;uiowa.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Iowa",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uiowa.edu",
        "aff_unique_abbr": "UIowa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.40",
        "title": "SyMCoM - Syntactic Measure of Code Mixing A Study Of English-Hindi Code-Mixing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations. Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models. For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed. Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model\u2019s performance is under explored. In this work, we investigate a collection of English(en)-Hindi(hi) code-mixed datasets from a syntactic lens to propose, SyMCoM, an indicator of syntactic variety in code-mixed text, with intuitive theoretical bounds. We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, and demonstrate the utility of SyMCoM by applying it on various syntactical categories on a collection of datasets, and compare datasets using the measure.",
        "author": "Prashant Kodali; Anmol Goel; Monojit Choudhury; Manish Shrivastava; Ponnurangam Kumaraguru",
        "authorids": "/p/prashant-kodali/; /a/anmol-goel/; /m/monojit-choudhury/; /m/manish-shrivastava/; /p/ponnurangam-kumaraguru/",
        "bibtex": "@inproceedings{kodali-etal-2022-symcom,\n    title = \"{S}y{MC}o{M} - Syntactic Measure of Code Mixing A Study Of {E}nglish-{H}indi Code-Mixing\",\n    author = \"Kodali, Prashant  and\n      Goel, Anmol  and\n      Choudhury, Monojit  and\n      Shrivastava, Manish  and\n      Kumaraguru, Ponnurangam\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.40/\",\n    doi = \"10.18653/v1/2022.findings-acl.40\",\n    pages = \"472--480\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.40.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.40/",
        "pdf_size": 1547258,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16576558413869485060&as_sdt=5,36&sciodt=0,36&hl=en",
        "gs_version_total": 3,
        "aff": "International Institute of Information Technology Hyderabad; International Institute of Information Technology Hyderabad; Microsoft Research, India; International Institute of Information Technology Hyderabad; International Institute of Information Technology Hyderabad",
        "aff_domain": "research.iiit.ac.in;research.iiit.ac.in;microsoft.com;iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;research.iiit.ac.in;microsoft.com;iiit.ac.in;iiit.ac.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "International Institute of Information Technology;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://iiit Hyderabad.ac.in;https://www.microsoft.com/en-us/research/group/india.aspx",
        "aff_unique_abbr": "IIIT Hyderabad;MSR India",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hyderabad;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.findings-acl.235",
        "title": "Synchronous Refinement for Neural Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Machine translation typically adopts an encoder-to-decoder framework, in which the decoder generates the target sentence word-by-word in an auto-regressive manner. However, the auto-regressive decoder faces a deep-rooted one-pass issue whereby each generated word is considered as one element of the final output regardless of whether it is correct or not. These generated wrong words further constitute the target historical context to affect the generation of subsequent target words. This paper proposes a novel synchronous refinement method to revise potential errors in the generated words by considering part of the target future context. Particularly, the proposed approach allows the auto-regressive decoder to refine the previously generated target words and generate the next target word synchronously. The experimental results on three widely-used machine translation tasks demonstrated the effectiveness of the proposed approach.",
        "author": "Kehai Chen; Masao Utiyama; Eiichiro Sumita; Rui Wang; Min Zhang",
        "authorids": "/k/kehai-chen/; /m/masao-utiyama/; /e/eiichiro-sumita/; /r/rui-wang/; /m/min-zhang/",
        "bibtex": "@inproceedings{chen-etal-2022-synchronous,\n    title = \"Synchronous Refinement for Neural Machine Translation\",\n    author = \"Chen, Kehai  and\n      Utiyama, Masao  and\n      Sumita, Eiichiro  and\n      Wang, Rui  and\n      Zhang, Min\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.235/\",\n    doi = \"10.18653/v1/2022.findings-acl.235\",\n    pages = \"2986--2996\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.235.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.235/",
        "pdf_size": 855813,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7164218889576496799&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Harbin Institute of Technology, Shenzhen, China; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; Shanghai Jiao Tong University, Shanghai, China; Harbin Institute of Technology, Shenzhen, China",
        "aff_domain": "hit.edu.cn;nict.go.jp;nict.go.jp;sjtu.edu.cn;hit.edu.cn",
        "email": "hit.edu.cn;nict.go.jp;nict.go.jp;sjtu.edu.cn;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "Harbin Institute of Technology;National Institute of Information and Communications Technology;Shanghai Jiao Tong University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://en.hhit.edu.cn/;https://www.nict.go.jp/;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "HIT;NICT;SJTU",
        "aff_campus_unique_index": "0;1;1;2;0",
        "aff_campus_unique": "Shenzhen;Kyoto;Shanghai",
        "aff_country_unique_index": "0;1;1;0;0",
        "aff_country_unique": "China;Japan"
    },
    {
        "id": "2022.findings-acl.191",
        "title": "Syntax-guided Contrastive Learning for Pre-trained Language Model",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Syntactic information has been proved to be useful for transformer-based pre-trained language models. Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks. This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios. In order to inject syntactic knowledge effectively and efficiently into pre-trained language models, we propose a novel syntax-guided contrastive learning method which does not change the transformer architecture. Based on constituency and dependency structures of syntax trees, we design phrase-guided and tree-guided contrastive objectives, and optimize them in the pre-training stage, so as to help the pre-trained language model to capture rich syntactic knowledge in its representations. Experimental results show that our contrastive method achieves consistent improvements in a variety of tasks, including grammatical error detection, entity tasks, structural probing and GLUE. Detailed analysis further verifies that the improvements come from the utilization of syntactic information, and the learned attention weights are more explainable in terms of linguistics.",
        "author": "Shuai Zhang; Wang Lijie; Xinyan Xiao; Hua Wu",
        "authorids": "/s/shuai-zhang/; /w/wang-lijie/; /x/xinyan-xiao/; /h/hua-wu/",
        "bibtex": "@inproceedings{zhang-etal-2022-syntax,\n    title = \"Syntax-guided Contrastive Learning for Pre-trained Language Model\",\n    author = \"Zhang, Shuai  and\n      Lijie, Wang  and\n      Xiao, Xinyan  and\n      Wu, Hua\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.191/\",\n    doi = \"10.18653/v1/2022.findings-acl.191\",\n    pages = \"2430--2440\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.191.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.191/",
        "pdf_size": 1137154,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9355192606755181946&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Baidu Inc, Beijing, China; Baidu Inc, Beijing, China; Baidu Inc, Beijing, China; Baidu Inc, Beijing, China",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Baidu Inc",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.95",
        "title": "Synthetic Question Value Estimation for Domain Adaptation of Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Synthesizing QA pairs with a question generator (QG) on the target domain has become a popular approach for domain adaptation of question answering (QA) models. Since synthetic questions are often noisy in practice, existing work adapts scores from a pretrained QA (or QG) model as criteria to select high-quality questions. However, these scores do not directly serve the ultimate goal of improving QA performance on the target domain. In this paper, we introduce a novel idea of training a question value estimator (QVE) that directly estimates the usefulness of synthetic questions for improving the target-domain QA performance. By conducting comprehensive experiments, we show that the synthetic questions selected by QVE can help achieve better target-domain QA performance, in comparison with existing techniques. We additionally show that by using such questions and only around 15% of the human annotations on the target domain, we can achieve comparable performance to the fully-supervised baselines.",
        "author": "Xiang Yue; Ziyu Yao; Huan Sun",
        "authorids": "/x/xiang-yue/; /z/ziyu-yao/; /h/huan-sun/",
        "bibtex": "@inproceedings{yue-etal-2022-synthetic,\n    title = \"Synthetic Question Value Estimation for Domain Adaptation of Question Answering\",\n    author = \"Yue, Xiang  and\n      Yao, Ziyu  and\n      Sun, Huan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.95/\",\n    doi = \"10.18653/v1/2022.acl-long.95\",\n    pages = \"1340--1351\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.95.pdf",
        "site": "https://aclanthology.org/2022.acl-long.95/",
        "pdf_size": 524964,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4761891627865180192&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The Ohio State University; George Mason University; The Ohio State University",
        "aff_domain": "osu.edu;gmu.edu;osu.edu",
        "email": "osu.edu;gmu.edu;osu.edu",
        "github": "https://github.com/xiangyue9607/QVE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Ohio State University;George Mason University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.osu.edu;https://www.gmu.edu",
        "aff_unique_abbr": "OSU;GMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.376",
        "title": "Systematic Inequalities in Language Technology Performance across the World\u2019s Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world\u2019s \u22486,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. Data and code to reproduce the findings discussed in this paper areavailable on GitHub (https://github.com/neubig/globalutility).",
        "author": "Damian Blasi; Antonios Anastasopoulos; Graham Neubig",
        "authorids": "/d/damian-blasi/; /a/antonios-anastasopoulos/; /g/graham-neubig/",
        "bibtex": "@inproceedings{blasi-etal-2022-systematic,\n    title = \"Systematic Inequalities in Language Technology Performance across the World`s Languages\",\n    author = \"Blasi, Damian  and\n      Anastasopoulos, Antonios  and\n      Neubig, Graham\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.376/\",\n    doi = \"10.18653/v1/2022.acl-long.376\",\n    pages = \"5486--5505\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.376.pdf",
        "site": "https://aclanthology.org/2022.acl-long.376/",
        "pdf_size": 3147611,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3469963221296306196&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "Harvard University; George Mason University; Carnegie Mellon University",
        "aff_domain": "fas.harvard.edu;gmu.edu;cs.cmu.edu",
        "email": "fas.harvard.edu;gmu.edu;cs.cmu.edu",
        "github": "https://github.com/neubig/globalutility",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Harvard University;George Mason University;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.harvard.edu;https://www.gmu.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Harvard;GMU;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.185",
        "title": "Systematicity, Compositionality and Transitivity of Deep NLP Models: a Metamorphic Testing Perspective",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Metamorphic testing has recently been used to check the safety of neural NLP models. Its main advantage is that it does not rely on a ground truth to generate test cases. However, existing studies are mostly concerned with robustness-like metamorphic relations, limiting the scope of linguistic properties they can test. We propose three new classes of metamorphic relations, which address the properties of systematicity, compositionality and transitivity. Unlike robustness, our relations are defined over multiple source inputs, thus increasing the number of test cases that we can produce by a polynomial factor. With them, we test the internal consistency of state-of-the-art NLP models, and show that they do not always behave according to their expected linguistic properties. Lastly, we introduce a novel graphical notation that efficiently summarises the inner structure of metamorphic relations.",
        "author": "Edoardo Manino; Julia Rozanova; Danilo Carvalho; Andre Freitas; Lucas Cordeiro",
        "authorids": "/e/edoardo-manino/; /j/julia-rozanova/; /d/danilo-carvalho/; /a/andre-freitas/; /l/lucas-cordeiro/",
        "bibtex": "@inproceedings{manino-etal-2022-systematicity,\n    title = \"Systematicity, Compositionality and Transitivity of Deep {NLP} Models: a Metamorphic Testing Perspective\",\n    author = \"Manino, Edoardo  and\n      Rozanova, Julia  and\n      Carvalho, Danilo  and\n      Freitas, Andre  and\n      Cordeiro, Lucas\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.185/\",\n    doi = \"10.18653/v1/2022.findings-acl.185\",\n    pages = \"2355--2366\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.185.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.185/",
        "pdf_size": 295371,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13678071339826257783&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Manchester, United Kingdom; University of Manchester, United Kingdom; University of Manchester, United Kingdom; University of Manchester, United Kingdom+Idiap Research Institute, Switzerland; University of Manchester, United Kingdom",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0+1;0",
        "aff_unique_norm": "University of Manchester;Idiap Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.manchester.ac.uk;https://www.idiap.ch",
        "aff_unique_abbr": "UoM;Idiap",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1;0",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "2022.findings-acl.169",
        "title": "TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Entity retrieval\u2014retrieving information about entity mentions in a query\u2014is a key step in open-domain tasks, such as question answering or fact checking. However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities. Incorporating knowledge graph types during training could help overcome popularity biases, but there are several challenges: (1) existing type-based retrieval methods require mention boundaries as input, but open-domain tasks run on unstructured text, (2) type-based methods should not compromise overall performance, and (3) type-based methods should be robust to noisy and missing types. In this work, we introduce TABi, a method to jointly train bi-encoders on knowledge graph types and unstructured text for entity retrieval for open-domain tasks. TABi leverages a type-enforced contrastive loss to encourage entities and queries of similar types to be close in the embedding space. TABi improves retrieval of rare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining strong overall retrieval performance on open-domain tasks in the KILT benchmark compared to state-of-the-art retrievers. TABi is also robust to incomplete type systems, improving rare entity retrieval over baselines with only 5% type coverage of the training dataset. We make our code publicly available.",
        "author": "Megan Leszczynski; Daniel Fu; Mayee Chen; Christopher Re",
        "authorids": "/m/megan-leszczynski/; /d/daniel-fu/; /m/mayee-chen/; /c/christopher-re/",
        "bibtex": "@inproceedings{leszczynski-etal-2022-tabi,\n    title = \"{TAB}i: {T}ype-Aware Bi-Encoders for Open-Domain Entity Retrieval\",\n    author = \"Leszczynski, Megan  and\n      Fu, Daniel  and\n      Chen, Mayee  and\n      Re, Christopher\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.169/\",\n    doi = \"10.18653/v1/2022.findings-acl.169\",\n    pages = \"2147--2166\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.169.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.169/",
        "pdf_size": 926770,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8378667852286943858&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "https://github.com/HazyResearch/tabi",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.277",
        "title": "THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "As more and more pre-trained language models adopt on-cloud deployment, the privacy issues grow quickly, mainly for the exposure of plain-text user data (e.g., search history, medical record, bank account). Privacy-preserving inference of transformer models is on the demand of cloud service users. To protect privacy, it is an attractive choice to compute only with ciphertext in homomorphic encryption (HE). However, enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet. In this work, we introduce THE-X, an approximation approach for transformers, which enables privacy-preserving inference of pre-trained models developed by popular frameworks. THE-X proposes a workflow to deal with complex computation in transformer networks, including all the non-polynomial functions like GELU, softmax, and LayerNorm. Experiments reveal our proposed THE-X can enable transformer inference on encrypted data for different downstream tasks, all with negligible performance drop but enjoying the theory-guaranteed privacy-preserving advantage.",
        "author": "Tianyu Chen; Hangbo Bao; Shaohan Huang; Li Dong; Binxing Jiao; Daxin Jiang; Haoyi Zhou; Jianxin Li; Furu Wei",
        "authorids": "/t/tianyu-chen/; /h/hangbo-bao/; /s/shaohan-huang/; /l/li-dong/; /b/binxing-jiao/; /d/daxin-jiang/; /h/haoyi-zhou/; /j/jianxin-li/; /f/furu-wei/",
        "bibtex": "@inproceedings{chen-etal-2022-x,\n    title = \"{THE}-{X}: Privacy-Preserving Transformer Inference with Homomorphic Encryption\",\n    author = \"Chen, Tianyu  and\n      Bao, Hangbo  and\n      Huang, Shaohan  and\n      Dong, Li  and\n      Jiao, Binxing  and\n      Jiang, Daxin  and\n      Zhou, Haoyi  and\n      Li, Jianxin  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.277/\",\n    doi = \"10.18653/v1/2022.findings-acl.277\",\n    pages = \"3510--3520\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.277.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.277/",
        "pdf_size": 1230252,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13221036204144004412&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "BDBC, Beihang University, China+SKLSDE, Beihang University, China; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft STCA; BDBC, Beihang University, China+SKLSDE, Beihang University, China; BDBC, Beihang University, China+SKLSDE, Beihang University, China; BDBC, Beihang University, China+SKLSDE, Beihang University, China; Microsoft Research",
        "aff_domain": "buaa.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;buaa.edu.cn;buaa.edu.cn;microsoft.com",
        "email": "buaa.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;buaa.edu.cn;buaa.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+0;1;1;1;1;0+0;0+0;0+0;1",
        "aff_unique_norm": "Beihang University;Microsoft",
        "aff_unique_dep": "BDBC;Microsoft Research",
        "aff_unique_url": "http://www.buaa.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": ";MSR",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;1;1;1;0+0;0+0;0+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.13",
        "title": "Table-based Fact Verification with Self-adaptive Mixture of Experts",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem. It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative). Considering that, we exploit mixture-of-experts and present in this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE). Specifically, we have developed a mixture-of-experts neural network to recognize and execute different types of reasoning\u2014the network is composed of multiple experts, each handling a specific part of the semantics for reasoning, whereas a management module is applied to decide the contribution of each expert network to the verification result. A self-adaptive method is developed to teach the management module combining results of different experts more efficiently without external knowledge. The experimental results illustrate that our framework achieves 85.1% accuracy on the benchmark dataset TabFact, comparable with the previous state-of-the-art models. We hope our framework can serve as a new baseline for table-based verification. Our code is available at https://github.com/THUMLP/SaMoE.",
        "author": "Yuxuan Zhou; Xien Liu; Kaiyin Zhou; Ji Wu",
        "authorids": "/y/yuxuan-zhou/; /x/xien-liu/; /k/kaiyin-zhou/; /j/ji-wu/",
        "bibtex": "@inproceedings{zhou-etal-2022-table,\n    title = \"Table-based Fact Verification with Self-adaptive Mixture of Experts\",\n    author = \"Zhou, Yuxuan  and\n      Liu, Xien  and\n      Zhou, Kaiyin  and\n      Wu, Ji\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.13/\",\n    doi = \"10.18653/v1/2022.findings-acl.13\",\n    pages = \"139--149\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.13.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.13/",
        "pdf_size": 527002,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14846033964092737549&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; THiFLY Research, Tsinghua University+State Key Laboratory of Cognitive Intelligence; Department of Electronic Engineering, Tsinghua University",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/THUMLP/SaMoE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "Tsinghua University;State Key Laboratory of Cognitive Intelligence",
        "aff_unique_dep": "Department of Electronic Engineering;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "THU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.40",
        "title": "TableFormer: Robust Transformer Modeling for Table-Text Encoding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding tables is an important aspect of natural language understanding. Existing models for table understanding require linearization of the table structure, where row or column order is encoded as an unwanted bias. Such spurious biases make the model vulnerable to row and column order perturbations. Additionally, prior work has not thoroughly modeled the table structures or table-text alignments, hindering the table-text understanding ability. In this work, we propose a robust and structurally aware table-text encoding architecture TableFormer, where tabular structural biases are incorporated completely through learnable attention biases. TableFormer is (1) strictly invariant to row and column orders, and, (2) could understand tables better due to its tabular inductive biases. Our evaluations showed that TableFormer outperforms strong baselines in all settings on SQA, WTQ and TabFact table reasoning datasets, and achieves state-of-the-art performance on SQA, especially when facing answer-invariant row and column order perturbations (6% improvement over the best baseline), because previous SOTA models\u2019 performance drops by 4% - 6% when facing such perturbations while TableFormer is not affected.",
        "author": "Jingfeng Yang; Aditya Gupta; Shyam Upadhyay; Luheng He; Rahul Goel; Shachi Paul",
        "authorids": "/j/jingfeng-yang/; /a/aditya-gupta/; /s/shyam-upadhyay/; /l/luheng-he/; /r/rahul-goel/; /s/shachi-paul/",
        "bibtex": "@inproceedings{yang-etal-2022-tableformer,\n    title = \"{T}able{F}ormer: Robust Transformer Modeling for Table-Text Encoding\",\n    author = \"Yang, Jingfeng  and\n      Gupta, Aditya  and\n      Upadhyay, Shyam  and\n      He, Luheng  and\n      Goel, Rahul  and\n      Paul, Shachi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.40/\",\n    doi = \"10.18653/v1/2022.acl-long.40\",\n    pages = \"528--537\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.40.pdf",
        "site": "https://aclanthology.org/2022.acl-long.40/",
        "pdf_size": 393649,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3704680821735791322&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 6,
        "aff": "Georgia Institute of Technology; Google Assistant; Google Assistant; Google Assistant; Google Assistant; Google Assistant",
        "aff_domain": "gmail.com;google.com;google.com;google.com;google.com;google.com",
        "email": "gmail.com;google.com;google.com;google.com;google.com;google.com",
        "github": "https://github.com/google-research/tapas",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Georgia Institute of Technology;Google",
        "aff_unique_dep": ";Google Assistant",
        "aff_unique_url": "https://www.gatech.edu;https://assistant.google.com",
        "aff_unique_abbr": "Georgia Tech;Google Assistant",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.97",
        "title": "Tackling Fake News Detection by Continually Improving Social Context Representations using Graph Neural Networks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Easy access, variety of content, and fast widespread interactions are some of the reasons making social media increasingly popular. However, this rise has also enabled the propagation of fake news, text published by news sources with an intent to spread misinformation and sway beliefs. Detecting it is an important and challenging problem to prevent large scale misinformation and maintain a healthy society. We view fake news detection as reasoning over the relations between sources, articles they publish, and engaging users on social media in a graph framework. After embedding this information, we formulate inference operators which augment the graph edges by revealing unobserved interactions between its elements, such as similarity between documents\u2019 contents and users\u2019 engagement patterns. Our experiments over two challenging fake news detection tasks show that using inference operators leads to a better understanding of the social media framework enabling fake news spread, resulting in improved performance.",
        "author": "Nikhil Mehta; Maria Leonor Pacheco; Dan Goldwasser",
        "authorids": "/n/nikhil-mehta/; /m/maria-leonor-pacheco/; /d/dan-goldwasser/",
        "bibtex": "@inproceedings{mehta-etal-2022-tackling,\n    title = \"Tackling Fake News Detection by Continually Improving Social Context Representations using Graph Neural Networks\",\n    author = \"Mehta, Nikhil  and\n      Pacheco, Maria Leonor  and\n      Goldwasser, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.97/\",\n    doi = \"10.18653/v1/2022.acl-long.97\",\n    pages = \"1363--1380\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.97.pdf",
        "site": "https://aclanthology.org/2022.acl-long.97/",
        "pdf_size": 637251,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7047029593313851287&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, Purdue University; Department of Computer Science, Purdue University; Department of Computer Science, Purdue University",
        "aff_domain": "purdue.edu;purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.228",
        "title": "Tailor: Generating and Perturbing Text with Semantic Controls",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Controlled text perturbation is useful for evaluating and improving model generalizability. However, current techniques rely on training a model for every target perturbation, which is expensive and hard to generalize. We present Tailor, a semantically-controlled text generation system. Tailor builds on a pretrained seq2seq model and produces textual outputs conditioned on control codes derived from semantic representations. We craft a set of operations to modify the control codes, which in turn steer generation towards targeted attributes. These operations can be further composed into higher-level ones, allowing for flexible perturbation strategies. We demonstrate the effectiveness of these perturbations in multiple applications. First, we use Tailor to automatically create high-quality contrast sets for four distinct natural language processing (NLP) tasks. These contrast sets contain fewer spurious artifacts and are complementary to manually annotated ones in their lexical diversity. Second, we show that Tailor perturbations can improve model generalization through data augmentation. Perturbing just \u223c2% of training data leads to a 5.8-point gain on an NLI challenge set measuring reliance on syntactic heuristics.",
        "author": "Alexis Ross; Tongshuang Wu; Hao Peng; Matthew Peters; Matt Gardner",
        "authorids": "/a/alexis-ross/; /t/tongshuang-wu/; /h/hao-peng/; /m/matthew-e-peters/; /m/matt-gardner/",
        "bibtex": "@inproceedings{ross-etal-2022-tailor,\n    title = \"Tailor: Generating and Perturbing Text with Semantic Controls\",\n    author = \"Ross, Alexis  and\n      Wu, Tongshuang  and\n      Peng, Hao  and\n      Peters, Matthew  and\n      Gardner, Matt\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.228/\",\n    doi = \"10.18653/v1/2022.acl-long.228\",\n    pages = \"3194--3213\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.228.pdf",
        "site": "https://aclanthology.org/2022.acl-long.228/",
        "pdf_size": 343723,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13067065866519367484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Allen Institute for Artificial Intelligence, Seattle, WA, USA+Microsoft Semantic Machines, USA; Paul G. Allen School of Computer Science and Engineering, University of Washington; Paul G. Allen School of Computer Science and Engineering, University of Washington; Allen Institute for Artificial Intelligence, Seattle, WA, USA; Microsoft Semantic Machines, USA",
        "aff_domain": "allenai.org;cs.washington.edu;cs.washington.edu;allenai.org;microsoft.com",
        "email": "allenai.org;cs.washington.edu;cs.washington.edu;allenai.org;microsoft.com",
        "github": "https://github.com/allenai/tailor",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;2;0;1",
        "aff_unique_norm": "Allen Institute for Artificial Intelligence;Microsoft;University of Washington",
        "aff_unique_dep": ";Microsoft Semantic Machines;Paul G. Allen School of Computer Science and Engineering",
        "aff_unique_url": "https://allenai.org;https://www.microsoft.com;https://www.washington.edu",
        "aff_unique_abbr": "AI2;MSM;UW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.247",
        "title": "Task-guided Disentangled Tuning for Pretrained Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pretrained language models (PLMs) trained on large-scale unlabeled corpus are typically fine-tuned on task-specific downstream datasets, which have produced state-of-the-art results on various NLP tasks. However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in low data regime. To address this issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which enhances the generalization of representations by disentangling task-relevant signals from the entangled representations. For a given task, we introduce a learnable confidence model to detect indicative guidance from context, and further propose a disentangled regularization to mitigate the over-reliance problem. Experimental results on GLUE and CLUE benchmarks show that TDT gives consistently better results than fine-tuning with different PLMs, and extensive analysis demonstrates the effectiveness and robustness of our method. Code is available at https://github.com/lemon0830/TDT.",
        "author": "Jiali Zeng; Yufan Jiang; Shuangzhi Wu; Yongjing Yin; Mu Li",
        "authorids": "/j/jiali-zeng/; /y/yufan-jiang/; /s/shuangzhi-wu/; /y/yongjing-yin/; /m/mu-li/",
        "bibtex": "@inproceedings{zeng-etal-2022-task,\n    title = \"Task-guided Disentangled Tuning for Pretrained Language Models\",\n    author = \"Zeng, Jiali  and\n      Jiang, Yufan  and\n      Wu, Shuangzhi  and\n      Yin, Yongjing  and\n      Li, Mu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.247/\",\n    doi = \"10.18653/v1/2022.findings-acl.247\",\n    pages = \"3126--3137\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.247.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.247/",
        "pdf_size": 1194149,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7076830976215986476&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Tencent Cloud Xiaowei, Beijing, China; Tencent Cloud Xiaowei, Beijing, China; Tencent Cloud Xiaowei, Beijing, China; Zhejiang University, Westlake University, Zhejiang, China; Tencent Cloud Xiaowei, Beijing, China",
        "aff_domain": "tencent.com;tencent.com;tencent.com;westlake.edu.cn;tencent.com",
        "email": "tencent.com;tencent.com;tencent.com;westlake.edu.cn;tencent.com",
        "github": "https://github.com/lemon0830/TDT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Tencent;Zhejiang University",
        "aff_unique_dep": "Tencent Cloud Xiaowei;",
        "aff_unique_url": "https://cloud.tencent.com;http://www.zju.edu.cn",
        "aff_unique_abbr": "Tencent Cloud;ZJU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.125",
        "title": "TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire. To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework. Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. With the help of these two types of knowledge, our model can learn what and how to generate. Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.",
        "author": "Chao-Hong Tan; Jia-Chen Gu; Chongyang Tao; Zhen-Hua Ling; Can Xu; Huang Hu; Xiubo Geng; Daxin Jiang",
        "authorids": "/c/chao-hong-tan/; /j/jia-chen-gu/; /c/chongyang-tao/; /z/zhen-hua-ling/; /c/can-xu/; /h/huang-hu/; /x/xiubo-geng/; /d/daxin-jiang/",
        "bibtex": "@inproceedings{tan-etal-2022-tegtok,\n    title = \"{T}eg{T}ok: Augmenting Text Generation via Task-specific and Open-world Knowledge\",\n    author = \"Tan, Chao-Hong  and\n      Gu, Jia-Chen  and\n      Tao, Chongyang  and\n      Ling, Zhen-Hua  and\n      Xu, Can  and\n      Hu, Huang  and\n      Geng, Xiubo  and\n      Jiang, Daxin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.125/\",\n    doi = \"10.18653/v1/2022.findings-acl.125\",\n    pages = \"1597--1609\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.125.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.125/",
        "pdf_size": 472826,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3437610745641115929&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "National Engineering Research Center for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China+Microsoft, Beijing, China; National Engineering Research Center for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China+Microsoft, Beijing, China; Microsoft, Beijing, China; National Engineering Research Center for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; Microsoft, Beijing, China; Microsoft, Beijing, China; Microsoft, Beijing, China; Microsoft, Beijing, China",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;microsoft.com;ustc.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;microsoft.com;ustc.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;1;0;1;1;1;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft",
        "aff_unique_dep": "National Engineering Research Center for Speech and Language Information Processing;Microsoft",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com",
        "aff_unique_abbr": "USTC;MSFT",
        "aff_campus_unique_index": "0+1;0+1;1;0;1;1;1;1",
        "aff_campus_unique": "Hefei;Beijing",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.97",
        "title": "Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Before entering the neural network, a token needs to be converted to its one-hot representation, which is a discrete distribution of the vocabulary. Smoothed representation is the probability of candidate tokens obtained from the pre-trained masked language model, which can be seen as a more informative augmented substitution to the one-hot representation. We propose an efficient data augmentation method, dub as text smoothing, by converting a sentence from its one-hot representation to controllable smoothed representation. We evaluate text smoothing on different datasets in a low-resource regime. Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin. Moreover, text smoothing can be combined with these data augmentation methods to achieve better performance.",
        "author": "Xing Wu; Chaochen Gao; Meng Lin; Liangjun Zang; Songlin Hu",
        "authorids": "/x/xing-wu/; /c/chaochen-gao/; /m/meng-lin/; /l/liangjun-zang/; /s/songlin-hu/",
        "bibtex": "@inproceedings{wu-etal-2022-text,\n    title = \"Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks\",\n    author = \"Wu, Xing  and\n      Gao, Chaochen  and\n      Lin, Meng  and\n      Zang, Liangjun  and\n      Hu, Songlin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.97/\",\n    doi = \"10.18653/v1/2022.acl-short.97\",\n    pages = \"871--875\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.97.pdf",
        "site": "https://aclanthology.org/2022.acl-short.97/",
        "pdf_size": 247623,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2789588313320487612&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China+Kuaishou Technology, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "kuaishou.com;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "kuaishou.com;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "https://github.com/caskcsg/TextSmoothing",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Kuaishou Technology",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;https://www.kuaishou.com",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.593",
        "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) (CITATION) is the only prior work addressing the generative aspect of speech pre-training, which builds a text-free language model using discovered units. Unfortunately, because the units used in GSLM discard most prosodic information, GSLM fails to leverage prosody for better comprehension and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.",
        "author": "Eugene Kharitonov; Ann Lee; Adam Polyak; Yossi Adi; Jade Copet; Kushal Lakhotia; Tu Anh Nguyen; Morgane Riviere; Abdelrahman Mohamed; Emmanuel Dupoux; Wei-Ning Hsu",
        "authorids": "/e/eugene-kharitonov/; /a/ann-lee/; /a/adam-polyak/; /y/yossi-adi/; /j/jade-copet/; /k/kushal-lakhotia/; /t/tu-anh-nguyen/; /m/morgane-riviere/; /a/abdelrahman-mohamed/; /e/emmanuel-dupoux/; /w/wei-ning-hsu/",
        "bibtex": "@inproceedings{kharitonov-etal-2022-text,\n    title = \"Text-Free Prosody-Aware Generative Spoken Language Modeling\",\n    author = \"Kharitonov, Eugene  and\n      Lee, Ann  and\n      Polyak, Adam  and\n      Adi, Yossi  and\n      Copet, Jade  and\n      Lakhotia, Kushal  and\n      Nguyen, Tu Anh  and\n      Riviere, Morgane  and\n      Mohamed, Abdelrahman  and\n      Dupoux, Emmanuel  and\n      Hsu, Wei-Ning\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.593/\",\n    doi = \"10.18653/v1/2022.acl-long.593\",\n    pages = \"8666--8681\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.593.pdf",
        "site": "https://aclanthology.org/2022.acl-long.593/",
        "pdf_size": 825498,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6860409912627023988&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research",
        "aff_domain": "fb.com;fb.com; ; ; ; ; ; ; ; ;fb.com",
        "email": "fb.com;fb.com; ; ; ; ; ; ; ; ;fb.com",
        "github": "https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm",
        "project": "https://speechbot.github.io/pgslm/",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Facebook AI Research",
        "aff_unique_url": "https://research.facebook.com",
        "aff_unique_abbr": "FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.180",
        "title": "Text-to-Table: A New Way of Information Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study a new problem setting of information extraction (IE), referred to as text-to-table. In text-to-table, given a text, one creates a table or several tables expressing the main content of the text, while the model is learned from text-table pair data. The problem setting differs from those of the existing methods for IE. First, the extraction can be carried out from long texts to large tables with complex structures. Second, the extraction is entirely data-driven, and there is no need to explicitly define the schemas. As far as we know, there has been no previous work that studies the problem. In this work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem. We first employ a seq2seq model fine-tuned from a pre-trained language model to perform the task. We also develop a new method within the seq2seq approach, exploiting two additional techniques in table generation: table constraint and table relation embeddings. We consider text-to-table as an inverse problem of the well-studied table-to-text, and make use of four existing table-to-text datasets in our experiments on text-to-table. Experimental results show that the vanilla seq2seq model can outperform the baseline methods of using relation extraction and named entity extraction. The results also show that our method can further boost the performances of the vanilla seq2seq model. We further discuss the main challenges of the proposed task. The code and data are available at https://github.com/shirley-wu/text_to_table.",
        "author": "Xueqing Wu; Jiacheng Zhang; Hang Li",
        "authorids": "/x/xueqing-wu/; /j/jiacheng-zhang/; /h/hang-li/",
        "bibtex": "@inproceedings{wu-etal-2022-text-table,\n    title = \"Text-to-Table: A New Way of Information Extraction\",\n    author = \"Wu, Xueqing  and\n      Zhang, Jiacheng  and\n      Li, Hang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.180/\",\n    doi = \"10.18653/v1/2022.acl-long.180\",\n    pages = \"2518--2533\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.180.pdf",
        "site": "https://aclanthology.org/2022.acl-long.180/",
        "pdf_size": 2716309,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3485773252216787914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Urbana-Champaign1; ByteDance AI Lab2; ByteDance AI Lab2",
        "aff_domain": "illinois.edu;bytedance.com;bytedance.com",
        "email": "illinois.edu;bytedance.com;bytedance.com",
        "github": "https://github.com/shirley-wu/text_to_table",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;ByteDance",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://illinois.edu;https://www.bytedance.com",
        "aff_unique_abbr": "UIUC;ByteDance",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.acl-long.335",
        "title": "Textomics: A Dataset for Genomics Data Summary Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Summarizing biomedical discovery from genomics data using natural languages is an essential step in biomedical research but is mostly done manually. Here, we introduce Textomics, a novel dataset of genomics data description, which contains 22,273 pairs of genomics data matrices and their summaries. Each summary is written by the researchers who generated the data and associated with a scientific paper. Based on this dataset, we study two novel tasks: generating textual summary from a genomics data matrix and vice versa. Inspired by the successful applications of k nearest neighbors in modeling genomics data, we propose a kNN-Vec2Text model to address these tasks and observe substantial improvement on our dataset. We further illustrate how Textomics can be used to advance other applications, including evaluating scientific paper embeddings and generating masked templates for scientific paper understanding. Textomics serves as the first benchmark for generating textual summaries for genomics data and we envision it will be broadly applied to other biomedical and natural language processing applications.",
        "author": "Mu-Chun Wang; Zixuan Liu; Sheng Wang",
        "authorids": "/m/mu-chun-wang/; /z/zixuan-liu/; /s/sheng-wang/",
        "bibtex": "@inproceedings{wang-etal-2022-textomics,\n    title = \"Textomics: A Dataset for Genomics Data Summary Generation\",\n    author = \"Wang, Mu-Chun  and\n      Liu, Zixuan  and\n      Wang, Sheng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.335/\",\n    doi = \"10.18653/v1/2022.acl-long.335\",\n    pages = \"4878--4891\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.335.pdf",
        "site": "https://aclanthology.org/2022.acl-long.335/",
        "pdf_size": 6366579,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=27894751569560168&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Science and Technology of China; Paul G. Allen Scholl of Computer Science and Engineering, University of Washington; Paul G. Allen Scholl of Computer Science and Engineering, University of Washington",
        "aff_domain": "mail.ustc.edu.cn;cs.washington.edu;cs.washington.edu",
        "email": "mail.ustc.edu.cn;cs.washington.edu;cs.washington.edu",
        "github": "https://github.com/amos814/Textomics",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Science and Technology of China;University of Washington",
        "aff_unique_dep": ";Paul G. Allen School of Computer Science & Engineering",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.cs.washington.edu",
        "aff_unique_abbr": "USTC;UW",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.116",
        "title": "Thai Nested Named Entity Recognition Corpus",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper presents the first Thai Nested Named Entity Recognition (N-NER) dataset. Thai N-NER consists of 264,798 mentions, 104 classes, and a maximum depth of 8 layers obtained from 4,894 documents in the domains of news articles and restaurant reviews. Our work, to the best of our knowledge, presents the largest non-English N-NER dataset and the first non-English one with fine-grained classes. To understand the new challenges our proposed dataset brings to the field, we conduct an experimental study on (i) cutting edge N-NER models with the state-of-the-art accuracy in English and (ii) baseline methods based on well-known language model architectures. From the experimental results, we obtained two key findings. First, all models produced poor F1 scores in the tail region of the class distribution. There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai dataset. These findings suggest that further investigation is required to make a multilingual N-NER solution that works well across different languages.",
        "author": "Weerayut Buaphet; Can Udomcharoenchaikit; Peerat Limkonchotiwat; Attapol Rutherford; Sarana Nutanong",
        "authorids": "/w/weerayut-buaphet/; /c/can-udomcharoenchaikit/; /p/peerat-limkonchotiwat/; /a/attapol-rutherford/; /s/sarana-nutanong/",
        "bibtex": "@inproceedings{buaphet-etal-2022-thai,\n    title = \"{T}hai Nested Named Entity Recognition Corpus\",\n    author = \"Buaphet, Weerayut  and\n      Udomcharoenchaikit, Can  and\n      Limkonchotiwat, Peerat  and\n      Rutherford, Attapol  and\n      Nutanong, Sarana\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.116/\",\n    doi = \"10.18653/v1/2022.findings-acl.116\",\n    pages = \"1473--1486\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.116.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.116/",
        "pdf_size": 1260130,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12837238284680927526&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "School of Information Science and Technology, VISTEC, Thailand; School of Information Science and Technology, VISTEC, Thailand; School of Information Science and Technology, VISTEC, Thailand; Department of Linguistic, Chulalongkorn University, Thailand; School of Information Science and Technology, VISTEC, Thailand",
        "aff_domain": "vistec.ac.th;vistec.ac.th;vistec.ac.th;chula.ac.th;vistec.ac.th",
        "email": "vistec.ac.th;vistec.ac.th;vistec.ac.th;chula.ac.th;vistec.ac.th",
        "github": "github.com/vistec-AI/Thai-NNER.git",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "VISTEC;Chulalongkorn University",
        "aff_unique_dep": "School of Information Science and Technology;Department of Linguistic",
        "aff_unique_url": "https://www.vistec.ac.th;http://www.chula.ac.th",
        "aff_unique_abbr": "VISTEC;Chula",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Thailand"
    },
    {
        "id": "2022.acl-long.540",
        "title": "That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The evolution of language follows the rule of gradual change. Grammar, vocabulary, and lexical semantic shifts take place over time, resulting in a diachronic linguistic gap. As such, a considerable amount of texts are written in languages of different eras, which creates obstacles for natural language processing tasks, such as word segmentation and machine translation. Although the Chinese language has a long history, previous Chinese natural language processing research has primarily focused on tasks within a specific era. Therefore, we propose a cross-era learning framework for Chinese word segmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to incorporate era-specific linguistic knowledge. Experiments on four corpora from different eras show that the performance of each corpus significantly improves. Further analyses also demonstrate that the SM can effectively integrate the knowledge of the eras into the neural network.",
        "author": "Xuemei Tang; Qi Su",
        "authorids": "/x/xuemei-tang/; /q/qi-su/",
        "bibtex": "@inproceedings{tang-su-2022-slepen,\n    title = \"That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory\",\n    author = \"Tang, Xuemei  and\n      Su, Qi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.540/\",\n    doi = \"10.18653/v1/2022.acl-long.540\",\n    pages = \"7830--7840\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.540.pdf",
        "site": "https://aclanthology.org/2022.acl-long.540/",
        "pdf_size": 865378,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8441839643820999619&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Information Management, Peking University, Bejing, China+Digital Humanities Center of Peking University, Bejing, China+School of Foreign Languages, Peking University, Bejing, China+MOE Key Lab of Computational Linguistics, School of EECS, Peking University, Bejing, China; Digital Humanities Center of Peking University, Bejing, China+School of Foreign Languages, Peking University, Bejing, China+MOE Key Lab of Computational Linguistics, School of EECS, Peking University, Bejing, China",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0+0+0;0+0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Department of Information Management",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "Peking U",
        "aff_campus_unique_index": "0+0+0+0;0+0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.458",
        "title": "The AI Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Task-oriented dialogue systems are increasingly prevalent in healthcare settings, and have been characterized by a diverse range of architectures and objectives. Although these systems have been surveyed in the medical community from a non-technical perspective, a systematic review from a rigorous computational perspective has to date remained noticeably absent. As a result, many important implementation details of healthcare-oriented dialogue systems remain limited or underspecified, slowing the pace of innovation in this area. To fill this gap, we investigated an initial pool of 4070 papers from well-known computer science, natural language processing, and artificial intelligence venues, identifying 70 papers discussing the system-level implementation of task-oriented dialogue systems for healthcare applications. We conducted a comprehensive technical review of these papers, and present our key findings including identified gaps and corresponding recommendations.",
        "author": "Mina Valizadeh; Natalie Parde",
        "authorids": "/m/mina-valizadeh/; /n/natalie-parde/",
        "bibtex": "@inproceedings{valizadeh-parde-2022-ai,\n    title = \"The {AI} Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications\",\n    author = \"Valizadeh, Mina  and\n      Parde, Natalie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.458/\",\n    doi = \"10.18653/v1/2022.acl-long.458\",\n    pages = \"6638--6660\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.458.pdf",
        "site": "https://aclanthology.org/2022.acl-long.458/",
        "pdf_size": 950893,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16746305158520716540&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Natural Language Processing Laboratory, Department of Computer Science, University of Illinois at Chicago; Natural Language Processing Laboratory, Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "uic.edu;uic.edu",
        "email": "uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.68",
        "title": "The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Discourse analysis allows us to attain inferences of a text document that extend beyond the sentence-level. The current performance of discourse models is very low on texts outside of the training distribution\u2019s coverage, diminishing the practical utility of existing models. There is need for a measure that can inform us to what extent our model generalizes from the training to the test sample when these samples may be drawn from distinct distributions. While this can be estimated via distribution shift, we argue that this does not directly correlate with change in the observed error of a classifier (i.e. error-gap). Thus, we propose to use a statistic from the theoretical domain adaptation literature which can be directly tied to error-gap. We study the bias of this statistic as an estimator of error-gap both theoretically and through a large-scale empirical study of over 2400 experiments on 6 discourse datasets from domains including, but not limited to: news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not only motivate our proposal and help us to understand its limitations, but also provide insight on the properties of discourse models and datasets which improve performance in domain adaptation. For instance, we find that non-news datasets are slightly easier to transfer to than news datasets when the training and test sets are very different. Our code and an associated Python package are available to allow practitioners to make more informed model and dataset choices.",
        "author": "Katherine Atwell; Anthony Sicilia; Seong Jae Hwang; Malihe Alikhani",
        "authorids": "/k/katherine-atwell/; /a/anthony-sicilia/; /s/seong-jae-hwang/; /m/malihe-alikhani/",
        "bibtex": "@inproceedings{atwell-etal-2022-change,\n    title = \"The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error\",\n    author = \"Atwell, Katherine  and\n      Sicilia, Anthony  and\n      Hwang, Seong Jae  and\n      Alikhani, Malihe\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.68/\",\n    doi = \"10.18653/v1/2022.findings-acl.68\",\n    pages = \"824--845\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.68.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.68/",
        "pdf_size": 815581,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5281279264131686446&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science+Intelligent Systems Program, University of Pittsburgh; Department of Computer Science+Intelligent Systems Program, University of Pittsburgh; Department of Artificial Intelligence, Yonsei University; Department of Computer Science+Intelligent Systems Program, University of Pittsburgh",
        "aff_domain": "pitt.edu;pitt.edu;yonsei.ac.kr;pitt.edu",
        "email": "pitt.edu;pitt.edu;yonsei.ac.kr;pitt.edu",
        "github": "https://github.com/anthonysicilia/change-that-matters-ACL2022",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;2;0+1",
        "aff_unique_norm": "Unknown Institution;University of Pittsburgh;Yonsei University",
        "aff_unique_dep": "Department of Computer Science;Intelligent Systems Program;Department of Artificial Intelligence",
        "aff_unique_url": ";https://www.pitt.edu;https://www.yonsei.ac.kr",
        "aff_unique_abbr": ";Pitt;Yonsei",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;2;1",
        "aff_country_unique": ";United States;South Korea"
    },
    {
        "id": "2022.acl-long.516",
        "title": "The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field\u2019s successes, often in response to the field\u2019s widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.",
        "author": "Samuel Bowman",
        "authorids": "/s/samuel-bowman/",
        "bibtex": "@inproceedings{bowman-2022-dangers,\n    title = \"The Dangers of Underclaiming: Reasons for Caution When Reporting How {NLP} Systems Fail\",\n    author = \"Bowman, Samuel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.516/\",\n    doi = \"10.18653/v1/2022.acl-long.516\",\n    pages = \"7484--7499\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.516.pdf",
        "site": "https://aclanthology.org/2022.acl-long.516/",
        "pdf_size": 343366,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14113680655824585152&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "New York University",
        "aff_domain": "nyu.edu",
        "email": "nyu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.568",
        "title": "The Grammar-Learning Trajectories of Neural Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired. Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
        "author": "Leshem Choshen; Guy Hacohen; Daphna Weinshall; Omri Abend",
        "authorids": "/l/leshem-choshen/; /g/guy-hacohen/; /d/daphna-weinshall/; /o/omri-abend/",
        "bibtex": "@inproceedings{choshen-etal-2022-grammar,\n    title = \"The Grammar-Learning Trajectories of Neural Language Models\",\n    author = \"Choshen, Leshem  and\n      Hacohen, Guy  and\n      Weinshall, Daphna  and\n      Abend, Omri\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.568/\",\n    doi = \"10.18653/v1/2022.acl-long.568\",\n    pages = \"8281--8297\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.568.pdf",
        "site": "https://aclanthology.org/2022.acl-long.568/",
        "pdf_size": 1417571,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=971497875030334223&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science\u2020; Department of Computer Science\u2020 + Department of Brain Sciences\u2021; Department of Computer Science\u2020; Department of Computer Science\u2020",
        "aff_domain": "mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il",
        "email": "mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il",
        "github": "https://github.com/borgr/ordert8281",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "University Affiliation Not Specified;University College London",
        "aff_unique_dep": "Department of Computer Science;Department of Brain Sciences",
        "aff_unique_url": ";https://www.ucl.ac.uk/brain-sciences",
        "aff_unique_abbr": ";UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "2022.findings-acl.249",
        "title": "The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Language models are increasingly becoming popular in AI-powered scientific IR systems. This paper evaluates popular scientific language models in handling (i) short-query texts and (ii) textual neighbors. Our experiments showcase the inability to retrieve relevant documents for a short-query text even under the most relaxed conditions. Additionally, we leverage textual neighbors, generated by small perturbations to the original text, to demonstrate that not all perturbations lead to close neighbors in the embedding space. Further, an exhaustive categorization yields several classes of orthographically and semantically related, partially related and completely unrelated neighbors. Retrieval performance turns out to be more influenced by the surface form rather than the semantics of the text.",
        "author": "Shruti Singh; Mayank Singh",
        "authorids": "/s/shruti-singh/; /m/mayank-singh/",
        "bibtex": "@inproceedings{singh-singh-2022-inefficiency,\n    title = \"The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through\",\n    author = \"Singh, Shruti  and\n      Singh, Mayank\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.249/\",\n    doi = \"10.18653/v1/2022.findings-acl.249\",\n    pages = \"3153--3173\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.249.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.249/",
        "pdf_size": 7672165,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12527486654684431265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Gandhinagar, Gujarat, India; Department of Computer Science and Engineering, Indian Institute of Technology Gandhinagar, Gujarat, India",
        "aff_domain": "iitgn.ac.in;iitgn.ac.in",
        "email": "iitgn.ac.in;iitgn.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Gandhinagar",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitgn.ac.in",
        "aff_unique_abbr": "IITGN",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Gandhinagar",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-long.601",
        "title": "The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "An audience\u2019s prior beliefs and morals are strong indicators of how likely they will be affected by a given argument. Utilizing such knowledge can help focus on shared values to bring disagreeing parties towards agreement. In argumentation technology, however, this is barely exploited so far. This paper studies the feasibility of automatically generating morally framed arguments as well as their effect on different audiences. Following the moral foundation theory, we propose a system that effectively generates arguments focusing on different morals. In an in-depth user study, we ask liberals and conservatives to evaluate the impact of these arguments. Our results suggest that, particularly when prior beliefs are challenged, an audience becomes more affected by morally framed arguments.",
        "author": "Milad Alshomary; Roxanne El Baff; Timon Gurcke; Henning Wachsmuth",
        "authorids": "/m/milad-alshomary/; /r/roxanne-el-baff/; /t/timon-gurcke/; /h/henning-wachsmuth/",
        "bibtex": "@inproceedings{alshomary-etal-2022-moral,\n    title = \"The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments\",\n    author = \"Alshomary, Milad  and\n      El Baff, Roxanne  and\n      Gurcke, Timon  and\n      Wachsmuth, Henning\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.601/\",\n    doi = \"10.18653/v1/2022.acl-long.601\",\n    pages = \"8782--8797\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.601.pdf",
        "site": "https://aclanthology.org/2022.acl-long.601/",
        "pdf_size": 577911,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13451010555843570492&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 7,
        "aff": "Paderborn University, Paderborn, Germany; German Aerospace Center, Oberpfaffenhofen, Germany; Paderborn University, Paderborn, Germany; Paderborn University, Paderborn, Germany",
        "aff_domain": "upb.de;dlr.de; ; ",
        "email": "upb.de;dlr.de; ; ",
        "github": "",
        "project": "https://early-access-program.debater.res.ibm.com/8782",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Paderborn University;German Aerospace Center",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-paderborn.de;https://www.dlr.de",
        "aff_unique_abbr": "UPB;DLR",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Paderborn;Oberpfaffenhofen",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.acl-long.261",
        "title": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user\u2019s trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot\u2019s reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models\u2019 implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic",
        "author": "Caleb Ziems; Jane Yu; Yi-Chia Wang; Alon Halevy; Diyi Yang",
        "authorids": "/c/caleb-ziems/; /j/jane-yu/; /y/yi-chia-wang/; /a/alon-halevy/; /d/diyi-yang/",
        "bibtex": "@inproceedings{ziems-etal-2022-moral,\n    title = \"The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems\",\n    author = \"Ziems, Caleb  and\n      Yu, Jane  and\n      Wang, Yi-Chia  and\n      Halevy, Alon  and\n      Yang, Diyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.261/\",\n    doi = \"10.18653/v1/2022.acl-long.261\",\n    pages = \"3755--3773\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.261.pdf",
        "site": "https://aclanthology.org/2022.acl-long.261/",
        "pdf_size": 2214846,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11966517505958815506&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Georgia Institute of Technology; Meta AI Research + Georgia Institute of Technology; Meta AI Research; Meta AI Research; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;fb.com;fb.com;fb.com;gatech.edu",
        "email": "gatech.edu;fb.com;fb.com;fb.com;gatech.edu",
        "github": "https://github.com/GT-SALT/mic",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+0;1;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Meta",
        "aff_unique_dep": ";Meta AI Research",
        "aff_unique_url": "https://www.gatech.edu;https://meta.com",
        "aff_unique_abbr": "Georgia Tech;Meta AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.286",
        "title": "The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Obtaining human-like performance in NLP is often argued to require compositional generalisation. Whether neural networks exhibit this ability is usually studied by training models on highly compositional synthetic data. However, compositionality in natural language is much more complex than the rigid, arithmetic-like version such data adheres to, and artificial compositionality tests thus do not allow us to determine how neural models deal with more realistic forms of compositionality. In this work, we re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT).Our results highlight that: i) unfavourably, models trained on more data are more compositional; ii) models are sometimes less compositional than expected, but sometimes more, exemplifying that different levels of compositionality are required, and models are not always able to modulate between them correctly; iii) some of the non-compositional behaviours are mistakes, whereas others reflect the natural variation in data. Apart from an empirical study, our work is a call to action: we should rethink the evaluation of compositionality in neural networks and develop benchmarks using real data to evaluate compositionality on natural language, where composing meaning is not as straightforward as doing the math.",
        "author": "Verna Dankers; Elia Bruni; Dieuwke Hupkes",
        "authorids": "/v/verna-dankers/; /e/elia-bruni/; /d/dieuwke-hupkes/",
        "bibtex": "@inproceedings{dankers-etal-2022-paradox,\n    title = \"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study\",\n    author = \"Dankers, Verna  and\n      Bruni, Elia  and\n      Hupkes, Dieuwke\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.286/\",\n    doi = \"10.18653/v1/2022.acl-long.286\",\n    pages = \"4154--4175\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.286.pdf",
        "site": "https://aclanthology.org/2022.acl-long.286/",
        "pdf_size": 743363,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14100737027080225872&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ILCC, University of Edinburgh; University of Osnabr\u00fcck; Facebook AI Research",
        "aff_domain": "gmail.com;gmail.com;fb.com",
        "email": "gmail.com;gmail.com;fb.com",
        "github": "https://github.com/i-machine-think/compositionality_paradox_mt",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Edinburgh;University of Osnabr\u00fcck;Meta",
        "aff_unique_dep": "ILCC;;Facebook AI Research",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.uni-osnabrueck.de;https://research.facebook.com",
        "aff_unique_abbr": "Edinburgh;UOS;FAIR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Edinburgh;",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "United Kingdom;Germany;United States"
    },
    {
        "id": "2022.findings-acl.252",
        "title": "The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for Chinese Spell Checking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors, which are mainly caused by the phonological or visual similarity. Recently, pre-trained language models (PLMs) promote the progress of CSC task. However, there exists a gap between the learned knowledge of PLMs and the goal of CSC task. PLMs focus on the semantics in text and tend to correct the erroneous characters to semantically proper or commonly used ones, but these aren\u2019t the ground-truth corrections. To address this issue, we propose an Error-driven COntrastive Probability Optimization (ECOPO) framework for CSC task. ECOPO refines the knowledge representations of PLMs, and guides the model to avoid predicting these common characters through an error-driven way. Particularly, ECOPO is model-agnostic and it can be combined with existing CSC methods to achieve better performance. Extensive experiments and detailed analyses on SIGHAN datasets demonstrate that ECOPO is simple yet effective.",
        "author": "Yinghui Li; Qingyu Zhou; Yangning Li; Zhongli Li; Ruiyang Liu; Rongyi Sun; Zizhen Wang; Chao Li; Yunbo Cao; Hai-Tao Zheng",
        "authorids": "/y/yinghui-li/; /q/qingyu-zhou/; /y/yangning-li/; /z/zhongli-li/; /r/ruiyang-liu/; /r/rongyi-sun/; /z/zizhen-wang/; /c/chao-li/; /y/yunbo-cao/; /h/hai-tao-zheng/",
        "bibtex": "@inproceedings{li-etal-2022-past,\n    title = \"The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for {C}hinese Spell Checking\",\n    author = \"Li, Yinghui  and\n      Zhou, Qingyu  and\n      Li, Yangning  and\n      Li, Zhongli  and\n      Liu, Ruiyang  and\n      Sun, Rongyi  and\n      Wang, Zizhen  and\n      Li, Chao  and\n      Cao, Yunbo  and\n      Zheng, Hai-Tao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.252/\",\n    doi = \"10.18653/v1/2022.findings-acl.252\",\n    pages = \"3202--3213\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.252.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.252/",
        "pdf_size": 1563675,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9807509832476547139&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Tsinghua Shenzhen International Graduate School, Tsinghua University; Tencent Cloud Xiaowei; Tsinghua Shenzhen International Graduate School, Tsinghua University; Tencent Cloud Xiaowei; Department of Computer Science and Technology, Tsinghua University; Tsinghua Shenzhen International Graduate School, Tsinghua University; Tencent Cloud Xiaowei; Tencent Cloud Xiaowei; Tencent Cloud Xiaowei; Tsinghua Shenzhen International Graduate School, Tsinghua University+Peng Cheng Laboratory",
        "aff_domain": "mails.tsinghua.edu.cn;tencent.com; ; ; ; ; ; ; ;sz.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;tencent.com; ; ; ; ; ; ; ;sz.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;0;1;0;0;1;1;1;0+2",
        "aff_unique_norm": "Tsinghua University;Tencent;Pengcheng Laboratory",
        "aff_unique_dep": "International Graduate School;Tencent Cloud Xiaowei;Peng Cheng Laboratory",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://cloud.tencent.com;http://www.pcl.ac.cn",
        "aff_unique_abbr": "THU;Tencent;PCL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.17",
        "title": "The Power of Prompt Tuning for Low-Resource Semantic Parsing",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing\u2014the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution.",
        "author": "Nathan Schucher; Siva Reddy; Harm de Vries",
        "authorids": "/n/nathan-schucher/; /s/siva-reddy/; /h/harm-de-vries/",
        "bibtex": "@inproceedings{schucher-etal-2022-power,\n    title = \"The Power of Prompt Tuning for Low-Resource Semantic Parsing\",\n    author = \"Schucher, Nathan  and\n      Reddy, Siva  and\n      de Vries, Harm\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.17/\",\n    doi = \"10.18653/v1/2022.acl-short.17\",\n    pages = \"148--156\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.17.pdf",
        "site": "https://aclanthology.org/2022.acl-short.17/",
        "pdf_size": 761147,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7817764869132244735&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "ServiceNow Research + Mila/McGill University; Mila/McGill University + Facebook CIFAR AI Chair; ServiceNow Research",
        "aff_domain": "servicenow.com;mila.quebec;servicenow.com",
        "email": "servicenow.com;mila.quebec;servicenow.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1+2;0",
        "aff_unique_norm": "ServiceNow;McGill University;Meta",
        "aff_unique_dep": "Research;Mila;Facebook CIFAR AI",
        "aff_unique_url": "https://www.servicenow.com;https://www.mcgill.ca;https://www.facebook.com",
        "aff_unique_abbr": "ServiceNow;McGill;FB",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1+0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2022.acl-long.264",
        "title": "The Trade-offs of Domain Adaptation for Neural Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This work connects language model adaptation with concepts of machine learning theory. We consider a training setup with a large out-of-domain set and a small in-domain set. We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions. We analyze how out-of-domain pre-training before in-domain fine-tuning achieves better generalization than either solution independently. Finally, we present how adaptation techniques based on data selection, such as importance sampling, intelligent data selection and influence functions, can be presented in a common framework which highlights their similarity and also their subtle differences.",
        "author": "David Grangier; Dan Iter",
        "authorids": "/d/david-grangier/; /d/dan-iter/",
        "bibtex": "@inproceedings{grangier-iter-2022-trade,\n    title = \"The Trade-offs of Domain Adaptation for Neural Language Models\",\n    author = \"Grangier, David  and\n      Iter, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.264/\",\n    doi = \"10.18653/v1/2022.acl-long.264\",\n    pages = \"3802--3813\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.264.pdf",
        "site": "https://aclanthology.org/2022.acl-long.264/",
        "pdf_size": 324334,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6394897304953600891&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Google, Mountain View, CA; Stanford, Palo Alto, CA",
        "aff_domain": "google.com;stanford.edu",
        "email": "google.com;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;Stanford University",
        "aff_unique_dep": "Google;",
        "aff_unique_url": "https://www.google.com;https://www.stanford.edu",
        "aff_unique_abbr": "Google;Stanford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Mountain View;Palo Alto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.173",
        "title": "The impact of lexical and grammatical processing on generating code from natural language",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Considering the seq2seq architecture of Yin and Neubig (2018) for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. To study the impact of these components, we use a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided. The paper highlights the importance of the lexical substitution component in the current natural language to code systems.",
        "author": "Nathana\u00ebl Beau; Benoit Crabb\u00e9",
        "authorids": "/n/nathanael-beau/; /b/benoit-crabbe/",
        "bibtex": "@inproceedings{beau-crabbe-2022-impact,\n    title = \"The impact of lexical and grammatical processing on generating code from natural language\",\n    author = {Beau, Nathana{\\\"e}l  and\n      Crabb{\\'e}, Benoit},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.173/\",\n    doi = \"10.18653/v1/2022.findings-acl.173\",\n    pages = \"2204--2214\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.173.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.173/",
        "pdf_size": 478570,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1156757314664949128&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Universit\u00e9 de Paris, LLF, CNRS, 75013 Paris, France + onepoint, 29 rue des Sablons, F-75116 Paris, France; Universit\u00e9 de Paris, LLF, CNRS, 75013 Paris, France",
        "aff_domain": "groupeonepoint.com;u-paris.fr",
        "email": "groupeonepoint.com;u-paris.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Universit\u00e9 de Paris;onepoint",
        "aff_unique_dep": "LLF;",
        "aff_unique_url": "https://www.universitedeparis.fr;",
        "aff_unique_abbr": "UP;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2022.acl-long.350",
        "title": "The patient is more dead than alive: exploring the current state of the multi-document summarisation of the biomedical literature",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although multi-document summarisation (MDS) of the biomedical literature is a highly valuable task that has recently attracted substantial interest, evaluation of the quality of biomedical summaries lacks consistency and transparency. In this paper, we examine the summaries generated by two current models in order to understand the deficiencies of existing evaluation approaches in the context of the challenges that arise in the MDS task. Based on this analysis, we propose a new approach to human evaluation and identify several challenges that must be overcome to develop effective biomedical MDS systems.",
        "author": "Yulia Otmakhova; Karin Verspoor; Timothy Baldwin; Jey Han Lau",
        "authorids": "/j/julia-otmakhova/; /k/karin-verspoor/; /t/timothy-baldwin/; /j/jey-han-lau/",
        "bibtex": "@inproceedings{otmakhova-etal-2022-patient,\n    title = \"The patient is more dead than alive: exploring the current state of the multi-document summarisation of the biomedical literature\",\n    author = \"Otmakhova, Yulia  and\n      Verspoor, Karin  and\n      Baldwin, Timothy  and\n      Lau, Jey Han\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.350/\",\n    doi = \"10.18653/v1/2022.acl-long.350\",\n    pages = \"5098--5111\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.350.pdf",
        "site": "https://aclanthology.org/2022.acl-long.350/",
        "pdf_size": 679120,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13042299487190045926&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Melbourne; RMIT University; The University of Melbourne+MBZUAI; The University of Melbourne",
        "aff_domain": "student.unimelb.edu.au;rmit.edu.au;ldwin.net;gmail.com",
        "email": "student.unimelb.edu.au;rmit.edu.au;ldwin.net;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0+2;0",
        "aff_unique_norm": "University of Melbourne;RMIT University;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.rmit.edu.au;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "UniMelb;RMIT;MBZUAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "Australia;United Arab Emirates"
    },
    {
        "id": "2022.acl-long.270",
        "title": "There Are a Thousand Hamlets in a Thousand People\u2019s Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge-grounded conversation (KGC) shows great potential in building an engaging and knowledgeable chatbot, and knowledge selection is a key ingredient in it. However, previous methods for knowledge selection only concentrate on the relevance between knowledge and dialogue context, ignoring the fact that age, hobby, education and life experience of an interlocutor have a major effect on his or her personal preference over external knowledge. Without taking the personalization issue into account, it is difficult for existing dialogue systems to select the proper knowledge and generate persona-consistent responses. In this work, we introduce personal memory into knowledge selection in KGC to address the personalization issue. We propose a variational method to model the underlying relationship between one\u2019s personal memory and his or her selection of knowledge, and devise a learning scheme in which the forward mapping from personal memory to knowledge and its inverse mapping is included in a closed loop so that they could teach each other. Experiment results show that our methods outperform existing KGC methods significantly on both automatic evaluation and human evaluation.",
        "author": "Tingchen Fu; Xueliang Zhao; Chongyang Tao; Ji-Rong Wen; Rui Yan",
        "authorids": "/t/tingchen-fu/; /x/xueliang-zhao/; /c/chongyang-tao/; /j/ji-rong-wen/; /r/rui-yan/",
        "bibtex": "@inproceedings{fu-etal-2022-thousand,\n    title = \"There Are a Thousand Hamlets in a Thousand People`s Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory\",\n    author = \"Fu, Tingchen  and\n      Zhao, Xueliang  and\n      Tao, Chongyang  and\n      Wen, Ji-Rong  and\n      Yan, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.270/\",\n    doi = \"10.18653/v1/2022.acl-long.270\",\n    pages = \"3901--3913\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.270.pdf",
        "site": "https://aclanthology.org/2022.acl-long.270/",
        "pdf_size": 827604,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1620833118980798460&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; Wangxuan Institute of Computer Technology, Peking University; Microsoft Corporation; Gaoling School of Artificial Intelligence, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China",
        "aff_domain": "gmail.com;gmail.com;gmail.com;ruc.edu.cn;ruc.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;ruc.edu.cn;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "Renmin University of China;Peking University;Microsoft",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;Wangxuan Institute of Computer Technology;Microsoft Corporation",
        "aff_unique_url": "http://www.ruc.edu.cn;http://www.pku.edu.cn;https://www.microsoft.com",
        "aff_unique_abbr": "RUC;PKU;Microsoft",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.81",
        "title": "There\u2019s a Time and Place for Reasoning Beyond the Image",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Images are often more significant than only the pixels to human eyes, as we can infer, associate, and reason with contextual information from other sources to establish a more complete picture. For example, in Figure 1, we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs, the buildings, the crowds, and more. This reasoning could provide the time and place the image was taken, which will help us in subsequent tasks, such as automatic storyline construction, correction of image source in intended effect photographs, and upper-stream processing such as image clustering for certain location or time. In this work, we formulate this problem and introduce TARA: a dataset with 16k images with their associated news, time, and location, automatically extracted from New York Times, and an additional 61k examples as distant supervision from WIT. On top of the extractions, we present a crowdsourced subset in which we believe it is possible to find the images\u2019 spatio-temporal information for evaluation purpose. We show that there exists a 70% gap between a state-of-the-art joint model and human performance, which is slightly filled by our proposed model that uses segment-wise reasoning, motivating higher-level vision-language joint models that can conduct open-ended reasoning with world knowledge. The data and code are publicly available at https://github.com/zeyofu/TARA.",
        "author": "Xingyu Fu; Ben Zhou; Ishaan Chandratreya; Carl Vondrick; Dan Roth",
        "authorids": "/x/xingyu-fu/; /b/ben-zhou/; /i/ishaan-chandratreya/; /c/carl-vondrick/; /d/dan-roth/",
        "bibtex": "@inproceedings{fu-etal-2022-theres,\n    title = \"There`s a Time and Place for Reasoning Beyond the Image\",\n    author = \"Fu, Xingyu  and\n      Zhou, Ben  and\n      Chandratreya, Ishaan  and\n      Vondrick, Carl  and\n      Roth, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.81/\",\n    doi = \"10.18653/v1/2022.acl-long.81\",\n    pages = \"1138--1149\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.81.pdf",
        "site": "https://aclanthology.org/2022.acl-long.81/",
        "pdf_size": 39293138,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12537552347304152335&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Pennsylvania; University of Pennsylvania; Columbia University; Columbia University; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;columbia.edu;columbia.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;columbia.edu;columbia.edu;seas.upenn.edu",
        "github": "https://github.com/zeyofu/TARA",
        "project": "https://developer.nytimes.com/docs/archive-product/1/overview",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "University of Pennsylvania;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UPenn;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.168",
        "title": "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Spatial commonsense, the knowledge about spatial position and relationship between objects (like the relative size of a lion and a girl, and the position of a boy relative to a bicycle when cycling), is an important part of commonsense knowledge. Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning. Starting from the observation that images are more likely to exhibit spatial commonsense than texts, we explore whether models with visual signals learn more spatial commonsense than text-based PLMs. We propose a spatial commonsense benchmark that focuses on the relative scales of objects, and the positional relationship between people and objects under different actions. We probe PLMs and models with visual signals, including vision-language pretrained models and image synthesis models, on this benchmark, and find that image synthesis models are more capable of learning accurate and consistent spatial knowledge than other models. The spatial knowledge from image synthesis models also helps in natural language understanding tasks that require spatial commonsense.",
        "author": "Xiao Liu; Da Yin; Yansong Feng; Dongyan Zhao",
        "authorids": "/x/xiao-liu/; /d/da-yin/; /y/yansong-feng/; /d/dongyan-zhao/",
        "bibtex": "@inproceedings{liu-etal-2022-things,\n    title = \"Things not Written in Text: Exploring Spatial Commonsense from Visual Signals\",\n    author = \"Liu, Xiao  and\n      Yin, Da  and\n      Feng, Yansong  and\n      Zhao, Dongyan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.168/\",\n    doi = \"10.18653/v1/2022.acl-long.168\",\n    pages = \"2365--2376\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.168.pdf",
        "site": "https://aclanthology.org/2022.acl-long.168/",
        "pdf_size": 1956764,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13316334486143556545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Wangxuan Institute of Computer Technology, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University + Artificial Intelligence Institute of Peking University + State Key Laboratory of Media Convergence Production Technology and Systems; Computer Science Department, University of California, Los Angeles; The MOE Key Laboratory of Computational Linguistics, Peking University; Artificial Intelligence Institute of Peking University + State Key Laboratory of Media Convergence Production Technology and Systems",
        "aff_domain": "pku.edu.cn;cs.ucla.edu;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;cs.ucla.edu;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/xxxiaol/spatial-commonsense",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+0+1;2;0;0+1",
        "aff_unique_norm": "Peking University;State Key Laboratory of Media Convergence Production Technology and Systems;University of California, Los Angeles",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;;Computer Science Department",
        "aff_unique_url": "http://www.pku.edu.cn;;https://www.ucla.edu",
        "aff_unique_abbr": "PKU;;UCLA",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0+0+0+0;1;0;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.88",
        "title": "Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Implicit knowledge, such as common sense, is key to fluid human conversations. Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge. In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak). We argue that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models. We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and transition between knowledge and dialogues. Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative, specific, and commonsense-following responses, as evaluated by human annotators. TBS also generates knowledge that makes sense and is relevant to the dialogue around 85% of the time",
        "author": "Pei Zhou; Karthik Gopalakrishnan; Behnam Hedayatnia; Seokhwan Kim; Jay Pujara; Xiang Ren; Yang Liu; Dilek Hakkani-Tur",
        "authorids": "/p/pei-zhou/; /k/karthik-gopalakrishnan/; /b/behnam-hedayatnia/; /s/seokhwan-kim/; /j/jay-pujara/; /x/xiang-ren/; /y/yang-liu-icsi/; /d/dilek-hakkani-tur/",
        "bibtex": "@inproceedings{zhou-etal-2022-think,\n    title = \"Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation\",\n    author = \"Zhou, Pei  and\n      Gopalakrishnan, Karthik  and\n      Hedayatnia, Behnam  and\n      Kim, Seokhwan  and\n      Pujara, Jay  and\n      Ren, Xiang  and\n      Liu, Yang  and\n      Hakkani-Tur, Dilek\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.88/\",\n    doi = \"10.18653/v1/2022.acl-long.88\",\n    pages = \"1237--1252\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.88.pdf",
        "site": "https://aclanthology.org/2022.acl-long.88/",
        "pdf_size": 1145958,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5156776188251816977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Southern California; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "usc.edu;amazon.com;amazon.com;amazon.com;usc.edu;usc.edu;amazon.com;amazon.com",
        "email": "usc.edu;amazon.com;amazon.com;amazon.com;usc.edu;usc.edu;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;0;0;1;1",
        "aff_unique_norm": "University of Southern California;Amazon",
        "aff_unique_dep": "Department of Computer Science;Amazon Alexa AI",
        "aff_unique_url": "https://www.usc.edu;https://www.amazon.com",
        "aff_unique_abbr": "USC;Amazon",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.39",
        "title": "To Find Waldo You Need Contextual Cues: Debiasing Who\u2019s Waldo",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We present a debiased dataset for the Person-centric Visual Grounding (PCVG) task first proposed by Cui et al. (2021) in the Who\u2019s Waldo dataset. Given an image and a caption, PCVG requires pairing up a person\u2019s name mentioned in a caption with a bounding box that points to the person in the image. We find that the original Who\u2019s Waldo dataset compiled for this task contains a large number of biased samples that are solvable simply by heuristic methods; for instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image. Naturally, models trained on these biased data lead to over-estimation of performance on the benchmark. To enforce models being correct for the correct reasons, we design automated tools to filter and debias the original dataset by ruling out all examples of insufficient context, such as those with no verb or with a long chain of conjunct names in their captions. Our experiments show that our new sub-sampled dataset contains less bias with much lowered heuristic performances and widened gaps between heuristic and supervised methods. We also demonstrate the same benchmark model trained on our debiased training set outperforms that trained on the original biased (and larger) training set on our debiased test set. We argue our debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements.",
        "author": "Yiran Luo; Pratyay Banerjee; Tejas Gokhale; Yezhou Yang; Chitta Baral",
        "authorids": "/y/yiran-luo/; /p/pratyay-banerjee/; /t/tejas-gokhale/; /y/yezhou-yang/; /c/chitta-baral/",
        "bibtex": "@inproceedings{luo-etal-2022-find,\n    title = \"To Find Waldo You Need Contextual Cues: Debiasing Who`s Waldo\",\n    author = \"Luo, Yiran  and\n      Banerjee, Pratyay  and\n      Gokhale, Tejas  and\n      Yang, Yezhou  and\n      Baral, Chitta\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.39/\",\n    doi = \"10.18653/v1/2022.acl-short.39\",\n    pages = \"355--361\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.39.pdf",
        "site": "https://aclanthology.org/2022.acl-short.39/",
        "pdf_size": 4067675,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16377575861123124968&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Arizona State University, Tempe, AZ, USA; Arizona State University, Tempe, AZ, USA; Arizona State University, Tempe, AZ, USA; Arizona State University, Tempe, AZ, USA; Arizona State University, Tempe, AZ, USA",
        "aff_domain": "asu.edu;asu.edu;asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;asu.edu;asu.edu",
        "github": "https://github.com/fpsluozi/tofindwaldo",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Tempe",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.76",
        "title": "To be or not to be an Integer? Encoding Variables for Mathematical Text",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The application of Natural Language Inference (NLI) methods over large textual corpora can facilitate scientific discovery, reducing the gap between current research and the available large-scale scientific knowledge. However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is an integral part of scientific argumentation for many disciplines. One of the fundamental requirements towards mathematical language understanding, is the creation of models able to meaningfully represent variables. This problem is particularly challenging since the meaning of a variable should be assigned exclusively from its defining type, i.e., the representation of a variable should come from its context. Recent research has formalised the variable typing task, a benchmark for the understanding of abstract mathematical types and variables in a sentence. In this work, we propose VarSlot, a Variable Slot-based approach, which not only delivers state-of-the-art results in the task of variable typing, but is also able to create context-based representations for variables.",
        "author": "Deborah Ferreira; Mokanarangan Thayaparan; Marco Valentino; Julia Rozanova; Andre Freitas",
        "authorids": "/d/deborah-ferreira/; /m/mokanarangan-thayaparan/; /m/marco-valentino/; /j/julia-rozanova/; /a/andre-freitas/",
        "bibtex": "@inproceedings{ferreira-etal-2022-integer,\n    title = \"To be or not to be an Integer? Encoding Variables for Mathematical Text\",\n    author = \"Ferreira, Deborah  and\n      Thayaparan, Mokanarangan  and\n      Valentino, Marco  and\n      Rozanova, Julia  and\n      Freitas, Andre\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.76/\",\n    doi = \"10.18653/v1/2022.findings-acl.76\",\n    pages = \"938--948\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.76.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.76/",
        "pdf_size": 469472,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2765806212575659447&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, University of Manchester, United Kingdom1 + Idiap Research Institute, Switzerland2; Department of Computer Science, University of Manchester, United Kingdom1 + Idiap Research Institute, Switzerland2; Department of Computer Science, University of Manchester, United Kingdom1 + Idiap Research Institute, Switzerland2; Department of Computer Science, University of Manchester, United Kingdom1; Department of Computer Science, University of Manchester, United Kingdom1 + Idiap Research Institute, Switzerland2",
        "aff_domain": "manchester.ac.uk; ; ; ; ",
        "email": "manchester.ac.uk; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0;0+1",
        "aff_unique_norm": "University of Manchester;Idiap Research Institute",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.manchester.ac.uk;https://www.idiap.ch",
        "aff_unique_abbr": "UoM;Idiap",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0+1;0;0+1",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "2022.acl-long.262",
        "title": "Token Dropping for Efficient BERT Pretraining",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer-based models generally allocate the same amount of computation for each token in a given sequence. We develop a simple but effective \u201ctoken dropping\u201d method to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks. In particular, we drop unimportant tokens starting from an intermediate layer in the model to make the model focus on important tokens more efficiently if with limited computational resource. The dropped tokens are later picked up by the last layer of the model so that the model still produces full-length sequences. We leverage the already built-in masked language modeling (MLM) loss to identify unimportant tokens with practically no computational overhead. In our experiments, this simple approach reduces the pretraining cost of BERT by 25% while achieving similar overall fine-tuning performance on standard downstream tasks.",
        "author": "Le Hou; Richard Yuanzhe Pang; Tianyi Zhou; Yuexin Wu; Xinying Song; Xiaodan Song; Denny Zhou",
        "authorids": "/l/le-hou/; /r/richard-yuanzhe-pang/; /t/tianyi-zhou/; /y/yuexin-wu/; /x/xinying-song/; /x/xiaodan-song/; /d/denny-zhou/",
        "bibtex": "@inproceedings{hou-etal-2022-token,\n    title = \"Token Dropping for Efficient {BERT} Pretraining\",\n    author = \"Hou, Le  and\n      Pang, Richard Yuanzhe  and\n      Zhou, Tianyi  and\n      Wu, Yuexin  and\n      Song, Xinying  and\n      Song, Xiaodan  and\n      Zhou, Denny\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.262/\",\n    doi = \"10.18653/v1/2022.acl-long.262\",\n    pages = \"3774--3784\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.262.pdf",
        "site": "https://aclanthology.org/2022.acl-long.262/",
        "pdf_size": 541153,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2951909174180462393&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Google; New York University; Google+University of Maryland, College Park; Google; Google; Google; Google",
        "aff_domain": "google.com;nyu.edu; ; ; ; ;google.com",
        "email": "google.com;nyu.edu; ; ; ; ;google.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0+2;0;0;0;0",
        "aff_unique_norm": "Google;New York University;University of Maryland",
        "aff_unique_dep": "Google;;",
        "aff_unique_url": "https://www.google.com;https://www.nyu.edu;https://www/umd.edu",
        "aff_unique_abbr": "Google;NYU;UMD",
        "aff_campus_unique_index": "0;0+2;0;0;0;0",
        "aff_campus_unique": "Mountain View;;College Park",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.13",
        "title": "TopWORDS-Seg: Simultaneous Text Segmentation and Word Discovery for Open-Domain Chinese Texts via Bayesian Inference",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Processing open-domain Chinese texts has been a critical bottleneck in computational linguistics for decades, partially because text segmentation and word discovery often entangle with each other in this challenging scenario. No existing methods yet can achieve effective text segmentation and word discovery simultaneously in open domain. This study fills in this gap by proposing a novel method called TopWORDS-Seg based on Bayesian inference, which enjoys robust performance and transparent interpretation when no training corpus and domain vocabulary are available. Advantages of TopWORDS-Seg are demonstrated by a series of experimental studies.",
        "author": "Changzai Pan; Maosong Sun; Ke Deng",
        "authorids": "/c/changzai-pan/; /m/maosong-sun/; /k/ke-deng/",
        "bibtex": "@inproceedings{pan-etal-2022-topwords,\n    title = \"{T}op{WORDS}-Seg: Simultaneous Text Segmentation and Word Discovery for Open-Domain {C}hinese Texts via {B}ayesian Inference\",\n    author = \"Pan, Changzai  and\n      Sun, Maosong  and\n      Deng, Ke\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.13/\",\n    doi = \"10.18653/v1/2022.acl-long.13\",\n    pages = \"158--169\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.13.pdf",
        "site": "https://aclanthology.org/2022.acl-long.13/",
        "pdf_size": 2619923,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17591402115142063210&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Statistical Science & Department of Industrial Engineering, Tsinghua University; Department of Computer Science and Technology & Guo Qiang Institute for Artificial Intelligence, Tsinghua University; Center for Statistical Science & Department of Industrial Engineering, Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Center for Statistical Science, Department of Industrial Engineering",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.126",
        "title": "Toward Annotator Group Bias in Crowdsourcing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Crowdsourcing has emerged as a popular approach for collecting annotated data to train supervised machine learning models. However, annotator bias can lead to defective annotations. Though there are a few works investigating individual annotator bias, the group effects in annotators are largely overlooked. In this work, we reveal that annotators within the same demographic group tend to show consistent group bias in annotation tasks and thus we conduct an initial study on annotator group bias. We first empirically verify the existence of annotator group bias in various real-world crowdsourcing datasets. Then, we develop a novel probabilistic graphical framework GroupAnno to capture annotator group bias with an extended Expectation Maximization (EM) algorithm. We conduct experiments on both synthetic and real-world datasets. Experimental results demonstrate the effectiveness of our model in modeling annotator group bias in label aggregation and model learning over competitive baselines.",
        "author": "Haochen Liu; Joseph Thekinen; Sinem Mollaoglu; Da Tang; Ji Yang; Youlong Cheng; Hui Liu; Jiliang Tang",
        "authorids": "/h/haochen-liu/; /j/joseph-thekinen/; /s/sinem-mollaoglu/; /d/da-tang/; /j/ji-yang/; /y/youlong-cheng/; /h/hui-liu/; /j/jiliang-tang/",
        "bibtex": "@inproceedings{liu-etal-2022-toward,\n    title = \"Toward Annotator Group Bias in Crowdsourcing\",\n    author = \"Liu, Haochen  and\n      Thekinen, Joseph  and\n      Mollaoglu, Sinem  and\n      Tang, Da  and\n      Yang, Ji  and\n      Cheng, Youlong  and\n      Liu, Hui  and\n      Tang, Jiliang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.126/\",\n    doi = \"10.18653/v1/2022.acl-long.126\",\n    pages = \"1797--1806\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.126.pdf",
        "site": "https://aclanthology.org/2022.acl-long.126/",
        "pdf_size": 1423090,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16694798935443019794&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Michigan State University; University of Calgary; Michigan State University; ByteDance Inc.; ByteDance Inc.; ByteDance Inc.; Michigan State University; Michigan State University",
        "aff_domain": "msu.edu;ucalgary.ca;msu.edu;bytedance.com;bytedance.com;bytedance.com;msu.edu;msu.edu",
        "email": "msu.edu;ucalgary.ca;msu.edu;bytedance.com;bytedance.com;bytedance.com;msu.edu;msu.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;2;2;2;0;0",
        "aff_unique_norm": "Michigan State University;University of Calgary;ByteDance",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.msu.edu;https://www.ucalgary.ca;https://www.bytedance.com",
        "aff_unique_abbr": "MSU;U of C;ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;2;2;0;0",
        "aff_country_unique": "United States;Canada;China"
    },
    {
        "id": "2022.acl-long.412",
        "title": "Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-the-art performance on the semantic textual similarity (STS) task. However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output. In this work, we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem, and then present the optimal transport-based distance measure, named RCMD; it identifies and leverages semantically-aligned token pairs. In the end, we propose CLRCMD, a contrastive learning framework that optimizes RCMD of sentence pairs, which enhances the quality of sentence similarity and their interpretation. Extensive experiments demonstrate that our learning framework outperforms other baselines on both STS and interpretable-STS benchmarks, indicating that it computes effective sentence similarity and also provides interpretation consistent with human judgement.",
        "author": "Seonghyeon Lee; Dongha Lee; Seongbo Jang; Hwanjo Yu",
        "authorids": "/s/seonghyeon-lee/; /d/dongha-lee/; /s/seongbo-jang/; /h/hwanjo-yu/",
        "bibtex": "@inproceedings{lee-etal-2022-toward,\n    title = \"Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning\",\n    author = \"Lee, Seonghyeon  and\n      Lee, Dongha  and\n      Jang, Seongbo  and\n      Yu, Hwanjo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.412/\",\n    doi = \"10.18653/v1/2022.acl-long.412\",\n    pages = \"5969--5979\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.412.pdf",
        "site": "https://aclanthology.org/2022.acl-long.412/",
        "pdf_size": 1118063,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=363026696468613479&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science and Engineering, POSTECH, Pohang, Republic of Korea; University of Illinois at Urbana-Champaign (UIUC), Urbana, IL, United States; Computer Science and Engineering, POSTECH, Pohang, Republic of Korea; Computer Science and Engineering, POSTECH, Pohang, Republic of Korea",
        "aff_domain": "postech.ac.kr;illinois.edu;postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;illinois.edu;postech.ac.kr;postech.ac.kr",
        "github": "https://github.com/sh0416/clrcmd",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "POSTECH;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Computer Science and Engineering;",
        "aff_unique_url": "https://www.postech.ac.kr;https://illinois.edu",
        "aff_unique_abbr": "POSTECH;UIUC",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Pohang;Urbana",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2022.findings-acl.44",
        "title": "Toward More Meaningful Resources for Lower-resourced Languages",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages. Before advancing that position, we first examine two massively multilingual resources used in language technology development, identifying shortcomings that limit their usefulness. We explore the contents of the names stored in Wikidata for a few lower-resourced languages and find that many of them are not in fact in the languages they claim to be, requiring non-trivial effort to correct. We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. We then discuss the importance of creating annotations for lower-resourced languages in a thoughtful and ethical way that includes the language speakers as part of the development process. We conclude with recommended guidelines for resource development.",
        "author": "Constantine Lignos; Nolan Holley; Chester Palen-Michel; Jonne S\u00e4lev\u00e4",
        "authorids": "/c/constantine-lignos/; /n/nolan-holley/; /c/chester-palen-michel/; /j/jonne-saleva/",
        "bibtex": "@inproceedings{lignos-etal-2022-toward,\n    title = \"Toward More Meaningful Resources for Lower-resourced Languages\",\n    author = {Lignos, Constantine  and\n      Holley, Nolan  and\n      Palen-Michel, Chester  and\n      S{\\\"a}lev{\\\"a}, Jonne},\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.44/\",\n    doi = \"10.18653/v1/2022.findings-acl.44\",\n    pages = \"523--532\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.44.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.44/",
        "pdf_size": 248977,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7383471328575477174&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Michtom School of Computer Science, Brandeis University; Michtom School of Computer Science, Brandeis University; Michtom School of Computer Science, Brandeis University; Michtom School of Computer Science, Brandeis University",
        "aff_domain": "brandeis.edu;williams.edu;brandeis.edu;brandeis.edu",
        "email": "brandeis.edu;williams.edu;brandeis.edu;brandeis.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Brandeis University",
        "aff_unique_dep": "Michtom School of Computer Science",
        "aff_unique_url": "https://www.brandeis.edu",
        "aff_unique_abbr": "Brandeis",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.302",
        "title": "Towards Abstractive Grounded Summarization of Podcast Transcripts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Podcasts have shown a recent rise in popularity. Summarization of podcasts is of practical benefit to both content providers and consumers. It helps people quickly decide whether they will listen to a podcast and/or reduces the cognitive load of content providers to write summaries. Nevertheless, podcast summarization faces significant challenges including factual inconsistencies of summaries with respect to the inputs. The problem is exacerbated by speech disfluencies and recognition errors in transcripts of spoken language. In this paper, we explore a novel abstractive summarization method to alleviate these issues. Our approach learns to produce an abstractive summary while grounding summary segments in specific regions of the transcript to allow for full inspection of summary details. We conduct a series of analyses of the proposed approach on a large podcast dataset and show that the approach can achieve promising results. Grounded summaries bring clear benefits in locating the summary and transcript segments that contain inconsistent information, and hence improve summarization quality in terms of automatic and human evaluation.",
        "author": "Kaiqiang Song; Chen Li; Xiaoyang Wang; Dong Yu; Fei Liu",
        "authorids": "/k/kaiqiang-song/; /c/chen-li/; /x/xiaoyang-wang/; /d/dong-yu/; /f/fei-liu-utdallas/",
        "bibtex": "@inproceedings{song-etal-2022-towards,\n    title = \"Towards Abstractive Grounded Summarization of Podcast Transcripts\",\n    author = \"Song, Kaiqiang  and\n      Li, Chen  and\n      Wang, Xiaoyang  and\n      Yu, Dong  and\n      Liu, Fei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.302/\",\n    doi = \"10.18653/v1/2022.acl-long.302\",\n    pages = \"4407--4418\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.302.pdf",
        "site": "https://aclanthology.org/2022.acl-long.302/",
        "pdf_size": 2233919,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3274480781934732169&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Tencent AI Lab, Seattle, WA\u2020\u2021; Tencent AI Lab, Seattle, WA\u2020\u2021; Tencent AI Lab, Seattle, WA\u2020\u2021; Tencent AI Lab, Seattle, WA\u2020\u2021; University of Central Florida, Orlando, FL\u2020",
        "aff_domain": "tencent.com;tencent.com;tencent.com;tencent.com;cs.ucf.edu",
        "email": "tencent.com;tencent.com;tencent.com;tencent.com;cs.ucf.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Tencent;University of Central Florida",
        "aff_unique_dep": "AI Lab;",
        "aff_unique_url": "https://ai.tencent.com;https://www.ucf.edu",
        "aff_unique_abbr": "Tencent AI;UCF",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Seattle;Orlando",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.134",
        "title": "Towards Adversarially Robust Text Classifiers by Learning to Reweight Clean Examples",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Most of the existing defense methods improve the adversarial robustness by making the models adapt to the training set augmented with some adversarial examples. However, the augmented adversarial examples may not be natural, which might distort the training distribution, resulting in inferior performance both in clean accuracy and adversarial robustness. In this study, we explore the feasibility of introducing a reweighting mechanism to calibrate the training distribution to obtain robust models. We propose to train text classifiers by a sample reweighting method in which the example weights are learned to minimize the loss of a validation set mixed with the clean examples and their adversarial ones in an online learning manner. Through extensive experiments, we show that there exists a reweighting mechanism to make the models more robust against adversarial attacks without the need to craft the adversarial examples for the entire training set.",
        "author": "Jianhan Xu; Cenyuan Zhang; Xiaoqing Zheng; Linyang Li; Cho-Jui Hsieh; Kai-Wei Chang; Xuanjing Huang",
        "authorids": "/j/jianhan-xu/; /c/cenyuan-zhang/; /x/xiaoqing-zheng/; /l/linyang-li/; /c/cho-jui-hsieh/; /k/kai-wei-chang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{xu-etal-2022-towards,\n    title = \"Towards Adversarially Robust Text Classifiers by Learning to Reweight Clean Examples\",\n    author = \"Xu, Jianhan  and\n      Zhang, Cenyuan  and\n      Zheng, Xiaoqing  and\n      Li, Linyang  and\n      Hsieh, Cho-Jui  and\n      Chang, Kai-Wei  and\n      Huang, Xuanjing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.134/\",\n    doi = \"10.18653/v1/2022.findings-acl.134\",\n    pages = \"1694--1707\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.134.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.134/",
        "pdf_size": 2126099,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4360288976639648281&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing; School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing; School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing; School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing; Department of Computer Science, University of California, Los Angeles, USA; Department of Computer Science, University of California, Los Angeles, USA; School of Computer Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Intelligent Information Processing",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn; ;cs.ucla.edu;cs.ucla.edu; ",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn; ;cs.ucla.edu;cs.ucla.edu; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1;0+1;2;2;0+1",
        "aff_unique_norm": "Fudan University;Shanghai Key Laboratory of Intelligent Information Processing;University of California, Los Angeles",
        "aff_unique_dep": "School of Computer Science;Intelligent Information Processing;Department of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn;;https://www.ucla.edu",
        "aff_unique_abbr": "Fudan;;UCLA",
        "aff_campus_unique_index": "0;0;0;0;2;2;0",
        "aff_campus_unique": "Shanghai;;Los Angeles",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;1;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.265",
        "title": "Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Aligning with ACL 2022 special Theme on \u201cLanguage Diversity: from Low Resource to Endangered Languages\u201d, we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages. Situating African languages in a typological framework, we discuss how the particulars of these languages can be harnessed. To facilitate future research, we also highlight current efforts, communities, venues, datasets, and tools. Our main objective is to motivate and advocate for an Afrocentric approach to technology development. With this in mind, we recommend what technologies to build and how to build, evaluate, and deploy them based on the needs of local African communities.",
        "author": "Ife Adebara; Muhammad Abdul-Mageed",
        "authorids": "/i/ife-adebara/; /m/muhammad-abdul-mageed/",
        "bibtex": "@inproceedings{adebara-abdul-mageed-2022-towards,\n    title = \"Towards Afrocentric {NLP} for {A}frican Languages: Where We Are and Where We Can Go\",\n    author = \"Adebara, Ife  and\n      Abdul-Mageed, Muhammad\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.265/\",\n    doi = \"10.18653/v1/2022.acl-long.265\",\n    pages = \"3814--3841\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.265.pdf",
        "site": "https://aclanthology.org/2022.acl-long.265/",
        "pdf_size": 1071034,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Deep Learning and Natural Language Processing Group, The University of British Columbia; Deep Learning and Natural Language Processing Group, The University of British Columbia",
        "aff_domain": "ubc.ca;ubc.ca",
        "email": "ubc.ca;ubc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.588",
        "title": "Towards Better Characterization of Paraphrases",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "To effectively characterize the nature of paraphrase pairs without expert human annotation, we proposes two new metrics: word position deviation (WPD) and lexical deviation (LD). WPD measures the degree of structural alteration, while LD measures the difference in vocabulary used. We apply these metrics to better understand the commonly-used MRPC dataset and study how it differs from PAWS, another paraphrase identification dataset. We also perform a detailed study on MRPC and propose improvements to the dataset, showing that it improves generalizability of models trained on the dataset. Lastly, we apply our metrics to filter the output of a paraphrase generation model and show how it can be used to generate specific forms of paraphrases for data augmentation or robustness testing of NLP models.",
        "author": "Timothy Liu; De Wen Soh",
        "authorids": "/t/timothy-liu/; /d/de-wen-soh/",
        "bibtex": "@inproceedings{liu-soh-2022-towards,\n    title = \"Towards Better Characterization of Paraphrases\",\n    author = \"Liu, Timothy  and\n      Soh, De Wen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.588/\",\n    doi = \"10.18653/v1/2022.acl-long.588\",\n    pages = \"8592--8601\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.588.pdf",
        "site": "https://aclanthology.org/2022.acl-long.588/",
        "pdf_size": 1609713,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16046667745783826009&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Singapore University of Technology and Design; Singapore University of Technology and Design",
        "aff_domain": "mymail.sutd.edu.sg;sutd.edu.sg",
        "email": "mymail.sutd.edu.sg;sutd.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Singapore University of Technology and Design",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sutd.edu.sg",
        "aff_unique_abbr": "SUTD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.findings-acl.328",
        "title": "Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent work in task-independent graph semantic parsing has shifted from grammar-based symbolic approaches to neural models, showing strong performance on different types of meaning representations. However, it is still unclear that what are the limitations of these neural parsers, and whether these limitations can be compensated by incorporating symbolic knowledge into model inference. In this paper, we address these questions by taking English Resource Grammar (ERG) parsing as a case study. Specifically, we first develop a state-of-the-art, T5-based neural ERG parser, and conduct detail analyses of parser performance within fine-grained linguistic categories. The neural parser attains superior performance on in-distribution test set, but degrades significantly on long-tail situations, while the symbolic parser performs more robustly. To address this, we further propose a simple yet principled collaborative framework for neural-symbolic semantic parsing, by designing a decision criterion for beam search that incorporates the prior knowledge from a symbolic parser and accounts for model uncertainty. Experimental results show that the proposed framework yields comprehensive improvement over neural baseline across long-tail categories, yielding the best known Smatch score (97.01) on the well-studied DeepBank benchmark.",
        "author": "Zi Lin; Jeremiah Zhe Liu; Jingbo Shang",
        "authorids": "/z/zi-lin/; /j/jeremiah-zhe-liu/; /j/jingbo-shang/",
        "bibtex": "@inproceedings{lin-etal-2022-towards,\n    title = \"Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty\",\n    author = \"Lin, Zi  and\n      Liu, Jeremiah Zhe  and\n      Shang, Jingbo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.328/\",\n    doi = \"10.18653/v1/2022.findings-acl.328\",\n    pages = \"4160--4173\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.328.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.328/",
        "pdf_size": 425840,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12802201634507533538&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "UC San Diego; Google Research + Harvard University; UC San Diego",
        "aff_domain": "ucsd.edu;google.com;ucsd.edu",
        "email": "ucsd.edu;google.com;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "University of California, San Diego;Google;Harvard University",
        "aff_unique_dep": ";Google Research;",
        "aff_unique_url": "https://www.ucsd.edu;https://research.google;https://www.harvard.edu",
        "aff_unique_abbr": "UCSD;Google Research;Harvard",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "San Diego;Mountain View;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.28",
        "title": "Towards Comprehensive Patent Approval Predictions:Beyond Traditional Document Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Predicting the approval chance of a patent application is a challenging problem involving multiple facets. The most crucial facet is arguably the novelty \u2014 35 U.S. Code \u00a7 102 rejects more recent applications that have very similar prior arts. Such novelty evaluations differ the patent approval prediction from conventional document classification \u2014 Successful patent applications may share similar writing patterns; however, too-similar newer applications would receive the opposite label, thus confusing standard document classifiers (e.g., BERT). To address this issue, we propose a novel framework that unifies the document classifier with handcrafted features, particularly time-dependent novelty scores. Specifically, we formulate the novelty scores by comparing each application with millions of prior arts using a hybrid of efficient filters and a neural bi-encoder. Moreover, we impose a new regularization term into the classification objective to enforce the monotonic change of approval prediction w.r.t. novelty scores. From extensive experiments on a large-scale USPTO dataset, we find that standard BERT fine-tuning can partially learn the correct relationship between novelty and approvals from inconsistent data. However, our time-dependent novelty features offer a boost on top of it. Also, our monotonic regularization, while shrinking the search space, can drive the optimizer to better local optima, yielding a further small performance gain.",
        "author": "Xiaochen Gao; Zhaoyi Hou; Yifei Ning; Kewen Zhao; Beilei He; Jingbo Shang; Vish Krishnan",
        "authorids": "/x/xiaochen-gao/; /z/zhaoyi-hou/; /y/yifei-ning/; /k/kewen-zhao/; /b/beilei-he/; /j/jingbo-shang/; /v/vish-krishnan/",
        "bibtex": "@inproceedings{gao-etal-2022-towards,\n    title = \"Towards Comprehensive Patent Approval Predictions:Beyond Traditional Document Classification\",\n    author = \"Gao, Xiaochen  and\n      Hou, Zhaoyi  and\n      Ning, Yifei  and\n      Zhao, Kewen  and\n      He, Beilei  and\n      Shang, Jingbo  and\n      Krishnan, Vish\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.28/\",\n    doi = \"10.18653/v1/2022.acl-long.28\",\n    pages = \"349--372\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.28.pdf",
        "site": "https://aclanthology.org/2022.acl-long.28/",
        "pdf_size": 1205319,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11372778549913573213&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.88",
        "title": "Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We consider the task of document-level entity linking (EL), where it is important to make consistent decisions for entity mentions over the full document jointly. We aim to leverage explicit \u201cconnections\u201d among mentions within the document itself: we propose to join EL and coreference resolution (coref) in a single structured prediction task over directed trees and use a globally normalized model to solve it. This contrasts with related works where two separate models are trained for each of the tasks and additional logic is required to merge the outputs. Experimental results on two datasets show a boost of up to +5% F1-score on both coref and EL tasks, compared to their standalone counterparts. For a subset of hard cases, with individual mentions lacking the correct EL in their candidate entity list, we obtain a +50% increase in accuracy.",
        "author": "Klim Zaporojets; Johannes Deleu; Yiwei Jiang; Thomas Demeester; Chris Develder",
        "authorids": "/k/klim-zaporojets/; /j/johannes-deleu/; /y/yiwei-jiang/; /t/thomas-demeester/; /c/chris-develder/",
        "bibtex": "@inproceedings{zaporojets-etal-2022-towards,\n    title = \"Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution\",\n    author = \"Zaporojets, Klim  and\n      Deleu, Johannes  and\n      Jiang, Yiwei  and\n      Demeester, Thomas  and\n      Develder, Chris\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.88/\",\n    doi = \"10.18653/v1/2022.acl-short.88\",\n    pages = \"778--784\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.88.pdf",
        "site": "https://aclanthology.org/2022.acl-short.88/",
        "pdf_size": 495576,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15316056035212198046&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 13,
        "aff": "Ghent University \u2013 imec, IDLab; Ghent University \u2013 imec, IDLab; Ghent University \u2013 imec, IDLab; Ghent University \u2013 imec, IDLab; Ghent University \u2013 imec, IDLab",
        "aff_domain": "ugent.be;ugent.be;ugent.be;ugent.be;ugent.be",
        "email": "ugent.be;ugent.be;ugent.be;ugent.be;ugent.be",
        "github": "https://github.com/klimzaporojets/consistent-EL",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Ghent University",
        "aff_unique_dep": "imec, IDLab",
        "aff_unique_url": "https://www.ugent.be/en",
        "aff_unique_abbr": "UGent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2022.acl-short.35",
        "title": "Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance.",
        "author": "Suvodip Dey; Ramamohan Kummara; Maunendra Desarkar",
        "authorids": "/s/suvodip-dey/; /r/ramamohan-kummara/; /m/maunendra-desarkar/",
        "bibtex": "@inproceedings{dey-etal-2022-towards,\n    title = \"Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances\",\n    author = \"Dey, Suvodip  and\n      Kummara, Ramamohan  and\n      Desarkar, Maunendra\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.35/\",\n    doi = \"10.18653/v1/2022.acl-short.35\",\n    pages = \"318--324\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.35.pdf",
        "site": "https://aclanthology.org/2022.acl-short.35/",
        "pdf_size": 511936,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14173704117372374335&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Indian Institute of Technology Hyderabad, India; Indian Institute of Technology Hyderabad, India; Indian Institute of Technology Hyderabad, India",
        "aff_domain": "iith.ac.in;iith.ac.in;cse.iith.ac.in",
        "email": "iith.ac.in;iith.ac.in;cse.iith.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iith.ac.in",
        "aff_unique_abbr": "IIT Hyderabad",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.findings-acl.329",
        "title": "Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Entity recognition is a fundamental task in understanding document images. Traditional sequence labeling frameworks treat the entity types as class IDs and rely on extensive data and high-quality annotations to learn semantics which are typically expensive in practice. In this paper, we aim to build an entity recognition model requiring only a few shots of annotated document images. To overcome the data limitation, we propose to leverage the label surface names to better inform the model of the target entity type semantics and also embed the labels into the spatial embedding space to capture the spatial correspondence between regions and labels. Specifically, we go beyond sequence labeling and develop a novel label-aware seq2seq framework, LASER. The proposed model follows a new labeling scheme that generates the label surface names word-by-word explicitly after generating the entities. During training, LASER refines the label semantics by updating the label surface name representations and also strengthens the label-region correlation. In this way, LASER recognizes the entities from document images through both semantic and layout correspondence. Extensive experiments on two benchmark datasets demonstrate the superiority of LASER under the few-shot setting.",
        "author": "Zilong Wang; Jingbo Shang",
        "authorids": "/z/zilong-wang/; /j/jingbo-shang/",
        "bibtex": "@inproceedings{wang-shang-2022-towards,\n    title = \"Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework\",\n    author = \"Wang, Zilong  and\n      Shang, Jingbo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.329/\",\n    doi = \"10.18653/v1/2022.findings-acl.329\",\n    pages = \"4174--4186\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.329.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.329/",
        "pdf_size": 3954050,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16327336174311133213&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, San Diego; University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.33",
        "title": "Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor. One possible solution to improve user experience and relieve the manual efforts of designers is to build an end-to-end dialogue system that can do reasoning itself while perceiving user\u2019s utterances. In this work, we propose a novel method to incorporate the knowledge reasoning capability into dialog systems in a more scalable and generalizable manner. Our proposed method allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses. To the best of our knowledge, this is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs. We investigate the reasoning abilities of the proposed method on both task-oriented and domain-specific chit-chat dialogues. Empirical results show that this method can effectively and efficiently incorporate a knowledge graph into a dialogue system with fully-interpretable reasoning paths.",
        "author": "Yi-Lin Tuan; Sajjad Beygi; Maryam Fazel-Zarandi; Qiaozi Gao; Alessandra Cervone; William Yang Wang",
        "authorids": "/y/yi-lin-tuan/; /s/sajjad-beygi/; /m/maryam-fazel-zarandi/; /q/qiaozi-gao/; /a/alessandra-cervone/; /w/william-yang-wang/",
        "bibtex": "@inproceedings{tuan-etal-2022-towards,\n    title = \"Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems\",\n    author = \"Tuan, Yi-Lin  and\n      Beygi, Sajjad  and\n      Fazel-Zarandi, Maryam  and\n      Gao, Qiaozi  and\n      Cervone, Alessandra  and\n      Wang, William Yang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.33/\",\n    doi = \"10.18653/v1/2022.findings-acl.33\",\n    pages = \"383--395\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.33.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.33/",
        "pdf_size": 570496,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14159093658626063823&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Santa Barbara+Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; University of California, Santa Barbara+Amazon Alexa AI",
        "aff_domain": "cs.ucsb.edu;amazon.com;amazon.com;amazon.com;amazon.com;cs.ucsb.edu",
        "email": "cs.ucsb.edu;amazon.com;amazon.com;amazon.com;amazon.com;cs.ucsb.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;0+1",
        "aff_unique_norm": "University of California, Santa Barbara;Amazon",
        "aff_unique_dep": ";Amazon Alexa AI",
        "aff_unique_url": "https://www.ucsb.edu;https://www.amazon.com",
        "aff_unique_abbr": "UCSB;Amazon",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Santa Barbara;",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.436",
        "title": "Towards Learning (Dis)-Similarity of Source Code from Program Contrasts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding the functional (dis)-similarity of source code is significant for code modeling tasks such as software vulnerability and code clone detection. We present DISCO (DIS-similarity of COde), a novel self-supervised model focusing on identifying (dis)similar functionalities of source code. Different from existing works, our approach does not require a huge amount of randomly collected datasets. Rather, we design structure-guided code transformation algorithms to generate synthetic code clones and inject real-world security bugs, augmenting the collected datasets in a targeted way. We propose to pre-train the Transformer model with such automatically generated program contrasts to better identify similar code in the wild and differentiate vulnerable programs from benign ones. To better capture the structural features of source code, we propose a new cloze objective to encode the local tree-based context (e.g., parents or sibling nodes). We pre-train our model with a much smaller dataset, the size of which is only 5% of the state-of-the-art models\u2019 training datasets, to illustrate the effectiveness of our data augmentation and the pre-training approach. The evaluation shows that, even with much less data, DISCO can still outperform the state-of-the-art models in vulnerability and code clone detection tasks.",
        "author": "Yangruibo Ding; Luca Buratti; Saurabh Pujar; Alessandro Morari; Baishakhi Ray; Saikat Chakraborty",
        "authorids": "/y/yangruibo-ding/; /l/luca-buratti/; /s/saurabh-pujar/; /a/alessandro-morari/; /b/baishakhi-ray/; /s/saikat-chakraborty/",
        "bibtex": "@inproceedings{ding-etal-2022-towards,\n    title = \"Towards Learning (Dis)-Similarity of Source Code from Program Contrasts\",\n    author = \"Ding, Yangruibo  and\n      Buratti, Luca  and\n      Pujar, Saurabh  and\n      Morari, Alessandro  and\n      Ray, Baishakhi  and\n      Chakraborty, Saikat\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.436/\",\n    doi = \"10.18653/v1/2022.acl-long.436\",\n    pages = \"6300--6312\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.436.pdf",
        "site": "https://aclanthology.org/2022.acl-long.436/",
        "pdf_size": 708777,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5414983959933381355&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Columbia University; IBM Research; IBM Research; IBM Research; Columbia University; Columbia University",
        "aff_domain": "cs.columbia.edu;ibm.com;ibm.com;us.ibm.com;cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;ibm.com;ibm.com;us.ibm.com;cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;0;0",
        "aff_unique_norm": "Columbia University;IBM",
        "aff_unique_dep": ";IBM Research",
        "aff_unique_url": "https://www.columbia.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "Columbia;IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.12",
        "title": "Towards Making the Most of Cross-Lingual Transfer for Zero-Shot Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper demonstrates that multilingual pretraining and multilingual fine-tuning are both critical for facilitating cross-lingual transfer in zero-shot translation, where the neural machine translation (NMT) model is tested on source languages unseen during supervised training. Following this idea, we present SixT+, a strong many-to-English NMT model that supports 100 source languages but is trained with a parallel dataset in only six source languages. SixT+ initializes the decoder embedding and the full encoder with XLM-R large and then trains the encoder and decoder layers with a simple two-stage training strategy. SixT+ achieves impressive performance on many-to-English translation. It significantly outperforms CRISS and m2m-100, two strong multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU respectively. Additionally, SixT+ offers a set of model parameters that can be further fine-tuned to other unsupervised tasks. We demonstrate that adding SixT+ initialization outperforms state-of-the-art explicitly designed unsupervised NMT models on Si<->En and Ne<->En by over 1.2 average BLEU. When applied to zero-shot cross-lingual abstractive summarization, it produces an average performance gain of 12.3 ROUGE-L over mBART-ft. We conduct detailed analyses to understand the key ingredients of SixT+, including multilinguality of the auxiliary parallel data, positional disentangled encoder, and the cross-lingual transferability of its encoder.",
        "author": "Guanhua Chen; Shuming Ma; Yun Chen; Dongdong Zhang; Jia Pan; Wenping Wang; Furu Wei",
        "authorids": "/g/guanhua-chen/; /s/shuming-ma/; /y/yun-chen/; /d/dongdong-zhang/; /j/jia-pan/; /w/wenping-wang/; /f/furu-wei/",
        "bibtex": "@inproceedings{chen-etal-2022-towards,\n    title = \"Towards Making the Most of Cross-Lingual Transfer for Zero-Shot Neural Machine Translation\",\n    author = \"Chen, Guanhua  and\n      Ma, Shuming  and\n      Chen, Yun  and\n      Zhang, Dongdong  and\n      Pan, Jia  and\n      Wang, Wenping  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.12/\",\n    doi = \"10.18653/v1/2022.acl-long.12\",\n    pages = \"142--157\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.12.pdf",
        "site": "https://aclanthology.org/2022.acl-long.12/",
        "pdf_size": 397941,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9848047122876613143&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Hong Kong; Microsoft Research; Shanghai University of Finance and Economics; Microsoft Research; The University of Hong Kong; Texas A&M University+The University of Hong Kong; Microsoft Research",
        "aff_domain": "cs.hku.hk;microsoft.com;sufe.edu.cn;microsoft.com;cs.hku.hk;cs.hku.hk;microsoft.com",
        "email": "cs.hku.hk;microsoft.com;sufe.edu.cn;microsoft.com;cs.hku.hk;cs.hku.hk;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;1;0;3+0;1",
        "aff_unique_norm": "University of Hong Kong;Microsoft;Shanghai University of Finance and Economics;Texas A&M University",
        "aff_unique_dep": ";Microsoft Research;;",
        "aff_unique_url": "https://www.hku.hk;https://www.microsoft.com/en-us/research;http://www.sufe.edu.cn;https://www.tamu.edu",
        "aff_unique_abbr": "HKU;MSR;SUFE;TAMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;0;1;0;1+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.findings-acl.31",
        "title": "Towards Responsible Natural Language Annotation for the Varieties of Arabic",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "When building NLP models, there is a tendency to aim for broader coverage, often overlooking cultural and (socio)linguistic nuance. In this position paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the inclusion of cultural and linguistic expertise in the process. We present a playbook for responsible dataset creation for polyglossic, multidialectal languages. This work is informed by a study on Arabic annotation of social media content.",
        "author": "A. Bergman; Mona Diab",
        "authorids": "/a/a-bergman/; /m/mona-diab/",
        "bibtex": "@inproceedings{bergman-diab-2022-towards,\n    title = \"Towards Responsible Natural Language Annotation for the Varieties of {A}rabic\",\n    author = \"Bergman, A.  and\n      Diab, Mona\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.31/\",\n    doi = \"10.18653/v1/2022.findings-acl.31\",\n    pages = \"364--371\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.31.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.31/",
        "pdf_size": 233496,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13963514395942125074&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 7,
        "aff": "Responsible AI, Meta / New York, NY; Responsible AI, Meta / Seattle, WA",
        "aff_domain": "fb.com;fb.com",
        "email": "fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Responsible AI",
        "aff_unique_url": "https://meta.com",
        "aff_unique_abbr": "Meta",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "New York;Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.142",
        "title": "Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The robustness of Text-to-SQL parsers against adversarial perturbations plays a crucial role in delivering highly reliable applications. Previous studies along this line primarily focused on perturbations in the natural language question side, neglecting the variability of tables. Motivated by this, we propose the Adversarial Table Perturbation (ATP) as a new attacking paradigm to measure robustness of Text-to-SQL models. Following this proposition, we curate ADVETA, the first robustness evaluation benchmark featuring natural and realistic ATPs. All tested state-of-the-art models experience dramatic performance drops on ADVETA, revealing significant room of improvement. To defense against ATP, we build a systematic adversarial training example generation framework tailored for better contextualization of tabular data. Experiments show that our approach brings models best robustness improvement against ATP, while also substantially boost model robustness against NL-side perturbations. We will release ADVETA and code to facilitate future research.",
        "author": "Xinyu Pi; Bing Wang; Yan Gao; Jiaqi Guo; Zhoujun Li; Jian-Guang Lou",
        "authorids": "/x/xinyu-pi/; /b/bing-wang/; /y/yan-gao/; /j/jiaqi-guo/; /z/zhoujun-li/; /j/jian-guang-lou/",
        "bibtex": "@inproceedings{pi-etal-2022-towards,\n    title = \"Towards Robustness of Text-to-{SQL} Models Against Natural and Realistic Adversarial Table Perturbation\",\n    author = \"Pi, Xinyu  and\n      Wang, Bing  and\n      Gao, Yan  and\n      Guo, Jiaqi  and\n      Li, Zhoujun  and\n      Lou, Jian-Guang\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.142/\",\n    doi = \"10.18653/v1/2022.acl-long.142\",\n    pages = \"2007--2022\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.142.pdf",
        "site": "https://aclanthology.org/2022.acl-long.142/",
        "pdf_size": 754180,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1501768368301146881&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Urbana-Champaign, Urbana, USA; State Key Lab of Software Development Environment, Beihang University; Microsoft Research Asia; Xi\u2019an Jiaotong University, Xi\u2019an, China; State Key Lab of Software Development Environment, Beihang University; Microsoft Research Asia",
        "aff_domain": "illinois.edu;buaa.edu.cn;buaa.edu.cn;stu.xjtu.edu.cn;microsoft.com;microsoft.com",
        "email": "illinois.edu;buaa.edu.cn;buaa.edu.cn;stu.xjtu.edu.cn;microsoft.com;microsoft.com",
        "github": "https://github.com/microsoft/ContextualSP",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;1;2",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Beihang University;Microsoft;Xi'an Jiao Tong University",
        "aff_unique_dep": ";State Key Lab of Software Development Environment;Research;",
        "aff_unique_url": "https://illinois.edu;http://www.buaa.edu.cn;https://www.microsoft.com/en-us/research/group/asia;https://www.xjtu.edu.cn",
        "aff_unique_abbr": "UIUC;Beihang;MSR Asia;XJTU",
        "aff_campus_unique_index": "0;2;3;2",
        "aff_campus_unique": "Urbana;;Asia;Xi'an",
        "aff_country_unique_index": "0;1;1;1;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2022.findings-acl.28",
        "title": "Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Existing studies on semantic parsing focus on mapping a natural-language utterance to a logical form (LF) in one turn. However, because natural language may contain ambiguity and variability, this is a difficult challenge. In this work, we investigate an interactive semantic parsing framework that explains the predicted LF step by step in natural language and enables the user to make corrections through natural-language feedback for individual steps. We focus on question answering over knowledge bases (KBQA) as an instantiation of our framework, aiming to increase the transparency of the parsing process and help the user trust the final answer. We construct INSPIRED, a crowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our experiments show that this framework has the potential to greatly improve overall parse accuracy. Furthermore, we develop a pipeline for dialogue simulation to evaluate our framework w.r.t. a variety of state-of-the-art KBQA models without further crowdsourcing effort. The results demonstrate that our framework promises to be effective across such models.",
        "author": "Lingbo Mo; Ashley Lewis; Huan Sun; Michael White",
        "authorids": "/l/lingbo-mo/; /a/ashley-lewis/; /h/huan-sun/; /m/michael-white/",
        "bibtex": "@inproceedings{mo-etal-2022-towards,\n    title = \"Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction\",\n    author = \"Mo, Lingbo  and\n      Lewis, Ashley  and\n      Sun, Huan  and\n      White, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.28/\",\n    doi = \"10.18653/v1/2022.findings-acl.28\",\n    pages = \"322--342\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.28.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.28/",
        "pdf_size": 1363199,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5962021227966975637&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University",
        "aff_domain": "osu.edu;osu.edu;osu.edu;osu.edu",
        "email": "osu.edu;osu.edu;osu.edu;osu.edu",
        "github": "https://github.com/molingbo/INSPIRED",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Ohio State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.3",
        "title": "Towards Unifying the Label Space for Aspect- and Sentence-based Sentiment Analysis",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The aspect-based sentiment analysis (ABSA) is a fine-grained task that aims to determine the sentiment polarity towards targeted aspect terms occurring in the sentence. The development of the ABSA task is very much hindered by the lack of annotated data. To tackle this, the prior works have studied the possibility of utilizing the sentiment analysis (SA) datasets to assist in training the ABSA model, primarily via pretraining or multi-task learning. In this article, we follow this line, and for the first time, we manage to apply the Pseudo-Label (PL) method to merge the two homogeneous tasks. While it seems straightforward to use generated pseudo labels to handle this case of label granularity unification for two highly related tasks, we identify its major challenge in this paper and propose a novel framework, dubbed as Dual-granularity Pseudo Labeling (DPL). Further, similar to PL, we regard the DPL as a general framework capable of combining other prior methods in the literature. Through extensive experiments, DPL has achieved state-of-the-art performance on standard benchmarks surpassing the prior work significantly.",
        "author": "Yiming Zhang; Min Zhang; Sai Wu; Junbo Zhao",
        "authorids": "/y/yiming-zhang/; /m/min-zhang/; /s/sai-wu/; /j/junbo-zhao/",
        "bibtex": "@inproceedings{zhang-etal-2022-towards,\n    title = \"Towards Unifying the Label Space for Aspect- and Sentence-based Sentiment Analysis\",\n    author = \"Zhang, Yiming  and\n      Zhang, Min  and\n      Wu, Sai  and\n      Zhao, Junbo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.3/\",\n    doi = \"10.18653/v1/2022.findings-acl.3\",\n    pages = \"20--30\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.3.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.3/",
        "pdf_size": 566009,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3188883913216259476&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.234",
        "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.",
        "author": "Thomas Hartvigsen; Saadia Gabriel; Hamid Palangi; Maarten Sap; Dipankar Ray; Ece Kamar",
        "authorids": "/t/thomas-hartvigsen/; /s/saadia-gabriel/; /h/hamid-palangi/; /m/maarten-sap/; /d/dipankar-ray/; /e/ece-kamar/",
        "bibtex": "@inproceedings{hartvigsen-etal-2022-toxigen,\n    title = \"{T}oxi{G}en: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection\",\n    author = \"Hartvigsen, Thomas  and\n      Gabriel, Saadia  and\n      Palangi, Hamid  and\n      Sap, Maarten  and\n      Ray, Dipankar  and\n      Kamar, Ece\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.234/\",\n    doi = \"10.18653/v1/2022.acl-long.234\",\n    pages = \"3309--3326\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.234.pdf",
        "site": "https://aclanthology.org/2022.acl-long.234/",
        "pdf_size": 1409889,
        "gs_citation": 461,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8610155546483873146&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Massachusetts Institute of Technology; University of Washington; Microsoft Research; Allen Institute for AI+Carnegie Mellon University; Microsoft; Microsoft Research",
        "aff_domain": "mit.edu;cs.washington.edu;microsoft.com;cmu.edu;microsoft.com;microsoft.com",
        "email": "mit.edu;cs.washington.edu;microsoft.com;cmu.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3+4;2;2",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Washington;Microsoft;Allen Institute for AI;Carnegie Mellon University",
        "aff_unique_dep": ";;Microsoft Research;;",
        "aff_unique_url": "https://web.mit.edu;https://www.washington.edu;https://www.microsoft.com/en-us/research;https://allenai.org;https://www.cmu.edu",
        "aff_unique_abbr": "MIT;UW;MSR;AI2;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.91",
        "title": "Tracing Origins: Coreference-aware Machine Reading Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models (PrLMs) and fine-tuning strategies, and recent studies have enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models. In this paper, we imitate the human reading process in connecting the anaphoric expressions and explicitly leverage the coreference information of the entities to enhance the word embeddings from the pre-trained language model, in order to highlight the coreference mentions of the entities that must be identified for coreference-intensive question answering in QUOREF, a relatively new dataset that is specifically designed to evaluate the coreference-related performance of a model. We use two strategies to fine-tune a pre-trained language model, namely, placing an additional encoder layer after a pre-trained language model to focus on the coreference mentions or constructing a relational graph convolutional network to model the coreference relations. We demonstrate that the explicit incorporation of coreference information in the fine-tuning stage performs better than the incorporation of the coreference information in pre-training a language model.",
        "author": "Baorong Huang; Zhuosheng Zhang; Hai Zhao",
        "authorids": "/b/baorong-huang/; /z/zhuosheng-zhang/; /h/hai-zhao/",
        "bibtex": "@inproceedings{huang-etal-2022-tracing,\n    title = \"Tracing Origins: Coreference-aware Machine Reading Comprehension\",\n    author = \"Huang, Baorong  and\n      Zhang, Zhuosheng  and\n      Zhao, Hai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.91/\",\n    doi = \"10.18653/v1/2022.acl-long.91\",\n    pages = \"1281--1292\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.91.pdf",
        "site": "https://aclanthology.org/2022.acl-long.91/",
        "pdf_size": 1102196,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8999566845366110189&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Corpus Studies and Applications, Shanghai International Studies University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University + Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University",
        "aff_domain": "shisu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "shisu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+1;1+1",
        "aff_unique_norm": "Shanghai International Studies University;Shanghai Jiao Tong University",
        "aff_unique_dep": "Institute of Corpus Studies and Applications;Department of Computer Science and Engineering",
        "aff_unique_url": "http://www.sisu.edu.cn/;https://www.sjtu.edu.cn",
        "aff_unique_abbr": ";SJTU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.226",
        "title": "Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge. However, the indexing and retrieving of large-scale corpora bring considerable computational cost. Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks. We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output. Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks. For instance, our proposed method achieved state-of-the-art results on XSum, BigPatent, and CommonsenseQA. Our code is released, https://github.com/microsoft/REINA .",
        "author": "Shuohang Wang; Yichong Xu; Yuwei Fang; Yang Liu; Siqi Sun; Ruochen Xu; Chenguang Zhu; Michael Zeng",
        "authorids": "/s/shuohang-wang/; /y/yichong-xu/; /y/yuwei-fang/; /y/yang-liu-microsoft/; /s/siqi-sun/; /r/ruochen-xu/; /c/chenguang-zhu/; /m/michael-zeng/",
        "bibtex": "@inproceedings{wang-etal-2022-training,\n    title = \"Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data\",\n    author = \"Wang, Shuohang  and\n      Xu, Yichong  and\n      Fang, Yuwei  and\n      Liu, Yang  and\n      Sun, Siqi  and\n      Xu, Ruochen  and\n      Zhu, Chenguang  and\n      Zeng, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.226/\",\n    doi = \"10.18653/v1/2022.acl-long.226\",\n    pages = \"3170--3179\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.226.pdf",
        "site": "https://aclanthology.org/2022.acl-long.226/",
        "pdf_size": 466223,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6367510514573518083&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Azure Cognitive Services Research; Microsoft Azure Cognitive Services Research; Microsoft Azure Cognitive Services Research; Microsoft Azure Cognitive Services Research; Microsoft Azure Cognitive Services Research; Microsoft Azure Cognitive Services Research; Microsoft Azure Cognitive Services Research; Microsoft Azure Cognitive Services Research",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/microsoft/REINA",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Azure Cognitive Services Research",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.163",
        "title": "Training Dynamics for Text Summarization Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.",
        "author": "Tanya Goyal; Jiacheng Xu; Junyi Jessy Li; Greg Durrett",
        "authorids": "/t/tanya-goyal/; /j/jiacheng-xu/; /j/junyi-jessy-li/; /g/greg-durrett/",
        "bibtex": "@inproceedings{goyal-etal-2022-training,\n    title = \"Training Dynamics for Text Summarization Models\",\n    author = \"Goyal, Tanya  and\n      Xu, Jiacheng  and\n      Li, Junyi Jessy  and\n      Durrett, Greg\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.163/\",\n    doi = \"10.18653/v1/2022.findings-acl.163\",\n    pages = \"2061--2073\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.163.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.163/",
        "pdf_size": 1441773,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=676412402913317382&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science; Department of Computer Science; Department of Linguistics; Department of Computer Science",
        "aff_domain": "utexas.edu; ;utexas.edu;utexas.edu",
        "email": "utexas.edu; ;utexas.edu;utexas.edu",
        "github": "https://github.com/tagoyal/training-dynamics-generation",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Unknown Institution;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science;Department of Linguistics",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.findings-acl.171",
        "title": "Training Text-to-Text Transformers with Privacy Guarantees",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data. Such models are often released to the public so that end users can fine-tune them on a task dataset. While it is common to treat pre-training data as public, it may still contain personally identifiable information (PII), such as names, phone numbers, and copyrighted material. Recent findings show that the capacity of these models allows them to memorize parts of the training data, and suggest differentially private (DP) training as a potential mitigation. While there is recent work on DP fine-tuning of NLP models, the effects of DP pre-training are less well understood: it is not clear how downstream performance is affected by DP pre-training, and whether DP pre-training mitigates some of the memorization concerns. We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE). Moreover, we show that T5\u2019s span corruption is a good defense against data memorization.",
        "author": "Natalia Ponomareva; Jasmijn Bastings; Sergei Vassilvitskii",
        "authorids": "/n/natalia-ponomareva/; /j/jasmijn-bastings/; /s/sergei-vassilvitskii/",
        "bibtex": "@inproceedings{ponomareva-etal-2022-training,\n    title = \"Training Text-to-Text Transformers with Privacy Guarantees\",\n    author = \"Ponomareva, Natalia  and\n      Bastings, Jasmijn  and\n      Vassilvitskii, Sergei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.171/\",\n    doi = \"10.18653/v1/2022.findings-acl.171\",\n    pages = \"2182--2193\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.171.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.171/",
        "pdf_size": 293433,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6287465279352454445&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.128",
        "title": "Transfer Learning and Prediction Consistency for Detecting Offensive Spans of Text",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Toxic span detection is the task of recognizing offensive spans in a text snippet. Although there has been prior work on classifying text snippets as offensive or not, the task of recognizing spans responsible for the toxicity of a text is not explored yet. In this work, we introduce a novel multi-task framework for toxic span detection in which the model seeks to simultaneously predict offensive words and opinion phrases to leverage their inter-dependencies and improve the performance. Moreover, we introduce a novel regularization mechanism to encourage the consistency of the model predictions across similar inputs for toxic span detection. Our extensive experiments demonstrate the effectiveness of the proposed model compared to strong baselines.",
        "author": "Amir Pouran Ben Veyseh; Ning Xu; Quan Tran; Varun Manjunatha; Franck Dernoncourt; Thien Nguyen",
        "authorids": "/a/amir-pouran-ben-veyseh/; /n/ning-xu/; /q/quan-hung-tran/; /v/varun-manjunatha/; /f/franck-dernoncourt/; /t/thien-nguyen/",
        "bibtex": "@inproceedings{pouran-ben-veyseh-etal-2022-transfer,\n    title = \"Transfer Learning and Prediction Consistency for Detecting Offensive Spans of Text\",\n    author = \"Pouran Ben Veyseh, Amir  and\n      Xu, Ning  and\n      Tran, Quan  and\n      Manjunatha, Varun  and\n      Dernoncourt, Franck  and\n      Nguyen, Thien\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.128/\",\n    doi = \"10.18653/v1/2022.findings-acl.128\",\n    pages = \"1630--1637\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.128.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.128/",
        "pdf_size": 269095,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16015890118630418683&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer and Information Science, University of Oregon, Eugene, OR, USA+Adobe Research, San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA",
        "aff_domain": "cs.uoregon.edu;adobe.com;adobe.com;adobe.com;adobe.com;cs.uoregon.edu",
        "email": "cs.uoregon.edu;adobe.com;adobe.com;adobe.com;adobe.com;cs.uoregon.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;0",
        "aff_unique_norm": "University of Oregon;Adobe",
        "aff_unique_dep": "Department of Computer and Information Science;Adobe Research",
        "aff_unique_url": "https://www.uoregon.edu;https://research.adobe.com",
        "aff_unique_abbr": "UO;Adobe",
        "aff_campus_unique_index": "0+1;1;1;1;1;0",
        "aff_campus_unique": "Eugene;San Jose",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.455",
        "title": "Transformers in the loop: Polarity in neural models of language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Representation of linguistic phenomena in computational language models is typically assessed against the predictions of existing linguistic theories of these phenomena. Using the notion of polarity as a case study, we show that this is not always the most adequate set-up. We probe polarity via so-called \u2018negative polarity items\u2019 (in particular, English \u2018any\u2019) in two pre-trained Transformer-based models (BERT and GPT-2). We show that \u2013 at least for polarity \u2013 metrics derived from language models are more consistent with data from psycholinguistic experiments than linguistic theory predictions. Establishing this allows us to more adequately evaluate the performance of language models and also to use language models to discover new insights into natural language grammar beyond existing linguistic theories. This work contributes to establishing closer ties between psycholinguistic experiments and experiments with language models.",
        "author": "Lisa Bylinina; Alexey Tikhonov",
        "authorids": "/l/lisa-bylinina/; /a/alexey-tikhonov/",
        "bibtex": "@inproceedings{bylinina-tikhonov-2022-transformers,\n    title = \"Transformers in the loop: Polarity in neural models of language\",\n    author = \"Bylinina, Lisa  and\n      Tikhonov, Alexey\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.455/\",\n    doi = \"10.18653/v1/2022.acl-long.455\",\n    pages = \"6601--6610\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.455.pdf",
        "site": "https://aclanthology.org/2022.acl-long.455/",
        "pdf_size": 465410,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9410689506680598847&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Bookarang, Amsterdam; Yandex Technologies GmbH, Berlin",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Bookarang;Yandex Technologies GmbH",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://yandex.com",
        "aff_unique_abbr": ";Yandex",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berlin",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Netherlands;Germany"
    },
    {
        "id": "2022.acl-long.502",
        "title": "Transkimmer: Transformer Learns to Layer-wise Skim",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer architecture has become the de-facto model for many machine learning tasks from natural language processing and computer vision. As such, improving its computational efficiency becomes paramount. One of the major computational inefficiency of Transformer based models is that they spend the identical amount of computation throughout all layers. Prior works have proposed to augment the Transformer model with the capability of skimming tokens to improve its computational efficiency. However, they suffer from not having effectual and end-to-end optimization of the discrete skimming predictor. To address the above limitations, we propose the Transkimmer architecture, which learns to identify hidden state tokens that are not required by each layer. The skimmed tokens are then forwarded directly to the final output, thus reducing the computation of the successive layers. The key idea in Transkimmer is to add a parameterized predictor before each layer that learns to make the skimming decision. We also propose to adopt reparameterization trick and add skim loss for the end-to-end training of Transkimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark compared with vanilla BERT-base baseline with less than 1% accuracy degradation.",
        "author": "Yue Guan; Zhengyi Li; Jingwen Leng; Zhouhan Lin; Minyi Guo",
        "authorids": "/y/yue-guan/; /z/zhengyi-li/; /j/jingwen-leng/; /z/zhouhan-lin/; /m/minyi-guo/",
        "bibtex": "@inproceedings{guan-etal-2022-transkimmer,\n    title = \"Transkimmer: Transformer Learns to Layer-wise Skim\",\n    author = \"Guan, Yue  and\n      Li, Zhengyi  and\n      Leng, Jingwen  and\n      Lin, Zhouhan  and\n      Guo, Minyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.502/\",\n    doi = \"10.18653/v1/2022.acl-long.502\",\n    pages = \"7275--7286\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.502.pdf",
        "site": "https://aclanthology.org/2022.acl-long.502/",
        "pdf_size": 489516,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11294799022804004206&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "Shanghai Jiao Tong University+Shanghai Qizhi Institute; Shanghai Jiao Tong University+Shanghai Qizhi Institute; Shanghai Jiao Tong University+Shanghai Qizhi Institute; Shanghai Jiao Tong University; Shanghai Jiao Tong University+Shanghai Qizhi Institute",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;gmail.com;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;gmail.com;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0;0+1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Shanghai Qizhi Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.qizhi-institute.org",
        "aff_unique_abbr": "SJTU;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.40",
        "title": "Translate-Train Embracing Translationese Artifacts",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Translate-train is a general training approach to multilingual tasks. The key idea is to use the translator of the target language to generate training data to mitigate the gap between the source and target languages. However, its performance is often hampered by the artifacts in the translated texts (translationese). We discover that such artifacts have common patterns in different languages and can be modeled by deep learning, and subsequently propose an approach to conduct translate-train using Translationese Embracing the effect of Artifacts (TEA). TEA learns to mitigate such effect on the training data of a source language (whose original and translationese are both available), and applies the learned module to facilitate the inference on the target language. Extensive experiments on the multilingual QA dataset TyDiQA demonstrate that TEA outperforms strong baselines.",
        "author": "Sicheng Yu; Qianru Sun; Hao Zhang; Jing Jiang",
        "authorids": "/s/sicheng-yu/; /q/qianru-sun/; /h/hao-zhang/; /j/jing-jiang/",
        "bibtex": "@inproceedings{yu-etal-2022-translate,\n    title = \"Translate-Train Embracing Translationese Artifacts\",\n    author = \"Yu, Sicheng  and\n      Sun, Qianru  and\n      Zhang, Hao  and\n      Jiang, Jing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.40/\",\n    doi = \"10.18653/v1/2022.acl-short.40\",\n    pages = \"362--370\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.40.pdf",
        "site": "https://aclanthology.org/2022.acl-short.40/",
        "pdf_size": 408864,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10685524677737883132&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Singapore Management University, Singapore; Singapore Management University, Singapore; Nanyang Technological University, Singapore + Centre for Frontier AI Research, A*STAR, Singapore; Singapore Management University, Singapore",
        "aff_domain": "phdcs.smu.edu.sg;smu.edu.sg;outlook.com;smu.edu.sg",
        "email": "phdcs.smu.edu.sg;smu.edu.sg;outlook.com;smu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1+2;0",
        "aff_unique_norm": "Singapore Management University;Nanyang Technological University;A*STAR",
        "aff_unique_dep": ";;Centre for Frontier AI Research",
        "aff_unique_url": "https://www.smu.edu.sg;https://www.ntu.edu.sg;https://www.a-star.edu.sg",
        "aff_unique_abbr": "SMU;NTU;A*STAR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2022.findings-acl.327",
        "title": "Translation Error Detection as Rationale Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent Quality Estimation (QE) models based on multilingual pre-trained representations have achieved very competitive results in predicting the overall quality of translated sentences. However, detecting specifically which translated words are incorrect is a more challenging task, especially when dealing with limited amounts of training data. We hypothesize that, not unlike humans, successful QE models rely on translation errors to predict overall sentence quality. By exploring a set of feature attribution methods that assign relevance scores to the inputs to explain model predictions, we study the behaviour of state-of-the-art sentence-level QE models and show that explanations (i.e. rationales) extracted from these models can indeed be used to detect translation errors. We therefore (i) introduce a novel semi-supervised method for word-level QE; and (ii) propose to use the QE task as a new benchmark for evaluating the plausibility of feature attribution, i.e. how interpretable model explanations are to humans.",
        "author": "Marina Fomicheva; Lucia Specia; Nikolaos Aletras",
        "authorids": "/m/marina-fomicheva/; /l/lucia-specia/; /n/nikolaos-aletras/",
        "bibtex": "@inproceedings{fomicheva-etal-2022-translation,\n    title = \"Translation Error Detection as Rationale Extraction\",\n    author = \"Fomicheva, Marina  and\n      Specia, Lucia  and\n      Aletras, Nikolaos\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.327/\",\n    doi = \"10.18653/v1/2022.findings-acl.327\",\n    pages = \"4148--4159\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.327.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.327/",
        "pdf_size": 399491,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13581231536775392328&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Sheffield; Imperial College London; University of Sheffield",
        "aff_domain": "sheffield.ac.uk;imperial.ac.uk;sheffield.ac.uk",
        "email": "sheffield.ac.uk;imperial.ac.uk;sheffield.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Sheffield;Imperial College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.imperial.ac.uk",
        "aff_unique_abbr": "Sheffield;ICL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-short.72",
        "title": "Triangular Transfer: Freezing the Pivot for Triangular Machine Translation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Triangular machine translation is a special case of low-resource machine translation where the language pair of interest has limited parallel data, but both languages have abundant parallel data with a pivot language. Naturally, the key to triangular machine translation is the successful exploitation of such auxiliary data. In this work, we propose a transfer-learning-based approach that utilizes all types of auxiliary data. As we train auxiliary source-pivot and pivot-target translation models, we initialize some parameters of the pivot side with a pre-trained language model and freeze them to encourage both translation models to work in the same pivot language space, so that they can be smoothly transferred to the source-target translation model. Experiments show that our approach can outperform previous ones.",
        "author": "Meng Zhang; Liangyou Li; Qun Liu",
        "authorids": "/m/meng-zhang/; /l/liangyou-li/; /q/qun-liu/",
        "bibtex": "@inproceedings{zhang-etal-2022-triangular,\n    title = \"Triangular Transfer: Freezing the Pivot for Triangular Machine Translation\",\n    author = \"Zhang, Meng  and\n      Li, Liangyou  and\n      Liu, Qun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.72/\",\n    doi = \"10.18653/v1/2022.acl-short.72\",\n    pages = \"644--650\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.72.pdf",
        "site": "https://aclanthology.org/2022.acl-short.72/",
        "pdf_size": 262504,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6181127366343393762&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "huawei.com;huawei.com;huawei.com",
        "email": "huawei.com;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Huawei",
        "aff_unique_dep": "Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.huawei.com",
        "aff_unique_abbr": "Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.229",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
        "author": "Stephanie Lin; Jacob Hilton; Owain Evans",
        "authorids": "/s/stephanie-lin/; /j/jacob-hilton/; /o/owain-evans/",
        "bibtex": "@inproceedings{lin-etal-2022-truthfulqa,\n    title = \"{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods\",\n    author = \"Lin, Stephanie  and\n      Hilton, Jacob  and\n      Evans, Owain\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.229/\",\n    doi = \"10.18653/v1/2022.acl-long.229\",\n    pages = \"3214--3252\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.229.pdf",
        "site": "https://aclanthology.org/2022.acl-long.229/",
        "pdf_size": 668807,
        "gs_citation": 1725,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=204624588678524687&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Oxford; OpenAI; University of Oxford",
        "aff_domain": "gmail.com;openai.com;gmail.com",
        "email": "gmail.com;openai.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Oxford;OpenAI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ox.ac.uk;https://openai.com",
        "aff_unique_abbr": "Oxford;OpenAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2022.acl-long.416",
        "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning. In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs. We evaluate our approach on three reasoning-focused reading comprehension datasets, and show that our model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling examples based on model errors leads to faster training and higher performance.",
        "author": "Ori Yoran; Alon Talmor; Jonathan Berant",
        "authorids": "/o/ori-yoran/; /a/alon-talmor/; /j/jonathan-berant/",
        "bibtex": "@inproceedings{yoran-etal-2022-turning,\n    title = \"Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills\",\n    author = \"Yoran, Ori  and\n      Talmor, Alon  and\n      Berant, Jonathan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.416/\",\n    doi = \"10.18653/v1/2022.acl-long.416\",\n    pages = \"6016--6031\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.416.pdf",
        "site": "https://aclanthology.org/2022.acl-long.416/",
        "pdf_size": 1700828,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17247191150864127958&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Tel-Aviv University\u2020; Tel-Aviv University; Tel-Aviv University",
        "aff_domain": "cs.tau.ac.il;cs.tau.ac.il;cs.tau.ac.il",
        "email": "cs.tau.ac.il;cs.tau.ac.il;cs.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2022.acl-long.473",
        "title": "TwittIrish: A Universal Dependencies Treebank of Tweets in Modern Irish",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modern Irish is a minority language lacking sufficient computational resources for the task of accurate automatic syntactic parsing of user-generated content such as tweets. Although language technology for the Irish language has been developing in recent years, these tools tend to perform poorly on user-generated content. As with other languages, the linguistic style observed in Irish tweets differs, in terms of orthography, lexicon, and syntax, from that of standard texts more commonly used for the development of language models and parsers. We release the first Universal Dependencies treebank of Irish tweets, facilitating natural language processing of user-generated content in Irish. In this paper, we explore the differences between Irish tweets and standard Irish text, and the challenges associated with dependency parsing of Irish tweets. We describe our bootstrapping method of treebank development and report on preliminary parsing experiments.",
        "author": "Lauren Cassidy; Teresa Lynn; James Barry; Jennifer Foster",
        "authorids": "/l/lauren-cassidy/; /t/teresa-lynn/; /j/james-barry/; /j/jennifer-foster/",
        "bibtex": "@inproceedings{cassidy-etal-2022-twittirish,\n    title = \"{T}witt{I}rish: A {U}niversal {D}ependencies Treebank of Tweets in {M}odern {I}rish\",\n    author = \"Cassidy, Lauren  and\n      Lynn, Teresa  and\n      Barry, James  and\n      Foster, Jennifer\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.473/\",\n    doi = \"10.18653/v1/2022.acl-long.473\",\n    pages = \"6869--6884\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.473.pdf",
        "site": "https://aclanthology.org/2022.acl-long.473/",
        "pdf_size": 454397,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6221419260754518007&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computing, Dublin City University + ADAPT Centre; School of Computing, Dublin City University + ADAPT Centre; School of Computing, Dublin City University + ADAPT Centre; School of Computing, Dublin City University",
        "aff_domain": "adaptcentre.ie;adaptcentre.ie;adaptcentre.ie;dcu.ie",
        "email": "adaptcentre.ie;adaptcentre.ie;adaptcentre.ie;dcu.ie",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0",
        "aff_unique_norm": "Dublin City University;ADAPT Centre",
        "aff_unique_dep": "School of Computing;",
        "aff_unique_url": "https://www.dcu.ie;https://adaptcentre.ie",
        "aff_unique_abbr": "DCU;ADAPT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Dublin;",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "Ireland"
    },
    {
        "id": "2022.findings-acl.274",
        "title": "Two Birds with One Stone: Unified Model Learning for Both Recall and Ranking in News Recommendation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recall and ranking are two critical steps in personalized news recommendation. Most existing news recommender systems conduct personalized news recall and ranking separately with different models. However, maintaining multiple models leads to high computational cost and poses great challenges to meeting the online latency requirement of news recommender systems. In order to handle this problem, in this paper we propose UniRec, a unified method for recall and ranking in news recommendation. In our method, we first infer user embedding for ranking from the historical news click behaviors of a user using a user encoder model. Then we derive the user embedding for recall from the obtained user embedding for ranking by using it as the attention query to select a set of basis user embeddings which encode different general user interests and synthesize them into a user embedding for recall. The extensive experiments on benchmark dataset demonstrate that our method can improve both efficiency and effectiveness for recall and ranking in news recommendation.",
        "author": "Chuhan Wu; Fangzhao Wu; Tao Qi; Yongfeng Huang",
        "authorids": "/c/chuhan-wu/; /f/fangzhao-wu/; /t/tao-qi/; /y/yongfeng-huang/",
        "bibtex": "@inproceedings{wu-etal-2022-two,\n    title = \"Two Birds with One Stone: Unified Model Learning for Both Recall and Ranking in News Recommendation\",\n    author = \"Wu, Chuhan  and\n      Wu, Fangzhao  and\n      Qi, Tao  and\n      Huang, Yongfeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.274/\",\n    doi = \"10.18653/v1/2022.findings-acl.274\",\n    pages = \"3474--3480\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.274.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.274/",
        "pdf_size": 638914,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1876709530600989543&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electronic Engineering, Tsinghua University, Beijing 100084, China+Microsoft Research Asia, Beijing 100080, China; Microsoft Research Asia, Beijing 100080, China; Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Department of Electronic Engineering, Tsinghua University, Beijing 100084, China",
        "aff_domain": "gmail.com;gmail.com;gmail.com;tsinghua.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "Tsinghua University;Microsoft",
        "aff_unique_dep": "Department of Electronic Engineering;Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "THU;MSRA",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.117",
        "title": "Two-Step Question Retrieval for Open-Domain QA",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The retriever-reader pipeline has shown promising performance in open-domain QA but suffers from a very slow inference speed. Recently proposed question retrieval models tackle this problem by indexing question-answer pairs and searching for similar questions. These models have shown a significant increase in inference speed, but at the cost of lower QA performance compared to the retriever-reader models. This paper proposes a two-step question retrieval model, SQuID (Sequential Question-Indexed Dense retrieval) and distant supervision for training. SQuID uses two bi-encoders for question retrieval. The first-step retriever selects top-k similar questions, and the second-step retriever finds the most similar question from the top-k questions. We evaluate the performance and the computational efficiency of SQuID. The results show that SQuID significantly increases the performance of existing question retrieval models with a negligible loss on inference speed.",
        "author": "Yeon Seonwoo; Juhee Son; Jiho Jin; Sang-Woo Lee; Ji-Hoon Kim; Jung-Woo Ha; Alice Oh",
        "authorids": "/y/yeon-seonwoo/; /j/juhee-son/; /j/jiho-jin/; /s/sang-woo-lee/; /j/ji-hoon-kim/; /j/jung-woo-ha/; /a/alice-oh/",
        "bibtex": "@inproceedings{seonwoo-etal-2022-two,\n    title = \"Two-Step Question Retrieval for Open-Domain {QA}\",\n    author = \"Seonwoo, Yeon  and\n      Son, Juhee  and\n      Jin, Jiho  and\n      Lee, Sang-Woo  and\n      Kim, Ji-Hoon  and\n      Ha, Jung-Woo  and\n      Oh, Alice\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.117/\",\n    doi = \"10.18653/v1/2022.findings-acl.117\",\n    pages = \"1487--1492\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.117.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.117/",
        "pdf_size": 388235,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17528134278957955459&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "KAIST; KAIST; KAIST; NA VER AI Lab+NA VER CLOV A; NA VER AI Lab+NA VER CLOV A; NA VER AI Lab+NA VER CLOV A; KAIST",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;navercorp.com;navercorp.com;navercorp.com;kaist.edu",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;navercorp.com;navercorp.com;navercorp.com;kaist.edu",
        "github": "https://github.com/yeonsw/SQuID.git",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;1;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;NAVER Corporation;",
        "aff_unique_dep": ";AI Lab;",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.naver.com;",
        "aff_unique_abbr": "KAIST;NAVER;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "2022.findings-acl.254",
        "title": "Type-Driven Multi-Turn Corrections for Grammatical Error Correction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Grammatical Error Correction (GEC) aims to automatically detect and correct grammatical errors. In this aspect, dominant models are trained by one-iteration learning while performing multiple iterations of corrections during inference. Previous studies mainly focus on the data augmentation approach to combat the exposure bias, which suffers from two drawbacks. First, they simply mix additionally-constructed training instances and original ones to train models, which fails to help models be explicitly aware of the procedure of gradual corrections. Second, they ignore the interdependence between different types of corrections. In this paper, we propose a Type-Driven Multi-Turn Corrections approach for GEC. Using this approach, from each training instance, we additionally construct multiple training instances, each of which involves the correction of a specific type of errors. Then, we use these additionally-constructed training instances and the original one to train the model in turn. Experimental results and in-depth analysis show that our approach significantly benefits the model training. Particularly, our enhanced model achieves state-of-the-art single-model performance on English GEC benchmarks. We release our code at Github.",
        "author": "Shaopeng Lai; Qingyu Zhou; Jiali Zeng; Zhongli Li; Chao Li; Yunbo Cao; Jinsong Su",
        "authorids": "/s/shaopeng-lai/; /q/qingyu-zhou/; /j/jiali-zeng/; /z/zhongli-li/; /c/chao-li/; /y/yunbo-cao/; /j/jinsong-su/",
        "bibtex": "@inproceedings{lai-etal-2022-type,\n    title = \"Type-Driven Multi-Turn Corrections for Grammatical Error Correction\",\n    author = \"Lai, Shaopeng  and\n      Zhou, Qingyu  and\n      Zeng, Jiali  and\n      Li, Zhongli  and\n      Li, Chao  and\n      Cao, Yunbo  and\n      Su, Jinsong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.254/\",\n    doi = \"10.18653/v1/2022.findings-acl.254\",\n    pages = \"3225--3236\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.254.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.254/",
        "pdf_size": 398255,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6190565201928291020&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Informatics, Xiamen University, China; Tencent Cloud Xiaowei, China; Tencent Cloud Xiaowei, China; Tencent Cloud Xiaowei, China; Tencent Cloud Xiaowei, China; Tencent Cloud Xiaowei, China; School of Informatics, Xiamen University, China+Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism, China",
        "aff_domain": "stu.xmu.edu.cn;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;xmu.edu.cn",
        "email": "stu.xmu.edu.cn;tencent.com;tencent.com;tencent.com;tencent.com;tencent.com;xmu.edu.cn",
        "github": "https://github.com/DeepLearnXMU/TMTC",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;0+2",
        "aff_unique_norm": "Xiamen University;Tencent;Ministry of Culture and Tourism",
        "aff_unique_dep": "School of Informatics;Tencent Cloud Xiaowei;Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan",
        "aff_unique_url": "https://www.xmu.edu.cn;https://cloud.tencent.com;",
        "aff_unique_abbr": "XMU;Tencent;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.426",
        "title": "UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "High-quality phrase representations are essential to finding topics and related terms in documents (a.k.a. topic mining). Existing phrase representation learning methods either simply combine unigram representations in a context-free manner or rely on extensive annotations to learn context-aware knowledge. In this paper, we propose UCTopic, a novel unsupervised contrastive learning framework for context-aware phrase representations and topic mining. UCTopic is pretrained in a large scale to distinguish if the contexts of two phrase mentions have the same semantics. The key to the pretraining is positive pair construction from our phrase-oriented assumptions. However, we find traditional in-batch negatives cause performance decay when finetuning on a dataset with small topic numbers. Hence, we propose cluster-assisted contrastive learning (CCL) which largely reduces noisy negatives by selecting negatives from clusters and further improves phrase representations for topics accordingly. UCTopic outperforms the state-of-the-art phrase representation model by 38.2% NMI in average on four entity clustering tasks. Comprehensive evaluation on topic mining shows that UCTopic can extract coherent and diverse topical phrases.",
        "author": "Jiacheng Li; Jingbo Shang; Julian McAuley",
        "authorids": "/j/jiacheng-li/; /j/jingbo-shang/; /j/julian-mcauley/",
        "bibtex": "@inproceedings{li-etal-2022-uctopic,\n    title = \"{UCT}opic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining\",\n    author = \"Li, Jiacheng  and\n      Shang, Jingbo  and\n      McAuley, Julian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.426/\",\n    doi = \"10.18653/v1/2022.acl-long.426\",\n    pages = \"6159--6169\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.426.pdf",
        "site": "https://aclanthology.org/2022.acl-long.426/",
        "pdf_size": 467490,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4715936120895040384&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, San Diego; University of California, San Diego; University of California, San Diego",
        "aff_domain": "eng.ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "email": "eng.ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.251",
        "title": "UNIMO-2: End-to-End Unified Vision-Language Grounded Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end unified-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. We build a unified Transformer model to jointly learn visual representations, textual representations and semantic alignment between images and texts. In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts, and align the visual and textual semantic spaces on different types of corpora. The experiments show that our grounded learning method can improve textual and visual semantic alignment for improving performance on various cross-modal tasks. Moreover, benefiting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. Our code and models are public at the UNIMO project page https://unimo-ptm.github.io/.",
        "author": "Wei Li; Can Gao; Guocheng Niu; Xinyan Xiao; Hao Liu; Jiachen Liu; Hua Wu; Haifeng Wang",
        "authorids": "/w/wei-li/; /c/can-gao/; /g/guocheng-niu/; /x/xinyan-xiao/; /h/hao-liu/; /j/jiachen-liu/; /h/hua-wu/; /h/haifeng-wang/",
        "bibtex": "@inproceedings{li-etal-2022-unimo,\n    title = \"{UNIMO}-2: End-to-End Unified Vision-Language Grounded Learning\",\n    author = \"Li, Wei  and\n      Gao, Can  and\n      Niu, Guocheng  and\n      Xiao, Xinyan  and\n      Liu, Hao  and\n      Liu, Jiachen  and\n      Wu, Hua  and\n      Wang, Haifeng\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.251/\",\n    doi = \"10.18653/v1/2022.findings-acl.251\",\n    pages = \"3187--3201\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.251.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.251/",
        "pdf_size": 3141624,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10406856756840324837&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "",
        "project": "https://unimo-ptm.github.io/",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Baidu Inc.",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.591",
        "title": "Uncertainty Determines the Adequacy of the Mode and the Tractability of Decoding in Sequence-to-Sequence Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In many natural language processing (NLP) tasks the same input (e.g. source sentence) can have multiple possible outputs (e.g. translations). To analyze how this ambiguity (also known as intrinsic uncertainty) shapes the distribution learned by neural sequence models we measure sentence-level uncertainty by computing the degree of overlap between references in multi-reference test sets from two different NLP tasks: machine translation (MT) and grammatical error correction (GEC). At both the sentence- and the task-level, intrinsic uncertainty has major implications for various aspects of search such as the inductive biases in beam search and the complexity of exact search. In particular, we show that well-known pathologies such as a high number of beam search errors, the inadequacy of the mode, and the drop in system performance with large beam sizes apply to tasks with high level of ambiguity such as MT but not to less uncertain tasks such as GEC. Furthermore, we propose a novel exact n-best search algorithm for neural sequence models, and show that intrinsic uncertainty affects model uncertainty as the model tends to overly spread out the probability mass for uncertain tasks and sentences.",
        "author": "Felix Stahlberg; Ilia Kulikov; Shankar Kumar",
        "authorids": "/f/felix-stahlberg/; /i/ilia-kulikov/; /s/shankar-kumar/",
        "bibtex": "@inproceedings{stahlberg-etal-2022-uncertainty,\n    title = \"Uncertainty Determines the Adequacy of the Mode and the Tractability of Decoding in Sequence-to-Sequence Models\",\n    author = \"Stahlberg, Felix  and\n      Kulikov, Ilia  and\n      Kumar, Shankar\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.591/\",\n    doi = \"10.18653/v1/2022.acl-long.591\",\n    pages = \"8634--8645\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.591.pdf",
        "site": "https://aclanthology.org/2022.acl-long.591/",
        "pdf_size": 655867,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3840412862531559155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Google Research; New York University + Google Research; Google Research",
        "aff_domain": "google.com;nyu.edu;google.com",
        "email": "google.com;nyu.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Google;New York University",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.nyu.edu",
        "aff_unique_abbr": "Google Research;NYU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.566",
        "title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.",
        "author": "Artem Vazhentsev; Gleb Kuzmin; Artem Shelmanov; Akim Tsvigun; Evgenii Tsymbalov; Kirill Fedyanin; Maxim Panov; Alexander Panchenko; Gleb Gusev; Mikhail Burtsev; Manvel Avetisian; Leonid Zhukov",
        "authorids": "/a/artem-vazhentsev/; /g/gleb-kuzmin/; /a/artem-shelmanov/; /a/akim-tsvigun/; /e/evgenii-tsymbalov/; /k/kirill-fedyanin/; /m/maxim-panov/; /a/alexander-panchenko/; /g/gleb-gusev/; /m/mikhail-burtsev/; /m/manvel-avetisian/; /l/leonid-zhukov/",
        "bibtex": "@inproceedings{vazhentsev-etal-2022-uncertainty,\n    title = \"Uncertainty Estimation of Transformer Predictions for Misclassification Detection\",\n    author = \"Vazhentsev, Artem  and\n      Kuzmin, Gleb  and\n      Shelmanov, Artem  and\n      Tsvigun, Akim  and\n      Tsymbalov, Evgenii  and\n      Fedyanin, Kirill  and\n      Panov, Maxim  and\n      Panchenko, Alexander  and\n      Gusev, Gleb  and\n      Burtsev, Mikhail  and\n      Avetisian, Manvel  and\n      Zhukov, Leonid\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.566/\",\n    doi = \"10.18653/v1/2022.acl-long.566\",\n    pages = \"8237--8252\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.566.pdf",
        "site": "https://aclanthology.org/2022.acl-long.566/",
        "pdf_size": 479509,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6568956768917328417&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "AIRI+Skoltech+MIPT+HSE+Sber AI Lab; AIRI+FRC CSC RAS+Skoltech+MIPT+HSE+Sber AI Lab; AIRI+ISP RAS Research Center for Trusted Artificial Intelligence+Skoltech+MIPT+HSE+Sber AI Lab; AIRI+HSE+Skoltech+MIPT+Sber AI Lab; Skoltech+MIPT+HSE+Sber AI Lab; Skoltech+MIPT+HSE+Sber AI Lab; Skoltech+MIPT+HSE+Sber AI Lab; Skoltech+MIPT+HSE+Sber AI Lab; AIRI+Sber AI Lab+MIPT+HSE; AIRI+Sber AI Lab+MIPT+HSE; AIRI+Sber AI Lab+MIPT+HSE; AIRI+HSE+MIPT+Sber AI Lab",
        "aff_domain": "airi.net;airi.net;airi.net;airi.net;skoltech.ru;skoltech.ru;skoltech.ru;skoltech.ru;airi.net;airi.net;airi.net;airi.net",
        "email": "airi.net;airi.net;airi.net;airi.net;skoltech.ru;skoltech.ru;skoltech.ru;skoltech.ru;airi.net;airi.net;airi.net;airi.net",
        "github": "https://github.com/AIRI-Institute/uncertainty_transformers",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0+1+2+3+4;0+5+1+2+3+4;0+6+1+2+3+4;0+3+1+2+4;1+2+3+4;1+2+3+4;1+2+3+4;1+2+3+4;0+4+2+3;0+4+2+3;0+4+2+3;0+3+2+4",
        "aff_unique_norm": "Artificial Intelligence Research Institute;Skolkovo Institute of Science and Technology;Moscow Institute of Physics and Technology;Higher School of Economics;Sberbank;Russian Academy of Sciences;Institute for System Programming of the Russian Academy of Sciences",
        "aff_unique_dep": ";;;;Sber AI Lab;Computer Science Center;Research Center for Trusted Artificial Intelligence",
        "aff_unique_url": "https://www.airi.jp;https://www.skoltech.ru;https://mipt.ru;https://www.hse.ru;https://sberbank.ru;https://csc.ras.ru;http://www.ispras.ru",
        "aff_unique_abbr": "AIRI;Skoltech;MIPT;HSE;Sber;RAS;ISP RAS",
        "aff_campus_unique_index": ";;;;;;;;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1+1+1+1;0+1+1+1+1+1;0+1+1+1+1+1;0+1+1+1+1;1+1+1+1;1+1+1+1;1+1+1+1;1+1+1+1;0+1+1+1;0+1+1+1;0+1+1+1;0+1+1+1",
        "aff_country_unique": "Japan;Russian Federation"
    },
    {
        "id": "2022.acl-long.127",
        "title": "Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Gender bias is largely recognized as a problematic phenomenon affecting language technologies, with recent studies underscoring that it might surface differently across languages. However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions. Such protocols overlook key features of grammatical gender languages, which are characterized by morphosyntactic chains of gender agreement, marked on a variety of lexical items and parts-of-speech (POS). To overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS and agreement chains), and explore to what extent different lexical categories and agreement phenomena are impacted by gender skews. Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French/Italian/Spanish), with models trained on varying amounts of data and different word segmentation techniques. By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.",
        "author": "Beatrice Savoldi; Marco Gaido; Luisa Bentivogli; Matteo Negri; Marco Turchi",
        "authorids": "/b/beatrice-savoldi/; /m/marco-gaido/; /l/luisa-bentivogli/; /m/matteo-negri/; /m/marco-turchi/",
        "bibtex": "@inproceedings{savoldi-etal-2022-morphosyntactic,\n    title = \"Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation\",\n    author = \"Savoldi, Beatrice  and\n      Gaido, Marco  and\n      Bentivogli, Luisa  and\n      Negri, Matteo  and\n      Turchi, Marco\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.127/\",\n    doi = \"10.18653/v1/2022.acl-long.127\",\n    pages = \"1807--1824\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.127.pdf",
        "site": "https://aclanthology.org/2022.acl-long.127/",
        "pdf_size": 526293,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3524388029806568294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Trento + Fondazione Bruno Kessler; University of Trento + Fondazione Bruno Kessler; Fondazione Bruno Kessler; Fondazione Bruno Kessler; Fondazione Bruno Kessler",
        "aff_domain": "unitn.it;fbk.eu;fbk.eu;fbk.eu;fbk.eu",
        "email": "unitn.it;fbk.eu;fbk.eu;fbk.eu;fbk.eu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;1;1;1",
        "aff_unique_norm": "University of Trento;Fondazione Bruno Kessler",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unitn.it;https://www.fbk.eu",
        "aff_unique_abbr": "UniTN;FBK",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2022.acl-short.90",
        "title": "Understanding Game-Playing Agents with Natural Language Annotations",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We present a new dataset containing 10K human-annotated games of Go and show how these natural language annotations can be used as a tool for model interpretability. Given a board state and its associated comment, our approach uses linear probing to predict mentions of domain-specific terms (e.g., ko, atari) from the intermediate state representations of game-playing agents like AlphaGo Zero. We find these game concepts are nontrivially encoded in two distinct policy networks, one trained via imitation learning and another trained via reinforcement learning. Furthermore, mentions of domain-specific terms are most easily predicted from the later layers of both models, suggesting that these policy networks encode high-level abstractions similar to those used in the natural language annotations.",
        "author": "Nicholas Tomlin; Andre He; Dan Klein",
        "authorids": "/n/nicholas-tomlin/; /a/andre-he/; /d/dan-klein/",
        "bibtex": "@inproceedings{tomlin-etal-2022-understanding,\n    title = \"Understanding Game-Playing Agents with Natural Language Annotations\",\n    author = \"Tomlin, Nicholas  and\n      He, Andre  and\n      Klein, Dan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.90/\",\n    doi = \"10.18653/v1/2022.acl-short.90\",\n    pages = \"797--807\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.90.pdf",
        "site": "https://aclanthology.org/2022.acl-short.90/",
        "pdf_size": 1972218,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3850974282249166442&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.98",
        "title": "Understanding Gender Bias in Knowledge Base Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge base (KB) embeddings have been shown to contain gender biases. In this paper, we study two questions regarding these biases: how to quantify them, and how to trace their origins in KB? Specifically, first, we develop two novel bias measures respectively for a group of person entities and an individual person entity. Evidence of their validity is observed by comparison with real-world census data. Second, we use the influence function to inspect the contribution of each triple in KB to the overall group bias. To exemplify the potential applications of our study, we also present two strategies (by adding and removing KB triples) to mitigate gender biases in KB embeddings.",
        "author": "Yupei Du; Qi Zheng; Yuanbin Wu; Man Lan; Yan Yang; Meirong Ma",
        "authorids": "/y/yupei-du/; /q/qi-zheng/; /y/yuanbin-wu/; /m/man-lan/; /y/yan-yang/; /m/meirong-ma/",
        "bibtex": "@inproceedings{du-etal-2022-understanding,\n    title = \"Understanding Gender Bias in Knowledge Base Embeddings\",\n    author = \"Du, Yupei  and\n      Zheng, Qi  and\n      Wu, Yuanbin  and\n      Lan, Man  and\n      Yang, Yan  and\n      Ma, Meirong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.98/\",\n    doi = \"10.18653/v1/2022.acl-long.98\",\n    pages = \"1381--1395\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.98.pdf",
        "site": "https://aclanthology.org/2022.acl-long.98/",
        "pdf_size": 1154514,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17455491691533661266&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Information and Computing Sciences, Utrecht University, the Netherlands; Department of Computer Science and Technology, East China Normal University, China; Department of Computer Science and Technology, East China Normal University, China; Department of Computer Science and Technology, East China Normal University, China; Department of Computer Science and Technology, East China Normal University, China+Shanghai Key Laboratory of Multidimensional Information Processing, China; Shanghai Transsion Co., Ltd, China",
        "aff_domain": "uu.nl;outlook.com;cs.ecnu.edu.cn;cs.ecnu.edu.cn;cs.ecnu.edu.cn;transsion.com",
        "email": "uu.nl;outlook.com;cs.ecnu.edu.cn;cs.ecnu.edu.cn;cs.ecnu.edu.cn;transsion.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1+2;3",
        "aff_unique_norm": "Utrecht University;East China Normal University;Shanghai Key Laboratory of Multidimensional Information Processing;Shanghai Transsion Co., Ltd",
        "aff_unique_dep": "Department of Information and Computing Sciences;Department of Computer Science and Technology;;",
        "aff_unique_url": "https://www.uu.nl;http://www.ecnu.edu.cn;;",
        "aff_unique_abbr": "UU;ECNU;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1+1;1",
        "aff_country_unique": "Netherlands;China"
    },
    {
        "id": "2022.acl-long.250",
        "title": "Understanding Iterative Revision from Human-Written Text",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process. A crucial part of writing is editing and revising the text. Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits, which differ from human\u2019s revision cycles. This work describes IteraTeR: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text. In particular, IteraTeR is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edit intentions, revision depths, and granularities. When we incorporate our annotated edit intentions, both generative and action-based text revision models significantly improve automatic evaluations. Through our work, we better understand the text revision process, making vital connections between edit intentions and writing quality, enabling the creation of diverse corpora to support computational modeling of iterative text revisions.",
        "author": "Wanyu Du; Vipul Raheja; Dhruv Kumar; Zae Myung Kim; Melissa Lopez; Dongyeop Kang",
        "authorids": "/w/wanyu-du/; /v/vipul-raheja/; /d/dhruv-kumar/; /z/zae-myung-kim/; /m/melissa-lopez/; /d/dongyeop-kang/",
        "bibtex": "@inproceedings{du-etal-2022-understanding-iterative,\n    title = \"Understanding Iterative Revision from Human-Written Text\",\n    author = \"Du, Wanyu  and\n      Raheja, Vipul  and\n      Kumar, Dhruv  and\n      Kim, Zae Myung  and\n      Lopez, Melissa  and\n      Kang, Dongyeop\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.250/\",\n    doi = \"10.18653/v1/2022.acl-long.250\",\n    pages = \"3573--3590\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.250.pdf",
        "site": "https://aclanthology.org/2022.acl-long.250/",
        "pdf_size": 1690444,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16321930607682723821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Virginia; Grammarly; Grammarly; Univ. Grenoble Alpes, CNRS, LIG; Grammarly; University of Minnesota",
        "aff_domain": "virginia.edu;grammarly.com;grammarly.com;univ-grenoble-alpes.fr;grammarly.com;umn.edu",
        "email": "virginia.edu;grammarly.com;grammarly.com;univ-grenoble-alpes.fr;grammarly.com;umn.edu",
        "github": "https://github.com/vipulraheja/IteraTeR",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;2;1;3",
        "aff_unique_norm": "University of Virginia;Grammarly;Universit\u00e9 Grenoble Alpes;University of Minnesota",
        "aff_unique_dep": ";;Laboratoire d'Informatique de Grenoble (LIG);",
        "aff_unique_url": "https://www.virginia.edu;https://www.grammarly.com;https://www.univ-grenoble-alpes.fr;https://www.minnesota.edu",
        "aff_unique_abbr": "UVA;Grammarly;UGA;UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "2022.acl-long.310",
        "title": "Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The ability to sequence unordered events is evidence of comprehension and reasoning about real world tasks/procedures. It is essential for applications such as task planning and multi-source instruction summarization. It often requires thorough understanding of temporal common sense and multimodal information, since these procedures are often conveyed by a combination of texts and images. While humans are capable of reasoning about and sequencing unordered procedural instructions, the extent to which the current machine learning methods possess such capability is still an open question. In this work, we benchmark models\u2019 capability of reasoning over and sequencing unordered multimodal instructions by curating datasets from online instructional manuals and collecting comprehensive human annotations. We find current state-of-the-art models not only perform significantly worse than humans but also seem incapable of efficiently utilizing multimodal information. To improve machines\u2019 performance on multimodal event sequencing, we propose sequence-aware pretraining techniques exploiting the sequential alignment properties of both texts and images, resulting in > 5% improvements on perfect match ratio.",
        "author": "Te-Lin Wu; Alex Spangher; Pegah Alipoormolabashi; Marjorie Freedman; Ralph Weischedel; Nanyun Peng",
        "authorids": "/t/te-lin-wu/; /a/alex-spangher/; /p/pegah-alipoormolabashi/; /m/marjorie-freedman/; /r/ralph-weischedel/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{wu-etal-2022-understanding,\n    title = \"Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals\",\n    author = \"Wu, Te-Lin  and\n      Spangher, Alex  and\n      Alipoormolabashi, Pegah  and\n      Freedman, Marjorie  and\n      Weischedel, Ralph  and\n      Peng, Nanyun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.310/\",\n    doi = \"10.18653/v1/2022.acl-long.310\",\n    pages = \"4525--4542\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.310.pdf",
        "site": "https://aclanthology.org/2022.acl-long.310/",
        "pdf_size": 6627040,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1001524930938732974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, Los Angeles; ISI, University of Southern California; Sharif University of Technology; ISI, University of Southern California; ISI, University of Southern California; University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;isi.edu;gmail.com;isi.edu;isi.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;isi.edu;gmail.com;isi.edu;isi.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;1;1;0",
        "aff_unique_norm": "University of California, Los Angeles;University of Southern California;Sharif University of Technology",
        "aff_unique_dep": ";Information Sciences Institute;",
        "aff_unique_url": "https://www.ucla.edu;https://www.isi.usc.edu;https://www.sharif.edu",
        "aff_unique_abbr": "UCLA;USC ISI;SUT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "United States;Iran"
    },
    {
        "id": "2022.acl-long.185",
        "title": "Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we present a substantial step in better understanding the SOTA sequence-to-sequence (Seq2Seq) pretraining for neural machine translation (NMT). We focus on studying the impact of the jointly pretrained decoder, which is the main difference between Seq2Seq pretraining and previous encoder-based pretraining approaches for NMT. By carefully designing experiments on three language pairs, we find that Seq2Seq pretraining is a double-edged sword: On one hand, it helps NMT models to produce more diverse translations and reduce adequacy-related translation errors. On the other hand, the discrepancies between Seq2Seq pretraining and NMT finetuning limit the translation quality (i.e., domain discrepancy) and induce the over-estimation issue (i.e., objective discrepancy). Based on these observations, we further propose simple and effective strategies, named in-domain pretraining and input adaptation to remedy the domain and objective discrepancies, respectively. Experimental results on several language pairs show that our approach can consistently improve both translation performance and model robustness upon Seq2Seq pretraining.",
        "author": "Wenxuan Wang; Wenxiang Jiao; Yongchang Hao; Xing Wang; Shuming Shi; Zhaopeng Tu; Michael Lyu",
        "authorids": "/w/wenxuan-wang/; /w/wenxiang-jiao/; /y/yongchang-hao/; /x/xing-wang/; /s/shuming-shi/; /z/zhaopeng-tu/; /m/michael-lyu/",
        "bibtex": "@inproceedings{wang-etal-2022-understanding,\n    title = \"Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation\",\n    author = \"Wang, Wenxuan  and\n      Jiao, Wenxiang  and\n      Hao, Yongchang  and\n      Wang, Xing  and\n      Shi, Shuming  and\n      Tu, Zhaopeng  and\n      Lyu, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.185/\",\n    doi = \"10.18653/v1/2022.acl-long.185\",\n    pages = \"2591--2600\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.185.pdf",
        "site": "https://aclanthology.org/2022.acl-long.185/",
        "pdf_size": 504717,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5639784589572414830&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, The Chinese University of Hong Kong; Tencent AI Lab; University of Alberta + Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Department of Computer Science and Engineering, The Chinese University of Hong Kong",
        "aff_domain": "cse.cuhk.edu.hk;tencent.com;ualberta.ca;tencent.com;tencent.com;tencent.com;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;tencent.com;ualberta.ca;tencent.com;tencent.com;tencent.com;cse.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2+1;1;1;1;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Tencent;University of Alberta",
        "aff_unique_dep": "Department of Computer Science and Engineering;Tencent AI Lab;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://ai.tencent.com;https://www.ualberta.ca",
        "aff_unique_abbr": "CUHK;Tencent AI Lab;UAlberta",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;1+0;0;0;0;0",
        "aff_country_unique": "China;Canada"
    },
    {
        "id": "2022.acl-short.66",
        "title": "UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "The goal-oriented document-grounded dialogue aims at responding to the user query based on the dialogue context and supporting document. Existing studies tackle this problem by decomposing it into two sub-tasks: knowledge identification and response generation. However, such pipeline methods would unavoidably suffer from the error propagation issue. This paper proposes to unify these two sub-tasks via sequentially generating the grounding knowledge and the response. We further develop a prompt-connected multi-task learning strategy to model the characteristics and connections of different tasks and introduce linear temperature scheduling to reduce the negative effect of irrelevant document information. Experimental results demonstrate the effectiveness of our framework.",
        "author": "Chang Gao; Wenxuan Zhang; Wai Lam",
        "authorids": "/c/chang-gao/; /w/wenxuan-zhang/; /w/wai-lam/",
        "bibtex": "@inproceedings{gao-etal-2022-unigdd,\n    title = \"{U}ni{GDD}: {A} Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue\",\n    author = \"Gao, Chang  and\n      Zhang, Wenxuan  and\n      Lam, Wai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.66/\",\n    doi = \"10.18653/v1/2022.acl-short.66\",\n    pages = \"599--605\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.66.pdf",
        "site": "https://aclanthology.org/2022.acl-short.66/",
        "pdf_size": 850609,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12310042463374918764&as_sdt=80000005&sciodt=0,23&hl=en",
        "gs_version_total": 5,
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "aff_domain": "se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk",
        "email": "se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.433",
        "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited. However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new PELT methods and tasks. In light of model diversity and the difficulty of model selection, we propose a unified framework, UniPELT, which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1 4% gains compared to the best individual PELT method that it incorporates and even outperforms fine-tuning under different setups. Moreover, UniPELT generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task, indicating that a mixture of multiple PELT methods may be inherently more effective than single methods.",
        "author": "Yuning Mao; Lambert Mathias; Rui Hou; Amjad Almahairi; Hao Ma; Jiawei Han; Scott Yih; Madian Khabsa",
        "authorids": "/y/yuning-mao/; /l/lambert-mathias/; /r/rui-hou/; /a/amjad-almahairi/; /h/hao-ma/; /j/jiawei-han/; /s/scott-yih/; /m/madian-khabsa/",
        "bibtex": "@inproceedings{mao-etal-2022-unipelt,\n    title = \"{U}ni{PELT}: A Unified Framework for Parameter-Efficient Language Model Tuning\",\n    author = \"Mao, Yuning  and\n      Mathias, Lambert  and\n      Hou, Rui  and\n      Almahairi, Amjad  and\n      Ma, Hao  and\n      Han, Jiawei  and\n      Yih, Scott  and\n      Khabsa, Madian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.433/\",\n    doi = \"10.18653/v1/2022.acl-long.433\",\n    pages = \"6253--6264\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.433.pdf",
        "site": "https://aclanthology.org/2022.acl-long.433/",
        "pdf_size": 394681,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10441934946059241081&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois Urbana-Champaign; Meta AI; Meta AI; Meta AI; Meta AI; University of Illinois Urbana-Champaign; Meta AI; Meta AI",
        "aff_domain": "illinois.edu;fb.com;fb.com;fb.com;fb.com;illinois.edu;fb.com;fb.com",
        "email": "illinois.edu;fb.com;fb.com;fb.com;fb.com;illinois.edu;fb.com;fb.com",
        "github": "https://github.com/morningmoni/UniPELT",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;0;1;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Meta",
        "aff_unique_dep": ";Meta AI",
        "aff_unique_url": "https://illinois.edu;https://meta.com",
        "aff_unique_abbr": "UIUC;Meta",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.558",
        "title": "UniTE: Unified Translation Evaluation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Translation quality evaluation plays a crucial role in machine translation. According to the input format, it is mainly separated into three tasks, i.e., reference-only, source-only and source-reference-combined. Recent methods, despite their promising results, are specifically designed and optimized on one of them. This limits the convenience of these methods, and overlooks the commonalities among tasks. In this paper, we propose , which is the first unified framework engaged with abilities to handle all three evaluation tasks. Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task training. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality Estimation benchmarks. Extensive analyses show that our single model can universally surpass various state-of-the-art or winner methods across tasks.Both source code and associated models are available at https://github.com/NLP2CT/UniTE.",
        "author": "Yu Wan; Dayiheng Liu; Baosong Yang; Haibo Zhang; Boxing Chen; Derek Wong; Lidia Chao",
        "authorids": "/y/yu-wan/; /d/dayiheng-liu/; /b/baosong-yang/; /h/haibo-zhang/; /b/boxing-chen/; /d/derek-wong/; /l/lidia-chao/",
        "bibtex": "@inproceedings{wan-etal-2022-unite,\n    title = \"{U}ni{TE}: Unified Translation Evaluation\",\n    author = \"Wan, Yu  and\n      Liu, Dayiheng  and\n      Yang, Baosong  and\n      Zhang, Haibo  and\n      Chen, Boxing  and\n      Wong, Derek  and\n      Chao, Lidia\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.558/\",\n    doi = \"10.18653/v1/2022.acl-long.558\",\n    pages = \"8117--8127\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.558.pdf",
        "site": "https://aclanthology.org/2022.acl-long.558/",
        "pdf_size": 336371,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15360294461664216852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "NLP2CT Lab, University of Macau + DAMO Academy, Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group + NLP2CT Lab, University of Macau; NLP2CT Lab, University of Macau",
        "aff_domain": "gmail.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;umac.mo;umac.mo",
        "email": "gmail.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;umac.mo;umac.mo",
        "github": "https://github.com/NLP2CT/UniTE",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;1;1;1+0;0",
        "aff_unique_norm": "University of Macau;Alibaba Group",
        "aff_unique_dep": "NLP2CT Lab;DAMO Academy",
        "aff_unique_url": "https://www.um.edu.mo;https://www.alibaba-group.com",
        "aff_unique_abbr": "UM;Alibaba",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Macau SAR;",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.9",
        "title": "UniTranSeR: A Unified Transformer Semantic Representation Framework for Multimodal Task-Oriented Dialog System",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As a more natural and intelligent interaction manner, multimodal task-oriented dialog system recently has received great attention and many remarkable progresses have been achieved. Nevertheless, almost all existing studies follow the pipeline to first learn intra-modal features separately and then conduct simple feature concatenation or attention-based feature fusion to generate responses, which hampers them from learning inter-modal interactions and conducting cross-modal feature alignment for generating more intention-aware responses. To address these issues, we propose UniTranSeR, a Unified Transformer Semantic Representation framework with feature alignment and intention reasoning for multimodal dialog systems. Specifically, we first embed the multimodal features into a unified Transformer semantic space to prompt inter-modal interactions, and then devise a feature alignment and intention reasoning (FAIR) layer to perform cross-modal entity alignment and fine-grained key-value reasoning, so as to effectively identify user\u2019s intention for generating more accurate responses. Experimental results verify the effectiveness of UniTranSeR, showing that it significantly outperforms state-of-the-art approaches on the representative MMD dataset.",
        "author": "Zhiyuan Ma; Jianjun Li; Guohui Li; Yongjing Cheng",
        "authorids": "/z/zhiyuan-ma/; /j/jianjun-li/; /g/guohui-li/; /y/yongjing-cheng/",
        "bibtex": "@inproceedings{ma-etal-2022-unitranser,\n    title = \"{U}ni{T}ran{S}e{R}: A Unified Transformer Semantic Representation Framework for Multimodal Task-Oriented Dialog System\",\n    author = \"Ma, Zhiyuan  and\n      Li, Jianjun  and\n      Li, Guohui  and\n      Cheng, Yongjing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.9/\",\n    doi = \"10.18653/v1/2022.acl-long.9\",\n    pages = \"103--114\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.9.pdf",
        "site": "https://aclanthology.org/2022.acl-long.9/",
        "pdf_size": 4632676,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1668344450098345666&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Huazhong University of Science and Technology (HUST), China; Huazhong University of Science and Technology (HUST), China; Huazhong University of Science and Technology (HUST), China; National University of Defense Technology (NUDT), China",
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn;163.com",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn;163.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Huazhong University of Science and Technology;National University of Defense Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.hust.edu.cn;http://www.nudt.edu.cn/",
        "aff_unique_abbr": "HUST;NUDT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.499",
        "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.",
        "author": "Daya Guo; Shuai Lu; Nan Duan; Yanlin Wang; Ming Zhou; Jian Yin",
        "authorids": "/d/daya-guo/; /s/shuai-lu/; /n/nan-duan/; /y/yanlin-wang/; /m/ming-zhou/; /j/jian-yin/",
        "bibtex": "@inproceedings{guo-etal-2022-unixcoder,\n    title = \"{U}ni{X}coder: Unified Cross-Modal Pre-training for Code Representation\",\n    author = \"Guo, Daya  and\n      Lu, Shuai  and\n      Duan, Nan  and\n      Wang, Yanlin  and\n      Zhou, Ming  and\n      Yin, Jian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.499/\",\n    doi = \"10.18653/v1/2022.acl-long.499\",\n    pages = \"7212--7225\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.499.pdf",
        "site": "https://aclanthology.org/2022.acl-long.499/",
        "pdf_size": 655038,
        "gs_citation": 659,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1206419703297717913&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University + Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, P.R.China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; School of Software Engineering, Sun Yat-sen University; Langboat Technology, Beijing, China; School of Computer Science and Engineering, Sun Yat-sen University + Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, P.R.China",
        "aff_domain": "mail2.sysu.edu.cn; ; ; ; ; ",
        "email": "mail2.sysu.edu.cn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;2;0;3;0+1",
        "aff_unique_norm": "Sun Yat-sen University;Guangdong Key Laboratory of Big Data Analysis and Processing;Microsoft;Langboat Technology",
        "aff_unique_dep": "School of Computer Science and Engineering;;Research;",
        "aff_unique_url": "http://www.sysu.edu.cn;;https://www.microsoft.com/en-us/research/group/asia;",
        "aff_unique_abbr": "SYSU;;MSRA;",
        "aff_campus_unique_index": "1;2;2;1",
        "aff_campus_unique": ";Guangzhou;Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.105",
        "title": "Unified Speech-Text Pre-training for Speech Translation and Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition. The proposed method utilizes multi-task learning to integrate four self-supervised and supervised subtasks for cross modality learning. A self-supervised speech subtask, which leverages unlabelled speech data, and a (self-)supervised text to text subtask, which makes use of abundant text training data, take up the majority of the pre-training time. Two auxiliary supervised speech tasks are included to unify speech and text modeling space. Detailed analysis reveals learning interference among subtasks. In order to alleviate the subtask interference, two pre-training configurations are proposed for speech translation and speech recognition respectively. Our experiments show the proposed method can effectively fuse speech and text information into one model. It achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the Librispeech speech recognition task.",
        "author": "Yun Tang; Hongyu Gong; Ning Dong; Changhan Wang; Wei-Ning Hsu; Jiatao Gu; Alexei Baevski; Xian Li; Abdelrahman Mohamed; Michael Auli; Juan Pino",
        "authorids": "/y/yun-tang/; /h/hongyu-gong/; /n/ning-dong/; /c/changhan-wang/; /w/wei-ning-hsu/; /j/jiatao-gu/; /a/alexei-baevski/; /x/xian-li/; /a/abdelrahman-mohamed/; /m/michael-auli/; /j/juan-pino/",
        "bibtex": "@inproceedings{tang-etal-2022-unified,\n    title = \"Unified Speech-Text Pre-training for Speech Translation and Recognition\",\n    author = \"Tang, Yun  and\n      Gong, Hongyu  and\n      Dong, Ning  and\n      Wang, Changhan  and\n      Hsu, Wei-Ning  and\n      Gu, Jiatao  and\n      Baevski, Alexei  and\n      Li, Xian  and\n      Mohamed, Abdelrahman  and\n      Auli, Michael  and\n      Pino, Juan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.105/\",\n    doi = \"10.18653/v1/2022.acl-long.105\",\n    pages = \"1488--1499\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.105.pdf",
        "site": "https://aclanthology.org/2022.acl-long.105/",
        "pdf_size": 1221593,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2077429492388415775&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI",
        "aff_domain": "fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com;fb.com",
        "github": "https://github.com/pytorch/fairseq/tree/main/examples/speech_textjoint_totext",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Meta AI",
        "aff_unique_url": "https://meta.com",
        "aff_unique_abbr": "Meta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.395",
        "title": "Unified Structure Generation for Universal Information Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism \u2013 structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",
        "author": "Yaojie Lu; Qing Liu; Dai Dai; Xinyan Xiao; Hongyu Lin; Xianpei Han; Le Sun; Hua Wu",
        "authorids": "/y/yaojie-lu/; /q/qing-liu/; /d/dai-dai/; /x/xinyan-xiao/; /h/hongyu-lin/; /x/xianpei-han/; /l/le-sun/; /h/hua-wu/",
        "bibtex": "@inproceedings{lu-etal-2022-unified,\n    title = \"Unified Structure Generation for Universal Information Extraction\",\n    author = \"Lu, Yaojie  and\n      Liu, Qing  and\n      Dai, Dai  and\n      Xiao, Xinyan  and\n      Lin, Hongyu  and\n      Han, Xianpei  and\n      Sun, Le  and\n      Wu, Hua\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.395/\",\n    doi = \"10.18653/v1/2022.acl-long.395\",\n    pages = \"5755--5772\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.395.pdf",
        "site": "https://aclanthology.org/2022.acl-long.395/",
        "pdf_size": 576383,
        "gs_citation": 484,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6605040782743502202&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Chinese Information Processing Laboratory+University of Chinese Academy of Sciences+Beijing Academy of Arti\ufb01cial Intelligence; Chinese Information Processing Laboratory+University of Chinese Academy of Sciences+Beijing Academy of Arti\ufb01cial Intelligence; Baidu Inc.; Baidu Inc.; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Institute of Software, Chinese Academy of Sciences; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Institute of Software, Chinese Academy of Sciences+Beijing Academy of Arti\ufb01cial Intelligence; Chinese Information Processing Laboratory+State Key Laboratory of Computer Science+Institute of Software, Chinese Academy of Sciences; Baidu Inc.",
        "aff_domain": "iscas.ac.cn;iscas.ac.cn;baidu.com;baidu.com;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;baidu.com",
        "email": "iscas.ac.cn;iscas.ac.cn;baidu.com;baidu.com;iscas.ac.cn;iscas.ac.cn;iscas.ac.cn;baidu.com",
        "github": "",
        "project": "https://universal-ie.github.io",
        "author_num": 8,
        "aff_unique_index": "0+1+2;0+1+2;3;3;0+4+5;0+4+5+2;0+4+5;3",
        "aff_unique_norm": "Chinese Information Processing Laboratory;University of Chinese Academy of Sciences;Beijing Academy of Artificial Intelligence;Baidu;State Key Laboratory of Computer Science;Chinese Academy of Sciences",
        "aff_unique_dep": "Information Processing;;;Baidu Inc.;;Institute of Software",
        "aff_unique_url": ";http://www.ucas.ac.cn;https://www.baaic.cn;https://www.baidu.com;;http://www.ios.ac.cn",
        "aff_unique_abbr": ";UCAS;BAAI;Baidu;;CAS",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0;0+0+0;0+0+0+0;0+0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.442",
        "title": "Universal Conditional Masked Language Pre-training for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking. We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. Code, data, and pre-trained models are available at https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT",
        "author": "Pengfei Li; Liangyou Li; Meng Zhang; Minghao Wu; Qun Liu",
        "authorids": "/p/pengfei-li/; /l/liangyou-li/; /m/meng-zhang/; /m/minghao-wu/; /q/qun-liu/",
        "bibtex": "@inproceedings{li-etal-2022-universal,\n    title = \"Universal Conditional Masked Language Pre-training for Neural Machine Translation\",\n    author = \"Li, Pengfei  and\n      Li, Liangyou  and\n      Zhang, Meng  and\n      Wu, Minghao  and\n      Liu, Qun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.442/\",\n    doi = \"10.18653/v1/2022.acl-long.442\",\n    pages = \"6379--6391\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.442.pdf",
        "site": "https://aclanthology.org/2022.acl-long.442/",
        "pdf_size": 493915,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8399313896472710074&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Monash University; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "huawei.com;huawei.com;huawei.com;monash.edu;huawei.com",
        "email": "huawei.com;huawei.com;huawei.com;monash.edu;huawei.com",
        "github": "https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Huawei;Monash University",
        "aff_unique_dep": "Noah\u2019s Ark Lab;",
        "aff_unique_url": "https://www.huawei.com;https://www.monash.edu",
        "aff_unique_abbr": "Huawei;Monash",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.findings-acl.310",
        "title": "Unsupervised Chinese Word Segmentation with BERT Oriented Probing and Transformation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Word Segmentation is a fundamental step for understanding Chinese language. Previous neural approaches for unsupervised Chinese Word Segmentation (CWS) only exploits shallow semantic information, which can miss important context. Large scale Pre-trained language models (PLM) have achieved great success in many areas because of its ability to capture the deep contextual semantic relation. In this paper, we propose to take advantage of the deep semantic information embedded in PLM (e.g., BERT) with a self-training manner, which iteratively probes and transforms the semantic information in PLM into explicit word segmentation ability. Extensive experiment results show that our proposed approach achieves state-of-the-art F1 score on two CWS benchmark datasets.",
        "author": "Wei Li; Yuhan Song; Qi Su; Yanqiu Shao",
        "authorids": "/w/wei-li/; /y/yuhan-song/; /q/qi-su/; /y/yanqiu-shao/",
        "bibtex": "@inproceedings{li-etal-2022-unsupervised,\n    title = \"Unsupervised {C}hinese Word Segmentation with {BERT} Oriented Probing and Transformation\",\n    author = \"Li, Wei  and\n      Song, Yuhan  and\n      Su, Qi  and\n      Shao, Yanqiu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.310/\",\n    doi = \"10.18653/v1/2022.findings-acl.310\",\n    pages = \"3935--3940\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.310.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.310/",
        "pdf_size": 260624,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9073219966367246597&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Information Science, Beijing Language and Culture University; School of EECS, Peking University; School of Foreign Languages, Peking University; School of Information Science, Beijing Language and Culture University",
        "aff_domain": "blcu.edu.cn;pku.edu.cn;pku.edu.cn;blcu.edu.cn",
        "email": "blcu.edu.cn;pku.edu.cn;pku.edu.cn;blcu.edu.cn",
        "github": "https://github.com/liweitj47/BERT_unsupervised_word_segmentation",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Beijing Language and Culture University;Peking University",
        "aff_unique_dep": "School of Information Science;School of EECS",
        "aff_unique_url": "http://www.blcu.edu.cn;http://www.pku.edu.cn",
        "aff_unique_abbr": ";PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.203",
        "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval. However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential. In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space. We use the recently proposed Condenser pre-training architecture, which learns to condense information into the dense vector through LM pre-training. On top of it, we propose coCondenser, which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space. Experiments on MS-MARCO, Natural Question, and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation, synthesis, or filtering, and the need for large batch training. It shows comparable performance to RocketQA, a state-of-the-art, heavily engineered system, using simple small batch fine-tuning.",
        "author": "Luyu Gao; Jamie Callan",
        "authorids": "/l/luyu-gao/; /j/jamie-callan/",
        "bibtex": "@inproceedings{gao-callan-2022-unsupervised,\n    title = \"Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval\",\n    author = \"Gao, Luyu  and\n      Callan, Jamie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.203/\",\n    doi = \"10.18653/v1/2022.acl-long.203\",\n    pages = \"2843--2853\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.203.pdf",
        "site": "https://aclanthology.org/2022.acl-long.203/",
        "pdf_size": 695432,
        "gs_citation": 351,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15994581619164529019&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/luyug/Condenser",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.327",
        "title": "Unsupervised Dependency Graph Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work has identified properties of pretrained self-attention models that mirror those of dependency parse structures. In particular, some self-attention heads correspond well to individual dependency types. Inspired by these developments, we propose a new competitive mechanism that encourages these attention heads to model different dependency relations. We introduce a new model, the Unsupervised Dependency Graph Network (UDGN), that can induce dependency structures from raw corpora and the masked language modeling task. Experiment results show that UDGN achieves very strong unsupervised dependency parsing performance without gold POS tags and any other external information. The competitive gated heads show a strong correlation with human-annotated dependency types. Furthermore, the UDGN can also achieve competitive performance on masked language modeling and sentence textual similarity tasks.",
        "author": "Yikang Shen; Shawn Tan; Alessandro Sordoni; Peng Li; Jie Zhou; Aaron Courville",
        "authorids": "/y/yikang-shen/; /s/shawn-tan/; /a/alessandro-sordoni/; /p/peng-li/; /j/jie-zhou/; /a/aaron-courville/",
        "bibtex": "@inproceedings{shen-etal-2022-unsupervised,\n    title = \"Unsupervised Dependency Graph Network\",\n    author = \"Shen, Yikang  and\n      Tan, Shawn  and\n      Sordoni, Alessandro  and\n      Li, Peng  and\n      Zhou, Jie  and\n      Courville, Aaron\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.327/\",\n    doi = \"10.18653/v1/2022.acl-long.327\",\n    pages = \"4767--4784\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.327.pdf",
        "site": "https://aclanthology.org/2022.acl-long.327/",
        "pdf_size": 1103254,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13613722238319790008&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Mila/Universit\u00e9 de Montr\u00e9al; Mila/Universit\u00e9 de Montr\u00e9al; Microsoft Research Montr\u00e9al; Institute for AI Industry Research (AIR), Tsinghua University; Pattern Recognition Center, WeChat AI, Tencent Inc; Mila/Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "gmail.com; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ",
        "github": "https://github.com/yikangshen/UDGN",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;3;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Microsoft;Tsinghua University;Tencent",
        "aff_unique_dep": "Mila;Microsoft Research;Institute for AI Industry Research (AIR);Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "https://www.umontreal.ca;https://www.microsoft.com/en-us/research/group/microsoft-research-montreal;https://www.tsinghua.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "UdeM;MSR Montreal;Tsinghua;Tencent",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Montr\u00e9al;",
        "aff_country_unique_index": "0;0;0;1;1;0",
        "aff_country_unique": "Canada;China"
    },
    {
        "id": "2022.acl-long.86",
        "title": "Unsupervised Extractive Opinion Summarization Using Sparse Coding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Opinion summarization is the task of automatically generating summaries that encapsulate information expressed in multiple user reviews. We present Semantic Autoencoder (SemAE) to perform extractive opinion summarization in an unsupervised manner. SemAE uses dictionary learning to implicitly capture semantic information from the review text and learns a latent representation of each sentence over semantic units. Our extractive summarization algorithm leverages the representations to identify representative opinions among hundreds of reviews. SemAE is also able to perform controllable summarization to generate aspect-specific summaries using only a few samples. We report strong performance on SPACE and AMAZON datasets and perform experiments to investigate the functioning of our model.",
        "author": "Somnath Basu Roy Chowdhury; Chao Zhao; Snigdha Chaturvedi",
        "authorids": "/s/somnath-basu-roy-chowdhury/; /c/chao-zhao/; /s/snigdha-chaturvedi/",
        "bibtex": "@inproceedings{basu-roy-chowdhury-etal-2022-unsupervised,\n    title = \"Unsupervised Extractive Opinion Summarization Using Sparse Coding\",\n    author = \"Basu Roy Chowdhury, Somnath  and\n      Zhao, Chao  and\n      Chaturvedi, Snigdha\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.86/\",\n    doi = \"10.18653/v1/2022.acl-long.86\",\n    pages = \"1209--1225\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.86.pdf",
        "site": "https://aclanthology.org/2022.acl-long.86/",
        "pdf_size": 2742506,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4009090412490496194&as_sdt=20000005&sciodt=0,21&hl=en",
        "gs_version_total": 4,
        "aff": "UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/brcsomnath/SemAE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.159",
        "title": "Unsupervised Natural Language Inference Using PHL Triplet Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Transformer-based models achieve impressive performance on numerous Natural Language Inference (NLI) benchmarks when trained on respective training datasets. However, in certain cases, training samples may not be available or collecting them could be time-consuming and resource-intensive. In this work, we address the above challenge and present an explorative study on unsupervised NLI, a paradigm in which no human-annotated training samples are available. We investigate it under three settings: PH, P, and NPH that differ in the extent of unlabeled data available for learning. As a solution, we propose a procedural data generation approach that leverages a set of sentence transformations to collect PHL (Premise, Hypothesis, Label) triplets for training NLI models, bypassing the need for human-annotated training data. Comprehensive experiments with several NLI datasets show that the proposed approach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH settings respectively, outperforming all existing unsupervised baselines. Furthermore, fine-tuning our model with as little as ~0.1% of the human-annotated training dataset (500 instances) leads to 12.2% higher accuracy than the model trained from scratch on the same 500 instances. Supported by this superior performance, we conclude with a recommendation for collecting high-quality task-specific data.",
        "author": "Neeraj Varshney; Pratyay Banerjee; Tejas Gokhale; Chitta Baral",
        "authorids": "/n/neeraj-varshney/; /p/pratyay-banerjee/; /t/tejas-gokhale/; /c/chitta-baral/",
        "bibtex": "@inproceedings{varshney-etal-2022-unsupervised,\n    title = \"Unsupervised Natural Language Inference Using {PHL} Triplet Generation\",\n    author = \"Varshney, Neeraj  and\n      Banerjee, Pratyay  and\n      Gokhale, Tejas  and\n      Baral, Chitta\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.159/\",\n    doi = \"10.18653/v1/2022.findings-acl.159\",\n    pages = \"2003--2016\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.159.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.159/",
        "pdf_size": 410870,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16770279679609820343&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Arizona State University; Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.303",
        "title": "Unsupervised Preference-Aware Language Identification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recognizing the language of ambiguous texts has become a main challenge in language identification (LID). When using multilingual applications, users have their own language preferences, which can be regarded as external knowledge for LID. Nevertheless, current studies do not consider the inter-personal variations due to the lack of user annotated training data. To fill this gap, we introduce preference-aware LID and propose a novel unsupervised learning strategy. Concretely, we construct pseudo training set for each user by extracting training samples from a standard LID corpus according to his/her historical language distribution. Besides, we contribute the first user labeled LID test set called \u201cU-LID\u201d. Experimental results reveal that our model can incarnate user traits and significantly outperforms existing LID systems on handling ambiguous texts. Our code and benchmark have been released.",
        "author": "Xingzhang Ren; Baosong Yang; Dayiheng Liu; Haibo Zhang; Xiaoyu Lv; Liang Yao; Jun Xie",
        "authorids": "/x/xingzhang-ren/; /b/baosong-yang/; /d/dayiheng-liu/; /h/haibo-zhang/; /x/xiaoyu-lv/; /l/liang-yao/; /j/jun-xie/",
        "bibtex": "@inproceedings{ren-etal-2022-unsupervised,\n    title = \"Unsupervised Preference-Aware Language Identification\",\n    author = \"Ren, Xingzhang  and\n      Yang, Baosong  and\n      Liu, Dayiheng  and\n      Zhang, Haibo  and\n      Lv, Xiaoyu  and\n      Yao, Liang  and\n      Xie, Jun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.303/\",\n    doi = \"10.18653/v1/2022.findings-acl.303\",\n    pages = \"3847--3852\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.303.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.303/",
        "pdf_size": 437008,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1745231158798852830&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "https://github.com/xzhren/PreferenceAwareLID",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "DAMO Academy",
        "aff_unique_url": "https://www.alibaba-group.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-short.83",
        "title": "Unsupervised multiple-choice question generation for out-of-domain Q&A fine-tuning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Pre-trained models have shown very good performances on a number of question answering benchmarks especially when fine-tuned on multiple question answering datasets at once. In this work, we propose an approach for generating a fine-tuning dataset thanks to a rule-based algorithm that generates questions and answers from unannotated sentences. We show that the state-of-the-art model UnifiedQA can greatly benefit from such a system on a multiple-choice benchmark about physics, biology and chemistry it has never been trained on. We further show that improved performances may be obtained by selecting the most challenging distractors (wrong answers), with a dedicated ranker based on a pretrained RoBERTa model.",
        "author": "Guillaume Le Berre; Christophe Cerisara; Philippe Langlais; Guy Lapalme",
        "authorids": "/g/guillaume-le-berre/; /c/christophe-cerisara/; /p/philippe-langlais/; /g/guy-lapalme/",
        "bibtex": "@inproceedings{le-berre-etal-2022-unsupervised,\n    title = \"Unsupervised multiple-choice question generation for out-of-domain {Q}{\\&}{A} fine-tuning\",\n    author = \"Le Berre, Guillaume  and\n      Cerisara, Christophe  and\n      Langlais, Philippe  and\n      Lapalme, Guy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.83/\",\n    doi = \"10.18653/v1/2022.acl-short.83\",\n    pages = \"732--738\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.83.pdf",
        "site": "https://aclanthology.org/2022.acl-short.83/",
        "pdf_size": 261300,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1470543518035833052&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "University of Lorraine, CNRS, LORIA, France+RALI/DIRO, University of Montreal, Canada; University of Lorraine, CNRS, LORIA, France; RALI/DIRO, University of Montreal, Canada; RALI/DIRO, University of Montreal, Canada",
        "aff_domain": "iro.umontreal.ca;loria.fr;iro.umontreal.ca;iro.umontreal.ca",
        "email": "iro.umontreal.ca;loria.fr;iro.umontreal.ca;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;1;1",
        "aff_unique_norm": "University of Lorraine;University of Montreal",
        "aff_unique_dep": "LORIA;RALI/DIRO",
        "aff_unique_url": "https://www.univ-lorraine.fr;https://www.umontreal.ca",
        "aff_unique_abbr": "UL;UM",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Montreal",
        "aff_country_unique_index": "0+1;0;1;1",
        "aff_country_unique": "France;Canada"
    },
    {
        "id": "2022.acl-long.446",
        "title": "Updated Headline Generation: Creating Updated Summaries for Evolving News Stories",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose the task of updated headline generation, in which a system generates a headline for an updated article, considering both the previous article and headline. The system must identify the novel information in the article update, and modify the existing headline accordingly. We create data for this task using the NewsEdits corpus by automatically identifying contiguous article versions that are likely to require a substantive headline update. We find that models conditioned on the prior headline and body revisions produce headlines judged by humans to be as factual as gold headlines while making fewer unnecessary edits compared to a standard headline generation model. Our experiments establish benchmarks for this new contextual summarization task.",
        "author": "Sheena Panthaplackel; Adrian Benton; Mark Dredze",
        "authorids": "/s/sheena-panthaplackel/; /a/adrian-benton/; /m/mark-dredze/",
        "bibtex": "@inproceedings{panthaplackel-etal-2022-updated,\n    title = \"Updated Headline Generation: Creating Updated Summaries for Evolving News Stories\",\n    author = \"Panthaplackel, Sheena  and\n      Benton, Adrian  and\n      Dredze, Mark\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.446/\",\n    doi = \"10.18653/v1/2022.acl-long.446\",\n    pages = \"6438--6461\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.446.pdf",
        "site": "https://aclanthology.org/2022.acl-long.446/",
        "pdf_size": 14520052,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=734427342679882341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science, The University of Texas at Austin, TX, USA; Bloomberg, New York, NY USA; Bloomberg, New York, NY USA+Computer Science, Johns Hopkins University, Baltimore, MD USA",
        "aff_domain": "cs.utexas.edu;google.com;cs.jhu.edu",
        "email": "cs.utexas.edu;google.com;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1+2",
        "aff_unique_norm": "University of Texas at Austin;Bloomberg;Johns Hopkins University",
        "aff_unique_dep": "Computer Science;;Computer Science",
        "aff_unique_url": "https://www.utexas.edu;https://www.bloomberg.com;https://www.jhu.edu",
        "aff_unique_abbr": "UT Austin;Bloomberg;JHU",
        "aff_campus_unique_index": "0;1;1+2",
        "aff_campus_unique": "Austin;New York;Baltimore",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.247",
        "title": "Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A few large, homogenous, pre-trained models undergird many machine learning systems \u2014 and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier\u2019s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",
        "author": "Ryan Steed; Swetasudha Panda; Ari Kobren; Michael Wick",
        "authorids": "/r/ryan-steed/; /s/swetasudha-panda/; /a/ari-kobren/; /m/michael-wick/",
        "bibtex": "@inproceedings{steed-etal-2022-upstream,\n    title = \"{U}pstream {M}itigation {I}s \\textit{ {N}ot} {A}ll {Y}ou {N}eed: {T}esting the {B}ias {T}ransfer {H}ypothesis in {P}re-{T}rained {L}anguage {M}odels\",\n    author = \"Steed, Ryan  and\n      Panda, Swetasudha  and\n      Kobren, Ari  and\n      Wick, Michael\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.247/\",\n    doi = \"10.18653/v1/2022.acl-long.247\",\n    pages = \"3524--3542\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.247.pdf",
        "site": "https://aclanthology.org/2022.acl-long.247/",
        "pdf_size": 1500707,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8021219888957388629&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Carnegie Mellon University; Oracle Labs; Oracle Labs; Oracle Labs",
        "aff_domain": "cmu.edu;oracle.com;oracle.com;oracle.com",
        "email": "cmu.edu;oracle.com;oracle.com;oracle.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Carnegie Mellon University;Oracle Corporation",
        "aff_unique_dep": ";Oracle Labs",
        "aff_unique_url": "https://www.cmu.edu;https://labs.oracle.com",
        "aff_unique_abbr": "CMU;Oracle Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.561",
        "title": "Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.",
        "author": "Jiangbin Zheng; Yile Wang; Ge Wang; Jun Xia; Yufei Huang; Guojiang Zhao; Yue Zhang; Stan Li",
        "authorids": "/j/jiangbin-zheng/; /y/yile-wang/; /g/ge-wang/; /j/jun-xia/; /y/yufei-huang/; /g/guojiang-zhao/; /y/yue-zhang/; /s/stan-li/",
        "bibtex": "@inproceedings{zheng-etal-2022-using,\n    title = \"Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings\",\n    author = \"Zheng, Jiangbin  and\n      Wang, Yile  and\n      Wang, Ge  and\n      Xia, Jun  and\n      Huang, Yufei  and\n      Zhao, Guojiang  and\n      Zhang, Yue  and\n      Li, Stan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.561/\",\n    doi = \"10.18653/v1/2022.acl-long.561\",\n    pages = \"8154--8163\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.561.pdf",
        "site": "https://aclanthology.org/2022.acl-long.561/",
        "pdf_size": 866908,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6413026031213676217&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "School of Engineering, Westlake University\u2020; School of Engineering, Westlake University\u2020; School of Engineering, Westlake University; School of Engineering, Westlake University; School of Engineering, Westlake University; School of Engineering, Westlake University; School of Engineering, Westlake University; Institute of Advanced Technology, Westlake Institute for Advanced Study*",
        "aff_domain": "westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "email": "westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;1",
        "aff_unique_norm": "Westlake University;Westlake Institute for Advanced Study",
        "aff_unique_dep": "School of Engineering;Institute of Advanced Technology",
        "aff_unique_url": "https://www.westlake.edu.cn;http://www.wias.org.cn/",
        "aff_unique_abbr": ";WIAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.75",
        "title": "Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Most research on question answering focuses on the pre-deployment stage; i.e., building an accurate model for deployment. In this paper, we ask the question: Can we improve QA systems further post-deployment based on user interactions? We focus on two kinds of improvements: 1) improving the QA system\u2019s performance itself, and 2) providing the model with the ability to explain the correctness or incorrectness of an answer. We collect a retrieval-based QA dataset, FeedbackQA, which contains interactive feedback from users. We collect this dataset by deploying a base QA system to crowdworkers who then engage with the system and provide feedback on the quality of its answers. The feedback contains both structured ratings and unstructured natural language explanations. We train a neural model with this feedback data that can generate explanations and re-score answer candidates. We show that feedback data not only improves the accuracy of the deployed QA system but also other stronger non-deployed systems. The generated explanations also help users make informed decisions about the correctness of answers.",
        "author": "Zichao Li; Prakhar Sharma; Xing Han Lu; Jackie Cheung; Siva Reddy",
        "authorids": "/z/zichao-li/; /p/prakhar-sharma/; /x/xing-han-lu/; /j/jackie-chi-kit-cheung/; /s/siva-reddy/",
        "bibtex": "@inproceedings{li-etal-2022-using,\n    title = \"Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment\",\n    author = \"Li, Zichao  and\n      Sharma, Prakhar  and\n      Lu, Xing Han  and\n      Cheung, Jackie  and\n      Reddy, Siva\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.75/\",\n    doi = \"10.18653/v1/2022.findings-acl.75\",\n    pages = \"926--937\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.75.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.75/",
        "pdf_size": 1462108,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15307368370129668526&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Mila, McGill University; University of California, Los Angeles; Mila, McGill University; Mila, McGill University; Mila, McGill University",
        "aff_domain": "mila.quebec; ; ; ; ",
        "email": "mila.quebec; ; ; ; ",
        "github": "",
        "project": "https://mcgill-nlp.github.io/feedbackqa",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "McGill University;University of California, Los Angeles",
        "aff_unique_dep": "Mila;",
        "aff_unique_url": "https://www.mcgill.ca;https://www.ucla.edu",
        "aff_unique_abbr": "McGill;UCLA",
        "aff_campus_unique_index": "0;1;0;0;0",
        "aff_campus_unique": "Montreal;Los Angeles",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.findings-acl.304",
        "title": "Using NLP to quantify the environmental cost and diversity benefits of in-person NLP conferences",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated. In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences. We obtain the necessary data by text-mining all publications from the ACL anthology available at the time of the study (n=60,572) and extracting information about an author\u2019s affiliation, including their address. This allows us to estimate the corresponding carbon cost and compare it to previously known values for training large models. Further, we look at the benefits of in-person conferences by demonstrating that they can increase participation diversity by encouraging attendance from the region surrounding the host country. We show how the trade-off between carbon cost and diversity of an event depends on its location and type. Our aim is to foster further discussion on the best way to address the joint issue of emissions and diversity in the future.",
        "author": "Piotr Przyby\u0142a; Matthew Shardlow",
        "authorids": "/p/piotr-przybyla/; /m/matthew-shardlow/",
        "bibtex": "@inproceedings{przybyla-shardlow-2022-using,\n    title = \"Using {NLP} to quantify the environmental cost and diversity benefits of in-person {NLP} conferences\",\n    author = \"Przyby{\\l}a, Piotr  and\n      Shardlow, Matthew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.304/\",\n    doi = \"10.18653/v1/2022.findings-acl.304\",\n    pages = \"3853--3863\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.304.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.304/",
        "pdf_size": 3571637,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9487819138794793545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, UK",
        "aff_domain": "ipipan.waw.pl;mmu.ac.uk",
        "email": "ipipan.waw.pl;mmu.ac.uk",
        "github": "",
        "project": "https://www.aclweb.org/anthology/3853",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Polish Academy of Sciences;Manchester Metropolitan University",
        "aff_unique_dep": "Institute of Computer Science;Department of Computing and Mathematics",
        "aff_unique_url": "https://www.pan.pl;https://www2.mmu.ac.uk/",
        "aff_unique_abbr": "PAS;MMU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Warsaw;Manchester",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Poland;United Kingdom"
    },
    {
        "id": "2022.findings-acl.245",
        "title": "Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English. We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism that are the most appropriate to generate CNs. Findings show that autoregressive models combined with stochastic decodings are the most promising. We then investigate how an LM performs in generating a CN with regard to an unseen target of hate. We find out that a key element for successful \u2018out of target\u2019 experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori. We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.",
        "author": "Serra Sinem Tekiro\u011flu; Helena Bonaldi; Margherita Fanton; Marco Guerini",
        "authorids": "/s/serra-sinem-tekiroglu/; /h/helena-bonaldi/; /m/margherita-fanton/; /m/marco-guerini/",
        "bibtex": "@inproceedings{tekiroglu-etal-2022-using,\n    title = \"Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study\",\n    author = \"Tekiro{\\u{g}}lu, Serra Sinem  and\n      Bonaldi, Helena  and\n      Fanton, Margherita  and\n      Guerini, Marco\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.245/\",\n    doi = \"10.18653/v1/2022.findings-acl.245\",\n    pages = \"3099--3114\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.245.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.245/",
        "pdf_size": 412799,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3776098972168322047&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Trento, Italy + Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento, Italy; University of Trento, Italy + Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento, Italy; University of Trento, Italy + Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento, Italy + University of Stuttgart, Germany; Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento, Italy",
        "aff_domain": "fbk.eu;fbk.eu;ims.uni-stuttgart.de;fbk.eu",
        "email": "fbk.eu;fbk.eu;ims.uni-stuttgart.de;fbk.eu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1+2;1",
        "aff_unique_norm": "University of Trento;Fondazione Bruno Kessler;University of Stuttgart",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unitn.it;https://www.fbk.eu;https://www.uni-stuttgart.de",
        "aff_unique_abbr": "UniTN;FBK;USTuttgart",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0+1;0",
        "aff_country_unique": "Italy;Germany"
    },
    {
        "id": "2022.acl-long.567",
        "title": "VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.",
        "author": "Letitia Parcalabescu; Michele Cafagna; Lilitta Muradjan; Anette Frank; Iacer Calixto; Albert Gatt",
        "authorids": "/l/letitia-parcalabescu/; /m/michele-cafagna/; /l/lilitta-muradjan/; /a/anette-frank/; /i/iacer-calixto/; /a/albert-gatt/",
        "bibtex": "@inproceedings{parcalabescu-etal-2022-valse,\n    title = \"{VALSE}: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena\",\n    author = \"Parcalabescu, Letitia  and\n      Cafagna, Michele  and\n      Muradjan, Lilitta  and\n      Frank, Anette  and\n      Calixto, Iacer  and\n      Gatt, Albert\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.567/\",\n    doi = \"10.18653/v1/2022.acl-long.567\",\n    pages = \"8253--8280\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.567.pdf",
        "site": "https://aclanthology.org/2022.acl-long.567/",
        "pdf_size": 5795875,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15630430112409308520&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 7,
        "aff": "Heidelberg University, Department of Computational Linguistics; University of Malta, Institute of Linguistics and Language Technology; Heidelberg University, Department of Computational Linguistics; Heidelberg University, Department of Computational Linguistics; New York University+ILLC, University of Amsterdam; Utrecht University, Department of Information and Computing Sciences",
        "aff_domain": "cl.uni-heidelberg.de; ; ; ; ; ",
        "email": "cl.uni-heidelberg.de; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;2+3;4",
        "aff_unique_norm": "Heidelberg University;University of Malta;New York University;University of Amsterdam;Utrecht University",
        "aff_unique_dep": "Department of Computational Linguistics;Institute of Linguistics and Language Technology;;ILLC;Department of Information and Computing Sciences",
        "aff_unique_url": "https://www.uni-heidelberg.de;https://www.um.edu.mt;https://www.nyu.edu;https://www.uva.nl;https://www.uu.nl",
        "aff_unique_abbr": "Uni Heidelberg;UM;NYU;UvA;UU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Amsterdam",
        "aff_country_unique_index": "0;1;0;0;2+3;3",
        "aff_country_unique": "Germany;Malta;United States;Netherlands"
    },
    {
        "id": "2022.acl-long.258",
        "title": "VALUE: Understanding Dialect Disparity in NLU",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "English Natural Language Understanding (NLU) systems have achieved great performances and even outperformed humans on benchmarks like GLUE and SuperGLUE. However, these benchmarks contain only textbook Standard American English (SAE). Other dialects have been largely overlooked in the NLP community. This leads to biased and inequitable NLU systems that serve only a sub-population of speakers. To understand disparities in current models and to facilitate more dialect-competent NLU systems, we introduce the VernAcular Language Understanding Evaluation (VALUE) benchmark, a challenging variant of GLUE that we created with a set of lexical and morphosyntactic transformation rules. In this initial release (V.1), we construct rules for 11 features of African American Vernacular English (AAVE), and we recruit fluent AAVE speakers to validate each feature transformation via linguistic acceptability judgments in a participatory design manner. Experiments show that these new dialectal features can lead to a drop in model performance.",
        "author": "Caleb Ziems; Jiaao Chen; Camille Harris; Jessica Anderson; Diyi Yang",
        "authorids": "/c/caleb-ziems/; /j/jiaao-chen/; /c/camille-harris/; /j/jessica-anderson/; /d/diyi-yang/",
        "bibtex": "@inproceedings{ziems-etal-2022-value,\n    title = \"{VALUE}: {U}nderstanding Dialect Disparity in {NLU}\",\n    author = \"Ziems, Caleb  and\n      Chen, Jiaao  and\n      Harris, Camille  and\n      Anderson, Jessica  and\n      Yang, Diyi\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.258/\",\n    doi = \"10.18653/v1/2022.acl-long.258\",\n    pages = \"3701--3720\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.258.pdf",
        "site": "https://aclanthology.org/2022.acl-long.258/",
        "pdf_size": 361220,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7288037742399051471&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.157",
        "title": "VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Interactive robots navigating photo-realistic environments need to be trained to effectively leverage and handle the dynamic nature of dialogue in addition to the challenges underlying vision-and-language navigation (VLN). In this paper, we present VISITRON, a multi-modal Transformer-based navigator better suited to the interactive regime inherent to Cooperative Vision-and-Dialog Navigation (CVDN). VISITRON is trained to: i) identify and associate object-level concepts and semantics between the environment and dialogue history, ii) identify when to interact vs. navigate via imitation learning of a binary classification head. We perform extensive pre-training and fine-tuning ablations with VISITRON to gain empirical insights and improve performance on CVDN. VISITRON\u2019s ability to identify when to interact leads to a natural generalization of the game-play mode introduced by Roman et al. (2020) for enabling the use of such models in different environments. VISITRON is competitive with models on the static CVDN leaderboard and attains state-of-the-art performance on the Success weighted by Path Length (SPL) metric.",
        "author": "Ayush Shrivastava; Karthik Gopalakrishnan; Yang Liu; Robinson Piramuthu; Gokhan Tur; Devi Parikh; Dilek Hakkani-Tur",
        "authorids": "/a/ayush-shrivastava/; /k/karthik-gopalakrishnan/; /y/yang-liu-icsi/; /r/robinson-piramuthu/; /g/gokhan-tur/; /d/devi-parikh/; /d/dilek-hakkani-tur/",
        "bibtex": "@inproceedings{shrivastava-etal-2022-visitron,\n    title = \"{VISITRON}: Visual Semantics-Aligned Interactively Trained Object-Navigator\",\n    author = \"Shrivastava, Ayush  and\n      Gopalakrishnan, Karthik  and\n      Liu, Yang  and\n      Piramuthu, Robinson  and\n      Tur, Gokhan  and\n      Parikh, Devi  and\n      Hakkani-Tur, Dilek\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.157/\",\n    doi = \"10.18653/v1/2022.findings-acl.157\",\n    pages = \"1984--1994\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.157.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.157/",
        "pdf_size": 1583412,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7960210451610188128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Georgia Tech+Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Georgia Tech+Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "gatech.edu;amazon.com;amazon.com;amazon.com;amazon.com;gatech.edu;amazon.com",
        "email": "gatech.edu;amazon.com;amazon.com;amazon.com;amazon.com;gatech.edu;amazon.com",
        "github": "www.github.com/alexa/visitron",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;1;1;0+1;1",
        "aff_unique_norm": "Georgia Institute of Technology;Amazon",
        "aff_unique_dep": ";Amazon Alexa AI",
        "aff_unique_url": "https://www.gatech.edu;https://www.amazon.com",
        "aff_unique_abbr": "Georgia Tech;Amazon",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.199",
        "title": "Variational Graph Autoencoding as Cheap Supervision for AMR Coreference Resolution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Coreference resolution over semantic graphs like AMRs aims to group the graph nodes that represent the same entity. This is a crucial step for making document-level formal semantic representations. With annotated data on AMR coreference resolution, deep learning approaches have recently shown great potential for this task, yet they are usually data hunger and annotations are costly. We propose a general pretraining method using variational graph autoencoder (VGAE) for AMR coreference resolution, which can leverage any general AMR corpus and even automatically parsed AMR data. Experiments on benchmarks show that the pretraining approach achieves performance gains of up to 6% absolute F1 points. Moreover, our model significantly improves on the previous state-of-the-art model by up to 11% F1.",
        "author": "Irene Li; Linfeng Song; Kun Xu; Dong Yu",
        "authorids": "/i/irene-li/; /l/linfeng-song/; /k/kun-xu/; /d/dong-yu/",
        "bibtex": "@inproceedings{li-etal-2022-variational,\n    title = \"Variational Graph Autoencoding as Cheap Supervision for {AMR} Coreference Resolution\",\n    author = \"Li, Irene  and\n      Song, Linfeng  and\n      Xu, Kun  and\n      Yu, Dong\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.199/\",\n    doi = \"10.18653/v1/2022.acl-long.199\",\n    pages = \"2790--2800\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.199.pdf",
        "site": "https://aclanthology.org/2022.acl-long.199/",
        "pdf_size": 1641713,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15523900431757489334&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Yale University, CT, USA; Tencent AI Lab, Bellevue, WA, USA; Tencent AI Lab, Bellevue, WA, USA; Tencent AI Lab, Bellevue, WA, USA",
        "aff_domain": "yale.edu;tencent.com;tencent.com;tencent.com",
        "email": "yale.edu;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Yale University;Tencent",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www.yale.edu;https://ai.tencent.com",
        "aff_unique_abbr": "Yale;Tencent AI Lab",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "New Haven;Bellevue",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.70",
        "title": "Virtual Augmentation Supported Contrastive Learning of Sentence Representations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Despite profound successes, contrastive representation learning relies on carefully designed data augmentations using domain-specific knowledge. This challenge is magnified in natural language processing, where no general rules exist for data augmentation due to the discrete nature of natural language. We tackle this challenge by presenting a Virtual augmentation Supported Contrastive Learning of sentence representations (VaSCL). Originating from the interpretation that data augmentation essentially constructs the neighborhoods of each training instance, we, in turn, utilize the neighborhood to generate effective data augmentations. Leveraging the large training batch size of contrastive learning, we approximate the neighborhood of an instance via its K-nearest in-batch neighbors in the representation space. We then define an instance discrimination task regarding the neighborhood and generate the virtual augmentation in an adversarial training manner. We access the performance of VaSCL on a wide range of downstream tasks and set a new state-of-the-art for unsupervised sentence representation learning.",
        "author": "Dejiao Zhang; Wei Xiao; Henghui Zhu; Xiaofei Ma; Andrew Arnold",
        "authorids": "/d/dejiao-zhang/; /w/wei-xiao/; /h/henghui-zhu/; /x/xiaofei-ma/; /a/andrew-arnold/",
        "bibtex": "@inproceedings{zhang-etal-2022-virtual,\n    title = \"Virtual Augmentation Supported Contrastive Learning of Sentence Representations\",\n    author = \"Zhang, Dejiao  and\n      Xiao, Wei  and\n      Zhu, Henghui  and\n      Ma, Xiaofei  and\n      Arnold, Andrew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.70/\",\n    doi = \"10.18653/v1/2022.findings-acl.70\",\n    pages = \"864--876\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.70.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.70/",
        "pdf_size": 450872,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7473764768104271175&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "AWS AI Labs, New York; AWS AI Labs, New York; AWS AI Labs, New York; AWS AI Labs, New York; AWS AI Labs, New York",
        "aff_domain": "amazon.com; ; ; ; ",
        "email": "amazon.com; ; ; ; ",
        "github": "https://github.com/amazon-research/sentence-representations",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "AWS AI Labs",
        "aff_unique_dep": "",
        "aff_unique_url": "https://aws.amazon.com/research/ai",
        "aff_unique_abbr": "AWS AI Labs",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.152",
        "title": "Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention inrecent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodalalignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify fine-grainedaspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-LanguagePre-training framework for MABSA (VLP-MABSA), which is a unified multimodal encoder-decoder architecture for all the pretrainingand downstream tasks. We further design three types of task-specific pre-training tasks from the language, vision, and multimodalmodalities, respectively. Experimental results show that our approach generally outperforms the state-of-the-art approaches on three MABSA subtasks. Further analysis demonstrates the effectiveness of each pre-training task. The source code is publicly released at https://github.com/NUSTM/VLP-MABSA.",
        "author": "Yan Ling; Jianfei Yu; Rui Xia",
        "authorids": "/y/yan-ling/; /j/jianfei-yu/; /r/rui-xia/",
        "bibtex": "@inproceedings{ling-etal-2022-vision,\n    title = \"Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis\",\n    author = \"Ling, Yan  and\n      Yu, Jianfei  and\n      Xia, Rui\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.152/\",\n    doi = \"10.18653/v1/2022.acl-long.152\",\n    pages = \"2149--2159\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.152.pdf",
        "site": "https://aclanthology.org/2022.acl-long.152/",
        "pdf_size": 1084479,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13819220868656640651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China",
        "aff_domain": "njust.edu.cn;njust.edu.cn;njust.edu.cn",
        "email": "njust.edu.cn;njust.edu.cn;njust.edu.cn",
        "github": "https://github.com/NUSTM/VLP-MABSA",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanjing University of Science and Technology",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.524",
        "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A long-term goal of AI research is to build intelligent agents that can communicate with humans in natural language, perceive the environment, and perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental and interdisciplinary research topic towards this goal, and receives increasing attention from natural language processing, computer vision, robotics, and machine learning communities. In this paper, we review contemporary studies in the emerging field of VLN, covering tasks, evaluation metrics, methods, etc. Through structured analysis of current progress and challenges, we also highlight the limitations of current VLN and opportunities for future work. This paper serves as a thorough reference for the VLN research community.",
        "author": "Jing Gu; Eliana Stefani; Qi Wu; Jesse Thomason; Xin Wang",
        "authorids": "/j/jing-gu/; /e/eliana-stefani/; /q/qi-wu/; /j/jesse-thomason/; /x/xin-wang/",
        "bibtex": "@inproceedings{gu-etal-2022-vision,\n    title = \"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions\",\n    author = \"Gu, Jing  and\n      Stefani, Eliana  and\n      Wu, Qi  and\n      Thomason, Jesse  and\n      Wang, Xin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.524/\",\n    doi = \"10.18653/v1/2022.acl-long.524\",\n    pages = \"7606--7623\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.524.pdf",
        "site": "https://aclanthology.org/2022.acl-long.524/",
        "pdf_size": 514002,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7769483078627542630&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Santa Cruz; University of California, Santa Cruz; The University of Adelaide; University of Southern California; University of California, Santa Cruz",
        "aff_domain": "ucsc.edu;ucsc.edu;adelaide.edu.au;usc.edu;ucsc.edu",
        "email": "ucsc.edu;ucsc.edu;adelaide.edu.au;usc.edu;ucsc.edu",
        "github": "https://github.com/eric-ai-lab/awesome-vision-language-navigation",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of California, Santa Cruz;University of Adelaide;University of Southern California",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucsc.edu;https://www.adelaide.edu.au;https://www.usc.edu",
        "aff_unique_abbr": "UCSC;Adelaide;USC",
        "aff_campus_unique_index": "0;0;2;0",
        "aff_campus_unique": "Santa Cruz;;Los Angeles",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;Australia"
    },
    {
        "id": "2022.acl-long.332",
        "title": "Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Vision-language navigation (VLN) is a challenging task due to its large searching space in the environment. To address this problem, previous works have proposed some methods of fine-tuning a large model that pretrained on large-scale datasets. However, the conventional fine-tuning methods require extra human-labeled navigation data and lack self-exploration capabilities in environments, which hinders their generalization of unseen scenes. To improve the ability of fast cross-domain adaptation, we propose Prompt-based Environmental Self-exploration (ProbES), which can self-explore the environments by sampling trajectories and automatically generates structured instructions via a large-scale cross-modal pretrained model (CLIP). Our method fully utilizes the knowledge learned from CLIP to build an in-domain dataset by self-exploration without human labeling. Unlike the conventional approach of fine-tuning, we introduce prompt tuning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge. By automatically synthesizing trajectory-instruction pairs in any environment without human supervision and instruction prompt tuning, our model can adapt to diverse vision-language navigation tasks, including VLN and REVERIE. Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model.",
        "author": "Xiwen Liang; Fengda Zhu; Li Lingling; Hang Xu; Xiaodan Liang",
        "authorids": "/x/xiwen-liang/; /f/fengda-zhu/; /l/li-lingling/; /h/hang-xu/; /x/xiaodan-liang/",
        "bibtex": "@inproceedings{liang-etal-2022-visual,\n    title = \"Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration\",\n    author = \"Liang, Xiwen  and\n      Zhu, Fengda  and\n      Lingling, Li  and\n      Xu, Hang  and\n      Liang, Xiaodan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.332/\",\n    doi = \"10.18653/v1/2022.acl-long.332\",\n    pages = \"4837--4851\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.332.pdf",
        "site": "https://aclanthology.org/2022.acl-long.332/",
        "pdf_size": 6445307,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3491997650579164031&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Shenzhen Campus of Sun Yat-sen University; Monash University; Sun Yat-sen University; Huawei Noah\u2019s Ark Lab; Shenzhen Campus of Sun Yat-sen University",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "https://github.com/liangcici/Probes-VLN",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Sun Yat-sen University;Monash University;Huawei",
        "aff_unique_dep": ";;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.monash.edu;https://www.huawei.com",
        "aff_unique_abbr": "SYSU;Monash;Huawei",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2022.findings-acl.35",
        "title": "Visualizing the Relationship Between Encoded Linguistic Information and Task Performance",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Probing is popular to analyze whether linguistic information can be captured by a well-trained deep neural model, but it is hard to answer how the change of the encoded linguistic information will affect task performance. To this end, we study the dynamic relationship between the encoded linguistic information and task performance from the viewpoint of Pareto Optimality. Its key idea is to obtain a set of models which are Pareto-optimal in terms of both objectives. From this viewpoint, we propose a method to optimize the Pareto-optimal models by formalizing it as a multi-objective optimization problem. We conduct experiments on two popular NLP tasks, i.e., machine translation and language modeling, and investigate the relationship between several kinds of linguistic information and task performances. Experimental results demonstrate that the proposed method is better than a baseline method. Our empirical findings suggest that some syntactic information is helpful for NLP tasks whereas encoding more syntactic information does not necessarily lead to better performance, because the model architecture is also an important factor.",
        "author": "Jiannan Xiang; Huayang Li; Defu Lian; Guoping Huang; Taro Watanabe; Lemao Liu",
        "authorids": "/j/jiannan-xiang/; /h/huayang-li/; /d/defu-lian/; /g/guoping-huang/; /t/taro-watanabe/; /l/lemao-liu/",
        "bibtex": "@inproceedings{xiang-etal-2022-visualizing,\n    title = \"Visualizing the Relationship Between Encoded Linguistic Information and Task Performance\",\n    author = \"Xiang, Jiannan  and\n      Li, Huayang  and\n      Lian, Defu  and\n      Huang, Guoping  and\n      Watanabe, Taro  and\n      Liu, Lemao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.35/\",\n    doi = \"10.18653/v1/2022.findings-acl.35\",\n    pages = \"410--422\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.35.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.35/",
        "pdf_size": 925676,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15142877544866204490&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University; Nara Institute of Science and Technology; University of Science and Techology of China; Tencent AI Lab; Nara Institute of Science and Technology; Tencent AI Lab",
        "aff_domain": "cs.cmu.edu;is.naist.jp;ustc.edu.cn;tencent.com;is.naist.jp;gmail.com",
        "email": "cs.cmu.edu;is.naist.jp;ustc.edu.cn;tencent.com;is.naist.jp;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;1;3",
        "aff_unique_norm": "Carnegie Mellon University;Nara Institute of Science and Technology;University of Science and Technology of China;Tencent",
        "aff_unique_dep": ";;;Tencent AI Lab",
        "aff_unique_url": "https://www.cmu.edu;https://www.nist.go.jp;https://www.ustc.edu.cn;https://ai.tencent.com",
        "aff_unique_abbr": "CMU;NIST;USTC;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;2;1;2",
        "aff_country_unique": "United States;Japan;China"
    },
    {
        "id": "2022.acl-short.7",
        "title": "Voxel-informed Language Grounding",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Natural language applied to natural 2D images describes a fundamentally 3D world. We present the Voxel-informed Language Grounder (VLG), a language grounding model that leverages 3D geometric information in the form of voxel maps derived from the visual input using a volumetric reconstruction model. We show that VLG significantly improves grounding accuracy on SNARE, an object reference game task. At the time of writing, VLG holds the top place on the SNARE leaderboard, achieving SOTA results with a 2.0% absolute improvement.",
        "author": "Rodolfo Corona; Shizhan Zhu; Dan Klein; Trevor Darrell",
        "authorids": "/r/rodolfo-corona/; /s/shizhan-zhu/; /d/dan-klein/; /t/trevor-darrell/",
        "bibtex": "@inproceedings{corona-etal-2022-voxel,\n    title = \"Voxel-informed Language Grounding\",\n    author = \"Corona, Rodolfo  and\n      Zhu, Shizhan  and\n      Klein, Dan  and\n      Darrell, Trevor\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.7/\",\n    doi = \"10.18653/v1/2022.acl-short.7\",\n    pages = \"54--60\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.7.pdf",
        "site": "https://aclanthology.org/2022.acl-short.7/",
        "pdf_size": 611225,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10283649187667848269&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "https://github.com/rcorona/voxel_informed_language_grounding",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.49",
        "title": "WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.",
        "author": "Federico Tavella; Viktor Schlegel; Marta Romeo; Aphrodite Galata; Angelo Cangelosi",
        "authorids": "/f/federico-tavella/; /v/viktor-schlegel/; /m/marta-romeo/; /a/aphrodite-galata/; /a/angelo-cangelosi/",
        "bibtex": "@inproceedings{tavella-etal-2022-wlasl,\n    title = \"{WLASL}-{LEX}: a Dataset for Recognising Phonological Properties in {A}merican {S}ign {L}anguage\",\n    author = \"Tavella, Federico  and\n      Schlegel, Viktor  and\n      Romeo, Marta  and\n      Galata, Aphrodite  and\n      Cangelosi, Angelo\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.49/\",\n    doi = \"10.18653/v1/2022.acl-short.49\",\n    pages = \"453--463\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.49.pdf",
        "site": "https://aclanthology.org/2022.acl-short.49/",
        "pdf_size": 481458,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=704981182704581395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, The University of Manchester; Department of Computer Science, The University of Manchester; Department of Computer Science, The University of Manchester; Department of Computer Science, The University of Manchester; Department of Computer Science, The University of Manchester",
        "aff_domain": "manchester.ac.uk;manchester.ac.uk;manchester.ac.uk;manchester.ac.uk;manchester.ac.uk",
        "email": "manchester.ac.uk;manchester.ac.uk;manchester.ac.uk;manchester.ac.uk;manchester.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Manchester",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.manchester.ac.uk",
        "aff_unique_abbr": "UoM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.acl-long.92",
        "title": "WatClaimCheck: A new Dataset for Claim Entailment and Inference",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We contribute a new dataset for the task of automated fact checking and an evaluation of state of the art algorithms. The dataset includes claims (from speeches, interviews, social media and news articles), review articles published by professional fact checkers and premise articles used by those professional fact checkers to support their review and verify the veracity of the claims. An important challenge in the use of premise articles is the identification of relevant passages that will help to infer the veracity of a claim. We show that transferring a dense passage retrieval model trained with review articles improves the retrieval quality of passages in premise articles. We report results for the prediction of claim veracity by inference from premise articles.",
        "author": "Kashif Khan; Ruizhe Wang; Pascal Poupart",
        "authorids": "/k/kashif-khan/; /r/ruizhe-wang/; /p/pascal-poupart/",
        "bibtex": "@inproceedings{khan-etal-2022-watclaimcheck,\n    title = \"{W}at{C}laim{C}heck: A new Dataset for Claim Entailment and Inference\",\n    author = \"Khan, Kashif  and\n      Wang, Ruizhe  and\n      Poupart, Pascal\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.92/\",\n    doi = \"10.18653/v1/2022.acl-long.92\",\n    pages = \"1293--1304\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.92.pdf",
        "site": "https://aclanthology.org/2022.acl-long.92/",
        "pdf_size": 618591,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1773637341822848770&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science, University of Waterloo, Ontario, Canada; Computer Science, University of Waterloo, Ontario, Canada; Computer Science, University of Waterloo, Vector Institute, Ontario, Canada",
        "aff_domain": "iwrk.com;uwaterloo.ca;uwaterloo.ca",
        "email": "iwrk.com;uwaterloo.ca;uwaterloo.ca",
        "github": "https://github.com/nxii/WatClaimCheck",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Waterloo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2022.acl-long.510",
        "title": "Weakly Supervised Word Segmentation for Computational Language Documentation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown. However, in most language documentation scenarios, linguists do not start from a blank page: they may already have a pre-existing dictionary or have initiated manual segmentation of a small part of their data. This paper studies how such a weak supervision can be taken advantage of in Bayesian non-parametric models of segmentation. Our experiments on two very low resource languages (Mboshi and Japhug), whose documentation is still in progress, show that weak supervision can be beneficial to the segmentation quality. In addition, we investigate an incremental learning scenario where manual segmentations are provided in a sequential manner. This work opens the way for interactive annotation tools for documentary linguists.",
        "author": "Shu Okabe; Laurent Besacier; Fran\u00e7ois Yvon",
        "authorids": "/s/shu-okabe/; /l/laurent-besacier/; /f/francois-yvon/",
        "bibtex": "@inproceedings{okabe-etal-2022-weakly,\n    title = \"Weakly Supervised Word Segmentation for Computational Language Documentation\",\n    author = \"Okabe, Shu  and\n      Besacier, Laurent  and\n      Yvon, Fran{\\c{c}}ois\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.510/\",\n    doi = \"10.18653/v1/2022.acl-long.510\",\n    pages = \"7385--7398\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.510.pdf",
        "site": "https://aclanthology.org/2022.acl-long.510/",
        "pdf_size": 334618,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17583972066073865201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Univ. Paris-Saclay & CNRS; Naver Labs Europe; Univ. Paris-Saclay & CNRS",
        "aff_domain": "lisn.fr;naverlabs.com;limsi.fr",
        "email": "lisn.fr;naverlabs.com;limsi.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Paris-Saclay;NAVER LABS",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.universite-paris-saclay.fr;https://labs.naver.com",
        "aff_unique_abbr": "Paris-Saclay;NLE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;Unknown"
    },
    {
        "id": "2022.findings-acl.139",
        "title": "Weighted self Distillation for Chinese word segmentation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent researches show that multi-criteria resources and n-gram features are beneficial to Chinese Word Segmentation (CWS). However, these methods rely heavily on such additional information mentioned above and focus less on the model itself. We thus propose a novel neural framework, named Weighted self Distillation for Chinese word segmentation (WeiDC). The framework, which only requires unigram features, adopts self-distillation technology with four hand-crafted weight modules and two teacher models configurations. Experiment results show that WeiDC can make use of character features to learn contextual knowledge and successfully achieve state-of-the-art or competitive performance in terms of strictly closed test settings on SIGHAN Bakeoff benchmark datasets. Moreover, further experiments and analyses also demonstrate the robustness of WeiDC. Source codes of this paper are available on Github.",
        "author": "Rian He; Shubin Cai; Zhong Ming; Jialei Zhang",
        "authorids": "/r/rian-he/; /s/shubin-cai/; /z/zhong-ming/; /j/jialei-zhang/",
        "bibtex": "@inproceedings{he-etal-2022-weighted,\n    title = \"Weighted self Distillation for {C}hinese word segmentation\",\n    author = \"He, Rian  and\n      Cai, Shubin  and\n      Ming, Zhong  and\n      Zhang, Jialei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.139/\",\n    doi = \"10.18653/v1/2022.findings-acl.139\",\n    pages = \"1757--1770\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.139.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.139/",
        "pdf_size": 996242,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5793968128426084986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "National Engineering Laboratory for Big Data System Computing Technology; National Engineering Laboratory for Big Data System Computing Technology; National Engineering Laboratory for Big Data System Computing Technology; National Engineering Laboratory for Big Data System Computing Technology",
        "aff_domain": "email.szu.edu.cn;szu.edu.cn;szu.edu.cn;email.szu.edu.cn",
        "email": "email.szu.edu.cn;szu.edu.cn;szu.edu.cn;email.szu.edu.cn",
        "github": "https://github.com/Anzi20/WeiDC",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National Engineering Laboratory for Big Data System Computing Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.479",
        "title": "What Makes Reading Comprehension Questions Difficult?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "For a natural language understanding benchmark to be useful in research, it has to consist of examples that are diverse and difficult enough to discriminate among current and near-future state-of-the-art systems. However, we do not yet know how best to select text sources to collect a variety of challenging examples. In this study, we crowdsource multiple-choice reading comprehension questions for passages taken from seven qualitatively distinct sources, analyzing what attributes of passages contribute to the difficulty and question types of the collected examples. To our surprise, we find that passage source, length, and readability measures do not significantly affect question difficulty. Through our manual annotation of seven reasoning types, we observe several trends between passage sources and reasoning types, e.g., logical reasoning is more often required in questions written for technical passages. These results suggest that when creating a new benchmark dataset, selecting a diverse set of passages can help ensure a diverse range of question types, but that passage difficulty need not be a priority.",
        "author": "Saku Sugawara; Nikita Nangia; Alex Warstadt; Samuel Bowman",
        "authorids": "/s/saku-sugawara/; /n/nikita-nangia/; /a/alex-warstadt/; /s/samuel-bowman/",
        "bibtex": "@inproceedings{sugawara-etal-2022-makes,\n    title = \"What Makes Reading Comprehension Questions Difficult?\",\n    author = \"Sugawara, Saku  and\n      Nangia, Nikita  and\n      Warstadt, Alex  and\n      Bowman, Samuel\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.479/\",\n    doi = \"10.18653/v1/2022.acl-long.479\",\n    pages = \"6951--6971\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.479.pdf",
        "site": "https://aclanthology.org/2022.acl-long.479/",
        "pdf_size": 1858206,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12667516516131166204&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "National Institute of Informatics; New York University; New York University; New York University",
        "aff_domain": "nii.ac.jp;nyu.edu;nyu.edu;nyu.edu",
        "email": "nii.ac.jp;nyu.edu;nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "National Institute of Informatics;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nii.ac.jp/;https://www.nyu.edu",
        "aff_unique_abbr": "NII;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2022.findings-acl.39",
        "title": "What Works and Doesn\u2019t Work, A Deep Decoder for Neural Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT). Transformer NMT models are typically strengthened by deeper encoder layers, but deepening their decoder layers usually results in failure. In this paper, we first identify the cause of the failure of the deep decoder in the Transformer model. Inspired by this discovery, we then propose approaches to improving it, with respect to model structure and model training, to make the deep decoder practical in NMT. Specifically, with respect to model structure, we propose a cross-attention drop mechanism to allow the decoder layers to perform their own different roles, to reduce the difficulty of deep-decoder learning. For model training, we propose a collapse reducing training approach to improve the stability and effectiveness of deep-decoder training. We experimentally evaluated our proposed Transformer NMT model structure modification and novel training methods on several popular machine translation benchmarks. The results showed that deepening the NMT model by increasing the number of decoder layers successfully prevented the deepened decoder from degrading to an unconditional language model. In contrast to prior work on deepening an NMT model on the encoder, our method can deepen the model on both the encoder and decoder at the same time, resulting in a deeper model and improved performance.",
        "author": "Zuchao Li; Yiran Wang; Masao Utiyama; Eiichiro Sumita; Hai Zhao; Taro Watanabe",
        "authorids": "/z/zuchao-li/; /y/yiran-wang/; /m/masao-utiyama/; /e/eiichiro-sumita/; /h/hai-zhao/; /t/taro-watanabe/",
        "bibtex": "@inproceedings{li-etal-2022-works,\n    title = \"What Works and Doesn`t Work, A Deep Decoder for Neural Machine Translation\",\n    author = \"Li, Zuchao  and\n      Wang, Yiran  and\n      Utiyama, Masao  and\n      Sumita, Eiichiro  and\n      Zhao, Hai  and\n      Watanabe, Taro\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.39/\",\n    doi = \"10.18653/v1/2022.findings-acl.39\",\n    pages = \"459--471\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.39.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.39/",
        "pdf_size": 418215,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15802222731005713050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Shanghai Jiao Tong University (SJTU), Shanghai, China; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; Shanghai Jiao Tong University (SJTU), Shanghai, China; Nara Institute of Science and Technology (NAIST), Nara, Japan",
        "aff_domain": "sjtu.edu.cn;nict.go.jp;nict.go.jp;nict.go.jp;cs.sjtu.edu.cn;is.naist.jp",
        "email": "sjtu.edu.cn;nict.go.jp;nict.go.jp;nict.go.jp;cs.sjtu.edu.cn;is.naist.jp",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;0;2",
        "aff_unique_norm": "Shanghai Jiao Tong University;National Institute of Information and Communications Technology;Nara Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.nict.go.jp;https://www.naist.jp",
        "aff_unique_abbr": "SJTU;NICT;NAIST",
        "aff_campus_unique_index": "0;1;1;1;0;2",
        "aff_campus_unique": "Shanghai;Kyoto;Nara",
        "aff_country_unique_index": "0;1;1;1;0;1",
        "aff_country_unique": "China;Japan"
    },
    {
        "id": "2022.findings-acl.275",
        "title": "What does it take to bake a cake? The RecipeRef corpus and anaphora resolution in procedural text",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Procedural text contains rich anaphoric phenomena, yet has not received much attention in NLP. To fill this gap, we investigate the textual properties of two types of procedural text, recipes and chemical patents, and generalize an anaphora annotation framework developed for the chemical domain for modeling anaphoric phenomena in recipes. We apply this framework to annotate the RecipeRef corpus with both bridging and coreference relations. Through comparison to chemical patents, we show the complexity of anaphora resolution in recipes. We demonstrate empirically that transfer learning from the chemical domain improves resolution of anaphora in recipes, suggesting transferability of general procedural knowledge.",
        "author": "Biaoyan Fang; Timothy Baldwin; Karin Verspoor",
        "authorids": "/b/biaoyan-fang/; /t/timothy-baldwin/; /k/karin-verspoor/",
        "bibtex": "@inproceedings{fang-etal-2022-take,\n    title = \"What does it take to bake a cake? The {R}ecipe{R}ef corpus and anaphora resolution in procedural text\",\n    author = \"Fang, Biaoyan  and\n      Baldwin, Timothy  and\n      Verspoor, Karin\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.275/\",\n    doi = \"10.18653/v1/2022.findings-acl.275\",\n    pages = \"3481--3495\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.275.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.275/",
        "pdf_size": 430463,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5046978018535074020&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Melbourne, Australia; MBZUAI, Abu Dhabi+The University of Melbourne, Australia; RMIT University, Australia+The University of Melbourne, Australia",
        "aff_domain": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "github": "https://github.com/biaoyanf/RecipeRef",
        "project": "http://doi.org/10.17632/rcyskfvdv7.1",
        "author_num": 3,
        "aff_unique_index": "0;1+0;2+0",
        "aff_unique_norm": "University of Melbourne;Mohamed bin Zayed University of Artificial Intelligence;RMIT University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.mbzuai.ac.ae;https://www.rmit.edu.au",
        "aff_unique_abbr": "UniMelb;MBZUAI;RMIT",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Abu Dhabi",
        "aff_country_unique_index": "0;1+0;0+0",
        "aff_country_unique": "Australia;United Arab Emirates"
    },
    {
        "id": "2022.acl-long.400",
        "title": "What does the sea say to the shore? A BERT based DST style approach for speaker to dialogue attribution in novels",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a complete pipeline to extract characters in a novel and link them to their direct-speech utterances. Our model is divided into three independent components: extracting direct-speech, compiling a list of characters, and attributing those characters to their utterances. Although we find that existing systems can perform the first two tasks accurately, attributing characters to direct speech is a challenging problem due to the narrator\u2019s lack of explicit character mentions, and the frequent use of nominal and pronominal coreference when such explicit mentions are made. We adapt the progress made on Dialogue State Tracking to tackle a new problem: attributing speakers to dialogues. This is the first application of deep learning to speaker attribution, and it shows that is possible to overcome the need for the hand-crafted features and rules used in the past. Our full pipeline improves the performance of state-of-the-art models by a relative 50% in F1-score.",
        "author": "Carolina Cuesta-Lazaro; Animesh Prasad; Trevor Wood",
        "authorids": "/c/carolina-cuesta-lazaro/; /a/animesh-prasad/; /t/trevor-wood/",
        "bibtex": "@inproceedings{cuesta-lazaro-etal-2022-sea,\n    title = \"What does the sea say to the shore? A {BERT} based {DST} style approach for speaker to dialogue attribution in novels\",\n    author = \"Cuesta-Lazaro, Carolina  and\n      Prasad, Animesh  and\n      Wood, Trevor\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.400/\",\n    doi = \"10.18653/v1/2022.acl-long.400\",\n    pages = \"5820--5829\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.400.pdf",
        "site": "https://aclanthology.org/2022.acl-long.400/",
        "pdf_size": 545192,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16253175566119550402&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Computational Cosmology, Durham University, UK; Alexa AI, Amazon, UK; Alexa AI, Amazon, UK",
        "aff_domain": "durham.ac.uk;amazon.com;amazon.com",
        "email": "durham.ac.uk;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Durham University;Amazon",
        "aff_unique_dep": "Institute for Computational Cosmology;Alexa AI",
        "aff_unique_url": "https://www.dur.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": ";Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.331",
        "title": "What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Accurate automatic evaluation metrics for open-domain dialogs are in high demand. Existing model-based metrics for system response evaluation are trained on human annotated data, which is cumbersome to collect. In this work, we propose to use information that can be automatically extracted from the next user utterance, such as its sentiment or whether the user explicitly ends the conversation, as a proxy to measure the quality of the previous system response. This allows us to train on a massive set of dialogs with weak supervision, without requiring manual system turn quality annotations. Experiments show that our model is comparable to models trained on human annotated data. Furthermore, our model generalizes across both spoken and written open-domain dialog corpora collected from real and paid users.",
        "author": "Sarik Ghazarian; Behnam Hedayatnia; Alexandros Papangelis; Yang Liu; Dilek Hakkani-Tur",
        "authorids": "/s/sarik-ghazarian/; /b/behnam-hedayatnia/; /a/alexandros-papangelis/; /y/yang-liu-icsi/; /d/dilek-hakkani-tur/",
        "bibtex": "@inproceedings{ghazarian-etal-2022-wrong,\n    title = \"What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation\",\n    author = \"Ghazarian, Sarik  and\n      Hedayatnia, Behnam  and\n      Papangelis, Alexandros  and\n      Liu, Yang  and\n      Hakkani-Tur, Dilek\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.331/\",\n    doi = \"10.18653/v1/2022.findings-acl.331\",\n    pages = \"4194--4204\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.331.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.331/",
        "pdf_size": 574320,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=736641025998451120&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of Southern California / Information Sciences Institute+Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "isi.edu;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "isi.edu;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "University of Southern California;Amazon",
        "aff_unique_dep": "Information Sciences Institute;Amazon Alexa AI",
        "aff_unique_url": "https://www.usc.edu;https://www.amazon.com",
        "aff_unique_abbr": "USC;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.86",
        "title": "What to Learn, and How: Toward Effective Learning from Rationales",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses:1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction. Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.",
        "author": "Samuel Carton; Surya Kanoria; Chenhao Tan",
        "authorids": "/s/samuel-carton/; /s/surya-kanoria/; /c/chenhao-tan/",
        "bibtex": "@inproceedings{carton-etal-2022-learn,\n    title = \"What to Learn, and How: {T}oward Effective Learning from Rationales\",\n    author = \"Carton, Samuel  and\n      Kanoria, Surya  and\n      Tan, Chenhao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.86/\",\n    doi = \"10.18653/v1/2022.findings-acl.86\",\n    pages = \"1075--1088\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.86.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.86/",
        "pdf_size": 1041573,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17736478576556903809&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Chicago; University of Colorado Boulder; University of Chicago",
        "aff_domain": "uchicago.edu;colorado.edu;uchicago.edu",
        "email": "uchicago.edu;colorado.edu;uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Chicago;University of Colorado",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uchicago.edu;https://www.colorado.edu",
        "aff_unique_abbr": "UChicago;CU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Boulder",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.84",
        "title": "When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Data Augmentation (DA) is known to improve the generalizability of deep neural networks. Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples. To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively generate or re-weight augmented samples with respect to the task objective during training. However, these adaptive DA methods: (1) are computationally expensive and not sample-efficient, and (2) are designed merely for a specific setting. In this work, we present a universal DA technique, called Glitter, to overcome both issues. Glitter can be plugged into any DA method, making training sample-efficient without sacrificing performance. From a pre-generated pool of augmented samples, Glitter adaptively selects a subset of worst-case samples with maximal loss, analogous to adversarial DA. Without altering the training strategy, the task objective can be optimized on the selected subset. Our thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three widely used training setups including consistency training, self-distillation and knowledge distillation reveal that Glitter is substantially faster to train and achieves a competitive performance, compared to strong baselines.",
        "author": "Ehsan Kamalloo; Mehdi Rezagholizadeh; Ali Ghodsi",
        "authorids": "/e/ehsan-kamalloo/; /m/mehdi-rezagholizadeh/; /a/ali-ghodsi/",
        "bibtex": "@inproceedings{kamalloo-etal-2022-chosen,\n    title = \"When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation\",\n    author = \"Kamalloo, Ehsan  and\n      Rezagholizadeh, Mehdi  and\n      Ghodsi, Ali\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.84/\",\n    doi = \"10.18653/v1/2022.findings-acl.84\",\n    pages = \"1048--1062\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.84.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.84/",
        "pdf_size": 1105514,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14149184201307142821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Alberta+Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; University of Waterloo",
        "aff_domain": "ualberta.ca;huawei.com;uwaterloo.ca",
        "email": "ualberta.ca;huawei.com;uwaterloo.ca",
        "github": "https://github.com/huawei-noah/KD-NLP/tree/main/Glitter",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;2",
        "aff_unique_norm": "University of Alberta;Huawei;University of Waterloo",
        "aff_unique_dep": ";Noah\u2019s Ark Lab;",
        "aff_unique_url": "https://www.ualberta.ca;https://www.huawei.com;https://uwaterloo.ca",
        "aff_unique_abbr": "UAlberta;Huawei;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "Canada;China"
    },
    {
        "id": "2022.acl-short.71",
        "title": "When classifying grammatical role, BERT doesn\u2019t care about word order... except when it matters",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey \u201cThe chef chopped the onion,\u201d not \u201cThe onion chopped the chef.\u201d Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like \u201cThe onion chopped the chef\u201d. We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.",
        "author": "Isabel Papadimitriou; Richard Futrell; Kyle Mahowald",
        "authorids": "/i/isabel-papadimitriou/; /r/richard-futrell/; /k/kyle-mahowald/",
        "bibtex": "@inproceedings{papadimitriou-etal-2022-classifying-grammatical,\n    title = \"When classifying grammatical role, {BERT} doesn`t care about word order... except when it matters\",\n    author = \"Papadimitriou, Isabel  and\n      Futrell, Richard  and\n      Mahowald, Kyle\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.71/\",\n    doi = \"10.18653/v1/2022.acl-short.71\",\n    pages = \"636--643\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.71.pdf",
        "site": "https://aclanthology.org/2022.acl-short.71/",
        "pdf_size": 320431,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11980597128910326793&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University; University of California, Irvine; The University of Texas at Austin",
        "aff_domain": "stanford.edu;uci.edu;utexas.edu",
        "email": "stanford.edu;uci.edu;utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Stanford University;University of California, Irvine;University of Texas at Austin",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.uci.edu;https://www.utexas.edu",
        "aff_unique_abbr": "Stanford;UCI;UT Austin",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Stanford;Irvine;Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.411",
        "title": "When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Indirect speech such as sarcasm achieves a constellation of discourse goals in human communication. While the indirectness of figurative language warrants speakers to achieve certain pragmatic goals, it is challenging for AI agents to comprehend such idiosyncrasies of human communication. Though sarcasm identification has been a well-explored topic in dialogue analysis, for conversational systems to truly grasp a conversation\u2019s innate meaning and generate appropriate responses, simply detecting sarcasm is not enough; it is vital to explain its underlying sarcastic connotation to capture its true essence. In this work, we study the discourse structure of sarcastic conversations and propose a novel task \u2013 Sarcasm Explanation in Dialogue (SED). Set in a multimodal and code-mixed setting, the task aims to generate natural language explanations of satirical conversations. To this end, we curate WITS, a new dataset to support our task. We propose MAF (Modality Aware Fusion), a multimodal context-aware attention and global information fusion module to capture multimodality and use it to benchmark WITS. The proposed attention module surpasses the traditional multimodal fusion baselines and reports the best performance on almost all metrics. Lastly, we carry out detailed analysis both quantitatively and qualitatively.",
        "author": "Shivani Kumar; Atharva Kulkarni; Md Shad Akhtar; Tanmoy Chakraborty",
        "authorids": "/s/shivani-kumar/; /a/atharva-kulkarni/; /m/md-shad-akhtar/; /t/tanmoy-chakraborty/",
        "bibtex": "@inproceedings{kumar-etal-2022-become,\n    title = \"When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues\",\n    author = \"Kumar, Shivani  and\n      Kulkarni, Atharva  and\n      Akhtar, Md Shad  and\n      Chakraborty, Tanmoy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.411/\",\n    doi = \"10.18653/v1/2022.acl-long.411\",\n    pages = \"5956--5968\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.411.pdf",
        "site": "https://aclanthology.org/2022.acl-long.411/",
        "pdf_size": 1203740,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15087369411307861203&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Indraprastha Institute of Information Technology Delhi, India; Indraprastha Institute of Information Technology Delhi, India; Indraprastha Institute of Information Technology Delhi, India; Indraprastha Institute of Information Technology Delhi, India",
        "aff_domain": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "email": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indraprastha Institute of Information Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://iiitd.ac.in",
        "aff_unique_abbr": "IIIT Delhi",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Delhi",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2022.acl-short.30",
        "title": "When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Transfer learning (TL) in natural language processing (NLP) has seen a surge of interest in recent years, as pre-trained models have shown an impressive ability to transfer to novel tasks. Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning: training on an intermediate task before training on the target task (STILTs), using multi-task learning (MTL) to train jointly on a supplementary task and the target task (pairwise MTL), or simply using MTL to train jointly on all available datasets (MTL-ALL). In this work, we compare all three TL methods in a comprehensive analysis on the GLUE dataset suite. We find that there is a simple heuristic for when to use one of these techniques over the other: pairwise MTL is better than STILTs when the target task has fewer instances than the supporting task and vice versa. We show that this holds true in more than 92% of applicable cases on the GLUE dataset and validate this hypothesis with experiments varying dataset size. The simplicity and effectiveness of this heuristic is surprising and warrants additional exploration by the TL community. Furthermore, we find that MTL-ALL is worse than the pairwise methods in almost every case. We hope this study will aid others as they choose between TL methods for NLP tasks.",
        "author": "Orion Weller; Kevin Seppi; Matt Gardner",
        "authorids": "/o/orion-weller/; /k/kevin-seppi/; /m/matt-gardner/",
        "bibtex": "@inproceedings{weller-etal-2022-use,\n    title = \"When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning\",\n    author = \"Weller, Orion  and\n      Seppi, Kevin  and\n      Gardner, Matt\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.30/\",\n    doi = \"10.18653/v1/2022.acl-short.30\",\n    pages = \"272--282\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.30.pdf",
        "site": "https://aclanthology.org/2022.acl-short.30/",
        "pdf_size": 711829,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1640991498951873044&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Johns Hopkins University; Brigham Young University; Microsoft Semantic Machines",
        "aff_domain": "jhu.edu; ; ",
        "email": "jhu.edu; ; ",
        "github": "https://github.com/orionw/MTLvsIFT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Johns Hopkins University;Brigham Young University;Microsoft",
        "aff_unique_dep": ";;Semantic Machines",
        "aff_unique_url": "https://www.jhu.edu;https://www.byu.edu;https://www.microsoft.com",
        "aff_unique_abbr": "JHU;BYU;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.73",
        "title": "Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most dialog systems posit that users have figured out clear and specific goals before starting an interaction. For example, users have determined the departure, the destination, and the travel time for booking a flight. However, in many scenarios, limited by experience and knowledge, users may know what they need, but still struggle to figure out clear and specific goals by determining all the necessary slots. In this paper, we identify this challenge, and make a step forward by collecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog sessions and 168k utterances for 4 dialog types and 5 domains. Within each session, an agent first provides user-goal-related knowledge to help figure out clear and specific goals, and then help achieve them. Furthermore, we propose a mixed-type dialog model with a novel Prompt-based continual learning mechanism. Specifically, the mechanism enables the model to continually strengthen its ability on any specific type by utilizing existing dialog corpora effectively.",
        "author": "Zeming Liu; Jun Xu; Zeyang Lei; Haifeng Wang; Zheng-Yu Niu; Hua Wu",
        "authorids": "/z/zeming-liu/; /j/jun-xu/; /z/zeyang-lei/; /h/haifeng-wang/; /z/zheng-yu-niu/; /h/hua-wu/",
        "bibtex": "@inproceedings{liu-etal-2022-go,\n    title = \"Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals\",\n    author = \"Liu, Zeming  and\n      Xu, Jun  and\n      Lei, Zeyang  and\n      Wang, Haifeng  and\n      Niu, Zheng-Yu  and\n      Wu, Hua\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.73/\",\n    doi = \"10.18653/v1/2022.acl-long.73\",\n    pages = \"1024--1034\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.73.pdf",
        "site": "https://aclanthology.org/2022.acl-long.73/",
        "pdf_size": 596038,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=195994378045036808&as_sdt=800005&sciodt=0,15&hl=en",
        "gs_version_total": 6,
        "aff": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China+Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China",
        "aff_domain": "ir.hit.edu.cn;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "ir.hit.edu.cn;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2022-DuClarifyDial1024",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "Harbin Institute of Technology;Baidu",
        "aff_unique_dep": "Research Center for Social Computing and Information Retrieval;Baidu Inc.",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.baidu.com",
        "aff_unique_abbr": "HIT;Baidu",
        "aff_campus_unique_index": "0+1;1;1;1;1;1",
        "aff_campus_unique": "Harbin;Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.341",
        "title": "Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Social media is a breeding ground for threat narratives and related conspiracy theories. In these, an outside group threatens the integrity of an inside group, leading to the emergence of sharply defined group identities: Insiders \u2013 agents with whom the authors identify and Outsiders \u2013 agents who threaten the insiders. Inferring the members of these groups constitutes a challenging new NLP task: (i) Information is distributed over many poorly-constructed posts; (ii) Threats and threat agents are highly contextual, with the same post potentially having multiple agents assigned to membership in either group; (iii) An agent\u2019s identity is often implicit and transitive; and (iv) Phrases used to imply Outsider status often do not follow common negative sentiment patterns. To address these challenges, we define a novel Insider-Outsider classification task. Because we are not aware of any appropriate existing datasets or attendant models, we introduce a labeled dataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages pretrained language modeling to classify Insiders and Outsiders. NP2IO is shown to be robust, generalizing to noun phrases not seen during training, and exceeding the performance of non-trivial baseline models by 20%.",
        "author": "Pavan Holur; Tianyi Wang; Shadi Shahsavari; Timothy Tangherlini; Vwani Roychowdhury",
        "authorids": "/p/pavan-holur/; /t/tianyi-wang/; /s/shadi-shahsavari/; /t/timothy-tangherlini/; /v/vwani-roychowdhury/",
        "bibtex": "@inproceedings{holur-etal-2022-side,\n    title = \"Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media\",\n    author = \"Holur, Pavan  and\n      Wang, Tianyi  and\n      Shahsavari, Shadi  and\n      Tangherlini, Timothy  and\n      Roychowdhury, Vwani\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.341/\",\n    doi = \"10.18653/v1/2022.acl-long.341\",\n    pages = \"4975--4987\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.341.pdf",
        "site": "https://aclanthology.org/2022.acl-long.341/",
        "pdf_size": 1171792,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17346365940962368572&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering, UCLA; Department of Electrical and Computer Engineering, UCLA; Department of Electrical and Computer Engineering, UCLA; Department of Scandinavian, UC Berkeley; Department of Electrical and Computer Engineering, UCLA",
        "aff_domain": "ucla.edu;ucla.edu;ucla.edu;berkeley.edu;ucla.edu",
        "email": "ucla.edu;ucla.edu;ucla.edu;berkeley.edu;ucla.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of California, Los Angeles;University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Scandinavian Studies",
        "aff_unique_url": "https://www.ucla.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "UCLA;UC Berkeley",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Los Angeles;Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.58",
        "title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis for this brittleness of generation models is that it is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors during generation, analyze why perplexity fails to capture this accumulation of errors, and empirically show that this accumulation results in poor generation quality.",
        "author": "Kushal Arora; Layla El Asri; Hareesh Bahuleyan; Jackie Cheung",
        "authorids": "/k/kushal-arora/; /l/layla-el-asri/; /h/hareesh-bahuleyan/; /j/jackie-chi-kit-cheung/",
        "bibtex": "@inproceedings{arora-etal-2022-exposure,\n    title = \"Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation\",\n    author = \"Arora, Kushal  and\n      El Asri, Layla  and\n      Bahuleyan, Hareesh  and\n      Cheung, Jackie\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.58/\",\n    doi = \"10.18653/v1/2022.findings-acl.58\",\n    pages = \"700--710\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.58.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.58/",
        "pdf_size": 344581,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11414223152617375138&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Mila / McGill University; Borealis AI; Zalando SE; Mila / McGill University",
        "aff_domain": "mail.mcgill.ca;cs.mcgill.ca;borealisai.com;gmail.com",
        "email": "mail.mcgill.ca;cs.mcgill.ca;borealisai.com;gmail.com",
        "github": "https://github.com/kushalarora/quantifying_exposure_bias",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "McGill University;Borealis AI;Zalando SE",
        "aff_unique_dep": "Mila;;",
        "aff_unique_url": "https://www.mcgill.ca;https://www.borealisai.com;https://www.zalando.de",
        "aff_unique_abbr": "McGill;Borealis AI;Zalando",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montreal;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Canada;Germany"
    },
    {
        "id": "2022.findings-acl.194",
        "title": "Why don\u2019t people use character-level machine translation?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.",
        "author": "Jind\u0159ich Libovick\u00fd; Helmut Schmid; Alexander Fraser",
        "authorids": "/j/jindrich-libovicky/; /h/helmut-schmid/; /a/alexander-fraser/",
        "bibtex": "@inproceedings{libovicky-etal-2022-dont,\n    title = \"Why don`t people use character-level machine translation?\",\n    author = \"Libovick{\\'y}, Jind{\\v{r}}ich  and\n      Schmid, Helmut  and\n      Fraser, Alexander\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.194/\",\n    doi = \"10.18653/v1/2022.findings-acl.194\",\n    pages = \"2470--2485\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.194.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.194/",
        "pdf_size": 290324,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8561117001114116048&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Faculty of Mathematics and Physics, Charles University, Prague, Czech Republic; Center for Information and Speech Processing, LMU Munich, Germany; Center for Information and Speech Processing, LMU Munich, Germany",
        "aff_domain": "ufal.mff.cuni.cz;cis.lmu.de;cis.lmu.de",
        "email": "ufal.mff.cuni.cz;cis.lmu.de;cis.lmu.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Charles University;LMU Munich",
        "aff_unique_dep": "Faculty of Mathematics and Physics;Center for Information and Speech Processing",
        "aff_unique_url": "https://www.cuni.cz;https://www.lmu.de",
        "aff_unique_abbr": "Charles University;LMU",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Prague;Munich",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Czech Republic;Germany"
    },
    {
        "id": "2022.acl-long.328",
        "title": "WikiDiverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal Entity Linking (MEL) which aims at linking mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia), is an essential task for many multimodal applications. Although much attention has been paid to MEL, the shortcomings of existing MEL datasets including limited contextual topics and entity types, simplified mention ambiguity, and restricted availability, have caused great obstacles to the research and application of MEL. In this paper, we present WikiDiverse, a high-quality human-annotated MEL dataset with diversified contextual topics and entity types from Wikinews, which uses Wikipedia as the corresponding knowledge base. A well-tailored annotation procedure is adopted to ensure the quality of the dataset. Based on WikiDiverse, a sequence of well-designed MEL models with intra-modality and inter-modality attentions are implemented, which utilize the visual information of images more adequately than existing MEL models do. Extensive experimental analyses are conducted to investigate the contributions of different modalities in terms of MEL, facilitating the future research on this task.",
        "author": "Xuwu Wang; Junfeng Tian; Min Gui; Zhixu Li; Rui Wang; Ming Yan; Lihan Chen; Yanghua Xiao",
        "authorids": "/x/xuwu-wang/; /j/junfeng-tian/; /m/min-gui/; /z/zhixu-li/; /r/rui-wang/; /m/ming-yan/; /l/lihan-chen/; /y/yanghua-xiao/",
        "bibtex": "@inproceedings{wang-etal-2022-wikidiverse,\n    title = \"{W}iki{D}iverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types\",\n    author = \"Wang, Xuwu  and\n      Tian, Junfeng  and\n      Gui, Min  and\n      Li, Zhixu  and\n      Wang, Rui  and\n      Yan, Ming  and\n      Chen, Lihan  and\n      Xiao, Yanghua\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.328/\",\n    doi = \"10.18653/v1/2022.acl-long.328\",\n    pages = \"4785--4797\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.328.pdf",
        "site": "https://aclanthology.org/2022.acl-long.328/",
        "pdf_size": 1798303,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11691621759442826110&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Fudan University, China; Alibaba Group, China; Shopee, Singapore; School of Computer Science, Fudan University, China; Vipshop (China) Co., Ltd., China; Alibaba Group, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China+Fudan-Aishu Cognitive Intelligence Joint Research Center, China",
        "aff_domain": "fudan.edu.cn;alibaba-inc.com;shopee.com;fudan.edu.cn;hotmail.com;alibaba-inc.com;gmail.com;fudan.edu.cn",
        "email": "fudan.edu.cn;alibaba-inc.com;shopee.com;fudan.edu.cn;hotmail.com;alibaba-inc.com;gmail.com;fudan.edu.cn",
        "github": "https://github.com/wangxw5/wikiDiverse",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;0;3;1;0;0+0",
        "aff_unique_norm": "Fudan University;Alibaba Group;Shopee;Vipshop (China) Co., Ltd.",
        "aff_unique_dep": "School of Computer Science;;;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.alibaba.com;https://shopee.sg;https://www.vip.com",
        "aff_unique_abbr": "Fudan;Alibaba;;Vipshop",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0+0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2022.acl-long.476",
        "title": "Word Order Does Matter and Shuffled Language Models Know It",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models\u2019 good performance with shuffled text. We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain a notion of word order information. We show this is in part due to a subtlety in how shuffling is implemented in previous work \u2013 before rather than after subword segmentation. Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities. Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.",
        "author": "Mostafa Abdou; Vinit Ravishankar; Artur Kulmizev; Anders S\u00f8gaard",
        "authorids": "/m/mostafa-abdou/; /v/vinit-ravishankar/; /a/artur-kulmizev/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{abdou-etal-2022-word,\n    title = \"Word Order Does Matter and Shuffled Language Models Know It\",\n    author = \"Abdou, Mostafa  and\n      Ravishankar, Vinit  and\n      Kulmizev, Artur  and\n      S{\\o}gaard, Anders\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.476/\",\n    doi = \"10.18653/v1/2022.acl-long.476\",\n    pages = \"6907--6919\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.476.pdf",
        "site": "https://aclanthology.org/2022.acl-long.476/",
        "pdf_size": 6964740,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15249702932229820537&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Copenhagen; Language Technology Group, Department of Informatics, University of Oslo; Department of Linguistics and Philology, Uppsala University; Department of Computer Science, University of Copenhagen",
        "aff_domain": "di.ku.dk;di.ku.dk;ifi.uio.no; ",
        "email": "di.ku.dk;di.ku.dk;ifi.uio.no; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Copenhagen;University of Oslo;Uppsala University",
        "aff_unique_dep": "Department of Computer Science;Department of Informatics;Department of Linguistics and Philology",
        "aff_unique_url": "https://www.ku.dk;https://www.uio.no;https://www.uu.se",
        "aff_unique_abbr": "UCPH;UiO;UU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Denmark;Norway;Sweden"
    },
    {
        "id": "2022.acl-long.283",
        "title": "Word Segmentation as Unsupervised Constituency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Word identification from continuous input is typically viewed as a segmentation task. Experiments with human adults suggest that familiarity with syntactic structures in their native language also influences word identification in artificial languages; however, the relation between syntactic processing and word identification is yet unclear. This work takes one step forward by exploring a radically different approach of word identification, in which segmentation of a continuous input is viewed as a process isomorphic to unsupervised constituency parsing. Besides formalizing the approach, this study reports simulations of human experiments with DIORA (Drozdov et al., 2020), a neural unsupervised constituency parser. Results show that this model can reproduce human behavior in word identification experiments, suggesting that this is a viable approach to study word identification and its relation to syntactic processing.",
        "author": "Raquel G. Alhama",
        "authorids": "/r/raquel-g-alhama/",
        "bibtex": "@inproceedings{alhama-2022-word,\n    title = \"Word Segmentation as Unsupervised Constituency Parsing\",\n    author = \"Alhama, Raquel G.\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.283/\",\n    doi = \"10.18653/v1/2022.acl-long.283\",\n    pages = \"4103--4112\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.283.pdf",
        "site": "https://aclanthology.org/2022.acl-long.283/",
        "pdf_size": 372591,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6707778315026249805&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tilburg University",
        "aff_domain": "uvt.nl",
        "email": "uvt.nl",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Tilburg University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tilburguniversity.edu/",
        "aff_unique_abbr": "Tilburg U",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2022.findings-acl.309",
        "title": "Word Segmentation by Separation Inference for East Asian Languages",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Chinese Word Segmentation (CWS) intends to divide a raw sentence into words through sequence labeling. Thinking in reverse, CWS can also be viewed as a process of grouping a sequence of characters into a sequence of words. In such a way, CWS is reformed as a separation inference task in every adjacent character pair. Since every character is either connected or not connected to the others, the tagging schema is simplified as two tags \u201cConnection\u201d (C) or \u201cNoConnection\u201d (NC). Therefore, bigram is specially tailored for \u201cC-NC\u201d to model the separation state of every two consecutive characters. Our Separation Inference (SpIn) framework is evaluated on five public datasets, is demonstrated to work for machine learning and deep learning models, and outperforms state-of-the-art performance for CWS in all experiments. Performance boosts on Japanese Word Segmentation (JWS) and Korean Word Segmentation (KWS) further prove the framework is universal and effective for East Asian Languages.",
        "author": "Yu Tong; Jingzhi Guo; Jizhe Zhou; Ge Chen; Guokai Zheng",
        "authorids": "/y/yu-tong/; /j/jingzhi-guo/; /j/jizhe-zhou/; /g/ge-chen/; /g/guokai-zheng/",
        "bibtex": "@inproceedings{tong-etal-2022-word,\n    title = \"Word Segmentation by Separation Inference for {E}ast {A}sian Languages\",\n    author = \"Tong, Yu  and\n      Guo, Jingzhi  and\n      Zhou, Jizhe  and\n      Chen, Ge  and\n      Zheng, Guokai\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.309/\",\n    doi = \"10.18653/v1/2022.findings-acl.309\",\n    pages = \"3924--3934\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.309.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.309/",
        "pdf_size": 1446496,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7917559588769322219&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Macau, Macau, China+Department of Computer Science, Sichuan University, China; Department of Computer Science, University of Macau, Macau, China+Department of Computer Science, Sichuan University, China; Department of Computer Science, Sichuan University, China; vivo AI Lab, Shenzhen, China; vivo AI Lab, Shenzhen, China",
        "aff_domain": "umac.mo;umac.mo;umac.mo;gmail.com;gmail.com",
        "email": "umac.mo;umac.mo;umac.mo;gmail.com;gmail.com",
        "github": "https://github.com/UM-NLPer/SpIn-WS",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;1;2;2",
        "aff_unique_norm": "University of Macau;Sichuan University;vivo AI Lab",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;",
        "aff_unique_url": "https://www.um.edu.mo;https://www.scu.edu.cn;",
        "aff_unique_abbr": "UM;SCU;",
        "aff_campus_unique_index": "0;0;2;2",
        "aff_campus_unique": "Macau;;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.258",
        "title": "Word-level Perturbation Considering Word Length and Compositional Subwords",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present two simple modifications for word-level perturbation: Word Replacement considering Length (WR-L) and Compositional Word Replacement (CWR).In conventional word replacement, a word in an input is replaced with a word sampled from the entire vocabulary, regardless of the length and context of the target word.WR-L considers the length of a target word by sampling words from the Poisson distribution.CWR considers the compositional candidates by restricting the source of sampling to related words that appear in subword regularization. Experimental results showed that the combination of WR-L and CWR improved the performance of text classification and machine translation.",
        "author": "Tatsuya Hiraoka; Sho Takase; Kei Uchiumi; Atsushi Keyaki; Naoaki Okazaki",
        "authorids": "/t/tatsuya-hiraoka/; /s/sho-takase/; /k/kei-uchiumi/; /a/atsushi-keyaki/; /n/naoaki-okazaki/",
        "bibtex": "@inproceedings{hiraoka-etal-2022-word,\n    title = \"Word-level Perturbation Considering Word Length and Compositional Subwords\",\n    author = \"Hiraoka, Tatsuya  and\n      Takase, Sho  and\n      Uchiumi, Kei  and\n      Keyaki, Atsushi  and\n      Okazaki, Naoaki\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.258/\",\n    doi = \"10.18653/v1/2022.findings-acl.258\",\n    pages = \"3268--3275\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.258.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.258/",
        "pdf_size": 482717,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8482352186240300998&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Tokyo Institute of Technology\u2020; Tokyo Institute of Technology\u2020; Denso IT Laboratory, Inc.\u2021; Denso IT Laboratory, Inc.\u2021; Tokyo Institute of Technology\u2020",
        "aff_domain": "nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;d-itlab.co.jp;d-itlab.co.jp;c.titech.ac.jp",
        "email": "nlp.c.titech.ac.jp;nlp.c.titech.ac.jp;d-itlab.co.jp;d-itlab.co.jp;c.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Tokyo Institute of Technology;Denso IT Laboratory, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.denso.com",
        "aff_unique_abbr": "Titech;Denso IT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.acl-long.161",
        "title": "Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Learning representations of words in a continuous space is perhaps the most fundamental task in NLP, however words interact in ways much richer than vector dot product similarity can provide. Many relationships between words can be expressed set-theoretically, for example, adjective-noun compounds (eg. \u201cred cars\u201d\u2286\u201ccars\u201d) and homographs (eg. \u201ctongue\u201d\u2229\u201cbody\u201d should be similar to \u201cmouth\u201d, while \u201ctongue\u201d\u2229\u201clanguage\u201d should be similar to \u201cdialect\u201d) have natural set-theoretic interpretations. Box embeddings are a novel region-based representation which provide the capability to perform these set-theoretic operations. In this work, we provide a fuzzy-set interpretation of box embeddings, and learn box representations of words using a set-theoretic training objective. We demonstrate improved performance on various word similarity tasks, particularly on less common words, and perform a quantitative and qualitative analysis exploring the additional unique expressivity provided by Word2Box.",
        "author": "Shib Dasgupta; Michael Boratko; Siddhartha Mishra; Shriya Atmakuri; Dhruvesh Patel; Xiang Li; Andrew McCallum",
        "authorids": "/s/shib-dasgupta/; /m/michael-boratko/; /s/siddhartha-mishra/; /s/shriya-atmakuri/; /d/dhruvesh-patel/; /x/xiang-li/; /a/andrew-mccallum/",
        "bibtex": "@inproceedings{dasgupta-etal-2022-word2box,\n    title = \"{W}ord2{B}ox: Capturing Set-Theoretic Semantics of Words using Box Embeddings\",\n    author = \"Dasgupta, Shib  and\n      Boratko, Michael  and\n      Mishra, Siddhartha  and\n      Atmakuri, Shriya  and\n      Patel, Dhruvesh  and\n      Li, Xiang  and\n      McCallum, Andrew\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.161/\",\n    doi = \"10.18653/v1/2022.acl-long.161\",\n    pages = \"2263--2276\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.161.pdf",
        "site": "https://aclanthology.org/2022.acl-long.161/",
        "pdf_size": 702353,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9574408615409915790&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Manning College of Information & Computer Sciences; Manning College of Information & Computer Sciences; Manning College of Information & Computer Sciences; Manning College of Information & Computer Sciences; Manning College of Information & Computer Sciences; Manning College of Information & Computer Sciences; Manning College of Information & Computer Sciences",
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Manning College",
        "aff_unique_dep": "College of Information & Computer Sciences",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2022.acl-short.52",
        "title": "XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Transformer-based models are widely used in natural language understanding (NLU) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders\u2019 success in visual-language tasks while we alter the learning objective to cater to the language-heavy characteristics of NLU. After training with a small number of extra adapting steps and finetuned, the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in general language understanding evaluation (GLUE), situations with adversarial generations (SWAG) benchmarks, and readability benchmarks. We analyze the performance of XDBERT on GLUE to show that the improvement is likely visually grounded.",
        "author": "Chan-Jan Hsu; Hung-yi Lee; Yu Tsao",
        "authorids": "/c/chan-jan-hsu/; /h/hung-yi-lee/; /y/yu-tsao/",
        "bibtex": "@inproceedings{hsu-etal-2022-xdbert,\n    title = \"{XDBERT}: {D}istilling Visual Information to {BERT} from Cross-Modal Systems to Improve Language Understanding\",\n    author = \"Hsu, Chan-Jan  and\n      Lee, Hung-yi  and\n      Tsao, Yu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.52/\",\n    doi = \"10.18653/v1/2022.acl-short.52\",\n    pages = \"479--489\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.52.pdf",
        "site": "https://aclanthology.org/2022.acl-short.52/",
        "pdf_size": 1057352,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2354600884321312312&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 6,
        "aff": "National Taiwan University, Taiwan + Academia Sinica, Taiwan; National Taiwan University, Taiwan; Academia Sinica, Taiwan",
        "aff_domain": "ntu.edu.tw;ntu.edu.tw;citi.sinica.edu.tw",
        "email": "ntu.edu.tw;ntu.edu.tw;citi.sinica.edu.tw",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "National Taiwan University;Academia Sinica",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.sinica.edu.tw",
        "aff_unique_abbr": "NTU;Academia Sinica",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.findings-acl.253",
        "title": "XFUND: A Benchmark Dataset for Multilingual Visually Rich Form Understanding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. However, the existed research work has focused only on the English domain while neglecting the importance of multilingual generalization. In this paper, we introduce a human-annotated multilingual form understanding benchmark dataset named XFUND, which includes form understanding samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese). Meanwhile, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually rich document understanding. Experimental results show that the LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset. The XFUND dataset and the pre-trained LayoutXLM model have been publicly available at https://aka.ms/layoutxlm.",
        "author": "Yiheng Xu; Tengchao Lv; Lei Cui; Guoxin Wang; Yijuan Lu; Dinei Florencio; Cha Zhang; Furu Wei",
        "authorids": "/y/yiheng-xu/; /t/tengchao-lv/; /l/lei-cui/; /g/guoxin-wang/; /y/yijuan-lu/; /d/dinei-florencio/; /c/cha-zhang/; /f/furu-wei/",
        "bibtex": "@inproceedings{xu-etal-2022-xfund,\n    title = \"{XFUND}: A Benchmark Dataset for Multilingual Visually Rich Form Understanding\",\n    author = \"Xu, Yiheng  and\n      Lv, Tengchao  and\n      Cui, Lei  and\n      Wang, Guoxin  and\n      Lu, Yijuan  and\n      Florencio, Dinei  and\n      Zhang, Cha  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.253/\",\n    doi = \"10.18653/v1/2022.findings-acl.253\",\n    pages = \"3214--3224\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.253.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.253/",
        "pdf_size": 9748799,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2309546899801856786&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Microsoft Research Asia+Microsoft Azure AI; Microsoft Research Asia+Microsoft Azure AI; Microsoft Research Asia+Microsoft Azure AI; Microsoft Azure AI; Microsoft Azure AI; Microsoft Azure AI; Microsoft Azure AI; Microsoft Research Asia+Microsoft Azure AI",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "https://aka.ms/layoutxlm",
        "author_num": 8,
        "aff_unique_index": "0+0;0+0;0+0;0;0;0;0;0+0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "MSR Asia",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Asia;",
        "aff_country_unique_index": "0+1;0+1;0+1;1;1;1;1;0+1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.427",
        "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we introduce ELECTRA-style tasks to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.",
        "author": "Zewen Chi; Shaohan Huang; Li Dong; Shuming Ma; Bo Zheng; Saksham Singhal; Payal Bajaj; Xia Song; Xian-Ling Mao; Heyan Huang; Furu Wei",
        "authorids": "/z/zewen-chi/; /s/shaohan-huang/; /l/li-dong/; /s/shuming-ma/; /b/bo-zheng/; /s/saksham-singhal/; /p/payal-bajaj/; /x/xia-song/; /x/xian-ling-mao/; /h/he-yan-huang/; /f/furu-wei/",
        "bibtex": "@inproceedings{chi-etal-2022-xlm,\n    title = \"{XLM}-{E}: Cross-lingual Language Model Pre-training via {ELECTRA}\",\n    author = \"Chi, Zewen  and\n      Huang, Shaohan  and\n      Dong, Li  and\n      Ma, Shuming  and\n      Zheng, Bo  and\n      Singhal, Saksham  and\n      Bajaj, Payal  and\n      Song, Xia  and\n      Mao, Xian-Ling  and\n      Huang, Heyan  and\n      Wei, Furu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.427/\",\n    doi = \"10.18653/v1/2022.acl-long.427\",\n    pages = \"6170--6182\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.427.pdf",
        "site": "https://aclanthology.org/2022.acl-long.427/",
        "pdf_size": 444779,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9187844331498870281&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff": "Beijing Institute of Technology\u2020; Microsoft Corporation\u2021; Microsoft Corporation\u2021; Microsoft Corporation\u2021; Microsoft Corporation\u2021; Microsoft Corporation\u2021; Microsoft Corporation\u2021; Microsoft Corporation\u2021; Beijing Institute of Technology\u2020; Beijing Institute of Technology\u2020; Microsoft Corporation\u2021",
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "github": "https://github.com/microsoft/unilm",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;1;1;1;1;1;1;1;0;0;1",
        "aff_unique_norm": "Beijing Institute of Technology;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "http://www.bit.edu.cn/;https://www.microsoft.com",
        "aff_unique_abbr": "BIT;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1;1;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.587",
        "title": "Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Handing in a paper or exercise and merely receiving \u201cbad\u201d or \u201cincorrect\u201d as feedback is not very helpful when the goal is to improve. Unfortunately, this is currently the kind of feedback given by Automatic Short Answer Grading (ASAG) systems. One of the reasons for this is a lack of content-focused elaborated feedback datasets. To encourage research on explainable and understandable feedback systems, we present the Short Answer Feedback dataset (SAF). Similar to other ASAG datasets, SAF contains learner responses and reference answers to German and English questions. However, instead of only assigning a label or score to the learners\u2019 answers, SAF also contains elaborated feedback explaining the given score. Thus, SAF enables supervised training of models that grade answers and explain where and why mistakes were made. This paper discusses the need for enhanced feedback models in real-world pedagogical scenarios, describes the dataset annotation process, gives a comprehensive analysis of SAF, and provides T5-based baselines for future comparison.",
        "author": "Anna Filighera; Siddharth Parihar; Tim Steuer; Tobias Meuser; Sebastian Ochs",
        "authorids": "/a/anna-filighera/; /s/siddharth-parihar/; /t/tim-steuer/; /t/tobias-meuser/; /s/sebastian-ochs/",
        "bibtex": "@inproceedings{filighera-etal-2022-answer,\n    title = \"Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset\",\n    author = \"Filighera, Anna  and\n      Parihar, Siddharth  and\n      Steuer, Tim  and\n      Meuser, Tobias  and\n      Ochs, Sebastian\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.587/\",\n    doi = \"10.18653/v1/2022.acl-long.587\",\n    pages = \"8577--8591\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.587.pdf",
        "site": "https://aclanthology.org/2022.acl-long.587/",
        "pdf_size": 342836,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5456141216126579659&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Multimedia Communications Lab, Technical University of Darmstadt, Germany; Multimedia Communications Lab, Technical University of Darmstadt, Germany; Multimedia Communications Lab, Technical University of Darmstadt, Germany; Multimedia Communications Lab, Technical University of Darmstadt, Germany; Multimedia Communications Lab, Technical University of Darmstadt, Germany",
        "aff_domain": "kom.tu-darmstadt.de;gmail.com;gmail.com;kom.tu-darmstadt.de;kom.tu-darmstadt.de",
        "email": "kom.tu-darmstadt.de;gmail.com;gmail.com;kom.tu-darmstadt.de;kom.tu-darmstadt.de",
        "github": "https://github.com/SebOchs/SAF",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Technical University of Darmstadt",
        "aff_unique_dep": "Multimedia Communications Lab",
        "aff_unique_url": "https://www.tu-darmstadt.de",
        "aff_unique_abbr": "TUD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2022.findings-acl.176",
        "title": "Your fairness may vary: Pretrained language model fairness in toxic text classification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact. The evaluation of such systems usually focuses on accuracy measures. Our findings in this paper call for attention to be paid to fairness measures as well. Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics. Specifically, we observe that fairness can vary even more than accuracy with increasing training data size and different random initializations. At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature. To improve model fairness without retraining, we show that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models. Warning: This paper contains samples of offensive text.",
        "author": "Ioana Baldini; Dennis Wei; Karthikeyan Natesan Ramamurthy; Moninder Singh; Mikhail Yurochkin",
        "authorids": "/i/ioana-baldini/; /d/dennis-wei/; /k/karthikeyan-natesan-ramamurthy/; /m/moninder-singh/; /m/mikhail-yurochkin/",
        "bibtex": "@inproceedings{baldini-etal-2022-fairness,\n    title = \"Your fairness may vary: Pretrained language model fairness in toxic text classification\",\n    author = \"Baldini, Ioana  and\n      Wei, Dennis  and\n      Natesan Ramamurthy, Karthikeyan  and\n      Singh, Moninder  and\n      Yurochkin, Mikhail\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.176/\",\n    doi = \"10.18653/v1/2022.findings-acl.176\",\n    pages = \"2245--2262\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.176.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.176/",
        "pdf_size": 7590044,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5796653994665162733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "IBM Research; IBM Research; IBM Research; IBM Research; IBM Research",
        "aff_domain": "us.ibm.com;us.ibm.com;us.ibm.com;ibm.com;us.ibm.com",
        "email": "us.ibm.com;us.ibm.com;us.ibm.com;ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.285",
        "title": "Zero-Shot Cross-lingual Semantic Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages. However, these advances assume access to high-quality machine translation systems and word alignment tools. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem, without parallel data (i.e., utterance-logical form pairs) for new languages. We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-logical form paired data and in-domain natural language corpora in each new language. Our model encourages language-agnostic encodings by jointly optimizing for logical-form generation with auxiliary objectives designed for cross-lingual latent representation alignment. Our parser performs significantly above translation-based baselines and, in some cases, competes with the supervised upper-bound.",
        "author": "Tom Sherborne; Mirella Lapata",
        "authorids": "/t/tom-sherborne/; /m/mirella-lapata/",
        "bibtex": "@inproceedings{sherborne-lapata-2022-zero,\n    title = \"Zero-Shot Cross-lingual Semantic Parsing\",\n    author = \"Sherborne, Tom  and\n      Lapata, Mirella\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.285/\",\n    doi = \"10.18653/v1/2022.acl-long.285\",\n    pages = \"4134--4153\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.285.pdf",
        "site": "https://aclanthology.org/2022.acl-long.285/",
        "pdf_size": 475788,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13423105302794726558&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;inf.ed.ac.uk",
        "github": "github.com/tomsherborne/zx-parse",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.316",
        "title": "Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Dense retrieval (DR) methods conduct text retrieval by first encoding texts in the embedding space and then matching them by nearest neighbor search. This requires strong locality properties from the representation space, e.g., close allocations of each small group of relevant texts, which are hard to generalize to domains without sufficient training data. In this paper, we aim to improve the generalization ability of DR models from source training domains with rich supervision signals to target domains without any relevance label, in the zero-shot setting. To achieve that, we propose Momentum adversarial Domain Invariant Representation learning (MoDIR), which introduces a momentum method to train a domain classifier that distinguishes source versus target domains, and then adversarially updates the DR encoder to learn domain invariant representations. Our experiments show that MoDIR robustly outperforms its baselines on 10+ ranking datasets collected in the BEIR benchmark in the zero-shot setup, with more than 10% relative gains on datasets with enough sensitivity for DR models\u2019 evaluation. Source code is available at https://github.com/ji-xin/modir.",
        "author": "Ji Xin; Chenyan Xiong; Ashwin Srinivasan; Ankita Sharma; Damien Jose; Paul Bennett",
        "authorids": "/j/ji-xin/; /c/chenyan-xiong/; /a/ashwin-srinivasan/; /a/ankita-sharma/; /d/damien-jose/; /p/paul-bennett/",
        "bibtex": "@inproceedings{xin-etal-2022-zero,\n    title = \"Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations\",\n    author = \"Xin, Ji  and\n      Xiong, Chenyan  and\n      Srinivasan, Ashwin  and\n      Sharma, Ankita  and\n      Jose, Damien  and\n      Bennett, Paul\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.316/\",\n    doi = \"10.18653/v1/2022.findings-acl.316\",\n    pages = \"4008--4020\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.316.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.316/",
        "pdf_size": 817950,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3337665841069892456&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 5,
        "aff": "University of Waterloo; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft",
        "aff_domain": "uwaterloo.ca;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "uwaterloo.ca;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/ji-xin/modir",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "University of Waterloo;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://uwaterloo.ca;https://www.microsoft.com",
        "aff_unique_abbr": "UW;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2022.acl-short.64",
        "title": "Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages. However, source and training languages are rarely related, when parsing truly low-resource languages. To close this gap, we adopt a method from multi-task learning, which relies on automated curriculum learning, to dynamically optimize for parsing performance on outlier languages. We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting.",
        "author": "Miryam de Lhoneux; Sheng Zhang; Anders S\u00f8gaard",
        "authorids": "/m/miryam-de-lhoneux/; /s/sheng-zhang/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{de-lhoneux-etal-2022-zero,\n    title = \"Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning\",\n    author = \"de Lhoneux, Miryam  and\n      Zhang, Sheng  and\n      S{\\o}gaard, Anders\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.64/\",\n    doi = \"10.18653/v1/2022.acl-short.64\",\n    pages = \"578--587\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.64.pdf",
        "site": "https://aclanthology.org/2022.acl-short.64/",
        "pdf_size": 336642,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18158548773616064235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Copenhagen, Denmark+Uppsala University, Sweden+KU Leuven, Belgium; National University of Defense Technology, China; University of Copenhagen, Denmark",
        "aff_domain": "di.ku.dk;nudt.edu.cn;di.ku.dk",
        "email": "di.ku.dk;nudt.edu.cn;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;3;0",
        "aff_unique_norm": "University of Copenhagen;Uppsala University;KU Leuven;National University of Defense Technology",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ku.dk;https://www.uu.se;https://www.kuleuven.be;http://www.nudt.edu.cn",
        "aff_unique_abbr": "UCPH;UU;KU Leuven;NUDT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1+2;3;0",
        "aff_country_unique": "Denmark;Sweden;Belgium;China"
    },
    {
        "id": "2022.findings-acl.166",
        "title": "Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields. Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-k nearest languages for which we have training sets. Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language. We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines.",
        "author": "Xinjian Li; Florian Metze; David Mortensen; Shinji Watanabe; Alan Black",
        "authorids": "/x/xinjian-li/; /f/florian-metze/; /d/david-r-mortensen/; /s/shinji-watanabe/; /a/alan-w-black/",
        "bibtex": "@inproceedings{li-etal-2022-zero,\n    title = \"Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble\",\n    author = \"Li, Xinjian  and\n      Metze, Florian  and\n      Mortensen, David  and\n      Watanabe, Shinji  and\n      Black, Alan\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.166/\",\n    doi = \"10.18653/v1/2022.findings-acl.166\",\n    pages = \"2106--2115\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.166.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.166/",
        "pdf_size": 273382,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3165593925458146537&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/xinjli/transphone",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.findings-acl.242",
        "title": "ZiNet: Linking Chinese Characters Spanning Three Thousand Years",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Modern Chinese characters evolved from 3,000 years ago. Up to now, tens of thousands of glyphs of ancient characters have been discovered, which must be deciphered by experts to interpret unearthed documents. Experts usually need to compare each ancient character to be examined with similar known ones in whole historical periods. However, it is inevitably limited by human memory and experience, which often cost a lot of time but associations are limited to a small scope. To help researchers discover glyph similar characters, this paper introduces ZiNet, the first diachronic knowledge base describing relationships and evolution of Chinese characters and words. In addition, powered by the knowledge of radical systems in ZiNet, this paper introduces glyph similarity measurement between ancient Chinese characters, which could capture similar glyph pairs that are potentially related in origins or semantics. Results show strong positive correlations between scores from the method and from human experts. Finally, qualitative analysis and implicit future applications are presented.",
        "author": "Yang Chi; Fausto Giunchiglia; Daqian Shi; Xiaolei Diao; Chuntao Li; Hao Xu",
        "authorids": "/y/yang-chi/; /f/fausto-giunchiglia/; /d/daqian-shi/; /x/xiaolei-diao/; /c/chuntao-li/; /h/hao-xu/",
        "bibtex": "@inproceedings{chi-etal-2022-zinet,\n    title = \"{Z}i{N}et: {L}inking {C}hinese Characters Spanning Three Thousand Years\",\n    author = \"Chi, Yang  and\n      Giunchiglia, Fausto  and\n      Shi, Daqian  and\n      Diao, Xiaolei  and\n      Li, Chuntao  and\n      Xu, Hao\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.242/\",\n    doi = \"10.18653/v1/2022.findings-acl.242\",\n    pages = \"3061--3070\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.242.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.242/",
        "pdf_size": 1236274,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4585535002108189647&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Artificial Intelligence, Jilin University, Changchun, China; School of Artificial Intelligence, Jilin University, Changchun, China+College of Computer Science and Technology, Jilin University, Changchun, China; DISI, University of Trento, Trento, Italy; DISI, University of Trento, Trento, Italy; School of Archaeology, Jilin University, Changchun, China; School of Artificial Intelligence, Jilin University, Changchun, China+College of Computer Science and Technology, Jilin University, Changchun, China",
        "aff_domain": "mails.jlu.edu.cn;unitn.it;unitn.it;unitn.it;jlu.edu.cn;jlu.edu.cn",
        "email": "mails.jlu.edu.cn;unitn.it;unitn.it;unitn.it;jlu.edu.cn;jlu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+0;1;1;0;0+0",
        "aff_unique_norm": "Jilin University;University of Trento",
        "aff_unique_dep": "School of Artificial Intelligence;DISI",
        "aff_unique_url": "http://www.jlu.edu.cn;https://www.unitn.it",
        "aff_unique_abbr": "JLU;",
        "aff_campus_unique_index": "0;0+0;1;1;0;0+0",
        "aff_campus_unique": "Changchun;Trento",
        "aff_country_unique_index": "0;0+0;1;1;0;0+0",
        "aff_country_unique": "China;Italy"
    },
    {
        "id": "2022.acl-long.311",
        "title": "Zoom Out and Observe: News Environment Perception for Fake News Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fake news detection is crucial for preventing the dissemination of misinformation on social media. To differentiate fake news from real ones, existing methods observe the language patterns of the news post and \u201czoom in\u201d to verify its content with knowledge sources or check its readers\u2019 replies. However, these methods neglect the information in the external news environment where a fake news post is created and disseminated. The news environment represents recent mainstream media opinion and public attention, which is an important inspiration of fake news fabrication because fake news is often designed to ride the wave of popular events and catch public attention with unexpected novel content for greater exposure and spread. To capture the environmental signals of news posts, we \u201czoom out\u201d to observe the news environment and propose the News Environment Perception Framework (NEP). For each post, we construct its macro and micro news environment from recent mainstream news. Then we design a popularity-oriented and a novelty-oriented module to perceive useful signals and further assist final prediction. Experiments on our newly built datasets show that the NEP can efficiently improve the performance of basic fake news detectors.",
        "author": "Qiang Sheng; Juan Cao; Xueyao Zhang; Rundong Li; Danding Wang; Yongchun Zhu",
        "authorids": "/q/qiang-sheng/; /j/juan-cao/; /x/xueyao-zhang/; /r/rundong-li/; /d/danding-wang/; /y/yongchun-zhu/",
        "bibtex": "@inproceedings{sheng-etal-2022-zoom,\n    title = \"Zoom Out and Observe: News Environment Perception for Fake News Detection\",\n    author = \"Sheng, Qiang  and\n      Cao, Juan  and\n      Zhang, Xueyao  and\n      Li, Rundong  and\n      Wang, Danding  and\n      Zhu, Yongchun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.311/\",\n    doi = \"10.18653/v1/2022.acl-long.311\",\n    pages = \"4543--4556\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.311.pdf",
        "site": "https://aclanthology.org/2022.acl-long.311/",
        "pdf_size": 1062924,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12573700479533328533&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/ICTMCG/News-Environment-Perception/",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Key Lab of Intelligent Information Processing",
        "aff_unique_url": "http://www.cas.cn/",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.8",
        "title": "[CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The recent success of reinforcement learning (RL) in solving complex tasks is often attributed to its capacity to explore and exploit an environment. Sample efficiency is usually not an issue for tasks with cheap simulators to sample data online. On the other hand, Task-oriented Dialogues (ToD) are usually learnt from offline data collected using human demonstrations. Collecting diverse demonstrations and annotating them is expensive. Unfortunately, RL policy trained on off-policy data are prone to issues of bias and generalization, which are further exacerbated by stochasticity in human response and non-markovian nature of annotated belief state of a dialogue management system. To this end, we propose a batch-RL framework for ToD policy learning: Causal-aware Safe Policy Improvement (CASPI). CASPI includes a mechanism to learn fine-grained reward that captures intention behind human response and also offers guarantee on dialogue policy\u2019s performance against a baseline. We demonstrate the effectiveness of this framework on end-to-end dialogue task of the Multiwoz2.0 dataset. The proposed method outperforms the current state of the art. Further more we demonstrate sample efficiency, where our method trained only on 20% of the data, are comparable to current state of the art method trained on 100% data on two out of there evaluation metrics.",
        "author": "Govardana Sachithanandam Ramachandran; Kazuma Hashimoto; Caiming Xiong",
        "authorids": "/g/govardana-sachithanandam-ramachandran/; /k/kazuma-hashimoto/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{ramachandran-etal-2022-caspi,\n    title = \"[{CASPI}] Causal-aware Safe Policy Improvement for Task-oriented Dialogue\",\n    author = \"Ramachandran, Govardana Sachithanandam  and\n      Hashimoto, Kazuma  and\n      Xiong, Caiming\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.8/\",\n    doi = \"10.18653/v1/2022.acl-long.8\",\n    pages = \"92--102\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.8.pdf",
        "site": "https://aclanthology.org/2022.acl-long.8/",
        "pdf_size": 1197181,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1523000375956152257&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Salesforce Research; Salesforce Research*; Salesforce Research",
        "aff_domain": "salesforce.com;logos.t.u-tokyo.ac.jp;salesforce.com",
        "email": "salesforce.com;logos.t.u-tokyo.ac.jp;salesforce.com",
        "github": "https://github.com/salesforce/CASPI",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Salesforce",
        "aff_unique_dep": "Salesforce Research",
        "aff_unique_url": "https://research.salesforce.com",
        "aff_unique_abbr": "Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.151",
        "title": "bert2BERT: Towards Reusable Pretrained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model\u2019s initialization. In addition, a two-stage learning method is proposed to further accelerate the pre-training. We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT BASE and GPT BASE by reusing the models of almost their half sizes.",
        "author": "Cheng Chen; Yichun Yin; Lifeng Shang; Xin Jiang; Yujia Qin; Fengyu Wang; Zhi Wang; Xiao Chen; Zhiyuan Liu; Qun Liu",
        "authorids": "/c/cheng-chen/; /y/yichun-yin/; /l/lifeng-shang/; /x/xin-jiang/; /y/yujia-qin/; /f/fengyu-wang/; /z/zhi-wang/; /x/xiao-chen/; /z/zhiyuan-liu/; /q/qun-liu/",
        "bibtex": "@inproceedings{chen-etal-2022-bert2bert,\n    title = \"bert2{BERT}: Towards Reusable Pretrained Language Models\",\n    author = \"Chen, Cheng  and\n      Yin, Yichun  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Qin, Yujia  and\n      Wang, Fengyu  and\n      Wang, Zhi  and\n      Chen, Xiao  and\n      Liu, Zhiyuan  and\n      Liu, Qun\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.151/\",\n    doi = \"10.18653/v1/2022.acl-long.151\",\n    pages = \"2134--2148\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.151.pdf",
        "site": "https://aclanthology.org/2022.acl-long.151/",
        "pdf_size": 1578250,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10666616535613089769&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Technology, Tsinghua University; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Department of Computer Science and Technology, Tsinghua University; Department of Computer Science and Technology, Tsinghua University; Tsinghua Shenzhen International Graduate School+Peng Cheng Laboratory; Huawei Noah\u2019s Ark Lab; Department of Computer Science and Technology, Tsinghua University; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "mails.tsinghua.edu.cn;huawei.com;huawei.com;huawei.com;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;sz.tsinghua.edu.cn;huawei.com;tsinghua.edu.cn;huawei.com",
        "email": "mails.tsinghua.edu.cn;huawei.com;huawei.com;huawei.com;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;sz.tsinghua.edu.cn;huawei.com;tsinghua.edu.cn;huawei.com",
        "github": "https://github.com/huawei-noah/Pretrained-Language-Model",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;1;1;0;0;0+2;1;0;1",
        "aff_unique_norm": "Tsinghua University;Huawei;Pengcheng Laboratory",
        "aff_unique_dep": "Department of Computer Science and Technology;Noah\u2019s Ark Lab;Peng Cheng Laboratory",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.huawei.com;http://www.pcl.ac.cn",
        "aff_unique_abbr": "THU;Huawei;PCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.33",
        "title": "e-CARE: a New Dataset for Exploring Explainable Causal Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding causality has vital importance for various Natural Language Processing (NLP) applications. Beyond the labeled instances, conceptual explanations of the causality can provide deep understanding of the causal fact to facilitate the causal reasoning process. However, such explanation information still remains absent in existing causal reasoning resources. In this paper, we fill this gap by presenting a human-annotated explainable CAusal REasoning dataset (e-CARE), which contains over 20K causal reasoning questions, together with natural language formed explanations of the causal questions. Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models, and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models.",
        "author": "Li Du; Xiao Ding; Kai Xiong; Ting Liu; Bing Qin",
        "authorids": "/l/li-du/; /x/xiao-ding/; /k/kai-xiong/; /t/ting-liu/; /b/bing-qin/",
        "bibtex": "@inproceedings{du-etal-2022-e,\n    title = \"e-{CARE}: a New Dataset for Exploring Explainable Causal Reasoning\",\n    author = \"Du, Li  and\n      Ding, Xiao  and\n      Xiong, Kai  and\n      Liu, Ting  and\n      Qin, Bing\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.33/\",\n    doi = \"10.18653/v1/2022.acl-long.33\",\n    pages = \"432--446\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.33.pdf",
        "site": "https://aclanthology.org/2022.acl-long.33/",
        "pdf_size": 600148,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14926345247611450632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China",
        "aff_domain": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "email": "ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn;ir.hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "Research Center for Social Computing and Information Retrieval",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Harbin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.276",
        "title": "ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored. Here, we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding. The dataset provides fine-grained annotation of aligned spans between proverbs and narratives, and contains minimal lexical overlaps between narratives and proverbs, ensuring that models need to go beyond surface-level reasoning to succeed. We explore three tasks: (1) proverb recommendation and alignment prediction, (2) narrative generation for a given proverb and topic, and (3) identifying narratives with similar motifs. Our experiments show that neural language models struggle on these tasks compared to humans, and these tasks pose multiple learning challenges.",
        "author": "Sayan Ghosh; Shashank Srivastava",
        "authorids": "/s/sayan-ghosh/; /s/shashank-srivastava/",
        "bibtex": "@inproceedings{ghosh-srivastava-2022-epic,\n    title = \"e{P}i{C}: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding\",\n    author = \"Ghosh, Sayan  and\n      Srivastava, Shashank\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.276/\",\n    doi = \"10.18653/v1/2022.acl-long.276\",\n    pages = \"3989--4004\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.276.pdf",
        "site": "https://aclanthology.org/2022.acl-long.276/",
        "pdf_size": 451839,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17687983881526652872&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "UNC Chapel Hill; UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-short.42",
        "title": "k-Rater Reliability: The Correct Unit of Reliability for Aggregated Human Annotations",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Since the inception of crowdsourcing, aggregation has been a common strategy for dealing with unreliable data. Aggregate ratings are more reliable than individual ones. However, many Natural Language Processing (NLP) applications that rely on aggregate ratings only report the reliability of individual ratings, which is the incorrect unit of analysis. In these instances, the data reliability is under-reported, and a proposed k-rater reliability (kRR) should be used as the correct data reliability for aggregated datasets. It is a multi-rater generalization of inter-rater reliability (IRR). We conducted two replications of the WordSim-353 benchmark, and present empirical, analytical, and bootstrap-based methods for computing kRR on WordSim-353. These methods produce very similar results. We hope this discussion will nudge researchers to report kRR in addition to IRR.",
        "author": "Ka Wong; Praveen Paritosh",
        "authorids": "/k/ka-wong/; /p/praveen-paritosh/",
        "bibtex": "@inproceedings{wong-paritosh-2022-k,\n    title = \"k-{R}ater {R}eliability: {T}he Correct Unit of Reliability for Aggregated Human Annotations\",\n    author = \"Wong, Ka  and\n      Paritosh, Praveen\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-short.42/\",\n    doi = \"10.18653/v1/2022.acl-short.42\",\n    pages = \"378--384\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-short.42.pdf",
        "site": "https://aclanthology.org/2022.acl-short.42/",
        "pdf_size": 266903,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=719287990084226245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Google Research; Google Research",
        "aff_domain": "gmail.com;google.com",
        "email": "gmail.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2022.acl-long.575",
        "title": "latent-GLAT: Glancing at Latent Variables for Parallel Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, parallel text generation has received widespread attention due to its success in generation efficiency. Although many advanced techniques are proposed to improve its generation quality, they still need the help of an autoregressive model for training to overcome the one-to-many multi-modal phenomenon in the dataset, limiting their applications. In this paper, we propose GLAT, which employs the discrete latent variables to capture word categorical information and invoke an advanced curriculum learning technique, alleviating the multi-modality problem. Experiment results show that our method outperforms strong baselines without the help of an autoregressive model, which further broadens the application scenarios of the parallel decoding paradigm.",
        "author": "Yu Bao; Hao Zhou; Shujian Huang; Dongqi Wang; Lihua Qian; Xinyu Dai; Jiajun Chen; Lei Li",
        "authorids": "/y/yu-bao/; /h/hao-zhou/; /s/shujian-huang/; /d/dongqi-wang/; /l/lihua-qian/; /x/xinyu-dai/; /j/jiajun-chen/; /l/lei-li/",
        "bibtex": "@inproceedings{bao-etal-2022-textit,\n    title = \"{latent-GLAT}: Glancing at Latent Variables for Parallel Text Generation\",\n    author = \"Bao, Yu  and\n      Zhou, Hao  and\n      Huang, Shujian  and\n      Wang, Dongqi  and\n      Qian, Lihua  and\n      Dai, Xinyu  and\n      Chen, Jiajun  and\n      Li, Lei\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.575/\",\n    doi = \"10.18653/v1/2022.acl-long.575\",\n    pages = \"8398--8409\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.575.pdf",
        "site": "https://aclanthology.org/2022.acl-long.575/",
        "pdf_size": 1437362,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18117776207270490066&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, China+Collaborative Innovation Center of Novel Software Technology and Industrialization, China; Bytedance AI Lab, China; National Key Laboratory for Novel Software Technology, Nanjing University, China+Collaborative Innovation Center of Novel Software Technology and Industrialization, China; National Key Laboratory for Novel Software Technology, Nanjing University, China+Collaborative Innovation Center of Novel Software Technology and Industrialization, China; Bytedance AI Lab, China; National Key Laboratory for Novel Software Technology, Nanjing University, China+Collaborative Innovation Center of Novel Software Technology and Industrialization, China; National Key Laboratory for Novel Software Technology, Nanjing University, China+Collaborative Innovation Center of Novel Software Technology and Industrialization, China; University of California Santa Barbara, USA",
        "aff_domain": "smail.nju.edu.cn;bytedance.com;nju.edu.cn;smail.nju.edu.cn;bytedance.com;nju.edu.cn;nju.edu.cn;cs.ucsb.edu",
        "email": "smail.nju.edu.cn;bytedance.com;nju.edu.cn;smail.nju.edu.cn;bytedance.com;nju.edu.cn;nju.edu.cn;cs.ucsb.edu",
        "github": "https://github.com/baoy-nlp/Latent-GLAT",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;0+1;0+1;2;0+1;0+1;3",
        "aff_unique_norm": "Nanjing University;Collaborative Innovation Center of Novel Software Technology and Industrialization;ByteDance;University of California, Santa Barbara",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;;AI Lab;",
        "aff_unique_url": "http://www.nju.edu.cn;;https://www.bytedance.com;https://www.ucsb.edu",
        "aff_unique_abbr": "Nanjing U;;Bytedance AI Lab;UCSB",
        "aff_campus_unique_index": ";;;;;1",
        "aff_campus_unique": ";Santa Barbara",
        "aff_country_unique_index": "0+0;0;0+0;0+0;0;0+0;0+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2022.acl-long.505",
        "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages with entity representations and showthe model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks. We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations.",
        "author": "Ryokan Ri; Ikuya Yamada; Yoshimasa Tsuruoka",
        "authorids": "/r/ryokan-ri/; /i/ikuya-yamada/; /y/yoshimasa-tsuruoka/",
        "bibtex": "@inproceedings{ri-etal-2022-mluke,\n    title = \"m{LUKE}: {T}he Power of Entity Representations in Multilingual Pretrained Language Models\",\n    author = \"Ri, Ryokan  and\n      Yamada, Ikuya  and\n      Tsuruoka, Yoshimasa\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.505/\",\n    doi = \"10.18653/v1/2022.acl-long.505\",\n    pages = \"7316--7330\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.505.pdf",
        "site": "https://aclanthology.org/2022.acl-long.505/",
        "pdf_size": 380734,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11057127712357112349&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 6,
        "aff": "Studio Ousia, Tokyo, Japan+The University of Tokyo, Tokyo, Japan; Studio Ousia, Tokyo, Japan+RIKEN AIP, Tokyo, Japan; The University of Tokyo, Tokyo, Japan",
        "aff_domain": "ousia.jp;ousia.jp;logos.t.u-tokyo.ac.jp",
        "email": "ousia.jp;ousia.jp;logos.t.u-tokyo.ac.jp",
        "github": "https://github.com/studio-ousia/luke",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+2;1",
        "aff_unique_norm": "Studio Ousia;University of Tokyo;RIKEN AIP",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.u-tokyo.ac.jp;https://aip.Riken.jp",
        "aff_unique_abbr": ";UTokyo;RIKEN AIP",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2022.findings-acl.223",
        "title": "uFACT: Unfaithful Alien-Corpora Training for Semantically Consistent Data-to-Text Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose uFACT (Un-Faithful Alien Corpora Training), a training corpus construction method for data-to-text (d2t) generation models. We show that d2t models trained on uFACT datasets generate utterances which represent the semantic content of the data sources more accurately compared to models trained on the target corpus alone. Our approach is to augment the training set of a given target corpus with alien corpora which have different semantic representations. We show that while it is important to have faithful data from the target corpus, the faithfulness of additional corpora only plays a minor role. Consequently, uFACT datasets can be constructed with large quantities of unfaithful data. We show how uFACT can be leveraged to obtain state-of-the-art results on the WebNLG benchmark using METEOR as our performance metric. Furthermore, we investigate the sensitivity of the generation faithfulness to the training corpus structure using the PARENT metric, and provide a baseline for this metric on the WebNLG (Gardent et al., 2017) benchmark to facilitate comparisons with future work.",
        "author": "Tisha Anders; Alexandru Coca; Bill Byrne",
        "authorids": "/t/tisha-anders/; /a/alexandru-coca/; /b/bill-byrne/",
        "bibtex": "@inproceedings{anders-etal-2022-ufact,\n    title = \"u{FACT}: Unfaithful Alien-Corpora Training for Semantically Consistent Data-to-Text Generation\",\n    author = \"Anders, Tisha  and\n      Coca, Alexandru  and\n      Byrne, Bill\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.223/\",\n    doi = \"10.18653/v1/2022.findings-acl.223\",\n    pages = \"2836--2841\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.223.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.223/",
        "pdf_size": 436861,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6190250970893570520&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Engineering, University of Cambridge, United Kingdom; Department of Engineering, University of Cambridge, United Kingdom; Department of Engineering, University of Cambridge, United Kingdom",
        "aff_domain": "gmail.com;cam.ac.uk;cam.ac.uk",
        "email": "gmail.com;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2022.findings-acl.196",
        "title": "xGQA: Cross-Lingual Visual Question Answering",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and\u2014vice versa\u2014multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.",
        "author": "Jonas Pfeiffer; Gregor Geigle; Aishwarya Kamath; Jan-Martin O. Steitz; Stefan Roth; Ivan Vuli\u0107; Iryna Gurevych",
        "authorids": "/j/jonas-pfeiffer/; /g/gregor-geigle/; /a/aishwarya-kamath/; /j/jan-martin-o-steitz/; /s/stefan-roth/; /i/ivan-vulic/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{pfeiffer-etal-2022-xgqa,\n    title = \"x{GQA}: Cross-Lingual Visual Question Answering\",\n    author = \"Pfeiffer, Jonas  and\n      Geigle, Gregor  and\n      Kamath, Aishwarya  and\n      Steitz, Jan-Martin O.  and\n      Roth, Stefan  and\n      Vuli{\\'c}, Ivan  and\n      Gurevych, Iryna\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.196/\",\n    doi = \"10.18653/v1/2022.findings-acl.196\",\n    pages = \"2497--2511\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.196.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.196/",
        "pdf_size": 2294597,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5897644011303594318&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt; Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt; Center for Data Science, New York University; Visual Inference Lab, Technical University of Darmstadt; Visual Inference Lab, Technical University of Darmstadt; Language Technology Lab, University of Cambridge; Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/Adapter-Hub/xGQA",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;2;0",
        "aff_unique_norm": "Technical University of Darmstadt;New York University;University of Cambridge",
        "aff_unique_dep": "Ubiquitous Knowledge Processing Lab;Center for Data Science;Language Technology Lab",
        "aff_unique_url": "https://www.tu-darmstadt.de;https://www.nyu.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": "TUD;NYU;Cambridge",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";New York;Cambridge",
        "aff_country_unique_index": "0;0;1;0;0;2;0",
        "aff_country_unique": "Germany;United States;United Kingdom"
    },
    {
        "id": "2022.findings-acl.1",
        "title": "\u201cIs Whole Word Masking Always Better for Chinese BERT?\u201d: Probing on Chinese Grammatical Error Correction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Whole word masking (WWM), which masks all subwords corresponding to a word at once, makes a better English BERT model. For the Chinese language, however, there is no subword because each token is an atomic character. The meaning of a word in Chinese is different in that a word is a compositional unit consisting of multiple characters. Such difference motivates us to investigate whether WWM leads to better context understanding ability for Chinese BERT. To achieve this, we introduce two probing tasks related to grammatical error correction and ask pretrained models to revise or insert tokens in a masked language modeling manner. We construct a dataset including labels for 19,075 tokens in 10,448 sentences. We train three Chinese BERT models with standard character-level masking (CLM), WWM, and a combination of CLM and WWM, respectively. Our major findings are as follows: First, when one character needs to be inserted or replaced, the model trained with CLM performs the best. Second, when more than one character needs to be handled, WWM is the key to better performance. Finally, when being fine-tuned on sentence-level downstream tasks, models trained with different masking strategies perform comparably.",
        "author": "Yong Dai; Linyang Li; Cong Zhou; Zhangyin Feng; Enbo Zhao; Xipeng Qiu; Piji Li; Duyu Tang",
        "authorids": "/y/yong-dai/; /l/linyang-li/; /c/cong-zhou/; /z/zhangyin-feng/; /e/enbo-zhao/; /x/xipeng-qiu/; /p/piji-li/; /d/duyu-tang/",
        "bibtex": "@inproceedings{dai-etal-2022-whole,\n    title = \"{\\textquotedblleft}Is Whole Word Masking Always Better for {C}hinese {BERT}?{\\textquotedblright}: Probing on {C}hinese Grammatical Error Correction\",\n    author = \"Dai, Yong  and\n      Li, Linyang  and\n      Zhou, Cong  and\n      Feng, Zhangyin  and\n      Zhao, Enbo  and\n      Qiu, Xipeng  and\n      Li, Piji  and\n      Tang, Duyu\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2022\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-acl.1/\",\n    doi = \"10.18653/v1/2022.findings-acl.1\",\n    pages = \"1--8\"\n}",
        "pdf": "https://aclanthology.org/2022.findings-acl.1.pdf",
        "site": "https://aclanthology.org/2022.findings-acl.1/",
        "pdf_size": 411082,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14522446932869774841&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tencent AI Lab; Fudan University + Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Fudan University; Tencent AI Lab; Tencent AI Lab",
        "aff_domain": "tencent.com;fudan.edu.cn;tencent.com;tencent.com;tencent.com;fudan.edu.cn;tencent.com;tencent.com",
        "email": "tencent.com;fudan.edu.cn;tencent.com;tencent.com;tencent.com;fudan.edu.cn;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1+0;0;0;0;1;0;0",
        "aff_unique_norm": "Tencent;Fudan University",
        "aff_unique_dep": "Tencent AI Lab;",
        "aff_unique_url": "https://ai.tencent.com;https://www.fudan.edu.cn",
        "aff_unique_abbr": "Tencent AI Lab;Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2022.acl-long.538",
        "title": "\u201cThat Is a Suspicious Reaction!\u201d: Interpreting Logits Variation to Detect NLP Adversarial Attacks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Adversarial attacks are a major challenge faced by current machine learning research. These purposely crafted inputs fool even the most advanced models, precluding their deployment in safety-critical applications. Extensive research in computer vision has been carried to develop reliable defense strategies. However, the same issue remains less explored in natural language processing. Our work presents a model-agnostic detector of adversarial text examples. The approach identifies patterns in the logits of the target classifier when perturbing the input text. The proposed detector improves the current state-of-the-art performance in recognizing adversarial inputs and exhibits strong generalization capabilities across different NLP models, datasets, and word-level attacks.",
        "author": "Edoardo Mosca; Shreyash Agarwal; Javier Rando Ram\u00edrez; Georg Groh",
        "authorids": "/e/edoardo-mosca/; /s/shreyash-agarwal/; /j/javier-rando-ramirez/; /g/georg-groh/",
        "bibtex": "@inproceedings{mosca-etal-2022-suspicious,\n    title = \"{\\textquotedblleft}That Is a Suspicious Reaction!{\\textquotedblright}: Interpreting Logits Variation to Detect {NLP} Adversarial Attacks\",\n    author = \"Mosca, Edoardo  and\n      Agarwal, Shreyash  and\n      Rando Ram{\\'i}rez, Javier  and\n      Groh, Georg\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.538/\",\n    doi = \"10.18653/v1/2022.acl-long.538\",\n    pages = \"7806--7816\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.538.pdf",
        "site": "https://aclanthology.org/2022.acl-long.538/",
        "pdf_size": 451572,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6130961932012633133&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "TU Munich, Department of Informatics, Germany; TU Munich, Department of Informatics, Germany; ETH Zurich, Department of Computer Science, Switzerland; TU Munich, Department of Informatics, Germany",
        "aff_domain": "tum.de;tum.de;student.ethz.ch;in.tum.de",
        "email": "tum.de;tum.de;student.ethz.ch;in.tum.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Technical University of Munich;ETH Zurich",
        "aff_unique_dep": "Department of Informatics;Department of Computer Science",
        "aff_unique_url": "https://www.tum.de;https://www.ethz.ch",
        "aff_unique_abbr": "TUM;ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "id": "2022.acl-long.153",
        "title": "\u201cYou might think about slightly revising the title\u201d: Identifying Hedges in Peer-tutoring Interactions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Hedges have an important role in the management of rapport. In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback. Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.",
        "author": "Yann Raphalen; Chlo\u00e9 Clavel; Justine Cassell",
        "authorids": "/y/yann-raphalen/; /c/chloe-clavel/; /j/justine-cassell/",
        "bibtex": "@inproceedings{raphalen-etal-2022-might,\n    title = \"{\\textquotedblleft}{Y}ou might think about slightly revising the title{\\textquotedblright}: Identifying Hedges in Peer-tutoring Interactions\",\n    author = \"Raphalen, Yann  and\n      Clavel, Chlo{\\'e}  and\n      Cassell, Justine\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.153/\",\n    doi = \"10.18653/v1/2022.acl-long.153\",\n    pages = \"2160--2174\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.153.pdf",
        "site": "https://aclanthology.org/2022.acl-long.153/",
        "pdf_size": 623130,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6161527750143530548&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Inria Paris; LTCI, Institut Polytechnique de Paris, Telecom-Paris; Carnegie Mellon University",
        "aff_domain": "gmail.com;telecom-paris.fr;cs.cmu.edu",
        "email": "gmail.com;telecom-paris.fr;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "INRIA;Institut Polytechnique de Paris;Carnegie Mellon University",
        "aff_unique_dep": ";LTCI;",
        "aff_unique_url": "https://www.inria.fr;https://www.ipparis.fr;https://www.cmu.edu",
        "aff_unique_abbr": "Inria;IP Paris;CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "2022.acl-long.375",
        "title": "\u221e-former: Infinite Memory Transformer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \u221e-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \u221e-former\u2019s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, \u221e-former maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \u221e-former\u2019s ability to retain information from long sequences.",
        "author": "Pedro Henrique Martins; Zita Marinho; Andre Martins",
        "authorids": "/p/pedro-henrique-martins/; /z/zita-marinho/; /a/andre-f-t-martins/",
        "bibtex": "@inproceedings{martins-etal-2022-former,\n    title = \"$\\infty$-former: Infinite Memory Transformer\",\n    author = \"Martins, Pedro Henrique  and\n      Marinho, Zita  and\n      Martins, Andre\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.375/\",\n    doi = \"10.18653/v1/2022.acl-long.375\",\n    pages = \"5468--5485\"\n}",
        "pdf": "https://aclanthology.org/2022.acl-long.375.pdf",
        "site": "https://aclanthology.org/2022.acl-long.375/",
        "pdf_size": 1767719,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9437855146277678834&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Instituto de Telecomunica\u00e7\u00f5es; DeepMind+Institute of Systems and Robotics; LUMLIS (Lisbon ELLIS Unit), Instituto Superior T\u00e9cnico+Unbabel",
        "aff_domain": "tecnico.ulisboa.pt;google.com;tecnico.ulisboa.pt",
        "email": "tecnico.ulisboa.pt;google.com;tecnico.ulisboa.pt",
        "github": "https://github.com/deep-spin/infinite-former",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;3+4",
        "aff_unique_norm": "Instituto de Telecomunica\u00e7\u00f5es;DeepMind;Institute of Systems and Robotics;Instituto Superior T\u00e9cnico;Unbabel",
        "aff_unique_dep": ";;;LUMLIS (Lisbon ELLIS Unit);",
        "aff_unique_url": "https://www.it.pt;https://deepmind.com;;https://www.ist.utl.pt;https://www.unbabel.com",
        "aff_unique_abbr": ";DeepMind;;IST;",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Lisbon",
        "aff_country_unique_index": "0;1;0+0",
        "aff_country_unique": "Portugal;United Kingdom;"
    }
]