[
    {
        "id": "af656b24c0",
        "title": "$\\ell_1$ Regression using Lewis Weights Preconditioning and Stochastic Gradient Descent",
        "site": "https://proceedings.mlr.press/v75/durfee18a.html",
        "author": "David Durfee; Kevin A. Lai; Saurabh Sawlani",
        "abstract": "We present preconditioned stochastic gradient descent (SGD) algorithms for the $\\ell_1$ minimization problem $\\min_{\\boldsymbol{\\mathit{x}}}\\|\\boldsymbol{\\mathit{A}} \\boldsymbol{\\mathit{x}} - \\boldsymbol{\\mathit{b}}\\|_1$ in the overdetermined case, where there are far more constraints than variables. Specifically, we have $\\boldsymbol{\\mathit{A}} \\in \\mathbb{R}^{n \\times d}$ for $n \\gg d$. Commonly known as the Least Absolute Deviations problem, $\\ell_1$ regression can be used to solve many important combinatorial problems, such as minimum cut and shortest path. SGD-based algorithms are appealing for their simplicity and practical efficiency. Our primary insight is that careful preprocessing can yield preconditioned matrices $\\tilde{\\boldsymbol{\\mathit{A}}}$ with strong properties (besides good condition number and low-dimension) that allow for faster convergence of gradient descent. In particular, we precondition using Lewis weights to obtain an isotropic matrix with fewer rows and strong upper bounds on all row norms. We leverage these conditions to find a good initialization, which we use along with recent smoothing reductions and accelerated stochastic gradient descent algorithms to achieve $\\epsilon$ relative error in $\\widetilde{O}(nnz(\\boldsymbol{\\mathit{A}}) + d^{2.5} \\epsilon^{-2})$ time with high probability, where $nnz(\\boldsymbol{\\mathit{A}})$ is the number of non-zeros in $\\boldsymbol{\\mathit{A}}$. This improves over the previous best result using gradient descent for $\\ell_1$ regression. We also match the best known running times for interior point methods in several settings. Finally, we also show that if our original matrix $\\boldsymbol{\\mathit{A}}$ is approximately isotropic and the row norms are approximately equal, we can give an algorithm that avoids using fast matrix multiplication and obtains a running time of $\\widetilde{O}(nnz(\\boldsymbol{\\mathit{A}}) + s d^{1.5}\\epsilon^{-2} + d^2\\epsilon^{-2})$, where $s$ is the maximum number of non-zeros in a row of $\\boldsymbol{\\mathit{A}}$. In this setting, we beat the best interior point methods for certain parameter regimes.",
        "bibtex": "@InProceedings{pmlr-v75-durfee18a,\n  title = \t {$\\ell_1$ Regression using Lewis Weights Preconditioning and Stochastic Gradient Descent},\n  author =       {Durfee, David and Lai, Kevin A. and Sawlani, Saurabh},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1626--1656},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/durfee18a/durfee18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/durfee18a.html},\n  abstract = \t {We present preconditioned stochastic gradient descent (SGD) algorithms for the $\\ell_1$ minimization problem $\\min_{\\boldsymbol{\\mathit{x}}}\\|\\boldsymbol{\\mathit{A}} \\boldsymbol{\\mathit{x}} - \\boldsymbol{\\mathit{b}}\\|_1$ in the overdetermined case, where there are far more constraints than variables. Specifically, we have $\\boldsymbol{\\mathit{A}} \\in \\mathbb{R}^{n \\times d}$ for $n \\gg d$. Commonly known as the Least Absolute Deviations problem, $\\ell_1$ regression can be used to solve many important combinatorial problems, such as minimum cut and shortest path. SGD-based algorithms are appealing for their simplicity and practical efficiency. Our primary insight is that careful preprocessing can yield preconditioned matrices $\\tilde{\\boldsymbol{\\mathit{A}}}$ with strong properties (besides good condition number and low-dimension) that allow for faster convergence of gradient descent. In particular, we precondition using Lewis weights to obtain an isotropic matrix with fewer rows and strong upper bounds on all row norms. We leverage these conditions to find a good initialization, which we use along with recent smoothing reductions and accelerated stochastic gradient descent algorithms to achieve $\\epsilon$ relative error in $\\widetilde{O}(nnz(\\boldsymbol{\\mathit{A}}) + d^{2.5} \\epsilon^{-2})$ time with high probability, where $nnz(\\boldsymbol{\\mathit{A}})$ is the number of non-zeros in $\\boldsymbol{\\mathit{A}}$. This improves over the previous best result using gradient descent for $\\ell_1$ regression. We also match the best known running times for interior point methods in several settings. Finally, we also show that if our original matrix $\\boldsymbol{\\mathit{A}}$ is approximately isotropic and the row norms are approximately equal, we can give an algorithm that avoids using fast matrix multiplication and obtains a running time of $\\widetilde{O}(nnz(\\boldsymbol{\\mathit{A}}) + s d^{1.5}\\epsilon^{-2} + d^2\\epsilon^{-2})$, where $s$ is the maximum number of non-zeros in a row of $\\boldsymbol{\\mathit{A}}$. In this setting, we beat the best interior point methods for certain parameter regimes.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/durfee18a/durfee18a.pdf",
        "supp": "",
        "pdf_size": 530213,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13837196433127516897&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "GATECH.EDU;GATECH.EDU;GATECH.EDU",
        "email": "GATECH.EDU;GATECH.EDU;GATECH.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5e93fefdef",
        "title": "A Data Prism: Semi-verified learning in the small-alpha regime",
        "site": "https://proceedings.mlr.press/v75/meister18a.html",
        "author": "Michela Meister; Gregory Valiant",
        "abstract": "We consider a simple model of unreliable or crowdsourced data where there is an underlying set of $n$ binary variables, each \u201cevaluator\u201d contributes a (possibly unreliable or adversarial) estimate of the values of some subset of $r$ of the variables, and the learner is given the true value of a \\emph{constant} number of variables.   We show that, provided an $\\alpha$-fraction of the evaluators are \u201cgood\u201d (either correct, or with independent noise rate $p < 1/2$), then the true values of a $(1-\\eps)$ fraction of the $n$ underlying variables can be deduced as long as $r > \\log_{2-2p}(1/\\alpha)$. For example, if the fraction of \u201cgood\u201d evaluators is larger than $1/16$ and there is no noise in their responses, then accurate recovery is possible provided each worker evaluates a random set of $4$ items.  This result is optimal in that if $r \\leq \\log_{2-2p}(1/\\alpha)$ the large dataset can contain no information.   This setting can be viewed as an instance of the  \\emph{semi-verified} learning model introduced by Charikar, Steinhardt, and Valiant, which explores the tradeoff between the number of items evaluated by each worker and the fraction of \u201cgood\u201d evaluators. In the standard adversarial setting, our algorithm requires $\\tilde{O}\\left(n^{\\log_{2-2p}(1/\\alpha)}\\right)$ evaluators. However, the algorithm runs in near linear time, $\\tilde{O}_{r,\\eps}(n)$, and hence would require only a near-linear number of evaluations in the weaker model in which the adversary\u2019s responses to each $r$-tuple of items are independent of the set of evaluations collected.  These settings and results can also be viewed as examining a general class of semi-adversarial CSPs with a planted assignment. This extreme parameter regime, where the fraction of reliable data is small (inverse exponential in the amount of data provided by each source), is relevant to a number of practical settings.  For example, the setting where you collect a dataset on customer preferences, with each customer specifying preferences for a small (constant) number of items, and the goal is to ascertain the preferences of a specific demographic of interest.   Our results show that this large dataset (which lacks demographic information) can be leveraged together with the preferences of the demographic of interest for a \\emph{constant} (polynomial in $1/\\alpha$ but independent of $n$), number of randomly selected items, to recover an accurate estimate of the entire set of preferences, even if the  fraction of the original dataset contributed by the demographic of interest is inverse exponential in the number of preferences supplied by each customer.   In this sense, our results can be viewed as a \u201cdata prism\u201d allowing one to extract the behavior of specific cohorts from a large, mixed, dataset.",
        "bibtex": "@InProceedings{pmlr-v75-meister18a,\n  title = \t {A Data Prism: Semi-verified learning in the small-alpha regime},\n  author =       {Meister, Michela and Valiant, Gregory},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1530--1546},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/meister18a/meister18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/meister18a.html},\n  abstract = \t {We consider a simple model of unreliable or crowdsourced data where there is an underlying set of $n$ binary variables, each \u201cevaluator\u201d contributes a (possibly unreliable or adversarial) estimate of the values of some subset of $r$ of the variables, and the learner is given the true value of a \\emph{constant} number of variables.   We show that, provided an $\\alpha$-fraction of the evaluators are \u201cgood\u201d (either correct, or with independent noise rate $p < 1/2$), then the true values of a $(1-\\eps)$ fraction of the $n$ underlying variables can be deduced as long as $r > \\log_{2-2p}(1/\\alpha)$. For example, if the fraction of \u201cgood\u201d evaluators is larger than $1/16$ and there is no noise in their responses, then accurate recovery is possible provided each worker evaluates a random set of $4$ items.  This result is optimal in that if $r \\leq \\log_{2-2p}(1/\\alpha)$ the large dataset can contain no information.   This setting can be viewed as an instance of the  \\emph{semi-verified} learning model introduced by Charikar, Steinhardt, and Valiant, which explores the tradeoff between the number of items evaluated by each worker and the fraction of \u201cgood\u201d evaluators. In the standard adversarial setting, our algorithm requires $\\tilde{O}\\left(n^{\\log_{2-2p}(1/\\alpha)}\\right)$ evaluators. However, the algorithm runs in near linear time, $\\tilde{O}_{r,\\eps}(n)$, and hence would require only a near-linear number of evaluations in the weaker model in which the adversary\u2019s responses to each $r$-tuple of items are independent of the set of evaluations collected.  These settings and results can also be viewed as examining a general class of semi-adversarial CSPs with a planted assignment. This extreme parameter regime, where the fraction of reliable data is small (inverse exponential in the amount of data provided by each source), is relevant to a number of practical settings.  For example, the setting where you collect a dataset on customer preferences, with each customer specifying preferences for a small (constant) number of items, and the goal is to ascertain the preferences of a specific demographic of interest.   Our results show that this large dataset (which lacks demographic information) can be leveraged together with the preferences of the demographic of interest for a \\emph{constant} (polynomial in $1/\\alpha$ but independent of $n$), number of randomly selected items, to recover an accurate estimate of the entire set of preferences, even if the  fraction of the original dataset contributed by the demographic of interest is inverse exponential in the number of preferences supplied by each customer.   In this sense, our results can be viewed as a \u201cdata prism\u201d allowing one to extract the behavior of specific cohorts from a large, mixed, dataset.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/meister18a/meister18a.pdf",
        "supp": "",
        "pdf_size": 298951,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16771141388575799311&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "STANFORD.EDU;STANFORD.EDU",
        "email": "STANFORD.EDU;STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "df084ffe9c",
        "title": "A Direct Sum Result for the Information Complexity of Learning",
        "site": "https://proceedings.mlr.press/v75/nachum18a.html",
        "author": "Ido Nachum; Jonathan Shafer; Amir Yehudayoff",
        "abstract": "How many bits of information are required to PAC learn a class of hypotheses of VC dimension $d$? The mathematical setting we follow is that of Bassily et al., where the value of interest is the mutual information $\\mathrm{I}(S;A(S))$ between the input sample $S$ and the hypothesis outputted by the learning algorithm $A$. We introduce a class of functions of VC dimension $d$ over the domain $\\mathcal{X}$ with information complexity at least $\\Omega \\left(d\\log \\log \\frac{|\\mathcal{X}|}{d}\\right)$ bits for any consistent and proper algorithm (deterministic or random). Bassily et al. proved a similar (but quantitatively weaker) result for the case $d=1$. The above result is in fact a special case of a more general phenomenon we explore. We define the notion of {\\em information complexity} of a given class of functions $\\cH$. Intuitively, it is the minimum amount of information that an algorithm for $\\mathcal{X}$ must retain about its input to ensure consistency and properness. We prove a direct sum result for information complexity in this context; roughly speaking, the information complexity sums when combining several classes.",
        "bibtex": "@InProceedings{pmlr-v75-nachum18a,\n  title = \t {A Direct Sum Result for the Information Complexity of Learning},\n  author =       {Nachum, Ido and Shafer, Jonathan and Yehudayoff, Amir},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1547--1568},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/nachum18a/nachum18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/nachum18a.html},\n  abstract = \t { How many bits of information are required to PAC learn a class of hypotheses of VC dimension $d$? The mathematical setting we follow is that of Bassily et al., where the value of interest is the mutual information $\\mathrm{I}(S;A(S))$ between the input sample $S$ and the hypothesis outputted by the learning algorithm $A$. We introduce a class of functions of VC dimension $d$ over the domain $\\mathcal{X}$ with information complexity at least $\\Omega \\left(d\\log \\log \\frac{|\\mathcal{X}|}{d}\\right)$ bits for any consistent and proper algorithm (deterministic or random). Bassily et al. proved a similar (but quantitatively weaker) result for the case $d=1$. The above result is in fact a special case of a more general phenomenon we explore. We define the notion of {\\em information complexity} of a given class of functions $\\cH$. Intuitively, it is the minimum amount of information that an algorithm for $\\mathcal{X}$ must retain about its input to ensure consistency and properness. We prove a direct sum result for information complexity in this context; roughly speaking, the information complexity sums when combining several classes. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/nachum18a/nachum18a.pdf",
        "supp": "",
        "pdf_size": 237975,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4471296121773561531&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Mathematics, Technion-IIT, Haifa, Israel; Computer Science Division, University of California, Berkeley, CA; Department of Mathematics, Technion-IIT, Haifa, Israel",
        "aff_domain": "tx.technion.ac.il;berkeley.edu;gmail.com",
        "email": "tx.technion.ac.il;berkeley.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Technion-IIT;University of California, Berkeley",
        "aff_unique_dep": "Department of Mathematics;Computer Science Division",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.berkeley.edu",
        "aff_unique_abbr": "Technion;UC Berkeley",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Haifa;Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "b0453f58bd",
        "title": "A Faster Approximation Algorithm for the Gibbs Partition Function",
        "site": "https://proceedings.mlr.press/v75/kolmogorov18a.html",
        "author": "Vladimir Kolmogorov",
        "abstract": "We consider the problem of estimating the partition function $Z(\\beta)=\\sum_x \\exp(-\\beta H(x))$ of a Gibbs distribution with a Hamilton $H(\\cdot)$, or more precisely the logarithm of the ratio $q=\\ln Z(0)/Z(\\beta)$. It has been recently shown how to approximate $q$ with high probability assuming the existence of an oracle that produces samples from the Gibbs distribution for a given parameter value in $[0,\\beta]$. The current best known approach due to Huber (2015) uses $O(q\\ln n\\cdot[\\ln q + \\ln \\ln n+\\varepsilon^{-2}])$  oracle calls on average where $\\varepsilon$ is the desired accuracy of approximation and $H(\\cdot)$ is assumed to lie in $\\{0\\}\\cup[1,n]$. We improve the complexity to $O(q\\ln n\\cdot\\varepsilon^{-2})$ oracle calls. We also show that the same complexity can be achieved if exact oracles are replaced with approximate sampling oracles that are within $O(\\frac{\\varepsilon^2}{q\\ln n})$ variation distance from exact oracles. Finally, we prove a lower bound of $\\Omega(q\\cdot \\varepsilon^{-2})$ oracle calls under a natural model of computation.",
        "bibtex": "@InProceedings{pmlr-v75-kolmogorov18a,\n  title = \t {A Faster Approximation Algorithm for the {G}ibbs Partition Function},\n  author =       {Kolmogorov, Vladimir},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {228--249},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/kolmogorov18a/kolmogorov18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/kolmogorov18a.html},\n  abstract = \t { We consider the problem of estimating the partition function $Z(\\beta)=\\sum_x \\exp(-\\beta H(x))$ of a Gibbs distribution with a Hamilton $H(\\cdot)$, or more precisely the logarithm of the ratio $q=\\ln Z(0)/Z(\\beta)$. It has been recently shown how to approximate $q$ with high probability assuming the existence of an oracle that produces samples from the Gibbs distribution for a given parameter value in $[0,\\beta]$. The current best known approach due to Huber (2015) uses $O(q\\ln n\\cdot[\\ln q + \\ln \\ln n+\\varepsilon^{-2}])$  oracle calls on average where $\\varepsilon$ is the desired accuracy of approximation and $H(\\cdot)$ is assumed to lie in $\\{0\\}\\cup[1,n]$. We improve the complexity to $O(q\\ln n\\cdot\\varepsilon^{-2})$ oracle calls. We also show that the same complexity can be achieved if exact oracles are replaced with approximate sampling oracles that are within $O(\\frac{\\varepsilon^2}{q\\ln n})$ variation distance from exact oracles. Finally, we prove a lower bound of $\\Omega(q\\cdot \\varepsilon^{-2})$ oracle calls under a natural model of computation. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/kolmogorov18a/kolmogorov18a.pdf",
        "supp": "",
        "pdf_size": 408974,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12710248229627852292&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IST Austria",
        "aff_domain": "IST.AC.AT",
        "email": "IST.AC.AT",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institute of Science and Technology Austria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ist.ac.at",
        "aff_unique_abbr": "IST Austria",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "a2425394c7",
        "title": "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation",
        "site": "https://proceedings.mlr.press/v75/bhandari18a.html",
        "author": "Jalaj Bhandari; Daniel Russo; Raghav Singal",
        "abstract": "Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its statistical efficiency are available. In this work, we provide a \\emph{simple and explicit finite time analysis} of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors standard techniques for  analyzing stochastic gradient descent algorithms, and therefore inherits the simplicity and elegance of that literature. A final section of the paper shows that all of our main results extend to the study of a variant of Q-learning applied to optimal stopping problems.",
        "bibtex": "@InProceedings{pmlr-v75-bhandari18a,\n  title = \t {A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation},\n  author =       {Bhandari, Jalaj and Russo, Daniel and Singal, Raghav},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1691--1692},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/bhandari18a/bhandari18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/bhandari18a.html},\n  abstract = \t {Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its statistical efficiency are available. In this work, we provide a \\emph{simple and explicit finite time analysis} of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors standard techniques for  analyzing stochastic gradient descent algorithms, and therefore inherits the simplicity and elegance of that literature. A final section of the paper shows that all of our main results extend to the study of a variant of Q-learning applied to optimal stopping problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/bhandari18a/bhandari18a.pdf",
        "supp": "",
        "pdf_size": 58159,
        "gs_citation": 466,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5005747932301112588&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Industrial Engineering and Operations Research, Columbia University; Decision Risk and Operations, Columbia Business School; Industrial Engineering and Operations Research, Columbia University",
        "aff_domain": "COLUMBIA.EDU;GMAIL.COM;COLUMBIA.EDU",
        "email": "COLUMBIA.EDU;GMAIL.COM;COLUMBIA.EDU",
        "github": "",
        "project": "https://arxiv.org/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Columbia University;Columbia Business School",
        "aff_unique_dep": "Industrial Engineering and Operations Research;Decision Risk and Operations",
        "aff_unique_url": "https://www.columbia.edu;https://www.gsb.columbia.edu",
        "aff_unique_abbr": "Columbia;CBS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "643f07ff3c",
        "title": "A General Approach to Multi-Armed Bandits Under Risk Criteria",
        "site": "https://proceedings.mlr.press/v75/cassel18a.html",
        "author": "Asaf Cassel; Shie Mannor; Assaf Zeevi",
        "abstract": "Different risk-related criteria have received recent interest in learning problems, where typically each case is  treated in a customized manner. In this paper we provide a more systematic approach to analyzing  such risk criteria within a stochastic multi-armed bandit (MAB) formulation. We identify a set of general conditions that yield a simple characterization of the oracle rule (which serves as the regret benchmark), and facilitate the design of upper confidence bound (UCB) learning policies. The conditions are derived from problem primitives, primarily focusing on the relation between the arm reward distributions and the (risk criteria) performance metric. Among other things, the work highlights some (possibly non-intuitive) subtleties that differentiate various criteria in conjunction with statistical properties of the arms. Our main findings are illustrated on several widely used objectives such as conditional value-at-risk, mean-variance, Sharpe-ratio, and more.",
        "bibtex": "@InProceedings{pmlr-v75-cassel18a,\n  title = \t {A General Approach to Multi-Armed Bandits Under Risk Criteria},\n  author =       {Cassel, Asaf and Mannor, Shie and Zeevi, Assaf},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1295--1306},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/cassel18a/cassel18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/cassel18a.html},\n  abstract = \t {Different risk-related criteria have received recent interest in learning problems, where typically each case is  treated in a customized manner. In this paper we provide a more systematic approach to analyzing  such risk criteria within a stochastic multi-armed bandit (MAB) formulation. We identify a set of general conditions that yield a simple characterization of the oracle rule (which serves as the regret benchmark), and facilitate the design of upper confidence bound (UCB) learning policies. The conditions are derived from problem primitives, primarily focusing on the relation between the arm reward distributions and the (risk criteria) performance metric. Among other things, the work highlights some (possibly non-intuitive) subtleties that differentiate various criteria in conjunction with statistical properties of the arms. Our main findings are illustrated on several widely used objectives such as conditional value-at-risk, mean-variance, Sharpe-ratio, and more.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/cassel18a/cassel18a.pdf",
        "supp": "",
        "pdf_size": 328950,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2987098595355941273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Faculty of Electrical Engineering, Technion, Israel Institute of Technology; Faculty of Electrical Engineering, Technion, Israel Institute of Technology; Graduate School of Business, Columbia University",
        "aff_domain": "campus.technion.ac.il;ee.technion.ac.il;gsb.columbia.edu",
        "email": "campus.technion.ac.il;ee.technion.ac.il;gsb.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Technion, Israel Institute of Technology;Columbia University",
        "aff_unique_dep": "Faculty of Electrical Engineering;Graduate School of Business",
        "aff_unique_url": "https://www.technion.ac.il;https://www.columbia.edu",
        "aff_unique_abbr": "Technion;Columbia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "69979001af",
        "title": "Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent",
        "site": "https://proceedings.mlr.press/v75/jin18a.html",
        "author": "Chi Jin; Praneeth Netrapalli; Michael I. Jordan",
        "abstract": "Nesterov\u2019s accelerated gradient descent (AGD), an instance of the general family of \u201cmomentum methods,\u201d provably achieves faster convergence rate than gradient descent (GD) in the convex setting. While these methods are widely used in modern \\emph{nonconvex} applications, including training of deep neural networks, whether they are provably superior to GD in the nonconvex setting remains open. This paper studies a simple variant of Nesterov\u2019s\u00a0AGD, and shows that it escapes saddle points and finds a second-order stationary point in $\\tilde{O}(1/\\epsilon^{7/4})$ iterations, matching the best known convergence rate, which is faster than the $\\tilde{O}(1/\\epsilon^{2})$ iterations required by GD. To the best of our knowledge, this is the first direct acceleration (single-loop) algorithm that is provably faster than GD in general nonconvex setting\u2014all previous nonconvex accelerated algorithms rely on more complex mechanisms such as nested loops and proximal terms. Our analysis is based on two key ideas: (1) the use of a simple Hamiltonian function, inspired by a continuous-time perspective, which AGD monotonically decreases on each step even for nonconvex functions, and (2) a novel framework called\u00a0\\emph{improve or localize}, which is useful for tracking the long-term behavior of gradient-based optimization algorithms. We believe that these techniques may deepen our understanding of both acceleration algorithms and nonconvex optimization.",
        "bibtex": "@InProceedings{pmlr-v75-jin18a,\n  title = \t {Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent},\n  author =       {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1042--1085},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/jin18a/jin18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/jin18a.html},\n  abstract = \t {Nesterov\u2019s accelerated gradient descent (AGD), an instance of the general family of \u201cmomentum methods,\u201d provably achieves faster convergence rate than gradient descent (GD) in the convex setting. While these methods are widely used in modern \\emph{nonconvex} applications, including training of deep neural networks, whether they are provably superior to GD in the nonconvex setting remains open. This paper studies a simple variant of Nesterov\u2019s\u00a0AGD, and shows that it escapes saddle points and finds a second-order stationary point in $\\tilde{O}(1/\\epsilon^{7/4})$ iterations, matching the best known convergence rate, which is faster than the $\\tilde{O}(1/\\epsilon^{2})$ iterations required by GD. To the best of our knowledge, this is the first direct acceleration (single-loop) algorithm that is provably faster than GD in general nonconvex setting\u2014all previous nonconvex accelerated algorithms rely on more complex mechanisms such as nested loops and proximal terms. Our analysis is based on two key ideas: (1) the use of a simple Hamiltonian function, inspired by a continuous-time perspective, which AGD monotonically decreases on each step even for nonconvex functions, and (2) a novel framework called\u00a0\\emph{improve or localize}, which is useful for tracking the long-term behavior of gradient-based optimization algorithms. We believe that these techniques may deepen our understanding of both acceleration algorithms and nonconvex optimization.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/jin18a/jin18a.pdf",
        "supp": "",
        "pdf_size": 561651,
        "gs_citation": 305,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7869756547712630025&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Berkeley; Microsoft Research India; University of California, Berkeley",
        "aff_domain": "CS.BERKELEY.EDU;MICROSOFT.COM;CS.BERKELEY.EDU",
        "email": "CS.BERKELEY.EDU;MICROSOFT.COM;CS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Microsoft",
        "aff_unique_dep": ";Microsoft Research India",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "UC Berkeley;MSR India",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "f78dcdc0e0",
        "title": "Accelerating Stochastic Gradient Descent for Least Squares Regression",
        "site": "https://proceedings.mlr.press/v75/jain18a.html",
        "author": "Prateek Jain; Sham M. Kakade; Rahul Kidambi; Praneeth Netrapalli; Aaron Sidford",
        "abstract": "There is widespread sentiment that fast gradient methods (e.g. Nesterov\u2019s acceleration, conjugate gradient, heavy ball) are not effective for the purposes of stochastic optimization due to their instability and error accumulation. Numerous works have attempted to quantify these instabilities in the face of either statistical or non-statistical. This work considers these issues for the special case of stochastic approximation for the least squares regression problem, and our main result refutes this conventional wisdom by showing that acceleration can be made robust to statistical errors.  In particular, this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent.  Critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process. We hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for more general convex and non-convex optimization problems.",
        "bibtex": "@InProceedings{pmlr-v75-jain18a,\n  title = \t {Accelerating Stochastic Gradient Descent for Least Squares Regression},\n  author =       {Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {545--604},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/jain18a/jain18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/jain18a.html},\n  abstract = \t {There is widespread sentiment that fast gradient methods (e.g. Nesterov\u2019s acceleration, conjugate gradient, heavy ball) are not effective for the purposes of stochastic optimization due to their instability and error accumulation. Numerous works have attempted to quantify these instabilities in the face of either statistical or non-statistical. This work considers these issues for the special case of stochastic approximation for the least squares regression problem, and our main result refutes this conventional wisdom by showing that acceleration can be made robust to statistical errors.  In particular, this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent.  Critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process. We hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for more general convex and non-convex optimization problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/jain18a/jain18a.pdf",
        "supp": "",
        "pdf_size": 1089512,
        "gs_citation": 154,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6950970486728633539&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; University of Washington, Seattle, WA, USA; University of Washington, Seattle, WA, USA; Stanford University, Palo Alto, CA, USA",
        "aff_domain": "MICROSOFT.COM;MICROSOFT.COM;CS.WASHINGTON.EDU;UW.EDU;STANFORD.COM",
        "email": "MICROSOFT.COM;MICROSOFT.COM;CS.WASHINGTON.EDU;UW.EDU;STANFORD.COM",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;2",
        "aff_unique_norm": "Microsoft;University of Washington;Stanford University",
        "aff_unique_dep": "Microsoft Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india;https://www.washington.edu;https://www.stanford.edu",
        "aff_unique_abbr": "MSR;UW;Stanford",
        "aff_campus_unique_index": "0;0;1;1;2",
        "aff_campus_unique": "Bangalore;Seattle;Palo Alto",
        "aff_country_unique_index": "0;0;1;1;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "d164abd588",
        "title": "Action-Constrained Markov Decision Processes With Kullback-Leibler Cost",
        "site": "https://proceedings.mlr.press/v75/busic18a.html",
        "author": "Ana Bu\u0161i\u0107; Sean Meyn",
        "abstract": "This paper concerns computation of optimal policies in which the one-step reward function contains a cost term that models Kullback-Leibler divergence with respect to nominal dynamics. This technique was introduced by Todorov in 2007, where it was shown under general conditions that the solution to the average-reward optimality equations reduce to a simple eigenvector problem. Since then many authors have sought to apply this technique to control problems and models of bounded rationality in economics.  A crucial assumption is that the input process is essentially unconstrained. For example, if the nominal dynamics include randomness from nature (e.g., the impact of wind on a moving vehicle), then the optimal control solution does not respect the exogenous nature of this disturbance.  This paper introduces a technique to solve a more general class of action-constrained MDPs. The main idea is to solve an entire parameterized family of MDPs, in which the parameter is a scalar weighting the one-step  reward function. The approach is new and practical even in the original unconstrained formulation.",
        "bibtex": "@InProceedings{pmlr-v75-busic18a,\n  title = \t {Action-Constrained {Markov Decision Processes} With {Kullback-Leibler} Cost},\n  author =       {Bu\\v{s}i\\'{c}, Ana and Meyn, Sean},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1431--1444},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/busic18a/busic18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/busic18a.html},\n  abstract = \t {This paper concerns computation of optimal policies in which the one-step reward function contains a cost term that models Kullback-Leibler divergence with respect to nominal dynamics. This technique was introduced by Todorov in 2007, where it was shown under general conditions that the solution to the average-reward optimality equations reduce to a simple eigenvector problem. Since then many authors have sought to apply this technique to control problems and models of bounded rationality in economics.  A crucial assumption is that the input process is essentially unconstrained. For example, if the nominal dynamics include randomness from nature (e.g., the impact of wind on a moving vehicle), then the optimal control solution does not respect the exogenous nature of this disturbance.  This paper introduces a technique to solve a more general class of action-constrained MDPs. The main idea is to solve an entire parameterized family of MDPs, in which the parameter is a scalar weighting the one-step  reward function. The approach is new and practical even in the original unconstrained formulation.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/busic18a/busic18a.pdf",
        "supp": "",
        "pdf_size": 2596870,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17431500819897356409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Inria, Paris research centre + DI ENS, \u00b4Ecole normale sup \u00b4erieure, CNRS, PSL Research University, Paris, France; Department of Electrical and Computer Engineering at the University of Florida",
        "aff_domain": "INRIA.FR;ECE.UFL.EDU",
        "email": "INRIA.FR;ECE.UFL.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "INRIA;Ecole Normale Sup\u00e9rieure;University of Florida",
        "aff_unique_dep": ";DI ENS;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.inria.fr;https://www.ens.fr;https://www.ufl.edu",
        "aff_unique_abbr": "Inria;ENS;UF",
        "aff_campus_unique_index": "0+0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0+0;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "8960936349",
        "title": "Active Tolerant Testing",
        "site": "https://proceedings.mlr.press/v75/blum18a.html",
        "author": "Avrim Blum; Lunjia Hu",
        "abstract": "In this work, we show that for a nontrivial hypothesis class $\\mathcal C$, we can estimate the distance of a target function $f$ to $\\mathcal C$ (estimate the error rate of the best $h\\in \\mathcal C$) using substantially fewer labeled examples than would be needed to actually {\\em learn} a good $h \\in \\mathcal C$.   Specifically, we show that for the class $\\mathcal C$ of unions of $d$ intervals on the line, in the active learning setting in which we have access to a pool of unlabeled examples drawn from an arbitrary underlying distribution $\\mathcal D$, we can estimate the error rate of the best $h \\in \\mathcal C$ to an additive error $\\epsilon$ with a number of label requests that is {\\em independent of $d$} and depends only on $\\epsilon$.  In particular, we make $O(\\frac{1}{\\epsilon^6}\\log \\frac{1}{\\epsilon})$ label queries to an unlabeled pool of size $O(\\frac{d}{\\epsilon^2}\\log \\frac{1}{\\epsilon})$.  This task of estimating the distance of an unknown $f$ to a given class $\\mathcal C$  is called {\\em tolerant testing} or {\\em distance estimation} in the testing literature, usually studied in a membership query model and with respect to the uniform distribution.  Our work extends that of Balcan et al. (2012) who solved the {\\em non}-tolerant testing problem for this class (distinguishing the zero-error case from the case that the best hypothesis in the class has error greater than $\\epsilon$).   We also consider the related problem of estimating the performance of a given learning algorithm $\\mathcal A$ in this setting.  That is, given a large pool of unlabeled examples drawn from distribution $\\mathcal D$, can we, from only a few label queries, estimate how well $\\mathcal A$ would perform if the entire dataset were labeled and given as training data to $\\mathcal A$?   We focus on $k$-Nearest Neighbor style algorithms, and also show how our results can be applied to the problem of hyperparameter tuning (selecting the best value of $k$ for the given learning problem).",
        "bibtex": "@InProceedings{pmlr-v75-blum18a,\n  title = \t {Active Tolerant Testing},\n  author =       {Blum, Avrim and Hu, Lunjia},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {474--497},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/blum18a/blum18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/blum18a.html},\n  abstract = \t {In this work, we show that for a nontrivial hypothesis class $\\mathcal C$, we can estimate the distance of a target function $f$ to $\\mathcal C$ (estimate the error rate of the best $h\\in \\mathcal C$) using substantially fewer labeled examples than would be needed to actually {\\em learn} a good $h \\in \\mathcal C$.   Specifically, we show that for the class $\\mathcal C$ of unions of $d$ intervals on the line, in the active learning setting in which we have access to a pool of unlabeled examples drawn from an arbitrary underlying distribution $\\mathcal D$, we can estimate the error rate of the best $h \\in \\mathcal C$ to an additive error $\\epsilon$ with a number of label requests that is {\\em independent of $d$} and depends only on $\\epsilon$.  In particular, we make $O(\\frac{1}{\\epsilon^6}\\log \\frac{1}{\\epsilon})$ label queries to an unlabeled pool of size $O(\\frac{d}{\\epsilon^2}\\log \\frac{1}{\\epsilon})$.  This task of estimating the distance of an unknown $f$ to a given class $\\mathcal C$  is called {\\em tolerant testing} or {\\em distance estimation} in the testing literature, usually studied in a membership query model and with respect to the uniform distribution.  Our work extends that of Balcan et al. (2012) who solved the {\\em non}-tolerant testing problem for this class (distinguishing the zero-error case from the case that the best hypothesis in the class has error greater than $\\epsilon$).   We also consider the related problem of estimating the performance of a given learning algorithm $\\mathcal A$ in this setting.  That is, given a large pool of unlabeled examples drawn from distribution $\\mathcal D$, can we, from only a few label queries, estimate how well $\\mathcal A$ would perform if the entire dataset were labeled and given as training data to $\\mathcal A$?   We focus on $k$-Nearest Neighbor style algorithms, and also show how our results can be applied to the problem of hyperparameter tuning (selecting the best value of $k$ for the given learning problem).}\n}",
        "pdf": "http://proceedings.mlr.press/v75/blum18a/blum18a.pdf",
        "supp": "",
        "pdf_size": 422556,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6687260049776277400&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Toyota Technological Institute at Chicago, Chicago, IL, USA; Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China",
        "aff_domain": "TTIC.EDU;MAILS.TSINGHUA.EDU.CN",
        "email": "TTIC.EDU;MAILS.TSINGHUA.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;Tsinghua University",
        "aff_unique_dep": ";Institute for Interdisciplinary Information Sciences",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "TTI Chicago;Tsinghua",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Chicago;Beijing",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "9af7cc7dcd",
        "title": "Actively Avoiding Nonsense in Generative Models",
        "site": "https://proceedings.mlr.press/v75/hanneke18a.html",
        "author": "Steve Hanneke; Adam Tauman Kalai; Gautam Kamath; Christos Tzamos",
        "abstract": "A generative model may generate utter nonsense when it is fit to maximize the likelihood of observed data. This happens due to \u201cmodel error,\u201d i.e., when the true data generating distribution does not fit within the class of generative models being learned. To address this, we propose a model of active distribution learning using a binary invalidity oracle that identifies some examples as clearly invalid, together with random positive examples sampled from the true distribution. The goal is to maximize the likelihood of the positive examples subject to the constraint of (almost) never generating examples labeled invalid by the oracle. Guarantees are agnostic compared to a class of probability distributions. We first show that proper learning may require exponentially many queries to the invalidity oracle. We then give an improper distribution learning algorithm that uses only polynomially many queries.",
        "bibtex": "@InProceedings{pmlr-v75-hanneke18a,\n  title = \t {Actively Avoiding Nonsense in Generative Models},\n  author =       {Hanneke, Steve and Kalai, Adam Tauman and Kamath, Gautam and Tzamos, Christos},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {209--227},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/hanneke18a/hanneke18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/hanneke18a.html},\n  abstract = \t {A generative model may generate utter nonsense when it is fit to maximize the likelihood of observed data. This happens due to \u201cmodel error,\u201d i.e., when the true data generating distribution does not fit within the class of generative models being learned. To address this, we propose a model of active distribution learning using a binary invalidity oracle that identifies some examples as clearly invalid, together with random positive examples sampled from the true distribution. The goal is to maximize the likelihood of the positive examples subject to the constraint of (almost) never generating examples labeled invalid by the oracle. Guarantees are agnostic compared to a class of probability distributions. We first show that proper learning may require exponentially many queries to the invalidity oracle. We then give an improper distribution learning algorithm that uses only polynomially many queries. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/hanneke18a/hanneke18a.pdf",
        "supp": "",
        "pdf_size": 356131,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12180679734305640284&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Princeton, NJ; Microsoft Research, New England; EECS & CSAIL, MIT; Microsoft Research, New England",
        "aff_domain": "GMAIL.COM;MICROSOFT.COM;CSAIL.MIT.EDU;MICROSOFT.COM",
        "email": "GMAIL.COM;MICROSOFT.COM;CSAIL.MIT.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Princeton University;Microsoft;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Microsoft Research;Electrical Engineering & Computer Science and Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.princeton.edu;https://www.microsoft.com/en-us/research/group/newengland;https://www.mit.edu",
        "aff_unique_abbr": "Princeton;MSR;MIT",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";New England;Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d49977479d",
        "title": "Adaptivity to Smoothness in X-armed bandits",
        "site": "https://proceedings.mlr.press/v75/locatelli18a.html",
        "author": "Andrea Locatelli; Alexandra Carpentier",
        "abstract": "We study the stochastic continuum-armed bandit problem from the angle of adaptivity to \\emph{unknown regularity} of the reward function $f$. We prove that there exists no strategy for the cumulative regret that adapts optimally to the \\emph{smoothness} of $f$. We show however that such minimax optimal adaptive strategies exist if the learner is given \\emph{extra-information} about $f$. Finally, we complement our positive results with matching lower bounds.",
        "bibtex": "@InProceedings{pmlr-v75-locatelli18a,\n  title = \t {Adaptivity to Smoothness in X-armed bandits},\n  author =       {Locatelli, Andrea and Carpentier, Alexandra},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1463--1492},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/locatelli18a/locatelli18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/locatelli18a.html},\n  abstract = \t {We study the stochastic continuum-armed bandit problem from the angle of adaptivity to \\emph{unknown regularity} of the reward function $f$. We prove that there exists no strategy for the cumulative regret that adapts optimally to the \\emph{smoothness} of $f$. We show however that such minimax optimal adaptive strategies exist if the learner is given \\emph{extra-information} about $f$. Finally, we complement our positive results with matching lower bounds.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/locatelli18a/locatelli18a.pdf",
        "supp": "",
        "pdf_size": 405671,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7361335721021666656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Mathematics Department, Otto-von-Guericke-Universit\u00e4t Magdeburg; Mathematics Department, Otto-von-Guericke-Universit\u00e4t Magdeburg",
        "aff_domain": "OVGU.DE;OVGU.DE",
        "email": "OVGU.DE;OVGU.DE",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Otto-von-Guericke-Universit\u00e4t Magdeburg",
        "aff_unique_dep": "Mathematics Department",
        "aff_unique_url": "https://www.ovgu.de",
        "aff_unique_abbr": "OVGU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Magdeburg",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "bf50957240",
        "title": "Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations",
        "site": "https://proceedings.mlr.press/v75/li18a.html",
        "author": "Yuanzhi Li; Tengyu Ma; Hongyang Zhang",
        "abstract": "We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\\star}$, we can recover $X^{\\star}$ by parameterizing it by $UU^\\top$ with $U\\in \\mathbb R^{d\\times d}$ and minimizing the squared loss, even if $r \\ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\\star}$ in $\\tilde{O}(\\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.\u201917 under the restricted isometry property.  The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications.",
        "bibtex": "@InProceedings{pmlr-v75-li18a,\n  title = \t {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},\n  author =       {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {2--47},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/li18a/li18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/li18a.html},\n  abstract = \t {We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\\star}$, we can recover $X^{\\star}$ by parameterizing it by $UU^\\top$ with $U\\in \\mathbb R^{d\\times d}$ and minimizing the squared loss, even if $r \\ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\\star}$ in $\\tilde{O}(\\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.\u201917 under the restricted isometry property.  The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/li18a/li18a.pdf",
        "supp": "",
        "pdf_size": 595379,
        "gs_citation": 395,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17695806198187777209&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Princeton University; Facebook AI Research + Stanford University; Stanford University",
        "aff_domain": "CS.PRINCETON.EDU;STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.PRINCETON.EDU;STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;2",
        "aff_unique_norm": "Princeton University;Meta;Stanford University",
        "aff_unique_dep": ";Facebook AI Research;",
        "aff_unique_url": "https://www.princeton.edu;https://research.facebook.com;https://www.stanford.edu",
        "aff_unique_abbr": "Princeton;FAIR;Stanford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e1b455a091",
        "title": "An Analysis of the t-SNE Algorithm for Data Visualization",
        "site": "https://proceedings.mlr.press/v75/arora18a.html",
        "author": "Sanjeev Arora; Wei Hu; Pravesh K. Kothari",
        "abstract": "A first line of attack in exploratory data analysis is \\emph{data visualization}, i.e., generating a 2-dimensional representation of data that makes \\emph{clusters} of similar points visually identifiable. Standard Johnson-Lindenstrauss dimensionality reduction does not produce data visualizations. The \\emph{t-SNE} heuristic of van der Maaten and Hinton, which is based on non-convex optimization, has become the \\emph{de facto} standard for visualization in a wide range of applications. This work gives a formal framework for the problem of data visualization \u2013 finding a 2-dimensional embedding of clusterable data that correctly separates individual clusters to make them visually identifiable. We then give a rigorous analysis of the performance of t-SNE under a natural, deterministic condition on the \u201cground-truth\u201d clusters (similar to conditions assumed in earlier analyses of clustering) in the underlying data. These are the first provable guarantees on t-SNE for constructing good data visualizations.  We show that our deterministic condition is satisfied by considerably general probabilistic generative models for clusterable data such as mixtures of well-separated log-concave distributions. Finally, we give theoretical evidence that t-SNE provably succeeds in \\emph{partially} recovering cluster structure even when the above deterministic condition is not met.",
        "bibtex": "@InProceedings{pmlr-v75-arora18a,\n  title = \t {An Analysis of the t-SNE Algorithm for Data Visualization},\n  author =       {Arora, Sanjeev and Hu, Wei and Kothari, Pravesh K.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1455--1462},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/arora18a/arora18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/arora18a.html},\n  abstract = \t {A first line of attack in exploratory data analysis is \\emph{data visualization}, i.e., generating a 2-dimensional representation of data that makes \\emph{clusters} of similar points visually identifiable. Standard Johnson-Lindenstrauss dimensionality reduction does not produce data visualizations. The \\emph{t-SNE} heuristic of van der Maaten and Hinton, which is based on non-convex optimization, has become the \\emph{de facto} standard for visualization in a wide range of applications. This work gives a formal framework for the problem of data visualization \u2013 finding a 2-dimensional embedding of clusterable data that correctly separates individual clusters to make them visually identifiable. We then give a rigorous analysis of the performance of t-SNE under a natural, deterministic condition on the \u201cground-truth\u201d clusters (similar to conditions assumed in earlier analyses of clustering) in the underlying data. These are the first provable guarantees on t-SNE for constructing good data visualizations.  We show that our deterministic condition is satisfied by considerably general probabilistic generative models for clusterable data such as mixtures of well-separated log-concave distributions. Finally, we give theoretical evidence that t-SNE provably succeeds in \\emph{partially} recovering cluster structure even when the above deterministic condition is not met.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/arora18a/arora18a.pdf",
        "supp": "",
        "pdf_size": 520105,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5188792214794382571&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Princeton University; Princeton University; Princeton University + Institute for Advanced Study",
        "aff_domain": "CS.PRINCETON.EDU;CS.PRINCETON.EDU;CS.PRINCETON.EDU",
        "email": "CS.PRINCETON.EDU;CS.PRINCETON.EDU;CS.PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Princeton University;Institute for Advanced Study",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.princeton.edu;https://ias.edu",
        "aff_unique_abbr": "Princeton;IAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "33a917b220",
        "title": "An Estimate Sequence for Geodesically Convex Optimization",
        "site": "https://proceedings.mlr.press/v75/zhang18a.html",
        "author": "Hongyi Zhang; Suvrit Sra",
        "abstract": "We propose a Riemannian version of Nesterov\u2019s Accelerated Gradient algorithm (\\textsc{Ragd}), and show that for \\emph{geodesically} smooth and strongly convex problems, within a neighborhood of the minimizer whose radius depends on the condition number as well as the sectional curvature of the manifold, \\textsc{Ragd} converges to the minimizer with acceleration. Unlike the algorithm in (Liu et al., 2017) that requires the exact solution to a nonlinear equation which in turn may be intractable, our algorithm is constructive and computationally tractable. Our proof exploits a new estimate sequence and a novel bound on the nonlinear metric distortion, both ideas may be of independent interest.",
        "bibtex": "@InProceedings{pmlr-v75-zhang18a,\n  title = \t {An Estimate Sequence for Geodesically Convex Optimization},\n  author =       {Zhang, Hongyi and Sra, Suvrit},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1703--1723},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/zhang18a/zhang18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/zhang18a.html},\n  abstract = \t {We propose a Riemannian version of Nesterov\u2019s Accelerated Gradient algorithm (\\textsc{Ragd}), and show that for \\emph{geodesically} smooth and strongly convex problems, within a neighborhood of the minimizer whose radius depends on the condition number as well as the sectional curvature of the manifold, \\textsc{Ragd} converges to the minimizer with acceleration. Unlike the algorithm in (Liu et al., 2017) that requires the exact solution to a nonlinear equation which in turn may be intractable, our algorithm is constructive and computationally tractable. Our proof exploits a new estimate sequence and a novel bound on the nonlinear metric distortion, both ideas may be of independent interest.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/zhang18a/zhang18a.pdf",
        "supp": "",
        "pdf_size": 303673,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8726841291642507499&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "BCS and LIDS, Massachusetts Institute of Technology; EECS and LIDS, Massachusetts Institute of Technology",
        "aff_domain": "MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "BCS and LIDS",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ba9fba2b9b",
        "title": "An Optimal Learning Algorithm for Online Unconstrained Submodular Maximization",
        "site": "https://proceedings.mlr.press/v75/roughgarden18a.html",
        "author": "Tim Roughgarden; Joshua R. Wang",
        "abstract": "We consider a basic problem at the interface of two fundamental fields: {\\em submodular optimization} and {\\em online learning}.  In the {\\em online unconstrained submodular maximization (online USM) problem}, there is a universe $[n]=\\{1,2,\\ldots,n\\}$ and a sequence of $T$ nonnegative (not necessarily monotone) submodular functions arrive over time.  The goal is to design a computationally efficient online algorithm, which chooses a subset of $[n]$ at each time step as a function only of the past, such that the accumulated value of the chosen subsets is as close as possible to the maximum total value of a fixed subset in hindsight.  Our main result is a polynomial-time  no-$\\frac12$-regret algorithm for this problem, meaning that for every sequence of nonnegative submodular functions, the algorithm\u2019s expected total value is at least $\\frac12$ times that of the best subset in hindsight, up to an error term sublinear in $T$. The factor of $\\tfrac 12$ cannot be improved upon by any polynomial-time online algorithm when the submodular functions are presented as value oracles. Previous work on the offline problem implies that picking a subset uniformly at random in each time step achieves zero $\\frac14$-regret. A byproduct of our techniques is an explicit subroutine for the two-experts problem that has an unusually strong regret guarantee: the total value of its choices is comparable to twice the total value of either expert on rounds it did not pick that expert. This subroutine may be of independent interest.",
        "bibtex": "@InProceedings{pmlr-v75-roughgarden18a,\n  title = \t {An Optimal Learning Algorithm for Online Unconstrained Submodular Maximization},\n  author =       {Roughgarden, Tim and Wang, Joshua R.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1307--1325},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/roughgarden18a/roughgarden18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/roughgarden18a.html},\n  abstract = \t { We consider a basic problem at the interface of two fundamental fields: {\\em submodular optimization} and {\\em online learning}.  In the {\\em online unconstrained submodular maximization (online USM) problem}, there is a universe $[n]=\\{1,2,\\ldots,n\\}$ and a sequence of $T$ nonnegative (not necessarily monotone) submodular functions arrive over time.  The goal is to design a computationally efficient online algorithm, which chooses a subset of $[n]$ at each time step as a function only of the past, such that the accumulated value of the chosen subsets is as close as possible to the maximum total value of a fixed subset in hindsight.  Our main result is a polynomial-time  no-$\\frac12$-regret algorithm for this problem, meaning that for every sequence of nonnegative submodular functions, the algorithm\u2019s expected total value is at least $\\frac12$ times that of the best subset in hindsight, up to an error term sublinear in $T$. The factor of $\\tfrac 12$ cannot be improved upon by any polynomial-time online algorithm when the submodular functions are presented as value oracles. Previous work on the offline problem implies that picking a subset uniformly at random in each time step achieves zero $\\frac14$-regret. A byproduct of our techniques is an explicit subroutine for the two-experts problem that has an unusually strong regret guarantee: the total value of its choices is comparable to twice the total value of either expert on rounds it did not pick that expert. This subroutine may be of independent interest. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/roughgarden18a/roughgarden18a.pdf",
        "supp": "",
        "pdf_size": 329033,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16579928713860612226&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3edc1fbe54",
        "title": "An explicit analysis of the entropic penalty in linear programming",
        "site": "https://proceedings.mlr.press/v75/weed18a.html",
        "author": "Jonathan Weed",
        "abstract": "Solving linear programs by using entropic penalization has recently attracted new interest in the optimization community, since this strategy forms the basis for the fastest-known algorithms for the optimal transport problem, with many applications in modern large-scale machine learning.  Crucial to these applications has been an analysis of how quickly solutions to the penalized program approach true optima to the original linear program.  More than 20 years ago, Cominetti and San Mart\u00edn showed that this convergence is exponentially fast; however, their proof is asymptotic and does not give any indication of how accurately the entropic program approximates the original program for any particular choice of the penalization parameter.  We close this long-standing gap in the literature regarding entropic penalization by giving a new proof of the exponential convergence, valid for any linear program.  Our proof is non-asymptotic, yields explicit constants, and has the virtue of being extremely simple.  We provide matching lower bounds and show that the entropic approach does not lead to a near-linear time approximation scheme for the linear assignment problem.",
        "bibtex": "@InProceedings{pmlr-v75-weed18a,\n  title = \t {An explicit analysis of the entropic penalty in linear programming},\n  author =       {Weed, Jonathan},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1841--1855},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/weed18a/weed18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/weed18a.html},\n  abstract = \t {Solving linear programs by using entropic penalization has recently attracted new interest in the optimization community, since this strategy forms the basis for the fastest-known algorithms for the optimal transport problem, with many applications in modern large-scale machine learning.  Crucial to these applications has been an analysis of how quickly solutions to the penalized program approach true optima to the original linear program.  More than 20 years ago, Cominetti and San Mart\u00edn showed that this convergence is exponentially fast; however, their proof is asymptotic and does not give any indication of how accurately the entropic program approximates the original program for any particular choice of the penalization parameter.  We close this long-standing gap in the literature regarding entropic penalization by giving a new proof of the exponential convergence, valid for any linear program.  Our proof is non-asymptotic, yields explicit constants, and has the virtue of being extremely simple.  We provide matching lower bounds and show that the entropic approach does not lead to a near-linear time approximation scheme for the linear assignment problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/weed18a/weed18a.pdf",
        "supp": "",
        "pdf_size": 311353,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2684127454370979748&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Massachusetts Institute of Technology, Cambridge, MA 02139, USA",
        "aff_domain": "MIT.EDU",
        "email": "MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8374ca9e0d",
        "title": "Approximate Nearest Neighbors in Limited  Space",
        "site": "https://proceedings.mlr.press/v75/indyk18a.html",
        "author": "Piotr Indyk; Tal Wagner",
        "abstract": "We consider the $(1+\\epsilon)$-approximate nearest neighbor search problem: given a set $X$ of $n$ points in a $d$-dimensional space, build a data structure that, given any query point  $y$,  finds a point $x \\in X$ whose distance to $y$ is at most $(1+\\epsilon) \\min_{x \\in X} \\|x-y\\|$ for an accuracy parameter $\\epsilon \\in (0,1)$.  Our main result is a data structure that occupies only $O(\\epsilon^{-2} n \\log(n) \\log(1/\\epsilon))$ bits of space, assuming all point coordinates are integers in the range  $\\{-n^{O(1)} \\ldots n^{O(1)}\\}$, i.e., the coordinates have $O(\\log n)$ bits of precision. This improves over the best previously known space bound of         $O(\\epsilon^{-2} n \\log(n)^2)$, obtained via the randomized dimensionality reduction method of Johnson and Lindenstrauss (1984).  We also consider the more general problem of estimating all distances from a collection of query points to all data points $X$, and provide almost tight upper and lower bounds for the space complexity of this problem.",
        "bibtex": "@InProceedings{pmlr-v75-indyk18a,\n  title = \t {Approximate Nearest Neighbors in Limited  Space},\n  author =       {Indyk, Piotr and Wagner, Tal},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {2012--2036},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/indyk18a/indyk18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/indyk18a.html},\n  abstract = \t {We consider the $(1+\\epsilon)$-approximate nearest neighbor search problem: given a set $X$ of $n$ points in a $d$-dimensional space, build a data structure that, given any query point  $y$,  finds a point $x \\in X$ whose distance to $y$ is at most $(1+\\epsilon) \\min_{x \\in X} \\|x-y\\|$ for an accuracy parameter $\\epsilon \\in (0,1)$.  Our main result is a data structure that occupies only $O(\\epsilon^{-2} n \\log(n) \\log(1/\\epsilon))$ bits of space, assuming all point coordinates are integers in the range  $\\{-n^{O(1)} \\ldots n^{O(1)}\\}$, i.e., the coordinates have $O(\\log n)$ bits of precision. This improves over the best previously known space bound of         $O(\\epsilon^{-2} n \\log(n)^2)$, obtained via the randomized dimensionality reduction method of Johnson and Lindenstrauss (1984).  We also consider the more general problem of estimating all distances from a collection of query points to all data points $X$, and provide almost tight upper and lower bounds for the space complexity of this problem. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/indyk18a/indyk18a.pdf",
        "supp": "",
        "pdf_size": 401919,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1679495049120630132&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "CSAIL, MIT; CSAIL, MIT",
        "aff_domain": "MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "96cd09daab",
        "title": "Approximation beats concentration? An approximation view  on inference with smooth radial kernels",
        "site": "https://proceedings.mlr.press/v75/belkin18a.html",
        "author": "Mikhail Belkin",
        "abstract": "Positive definite kernels and their associated Reproducing Kernel Hilbert Spaces provide a mathematically compelling and practically competitive framework for learning from data.  In this paper we take the approximation theory point of view to explore various aspects of  smooth kernels related to their inferential properties. We  analyze eigenvalue decay of  kernels operators and  matrices,  properties of eigenfunctions/eigenvectors and \u201cFourier\u201d coefficients of functions in the kernel space restricted to a discrete set of data points. We also investigate the fitting capacity of kernels,  giving explicit bounds on the fat shattering dimension of the balls in  Reproducing Kernel Hilbert spaces.  Interestingly, the same properties that make kernels  very effective approximators for functions in their \u201cnative\u201d kernel space,  also limit their capacity to represent arbitrary functions.  We discuss various implications, including those for gradient descent type methods. It is important to note that most of our  bounds are measure independent.  Moreover,  at least in moderate dimension, the bounds for eigenvalues are much tighter than the bounds which can be obtained from the usual matrix concentration results. For example, we see that  eigenvalues of kernel matrices show nearly exponential decay with constants depending only on the kernel and the domain. We call this \u201capproximation beats concentration\u201d phenomenon as even when the data are sampled from a probability distribution, some of their aspects are better understood in terms of approximation theory.",
        "bibtex": "@InProceedings{pmlr-v75-belkin18a,\n  title = \t {Approximation beats concentration? An approximation view  on inference with smooth radial kernels},\n  author =       {Belkin, Mikhail},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1348--1361},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/belkin18a/belkin18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/belkin18a.html},\n  abstract = \t {Positive definite kernels and their associated Reproducing Kernel Hilbert Spaces provide a mathematically compelling and practically competitive framework for learning from data.  In this paper we take the approximation theory point of view to explore various aspects of  smooth kernels related to their inferential properties. We  analyze eigenvalue decay of  kernels operators and  matrices,  properties of eigenfunctions/eigenvectors and \u201cFourier\u201d coefficients of functions in the kernel space restricted to a discrete set of data points. We also investigate the fitting capacity of kernels,  giving explicit bounds on the fat shattering dimension of the balls in  Reproducing Kernel Hilbert spaces.  Interestingly, the same properties that make kernels  very effective approximators for functions in their \u201cnative\u201d kernel space,  also limit their capacity to represent arbitrary functions.  We discuss various implications, including those for gradient descent type methods. It is important to note that most of our  bounds are measure independent.  Moreover,  at least in moderate dimension, the bounds for eigenvalues are much tighter than the bounds which can be obtained from the usual matrix concentration results. For example, we see that  eigenvalues of kernel matrices show nearly exponential decay with constants depending only on the kernel and the domain. We call this \u201capproximation beats concentration\u201d phenomenon as even when the data are sampled from a probability distribution, some of their aspects are better understood in terms of approximation theory.  }\n}",
        "pdf": "http://proceedings.mlr.press/v75/belkin18a/belkin18a.pdf",
        "supp": "",
        "pdf_size": 424875,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15875761670146299615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b67671c9e2",
        "title": "Averaging Stochastic Gradient Descent on Riemannian Manifolds",
        "site": "https://proceedings.mlr.press/v75/tripuraneni18a.html",
        "author": "Nilesh Tripuraneni; Nicolas Flammarion; Francis Bach; Michael I. Jordan",
        "abstract": "We consider the minimization of a function defined on a Riemannian manifold $\\mathcal{M}$ accessible only through unbiased estimates of its gradients. We develop a geometric framework to transform a sequence of slowly converging iterates generated from stochastic gradient descent (SGD) on $\\mathcal{M}$ to an averaged iterate sequence with a robust and fast $O(1/n)$ convergence rate. We then present an application of our framework to geodesically-strongly-convex (and possibly Euclidean non-convex) problems.  Finally, we demonstrate how these ideas apply to the case of streaming $k$-PCA, where we show how to accelerate the slow rate of the randomized power method (without requiring knowledge of the eigengap) into a robust algorithm achieving the optimal rate of convergence.",
        "bibtex": "@InProceedings{pmlr-v75-tripuraneni18a,\n  title = \t {Averaging Stochastic Gradient Descent on {R}iemannian Manifolds},\n  author =       {Tripuraneni, Nilesh and Flammarion, Nicolas and Bach, Francis and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {650--687},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/tripuraneni18a/tripuraneni18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/tripuraneni18a.html},\n  abstract = \t {We consider the minimization of a function defined on a Riemannian manifold $\\mathcal{M}$ accessible only through unbiased estimates of its gradients. We develop a geometric framework to transform a sequence of slowly converging iterates generated from stochastic gradient descent (SGD) on $\\mathcal{M}$ to an averaged iterate sequence with a robust and fast $O(1/n)$ convergence rate. We then present an application of our framework to geodesically-strongly-convex (and possibly Euclidean non-convex) problems.  Finally, we demonstrate how these ideas apply to the case of streaming $k$-PCA, where we show how to accelerate the slow rate of the randomized power method (without requiring knowledge of the eigengap) into a robust algorithm achieving the optimal rate of convergence.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/tripuraneni18a/tripuraneni18a.pdf",
        "supp": "",
        "pdf_size": 512416,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15149619372034901477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, Berkeley; University of California, Berkeley; INRIA, Ecole Normale Sup\u00e9rieure and PSL Research University; University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;inria.fr;cs.berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;inria.fr;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of California, Berkeley;INRIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.inria.fr",
        "aff_unique_abbr": "UC Berkeley;INRIA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "37662b9573",
        "title": "Best of both worlds: Stochastic & adversarial best-arm identification",
        "site": "https://proceedings.mlr.press/v75/abbasi-yadkori18a.html",
        "author": "Yasin Abbasi-Yadkori; Peter Bartlett; Victor Gabillon; Alan Malek; Michal Valko",
        "abstract": "We study bandit best-arm identification with arbitrary and potentially adversarial rewards. A simple random uniform learner obtains the optimal rate of error in the adversarial scenario. However, this type of strategy is suboptimal when the rewards are sampled stochastically. Therefore, we ask: $\\backslash$emph{\\{}Can we design a learner that performs optimally in both the stochastic and adversarial problems while not being aware of the nature of the rewards?{\\}} First, we show that designing such a learner is impossible in general. In particular, to be robust to adversarial rewards, we can only guarantee optimal rates of error on a subset of the stochastic problems. We give a lower bound that characterizes the optimal rate in stochastic problems if the strategy is constrained to be robust to adversarial rewards. Finally, we design a simple parameter-free algorithm and show that its probability of error matches (up to log factors) the lower bound in stochastic problems, and it is also robust to adversarial ones.",
        "bibtex": "@InProceedings{pmlr-v75-abbasi-yadkori18a,\n  title = \t {{Best of both worlds: Stochastic {&} adversarial best-arm identification}},\n  author =       {Abbasi-Yadkori, Yasin and Bartlett, Peter and Gabillon, Victor and Malek, Alan and Valko, Michal},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {918--949},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/abbasi-yadkori18a/abbasi-yadkori18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/abbasi-yadkori18a.html},\n  abstract = \t {We study bandit best-arm identification with arbitrary and potentially adversarial rewards. A simple random uniform learner obtains the optimal rate of error in the adversarial scenario. However, this type of strategy is suboptimal when the rewards are sampled stochastically. Therefore, we ask: $\\backslash$emph{\\{}Can we design a learner that performs optimally in both the stochastic and adversarial problems while not being aware of the nature of the rewards?{\\}} First, we show that designing such a learner is impossible in general. In particular, to be robust to adversarial rewards, we can only guarantee optimal rates of error on a subset of the stochastic problems. We give a lower bound that characterizes the optimal rate in stochastic problems if the strategy is constrained to be robust to adversarial rewards. Finally, we design a simple parameter-free algorithm and show that its probability of error matches (up to log factors) the lower bound in stochastic problems, and it is also robust to adversarial ones.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/abbasi-yadkori18a/abbasi-yadkori18a.pdf",
        "supp": "",
        "pdf_size": 1157689,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10175560999800033634&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Adobe Research, USA; University of California, Berkeley, USA; Queensland University of Technology - ACEMS, Australia; Massachusetts Institute of Technology, USA; SequeL team, INRIA Lille - Nord Europe, France",
        "aff_domain": "ADOBE.COM;BERKELEY.EDU;QUT.EDU.AU;MIT.EDU;INRIA.FR",
        "email": "ADOBE.COM;BERKELEY.EDU;QUT.EDU.AU;MIT.EDU;INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "Adobe;University of California, Berkeley;Queensland University of Technology;Massachusetts Institute of Technology;INRIA Lille - Nord Europe",
        "aff_unique_dep": "Adobe Research;;ACEMS;;SequeL team",
        "aff_unique_url": "https://research.adobe.com;https://www.berkeley.edu;https://www.qut.edu.au;https://web.mit.edu;https://www.inria.fr/en/centre/lille-nord-europe",
        "aff_unique_abbr": "Adobe;UC Berkeley;QUT;MIT;INRIA",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Berkeley;Lille",
        "aff_country_unique_index": "0;0;1;0;2",
        "aff_country_unique": "United States;Australia;France"
    },
    {
        "id": "e82cebb037",
        "title": "Black-Box Reductions for Parameter-free Online Learning in Banach Spaces",
        "site": "https://proceedings.mlr.press/v75/cutkosky18a.html",
        "author": "Ashok Cutkosky; Francesco Orabona",
        "abstract": "We introduce several new black-box reductions that significantly improve the design of adaptive and parameter-free online learning algorithms by simplifying analysis, improving regret guarantees, and sometimes even improving runtime. We reduce parameter-free online learning to online exp-concave optimization, we reduce optimization in a Banach space to one-dimensional optimization, and we reduce optimization over a constrained domain to unconstrained optimization. All of our reductions run as fast as online gradient descent. We use our new techniques to improve upon the previously best regret bounds for parameter-free learning, and do so for arbitrary norms.",
        "bibtex": "@InProceedings{pmlr-v75-cutkosky18a,\n  title = \t {Black-Box Reductions for Parameter-free Online Learning in Banach Spaces},\n  author =       {Cutkosky, Ashok and Orabona, Francesco},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1493--1529},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/cutkosky18a/cutkosky18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/cutkosky18a.html},\n  abstract = \t {We introduce several new black-box reductions that significantly improve the design of adaptive and parameter-free online learning algorithms by simplifying analysis, improving regret guarantees, and sometimes even improving runtime. We reduce parameter-free online learning to online exp-concave optimization, we reduce optimization in a Banach space to one-dimensional optimization, and we reduce optimization over a constrained domain to unconstrained optimization. All of our reductions run as fast as online gradient descent. We use our new techniques to improve upon the previously best regret bounds for parameter-free learning, and do so for arbitrary norms.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/cutkosky18a/cutkosky18a.pdf",
        "supp": "",
        "pdf_size": 372663,
        "gs_citation": 166,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18124213835733373106&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University; Stony Brook University",
        "aff_domain": "CS.STANFORD.EDU;ORABONA.COM",
        "email": "CS.STANFORD.EDU;ORABONA.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Stony Brook University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.stonybrook.edu",
        "aff_unique_abbr": "Stanford;SBU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "34a2942813",
        "title": "Breaking the $1/\\sqrt{n}$ Barrier: Faster Rates for Permutation-based Models in Polynomial Time",
        "site": "https://proceedings.mlr.press/v75/mao18a.html",
        "author": "Cheng Mao; Ashwin Pananjady; Martin J. Wainwright",
        "abstract": "Many applications, including rank aggregation and crowd-labeling, can be modeled in terms of a bivariate isotonic matrix with unknown permutations acting on its rows and columns.  We consider the problem of estimating such a matrix based on noisy observations of a subset of its entries, and design and analyze a polynomial-time algorithm that improves upon the state of the art. In particular, our results imply that any such $n \\times n$ matrix can be estimated efficiently in the normalized Frobenius norm at rate $\\widetilde{\\mathcal O}(n^{-3/4})$, thus narrowing the gap between $\\widetilde{\\mathcal O}(n^{-1})$ and $\\widetilde{\\mathcal O}(n^{-1/2})$, which were hitherto the rates of the most statistically and computationally efficient methods, respectively.",
        "bibtex": "@InProceedings{pmlr-v75-mao18a,\n  title = \t {Breaking the $1/\\sqrt{n}$ Barrier: Faster Rates for Permutation-based Models in Polynomial Time},\n  author =       {Mao, Cheng and Pananjady, Ashwin and Wainwright, Martin J.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {2037--2042},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/mao18a/mao18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/mao18a.html},\n  abstract = \t {Many applications, including rank aggregation and crowd-labeling, can be modeled in terms of a bivariate isotonic matrix with unknown permutations acting on its rows and columns.  We consider the problem of estimating such a matrix based on noisy observations of a subset of its entries, and design and analyze a polynomial-time algorithm that improves upon the state of the art. In particular, our results imply that any such $n \\times n$ matrix can be estimated efficiently in the normalized Frobenius norm at rate $\\widetilde{\\mathcal O}(n^{-3/4})$, thus narrowing the gap between $\\widetilde{\\mathcal O}(n^{-1})$ and $\\widetilde{\\mathcal O}(n^{-1/2})$, which were hitherto the rates of the most statistically and computationally efficient methods, respectively.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/mao18a/mao18a.pdf",
        "supp": "",
        "pdf_size": 300374,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10301135750720066749&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Massachusetts Institute of Technology, Cambridge, MA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA",
        "aff_domain": "MIT.EDU;BERKELEY.EDU;BERKELEY.EDU",
        "email": "MIT.EDU;BERKELEY.EDU;BERKELEY.EDU",
        "github": "",
        "project": "arXiv:1802.09963",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mit.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "MIT;UC Berkeley",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Cambridge;Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "59e98aa1b3",
        "title": "Calibrating Noise to Variance in Adaptive Data Analysis",
        "site": "https://proceedings.mlr.press/v75/feldman18a.html",
        "author": "Vitaly Feldman; Thomas Steinke",
        "abstract": "Datasets are often used multiple times and each successive analysis may depend on the outcome of previous analyses. Standard techniques for ensuring generalization and statistical validity do not account for this adaptive dependence. A recent line of work studies the challenges that arise from such adaptive data reuse by considering the problem of answering a sequence of \u201cqueries\u201d about the data distribution where each query may depend arbitrarily on answers to previous queries. The strongest results obtained for this problem rely on differential privacy \u2013 a strong notion of algorithmic stability with the important property that it \u201ccomposes\u201d well when data is reused. However the notion is rather strict, as it requires stability under replacement of an arbitrary data element. The simplest algorithm is to add Gaussian (or Laplace) noise to distort the empirical answers. However, analysing this technique using differential privacy yields suboptimal accuracy guarantees when the queries have low variance. Here we propose a relaxed notion of stability based on KL divergence that also composes adaptively. We show that our notion of stability implies a bound on the mutual information between the dataset and the output of the algorithm and then derive new generalization guarantees implied by bounded mutual information. We demonstrate that a simple and natural algorithm based on adding noise scaled to the standard deviation of the query provides our notion of stability. This implies an algorithm that can answer statistical queries about the dataset with substantially improved accuracy guarantees for low-variance queries. The only previous approach that provides such accuracy guarantees is based on a more involved differentially private median-of-means algorithm and its analysis exploits stronger \u201cgroup\u201d stability of the algorithm.",
        "bibtex": "@InProceedings{pmlr-v75-feldman18a,\n  title = \t {Calibrating Noise to Variance in Adaptive Data Analysis},\n  author =       {Feldman, Vitaly and Steinke, Thomas},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {535--544},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/feldman18a/feldman18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/feldman18a.html},\n  abstract = \t { Datasets are often used multiple times and each successive analysis may depend on the outcome of previous analyses. Standard techniques for ensuring generalization and statistical validity do not account for this adaptive dependence. A recent line of work studies the challenges that arise from such adaptive data reuse by considering the problem of answering a sequence of \u201cqueries\u201d about the data distribution where each query may depend arbitrarily on answers to previous queries. The strongest results obtained for this problem rely on differential privacy \u2013 a strong notion of algorithmic stability with the important property that it \u201ccomposes\u201d well when data is reused. However the notion is rather strict, as it requires stability under replacement of an arbitrary data element. The simplest algorithm is to add Gaussian (or Laplace) noise to distort the empirical answers. However, analysing this technique using differential privacy yields suboptimal accuracy guarantees when the queries have low variance. Here we propose a relaxed notion of stability based on KL divergence that also composes adaptively. We show that our notion of stability implies a bound on the mutual information between the dataset and the output of the algorithm and then derive new generalization guarantees implied by bounded mutual information. We demonstrate that a simple and natural algorithm based on adding noise scaled to the standard deviation of the query provides our notion of stability. This implies an algorithm that can answer statistical queries about the dataset with substantially improved accuracy guarantees for low-variance queries. The only previous approach that provides such accuracy guarantees is based on a more involved differentially private median-of-means algorithm and its analysis exploits stronger \u201cgroup\u201d stability of the algorithm. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/feldman18a/feldman18a.pdf",
        "supp": "",
        "pdf_size": 362772,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16498418083909954159&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Google Brain; IBM Research \u2013 Almaden",
        "aff_domain": "post.harvard.edu;thomas-steinke.net",
        "email": "post.harvard.edu;thomas-steinke.net",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;IBM",
        "aff_unique_dep": "Google Brain;IBM Research",
        "aff_unique_url": "https://brain.google.com;https://www.ibm.com/research",
        "aff_unique_abbr": "Google Brain;IBM",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Mountain View;Almaden",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d86e9e4ef2",
        "title": "Certified Computation from Unreliable Datasets",
        "site": "https://proceedings.mlr.press/v75/gouleakis18a.html",
        "author": "Themis Gouleakis; Christos Tzamos; Manolis Zampetakis",
        "abstract": "A wide range of learning tasks require human input in labeling massive data. The collected data though are usually low quality and contain inaccuracies and errors. As a result, modern science and business face the problem of learning from unreliable data sets. In this work, we provide a generic approach that is based on \\textit{verification} of only few records of the data set to guarantee high quality learning outcomes for various optimization objectives. Our method, identifies small sets of critical records and verifies their validity. We show that many problems only need $\\text{poly}(1/\\varepsilon)$ verifications, to ensure that the output of the computation is at most a factor of $(1 \\pm \\varepsilon)$ away from the truth. For any given instance, we provide an \\textit{instance optimal} solution that verifies the minimum possible number of records to approximately certify correctness. Then using this instance optimal formulation of the problem we prove our main result: \u201cevery function that satisfies some Lipschitz continuity condition can be certified with a small number of verifications\u201d. We show that the required Lipschitz continuity condition is satisfied even by some NP-complete problems, which illustrates the generality and importance of this theorem. In case this certification step fails, an invalid record will be identified. Removing these records and repeating until success, guarantees that the result will be accurate and will depend only on the verified records. Surprisingly, as we show, for several computation tasks more efficient methods are possible. These methods always guarantee that the produced result is not affected by the invalid records, since any invalid record that affects the output will be detected and verified.",
        "bibtex": "@InProceedings{pmlr-v75-gouleakis18a,\n  title = \t {Certified Computation from Unreliable Datasets},\n  author =       {Gouleakis, Themis and Tzamos, Christos and Zampetakis, Manolis},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3271--3294},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/gouleakis18a/gouleakis18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/gouleakis18a.html},\n  abstract = \t {A wide range of learning tasks require human input in labeling massive data. The collected data though are usually low quality and contain inaccuracies and errors. As a result, modern science and business face the problem of learning from unreliable data sets. In this work, we provide a generic approach that is based on \\textit{verification} of only few records of the data set to guarantee high quality learning outcomes for various optimization objectives. Our method, identifies small sets of critical records and verifies their validity. We show that many problems only need $\\text{poly}(1/\\varepsilon)$ verifications, to ensure that the output of the computation is at most a factor of $(1 \\pm \\varepsilon)$ away from the truth. For any given instance, we provide an \\textit{instance optimal} solution that verifies the minimum possible number of records to approximately certify correctness. Then using this instance optimal formulation of the problem we prove our main result: \u201cevery function that satisfies some Lipschitz continuity condition can be certified with a small number of verifications\u201d. We show that the required Lipschitz continuity condition is satisfied even by some NP-complete problems, which illustrates the generality and importance of this theorem. In case this certification step fails, an invalid record will be identified. Removing these records and repeating until success, guarantees that the result will be accurate and will depend only on the verified records. Surprisingly, as we show, for several computation tasks more efficient methods are possible. These methods always guarantee that the produced result is not affected by the invalid records, since any invalid record that affects the output will be detected and verified.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/gouleakis18a/gouleakis18a.pdf",
        "supp": "",
        "pdf_size": 403341,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15698911763873168485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "EECS and CSAIL, MIT; Microsoft Research; EECS and CSAIL, MIT",
        "aff_domain": "MIT.EDU;MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Microsoft",
        "aff_unique_dep": "Electrical Engineering & Computer Science and Computer Science and Artificial Intelligence Laboratory;Microsoft Research",
        "aff_unique_url": "https://www.mit.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MIT;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e23fa89ce4",
        "title": "Conference on Learning Theory 2018: Preface",
        "site": "https://proceedings.mlr.press/v75/bubeck18a.html",
        "author": "S\u00e9bastien Bubeck; Philippe Rigollet",
        "abstract": "Preface to the proceedings of the 31st Conference On Learning Theory.",
        "bibtex": "@InProceedings{pmlr-v75-bubeck18a,\n  title = \t {Conference on Learning Theory 2018: Preface},\n  author =       {Bubeck, S\\'ebastien and Rigollet, Philippe},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1--1},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/bubeck18a/bubeck18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/bubeck18a.html},\n  abstract = \t {Preface to the proceedings of the 31st Conference On Learning Theory.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/bubeck18a/bubeck18a.pdf",
        "supp": "",
        "pdf_size": 49926,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:MrcEQBwHeq0J:scholar.google.com/&scioq=Conference+on+Learning+Theory+2018:+Preface&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Microsoft Research; Massachusetts Institute of Technology",
        "aff_domain": "MICROSOFT.COM;MIT.EDU",
        "email": "MICROSOFT.COM;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;Massachusetts Institute of Technology",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://web.mit.edu",
        "aff_unique_abbr": "MSR;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "23dd7da746",
        "title": "Convex Optimization with Unbounded Nonconvex Oracles using Simulated Annealing",
        "site": "https://proceedings.mlr.press/v75/mangoubi18a.html",
        "author": "Oren Mangoubi; Nisheeth K. Vishnoi",
        "abstract": "We consider the problem of minimizing a convex objective function $F$ when one can only evaluate its noisy approximation $\\hat{F}$. Unless one assumes some structure on the noise, $\\hat{F}$ may be an arbitrary nonconvex function, making the task of minimizing $F$ intractable. To overcome this, prior work has often focused on the case when $F(x)-\\hat{F}(x)$ is uniformly-bounded. In this paper we study the more general case when the noise has magnitude $\\alpha F(x) + \\beta$ for some $\\alpha, \\beta > 0$, and present a polynomial time algorithm that finds an approximate minimizer of $F$ for this noise model. Previously, Markov chains, such as the stochastic gradient Langevin dynamics, have been used to arrive at approximate solutions to these optimization problems. However, for the noise model considered in this paper, no single temperature allows such a Markov chain to both mix quickly and concentrate near the global minimizer. We bypass this by combining \u201csimulated annealing\" with the stochastic gradient Langevin dynamics, and gradually decreasing the temperature of the chain in order to approach the global minimizer. As a corollary one can approximately minimize a nonconvex function that is close to a convex function; however, the closeness can deteriorate as one moves away from the optimum.",
        "bibtex": "@InProceedings{pmlr-v75-mangoubi18a,\n  title = \t {Convex Optimization with Unbounded Nonconvex Oracles using Simulated Annealing},\n  author =       {Mangoubi, Oren and Vishnoi, Nisheeth K.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1086--1124},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/mangoubi18a/mangoubi18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/mangoubi18a.html},\n  abstract = \t {We consider the problem of minimizing a convex objective function $F$ when one can only evaluate its noisy approximation $\\hat{F}$. Unless one assumes some structure on the noise, $\\hat{F}$ may be an arbitrary nonconvex function, making the task of minimizing $F$ intractable. To overcome this, prior work has often focused on the case when $F(x)-\\hat{F}(x)$ is uniformly-bounded. In this paper we study the more general case when the noise has magnitude $\\alpha F(x) + \\beta$ for some $\\alpha, \\beta > 0$, and present a polynomial time algorithm that finds an approximate minimizer of $F$ for this noise model. Previously, Markov chains, such as the stochastic gradient Langevin dynamics, have been used to arrive at approximate solutions to these optimization problems. However, for the noise model considered in this paper, no single temperature allows such a Markov chain to both mix quickly and concentrate near the global minimizer. We bypass this by combining \u201csimulated annealing\" with the stochastic gradient Langevin dynamics, and gradually decreasing the temperature of the chain in order to approach the global minimizer. As a corollary one can approximately minimize a nonconvex function that is close to a convex function; however, the closeness can deteriorate as one moves away from the optimum.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/mangoubi18a/mangoubi18a.pdf",
        "supp": "",
        "pdf_size": 638903,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2337223188823889058&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "\u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL), Switzerland; \u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL), Switzerland",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "194ac53552",
        "title": "Counting Motifs with Graph Sampling",
        "site": "https://proceedings.mlr.press/v75/klusowski18a.html",
        "author": "Jason M. Klusowski; Yihong Wu",
        "abstract": "Applied researchers often construct a network from data that has been collected from a random sample of nodes, with the goal to infer properties of the parent network from the sampled version. Two of the most widely used sampling schemes are",
        "bibtex": "@InProceedings{pmlr-v75-klusowski18a,\n  title = \t {Counting Motifs with Graph Sampling},\n  author =       {Klusowski, Jason M. and Wu, Yihong},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1966--2011},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/klusowski18a/klusowski18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/klusowski18a.html},\n  abstract = \t {Applied researchers often construct a network from data that has been collected from a random sample of nodes, with the goal to infer properties of the parent network from the sampled version. Two of the most widely used sampling schemes are",
        "pdf": "http://proceedings.mlr.press/v75/klusowski18a/klusowski18a.pdf",
        "supp": "",
        "pdf_size": 1400572,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1781503747341705599&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics and Data Science, Yale University; Department of Statistics and Data Science, Yale University",
        "aff_domain": "yale.edu;yale.edu",
        "email": "yale.edu;yale.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "Department of Statistics and Data Science",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0d1fd83890",
        "title": "Cutting plane methods can be extended into nonconvex optimization",
        "site": "https://proceedings.mlr.press/v75/hinder18a.html",
        "author": "Oliver Hinder",
        "abstract": "We show that it is possible to obtain an $O(\\epsilon^{-4/3})$ runtime \u2014 including computational cost \u2014 for finding $\\epsilon$-stationary points of nonconvex functions using cutting plane methods. This improves on the best known epsilon dependence achieved by cubic regularized Newton of $O(\\epsilon^{-3/2})$ as proved by Nesterov and Polyak (2006). Our techniques utilize the convex until proven guilty principle proposed by Carmon, Duchi, Hinder, and Sidford (2017).",
        "bibtex": "@InProceedings{pmlr-v75-hinder18a,\n  title = \t {Cutting plane methods can be extended into nonconvex optimization},\n  author =       {Hinder, Oliver},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1451--1454},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/hinder18a/hinder18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/hinder18a.html},\n  abstract = \t {We show that it is possible to obtain an $O(\\epsilon^{-4/3})$ runtime \u2014 including computational cost \u2014 for finding $\\epsilon$-stationary points of nonconvex functions using cutting plane methods. This improves on the best known epsilon dependence achieved by cubic regularized Newton of $O(\\epsilon^{-3/2})$ as proved by Nesterov and Polyak (2006). Our techniques utilize the convex until proven guilty principle proposed by Carmon, Duchi, Hinder, and Sidford (2017). }\n}",
        "pdf": "http://proceedings.mlr.press/v75/hinder18a/hinder18a.pdf",
        "supp": "",
        "pdf_size": 149636,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10331977095192966032&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Management Science and Engineering Department, Stanford",
        "aff_domain": "STANFORD.EDU",
        "email": "STANFORD.EDU",
        "github": "",
        "project": "arXiv:1805.08370",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Management Science and Engineering Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3d5539fcd1",
        "title": "Detecting Correlations with Little Memory and Communication",
        "site": "https://proceedings.mlr.press/v75/dagan18a.html",
        "author": "Yuval Dagan; Ohad Shamir",
        "abstract": "We study the problem of identifying correlations in multivariate data, under information constraints: Either on the amount of memory that can be used by the algorithm, or the amount of communication when the data is distributed across several machines. We prove a tight trade-off between the memory/communication complexity and the sample complexity, implying (for example) that to detect pairwise correlations with optimal sample complexity, the number of required memory/communication bits is at least quadratic in the dimension. Our results substantially improve those of Shamir (2014), which studied a similar question in a much more restricted setting. To the best of our knowledge, these are the first provable sample/memory/communication trade-offs for a practical estimation problem, using standard distributions, and in the natural regime where the memory/communication budget is larger than the size of a single data point. To derive our theorems, we prove a new information-theoretic result, which may be relevant for studying other information-constrained learning problems.",
        "bibtex": "@InProceedings{pmlr-v75-dagan18a,\n  title = \t {Detecting Correlations with Little Memory and Communication},\n  author =       {Dagan, Yuval and Shamir, Ohad},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1145--1198},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/dagan18a/dagan18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/dagan18a.html},\n  abstract = \t {We study the problem of identifying correlations in multivariate data, under information constraints: Either on the amount of memory that can be used by the algorithm, or the amount of communication when the data is distributed across several machines. We prove a tight trade-off between the memory/communication complexity and the sample complexity, implying (for example) that to detect pairwise correlations with optimal sample complexity, the number of required memory/communication bits is at least quadratic in the dimension. Our results substantially improve those of Shamir (2014), which studied a similar question in a much more restricted setting. To the best of our knowledge, these are the first provable sample/memory/communication trade-offs for a practical estimation problem, using standard distributions, and in the natural regime where the memory/communication budget is larger than the size of a single data point. To derive our theorems, we prove a new information-theoretic result, which may be relevant for studying other information-constrained learning problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/dagan18a/dagan18a.pdf",
        "supp": "",
        "pdf_size": 646541,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6795314489292924397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Weizmann Institute of Science; Weizmann Institute of Science",
        "aff_domain": "weizmann.ac.il;weizmann.ac.il",
        "email": "weizmann.ac.il;weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Weizmann Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.weizmann.org.il",
        "aff_unique_abbr": "Weizmann",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "6da906dbc5",
        "title": "Detection limits in the high-dimensional spiked rectangular model",
        "site": "https://proceedings.mlr.press/v75/el-alaoui18a.html",
        "author": "Ahmed El Alaoui; Michael I. Jordan",
        "abstract": "We study the problem of detecting the presence of a single unknown spike in a rectangular data matrix, in a high-dimensional regime where the spike has fixed strength and the aspect ratio of the matrix converges to a finite limit. This setup includes Johnstone\u2019s spiked covariance model. We analyze the likelihood ratio of the spiked model against an \u201call noise\" null model of reference, and show it has asymptotically Gaussian fluctuations in a region below\u2014but in general not up to\u2014the so-called BBP threshold from random matrix theory. Our result parallels earlier findings of Onatski et al. (2013) and Johnstone-Onatski (2015) for spherical spikes. We present a probabilistic approach capable of treating generic product priors. In particular, sparsity in the spike is allowed. Our approach operates through the principle of the cavity method from spin-glass theory.  The question of the maximal parameter region where asymptotic normality is expected to hold is left open. This region, not necessarily given by BBP, is shaped by the prior in a non-trivial way. We conjecture that this is the entire paramagnetic phase of an associated spin-glass model, and is defined by the vanishing of the replica-symmetric solution of Lesieur et al. (2015).",
        "bibtex": "@InProceedings{pmlr-v75-el-alaoui18a,\n  title = \t {Detection limits in the high-dimensional spiked rectangular model},\n  author =       {{El Alaoui}, Ahmed and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {410--438},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/el-alaoui18a/el-alaoui18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/el-alaoui18a.html},\n  abstract = \t {We study the problem of detecting the presence of a single unknown spike in a rectangular data matrix, in a high-dimensional regime where the spike has fixed strength and the aspect ratio of the matrix converges to a finite limit. This setup includes Johnstone\u2019s spiked covariance model. We analyze the likelihood ratio of the spiked model against an \u201call noise\" null model of reference, and show it has asymptotically Gaussian fluctuations in a region below\u2014but in general not up to\u2014the so-called BBP threshold from random matrix theory. Our result parallels earlier findings of Onatski et al. (2013) and Johnstone-Onatski (2015) for spherical spikes. We present a probabilistic approach capable of treating generic product priors. In particular, sparsity in the spike is allowed. Our approach operates through the principle of the cavity method from spin-glass theory.  The question of the maximal parameter region where asymptotic normality is expected to hold is left open. This region, not necessarily given by BBP, is shaped by the prior in a non-trivial way. We conjecture that this is the entire paramagnetic phase of an associated spin-glass model, and is defined by the vanishing of the replica-symmetric solution of Lesieur et al. (2015). }\n}",
        "pdf": "http://proceedings.mlr.press/v75/el-alaoui18a/el-alaoui18a.pdf",
        "supp": "",
        "pdf_size": 394183,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13033646138109305889&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Electrical Engineering & Computer Sciences, UC Berkeley; Electrical Engineering & Computer Sciences+Statistics, UC Berkeley",
        "aff_domain": "BERKELEY.EDU;BERKELEY.EDU",
        "email": "BERKELEY.EDU;BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Electrical Engineering & Computer Sciences",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0+0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9fdf36b528",
        "title": "Efficient Algorithms for Outlier-Robust Regression",
        "site": "https://proceedings.mlr.press/v75/klivans18a.html",
        "author": "Adam Klivans; Pravesh K. Kothari; Raghu Meka",
        "abstract": "We give the first polynomial-time algorithm for performing linear or polynomial regression resilient to adversarial corruptions in both examples and labels. Given a sufficiently large (polynomial-size) training set drawn i.i.d. from distribution ${\\mathcal{D}}$ and subsequently corrupted on some fraction of points, our algorithm outputs a linear function whose squared error is close to the squared error of the best-fitting linear function with respect to ${\\mathcal{D}}$, assuming that the marginal distribution of $\\mathcal{D}$ over the input space is \\emph{certifiably hypercontractive}. This natural property is satisfied by many well-studied distributions such as Gaussian, strongly log-concave distributions and, uniform distribution on the hypercube among others.  We also give a simple statistical lower bound showing that some distributional assumption is necessary to succeed in this setting.  These results are the first of their kind and were not known to be even information-theoretically possible prior to our work.   Our approach is based on the sum-of-squares (SoS) method and is inspired by the recent applications of the method for parameter recovery problems in unsupervised learning. Our algorithm can be seen as a natural convex relaxation of the following conceptually simple non-convex optimization problem: find a linear function and a large subset of the input corrupted sample such that the least squares loss of the function over the subset is minimized over all possible large subsets.",
        "bibtex": "@InProceedings{pmlr-v75-klivans18a,\n  title = \t {Efficient Algorithms for Outlier-Robust Regression},\n  author =       {Klivans, Adam and Kothari, Pravesh K. and Meka, Raghu},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1420--1430},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/klivans18a/klivans18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/klivans18a.html},\n  abstract = \t { We give the first polynomial-time algorithm for performing linear or polynomial regression resilient to adversarial corruptions in both examples and labels. Given a sufficiently large (polynomial-size) training set drawn i.i.d. from distribution ${\\mathcal{D}}$ and subsequently corrupted on some fraction of points, our algorithm outputs a linear function whose squared error is close to the squared error of the best-fitting linear function with respect to ${\\mathcal{D}}$, assuming that the marginal distribution of $\\mathcal{D}$ over the input space is \\emph{certifiably hypercontractive}. This natural property is satisfied by many well-studied distributions such as Gaussian, strongly log-concave distributions and, uniform distribution on the hypercube among others.  We also give a simple statistical lower bound showing that some distributional assumption is necessary to succeed in this setting.  These results are the first of their kind and were not known to be even information-theoretically possible prior to our work.   Our approach is based on the sum-of-squares (SoS) method and is inspired by the recent applications of the method for parameter recovery problems in unsupervised learning. Our algorithm can be seen as a natural convex relaxation of the following conceptually simple non-convex optimization problem: find a linear function and a large subset of the input corrupted sample such that the least squares loss of the function over the subset is minimized over all possible large subsets. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/klivans18a/klivans18a.pdf",
        "supp": "",
        "pdf_size": 230583,
        "gs_citation": 196,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14989934004338223962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Texas at Austin; Princeton University and IAS; University of California, Los Angeles",
        "aff_domain": "/c.sc/s.sc./u.sc/t.sc/e.sc/x.sc/a.sc/s.sc./e.sc/d.sc/u.sc;/c.sc/s.sc./p.sc/r.sc/i.sc/n.sc/c.sc/e.sc/t.sc/o.sc/n.sc./e.sc/d.sc/u.sc;/c.sc/s.sc./u.sc/c.sc/l.sc/a.sc./e.sc/d.sc/u.sc",
        "email": "/c.sc/s.sc./u.sc/t.sc/e.sc/x.sc/a.sc/s.sc./e.sc/d.sc/u.sc;/c.sc/s.sc./p.sc/r.sc/i.sc/n.sc/c.sc/e.sc/t.sc/o.sc/n.sc./e.sc/d.sc/u.sc;/c.sc/s.sc./u.sc/c.sc/l.sc/a.sc./e.sc/d.sc/u.sc",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Texas at Austin;Princeton University;University of California, Los Angeles",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utexas.edu;https://www.princeton.edu;https://www.ucla.edu",
        "aff_unique_abbr": "UT Austin;Princeton;UCLA",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Austin;;Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cab572043b",
        "title": "Efficient Contextual Bandits in Non-stationary Worlds",
        "site": "https://proceedings.mlr.press/v75/luo18a.html",
        "author": "Haipeng Luo; Chen-Yu Wei; Alekh Agarwal; John Langford",
        "abstract": "Most contextual bandit algorithms minimize regret against the best fixed policy, a questionable benchmark for non-stationary environments that are ubiquitous in applications.  In this work, we develop several efficient contextual bandit algorithms for non-stationary environments by equipping existing methods for i.i.d. problems with sophisticated statistical tests so as to dynamically adapt to a change in distribution.   We analyze various standard notions of regret suited to non-stationary environments for these algorithms, including interval regret, switching regret, and dynamic regret. When competing with the best policy at each time, one of our algorithms achieves regret $\\mathcal{O}(\\sqrt{ST})$ if there are $T$ rounds with $S$ stationary periods, or more generally $\\mathcal{O}(\\Delta^{1/3}T^{2/3})$ where $\\Delta$ is some non-stationarity measure. These results almost match the optimal guarantees achieved by an inefficient baseline that is a variant of the classic Exp4 algorithm. The dynamic regret result is also the first one for efficient and fully adversarial contextual bandit. Furthermore, while the results above require tuning a parameter based on the unknown quantity $S$ or $\\Delta$, we also develop a parameter free algorithm achieving regret $\\min\\{S^{1/4}T^{3/4}, \\Delta^{1/5}T^{4/5}\\}$. This improves and generalizes the best existing result $\\Delta^{0.18}T^{0.82}$ by Karnin and Anava (2016) which only holds for the two-armed bandit problem.",
        "bibtex": "@InProceedings{pmlr-v75-luo18a,\n  title = \t {Efficient Contextual Bandits in Non-stationary Worlds},\n  author =       {Luo, Haipeng and Wei, Chen-Yu and Agarwal, Alekh and Langford, John},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1739--1776},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/luo18a/luo18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/luo18a.html},\n  abstract = \t {Most contextual bandit algorithms minimize regret against the best fixed policy, a questionable benchmark for non-stationary environments that are ubiquitous in applications.  In this work, we develop several efficient contextual bandit algorithms for non-stationary environments by equipping existing methods for i.i.d. problems with sophisticated statistical tests so as to dynamically adapt to a change in distribution.   We analyze various standard notions of regret suited to non-stationary environments for these algorithms, including interval regret, switching regret, and dynamic regret. When competing with the best policy at each time, one of our algorithms achieves regret $\\mathcal{O}(\\sqrt{ST})$ if there are $T$ rounds with $S$ stationary periods, or more generally $\\mathcal{O}(\\Delta^{1/3}T^{2/3})$ where $\\Delta$ is some non-stationarity measure. These results almost match the optimal guarantees achieved by an inefficient baseline that is a variant of the classic Exp4 algorithm. The dynamic regret result is also the first one for efficient and fully adversarial contextual bandit. Furthermore, while the results above require tuning a parameter based on the unknown quantity $S$ or $\\Delta$, we also develop a parameter free algorithm achieving regret $\\min\\{S^{1/4}T^{3/4}, \\Delta^{1/5}T^{4/5}\\}$. This improves and generalizes the best existing result $\\Delta^{0.18}T^{0.82}$ by Karnin and Anava (2016) which only holds for the two-armed bandit problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/luo18a/luo18a.pdf",
        "supp": "",
        "pdf_size": 552392,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10430294198596277193&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Southern California; University of Southern California; Microsoft Research, NYC; Microsoft Research, NYC",
        "aff_domain": "USC.EDU;USC.EDU;MICROSOFT.COM;MICROSOFT.COM",
        "email": "USC.EDU;USC.EDU;MICROSOFT.COM;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "University of Southern California;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.usc.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "USC;MSR",
        "aff_campus_unique_index": "0;0;1;1",
        "aff_campus_unique": "Los Angeles;New York City",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "07473725c3",
        "title": "Efficient Convex Optimization with Membership Oracles",
        "site": "https://proceedings.mlr.press/v75/lee18a.html",
        "author": "Yin Tat Lee; Aaron Sidford; Santosh S. Vempala",
        "abstract": "We consider the problem of minimizing a convex function over a convex set given access only to an evaluation oracle for the function and a membership oracle for the set. We give a simple algorithm which solves this problem with $\\tilde{O}(n^{2})$ oracle calls and $\\tilde{O}(n^{3})$ additional arithmetic operations. Using this result, we obtain more efficient reductions among the five basic oracles for convex sets and functions defined by Gr{\u00f6}tschel, Lov{\u00e1}sz, and Schrijver (1988).",
        "bibtex": "@InProceedings{pmlr-v75-lee18a,\n  title = \t {Efficient Convex Optimization with Membership Oracles},\n  author =       {Lee, Yin Tat and Sidford, Aaron and Vempala, Santosh S.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1292--1294},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/lee18a/lee18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/lee18a.html},\n  abstract = \t { We consider the problem of minimizing a convex function over a convex set given access only to an evaluation oracle for the function and a membership oracle for the set. We give a simple algorithm which solves this problem with $\\tilde{O}(n^{2})$ oracle calls and $\\tilde{O}(n^{3})$ additional arithmetic operations. Using this result, we obtain more efficient reductions among the five basic oracles for convex sets and functions defined by Gr{\u00f6}tschel, Lov{\u00e1}sz, and Schrijver (1988). }\n}",
        "pdf": "http://proceedings.mlr.press/v75/lee18a/lee18a.pdf",
        "supp": "",
        "pdf_size": 162564,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2619461621645405303&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Washington; Stanford University; Georgia Tech",
        "aff_domain": "UW.EDU;STANFORD.EDU;GATECH.EDU",
        "email": "UW.EDU;STANFORD.EDU;GATECH.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Washington;Stanford University;Georgia Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.washington.edu;https://www.stanford.edu;https://www.gatech.edu",
        "aff_unique_abbr": "UW;Stanford;Georgia Tech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "93f37271d8",
        "title": "Efficient active learning of sparse halfspaces",
        "site": "https://proceedings.mlr.press/v75/zhang18b.html",
        "author": "Chicheng Zhang",
        "abstract": "We study the problem of efficient PAC active learning of homogeneous linear classifiers (halfspaces) in $\\mathbb{R}^d$, where the goal is to learn a halfspace with low error using as few label queries as possible. Under the extra assumption that there is a $t$-sparse halfspace that performs well on the data ($t \\ll d$), we would like our active learning algorithm to be {\\em attribute efficient}, i.e. to have label requirements sublinear in $d$. In this paper, we provide a computationally efficient algorithm that achieves this goal. Under certain distributional assumptions on the data, our algorithm achieves a label complexity of $O(t \\cdot \\mathrm{polylog}(d, \\frac 1 \\epsilon))$. In contrast, existing algorithms in this setting are either computationally inefficient, or subject to label requirements polynomial in $d$ or $\\frac 1 \\epsilon$.",
        "bibtex": "@InProceedings{pmlr-v75-zhang18b,\n  title = \t {Efficient active learning of sparse halfspaces},\n  author =       {Zhang, Chicheng},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1856--1880},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/zhang18b/zhang18b.pdf},\n  url = \t {https://proceedings.mlr.press/v75/zhang18b.html},\n  abstract = \t {We study the problem of efficient PAC active learning of homogeneous linear classifiers (halfspaces) in $\\mathbb{R}^d$, where the goal is to learn a halfspace with low error using as few label queries as possible. Under the extra assumption that there is a $t$-sparse halfspace that performs well on the data ($t \\ll d$), we would like our active learning algorithm to be {\\em attribute efficient}, i.e. to have label requirements sublinear in $d$. In this paper, we provide a computationally efficient algorithm that achieves this goal. Under certain distributional assumptions on the data, our algorithm achieves a label complexity of $O(t \\cdot \\mathrm{polylog}(d, \\frac 1 \\epsilon))$. In contrast, existing algorithms in this setting are either computationally inefficient, or subject to label requirements polynomial in $d$ or $\\frac 1 \\epsilon$.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/zhang18b/zhang18b.pdf",
        "supp": "",
        "pdf_size": 343243,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6930841728792654225&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Research",
        "aff_domain": "MICROSOFT.COM",
        "email": "MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "615d378468",
        "title": "Empirical bounds for functions with weak interactions",
        "site": "https://proceedings.mlr.press/v75/maurer18a.html",
        "author": "Andreas Maurer; Massimiliano Pontil",
        "abstract": "We provide sharp empirical estimates of expectation, variance and normal approximation for a class of statistics whose variation in any argument does not change too much when another argument is modified. Examples of such weak interactions are furnished by U- and V-statistics, Lipschitz L-statistics and various error functionals of $\\ell_2$-regularized algorithms and Gibbs algorithms.",
        "bibtex": "@InProceedings{pmlr-v75-maurer18a,\n  title = \t {Empirical bounds for functions with weak interactions},\n  author =       {Maurer, Andreas and Pontil, Massimiliano},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {987--1010},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/maurer18a/maurer18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/maurer18a.html},\n  abstract = \t {We provide sharp empirical estimates of expectation, variance and normal approximation for a class of statistics whose variation in any argument does not change too much when another argument is modified. Examples of such weak interactions are furnished by U- and V-statistics, Lipschitz L-statistics and various error functionals of $\\ell_2$-regularized algorithms and Gibbs algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/maurer18a/maurer18a.pdf",
        "supp": "",
        "pdf_size": 358504,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3951574128530112679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Adalbertstrasse 55, D-80799 Mnchen, Germany; Istituto Italiano di Tecnologia, 16163 Genoa, Italy + University College London, London WC1E 6BT, UK",
        "aff_domain": "ANDREAS-MAURER.EU;IIT.IT",
        "email": "ANDREAS-MAURER.EU;IIT.IT",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Technical University of Munich;Istituto Italiano di Tecnologia;University College London",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tum.de;https://www.iit.it;https://www.ucl.ac.uk",
        "aff_unique_abbr": "TUM;IIT;UCL",
        "aff_campus_unique_index": "0;1+2",
        "aff_campus_unique": "Munich;Genoa;London",
        "aff_country_unique_index": "0;1+2",
        "aff_country_unique": "Germany;Italy;United Kingdom"
    },
    {
        "id": "36cf658681",
        "title": "Exact and Robust Conformal Inference Methods for Predictive Machine Learning with Dependent Data",
        "site": "https://proceedings.mlr.press/v75/chernozhukov18a.html",
        "author": "Victor Chernozhukov; Kaspar W\u00fcthrich; Zhu Yinchu",
        "abstract": "We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and  accounts for potential serial dependence by including  block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods.  When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score.",
        "bibtex": "@InProceedings{pmlr-v75-chernozhukov18a,\n  title = \t {Exact and Robust Conformal Inference Methods for Predictive Machine Learning with Dependent Data},\n  author =       {Chernozhukov, Victor and W\\\"{u}thrich, Kaspar and Yinchu, Zhu},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {732--749},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/chernozhukov18a/chernozhukov18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/chernozhukov18a.html},\n  abstract = \t {We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and  accounts for potential serial dependence by including  block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods.  When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/chernozhukov18a/chernozhukov18a.pdf",
        "supp": "",
        "pdf_size": 253230,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17533275616030548833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Massachusetts Institute of Technology; University of California, San Diego; University of Oregon",
        "aff_domain": "MIT.EDU;UCSD.EDU;UOREGON.EDU",
        "email": "MIT.EDU;UCSD.EDU;UOREGON.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of California, San Diego;University of Oregon",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://web.mit.edu;https://www.ucsd.edu;https://www.uoregon.edu",
        "aff_unique_abbr": "MIT;UCSD;UO",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cc83704547",
        "title": "Exponential Convergence of Testing Error for Stochastic Gradient Methods",
        "site": "https://proceedings.mlr.press/v75/pillaud-vivien18a.html",
        "author": "Loucas Pillaud-Vivien; Alessandro Rudi; Francis Bach",
        "abstract": "We consider binary classification problems with positive definite kernels and square loss, and study the convergence rates of stochastic gradient methods. We show that while the excess testing \\emph{loss} (squared loss) converges slowly to zero as the number of observations (and thus iterations) goes to infinity, the testing \\emph{error} (classification error) converges exponentially fast if low-noise conditions are assumed. To achieve these rates of convergence we show sharper high-probability bounds with respect to the number of observations for stochastic gradient descent.",
        "bibtex": "@InProceedings{pmlr-v75-pillaud-vivien18a,\n  title = \t {Exponential Convergence of Testing Error for Stochastic Gradient Methods},\n  author =       {Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {250--296},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/pillaud-vivien18a/pillaud-vivien18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/pillaud-vivien18a.html},\n  abstract = \t {We consider binary classification problems with positive definite kernels and square loss, and study the convergence rates of stochastic gradient methods. We show that while the excess testing \\emph{loss} (squared loss) converges slowly to zero as the number of observations (and thus iterations) goes to infinity, the testing \\emph{error} (classification error) converges exponentially fast if low-noise conditions are assumed. To achieve these rates of convergence we show sharper high-probability bounds with respect to the number of observations for stochastic gradient descent.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/pillaud-vivien18a/pillaud-vivien18a.pdf",
        "supp": "",
        "pdf_size": 753417,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17046635038832780364&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "INRIA - D\u00b4epartement d\u2019Informatique de l\u2019ENS; INRIA - D\u00b4epartement d\u2019Informatique de l\u2019ENS; INRIA - D\u00b4epartement d\u2019Informatique de l\u2019ENS",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "D 'epartement d\u2019Informatique de l\u2019ENS",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "aabf64c129",
        "title": "Fast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms",
        "site": "https://proceedings.mlr.press/v75/diakonikolas18a.html",
        "author": "Ilias Diakonikolas; Jerry Li; Ludwig Schmidt",
        "abstract": "We study the problem of robustly learning multi-dimensional histograms.  A $d$-dimensional function $h: D \\to \\R$ is called a $k$-histogram if there exists a partition of the  domain $D \\subseteq \\R^d$ into $k$ axis-aligned rectangles such that $h$ is constant within each such rectangle. Let $f: D \\to \\R$ be a $d$-dimensional probability density function  and suppose that $f$ is $\\mathrm{OPT}$-close, in $L_1$-distance,  to an unknown $k$-histogram (with unknown partition). Our goal is to output a hypothesis that is $O(\\mathrm{OPT}) + \\epsilon$ close to $f$, in $L_1$-distance. We give an algorithm for this learning  problem that uses  $n = \\tilde{O}_d(k/\\eps^2)$ samples and runs in time $\\tilde{O}_d(n)$. For any fixed dimension, our algorithm has optimal sample complexity, up to logarithmic factors, and runs in near-linear time. Prior to our work, the time complexity of the $d=1$ case was well-understood,  but significant gaps in our understanding remained even for $d=2$.",
        "bibtex": "@InProceedings{pmlr-v75-diakonikolas18a,\n  title = \t {Fast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms},\n  author =       {Diakonikolas, Ilias and Li, Jerry and Schmidt, Ludwig},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {819--842},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/diakonikolas18a/diakonikolas18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/diakonikolas18a.html},\n  abstract = \t {We study the problem of robustly learning multi-dimensional histograms.  A $d$-dimensional function $h: D \\to \\R$ is called a $k$-histogram if there exists a partition of the  domain $D \\subseteq \\R^d$ into $k$ axis-aligned rectangles such that $h$ is constant within each such rectangle. Let $f: D \\to \\R$ be a $d$-dimensional probability density function  and suppose that $f$ is $\\mathrm{OPT}$-close, in $L_1$-distance,  to an unknown $k$-histogram (with unknown partition). Our goal is to output a hypothesis that is $O(\\mathrm{OPT}) + \\epsilon$ close to $f$, in $L_1$-distance. We give an algorithm for this learning  problem that uses  $n = \\tilde{O}_d(k/\\eps^2)$ samples and runs in time $\\tilde{O}_d(n)$. For any fixed dimension, our algorithm has optimal sample complexity, up to logarithmic factors, and runs in near-linear time. Prior to our work, the time complexity of the $d=1$ case was well-understood,  but significant gaps in our understanding remained even for $d=2$.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/diakonikolas18a/diakonikolas18a.pdf",
        "supp": "",
        "pdf_size": 337085,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12897301041312994613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "USC; MIT; MIT",
        "aff_domain": "USC.EDU;MIT.EDU;MIT.EDU",
        "email": "USC.EDU;MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Southern California;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://web.mit.edu",
        "aff_unique_abbr": "USC;MIT",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5637855282",
        "title": "Faster Rates for Convex-Concave Games",
        "site": "https://proceedings.mlr.press/v75/abernethy18a.html",
        "author": "Jacob Abernethy; Kevin A. Lai; Kfir Y. Levy; Jun-Kun Wang",
        "abstract": "We consider the use of no-regret algorithms to compute equilibria for particular classes of convex-concave games. While standard regret bounds would lead to convergence rates on the order of $O(T^{-1/2})$, recent work \\citep{RS13,SALS15} has established $O(1/T)$ rates by taking advantage of a particular class of optimistic prediction algorithms. In this work we go further, showing that for a particular class of games one achieves a $O(1/T^2)$ rate, and we show how this applies to the Frank-Wolfe method and recovers a similar bound \\citep{D15}. We also show that such no-regret techniques can even achieve a linear rate, $O(\\exp(-T))$, for equilibrium computation under additional curvature assumptions.",
        "bibtex": "@InProceedings{pmlr-v75-abernethy18a,\n  title = \t {Faster Rates for Convex-Concave Games},\n  author =       {Abernethy, Jacob and Lai, Kevin A. and Levy, Kfir Y. and Wang, Jun-Kun},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1595--1625},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/abernethy18a/abernethy18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/abernethy18a.html},\n  abstract = \t {We consider the use of no-regret algorithms to compute equilibria for particular classes of convex-concave games. While standard regret bounds would lead to convergence rates on the order of $O(T^{-1/2})$, recent work \\citep{RS13,SALS15} has established $O(1/T)$ rates by taking advantage of a particular class of optimistic prediction algorithms. In this work we go further, showing that for a particular class of games one achieves a $O(1/T^2)$ rate, and we show how this applies to the Frank-Wolfe method and recovers a similar bound \\citep{D15}. We also show that such no-regret techniques can even achieve a linear rate, $O(\\exp(-T))$, for equilibrium computation under additional curvature assumptions. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/abernethy18a/abernethy18a.pdf",
        "supp": "",
        "pdf_size": 424655,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4240743768314311364&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Georgia Tech; Georgia Tech; ETH Zurich; Georgia Tech",
        "aff_domain": " gatech.edu; gatech.edu; inf.ethz.ch; gatech.edu",
        "email": " gatech.edu; gatech.edu; inf.ethz.ch; gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.ethz.ch",
        "aff_unique_abbr": "Georgia Tech;ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "6ef51c7246",
        "title": "Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v75/dalal18a.html",
        "author": "Gal Dalal; Gugan Thoppe; Bal\u00e1zs Sz\u00f6r\u00e9nyi; Shie Mannor",
        "abstract": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). Their iterates have two parts that are updated using distinct stepsizes. In this work, we develop a novel recipe for their finite sample analysis. Using this, we provide a concentration bound, which is the first such result for a two-timescale SA. The type of bound we obtain is known as \u201clock-in probability\u201d. We also introduce a new projection scheme, in which the time between successive projections increases exponentially. This scheme allows one to elegantly transform a lock-in probability into a convergence rate result for projected two-timescale SA. From this latter result, we then extract key insights on stepsize selection.  As an application, we finally obtain convergence rates for the projected two-timescale RL algorithms GTD(0), GTD2, and TDC.",
        "bibtex": "@InProceedings{pmlr-v75-dalal18a,\n  title = \t {Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning},\n  author =       {Dalal, Gal and Thoppe, Gugan and Sz{\\\"o}r{\\'e}nyi, Bal{\\'a}zs and Mannor, Shie},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1199--1233},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/dalal18a.html},\n  abstract = \t {Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). Their iterates have two parts that are updated using distinct stepsizes. In this work, we develop a novel recipe for their finite sample analysis. Using this, we provide a concentration bound, which is the first such result for a two-timescale SA. The type of bound we obtain is known as \u201clock-in probability\u201d. We also introduce a new projection scheme, in which the time between successive projections increases exponentially. This scheme allows one to elegantly transform a lock-in probability into a convergence rate result for projected two-timescale SA. From this latter result, we then extract key insights on stepsize selection.  As an application, we finally obtain convergence rates for the projected two-timescale RL algorithms GTD(0), GTD2, and TDC.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf",
        "supp": "",
        "pdf_size": 452008,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=576439098208382327&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Technion, Israel; Yahoo Research, NYC; Duke University, USA; Technion, Israel",
        "aff_domain": "CAMPUS.TECHNION.AC.IL;GMAIL.COM;GMAIL.COM;EE.TECHNION.AC.IL",
        "email": "CAMPUS.TECHNION.AC.IL;GMAIL.COM;GMAIL.COM;EE.TECHNION.AC.IL",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Yahoo Research;Duke University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://research.yahoo.com;https://www.duke.edu",
        "aff_unique_abbr": "Technion;Yahoo Res.;Duke",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York City",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "ccc947dbe8",
        "title": "Fitting a Putative Manifold to Noisy Data",
        "site": "https://proceedings.mlr.press/v75/fefferman18a.html",
        "author": "Charles Fefferman; Sergei Ivanov; Yaroslav Kurylev; Matti Lassas; Hariharan Narayanan",
        "abstract": "In the present work,  we give a solution to the following  question from manifold learning.  Suppose data belonging to a high dimensional Euclidean space is drawn independently, identically distributed  from a measure supported on a low dimensional twice  differentiable embedded manifold $M$, and corrupted by a small amount of  gaussian noise. How can we produce a manifold $M\u2019$ whose Hausdorff distance to $M$ is small and whose reach is not much smaller than the reach of $M$?",
        "bibtex": "@InProceedings{pmlr-v75-fefferman18a,\n  title = \t {Fitting a Putative Manifold to Noisy Data},\n  author =       {Fefferman, Charles and Ivanov, Sergei and Kurylev, Yaroslav and Lassas, Matti and Narayanan, Hariharan},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {688--720},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/fefferman18a/fefferman18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/fefferman18a.html},\n  abstract = \t {In the present work,  we give a solution to the following  question from manifold learning.  Suppose data belonging to a high dimensional Euclidean space is drawn independently, identically distributed  from a measure supported on a low dimensional twice  differentiable embedded manifold $M$, and corrupted by a small amount of  gaussian noise. How can we produce a manifold $M\u2019$ whose Hausdorff distance to $M$ is small and whose reach is not much smaller than the reach of $M$?}\n}",
        "pdf": "http://proceedings.mlr.press/v75/fefferman18a/fefferman18a.pdf",
        "supp": "",
        "pdf_size": 445669,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1167781781307021149&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Princeton University, Mathematics Department; Steklov Institute of Mathematics, Russian Academy of Sciences; University College London, Department of Mathematics; University of Helsinki, Department of Mathematics and Statistics; School of Technology and Computer Science, Tata Institute of Fundamental Research",
        "aff_domain": "MATH.PRINCETON.EDU;PDMI.RAS.RU;UCL.AC.UK;HELSINKI.FI;TIFR.RES.IN",
        "email": "MATH.PRINCETON.EDU;PDMI.RAS.RU;UCL.AC.UK;HELSINKI.FI;TIFR.RES.IN",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "Princeton University;Steklov Institute of Mathematics;University College London;University of Helsinki;Tata Institute of Fundamental Research",
        "aff_unique_dep": "Mathematics Department;Russian Academy of Sciences;Department of Mathematics;Department of Mathematics and Statistics;School of Technology and Computer Science",
        "aff_unique_url": "https://www.princeton.edu;http://www.mathnet.ru/php/index.php?dispatch=rating.item&id=1;https://www.ucl.ac.uk;https://www.helsinki.fi;https://www.tifr.res.in",
        "aff_unique_abbr": "Princeton;SIM RAS;UCL;UH;TIFR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;3;4",
        "aff_country_unique": "United States;Russian Federation;United Kingdom;Finland;India"
    },
    {
        "id": "2896acc81f",
        "title": "Fundamental Limits of Weak Recovery with Applications to Phase Retrieval",
        "site": "https://proceedings.mlr.press/v75/mondelli18a.html",
        "author": "Marco Mondelli; Andrea Montanari",
        "abstract": "In phase retrieval we want to recover an unknown signal $\\boldsymbol x\\in\\mathbb C^d$ from $n$ quadratic measurements of the form  $y_i = |\u27e8\\boldsymbol a_i,\\boldsymbol x\u27e9|^2+w_i$ where $\\boldsymbol a_i\\in \\mathbb C^d$ are known sensing vectors and  $w_i$ is measurement noise. We ask the following \\emph{weak recovery} question: what is the minimum number of measurements $n$ needed to produce an estimator $\\hat{\\boldsymbol x}(\\boldsymbol y)$ that is positively correlated with  the signal $\\boldsymbol x$? We consider the case of Gaussian  vectors $\\boldsymbol a_i$. We prove that \u2013 in the high-dimensional limit \u2013 a sharp phase transition takes place, and we locate the threshold in the regime of vanishingly small noise. For $n\\le d-o(d)$ no estimator can do significantly better than random and achieve a strictly positive correlation. For $n\\ge d+o(d)$ a simple spectral estimator achieves a positive correlation.  Surprisingly, numerical simulations with the same spectral estimator  demonstrate promising performance with realistic sensing matrices.  Spectral methods are used to initialize non-convex optimization algorithms in phase retrieval, and our approach can boost the performance in this setting as well. Our impossibility result is based on classical information-theory arguments. The spectral algorithm computes the leading eigenvector of a weighted empirical covariance matrix. We obtain a sharp characterization of the spectral properties of this random matrix  using tools from free probability and generalizing a recent result by Lu and Li.  Both the upper and lower bound generalize beyond phase retrieval to measurements $y_i$ produced according to a generalized linear model.  As a byproduct of our analysis, we compare the threshold of the proposed spectral method with that of a message passing algorithm.",
        "bibtex": "@InProceedings{pmlr-v75-mondelli18a,\n  title = \t {Fundamental Limits of Weak Recovery with Applications to Phase Retrieval},\n  author =       {Mondelli, Marco and Montanari, Andrea},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1445--1450},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/mondelli18a/mondelli18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/mondelli18a.html},\n  abstract = \t {In phase retrieval we want to recover an unknown signal $\\boldsymbol x\\in\\mathbb C^d$ from $n$ quadratic measurements of the form  $y_i = |\u27e8\\boldsymbol a_i,\\boldsymbol x\u27e9|^2+w_i$ where $\\boldsymbol a_i\\in \\mathbb C^d$ are known sensing vectors and  $w_i$ is measurement noise. We ask the following \\emph{weak recovery} question: what is the minimum number of measurements $n$ needed to produce an estimator $\\hat{\\boldsymbol x}(\\boldsymbol y)$ that is positively correlated with  the signal $\\boldsymbol x$? We consider the case of Gaussian  vectors $\\boldsymbol a_i$. We prove that \u2013 in the high-dimensional limit \u2013 a sharp phase transition takes place, and we locate the threshold in the regime of vanishingly small noise. For $n\\le d-o(d)$ no estimator can do significantly better than random and achieve a strictly positive correlation. For $n\\ge d+o(d)$ a simple spectral estimator achieves a positive correlation.  Surprisingly, numerical simulations with the same spectral estimator  demonstrate promising performance with realistic sensing matrices.  Spectral methods are used to initialize non-convex optimization algorithms in phase retrieval, and our approach can boost the performance in this setting as well. Our impossibility result is based on classical information-theory arguments. The spectral algorithm computes the leading eigenvector of a weighted empirical covariance matrix. We obtain a sharp characterization of the spectral properties of this random matrix  using tools from free probability and generalizing a recent result by Lu and Li.  Both the upper and lower bound generalize beyond phase retrieval to measurements $y_i$ produced according to a generalized linear model.  As a byproduct of our analysis, we compare the threshold of the proposed spectral method with that of a message passing algorithm.  }\n}",
        "pdf": "http://proceedings.mlr.press/v75/mondelli18a/mondelli18a.pdf",
        "supp": "",
        "pdf_size": 195062,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12909318189143035016&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical Engineering, Stanford University; Department of Electrical Engineering and Department of Statistics, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e3ef7242f2",
        "title": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints",
        "site": "https://proceedings.mlr.press/v75/mou18a.html",
        "author": "Wenlong Mou; Liwei Wang; Xiyu Zhai; Kai Zheng",
        "abstract": "We study the generalization errors of \\emph{non-convex} regularized ERM procedures using Stochastic Gradient Langevin Dynamics (SGLD). Two theories are proposed with non-asymptotic discrete-time analysis, using stability and PAC-Bayesian theory respectively. The stability-based theory obtains a bound of $O\\left(\\frac{1}{n}L\\sqrt{\\beta T_N}\\right)$, where $L$ is Lipschitz parameter, $\\beta$ is inverse temperature, and $T_N$ is the sum of step sizes. For PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate, the contribution of each step decays exponentially through time, and the uniform Lipschitz constant is also replaced by actual norms of gradients along the optimization trajectory. Our bounds have reasonable dependence on aggregated step sizes, and do not explicitly depend on dimensions, norms or other capacity measures of the parameter. The bounds characterize how the noises in the algorithm itself controls the statistical learning behavior in non-convex problems, without uniform convergence in the hypothesis space, which sheds light on the effect of training algorithms on the generalization error for deep neural networks.",
        "bibtex": "@InProceedings{pmlr-v75-mou18a,\n  title = \t {Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints},\n  author =       {Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {605--638},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/mou18a/mou18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/mou18a.html},\n  abstract = \t {We study the generalization errors of \\emph{non-convex} regularized ERM procedures using Stochastic Gradient Langevin Dynamics (SGLD). Two theories are proposed with non-asymptotic discrete-time analysis, using stability and PAC-Bayesian theory respectively. The stability-based theory obtains a bound of $O\\left(\\frac{1}{n}L\\sqrt{\\beta T_N}\\right)$, where $L$ is Lipschitz parameter, $\\beta$ is inverse temperature, and $T_N$ is the sum of step sizes. For PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate, the contribution of each step decays exponentially through time, and the uniform Lipschitz constant is also replaced by actual norms of gradients along the optimization trajectory. Our bounds have reasonable dependence on aggregated step sizes, and do not explicitly depend on dimensions, norms or other capacity measures of the parameter. The bounds characterize how the noises in the algorithm itself controls the statistical learning behavior in non-convex problems, without uniform convergence in the hypothesis space, which sheds light on the effect of training algorithms on the generalization error for deep neural networks.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/mou18a/mou18a.pdf",
        "supp": "",
        "pdf_size": 772259,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1496248537429500862&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of EECS, University of California, Berkeley; Key Laboratory of Machine Perception, MOE, School of EECS, Peking University+Center for Data Science, Peking University, Beijing Institute of Big Data Research; University of Cambridge; Key Laboratory of Machine Perception, MOE, School of EECS, Peking University",
        "aff_domain": "EECS.BERKELEY.EDU;CIS.PKU.EDU.CN;CAM.AC.UK;PKU.EDU.CN",
        "email": "EECS.BERKELEY.EDU;CIS.PKU.EDU.CN;CAM.AC.UK;PKU.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+1;2;1",
        "aff_unique_norm": "University of California, Berkeley;Peking University;University of Cambridge",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences;School of EECS;",
        "aff_unique_url": "https://www.berkeley.edu;http://www.pku.edu.cn;https://www.cam.ac.uk",
        "aff_unique_abbr": "UC Berkeley;Peking U;Cambridge",
        "aff_campus_unique_index": "0;2;3",
        "aff_campus_unique": "Berkeley;;Beijing;Cambridge",
        "aff_country_unique_index": "0;1+1;2;1",
        "aff_country_unique": "United States;China;United Kingdom"
    },
    {
        "id": "a94e25cb73",
        "title": "Geometric Lower Bounds for Distributed Parameter Estimation under Communication Constraints",
        "site": "https://proceedings.mlr.press/v75/han18a.html",
        "author": "Yanjun Han; Ayfer \u00d6zg\u00fcr; Tsachy Weissman",
        "abstract": "We consider parameter estimation in distributed networks, where each sensor in the network observes an independent sample from an underlying distribution and has $k$ bits to communicate its sample to a centralized processor which computes an estimate of a desired parameter. We develop lower bounds for the minimax risk of estimating the underlying parameter under squared $\\ell_2$ loss for a large class of distributions. Our results show that under mild regularity conditions, the communication constraint reduces the effective sample size by a factor of $d$ when $k$ is small, where $d$ is the dimension of the estimated parameter. Furthermore, this penalty reduces at most exponentially with increasing $k$, which is the case for some models, e.g., estimating high-dimensional distributions. For other models however, we show that the sample size reduction is re-mediated only linearly with increasing $k$, e.g. when some sub-Gaussian structure is available. We apply our results to the distributed setting with product Bernoulli model, multinomial model, and dense/sparse Gaussian location models which recover or strengthen existing results. Our approach significantly deviates from existing approaches for developing information-theoretic lower bounds for communication-efficient estimation. We circumvent the need for strong data processing inequalities used in prior work and develop a geometric approach which builds on a new representation of the communication constraint. This approach allows us to strengthen and generalize existing results with simpler and more transparent proofs.",
        "bibtex": "@InProceedings{pmlr-v75-han18a,\n  title = \t {Geometric Lower Bounds for Distributed Parameter Estimation under Communication Constraints},\n  author =       {Han, Yanjun and \\\"{O}zg\\\"{u}r, Ayfer and Weissman, Tsachy},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3163--3188},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/han18a/han18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/han18a.html},\n  abstract = \t {We consider parameter estimation in distributed networks, where each sensor in the network observes an independent sample from an underlying distribution and has $k$ bits to communicate its sample to a centralized processor which computes an estimate of a desired parameter. We develop lower bounds for the minimax risk of estimating the underlying parameter under squared $\\ell_2$ loss for a large class of distributions. Our results show that under mild regularity conditions, the communication constraint reduces the effective sample size by a factor of $d$ when $k$ is small, where $d$ is the dimension of the estimated parameter. Furthermore, this penalty reduces at most exponentially with increasing $k$, which is the case for some models, e.g., estimating high-dimensional distributions. For other models however, we show that the sample size reduction is re-mediated only linearly with increasing $k$, e.g. when some sub-Gaussian structure is available. We apply our results to the distributed setting with product Bernoulli model, multinomial model, and dense/sparse Gaussian location models which recover or strengthen existing results. Our approach significantly deviates from existing approaches for developing information-theoretic lower bounds for communication-efficient estimation. We circumvent the need for strong data processing inequalities used in prior work and develop a geometric approach which builds on a new representation of the communication constraint. This approach allows us to strengthen and generalize existing results with simpler and more transparent proofs.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/han18a/han18a.pdf",
        "supp": "",
        "pdf_size": 357523,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=86375404544427578&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Electrical Engineering, Stanford University; Department of Electrical Engineering, Stanford University; Department of Electrical Engineering, Stanford University",
        "aff_domain": "STANFORD.EDU;STANFORD.EDU;STANFORD.EDU",
        "email": "STANFORD.EDU;STANFORD.EDU;STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "45dc448cc0",
        "title": "Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk",
        "site": "https://proceedings.mlr.press/v75/hand18a.html",
        "author": "Paul Hand; Vladislav Voroninski",
        "abstract": "We examine the theoretical properties of enforcing priors provided by generative deep neural networks via empirical risk minimization. In particular we consider two models, one in which the task is to invert a generative neural network given access to its last layer and another in which the task is to invert a generative neural network given only compressive linear observations of its last layer.  We establish that in both cases, in suitable regimes of network layer sizes and a randomness assumption on the network weights, that the non-convex objective function given by empirical risk minimization does not have any spurious stationary points. That is, we establish that with high probability, at any point away from small neighborhoods around two scalar multiples of the desired solution, there is a descent direction. Hence, there are no local minima, saddle points, or other stationary points outside these neighborhoods.  These results constitute the first theoretical guarantees which establish the favorable global geometry of these non-convex optimization problems, and they bridge the gap between the empirical success of  enforcing deep generative priors and a rigorous understanding of non-linear inverse problems.",
        "bibtex": "@InProceedings{pmlr-v75-hand18a,\n  title = \t {Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk},\n  author =       {Hand, Paul and Voroninski, Vladislav},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {970--978},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/hand18a/hand18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/hand18a.html},\n  abstract = \t {We examine the theoretical properties of enforcing priors provided by generative deep neural networks via empirical risk minimization. In particular we consider two models, one in which the task is to invert a generative neural network given access to its last layer and another in which the task is to invert a generative neural network given only compressive linear observations of its last layer.  We establish that in both cases, in suitable regimes of network layer sizes and a randomness assumption on the network weights, that the non-convex objective function given by empirical risk minimization does not have any spurious stationary points. That is, we establish that with high probability, at any point away from small neighborhoods around two scalar multiples of the desired solution, there is a descent direction. Hence, there are no local minima, saddle points, or other stationary points outside these neighborhoods.  These results constitute the first theoretical guarantees which establish the favorable global geometry of these non-convex optimization problems, and they bridge the gap between the empirical success of  enforcing deep generative priors and a rigorous understanding of non-linear inverse problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/hand18a/hand18a.pdf",
        "supp": "",
        "pdf_size": 206195,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13543035107030106805&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Rice University, Department of Computational and Applied Mathematics; Helm.ai",
        "aff_domain": "RICE.EDU;HELM.AI",
        "email": "RICE.EDU;HELM.AI",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Rice University;Helm.ai",
        "aff_unique_dep": "Department of Computational and Applied Mathematics;",
        "aff_unique_url": "https://www.rice.edu;https://www.helm.ai",
        "aff_unique_abbr": "Rice;Helm.ai",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ca702cce3c",
        "title": "Hardness of Learning Noisy Halfspaces using Polynomial Thresholds",
        "site": "https://proceedings.mlr.press/v75/bhattacharyya18a.html",
        "author": "Arnab Bhattacharyya; Suprovat Ghoshal; Rishi Saket",
        "abstract": "We prove the hardness of weakly learning halfspaces in the presence of adversarial noise using polynomial threshold functions (PTFs). In particular, we prove that for any constants $d \\in \\mathbb{Z}^+$ and $\\eps > 0$, it is NP-hard to decide: given a set of $\\{-1,1\\}$-labeled points in $\\mathbb{R}^n$  whether (YES Case) there exists a halfspace that classifies $(1-\\eps)$-fraction of the points correctly, or (NO Case) any degree-$d$ PTF classifies at most $(1/2 + \\eps)$-fraction of the points correctly. This strengthens to all constant degrees the previous NP-hardness of learning using degree-$2$ PTFs shown by Diakonikolas et al. (2011). The latter result had remained the only progress over the works of Feldman et al. (2006) and Guruswami et al. (2006) ruling out weakly proper learning adversarially noisy halfspaces.",
        "bibtex": "@InProceedings{pmlr-v75-bhattacharyya18a,\n  title = \t {Hardness of Learning Noisy Halfspaces using Polynomial Thresholds},\n  author =       {Bhattacharyya, Arnab and Ghoshal, Suprovat and Saket, Rishi},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {876--917},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/bhattacharyya18a/bhattacharyya18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/bhattacharyya18a.html},\n  abstract = \t {We prove the hardness of weakly learning halfspaces in the presence of adversarial noise using polynomial threshold functions (PTFs). In particular, we prove that for any constants $d \\in \\mathbb{Z}^+$ and $\\eps > 0$, it is NP-hard to decide: given a set of $\\{-1,1\\}$-labeled points in $\\mathbb{R}^n$  whether (YES Case) there exists a halfspace that classifies $(1-\\eps)$-fraction of the points correctly, or (NO Case) any degree-$d$ PTF classifies at most $(1/2 + \\eps)$-fraction of the points correctly. This strengthens to all constant degrees the previous NP-hardness of learning using degree-$2$ PTFs shown by Diakonikolas et al. (2011). The latter result had remained the only progress over the works of Feldman et al. (2006) and Guruswami et al. (2006) ruling out weakly proper learning adversarially noisy halfspaces.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/bhattacharyya18a/bhattacharyya18a.pdf",
        "supp": "",
        "pdf_size": 646579,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10676045612627265643&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India; IBM Research, Bangalore, India",
        "aff_domain": "iisc.ac.in;iisc.ac.in;in.ibm.com",
        "email": "iisc.ac.in;iisc.ac.in;in.ibm.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Indian Institute of Science;IBM",
        "aff_unique_dep": ";IBM Research",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.ibm.com/research",
        "aff_unique_abbr": "IISc;IBM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "6d05711eee",
        "title": "Hidden Integrality of SDP Relaxations for Sub-Gaussian Mixture Models",
        "site": "https://proceedings.mlr.press/v75/fei18a.html",
        "author": "Yingjie Fei; Yudong Chen",
        "abstract": "We consider the problem of finding discrete clustering structures under Sub-Gaussian Mixture Models. We establish a hidden integrality property of a semidefinite programming (SDP) relaxation for this problem: while the optimal solutions to the SDP are not integer-valued in general, their estimation errors can be upper bounded by the error of an idealized integer program. The error of the integer program, and hence that of the SDP, are further shown to decay exponentially in the signal-to-noise ratio. To the best of our knowledge, this is the first exponentially decaying error bound for convex relaxations of mixture models. A special case of this result shows that in certain regimes the SDP solutions are in fact integral and exact, improving on existing exact recovery results for convex relaxations. More generally, our result establishes sufficient conditions for the SDP to correctly recover the cluster memberships of at least $(1-\\delta)$ fraction of the points for any $\\delta\\in(0,1)$. Error bounds for estimating cluster centers can also be derived directly from our results.",
        "bibtex": "@InProceedings{pmlr-v75-fei18a,\n  title = \t {Hidden Integrality of SDP Relaxations for Sub-Gaussian Mixture Models},\n  author =       {Fei, Yingjie and Chen, Yudong},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1931--1965},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/fei18a/fei18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/fei18a.html},\n  abstract = \t {We consider the problem of finding discrete clustering structures under Sub-Gaussian Mixture Models. We establish a hidden integrality property of a semidefinite programming (SDP) relaxation for this problem: while the optimal solutions to the SDP are not integer-valued in general, their estimation errors can be upper bounded by the error of an idealized integer program. The error of the integer program, and hence that of the SDP, are further shown to decay exponentially in the signal-to-noise ratio. To the best of our knowledge, this is the first exponentially decaying error bound for convex relaxations of mixture models. A special case of this result shows that in certain regimes the SDP solutions are in fact integral and exact, improving on existing exact recovery results for convex relaxations. More generally, our result establishes sufficient conditions for the SDP to correctly recover the cluster memberships of at least $(1-\\delta)$ fraction of the points for any $\\delta\\in(0,1)$. Error bounds for estimating cluster centers can also be derived directly from our results.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/fei18a/fei18a.pdf",
        "supp": "",
        "pdf_size": 478388,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4174398508868981458&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Cornell University; Cornell University",
        "aff_domain": "CORNELL.EDU;CORNELL.EDU",
        "email": "CORNELL.EDU;CORNELL.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b4f10f90c5",
        "title": "Incentivizing Exploration by Heterogeneous Users",
        "site": "https://proceedings.mlr.press/v75/chen18a.html",
        "author": "Bangrui Chen; Peter Frazier; David Kempe",
        "abstract": "We consider the problem of incentivizing exploration with heterogeneous agents. In this problem, $N$ bandit arms provide vector-valued outcomes equal to an unknown arm-specific attribute vector, perturbed by independent noise.Agents arrive sequentially and choose arms to pull based on their own private and heterogeneous linear utility functions over attributes and the estimates of the arms\u2019 attribute vectors derived from observations of other agents\u2019 past pulls. Agents are myopic and selfish and thus would choose the arm with maximum estimated utility. A principal, knowing only the distribution from which agents\u2019 preferences are drawn, but not the specific draws, can offer arm-specific incentive payments to encourage agents to explore underplayed arms. The principal seeks to minimize the total expected cumulative regret incurred by agents relative to their best arms, while also making a small expected cumulative payment. We propose an algorithm that incentivizes arms played infrequently in the past whose probability of being played in the next round would be small without incentives. Under the assumption that each arm is preferred by at least a fraction $p > 0$ of agents, we show that this algorithm achieves expected cumulative regret of $O (N \\e^{2/p} + N \\log^3(T))$, using expected cumulative payments of $O(N^2 \\e^{2/p})$. If $p$ is known or the distribution over agent preferences is discrete, the exponential term $\\e^{2/p}$ can be replaced with suitable polynomials in $N$ and $1/p$. For discrete preferences, the regret\u2019s dependence on $T$ can be eliminated entirely, giving constant (depending only polynomially on $N$ and $1/p$) expected regret and payments. This constant regret stands in contrast to the $\\Theta(\\log(T))$ dependence of regret in standard multi-armed bandit problems. It arises because even unobserved heterogeneity in agent preferences causes exploitation of arms to also explore arms fully; succinctly, heterogeneity provides free exploration.",
        "bibtex": "@InProceedings{pmlr-v75-chen18a,\n  title = \t {Incentivizing Exploration by Heterogeneous Users},\n  author =       {Chen, Bangrui and Frazier, Peter and Kempe, David},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {798--818},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/chen18a/chen18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/chen18a.html},\n  abstract = \t {We consider the problem of incentivizing exploration with heterogeneous agents. In this problem, $N$ bandit arms provide vector-valued outcomes equal to an unknown arm-specific attribute vector, perturbed by independent noise.Agents arrive sequentially and choose arms to pull based on their own private and heterogeneous linear utility functions over attributes and the estimates of the arms\u2019 attribute vectors derived from observations of other agents\u2019 past pulls. Agents are myopic and selfish and thus would choose the arm with maximum estimated utility. A principal, knowing only the distribution from which agents\u2019 preferences are drawn, but not the specific draws, can offer arm-specific incentive payments to encourage agents to explore underplayed arms. The principal seeks to minimize the total expected cumulative regret incurred by agents relative to their best arms, while also making a small expected cumulative payment. We propose an algorithm that incentivizes arms played infrequently in the past whose probability of being played in the next round would be small without incentives. Under the assumption that each arm is preferred by at least a fraction $p > 0$ of agents, we show that this algorithm achieves expected cumulative regret of $O (N \\e^{2/p} + N \\log^3(T))$, using expected cumulative payments of $O(N^2 \\e^{2/p})$. If $p$ is known or the distribution over agent preferences is discrete, the exponential term $\\e^{2/p}$ can be replaced with suitable polynomials in $N$ and $1/p$. For discrete preferences, the regret\u2019s dependence on $T$ can be eliminated entirely, giving constant (depending only polynomially on $N$ and $1/p$) expected regret and payments. This constant regret stands in contrast to the $\\Theta(\\log(T))$ dependence of regret in standard multi-armed bandit problems. It arises because even unobserved heterogeneity in agent preferences causes exploitation of arms to also explore arms fully; succinctly, heterogeneity provides free exploration. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/chen18a/chen18a.pdf",
        "supp": "",
        "pdf_size": 338271,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7880947005798099009&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Operations Research and Information Engineering, Cornell University; Operations Research and Information Engineering, Cornell University; Department of Computer Science, University of Southern California",
        "aff_domain": "CORNELL.EDU;CORNELL.EDU;GMAIL.COM",
        "email": "CORNELL.EDU;CORNELL.EDU;GMAIL.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Cornell University;University of Southern California",
        "aff_unique_dep": "Operations Research and Information Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu;https://www.usc.edu",
        "aff_unique_abbr": "Cornell;USC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2320683308",
        "title": "Information Directed Sampling and Bandits with Heteroscedastic Noise",
        "site": "https://proceedings.mlr.press/v75/kirschner18a.html",
        "author": "Johannes Kirschner; Andreas Krause",
        "abstract": "In the stochastic bandit problem, the goal is to maximize an unknown function via a sequence of noisy evaluations. Typically, the observation noise is assumed to be independent of the evaluation point and to satisfy a tail bound uniformly on the domain; a restrictive assumption for many applications. In this work, we consider bandits with heteroscedastic noise, where we explicitly allow the noise distribution to depend on the evaluation point. We show that this leads to new trade-offs for information and regret, which are not taken into account by existing approaches like upper confidence bound algorithms (UCB) or Thompson Sampling. To address these shortcomings, we introduce a frequentist regret analysis framework, that is similar to the Bayesian framework of Russo and Van Roy (2014), and we prove a new high-probability regret bound for general, possibly randomized policies, which depends on a quantity we refer to as regret-information ratio. From this bound, we define a frequentist version of Information Directed Sampling (IDS) to minimize the regret-information ratio over all possible action sampling distributions. This further relies on concentration inequalities for online least squares regression in separable Hilbert spaces, which we generalize to the case of heteroscedastic noise. We then formulate several variants of IDS for linear and reproducing kernel Hilbert space response functions, yielding novel algorithms for Bayesian optimization. We also prove frequentist regret bounds, which in the homoscedastic case recover known bounds for UCB, but can be much better when the noise is heteroscedastic. Empirically, we demonstrate in a linear setting with heteroscedastic noise, that some of our methods can outperform UCB and Thompson Sampling, while staying competitive when the noise is homoscedastic.",
        "bibtex": "@InProceedings{pmlr-v75-kirschner18a,\n  title = \t {Information Directed Sampling and Bandits with Heteroscedastic Noise},\n  author =       {Kirschner, Johannes and Krause, Andreas},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {358--384},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/kirschner18a/kirschner18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/kirschner18a.html},\n  abstract = \t {In the stochastic bandit problem, the goal is to maximize an unknown function via a sequence of noisy evaluations. Typically, the observation noise is assumed to be independent of the evaluation point and to satisfy a tail bound uniformly on the domain; a restrictive assumption for many applications. In this work, we consider bandits with heteroscedastic noise, where we explicitly allow the noise distribution to depend on the evaluation point. We show that this leads to new trade-offs for information and regret, which are not taken into account by existing approaches like upper confidence bound algorithms (UCB) or Thompson Sampling. To address these shortcomings, we introduce a frequentist regret analysis framework, that is similar to the Bayesian framework of Russo and Van Roy (2014), and we prove a new high-probability regret bound for general, possibly randomized policies, which depends on a quantity we refer to as regret-information ratio. From this bound, we define a frequentist version of Information Directed Sampling (IDS) to minimize the regret-information ratio over all possible action sampling distributions. This further relies on concentration inequalities for online least squares regression in separable Hilbert spaces, which we generalize to the case of heteroscedastic noise. We then formulate several variants of IDS for linear and reproducing kernel Hilbert space response functions, yielding novel algorithms for Bayesian optimization. We also prove frequentist regret bounds, which in the homoscedastic case recover known bounds for UCB, but can be much better when the noise is heteroscedastic. Empirically, we demonstrate in a linear setting with heteroscedastic noise, that some of our methods can outperform UCB and Thompson Sampling, while staying competitive when the noise is homoscedastic.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/kirschner18a/kirschner18a.pdf",
        "supp": "",
        "pdf_size": 436108,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17076953548233503292&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science; Department of Computer Science",
        "aff_domain": "INF.ETHZ.CH;INF.ETHZ.CH",
        "email": "INF.ETHZ.CH;INF.ETHZ.CH",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Unknown Institution",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "872ade8b15",
        "title": "Iterate Averaging as Regularization for Stochastic Gradient Descent",
        "site": "https://proceedings.mlr.press/v75/neu18a.html",
        "author": "Gergely Neu; Lorenzo Rosasco",
        "abstract": "We propose and analyze a variant of the classic  Polyak\u2013Ruppert averaging scheme, broadly used in stochastic gradient methods.  Rather than a uniform average of the iterates, we consider a weighted average, with weights decaying in a geometric fashion. In the context of linear least-squares regression, we show that this averaging scheme has the same regularizing effect, and indeed is asymptotically equivalent, to ridge regression. In particular, we derive finite-sample bounds for the proposed approach that match the best known results for regularized stochastic gradient methods.",
        "bibtex": "@InProceedings{pmlr-v75-neu18a,\n  title = \t {Iterate Averaging as Regularization for Stochastic Gradient Descent},\n  author =       {Neu, Gergely and Rosasco, Lorenzo},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3222--3242},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/neu18a/neu18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/neu18a.html},\n  abstract = \t {We propose and analyze a variant of the classic  Polyak\u2013Ruppert averaging scheme, broadly used in stochastic gradient methods.  Rather than a uniform average of the iterates, we consider a weighted average, with weights decaying in a geometric fashion. In the context of linear least-squares regression, we show that this averaging scheme has the same regularizing effect, and indeed is asymptotically equivalent, to ridge regression. In particular, we derive finite-sample bounds for the proposed approach that match the best known results for regularized stochastic gradient methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/neu18a/neu18a.pdf",
        "supp": "",
        "pdf_size": 279472,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10408268689315666032&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Universitat Pompeu Fabra, Barcelona, Spain; LCSL, Massachusetts Institute of Technology, Cambridge MA, USA + Istituto Italiano di Tecnologia, Genova, Italy + Universit `a degli Studi di Genova, Genova, Italy",
        "aff_domain": "GMAIL.COM;MIT.EDU",
        "email": "GMAIL.COM;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2+3",
        "aff_unique_norm": "Universitat Pompeu Fabra;Massachusetts Institute of Technology;Istituto Italiano di Tecnologia;University of Genoa",
        "aff_unique_dep": ";LCSL;;",
        "aff_unique_url": "https://www.upf.edu/;https://web.mit.edu;https://www.iit.it;https://www.unige.it",
        "aff_unique_abbr": "UPF;MIT;IIT;UniGe",
        "aff_campus_unique_index": "0;1+2+2",
        "aff_campus_unique": "Barcelona;Cambridge;Genova",
        "aff_country_unique_index": "0;1+2+2",
        "aff_country_unique": "Spain;United States;Italy"
    },
    {
        "id": "b29b239aa3",
        "title": "Langevin Monte Carlo and JKO splitting",
        "site": "https://proceedings.mlr.press/v75/bernton18a.html",
        "author": "Espen Bernton",
        "abstract": "Algorithms based on discretizing Langevin diffusion are popular tools for sampling from high-dimensional distributions. We develop novel connections between such Monte Carlo algorithms, the theory of Wasserstein gradient flow, and the operator splitting approach to solving PDEs. In particular, we show that a proximal version of the Unadjusted Langevin Algorithm corresponds to a scheme that alternates between solving the gradient flows of two specific functionals on the space of probability measures. Using this perspective, we derive some new non-asymptotic results on the convergence properties of this algorithm.",
        "bibtex": "@InProceedings{pmlr-v75-bernton18a,\n  title = \t {{L}angevin {M}onte {C}arlo and {JKO} splitting},\n  author =       {Bernton, Espen},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1777--1798},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/bernton18a/bernton18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/bernton18a.html},\n  abstract = \t {Algorithms based on discretizing Langevin diffusion are popular tools for sampling from high-dimensional distributions. We develop novel connections between such Monte Carlo algorithms, the theory of Wasserstein gradient flow, and the operator splitting approach to solving PDEs. In particular, we show that a proximal version of the Unadjusted Langevin Algorithm corresponds to a scheme that alternates between solving the gradient flows of two specific functionals on the space of probability measures. Using this perspective, we derive some new non-asymptotic results on the convergence properties of this algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/bernton18a/bernton18a.pdf",
        "supp": "",
        "pdf_size": 280420,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5791324961364713631&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Harvard University, 1 Oxford Street, Cambridge, MA 02138, USA",
        "aff_domain": "g.harvard.edu",
        "email": "g.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fb4c15da44",
        "title": "Learning Mixtures of Linear Regressions with Nearly Optimal Complexity",
        "site": "https://proceedings.mlr.press/v75/li18b.html",
        "author": "Yuanzhi Li; Yingyu Liang",
        "abstract": "Mixtures of Linear Regressions (MLR) is an important mixture model with many applications. In this model, each observation is generated from one of the several unknown linear regression components, where the identity of the generated component is also unknown. Previous works either assume strong assumptions on the data distribution or have high complexity. This paper proposes a fixed parameter tractable algorithm for the problem under general conditions, which achieves global convergence and the sample complexity scales nearly linearly in the dimension. In particular, different from previous works that require the data to be from the standard Gaussian, the algorithm allows the data from Gaussians with different covariances. When the conditional number of the covariances and the number of components are fixed, the algorithm has nearly optimal sample complexity $N = \\tilde{O}(d)$ as well as nearly optimal computational complexity $\\tilde{O}(Nd)$, where $d$ is the dimension of the data space. To the best of our knowledge, this approach provides the first such recovery guarantee for this general setting.",
        "bibtex": "@InProceedings{pmlr-v75-li18b,\n  title = \t {Learning Mixtures of Linear Regressions with Nearly Optimal Complexity},\n  author =       {Li, Yuanzhi and Liang, Yingyu},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1125--1144},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/li18b/li18b.pdf},\n  url = \t {https://proceedings.mlr.press/v75/li18b.html},\n  abstract = \t {Mixtures of Linear Regressions (MLR) is an important mixture model with many applications. In this model, each observation is generated from one of the several unknown linear regression components, where the identity of the generated component is also unknown. Previous works either assume strong assumptions on the data distribution or have high complexity. This paper proposes a fixed parameter tractable algorithm for the problem under general conditions, which achieves global convergence and the sample complexity scales nearly linearly in the dimension. In particular, different from previous works that require the data to be from the standard Gaussian, the algorithm allows the data from Gaussians with different covariances. When the conditional number of the covariances and the number of components are fixed, the algorithm has nearly optimal sample complexity $N = \\tilde{O}(d)$ as well as nearly optimal computational complexity $\\tilde{O}(Nd)$, where $d$ is the dimension of the data space. To the best of our knowledge, this approach provides the first such recovery guarantee for this general setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/li18b/li18b.pdf",
        "supp": "",
        "pdf_size": 165115,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14277441945879209934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Princeton University, Computer Science Department; University of Wisconsin-Madison, Computer Sciences Department",
        "aff_domain": "CS.PRINCETON.EDU;CS.WISC.EDU",
        "email": "CS.PRINCETON.EDU;CS.WISC.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Princeton University;University of Wisconsin-Madison",
        "aff_unique_dep": "Computer Science Department;Computer Sciences Department",
        "aff_unique_url": "https://www.princeton.edu;https://www.wisc.edu",
        "aff_unique_abbr": "Princeton;UW-Madison",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Madison",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "391dd38cdd",
        "title": "Learning Patterns for Detection with Multiscale Scan Statistics",
        "site": "https://proceedings.mlr.press/v75/sharpnack18a.html",
        "author": "James Sharpnack",
        "abstract": "This paper addresses detecting anomalous patterns in images, time-series, and tensor data when the location and scale of the pattern and the pattern itself is unknown a priori. The multiscale scan statistic convolves the proposed pattern with the image at various scales and returns the maximum of the resulting tensor. Scale corrected multiscale scan statistics apply different standardizations at each scale, and the limiting distribution under the null hypothesis\u2014that the data is only noise\u2014is known for smooth patterns.  We consider the problem of simultaneously learning and detecting the anomalous pattern from a dictionary of smooth patterns and a database of many tensors. To this end, we show that the multiscale scan statistic is a subexponential random variable, and prove a chaining lemma for standardized suprema, which may be of independent interest. Then by averaging the statistics over the database of tensors we can learn the pattern and obtain Bernstein-type error bounds. We will also provide a construction of an $\\epsilon$-net of the location and scale parameters, providing a computationally tractable approximation with similar error bounds.",
        "bibtex": "@InProceedings{pmlr-v75-sharpnack18a,\n  title = \t {Learning Patterns for Detection with Multiscale Scan Statistics},\n  author =       {Sharpnack, James},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {950--969},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/sharpnack18a/sharpnack18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/sharpnack18a.html},\n  abstract = \t {This paper addresses detecting anomalous patterns in images, time-series, and tensor data when the location and scale of the pattern and the pattern itself is unknown a priori. The multiscale scan statistic convolves the proposed pattern with the image at various scales and returns the maximum of the resulting tensor. Scale corrected multiscale scan statistics apply different standardizations at each scale, and the limiting distribution under the null hypothesis\u2014that the data is only noise\u2014is known for smooth patterns.  We consider the problem of simultaneously learning and detecting the anomalous pattern from a dictionary of smooth patterns and a database of many tensors. To this end, we show that the multiscale scan statistic is a subexponential random variable, and prove a chaining lemma for standardized suprema, which may be of independent interest. Then by averaging the statistics over the database of tensors we can learn the pattern and obtain Bernstein-type error bounds. We will also provide a construction of an $\\epsilon$-net of the location and scale parameters, providing a computationally tractable approximation with similar error bounds.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/sharpnack18a/sharpnack18a.pdf",
        "supp": "",
        "pdf_size": 368094,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14717320482504189626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "1 Shields Ave. Davis, CA, 95616",
        "aff_domain": "UCDAVIS.EDU",
        "email": "UCDAVIS.EDU",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Davis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucdavis.edu",
        "aff_unique_abbr": "UC Davis",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Davis",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fd80fcb97c",
        "title": "Learning Single-Index Models in Gaussian Space",
        "site": "https://proceedings.mlr.press/v75/dudeja18a.html",
        "author": "Rishabh Dudeja; Daniel Hsu",
        "abstract": "We consider regression problems where the response is a smooth but non-linear function of a $k$-dimensional projection of $p$ normally-distributed covariates, contaminated with additive Gaussian noise. The goal is to recover the range of the $k$-dimensional projection, i.e., the index space. This model is called the multi-index model, and the $k=1$ case is called the single-index model. For the single-index model, we characterize the population landscape of a natural semi-parametric maximum likelihood objective in terms of the link function and prove that it has no spurious local minima. We also propose and analyze an efficient iterative procedure that recovers the index space up to error $\\epsilon$ using a sample size $\\tilde{O}(p^{O(R^2/\\mu)} + p/\\epsilon^2)$, where $R$ and $\\mu$, respectively, parameterize the smoothness of the link function and the signal strength. When a multi-index model is incorrectly specified as a single-index model, we prove that essentially the same procedure, with sample size $\\tilde{O}(p^{O(kR^2/\\mu)} + p/\\epsilon^2)$, returns a vector that is $\\epsilon$-close to being completely in the index space.",
        "bibtex": "@InProceedings{pmlr-v75-dudeja18a,\n  title = \t {Learning Single-Index Models in Gaussian Space},\n  author =       {Dudeja, Rishabh and Hsu, Daniel},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1887--1930},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/dudeja18a/dudeja18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/dudeja18a.html},\n  abstract = \t {We consider regression problems where the response is a smooth but non-linear function of a $k$-dimensional projection of $p$ normally-distributed covariates, contaminated with additive Gaussian noise. The goal is to recover the range of the $k$-dimensional projection, i.e., the index space. This model is called the multi-index model, and the $k=1$ case is called the single-index model. For the single-index model, we characterize the population landscape of a natural semi-parametric maximum likelihood objective in terms of the link function and prove that it has no spurious local minima. We also propose and analyze an efficient iterative procedure that recovers the index space up to error $\\epsilon$ using a sample size $\\tilde{O}(p^{O(R^2/\\mu)} + p/\\epsilon^2)$, where $R$ and $\\mu$, respectively, parameterize the smoothness of the link function and the signal strength. When a multi-index model is incorrectly specified as a single-index model, we prove that essentially the same procedure, with sample size $\\tilde{O}(p^{O(kR^2/\\mu)} + p/\\epsilon^2)$, returns a vector that is $\\epsilon$-close to being completely in the index space.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/dudeja18a/dudeja18a.pdf",
        "supp": "",
        "pdf_size": 487044,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7419546781261695734&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Statistics, Columbia University; Computer Science Department, Columbia University",
        "aff_domain": "columbia.edu;cs.columbia.edu",
        "email": "columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c938f13d50",
        "title": "Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification",
        "site": "https://proceedings.mlr.press/v75/simchowitz18a.html",
        "author": "Max Simchowitz; Horia Mania; Stephen Tu; Michael I. Jordan; Benjamin Recht",
        "abstract": "We prove that the ordinary least-squares (OLS) estimator attains nearly minimax optimal performance for the identification of linear dynamical systems from a single observed trajectory. Our upper bound relies on a generalization of Mendelson\u2019s small-ball method to dependent data, eschewing the use of standard mixing-time arguments. Our lower bounds reveal that these upper bounds match up to logarithmic factors. In particular, we capture the correct signal-to-noise behavior of the problem, showing that \\emph{more unstable} linear systems are \\emph{easier} to estimate. This behavior is qualitatively different from arguments which rely on mixing-time calculations that suggest that unstable systems are more difficult to estimate. We generalize our technique to provide bounds for a more general class of linear response time-series.",
        "bibtex": "@InProceedings{pmlr-v75-simchowitz18a,\n  title = \t {Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification},\n  author =       {Simchowitz, Max and Mania, Horia and Tu, Stephen and Jordan, Michael I. and Recht, Benjamin},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {439--473},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/simchowitz18a/simchowitz18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/simchowitz18a.html},\n  abstract = \t {We prove that the ordinary least-squares (OLS) estimator attains nearly minimax optimal performance for the identification of linear dynamical systems from a single observed trajectory. Our upper bound relies on a generalization of Mendelson\u2019s small-ball method to dependent data, eschewing the use of standard mixing-time arguments. Our lower bounds reveal that these upper bounds match up to logarithmic factors. In particular, we capture the correct signal-to-noise behavior of the problem, showing that \\emph{more unstable} linear systems are \\emph{easier} to estimate. This behavior is qualitatively different from arguments which rely on mixing-time calculations that suggest that unstable systems are more difficult to estimate. We generalize our technique to provide bounds for a more general class of linear response time-series. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/simchowitz18a/simchowitz18a.pdf",
        "supp": "",
        "pdf_size": 439614,
        "gs_citation": 429,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11664447453912826616&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley",
        "aff_domain": "BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU",
        "email": "BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "554f242082",
        "title": "Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability",
        "site": "https://proceedings.mlr.press/v75/tzen18a.html",
        "author": "Belinda Tzen; Tengyuan Liang; Maxim Raginsky",
        "abstract": "We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \\textit{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the $\\varepsilon$-neighborhood of this particular optimum within a short \\textit{recurrence time}; (2) it enters this $\\varepsilon$-neighborhood by the recurrence time and stays there until a potentially exponentially long \\textit{escape time}. We call this phenomenon \\textit{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality.",
        "bibtex": "@InProceedings{pmlr-v75-tzen18a,\n  title = \t {Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability},\n  author =       {Tzen, Belinda and Liang, Tengyuan and Raginsky, Maxim},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {857--875},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/tzen18a/tzen18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/tzen18a.html},\n  abstract = \t {We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \\textit{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the $\\varepsilon$-neighborhood of this particular optimum within a short \\textit{recurrence time}; (2) it enters this $\\varepsilon$-neighborhood by the recurrence time and stays there until a potentially exponentially long \\textit{escape time}. We call this phenomenon \\textit{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/tzen18a/tzen18a.pdf",
        "supp": "",
        "pdf_size": 298659,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14225338961682521555&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois; University of Chicago, Booth School of Business; University of Illinois",
        "aff_domain": "ILLINOIS.EDU;CHICAGOBOOTH.EDU;ILLINOIS.EDU",
        "email": "ILLINOIS.EDU;CHICAGOBOOTH.EDU;ILLINOIS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois;University of Chicago",
        "aff_unique_dep": ";Booth School of Business",
        "aff_unique_url": "https://www.illinois.edu;https://www.chicagobooth.edu",
        "aff_unique_abbr": "UIUC;UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4b1b5a048a",
        "title": "Local moment matching: A unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance",
        "site": "https://proceedings.mlr.press/v75/han18b.html",
        "author": "Yanjun Han; Jiantao Jiao; Tsachy Weissman",
        "abstract": "We present \\emph{Local Moment Matching (LMM)}, a unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance. We construct an efficiently computable estimator that achieves the minimax rates in estimating the distribution up to permutation, and show that the plug-in approach of our unlabeled distribution estimator is \u201cuniversal\" in estimating symmetric functionals of discrete distributions. Instead of doing best polynomial approximation explicitly as in existing literature of functional estimation, the plug-in approach conducts polynomial approximation implicitly and attains the optimal sample complexity for the entropy, power sum and support size functionals.",
        "bibtex": "@InProceedings{pmlr-v75-han18b,\n  title = \t {Local moment matching: A unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance},\n  author =       {Han, Yanjun and Jiao, Jiantao and Weissman, Tsachy},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3189--3221},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/han18b/han18b.pdf},\n  url = \t {https://proceedings.mlr.press/v75/han18b.html},\n  abstract = \t {We present \\emph{Local Moment Matching (LMM)}, a unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance. We construct an efficiently computable estimator that achieves the minimax rates in estimating the distribution up to permutation, and show that the plug-in approach of our unlabeled distribution estimator is \u201cuniversal\" in estimating symmetric functionals of discrete distributions. Instead of doing best polynomial approximation explicitly as in existing literature of functional estimation, the plug-in approach conducts polynomial approximation implicitly and attains the optimal sample complexity for the entropy, power sum and support size functionals.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/han18b/han18b.pdf",
        "supp": "",
        "pdf_size": 442468,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10357127642755608307&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical Engineering, Stanford University; Department of Electrical Engineering, Stanford University; Department of Electrical Engineering, Stanford University",
        "aff_domain": "STANFORD.EDU;STANFORD.EDU;STANFORD.EDU",
        "email": "STANFORD.EDU;STANFORD.EDU;STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "732a5ee793",
        "title": "Log-concave sampling: Metropolis-Hastings algorithms are fast!",
        "site": "https://proceedings.mlr.press/v75/dwivedi18a.html",
        "author": "Raaz Dwivedi; Yuansi Chen; Martin J Wainwright; Bin Yu",
        "abstract": "We consider the problem of sampling from a strongly log-concave density in $\\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by running a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step to ensure the correct stationary distribution. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds reveal that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most $\\delta$ for a density with condition number $\\kappa$, we show that MALA requires $\\mathcal{O} \\big(\\kappa d \\log(1/\\delta) \\big)$ steps, as compared to the $\\mathcal{O} \\big(\\kappa^2 d/\\delta^2 \\big)$ steps established in past work on ULA.  We also demonstrate the gains of MALA over ULA for weakly log-concave densities.  Furthermore, we derive mixing time bounds for a zeroth-order method Metropolized random walk (MRW) and show that it mixes $\\mathcal{O}(\\kappa d)$ slower than MALA.",
        "bibtex": "@InProceedings{pmlr-v75-dwivedi18a,\n  title = \t {Log-concave sampling: Metropolis-Hastings algorithms are fast!},\n  author =       {Dwivedi, Raaz and Chen, Yuansi and Wainwright, Martin J and Yu, Bin},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {793--797},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/dwivedi18a/dwivedi18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/dwivedi18a.html},\n  abstract = \t {We consider the problem of sampling from a strongly log-concave density in $\\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by running a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step to ensure the correct stationary distribution. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds reveal that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most $\\delta$ for a density with condition number $\\kappa$, we show that MALA requires $\\mathcal{O} \\big(\\kappa d \\log(1/\\delta) \\big)$ steps, as compared to the $\\mathcal{O} \\big(\\kappa^2 d/\\delta^2 \\big)$ steps established in past work on ULA.  We also demonstrate the gains of MALA over ULA for weakly log-concave densities.  Furthermore, we derive mixing time bounds for a zeroth-order method Metropolized random walk (MRW) and show that it mixes $\\mathcal{O}(\\kappa d)$ slower than MALA.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/dwivedi18a/dwivedi18a.pdf",
        "supp": "",
        "pdf_size": 189533,
        "gs_citation": 326,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7600930233826840675&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering and Computer Sciences, UC Berkeley; Department of Statistics, UC Berkeley; Department EECS and Department of Statistics, UC Berkeley; Department EECS and Department of Statistics, UC Berkeley",
        "aff_domain": "BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU",
        "email": "BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU",
        "github": "",
        "project": "https://arxiv.org/abs/1801.02309",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3d97453875",
        "title": "Logistic Regression: The Importance of Being Improper",
        "site": "https://proceedings.mlr.press/v75/foster18a.html",
        "author": "Dylan J. Foster; Satyen Kale; Haipeng Luo; Mehryar Mohri; Karthik Sridharan",
        "abstract": "Learning linear predictors with the logistic loss\u2014both in stochastic and online settings\u2014is a fundamental task in machine learning and statistics, with direct connections to classification and boosting. Existing \u201cfast rates\u201d for this setting exhibit exponential dependence on the predictor norm, and Hazan et al. (2014) showed that this is unfortunately unimprovable. Starting with the simple observation that the logistic loss is $1$-mixable, we design a new efficient improper learning algorithm for online logistic regression that circumvents the aforementioned lower bound with a regret bound exhibiting a doubly-exponential improvement in dependence on the predictor norm. This provides a positive resolution to a variant of the COLT 2012 open problem of McMahan and Streeter (2012) when improper learning is allowed. This improvement is obtained both in the online setting and, with some extra work, in the batch statistical setting with high probability. We also show that the improved dependence on predictor norm is near-optimal.  Leveraging this improved dependency on the predictor norm yields the following applications: (a) we give algorithms for online bandit multiclass learning with the logistic loss with an $\\tilde{O}(\\sqrt{n})$ relative mistake bound across essentially all parameter ranges, thus providing a solution to the COLT 2009 open problem of Abernethy and Rakhlin (2009), and (b) we give an adaptive algorithm for online multiclass boosting with optimal sample complexity, thus partially resolving an open problem of Beygelzimer et al. (2015) and Jung et al. (2017). Finally, we give information-theoretic bounds on the optimal rates for improper logistic regression with general function classes, thereby characterizing the extent to which our improvement for linear classes extends to other parametric and even nonparametric settings.",
        "bibtex": "@InProceedings{pmlr-v75-foster18a,\n  title = \t {Logistic Regression: The Importance of Being Improper},\n  author =       {Foster, Dylan J. and Kale, Satyen and Luo, Haipeng and Mohri, Mehryar and Sridharan, Karthik},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {167--208},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/foster18a/foster18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/foster18a.html},\n  abstract = \t {Learning linear predictors with the logistic loss\u2014both in stochastic and online settings\u2014is a fundamental task in machine learning and statistics, with direct connections to classification and boosting. Existing \u201cfast rates\u201d for this setting exhibit exponential dependence on the predictor norm, and Hazan et al. (2014) showed that this is unfortunately unimprovable. Starting with the simple observation that the logistic loss is $1$-mixable, we design a new efficient improper learning algorithm for online logistic regression that circumvents the aforementioned lower bound with a regret bound exhibiting a doubly-exponential improvement in dependence on the predictor norm. This provides a positive resolution to a variant of the COLT 2012 open problem of McMahan and Streeter (2012) when improper learning is allowed. This improvement is obtained both in the online setting and, with some extra work, in the batch statistical setting with high probability. We also show that the improved dependence on predictor norm is near-optimal.  Leveraging this improved dependency on the predictor norm yields the following applications: (a) we give algorithms for online bandit multiclass learning with the logistic loss with an $\\tilde{O}(\\sqrt{n})$ relative mistake bound across essentially all parameter ranges, thus providing a solution to the COLT 2009 open problem of Abernethy and Rakhlin (2009), and (b) we give an adaptive algorithm for online multiclass boosting with optimal sample complexity, thus partially resolving an open problem of Beygelzimer et al. (2015) and Jung et al. (2017). Finally, we give information-theoretic bounds on the optimal rates for improper logistic regression with general function classes, thereby characterizing the extent to which our improvement for linear classes extends to other parametric and even nonparametric settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/foster18a/foster18a.pdf",
        "supp": "",
        "pdf_size": 604790,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7299774749500730699&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Cornell University; Google Research; University of Southern California; New York University+Google Research; Cornell University",
        "aff_domain": "CS.CORNELL.EDU;GOOGLE.COM;USC.EDU;CS.NYU.EDU;CS.CORNELL.EDU",
        "email": "CS.CORNELL.EDU;GOOGLE.COM;USC.EDU;CS.NYU.EDU;CS.CORNELL.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3+1;0",
        "aff_unique_norm": "Cornell University;Google;University of Southern California;New York University",
        "aff_unique_dep": ";Google Research;;",
        "aff_unique_url": "https://www.cornell.edu;https://research.google;https://www.usc.edu;https://www.nyu.edu",
        "aff_unique_abbr": "Cornell;Google Research;USC;NYU",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Mountain View;Los Angeles",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b675275d3d",
        "title": "Lower Bounds for Higher-Order Convex Optimization",
        "site": "https://proceedings.mlr.press/v75/agarwal18a.html",
        "author": "Naman Agarwal; Elad Hazan",
        "abstract": "State-of-the-art methods in mathematical optimization employ higher-order derivative information. We explore the limitations of higher-order optimization and prove that even for convex optimization, a polynomial dependence on the approximation guarantee and higher-order smoothness parameters is necessary. This refutes the hope that higher-order smoothness and higher-order derivatives can lead to dimension free polynomial time algorithms for convex optimization. As a special case, we show Nesterov\u2019s accelerated cubic regularization method and higher-order methods to be nearly tight.",
        "bibtex": "@InProceedings{pmlr-v75-agarwal18a,\n  title = \t {Lower Bounds for Higher-Order Convex Optimization},\n  author =       {Agarwal, Naman and Hazan, Elad},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {774--792},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/agarwal18a/agarwal18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/agarwal18a.html},\n  abstract = \t {State-of-the-art methods in mathematical optimization employ higher-order derivative information. We explore the limitations of higher-order optimization and prove that even for convex optimization, a polynomial dependence on the approximation guarantee and higher-order smoothness parameters is necessary. This refutes the hope that higher-order smoothness and higher-order derivatives can lead to dimension free polynomial time algorithms for convex optimization. As a special case, we show Nesterov\u2019s accelerated cubic regularization method and higher-order methods to be nearly tight.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/agarwal18a/agarwal18a.pdf",
        "supp": "",
        "pdf_size": 288829,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=972668425034490078&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Princeton University, Princeton, NJ + Google Brain; Department of Computer Science, Princeton University, Princeton, NJ",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Princeton University;Google",
        "aff_unique_dep": "Department of Computer Science;Google Brain",
        "aff_unique_url": "https://www.princeton.edu;https://brain.google.com",
        "aff_unique_abbr": "Princeton;Google Brain",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Princeton;Mountain View",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3e111f40ec",
        "title": "Marginal Singularity, and the Benefits of Labels in Covariate-Shift",
        "site": "https://proceedings.mlr.press/v75/kpotufe18a.html",
        "author": "Samory Kpotufe; Guillaume Martinet",
        "abstract": "We present new minimax results that concisely capture the relative benefits of source and target labeled data, under {covariate-shift}. Namely, we show that, in general classification settings, the benefits of target labels are controlled by a \\emph{transfer-exponent} $\\gamma$ that encodes how \\emph{singular} $Q$ is locally w.r.t. $P$, and interestingly allows situations where transfer did not seem possible under previous insights. In fact, our new minimax analysis \u2013 in terms of $\\gamma$ \u2013 reveals a \\emph{continuum of regimes} ranging from situations where target labels have little benefit, to regimes where target labels dramatically improve classification.  We then show that a recently proposed semi-supervised procedure can be extended to adapt to unknown $\\gamma$, and therefore requests target labels only when beneficial, while achieving nearly minimax transfer rates.",
        "bibtex": "@InProceedings{pmlr-v75-kpotufe18a,\n  title = \t {Marginal Singularity, and the Benefits of Labels in Covariate-Shift},\n  author =       {Kpotufe, Samory and Martinet, Guillaume},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1882--1886},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/kpotufe18a/kpotufe18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/kpotufe18a.html},\n  abstract = \t {We present new minimax results that concisely capture the relative benefits of source and target labeled data, under {covariate-shift}. Namely, we show that, in general classification settings, the benefits of target labels are controlled by a \\emph{transfer-exponent} $\\gamma$ that encodes how \\emph{singular} $Q$ is locally w.r.t. $P$, and interestingly allows situations where transfer did not seem possible under previous insights. In fact, our new minimax analysis \u2013 in terms of $\\gamma$ \u2013 reveals a \\emph{continuum of regimes} ranging from situations where target labels have little benefit, to regimes where target labels dramatically improve classification.  We then show that a recently proposed semi-supervised procedure can be extended to adapt to unknown $\\gamma$, and therefore requests target labels only when beneficial, while achieving nearly minimax transfer rates. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/kpotufe18a/kpotufe18a.pdf",
        "supp": "",
        "pdf_size": 163835,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5838109946767994256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "ORFE, Princeton University; ORFE, Princeton University",
        "aff_domain": "PRINCETON.EDU;PRINCETON.EDU",
        "email": "PRINCETON.EDU;PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "811aab276a",
        "title": "Minimax Bounds on Stochastic Batched Convex Optimization",
        "site": "https://proceedings.mlr.press/v75/duchi18a.html",
        "author": "John Duchi; Feng Ruan; Chulhee Yun",
        "abstract": "We study the stochastic batched convex optimization problem, in which we use many \\emph{parallel} observations to optimize a convex function given limited rounds of interaction.  In each of $M$ rounds, an algorithm may query for information at $n$ points, and after issuing all $n$ queries, it receives unbiased noisy function and/or (sub)gradient evaluations at the $n$ points.  After $M$ such rounds, the algorithm must output an estimator.  We provide lower and upper bounds on the performance of such batched convex optimization algorithms in zeroth and first-order settings for Lipschitz convex and smooth strongly convex functions.  Our rates of convergence (nearly) achieve the fully sequential rate once $M = O(d \\log \\log n)$, where $d$ is the problem dimension, but the rates may exponentially degrade as the dimension $d$ increases, in distinction from fully sequential settings.",
        "bibtex": "@InProceedings{pmlr-v75-duchi18a,\n  title = \t {Minimax Bounds on Stochastic Batched Convex Optimization},\n  author =       {Duchi, John and Ruan, Feng and Yun, Chulhee},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3065--3162},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/duchi18a/duchi18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/duchi18a.html},\n  abstract = \t { We study the stochastic batched convex optimization problem, in which we use many \\emph{parallel} observations to optimize a convex function given limited rounds of interaction.  In each of $M$ rounds, an algorithm may query for information at $n$ points, and after issuing all $n$ queries, it receives unbiased noisy function and/or (sub)gradient evaluations at the $n$ points.  After $M$ such rounds, the algorithm must output an estimator.  We provide lower and upper bounds on the performance of such batched convex optimization algorithms in zeroth and first-order settings for Lipschitz convex and smooth strongly convex functions.  Our rates of convergence (nearly) achieve the fully sequential rate once $M = O(d \\log \\log n)$, where $d$ is the problem dimension, but the rates may exponentially degrade as the dimension $d$ increases, in distinction from fully sequential settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/duchi18a/duchi18a.pdf",
        "supp": "",
        "pdf_size": 843195,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2493809142236868706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University; Stanford University; Massachusetts Institute of Technology",
        "aff_domain": "STANFORD.EDU;STANFORD.EDU;MIT.EDU",
        "email": "STANFORD.EDU;STANFORD.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Stanford University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://web.mit.edu",
        "aff_unique_abbr": "Stanford;MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "947f19cbc3",
        "title": "More Adaptive Algorithms for Adversarial Bandits",
        "site": "https://proceedings.mlr.press/v75/wei18a.html",
        "author": "Chen-Yu Wei; Haipeng Luo",
        "abstract": "We develop a novel and generic algorithm for the adversarial multi-armed bandit problem (or more generally the combinatorial semi-bandit problem). When instantiated differently, our algorithm achieves various new data-dependent regret bounds improving previous work. Examples include: 1) a regret bound depending on the variance of only the best arm; 2) a regret bound depending on the first-order path-length of only the best arm; 3) a regret bound depending on the sum of the first-order path-lengths of all arms as well as an important negative term, which together lead to faster convergence rates for some normal form games with partial feedback; 4) a regret bound that simultaneously implies small regret when the best arm has small loss {\\it and} logarithmic regret when there exists an arm whose expected loss is always smaller than those of other arms by a fixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last two results, our algorithm is completely parameter-free. The main idea of our algorithm is to apply the optimism and adaptivity techniques to the well-known Online Mirror Descent framework with a special log-barrier regularizer. The challenges are to come up with appropriate optimistic predictions and correction terms in this framework. Some of our results also crucially rely on using a sophisticated increasing learning rate schedule.",
        "bibtex": "@InProceedings{pmlr-v75-wei18a,\n  title = \t {More Adaptive Algorithms for Adversarial Bandits},\n  author =       {Wei, Chen-Yu and Luo, Haipeng},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1263--1291},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/wei18a/wei18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/wei18a.html},\n  abstract = \t {We develop a novel and generic algorithm for the adversarial multi-armed bandit problem (or more generally the combinatorial semi-bandit problem). When instantiated differently, our algorithm achieves various new data-dependent regret bounds improving previous work. Examples include: 1) a regret bound depending on the variance of only the best arm; 2) a regret bound depending on the first-order path-length of only the best arm; 3) a regret bound depending on the sum of the first-order path-lengths of all arms as well as an important negative term, which together lead to faster convergence rates for some normal form games with partial feedback; 4) a regret bound that simultaneously implies small regret when the best arm has small loss {\\it and} logarithmic regret when there exists an arm whose expected loss is always smaller than those of other arms by a fixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last two results, our algorithm is completely parameter-free. The main idea of our algorithm is to apply the optimism and adaptivity techniques to the well-known Online Mirror Descent framework with a special log-barrier regularizer. The challenges are to come up with appropriate optimistic predictions and correction terms in this framework. Some of our results also crucially rely on using a sophisticated increasing learning rate schedule.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/wei18a/wei18a.pdf",
        "supp": "",
        "pdf_size": 460108,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4706419735838478164&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Southern California; University of Southern California",
        "aff_domain": "usc.edu;usc.edu",
        "email": "usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f46cdcf1f5",
        "title": "Near-Optimal Sample Complexity Bounds for Maximum Likelihood Estimation of Multivariate Log-concave Densities",
        "site": "https://proceedings.mlr.press/v75/carpenter18a.html",
        "author": "Timothy Carpenter; Ilias Diakonikolas; Anastasios Sidiropoulos; Alistair Stewart",
        "abstract": "We study the problem of learning multivariate log-concave densities with respect to a global loss function. We obtain the first upper bound on the sample complexity of the maximum likelihood estimator (MLE) for a log-concave density on $\\mathbb{R}^d$, for all $d \\geq 4$. Prior to this work, no finite sample upper bound was known for this estimator in more than $3$ dimensions. In more detail, we prove that for any $d \\geq 1$ and $\\epsilon>0$, given  $\\tilde{O}_d((1/\\epsilon)^{(d+3)/2})$ samples drawn from an unknown log-concave density $f_0$ on $\\mathbb{R}^d$, the MLE outputs a hypothesis $h$ that with high probability is $\\epsilon$-close to $f_0$, in squared Hellinger loss. A sample complexity lower bound of $\\Omega_d((1/\\epsilon)^{(d+1)/2})$ was previously known for any learning algorithm that achieves this guarantee. We thus establish that the sample complexity of the log-concave MLE is near-optimal, up to an $\\tilde{O}(1/\\epsilon)$ factor.",
        "bibtex": "@InProceedings{pmlr-v75-carpenter18a,\n  title = \t {Near-Optimal Sample Complexity Bounds for Maximum Likelihood Estimation of Multivariate Log-concave Densities},\n  author =       {Carpenter, Timothy and Diakonikolas, Ilias and Sidiropoulos, Anastasios and Stewart, Alistair},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1234--1262},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/carpenter18a/carpenter18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/carpenter18a.html},\n  abstract = \t {We study the problem of learning multivariate log-concave densities with respect to a global loss function. We obtain the first upper bound on the sample complexity of the maximum likelihood estimator (MLE) for a log-concave density on $\\mathbb{R}^d$, for all $d \\geq 4$. Prior to this work, no finite sample upper bound was known for this estimator in more than $3$ dimensions. In more detail, we prove that for any $d \\geq 1$ and $\\epsilon>0$, given  $\\tilde{O}_d((1/\\epsilon)^{(d+3)/2})$ samples drawn from an unknown log-concave density $f_0$ on $\\mathbb{R}^d$, the MLE outputs a hypothesis $h$ that with high probability is $\\epsilon$-close to $f_0$, in squared Hellinger loss. A sample complexity lower bound of $\\Omega_d((1/\\epsilon)^{(d+1)/2})$ was previously known for any learning algorithm that achieves this guarantee. We thus establish that the sample complexity of the log-concave MLE is near-optimal, up to an $\\tilde{O}(1/\\epsilon)$ factor.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/carpenter18a/carpenter18a.pdf",
        "supp": "",
        "pdf_size": 585242,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6345833480526623076&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Ohio State University; University of Southern California; University of Illinois at Chicago; University of Southern California",
        "aff_domain": "OSU.EDU;USC.EDU;GMAIL.COM;USC.EDU",
        "email": "OSU.EDU;USC.EDU;GMAIL.COM;USC.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Ohio State University;University of Southern California;University of Illinois at Chicago",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.osu.edu;https://www.usc.edu;https://www.uic.edu",
        "aff_unique_abbr": "OSU;USC;UIC",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Los Angeles;Chicago",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3514221e4f",
        "title": "Non-Convex Matrix Completion Against a Semi-Random Adversary",
        "site": "https://proceedings.mlr.press/v75/cheng18b.html",
        "author": "Yu Cheng; Rong Ge",
        "abstract": "Matrix completion is a well-studied problem with many machine learning applications. In practice, the problem is often solved by non-convex optimization algorithms. However, the current theoretical analysis for non-convex algorithms relies crucially on the assumption that each entry of the matrix is observed with exactly the same probability $p$, which is not realistic in practice. In this paper, we investigate a more realistic semi-random model, where the probability of observing each entry is {\\em at least} $p$.  Even with this mild semi-random perturbation, we can construct counter-examples where existing non-convex algorithms get stuck in bad local optima. In light of the negative results, we propose a pre-processing step that tries to re-weight the semi-random input, so that it becomes \u201csimilar\u201d to a random input. We give a nearly-linear time algorithm for this problem, and show that after our pre-processing, all the local minima of the non-convex objective can be used to approximately recover the underlying ground-truth matrix.",
        "bibtex": "@InProceedings{pmlr-v75-cheng18b,\n  title = \t {Non-Convex Matrix Completion Against a Semi-Random Adversary},\n  author =       {Cheng, Yu and Ge, Rong},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1362--1394},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/cheng18b/cheng18b.pdf},\n  url = \t {https://proceedings.mlr.press/v75/cheng18b.html},\n  abstract = \t { Matrix completion is a well-studied problem with many machine learning applications. In practice, the problem is often solved by non-convex optimization algorithms. However, the current theoretical analysis for non-convex algorithms relies crucially on the assumption that each entry of the matrix is observed with exactly the same probability $p$, which is not realistic in practice. In this paper, we investigate a more realistic semi-random model, where the probability of observing each entry is {\\em at least} $p$.  Even with this mild semi-random perturbation, we can construct counter-examples where existing non-convex algorithms get stuck in bad local optima. In light of the negative results, we propose a pre-processing step that tries to re-weight the semi-random input, so that it becomes \u201csimilar\u201d to a random input. We give a nearly-linear time algorithm for this problem, and show that after our pre-processing, all the local minima of the non-convex objective can be used to approximately recover the underlying ground-truth matrix. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/cheng18b/cheng18b.pdf",
        "supp": "",
        "pdf_size": 385357,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7503316442346382736&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Duke University; Duke University",
        "aff_domain": "CS.DUKE.EDU;CS.DUKE.EDU",
        "email": "CS.DUKE.EDU;CS.DUKE.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "95eb662e7f",
        "title": "Nonstochastic Bandits with Composite Anonymous Feedback",
        "site": "https://proceedings.mlr.press/v75/cesa-bianchi18a.html",
        "author": "Nicol\u00f2 Cesa-Bianchi; Claudio Gentile; Yishay Mansour",
        "abstract": "We investigate a nonstochastic bandit setting in which the loss of an action is not immediately charged to the player, but rather spread over at most d consecutive steps in an adversarial way. This implies that the instantaneous loss observed by the player at the end of each round is a sum of as many as d loss components of previously played actions. Hence, unlike the standard bandit setting with delayed feedback, here the player cannot observe the individual delayed losses, but only their sum. Our main contribution is a general reduction transforming a standard bandit algorithm into one that can operate in this harder setting. We also show how the regret of the transformed algorithm can be bounded in terms of the regret of the original algorithm. Our reduction cannot be improved in general: we prove a lower bound on the regret of any bandit algorithm in this setting that matches (up to log factors) the upper bound obtained via our reduction. Finally, we show how our reduction can be extended to more complex bandit settings, such as combinatorial linear bandits and online bandit convex optimization.",
        "bibtex": "@InProceedings{pmlr-v75-cesa-bianchi18a,\n  title = \t {Nonstochastic Bandits with Composite Anonymous Feedback},\n  author =       {Cesa-Bianchi, Nicol\\`o and Gentile, Claudio and Mansour, Yishay},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {750--773},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/cesa-bianchi18a/cesa-bianchi18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/cesa-bianchi18a.html},\n  abstract = \t {We investigate a nonstochastic bandit setting in which the loss of an action is not immediately charged to the player, but rather spread over at most d consecutive steps in an adversarial way. This implies that the instantaneous loss observed by the player at the end of each round is a sum of as many as d loss components of previously played actions. Hence, unlike the standard bandit setting with delayed feedback, here the player cannot observe the individual delayed losses, but only their sum. Our main contribution is a general reduction transforming a standard bandit algorithm into one that can operate in this harder setting. We also show how the regret of the transformed algorithm can be bounded in terms of the regret of the original algorithm. Our reduction cannot be improved in general: we prove a lower bound on the regret of any bandit algorithm in this setting that matches (up to log factors) the upper bound obtained via our reduction. Finally, we show how our reduction can be extended to more complex bandit settings, such as combinatorial linear bandits and online bandit convex optimization.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/cesa-bianchi18a/cesa-bianchi18a.pdf",
        "supp": "",
        "pdf_size": 414402,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17185356328916219126&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dipartimento di Informatica, Universit\u00e0 degli Studi di Milano, Italy; INRIA Lille Nord Europe (France)+Google LLC (USA); Blavatnik School of Computer Science, Tel Aviv University+Google",
        "aff_domain": "UNIMI.IT;GMAIL.COM;GMAIL.COM",
        "email": "UNIMI.IT;GMAIL.COM;GMAIL.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;3+2",
        "aff_unique_norm": "Universit\u00e0 degli Studi di Milano;INRIA;Google;Tel Aviv University",
        "aff_unique_dep": "Dipartimento di Informatica;;Google LLC;Blavatnik School of Computer Science",
        "aff_unique_url": "https://www.unimi.it;https://www.inria.fr;https://www.google.com;https://www.tau.ac.il",
        "aff_unique_abbr": "UniMi;INRIA;Google;TAU",
        "aff_campus_unique_index": "1;2+3",
        "aff_campus_unique": ";Lille;Tel Aviv;Mountain View",
        "aff_country_unique_index": "0;1+2;3+2",
        "aff_country_unique": "Italy;France;United States;Israel"
    },
    {
        "id": "9f1d253a90",
        "title": "Online Learning: Sufficient Statistics and the Burkholder Method",
        "site": "https://proceedings.mlr.press/v75/foster18b.html",
        "author": "Dylan J. Foster; Alexander Rakhlin; Karthik Sridharan",
        "abstract": "We uncover a fairly general principle in online learning: If a regret inequality can be (approximately) expressed as a function of certain \"sufficient statistics\" for the data sequence, then there exists a special Burkholder function that 1) can be used algorithmically to achieve the regret bound and 2) only depends on these sufficient statistics, not the entire data sequence,  so that the online strategy is only required to keep the sufficient statistics in memory. This characterization is achieved by bringing the full power of the Burkholder Method\u2014originally developed for certifying probabilistic martingale inequalities\u2014to bear on the online learning setting. To demonstrate the scope and effectiveness of the Burkholder method, we develop a novel online strategy for matrix prediction that attains a regret bound corresponding to the variance term in matrix concentration inequalities. We also present a linear-time/space prediction strategy for parameter-free supervised learning with linear classes and general smooth norms.",
        "bibtex": "@InProceedings{pmlr-v75-foster18b,\n  title = \t {Online Learning: Sufficient Statistics and the Burkholder Method},\n  author =       {Foster, Dylan J. and Rakhlin, Alexander and Sridharan, Karthik},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3028--3064},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/foster18b/foster18b.pdf},\n  url = \t {https://proceedings.mlr.press/v75/foster18b.html},\n  abstract = \t {We uncover a fairly general principle in online learning: If a regret inequality can be (approximately) expressed as a function of certain \"sufficient statistics\" for the data sequence, then there exists a special Burkholder function that 1) can be used algorithmically to achieve the regret bound and 2) only depends on these sufficient statistics, not the entire data sequence,  so that the online strategy is only required to keep the sufficient statistics in memory. This characterization is achieved by bringing the full power of the Burkholder Method\u2014originally developed for certifying probabilistic martingale inequalities\u2014to bear on the online learning setting. To demonstrate the scope and effectiveness of the Burkholder method, we develop a novel online strategy for matrix prediction that attains a regret bound corresponding to the variance term in matrix concentration inequalities. We also present a linear-time/space prediction strategy for parameter-free supervised learning with linear classes and general smooth norms.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/foster18b/foster18b.pdf",
        "supp": "",
        "pdf_size": 492506,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10279953885651648318&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Cornell University; Massachusetts Institute of Technology; Cornell University",
        "aff_domain": "CS.CORNELL.EDU;MIT.EDU;CS.CORNELL.EDU",
        "email": "CS.CORNELL.EDU;MIT.EDU;CS.CORNELL.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Cornell University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cornell.edu;https://web.mit.edu",
        "aff_unique_abbr": "Cornell;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2bc79fa893",
        "title": "Online Variance Reduction for Stochastic Optimization",
        "site": "https://proceedings.mlr.press/v75/borsos18a.html",
        "author": "Zalan Borsos; Andreas Krause; Kfir Y. Levy",
        "abstract": "Modern stochastic optimization methods often rely on uniform sampling which is agnostic to the underlying characteristics of the data. This might degrade the convergence by  yielding estimates that suffer from a high variance. A possible remedy is to employ non-uniform \\emph{importance sampling} techniques, which take the structure of the dataset into account. In this work, we investigate a recently proposed setting which poses variance reduction as an online optimization problem with bandit feedback. We devise a novel and efficient algorithm for this setting that finds a sequence of importance sampling distributions competitive with the best fixed distribution in hindsight, the first result of this kind. While we present our method for sampling data points, it naturally extends to selecting coordinates or even blocks of thereof. Empirical validations underline the benefits of our method in several settings.",
        "bibtex": "@InProceedings{pmlr-v75-borsos18a,\n  title = \t {Online Variance Reduction for Stochastic Optimization},\n  author =       {Borsos, Zalan and Krause, Andreas and Levy, Kfir Y.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {324--357},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/borsos18a/borsos18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/borsos18a.html},\n  abstract = \t {Modern stochastic optimization methods often rely on uniform sampling which is agnostic to the underlying characteristics of the data. This might degrade the convergence by  yielding estimates that suffer from a high variance. A possible remedy is to employ non-uniform \\emph{importance sampling} techniques, which take the structure of the dataset into account. In this work, we investigate a recently proposed setting which poses variance reduction as an online optimization problem with bandit feedback. We devise a novel and efficient algorithm for this setting that finds a sequence of importance sampling distributions competitive with the best fixed distribution in hindsight, the first result of this kind. While we present our method for sampling data points, it naturally extends to selecting coordinates or even blocks of thereof. Empirical validations underline the benefits of our method in several settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/borsos18a/borsos18a.pdf",
        "supp": "",
        "pdf_size": 442029,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=403709064278312073&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science; Department of Computer Science; Department of Computer Science",
        "aff_domain": "INF.ETHZ.CH;INF.ETHZ.CH;INF.ETHZ.CH",
        "email": "INF.ETHZ.CH;INF.ETHZ.CH;INF.ETHZ.CH",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Unknown Institution",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3fc51c9710",
        "title": "Online learning over a finite action set with limited switching",
        "site": "https://proceedings.mlr.press/v75/altschuler18a.html",
        "author": "Jason Altschuler; Kunal Talwar",
        "abstract": "We study the value of switching actions in the Prediction From Experts (PFE) problem and Adversarial Multi-Armed Bandits (MAB) problem. First, we revisit the well-studied and practically motivated setting of PFE with switching costs. Many algorithms are known to achieve the minimax optimal order of $O(\\sqrt{T \\log n})$ in \\textit{expectation} for both regret and number of switches, where $T$ is the number of iterations and $n$ the number of actions. However, no \\textit{high probability} guarantees are known. Our main technical contribution is the first algorithms which with high probability achieve this optimal order for both regret and number of switches. This settles an open problem of [Devroye et al., 2015], directly implies the first high probability guarantees for several problems of interest, and is efficiently adaptable to the related problem of online combinatorial optimization with limited switching. \\par Next, to investigate the value of switching actions at a more granular level, we introduce the setting of \\textit{switching budgets}, in which the algorithm is limited to $S \\leq T$ switches between actions. This entails a limited number of free switches, in contrast to the unlimited number of expensive switches allowed in the switching cost setting. Using the above result and several reductions, we unify previous work and completely characterize the complexity of this switching budget setting up to small polylogarithmic factors: for both the PFE and MAB problems, for all switching budgets $S \\leq T$, and for both expectation and high probability guarantees. For PFE, we show that the optimal rate is of order $\\tilde{\\Theta}(\\sqrt{T\\log n})$ for $S = \\Omega(\\sqrt{T\\log n})$, and $\\min(\\tilde{\\Theta}(\\tfrac{T\\log n}{S}), T)$ for $S = O(\\sqrt{T \\log n})$. Interestingly, the bandit setting does not exhibit such a phase transition; instead we show the minimax rate decays steadily as $\\min(\\tilde{\\Theta}(\\tfrac{T\\sqrt{n}}{\\sqrt{S}}), T)$ for all ranges of $S \\leq T$. These results recover and generalize the known minimax rates for the (arbitrary) switching cost setting.",
        "bibtex": "@InProceedings{pmlr-v75-altschuler18a,\n  title = \t {Online learning over a finite action set with limited switching},\n  author =       {Altschuler, Jason and Talwar, Kunal},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1569--1573},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/altschuler18a/altschuler18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/altschuler18a.html},\n  abstract = \t {We study the value of switching actions in the Prediction From Experts (PFE) problem and Adversarial Multi-Armed Bandits (MAB) problem. First, we revisit the well-studied and practically motivated setting of PFE with switching costs. Many algorithms are known to achieve the minimax optimal order of $O(\\sqrt{T \\log n})$ in \\textit{expectation} for both regret and number of switches, where $T$ is the number of iterations and $n$ the number of actions. However, no \\textit{high probability} guarantees are known. Our main technical contribution is the first algorithms which with high probability achieve this optimal order for both regret and number of switches. This settles an open problem of [Devroye et al., 2015], directly implies the first high probability guarantees for several problems of interest, and is efficiently adaptable to the related problem of online combinatorial optimization with limited switching. \\par Next, to investigate the value of switching actions at a more granular level, we introduce the setting of \\textit{switching budgets}, in which the algorithm is limited to $S \\leq T$ switches between actions. This entails a limited number of free switches, in contrast to the unlimited number of expensive switches allowed in the switching cost setting. Using the above result and several reductions, we unify previous work and completely characterize the complexity of this switching budget setting up to small polylogarithmic factors: for both the PFE and MAB problems, for all switching budgets $S \\leq T$, and for both expectation and high probability guarantees. For PFE, we show that the optimal rate is of order $\\tilde{\\Theta}(\\sqrt{T\\log n})$ for $S = \\Omega(\\sqrt{T\\log n})$, and $\\min(\\tilde{\\Theta}(\\tfrac{T\\log n}{S}), T)$ for $S = O(\\sqrt{T \\log n})$. Interestingly, the bandit setting does not exhibit such a phase transition; instead we show the minimax rate decays steadily as $\\min(\\tilde{\\Theta}(\\tfrac{T\\sqrt{n}}{\\sqrt{S}}), T)$ for all ranges of $S \\leq T$. These results recover and generalize the known minimax rates for the (arbitrary) switching cost setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/altschuler18a/altschuler18a.pdf",
        "supp": "",
        "pdf_size": 304494,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16756051887403689010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Massachusetts Institute of Technology; Google Brain",
        "aff_domain": "mit.edu;google.com",
        "email": "mit.edu;google.com",
        "github": "",
        "project": "https://arxiv.org/abs/1803.01548v2",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Google",
        "aff_unique_dep": ";Google Brain",
        "aff_unique_url": "https://web.mit.edu;https://brain.google.com",
        "aff_unique_abbr": "MIT;Google Brain",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "01eb5b3cfd",
        "title": "Open Problem: The Dependence of Sample Complexity Lower Bounds on Planning Horizon",
        "site": "https://proceedings.mlr.press/v75/jiang18a.html",
        "author": "Nan Jiang; Alekh Agarwal",
        "abstract": "In reinforcement learning (RL), problems with long planning horizons are perceived as very challenging. The recent advances in PAC RL, however, show that the sample complexity of RL does not depend on planning horizon except at a superficial level. How can we explain such a difference? Noting that the technical assumptions in these upper bounds might have hidden away the challenges of long horizons, we ask the question: \\emph{can we prove a lower bound with a horizon dependence when such assumptions are removed?} We also provide a few observations on the desired characteristics of the lower bound construction.",
        "bibtex": "@InProceedings{pmlr-v75-jiang18a,\n  title = \t {Open Problem: The Dependence of Sample Complexity Lower Bounds on Planning Horizon},\n  author =       {Jiang, Nan and Agarwal, Alekh},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3395--3398},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/jiang18a/jiang18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/jiang18a.html},\n  abstract = \t {In reinforcement learning (RL), problems with long planning horizons are perceived as very challenging. The recent advances in PAC RL, however, show that the sample complexity of RL does not depend on planning horizon except at a superficial level. How can we explain such a difference? Noting that the technical assumptions in these upper bounds might have hidden away the challenges of long horizons, we ask the question: \\emph{can we prove a lower bound with a horizon dependence when such assumptions are removed?} We also provide a few observations on the desired characteristics of the lower bound construction.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/jiang18a/jiang18a.pdf",
        "supp": "",
        "pdf_size": 207152,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12319355216397209774&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Illinois; Microsoft Research, New York",
        "aff_domain": "illinois.edu;microsoft.com",
        "email": "illinois.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Illinois;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.illinois.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UIUC;MSR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "751ea53e66",
        "title": "Open problem: Improper learning of mixtures of Gaussians",
        "site": "https://proceedings.mlr.press/v75/hazan18a.html",
        "author": "Elad Hazan; Livni Roi",
        "abstract": "We ask whether there exists an efficient unsupervised learning algorithm for mixture of Gaussians in the over-complete case (number of mixtures is larger than the dimension). The notion of learning is taken to be worst-case compression-based, to allow for improper learning.",
        "bibtex": "@InProceedings{pmlr-v75-hazan18a,\n  title = \t {Open problem: Improper learning of mixtures of {G}aussians},\n  author =       {Hazan, Elad and Roi, Livni},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3399--3402},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/hazan18a/hazan18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/hazan18a.html},\n  abstract = \t {We ask whether there exists an efficient unsupervised learning algorithm for mixture of Gaussians in the over-complete case (number of mixtures is larger than the dimension). The notion of learning is taken to be worst-case compression-based, to allow for improper learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/hazan18a/hazan18a.pdf",
        "supp": "",
        "pdf_size": 194379,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:DohUWuYmWlIJ:scholar.google.com/&scioq=Open+problem:+Improper+learning+of+mixtures+of+Gaussians&hl=en&as_sdt=0,5",
        "gs_version_total": 6,
        "aff": "Princeton University, Princeton, NJ; Princeton University, Princeton, NJ",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c3cfd17cdc",
        "title": "Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models",
        "site": "https://proceedings.mlr.press/v75/barbier18a.html",
        "author": "Jean Barbier; Florent Krzakala; Nicolas Macris; L\u00e9o Miolane; Lenka Zdeborov\u00e1",
        "abstract": "Generalized linear models (GLMs) arise in high-dimensional machine learning, statistics, communications and signal processing. % In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes or benchmarks models in neural networks. % We evaluate the mutual information (or \u201cfree entropy\u201d) from which we deduce the Bayes-optimal inference and generalization errors.  Our analysis applies to the high-dimensional limit where both the number of samples and dimensions are large and their ratio is fixed. % Non-rigorous predictions for the optimal inference and generalization errors existed for special cases of GLMs, e.g. for the perceptron in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. % Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance, and locate the associated sharp phase transitions separating learnable and non-learnable regions.",
        "bibtex": "@InProceedings{pmlr-v75-barbier18a,\n  title = \t {Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models},\n  author =       {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\\'e}o and Zdeborov{\\'a}, Lenka},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {728--731},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/barbier18a/barbier18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/barbier18a.html},\n  abstract = \t {Generalized linear models (GLMs) arise in high-dimensional machine learning, statistics, communications and signal processing. % In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes or benchmarks models in neural networks. % We evaluate the mutual information (or \u201cfree entropy\u201d) from which we deduce the Bayes-optimal inference and generalization errors.  Our analysis applies to the high-dimensional limit where both the number of samples and dimensions are large and their ratio is fixed. % Non-rigorous predictions for the optimal inference and generalization errors existed for special cases of GLMs, e.g. for the perceptron in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. % Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance, and locate the associated sharp phase transitions separating learnable and non-learnable regions.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/barbier18a/barbier18a.pdf",
        "supp": "",
        "pdf_size": 135157,
        "gs_citation": 337,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14414254285744051513&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "Laboratoire de Th\u00e9orie des Communications, Facult\u00e9 Informatique et Communications, Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne, CH-1015 Lausanne, Suisse+Probability and Applications Group, School of Mathematical Sciences, Queen Mary University of London, E14NS London, United-Kingdom; Laboratoire de Physique Statistique, CNRS & Sorbonne Universit\u00e9s & Ecole Normale Sup\u00e9rieure & PSL University, 75005 Paris, France; Laboratoire de Th\u00e9orie des Communications, Facult\u00e9 Informatique et Communications, Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne, CH-1015 Lausanne, Suisse; D\u00e9partement d\u2019Informatique de l\u2019ENS, Ecole Normale Sup\u00e9rieure & CNRS & PSL University & Inria, 75005 Paris, France; Institut de Physique Th\u00e9orique, CNRS & CEA & Universit\u00e9 Paris-Saclay, 91191 Gif-sur-Yvette, France",
        "aff_domain": "epfl.ch;ens.fr;epfl.ch;inria.fr;ipht.fr",
        "email": "epfl.ch;ens.fr;epfl.ch;inria.fr;ipht.fr",
        "github": "",
        "project": "https://arxiv.org/abs/1708.03395",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0;3;4",
        "aff_unique_norm": "EPFL;Queen Mary University of London;CNRS;Ecole Normale Sup\u00e9rieure;Institut de Physique Th\u00e9orique",
        "aff_unique_dep": "Facult\u00e9 Informatique et Communications;School of Mathematical Sciences;Laboratoire de Physique Statistique;D\u00e9partement d\u2019Informatique;CNRS & CEA & Universit\u00e9 Paris-Saclay",
        "aff_unique_url": "https://www.epfl.ch;https://www.qmul.ac.uk;https://www.cnrs.fr;https://www.ens.fr;",
        "aff_unique_abbr": "EPFL;QMUL;CNRS;ENS;",
        "aff_campus_unique_index": "0+1;0;3",
        "aff_campus_unique": "Lausanne;London;;Paris",
        "aff_country_unique_index": "0+1;2;0;2;2",
        "aff_country_unique": "Switzerland;United Kingdom;France"
    },
    {
        "id": "1582ae7c7f",
        "title": "Optimal Single Sample Tests for Structured versus Unstructured Network Data",
        "site": "https://proceedings.mlr.press/v75/bresler18a.html",
        "author": "Guy Bresler; Dheeraj Nagaraj",
        "abstract": "We study the problem of testing, using only a single sample, between mean field distribu- tions (like Curie-Weiss, Erd\u0151s-R\u00e9nyi) and structured Gibbs distributions (like Ising model on sparse graphs and Exponential Random Graphs). Our goal is to test without know- ing the parameter values of the underlying models: only the structure of dependencies is known. We develop a new approach that applies to both the Ising and Exponential Random Graph settings based on a general and natural statistical test. The test can dis- tinguish the hypotheses with high probability above a certain threshold in the (inverse) temperature parameter, and is optimal in that below the threshold no test can distinguish the hypotheses. The thresholds do not correspond to the presence of long-range order in the models. By aggregating information at a global scale, our test works even at very high temperatures. The proofs are based on distributional approximation and sharp concentration of quadratic forms, when restricted to Hamming spheres. The restriction to Hamming spheres is necessary, since otherwise any scalar statistic is useless without explicit knowledge of the temperature parameter. At the same time, this restriction changes the behavior of the functions under consideration, making it hard to directly apply standard methods (i.e., Stein\u2019s method) for concentration of weakly dependent variables. Instead, we carry out an additional tensorization argument using a Markov chain that respects the symmetry of the Hamming sphere.",
        "bibtex": "@InProceedings{pmlr-v75-bresler18a,\n  title = \t {Optimal Single Sample Tests for Structured versus Unstructured Network Data},\n  author =       {Bresler, Guy and Nagaraj, Dheeraj},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1657--1690},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/bresler18a/bresler18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/bresler18a.html},\n  abstract = \t { We study the problem of testing, using only a single sample, between mean field distribu- tions (like Curie-Weiss, Erd\u0151s-R\u00e9nyi) and structured Gibbs distributions (like Ising model on sparse graphs and Exponential Random Graphs). Our goal is to test without know- ing the parameter values of the underlying models: only the structure of dependencies is known. We develop a new approach that applies to both the Ising and Exponential Random Graph settings based on a general and natural statistical test. The test can dis- tinguish the hypotheses with high probability above a certain threshold in the (inverse) temperature parameter, and is optimal in that below the threshold no test can distinguish the hypotheses. The thresholds do not correspond to the presence of long-range order in the models. By aggregating information at a global scale, our test works even at very high temperatures. The proofs are based on distributional approximation and sharp concentration of quadratic forms, when restricted to Hamming spheres. The restriction to Hamming spheres is necessary, since otherwise any scalar statistic is useless without explicit knowledge of the temperature parameter. At the same time, this restriction changes the behavior of the functions under consideration, making it hard to directly apply standard methods (i.e., Stein\u2019s method) for concentration of weakly dependent variables. Instead, we carry out an additional tensorization argument using a Markov chain that respects the symmetry of the Hamming sphere. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/bresler18a/bresler18a.pdf",
        "supp": "",
        "pdf_size": 469295,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7177166812530576250&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Electrical Engineering and Computer Science, MIT; Department of Electrical Engineering and Computer Science, MIT",
        "aff_domain": "/m.sc/i.sc/t.sc./e.sc/d.sc/u.sc;/m.sc/i.sc/t.sc./e.sc/d.sc/u.sc",
        "email": "/m.sc/i.sc/t.sc./e.sc/d.sc/u.sc;/m.sc/i.sc/t.sc./e.sc/d.sc/u.sc",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f014a011c4",
        "title": "Optimal approximation of continuous functions by very deep ReLU networks",
        "site": "https://proceedings.mlr.press/v75/yarotsky18a.html",
        "author": "Dmitry Yarotsky",
        "abstract": "We consider approximations of general continuous functions on finite-dimensional cubes by general deep ReLU neural networks and study the approximation rates with respect to the modulus of continuity of the function and the total number of weights $W$ in the network. We establish the complete phase diagram of feasible approximation rates and show that it includes two distinct phases. One phase corresponds to slower approximations that can be achieved with constant-depth networks and continuous weight assignments. The other phase provides faster approximations at the cost of depths necessarily growing as a power law $L\\sim W^{\\alpha}, 0<\\alpha\\le 1,$ and with necessarily discontinuous weight assignments. In particular, we prove that constant-width fully-connected networks of depth $L\\sim W$ provide the fastest possible approximation rate $\\|f-\\widetilde f\\|_\\infty = O(\\omega_f(O(W^{-2/\\nu})))$ that cannot be achieved with less deep networks.",
        "bibtex": "@InProceedings{pmlr-v75-yarotsky18a,\n  title = \t {Optimal approximation of continuous functions by very deep ReLU networks},\n  author =       {Yarotsky, Dmitry},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {639--649},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/yarotsky18a/yarotsky18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/yarotsky18a.html},\n  abstract = \t {We consider approximations of general continuous functions on finite-dimensional cubes by general deep ReLU neural networks and study the approximation rates with respect to the modulus of continuity of the function and the total number of weights $W$ in the network. We establish the complete phase diagram of feasible approximation rates and show that it includes two distinct phases. One phase corresponds to slower approximations that can be achieved with constant-depth networks and continuous weight assignments. The other phase provides faster approximations at the cost of depths necessarily growing as a power law $L\\sim W^{\\alpha}, 0<\\alpha\\le 1,$ and with necessarily discontinuous weight assignments. In particular, we prove that constant-width fully-connected networks of depth $L\\sim W$ provide the fastest possible approximation rate $\\|f-\\widetilde f\\|_\\infty = O(\\omega_f(O(W^{-2/\\nu})))$ that cannot be achieved with less deep networks. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/yarotsky18a/yarotsky18a.pdf",
        "supp": "",
        "pdf_size": 785395,
        "gs_citation": 366,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3193906420203800115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Skolkovo Institute of Science and Technology, Skolkovo Innovation Center, 3 Nobel st., Moscow 143026, Russia+Institute for Information Transmission Problems, Bolshoy Karetny per. 19, build.1, Moscow 127051, Russia",
        "aff_domain": "skoltech.ru",
        "email": "skoltech.ru",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Skolkovo Institute of Science and Technology;Institute for Information Transmission Problems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.skoltech.ru;",
        "aff_unique_abbr": "Skoltech;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Skolkovo;",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "id": "10b6340485",
        "title": "Polynomial Time and Sample Complexity for Non-Gaussian Component Analysis: Spectral Methods",
        "site": "https://proceedings.mlr.press/v75/tan18a.html",
        "author": "Yan Shuo Tan; Roman Vershynin",
        "abstract": "The problem of Non-Gaussian Component Analysis (NGCA) is about finding a maximal low-dimensional subspace $E$ in $\\mathbb{R}^n$ so that data points projected onto $E$ follow a non-Gaussian distribution. Vempala and Xiao (2011) proposed a local search algorithm, and showed that it was able to estimate $E$ accurately with polynomial time and sample complexity, if the dimension of $E$ is treated as a constant and with the assumption that all one-dimensional marginals of the non-Gaussian distribution over $E$ have non-Gaussian moments. In this paper, we propose a simple spectral algorithm called \\textsc{Reweighted PCA}, and prove that it possesses the same guarantee. The principle that underlies this approach is a new characterization of multivariate Gaussian distributions.",
        "bibtex": "@InProceedings{pmlr-v75-tan18a,\n  title = \t {Polynomial Time and Sample Complexity for Non-Gaussian Component Analysis: Spectral Methods},\n  author =       {Tan, Yan Shuo and Vershynin, Roman},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {498--534},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/tan18a/tan18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/tan18a.html},\n  abstract = \t {The problem of Non-Gaussian Component Analysis (NGCA) is about finding a maximal low-dimensional subspace $E$ in $\\mathbb{R}^n$ so that data points projected onto $E$ follow a non-Gaussian distribution. Vempala and Xiao (2011) proposed a local search algorithm, and showed that it was able to estimate $E$ accurately with polynomial time and sample complexity, if the dimension of $E$ is treated as a constant and with the assumption that all one-dimensional marginals of the non-Gaussian distribution over $E$ have non-Gaussian moments. In this paper, we propose a simple spectral algorithm called \\textsc{Reweighted PCA}, and prove that it possesses the same guarantee. The principle that underlies this approach is a new characterization of multivariate Gaussian distributions.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/tan18a/tan18a.pdf",
        "supp": "",
        "pdf_size": 532407,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15965244669614074524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematics, University of Michigan; Department of Mathematics, University of California, Irvine",
        "aff_domain": "UMICH.EDU;UCI.EDU",
        "email": "UMICH.EDU;UCI.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Michigan;University of California, Irvine",
        "aff_unique_dep": "Department of Mathematics;Department of Mathematics",
        "aff_unique_url": "https://www.umich.edu;https://www.uci.edu",
        "aff_unique_abbr": "UM;UCI",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Ann Arbor;Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "439f0badab",
        "title": "Privacy-preserving Prediction",
        "site": "https://proceedings.mlr.press/v75/dwork18a.html",
        "author": "Cynthia Dwork; Vitaly Feldman",
        "abstract": "Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years. It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression. We first describe a simple baseline approach based on training several models on disjoint subsets of data and using standard private aggregation techniques to predict. We show that this approach has nearly optimal sample complexity for (realizable) PAC learning of any class of Boolean functions. At the same time, without strong assumptions on the data distribution, the aggregation step introduces a substantial overhead. We demonstrate that this overhead can be avoided for the well-studied class of thresholds on a line and for a number of standard settings of convex regression. The analysis of our algorithm for learning thresholds relies crucially on strong generalization guarantees that we establish for all differentially private prediction algorithms.",
        "bibtex": "@InProceedings{pmlr-v75-dwork18a,\n  title = \t {Privacy-preserving Prediction},\n  author =       {Dwork, Cynthia and Feldman, Vitaly},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1693--1702},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/dwork18a/dwork18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/dwork18a.html},\n  abstract = \t {Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years. It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression. We first describe a simple baseline approach based on training several models on disjoint subsets of data and using standard private aggregation techniques to predict. We show that this approach has nearly optimal sample complexity for (realizable) PAC learning of any class of Boolean functions. At the same time, without strong assumptions on the data distribution, the aggregation step introduces a substantial overhead. We demonstrate that this overhead can be avoided for the well-studied class of thresholds on a line and for a number of standard settings of convex regression. The analysis of our algorithm for learning thresholds relies crucially on strong generalization guarantees that we establish for all differentially private prediction algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/dwork18a/dwork18a.pdf",
        "supp": "",
        "pdf_size": 271878,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13226820274757123752&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Harvard University; Google Brain",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Harvard University;Google",
        "aff_unique_dep": ";Google Brain",
        "aff_unique_url": "https://www.harvard.edu;https://brain.google.com",
        "aff_unique_abbr": "Harvard;Google Brain",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "06bab633f5",
        "title": "Private Sequential Learning",
        "site": "https://proceedings.mlr.press/v75/tsitsiklis18a.html",
        "author": "John Tsitsiklis; Kuang Xu; Zhi Xu",
        "abstract": "We formulate a private learning model to study an intrinsic tradeoff between privacy and query complexity in sequential learning. Our model involves a learner who aims to determine a scalar value, $v^*$, by sequentially querying an external database and receiving binary responses. In the meantime, an adversary observes the learner\u2019s queries, though not the responses, and tries to infer from them the value of $v^*$. The objective of the learner is to obtain an accurate estimate of $v^*$ using only a small number of queries, while simultaneously protecting her privacy by making $v^*$ provably difficult to learn for the adversary. Our main results provide tight upper and lower bounds on the learner\u2019s query complexity as a function of desired levels of privacy and estimation accuracy. We also construct explicit query strategies whose complexity is optimal up to an additive constant.",
        "bibtex": "@InProceedings{pmlr-v75-tsitsiklis18a,\n  title = \t {Private Sequential Learning},\n  author =       {Tsitsiklis, John and Xu, Kuang and Xu, Zhi},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {721--727},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/tsitsiklis18a/tsitsiklis18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/tsitsiklis18a.html},\n  abstract = \t {We formulate a private learning model to study an intrinsic tradeoff between privacy and query complexity in sequential learning. Our model involves a learner who aims to determine a scalar value, $v^*$, by sequentially querying an external database and receiving binary responses. In the meantime, an adversary observes the learner\u2019s queries, though not the responses, and tries to infer from them the value of $v^*$. The objective of the learner is to obtain an accurate estimate of $v^*$ using only a small number of queries, while simultaneously protecting her privacy by making $v^*$ provably difficult to learn for the adversary. Our main results provide tight upper and lower bounds on the learner\u2019s query complexity as a function of desired levels of privacy and estimation accuracy. We also construct explicit query strategies whose complexity is optimal up to an additive constant.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/tsitsiklis18a/tsitsiklis18a.pdf",
        "supp": "",
        "pdf_size": 178070,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17439086168983549957&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "LIDS, Massachusetts Institute of Technology, Cambridge, MA 02139; Graduate School of Business, Stanford University, Stanford, CA 94305; LIDS, Massachusetts Institute of Technology, Cambridge, MA 02139",
        "aff_domain": "MIT.EDU;STANFORD.EDU;MIT.EDU",
        "email": "MIT.EDU;STANFORD.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Stanford University",
        "aff_unique_dep": "LIDS;Graduate School of Business",
        "aff_unique_url": "https://web.mit.edu;https://www.stanford.edu",
        "aff_unique_abbr": "MIT;Stanford",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "99c42e35b1",
        "title": "Reducibility and Computational Lower Bounds for Problems with Planted Sparse Structure",
        "site": "https://proceedings.mlr.press/v75/brennan18a.html",
        "author": "Matthew Brennan; Guy Bresler; Wasim Huleihel",
        "abstract": "Recently, research in unsupervised learning has gravitated towards exploring statistical-computational gaps induced by sparsity. A line of work initiated in Berthet and Rigollet (2013) has aimed to explain these gaps through reductions to conjecturally hard problems from complexity theory. However, the delicate nature of average-case reductions has limited the development of techniques and often led to weaker hardness results that only apply to algorithms robust to different noise distributions or that do not need to know the parameters of the problem. We introduce several new techniques to give a web of average-case reductions showing strong computational lower bounds based on the planted clique conjecture. Our new lower bounds include:",
        "bibtex": "@InProceedings{pmlr-v75-brennan18a,\n  title = \t {Reducibility and Computational Lower Bounds for Problems with Planted Sparse Structure},\n  author =       {Brennan, Matthew and Bresler, Guy and Huleihel, Wasim},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {48--166},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/brennan18a/brennan18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/brennan18a.html},\n  abstract = \t {Recently, research in unsupervised learning has gravitated towards exploring statistical-computational gaps induced by sparsity. A line of work initiated in Berthet and Rigollet (2013) has aimed to explain these gaps through reductions to conjecturally hard problems from complexity theory. However, the delicate nature of average-case reductions has limited the development of techniques and often led to weaker hardness results that only apply to algorithms robust to different noise distributions or that do not need to know the parameters of the problem. We introduce several new techniques to give a web of average-case reductions showing strong computational lower bounds based on the planted clique conjecture. Our new lower bounds include:",
        "pdf": "http://proceedings.mlr.press/v75/brennan18a/brennan18a.pdf",
        "supp": "",
        "pdf_size": 845208,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17005037680632931686&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of EECS, MIT; Department of EECS, MIT; RLE, MIT",
        "aff_domain": "MIT.EDU;MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "34582200d4",
        "title": "Restricted Eigenvalue from Stable Rank with Applications to Sparse Linear Regression",
        "site": "https://proceedings.mlr.press/v75/kasiviswanathan18a.html",
        "author": "Shiva Prasad Kasiviswanathan; Mark Rudelson",
        "abstract": "High-dimensional settings, where the data dimension ($d$) far exceeds the number of observations ($n$), are common in many statistical and machine learning applications. Methods based on $\\ell_1$-relaxation, such as Lasso, are very popular for sparse recovery in these settings. Restricted Eigenvalue (RE) condition is among the weakest, and hence the most general, condition in literature imposed on the Gram matrix that guarantees nice statistical properties for the Lasso estimator. It is hence natural to ask: what families of matrices satisfy the RE condition?  Following a line of work in this area, we construct a new broad ensemble of dependent random design matrices that have an explicit RE bound. Our construction starts with a fixed (deterministic) matrix $X \\in \\mathbb{R}^{n \\times d}$ satisfying a simple stable rank condition, and we show that a matrix drawn from the distribution $X \\Phi^\\top \\Phi$, where $\\Phi \\in \\mathbb{R}^{m \\times d}$ is a subgaussian random matrix, with high probability, satisfies the RE condition. This construction allows incorporating a fixed matrix that has an easily {\\em verifiable} condition into the design process, and allows for generation of {\\em compressed} design matrices that have a lower storage requirement than a standard design matrix. We give two applications of this construction to sparse linear regression problems, including one to a compressed sparse regression setting where the regression algorithm only has access to a compressed representation of a fixed design matrix $X$.",
        "bibtex": "@InProceedings{pmlr-v75-kasiviswanathan18a,\n  title = \t {Restricted Eigenvalue from Stable Rank with Applications to Sparse Linear Regression},\n  author =       {Kasiviswanathan, Shiva Prasad and Rudelson, Mark},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1011--1041},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/kasiviswanathan18a/kasiviswanathan18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/kasiviswanathan18a.html},\n  abstract = \t {High-dimensional settings, where the data dimension ($d$) far exceeds the number of observations ($n$), are common in many statistical and machine learning applications. Methods based on $\\ell_1$-relaxation, such as Lasso, are very popular for sparse recovery in these settings. Restricted Eigenvalue (RE) condition is among the weakest, and hence the most general, condition in literature imposed on the Gram matrix that guarantees nice statistical properties for the Lasso estimator. It is hence natural to ask: what families of matrices satisfy the RE condition?  Following a line of work in this area, we construct a new broad ensemble of dependent random design matrices that have an explicit RE bound. Our construction starts with a fixed (deterministic) matrix $X \\in \\mathbb{R}^{n \\times d}$ satisfying a simple stable rank condition, and we show that a matrix drawn from the distribution $X \\Phi^\\top \\Phi$, where $\\Phi \\in \\mathbb{R}^{m \\times d}$ is a subgaussian random matrix, with high probability, satisfies the RE condition. This construction allows incorporating a fixed matrix that has an easily {\\em verifiable} condition into the design process, and allows for generation of {\\em compressed} design matrices that have a lower storage requirement than a standard design matrix. We give two applications of this construction to sparse linear regression problems, including one to a compressed sparse regression setting where the regression algorithm only has access to a compressed representation of a fixed design matrix $X$.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/kasiviswanathan18a/kasiviswanathan18a.pdf",
        "supp": "",
        "pdf_size": 460958,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4839555228749226470&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Amazon AWS AI, Palo Alto, CA, USA; University of Michigan, Ann Arbor, MI, USA",
        "aff_domain": "GMAIL.COM;UMICH.EDU",
        "email": "GMAIL.COM;UMICH.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Amazon;University of Michigan",
        "aff_unique_dep": "Amazon AWS AI;",
        "aff_unique_url": "https://aws.amazon.com;https://www.umich.edu",
        "aff_unique_abbr": "Amazon;UM",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Palo Alto;Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b339d4f647",
        "title": "Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem",
        "site": "https://proceedings.mlr.press/v75/wibisono18a.html",
        "author": "Andre Wibisono",
        "abstract": "We study sampling as optimization in the space of measures. We focus on gradient flow-based optimization with the Langevin dynamics as a case study. We investigate the source of the bias of the unadjusted Langevin algorithm (ULA) in discrete time, and consider how to remove or reduce the bias. We point out the difficulty is that the heat flow is exactly solvable, but neither its forward nor backward method is implementable in general, except for Gaussian data. We propose the symmetrized Langevin algorithm (SLA), which should have a smaller bias than ULA, at the price of implementing a proximal gradient step in space. We show SLA is in fact consistent for Gaussian target measure, whereas ULA is not. We also illustrate various algorithms explicitly for Gaussian target measure with Gaussian data, including gradient descent, proximal gradient, and Forward-Backward, and show they are all consistent.",
        "bibtex": "@InProceedings{pmlr-v75-wibisono18a,\n  title = \t {Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem},\n  author =       {Wibisono, Andre},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {2093--3027},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/wibisono18a/wibisono18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/wibisono18a.html},\n  abstract = \t {We study sampling as optimization in the space of measures. We focus on gradient flow-based optimization with the Langevin dynamics as a case study. We investigate the source of the bias of the unadjusted Langevin algorithm (ULA) in discrete time, and consider how to remove or reduce the bias. We point out the difficulty is that the heat flow is exactly solvable, but neither its forward nor backward method is implementable in general, except for Gaussian data. We propose the symmetrized Langevin algorithm (SLA), which should have a smaller bias than ULA, at the price of implementing a proximal gradient step in space. We show SLA is in fact consistent for Gaussian target measure, whereas ULA is not. We also illustrate various algorithms explicitly for Gaussian target measure with Gaussian data, including gradient descent, proximal gradient, and Forward-Backward, and show they are all consistent.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/wibisono18a/wibisono18a.pdf",
        "supp": "",
        "pdf_size": 374773,
        "gs_citation": 226,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17007323827021395757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3e75251d9e",
        "title": "Size-Independent  Sample Complexity of Neural Networks",
        "site": "https://proceedings.mlr.press/v75/golowich18a.html",
        "author": "Noah Golowich; Alexander Rakhlin; Ohad Shamir",
        "abstract": "We study the sample complexity of learning neural networks, by  providing new bounds on their Rademacher complexity assuming norm constraints  on the parameter matrix of each layer. Compared to previous work, these  complexity bounds have improved dependence on the network depth, and under some  additional assumptions, are fully independent of the network size (both depth  and width). These results are derived using some novel techniques, which may be  of independent interest.",
        "bibtex": "@InProceedings{pmlr-v75-golowich18a,\n  title = \t {Size-Independent  Sample Complexity of Neural Networks},\n  author =       {Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {297--299},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/golowich18a/golowich18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/golowich18a.html},\n  abstract = \t {We study the sample complexity of learning neural networks, by  providing new bounds on their Rademacher complexity assuming norm constraints  on the parameter matrix of each layer. Compared to previous work, these  complexity bounds have improved dependence on the network depth, and under some  additional assumptions, are fully independent of the network size (both depth  and width). These results are derived using some novel techniques, which may be  of independent interest.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/golowich18a/golowich18a.pdf",
        "supp": "",
        "pdf_size": 161745,
        "gs_citation": 528,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17373874030073457972&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Harvard University; MIT; Weizmann Institute of Science + Microsoft Research",
        "aff_domain": "college.harvard.edu;mit.edu;weizmann.ac.il",
        "email": "college.harvard.edu;mit.edu;weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "Harvard University;Massachusetts Institute of Technology;Weizmann Institute of Science;Microsoft",
        "aff_unique_dep": ";;;Microsoft Research",
        "aff_unique_url": "https://www.harvard.edu;https://web.mit.edu;https://www.weizmann.org.il;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Harvard;MIT;Weizmann;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "cee62dec05",
        "title": "Small-loss bounds for online learning with partial information",
        "site": "https://proceedings.mlr.press/v75/lykouris18a.html",
        "author": "Thodoris Lykouris; Karthik Sridharan; \u00c9va Tardos",
        "abstract": "We consider the problem of adversarial (non-stochastic) online learning with partial information feedback, where at each round, a decision maker selects an action from a finite set of alternatives. We develop a black-box approach for such problems where the learner observes as feedback only losses of a subset of the actions that includes the selected action. When losses of actions are non-negative, under the graph-based feedback model introduced by Mannor and Shamir, we offer algorithms that attain the so called \u201csmall-loss\u201d $o(\\alpha L^{\\star})$ regret bounds with high probability, where $\\alpha$ is the independence number of the graph, and $L^{\\star}$ is the loss of the best action. Prior to our work, there was no data-dependent guarantee for general feedback graphs even for pseudo-regret (without dependence on the number of actions, i.e. utilizing the increased information feedback). Taking advantage of the black-box nature of our technique, we extend our results to many other applications such as semi-bandits (including routing in networks), contextual bandits (even with an infinite comparator class), as well as learning with slowly changing (shifting) comparators. In the special case of classical bandit and semi-bandit problems, we provide optimal small-loss,  high-probability guarantees of $\\tilde{O}(\\sqrt{dL^{\\star}})$ for actual regret, where $d$ is the number of actions, answering open questions of Neu.  Previous bounds for bandits and semi-bandits were known only for pseudo-regret and only in expectation. We also offer an optimal $\\tilde{O}(\\sqrt{\\kappa L^{\\star}})$ regret guarantee for fixed feedback graphs with clique-partition number at most $\\kappa$.",
        "bibtex": "@InProceedings{pmlr-v75-lykouris18a,\n  title = \t {Small-loss bounds for online learning with partial information},\n  author =       {Lykouris, Thodoris and Sridharan, Karthik and Tardos, \\'Eva},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {979--986},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/lykouris18a/lykouris18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/lykouris18a.html},\n  abstract = \t {We consider the problem of adversarial (non-stochastic) online learning with partial information feedback, where at each round, a decision maker selects an action from a finite set of alternatives. We develop a black-box approach for such problems where the learner observes as feedback only losses of a subset of the actions that includes the selected action. When losses of actions are non-negative, under the graph-based feedback model introduced by Mannor and Shamir, we offer algorithms that attain the so called \u201csmall-loss\u201d $o(\\alpha L^{\\star})$ regret bounds with high probability, where $\\alpha$ is the independence number of the graph, and $L^{\\star}$ is the loss of the best action. Prior to our work, there was no data-dependent guarantee for general feedback graphs even for pseudo-regret (without dependence on the number of actions, i.e. utilizing the increased information feedback). Taking advantage of the black-box nature of our technique, we extend our results to many other applications such as semi-bandits (including routing in networks), contextual bandits (even with an infinite comparator class), as well as learning with slowly changing (shifting) comparators. In the special case of classical bandit and semi-bandit problems, we provide optimal small-loss,  high-probability guarantees of $\\tilde{O}(\\sqrt{dL^{\\star}})$ for actual regret, where $d$ is the number of actions, answering open questions of Neu.  Previous bounds for bandits and semi-bandits were known only for pseudo-regret and only in expectation. We also offer an optimal $\\tilde{O}(\\sqrt{\\kappa L^{\\star}})$ regret guarantee for fixed feedback graphs with clique-partition number at most $\\kappa$.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/lykouris18a/lykouris18a.pdf",
        "supp": "",
        "pdf_size": 174724,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8668727022893947425&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Cornell University; Cornell University; Cornell University",
        "aff_domain": "CS.CORNELL.EDU;CS.CORNELL.EDU;CORNELL.EDU",
        "email": "CS.CORNELL.EDU;CS.CORNELL.EDU;CORNELL.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "140ce8a545",
        "title": "Smoothed Online Convex Optimization in High Dimensions via Online Balanced Descent",
        "site": "https://proceedings.mlr.press/v75/chen18b.html",
        "author": "Niangjun Chen; Gautam Goel; Adam Wierman",
        "abstract": "We study \\emph{smoothed online convex optimization}, a version of online convex optimization where the learner incurs a penalty for changing her actions between rounds. Given a $\\Omega(\\sqrt{d})$ lower bound on the competitive ratio of any online algorithm, where $d$ is the dimension of the action space, we ask under what conditions this bound can be beaten. We introduce a novel algorithmic framework for this problem, Online Balanced Descent (OBD), which works by iteratively projecting the previous point onto a carefully chosen level set of the current cost function so as to balance the switching costs and hitting costs. We demonstrate the generality of the OBD framework by showing how, with different choices of \u201cbalance,\u201d OBD can improve upon state-of-the-art performance guarantees for both competitive ratio and regret; in particular, OBD is the first algorithm to achieve a dimension-free competitive ratio, $3 + O(1/\\alpha)$,  for locally polyhedral costs, where $\\alpha$ measures the \u201csteepness\u201d of the costs.  We also prove bounds on the dynamic regret of OBD when the balance is performed in the dual space that are dimension-free and imply that OBD has sublinear static regret.",
        "bibtex": "@InProceedings{pmlr-v75-chen18b,\n  title = \t {Smoothed Online Convex Optimization in High Dimensions via Online Balanced Descent},\n  author =       {Chen, Niangjun and Goel, Gautam and Wierman, Adam},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1574--1594},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/chen18b/chen18b.pdf},\n  url = \t {https://proceedings.mlr.press/v75/chen18b.html},\n  abstract = \t {We study \\emph{smoothed online convex optimization}, a version of online convex optimization where the learner incurs a penalty for changing her actions between rounds. Given a $\\Omega(\\sqrt{d})$ lower bound on the competitive ratio of any online algorithm, where $d$ is the dimension of the action space, we ask under what conditions this bound can be beaten. We introduce a novel algorithmic framework for this problem, Online Balanced Descent (OBD), which works by iteratively projecting the previous point onto a carefully chosen level set of the current cost function so as to balance the switching costs and hitting costs. We demonstrate the generality of the OBD framework by showing how, with different choices of \u201cbalance,\u201d OBD can improve upon state-of-the-art performance guarantees for both competitive ratio and regret; in particular, OBD is the first algorithm to achieve a dimension-free competitive ratio, $3 + O(1/\\alpha)$,  for locally polyhedral costs, where $\\alpha$ measures the \u201csteepness\u201d of the costs.  We also prove bounds on the dynamic regret of OBD when the balance is performed in the dual space that are dimension-free and imply that OBD has sublinear static regret.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/chen18b/chen18b.pdf",
        "supp": "",
        "pdf_size": 328134,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17455911431286001651&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Institute of High Performance Computing; California Institute of Technology; California Institute of Technology",
        "aff_domain": "IHPC.A-STAR.EDU.SG;CALTECH.EDU;CALTECH.EDU",
        "email": "IHPC.A-STAR.EDU.SG;CALTECH.EDU;CALTECH.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Institute of High Performance Computing;California Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ihpc.a-star.edu.sg;https://www.caltech.edu",
        "aff_unique_abbr": "IHPC;Caltech",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pasadena",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "825b997fa2",
        "title": "Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form",
        "site": "https://proceedings.mlr.press/v75/bhojanapalli18a.html",
        "author": "Srinadh Bhojanapalli; Nicolas Boumal; Prateek Jain; Praneeth Netrapalli",
        "abstract": "Semidefinite programs (SDP) are important in learning and combinatorial optimization with numerous applications. In pursuit of low-rank solutions and low complexity algorithms, we consider the Burer\u2013Monteiro factorization approach for solving SDPs. For a large class of SDPs, upon random perturbation of the cost matrix, with high probability, we show that all approximate second-order stationary points are approximate global optima for the penalty formulation of appropriately rank-constrained SDPs, as long as the number of constraints scales sub-quadratically with the desired rank. Our result is based on a simple penalty function formulation of the rank-constrained SDP along with a smoothed analysis to avoid worst-case cost matrices. We particularize our results to two applications, namely, Max-Cut and matrix completion.",
        "bibtex": "@InProceedings{pmlr-v75-bhojanapalli18a,\n  title = \t {Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form},\n  author =       {Bhojanapalli, Srinadh and Boumal, Nicolas and Jain, Prateek and Netrapalli, Praneeth},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {3243--3270},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/bhojanapalli18a/bhojanapalli18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/bhojanapalli18a.html},\n  abstract = \t {Semidefinite programs (SDP) are important in learning and combinatorial optimization with numerous applications. In pursuit of low-rank solutions and low complexity algorithms, we consider the Burer\u2013Monteiro factorization approach for solving SDPs. For a large class of SDPs, upon random perturbation of the cost matrix, with high probability, we show that all approximate second-order stationary points are approximate global optima for the penalty formulation of appropriately rank-constrained SDPs, as long as the number of constraints scales sub-quadratically with the desired rank. Our result is based on a simple penalty function formulation of the rank-constrained SDP along with a smoothed analysis to avoid worst-case cost matrices. We particularize our results to two applications, namely, Max-Cut and matrix completion.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/bhojanapalli18a/bhojanapalli18a.pdf",
        "supp": "",
        "pdf_size": 366044,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7649580264984070959&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "TTI Chicago; Princeton University; Microsoft Research; Microsoft Research",
        "aff_domain": "TTIC.EDU;MATH.PRINCETON.EDU;MICROSOFT.COM;MICROSOFT.COM",
        "email": "TTIC.EDU;MATH.PRINCETON.EDU;MICROSOFT.COM;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;Princeton University;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.princeton.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "TTI;Princeton;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0cd9541220",
        "title": "Subpolynomial trace reconstruction for random strings \\{and arbitrary deletion probability",
        "site": "https://proceedings.mlr.press/v75/holden18a.html",
        "author": "Nina Holden; Robin Pemantle; Yuval Peres",
        "abstract": "The deletion-insertion channel takes as input a bit string ${\\bf x}\\in \\{0,1\\}^{n}$, and outputs a string where bits have been deleted and inserted independently at random. The trace reconstruction problem is to recover $\\bf x$ from many independent outputs (called \u201ctraces\u201d) of the deletion-insertion channel applied to $\\bf x$. We show that if $\\bf x$ is chosen uniformly at random, then $\\exp(O(\\log^{1/3} n))$ traces suffice to reconstruct $\\bf x$ with high probability. For the deletion channel with deletion probability $q<1/2$ the earlier upper bound was $\\exp(O(\\log^{1/2} n))$. The case of $q\\geq 1/2$ or the case where insertions are allowed has not been previously analysed, and therefore the earlier upper bound was as for worst-case strings, i.e., $\\exp(O( n^{1/3}))$. A key ingredient in our proof is a delicate two-step alignment procedure where we estimate the location in each trace corresponding to a given bit of $\\bf x$. The alignment is done by viewing the strings as random walks, and comparing the increments in the walk associated with the input string and the trace, respectively.",
        "bibtex": "@InProceedings{pmlr-v75-holden18a,\n  title = \t {Subpolynomial trace reconstruction for random strings \\\\{and} arbitrary deletion probability},\n  author =       {Holden, Nina and Pemantle, Robin and Peres, Yuval},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1799--1840},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/holden18a/holden18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/holden18a.html},\n  abstract = \t {The deletion-insertion channel takes as input a bit string ${\\bf x}\\in \\{0,1\\}^{n}$, and outputs a string where bits have been deleted and inserted independently at random. The trace reconstruction problem is to recover $\\bf x$ from many independent outputs (called \u201ctraces\u201d) of the deletion-insertion channel applied to $\\bf x$. We show that if $\\bf x$ is chosen uniformly at random, then $\\exp(O(\\log^{1/3} n))$ traces suffice to reconstruct $\\bf x$ with high probability. For the deletion channel with deletion probability $q<1/2$ the earlier upper bound was $\\exp(O(\\log^{1/2} n))$. The case of $q\\geq 1/2$ or the case where insertions are allowed has not been previously analysed, and therefore the earlier upper bound was as for worst-case strings, i.e., $\\exp(O( n^{1/3}))$. A key ingredient in our proof is a delicate two-step alignment procedure where we estimate the location in each trace corresponding to a given bit of $\\bf x$. The alignment is done by viewing the strings as random walks, and comparing the increments in the walk associated with the input string and the trace, respectively.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/holden18a/holden18a.pdf",
        "supp": "",
        "pdf_size": 1127316,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13999574761675203898&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Mathematics, Massachusetts Institute of Technology, Cambridge, MA; Department of Mathematics, University of Pennsylvania, Philadelphia, PA; Microsoft Research, Redmond, WA",
        "aff_domain": "MATH.MIT.EDU;MATH.UPENN.EDU;MICROSOFT.COM",
        "email": "MATH.MIT.EDU;MATH.UPENN.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Pennsylvania;Microsoft",
        "aff_unique_dep": "Department of Mathematics;Department of Mathematics;Microsoft Research",
        "aff_unique_url": "https://web.mit.edu;https://www.upenn.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MIT;UPenn;MSR",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Cambridge;Philadelphia;Redmond",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5d2c046b04",
        "title": "Testing Symmetric Markov Chains From a Single Trajectory",
        "site": "https://proceedings.mlr.press/v75/daskalakis18a.html",
        "author": "Constantinos Daskalakis; Nishanth Dikkala; Nick Gravin",
        "abstract": "The paper\u2019s abstract in valid LaTeX, without non-standard macros or \\cite commands. Classical distribution testing assumes access to i.i.d.\u00a0samples from the distribution that is being tested. We initiate the study of Markov chain  testing, assuming access to a {\\em single trajectory of a Markov Chain.} In particular, we observe a single trajectory $X_0,\\ldots,X_t,\\ldots$ of an unknown, symmetric, and finite state Markov Chain $\\cal M$. We do not control the starting state $X_0$, and we cannot restart the chain. Given our single trajectory, the goal is to test whether  $\\cal M$ is identical to a model Markov Chain ${\\cal M}\u2019$, or far from it under an appropriate notion of difference. We propose a measure of difference between two Markov chains, motivated by the early work of Kazakos [78],  which captures the scaling behavior of the total variation distance between trajectories sampled from the Markov chains as the length of these trajectories grows. We provide efficient testers and information-theoretic lower bounds for testing identity of symmetric Markov chains under our proposed measure of difference, which are tight up to logarithmic factors if the hitting times of the model chain ${\\cal M}\u2019$ is $\\tilde{O}(n)$ in  the size of the state space $n$.",
        "bibtex": "@InProceedings{pmlr-v75-daskalakis18a,\n  title = \t {Testing Symmetric Markov Chains From a Single Trajectory},\n  author =       {Daskalakis, Constantinos and Dikkala, Nishanth and Gravin, Nick},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {385--409},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/daskalakis18a/daskalakis18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/daskalakis18a.html},\n  abstract = \t {The paper\u2019s abstract in valid LaTeX, without non-standard macros or \\cite commands. Classical distribution testing assumes access to i.i.d.\u00a0samples from the distribution that is being tested. We initiate the study of Markov chain  testing, assuming access to a {\\em single trajectory of a Markov Chain.} In particular, we observe a single trajectory $X_0,\\ldots,X_t,\\ldots$ of an unknown, symmetric, and finite state Markov Chain $\\cal M$. We do not control the starting state $X_0$, and we cannot restart the chain. Given our single trajectory, the goal is to test whether  $\\cal M$ is identical to a model Markov Chain ${\\cal M}\u2019$, or far from it under an appropriate notion of difference. We propose a measure of difference between two Markov chains, motivated by the early work of Kazakos [78],  which captures the scaling behavior of the total variation distance between trajectories sampled from the Markov chains as the length of these trajectories grows. We provide efficient testers and information-theoretic lower bounds for testing identity of symmetric Markov chains under our proposed measure of difference, which are tight up to logarithmic factors if the hitting times of the model chain ${\\cal M}\u2019$ is $\\tilde{O}(n)$ in  the size of the state space $n$. }\n}",
        "pdf": "http://proceedings.mlr.press/v75/daskalakis18a/daskalakis18a.pdf",
        "supp": "",
        "pdf_size": 634542,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11966553288902580154&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "EECS, MIT; EECS, MIT; ITCS, SHUFE",
        "aff_domain": "mit.edu;csail.mit.edu;mail.shufe.edu.cn",
        "email": "mit.edu;csail.mit.edu;mail.shufe.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Shanghai University of Finance and Economics",
        "aff_unique_dep": "Electrical Engineering and Computer Science;School of Information Technology and Computational Science",
        "aff_unique_url": "https://www.mit.edu;http://www.shufe.edu.cn",
        "aff_unique_abbr": "MIT;SHUFE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "5ec8532d88",
        "title": "The Externalities of Exploration and How Data Diversity Helps Exploitation",
        "site": "https://proceedings.mlr.press/v75/raghavan18a.html",
        "author": "Manish Raghavan; Aleksandrs Slivkins; Jennifer Vaughan Wortman; Zhiwei Steven Wu",
        "abstract": "Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users in order to gain information that will lead to better decisions in the future.  Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups.  Motivated by these concerns, we initiate the study of the externalities of exploration\u2014the undesirable side effects that the presence of one party may impose on another\u2014under the linear contextual bandits model.  We introduce the notion of a group externality, measuring the extent to which the presence of one population of users (the majority) impacts the rewards of another (the minority). We show that this impact can, in some cases, be negative, and that, in a certain sense, no algorithm can avoid it.  We then move on to study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary.  We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal. We improve on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most $\\tilde{O}(T^{1/3})$. Returning to group-level effects, we show that under the same conditions, negative group externalities essentially vanish if one runs the greedy algorithm. Together, our results uncover a sharp contrast between the high externalities that exist in the worst case, and the ability to remove all externalities if the data is sufficiently diverse.",
        "bibtex": "@InProceedings{pmlr-v75-raghavan18a,\n  title = \t {The Externalities of Exploration and How Data Diversity Helps Exploitation},\n  author =       {Raghavan, Manish and Slivkins, Aleksandrs and Wortman, Jennifer Vaughan and Wu, Zhiwei Steven},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1724--1738},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/raghavan18a/raghavan18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/raghavan18a.html},\n  abstract = \t {Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users in order to gain information that will lead to better decisions in the future.  Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups.  Motivated by these concerns, we initiate the study of the externalities of exploration\u2014the undesirable side effects that the presence of one party may impose on another\u2014under the linear contextual bandits model.  We introduce the notion of a group externality, measuring the extent to which the presence of one population of users (the majority) impacts the rewards of another (the minority). We show that this impact can, in some cases, be negative, and that, in a certain sense, no algorithm can avoid it.  We then move on to study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary.  We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal. We improve on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most $\\tilde{O}(T^{1/3})$. Returning to group-level effects, we show that under the same conditions, negative group externalities essentially vanish if one runs the greedy algorithm. Together, our results uncover a sharp contrast between the high externalities that exist in the worst case, and the ability to remove all externalities if the data is sufficiently diverse.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/raghavan18a/raghavan18a.pdf",
        "supp": "",
        "pdf_size": 284762,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3812661684375128250&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Cornell University; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "CS.CORNELL.EDU;MICROSOFT.COM;MICROSOFT.COM;UMN.EDU",
        "email": "CS.CORNELL.EDU;MICROSOFT.COM;MICROSOFT.COM;UMN.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Cornell University;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.cornell.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Cornell;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ed8ac35c40",
        "title": "The Many Faces of Exponential Weights in Online Learning",
        "site": "https://proceedings.mlr.press/v75/hoeven18a.html",
        "author": "Dirk Hoeven; Tim Erven; Wojciech Kot\u0142owski",
        "abstract": "A standard introduction to online learning might place Online Gradient Descent at its center and then proceed to develop generalizations and extensions like Online Mirror Descent and second-order methods. Here we explore the alternative approach of putting Exponential Weights (EW) first. We show that many standard methods and their regret bounds then follow as a special case by plugging in suitable surrogate losses and playing the EW posterior mean. For instance, we easily recover Online Gradient Descent by using EW with a Gaussian prior on linearized losses, and, more generally, all instances of Online Mirror Descent based on regular Bregman divergences also correspond to EW with a prior that depends on the mirror map. Furthermore, appropriate quadratic surrogate losses naturally give rise to Online Gradient Descent for strongly convex losses and to Online Newton Step. We further interpret several recent adaptive methods (iProd, Squint, and a variation of Coin Betting for experts) as a series of closely related reductions to exp-concave surrogate losses that are then handled by Exponential Weights. Finally, a benefit of our EW interpretation is that it opens up the possibility of sampling from the EW posterior distribution instead of playing the mean. As already observed by Bubeck and Eldan, this recovers the best-known rate in Online Bandit Linear Optimization.",
        "bibtex": "@InProceedings{pmlr-v75-hoeven18a,\n  title = \t {The Many Faces of Exponential Weights in Online Learning},\n  author =       {van der Hoeven, Dirk and van Erven, Tim and Kot{\\l}owski, Wojciech},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {2067--2092},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/hoeven18a/hoeven18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/hoeven18a.html},\n  abstract = \t {A standard introduction to online learning might place Online Gradient Descent at its center and then proceed to develop generalizations and extensions like Online Mirror Descent and second-order methods. Here we explore the alternative approach of putting Exponential Weights (EW) first. We show that many standard methods and their regret bounds then follow as a special case by plugging in suitable surrogate losses and playing the EW posterior mean. For instance, we easily recover Online Gradient Descent by using EW with a Gaussian prior on linearized losses, and, more generally, all instances of Online Mirror Descent based on regular Bregman divergences also correspond to EW with a prior that depends on the mirror map. Furthermore, appropriate quadratic surrogate losses naturally give rise to Online Gradient Descent for strongly convex losses and to Online Newton Step. We further interpret several recent adaptive methods (iProd, Squint, and a variation of Coin Betting for experts) as a series of closely related reductions to exp-concave surrogate losses that are then handled by Exponential Weights. Finally, a benefit of our EW interpretation is that it opens up the possibility of sampling from the EW posterior distribution instead of playing the mean. As already observed by Bubeck and Eldan, this recovers the best-known rate in Online Bandit Linear Optimization.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/hoeven18a/hoeven18a.pdf",
        "supp": "",
        "pdf_size": 463043,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8171784865091275670&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Statistics Department, Leiden University, the Netherlands; Statistics Department, Leiden University, the Netherlands; Institute of Computing Science, Pozna \u00b4n University of Technology, Poland",
        "aff_domain": "GMAIL.COM;TIMVANERVEN.NL;CS.PUT.POZNAN.PL",
        "email": "GMAIL.COM;TIMVANERVEN.NL;CS.PUT.POZNAN.PL",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Leiden University;Pozna\u0144 University of Technology",
        "aff_unique_dep": "Statistics Department;Institute of Computing Science",
        "aff_unique_url": "https://www.universiteitleiden.nl;https://www.put.poznan.pl/",
        "aff_unique_abbr": "LU;PUT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pozna\u0144",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Netherlands;Poland"
    },
    {
        "id": "7dd1caf3d5",
        "title": "The Mean-Field Approximation: Information Inequalities, Algorithms, and Complexity",
        "site": "https://proceedings.mlr.press/v75/jain18b.html",
        "author": "Vishesh Jain; Frederic Koehler; Elchanan Mossel",
        "abstract": "The mean field approximation to the Ising model is a canonical variational tool that is used for analysis and inference in Ising models. We provide a simple and optimal bound for the KL error of the mean field approximation for Ising models on general graphs, and extend it to higher order Markov random fields. Our bound improves on previous bounds obtained in work in the graph limit literature by Borgs, Chayes, Lov{\u00e1}sz, S{\u00f3}s, and Vesztergombi and recent works by Basak and Mukherjee, and Eldan. Our bound is tight up to lower order terms.  Building on the methods used to prove the bound, along with techniques from combinatorics and optimization,  we study the algorithmic problem of estimating the (variational) free energy for Ising models and general Markov random fields. For a graph $G$ on $n$ vertices and interaction matrix $J$ with Frobenius norm $\\|{J} \\|_F$, we provide algorithms that approximate the free energy within an additive error of $\\epsilon n \\|J\\|_F$ in time $\\exp(poly(1/\\epsilon))$. We also show that approximation within $(n \\|J\\|_F)^{1-\\delta}$ is NP-hard for every $\\delta > 0$. Finally, we provide more efficient approximation algorithms, which find the optimal mean field approximation, for ferromagnetic Ising models and for Ising models satisfying Dobrushin\u2019s condition.",
        "bibtex": "@InProceedings{pmlr-v75-jain18b,\n  title = \t {The Mean-Field Approximation: Information Inequalities, Algorithms, and Complexity},\n  author =       {Jain, Vishesh and Koehler, Frederic and Mossel, Elchanan},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1326--1347},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/jain18b/jain18b.pdf},\n  url = \t {https://proceedings.mlr.press/v75/jain18b.html},\n  abstract = \t {The mean field approximation to the Ising model is a canonical variational tool that is used for analysis and inference in Ising models. We provide a simple and optimal bound for the KL error of the mean field approximation for Ising models on general graphs, and extend it to higher order Markov random fields. Our bound improves on previous bounds obtained in work in the graph limit literature by Borgs, Chayes, Lov{\u00e1}sz, S{\u00f3}s, and Vesztergombi and recent works by Basak and Mukherjee, and Eldan. Our bound is tight up to lower order terms.  Building on the methods used to prove the bound, along with techniques from combinatorics and optimization,  we study the algorithmic problem of estimating the (variational) free energy for Ising models and general Markov random fields. For a graph $G$ on $n$ vertices and interaction matrix $J$ with Frobenius norm $\\|{J} \\|_F$, we provide algorithms that approximate the free energy within an additive error of $\\epsilon n \\|J\\|_F$ in time $\\exp(poly(1/\\epsilon))$. We also show that approximation within $(n \\|J\\|_F)^{1-\\delta}$ is NP-hard for every $\\delta > 0$. Finally, we provide more efficient approximation algorithms, which find the optimal mean field approximation, for ferromagnetic Ising models and for Ising models satisfying Dobrushin\u2019s condition.   }\n}",
        "pdf": "http://proceedings.mlr.press/v75/jain18b/jain18b.pdf",
        "supp": "",
        "pdf_size": 350406,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4363516495923883276&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Massachusetts Institute of Technology. Department of Mathematics; Massachusetts Institute of Technology. Department of Mathematics; Massachusetts Institute of Technology. Department of Mathematics and IDSS",
        "aff_domain": "MIT.EDU;MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f2d8d99048",
        "title": "The Vertex Sample Complexity of Free Energy is Polynomial",
        "site": "https://proceedings.mlr.press/v75/jain18c.html",
        "author": "Vishesh Jain; Frederic Koehler; Elchanan Mossel",
        "abstract": "The free energy is a key quantity which is associated to Markov random fields. Classical results in statistical physics show how, given an analytic formula of the free energy, it is possible to compute many key quantities associated with Markov random fields including quantities such as magnetization and the location of various phase transitions.  Given a massive Markov random field on $n$ nodes, can a small sample from it provide a rough approximation to the free energy $\\mathcal{F}_n = \\log{Z_n}$?  Results in the graph limit literature by Borgs, Chayes, Lov{\u00e1}sz, S{\u00f3}s, and Vesztergombi show that  for Ising models on $n$ nodes and interactions of strength $\\Theta(1/n)$, an $\\epsilon$ approximation to $\\log Z_n / n$ can be achieved by sampling a randomly induced model on $2^{O(1/\\epsilon^2)}$ nodes. We show that the sampling complexity of this problem is {\\em polynomial in }$1/\\epsilon$. We further show a polynomial dependence on $\\epsilon$ cannot be avoided.  Our results are very general as they apply to higher order Markov random fields. For Markov random fields of order $r$, we obtain an algorithm that achieves $\\epsilon$ approximation using a number of samples polynomial in $r$ and $1/\\epsilon$ and running time that is $2^{O(1/\\epsilon^2)}$ up to polynomial factors in $r$ and $\\epsilon$. For ferromagnetic Ising models, the running time is polynomial in $1/\\epsilon$.  Our results are intimately connected to recent research on the regularity lemma and property testing, where the interest is in finding which properties can tested within $\\epsilon$ error in time polynomial in $1/\\epsilon$. In particular, our proofs build on results of Alon, de la Vega, Kannan and Karpinski, who also introduced the notion of polynomial vertex sample complexity. Another critical ingredient of the proof is an effective bound by the authors of this paper relating the variational free energy and the free energy.",
        "bibtex": "@InProceedings{pmlr-v75-jain18c,\n  title = \t {The Vertex Sample Complexity of Free Energy is Polynomial},\n  author =       {Jain, Vishesh and Koehler, Frederic and Mossel, Elchanan},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {1395--1419},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/jain18c/jain18c.pdf},\n  url = \t {https://proceedings.mlr.press/v75/jain18c.html},\n  abstract = \t {The free energy is a key quantity which is associated to Markov random fields. Classical results in statistical physics show how, given an analytic formula of the free energy, it is possible to compute many key quantities associated with Markov random fields including quantities such as magnetization and the location of various phase transitions.  Given a massive Markov random field on $n$ nodes, can a small sample from it provide a rough approximation to the free energy $\\mathcal{F}_n = \\log{Z_n}$?  Results in the graph limit literature by Borgs, Chayes, Lov{\u00e1}sz, S{\u00f3}s, and Vesztergombi show that  for Ising models on $n$ nodes and interactions of strength $\\Theta(1/n)$, an $\\epsilon$ approximation to $\\log Z_n / n$ can be achieved by sampling a randomly induced model on $2^{O(1/\\epsilon^2)}$ nodes. We show that the sampling complexity of this problem is {\\em polynomial in }$1/\\epsilon$. We further show a polynomial dependence on $\\epsilon$ cannot be avoided.  Our results are very general as they apply to higher order Markov random fields. For Markov random fields of order $r$, we obtain an algorithm that achieves $\\epsilon$ approximation using a number of samples polynomial in $r$ and $1/\\epsilon$ and running time that is $2^{O(1/\\epsilon^2)}$ up to polynomial factors in $r$ and $\\epsilon$. For ferromagnetic Ising models, the running time is polynomial in $1/\\epsilon$.  Our results are intimately connected to recent research on the regularity lemma and property testing, where the interest is in finding which properties can tested within $\\epsilon$ error in time polynomial in $1/\\epsilon$. In particular, our proofs build on results of Alon, de la Vega, Kannan and Karpinski, who also introduced the notion of polynomial vertex sample complexity. Another critical ingredient of the proof is an effective bound by the authors of this paper relating the variational free energy and the free energy.  }\n}",
        "pdf": "http://proceedings.mlr.press/v75/jain18c/jain18c.pdf",
        "supp": "",
        "pdf_size": 390705,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11260185169123834938&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Massachusetts Institute of Technology. Department of Mathematics; Massachusetts Institute of Technology. Department of Mathematics; Massachusetts Institute of Technology. Department of Mathematics and IDSS",
        "aff_domain": "MIT.EDU;MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a8eb20ef95",
        "title": "Time-Space Tradeoffs for Learning Finite Functions from Random Evaluations, with Applications to Polynomials",
        "site": "https://proceedings.mlr.press/v75/beame18a.html",
        "author": "Paul Beame; Shayan Oveis Gharan; Xin Yang",
        "abstract": "We develop an extension of recent analytic methods for  obtaining time-space tradeoff lower bounds for problems of learning  from uniformly random labelled examples.  With our methods we can obtain bounds for learning concept classes of finite functions from random evaluations even when the sample space of random inputs can be significantly smaller than the concept class of functions and the function values can be from an arbitrary finite set. At the core of our results, we reduce the time-space complexity of learning from random evaluations to the question of how much the corresponding evaluation matrix amplifies the 2-norms of \u201calmost uniform\u201d probability distributions. To analyze the latter, we formulate it as a semidefinite program, and we analyze its dual.   In order to handle function values from arbitrary finite sets, we apply this norm amplification analysis to complex matrices. As applications that follow from our new techniques, we show that any algorithm that learns $n$-variate  polynomial functions of degree at most $d$ over $\\mathbb{F}_2$ with success at least $2^{-O(n)}$ from evaluations on randomly chosen inputs either requires space $\\Omega(nm/d)$ or $2^{\\Omega(n/d)}$ time where $m=(n/d)^{\\Theta(d)}$ is the dimension of the space of such polynomials.   These bounds are asymptotically optimal for polynomials of arbitrary constant degree since they match the tradeoffs achieved by natural learning algorithms for the problems. We extend these results to learning polynomials of degree at most $d$ over any odd prime field $\\mathbb{F}_p$ where we show that $\\Omega((mn/d)\\log p)$ space or time $p^{\\Omega(n/d)}$ is required. To derive our bounds for learning polynomials over finite fields, we show that an analysis of the dual of the corresponding semidefinite program follows from an understanding of the distribution of the bias of all degree $d$ polynomials with respect to uniformly random inputs.",
        "bibtex": "@InProceedings{pmlr-v75-beame18a,\n  title = \t {Time-Space Tradeoffs for Learning Finite Functions from Random Evaluations, with Applications to Polynomials},\n  author =       {Beame, Paul and Oveis Gharan, Shayan and Yang, Xin},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {843--856},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/beame18a/beame18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/beame18a.html},\n  abstract = \t {We develop an extension of recent analytic methods for  obtaining time-space tradeoff lower bounds for problems of learning  from uniformly random labelled examples.  With our methods we can obtain bounds for learning concept classes of finite functions from random evaluations even when the sample space of random inputs can be significantly smaller than the concept class of functions and the function values can be from an arbitrary finite set. At the core of our results, we reduce the time-space complexity of learning from random evaluations to the question of how much the corresponding evaluation matrix amplifies the 2-norms of \u201calmost uniform\u201d probability distributions. To analyze the latter, we formulate it as a semidefinite program, and we analyze its dual.   In order to handle function values from arbitrary finite sets, we apply this norm amplification analysis to complex matrices. As applications that follow from our new techniques, we show that any algorithm that learns $n$-variate  polynomial functions of degree at most $d$ over $\\mathbb{F}_2$ with success at least $2^{-O(n)}$ from evaluations on randomly chosen inputs either requires space $\\Omega(nm/d)$ or $2^{\\Omega(n/d)}$ time where $m=(n/d)^{\\Theta(d)}$ is the dimension of the space of such polynomials.   These bounds are asymptotically optimal for polynomials of arbitrary constant degree since they match the tradeoffs achieved by natural learning algorithms for the problems. We extend these results to learning polynomials of degree at most $d$ over any odd prime field $\\mathbb{F}_p$ where we show that $\\Omega((mn/d)\\log p)$ space or time $p^{\\Omega(n/d)}$ is required. To derive our bounds for learning polynomials over finite fields, we show that an analysis of the dual of the corresponding semidefinite program follows from an understanding of the distribution of the bias of all degree $d$ polynomials with respect to uniformly random inputs.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/beame18a/beame18a.pdf",
        "supp": "",
        "pdf_size": 246687,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15724114414682975451&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Washington; University of Washington; University of Washington",
        "aff_domain": "CS.WASHINGTON.EDU;CS.WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "email": "CS.WASHINGTON.EDU;CS.WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c16b69e283",
        "title": "Underdamped Langevin MCMC: A non-asymptotic analysis",
        "site": "https://proceedings.mlr.press/v75/cheng18a.html",
        "author": "Xiang Cheng; Niladri S. Chatterji; Peter L. Bartlett; Michael I. Jordan",
        "abstract": "We study the underdamped Langevin diffusion when the log of the target distribution is smooth and strongly concave. We present a MCMC algorithm based on its discretization and show that it achieves $\\varepsilon$ error (in 2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a significant improvement over the best known rate for overdamped Langevin MCMC, which is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same smoothness/concavity assumptions. The underdamped Langevin MCMC scheme can be viewed as a version of Hamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped Langevin MCMC methods in a number of application areas. We provide quantitative rates that support this empirical wisdom.",
        "bibtex": "@InProceedings{pmlr-v75-cheng18a,\n  title = \t {Underdamped Langevin MCMC: A non-asymptotic analysis},\n  author =       {Cheng, Xiang and Chatterji, Niladri S. and Bartlett, Peter L. and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {300--323},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/cheng18a/cheng18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/cheng18a.html},\n  abstract = \t {We study the underdamped Langevin diffusion when the log of the target distribution is smooth and strongly concave. We present a MCMC algorithm based on its discretization and show that it achieves $\\varepsilon$ error (in 2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a significant improvement over the best known rate for overdamped Langevin MCMC, which is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same smoothness/concavity assumptions. The underdamped Langevin MCMC scheme can be viewed as a version of Hamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped Langevin MCMC methods in a number of application areas. We provide quantitative rates that support this empirical wisdom.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/cheng18a/cheng18a.pdf",
        "supp": "",
        "pdf_size": 164849,
        "gs_citation": 379,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7967069085924733867&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of California Berkeley & Adobe Research; University of California Berkeley; University of California Berkeley; University of California Berkeley",
        "aff_domain": "BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU;CS.BERKELEY.EDU",
        "email": "BERKELEY.EDU;BERKELEY.EDU;BERKELEY.EDU;CS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8aadb48486",
        "title": "Unleashing Linear Optimizers for Group-Fair Learning and Optimization",
        "site": "https://proceedings.mlr.press/v75/alabi18a.html",
        "author": "Daniel Alabi; Nicole Immorlica; Adam Kalai",
        "abstract": "Most systems and learning algorithms optimize average performance or average loss \u2013 one reason being computational complexity. However, many objectives of practical interest are more complex than simply average loss. This arises, for example, when balancing performance or loss with fairness across people. We prove that, from a computational perspective, optimizing arbitrary objectives that take into account performance over a small number of groups is not significantly harder to optimize than average performance. Our main result is a polynomial-time reduction that uses a linear optimizer to optimize an arbitrary (Lipschitz continuous) function of performance over a (constant) number of possibly-overlapping groups. This includes fairness objectives over small numbers of groups, and we further point out that other existing notions of fairness such as individual fairness can be cast as convex optimization and hence more standard convex techniques can be used. Beyond learning, our approach applies to multi-objective optimization, more generally.",
        "bibtex": "@InProceedings{pmlr-v75-alabi18a,\n  title = \t {Unleashing Linear Optimizers for Group-Fair Learning and Optimization},\n  author =       {Alabi, Daniel and Immorlica, Nicole and Kalai, Adam},\n  booktitle = \t {Proceedings of the 31st  Conference On Learning Theory},\n  pages = \t {2043--2066},\n  year = \t {2018},\n  editor = \t {Bubeck, S\u00e9bastien and Perchet, Vianney and Rigollet, Philippe},\n  volume = \t {75},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {06--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v75/alabi18a/alabi18a.pdf},\n  url = \t {https://proceedings.mlr.press/v75/alabi18a.html},\n  abstract = \t {Most systems and learning algorithms optimize average performance or average loss \u2013 one reason being computational complexity. However, many objectives of practical interest are more complex than simply average loss. This arises, for example, when balancing performance or loss with fairness across people. We prove that, from a computational perspective, optimizing arbitrary objectives that take into account performance over a small number of groups is not significantly harder to optimize than average performance. Our main result is a polynomial-time reduction that uses a linear optimizer to optimize an arbitrary (Lipschitz continuous) function of performance over a (constant) number of possibly-overlapping groups. This includes fairness objectives over small numbers of groups, and we further point out that other existing notions of fairness such as individual fairness can be cast as convex optimization and hence more standard convex techniques can be used. Beyond learning, our approach applies to multi-objective optimization, more generally.}\n}",
        "pdf": "http://proceedings.mlr.press/v75/alabi18a/alabi18a.pdf",
        "supp": "",
        "pdf_size": 486076,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4216272633481155642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Harvard University, Cambridge, MA; Microsoft Research New England, Cambridge, MA; Microsoft Research New England, Cambridge, MA",
        "aff_domain": "g.harvard.edu;microsoft.com;microsoft.com",
        "email": "g.harvard.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Harvard University;Microsoft",
        "aff_unique_dep": ";New England",
        "aff_unique_url": "https://www.harvard.edu;https://www.microsoft.com/en-us/research/group/new-england",
        "aff_unique_abbr": "Harvard;MSR NE",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    }
]