[
    {
        "id": "c2d0580604",
        "title": "Accelerated Coordinate Descent with Adaptive Coordinate Frequencies",
        "site": "https://proceedings.mlr.press/v29/Glasmachers13.html",
        "author": "Tobias Glasmachers; Urun Dogan",
        "abstract": "Coordinate descent (CD) algorithms have become the method of choice for solving a number of machine learning tasks. They are particularly popular for training linear models, including linear support vector machine classification, LASSO regression, and logistic regression. We propose an extension of the CD algorithm, called the adaptive coordinate frequencies (ACF) method. This modified CD scheme does not treat all coordinates equally, in that it does not pick all coordinates equally often for optimization. Instead the relative frequencies of coordinates are subject to online adaptation. The resulting optimization scheme can result in significant speed-ups. We demonstrate the usefulness of our approach on a number of large scale machine learning problems.",
        "bibtex": "@InProceedings{pmlr-v29-Glasmachers13,\n  title = \t {Accelerated Coordinate Descent with Adaptive Coordinate Frequencies},\n  author = \t {Glasmachers, Tobias and Dogan, Urun},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {72--86},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Glasmachers13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Glasmachers13.html},\n  abstract = \t {Coordinate descent (CD) algorithms have become the method of choice for solving a number of machine learning tasks. They are particularly popular for training linear models, including linear support vector machine classification, LASSO regression, and logistic regression. We propose an extension of the CD algorithm, called the adaptive coordinate frequencies (ACF) method. This modified CD scheme does not treat all coordinates equally, in that it does not pick all coordinates equally often for optimization. Instead the relative frequencies of coordinates are subject to online adaptation. The resulting optimization scheme can result in significant speed-ups. We demonstrate the usefulness of our approach on a number of large scale machine learning problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Glasmachers13.pdf",
        "supp": "",
        "pdf_size": 321542,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=679583242133648328&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Institut f\u00fcr Neuroinformatik, Ruhr-Universit\u00e4t Bochum, Germany; Institut f\u00fcr Mathematik, Universit\u00e4t Potsdam, Germany",
        "aff_domain": "ini.rub.de;math.uni-potsdam.de",
        "email": "ini.rub.de;math.uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Ruhr-Universit\u00e4t Bochum;Universit\u00e4t Potsdam",
        "aff_unique_dep": "Institut f\u00fcr Neuroinformatik;Institut f\u00fcr Mathematik",
        "aff_unique_url": "https://www.ruhr-uni-bochum.de;https://www.uni-potsdam.de",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2df56b1afc",
        "title": "Achievability of Asymptotic Minimax Regret in Online and Batch Prediction",
        "site": "https://proceedings.mlr.press/v29/Watanabe13.html",
        "author": "Kazuho Watanabe; Teemu Roos; Petri Myllym\u00e4ki",
        "abstract": "The normalized maximum likelihood model achieves the minimax coding (log-loss) regret for data of fixed sample size n. However, it is a batch strategy, i.e., it requires that n be known in advance. Furthermore, it is computationally infeasible for most statistical models, and several computationally feasible alternative strategies have been devised. We characterize the achievability of asymptotic minimaxity by batch strategies (i.e., strategies that depend on n) as well as online strategies (i.e., strategies independent of n). On one hand, we conjecture that for a large class of models, no online strategy can be asymptotically minimax. We prove that this holds under a slightly stronger definition of asymptotic minimaxity. Our numerical experiments support the conjecture about non-achievability by so called last-step minimax algorithms, which are independent of n.  On the other hand, we show that in the multinomial model, a Bayes mixture defined by the conjugate Dirichlet prior with a simple dependency on n achieves asymptotic minimaxity for all sequences, thus providing a simpler asymptotic minimax strategy compared to earlier work by Xie and Barron. The numerical results also demonstrate superior finite-sample behavior by a number of novel batch and online algorithms.",
        "bibtex": "@InProceedings{pmlr-v29-Watanabe13,\n  title = \t {Achievability of Asymptotic Minimax Regret in Online and Batch Prediction},\n  author = \t {Watanabe, Kazuho and Roos, Teemu and Myllym\u00e4ki, Petri},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {181--196},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Watanabe13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Watanabe13.html},\n  abstract = \t {The normalized maximum likelihood model achieves the minimax coding (log-loss) regret for data of fixed sample size n. However, it is a batch strategy, i.e., it requires that n be known in advance. Furthermore, it is computationally infeasible for most statistical models, and several computationally feasible alternative strategies have been devised. We characterize the achievability of asymptotic minimaxity by batch strategies (i.e., strategies that depend on n) as well as online strategies (i.e., strategies independent of n). On one hand, we conjecture that for a large class of models, no online strategy can be asymptotically minimax. We prove that this holds under a slightly stronger definition of asymptotic minimaxity. Our numerical experiments support the conjecture about non-achievability by so called last-step minimax algorithms, which are independent of n.  On the other hand, we show that in the multinomial model, a Bayes mixture defined by the conjugate Dirichlet prior with a simple dependency on n achieves asymptotic minimaxity for all sequences, thus providing a simpler asymptotic minimax strategy compared to earlier work by Xie and Barron. The numerical results also demonstrate superior finite-sample behavior by a number of novel batch and online algorithms. }\n}",
        "pdf": "http://proceedings.mlr.press/v29/Watanabe13.pdf",
        "supp": "",
        "pdf_size": 365716,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3371958506494620383&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Graduate School of Information Science, Nara Institute of Science and Technology; Department of Computer Science, University of Helsinki + Helsinki Institute for Information Technology HIIT; Department of Computer Science, University of Helsinki + Helsinki Institute for Information Technology HIIT",
        "aff_domain": "is.naist.jp;cs.helsinki.fi;cs.helsinki.fi",
        "email": "is.naist.jp;cs.helsinki.fi;cs.helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1+2",
        "aff_unique_norm": "Nara Institute of Science and Technology;University of Helsinki;Helsinki Institute for Information Technology",
        "aff_unique_dep": "Graduate School of Information Science;Department of Computer Science;HIIT",
        "aff_unique_url": "https://www.nist.go.jp;https://www.helsinki.fi;https://www.hiit.fi",
        "aff_unique_abbr": "NIST;UH;HIIT",
        "aff_campus_unique_index": "0;;",
        "aff_campus_unique": "Nara;",
        "aff_country_unique_index": "0;1+1;1+1",
        "aff_country_unique": "Japan;Finland"
    },
    {
        "id": "bfc3ea208e",
        "title": "Active Sampling of Pairs and Points for Large-scale Linear Bipartite Ranking",
        "site": "https://proceedings.mlr.press/v29/Shen13.html",
        "author": "Wei-Yuan Shen; Hsuan-Tien Lin",
        "abstract": "Bipartite ranking is a fundamental ranking problem that learns to order relevant instances ahead of irrelevant ones. One major approach for bipartite ranking, called the pair-wise approach, tackles an equivalent binary classification problem of whether one instance out of a pair of instances should be ranked higher than the other. Nevertheless, the number of instance pairs constructed from the input data could be quadratic to the size of the input data, which makes pair-wise ranking generally infeasible on large-scale data sets. Another major approach for bipartite ranking, called the point-wise approach, directly solves a binary classification problem between relevant and irrelevant instance points. This approach is feasible for large-scale data sets, but the resulting ranking performance can be inferior. That is, it is difficult to conduct bipartite ranking accurately and efficiently at the same time. In this paper, we develop a novel scheme within the pair-wise approach to conduct bipartite ranking efficiently. The scheme, called Active Sampling, is inspired from the rich field of active learning and can reach a competitive ranking performance while focusing only on a small subset of the many pairs during training. Moreover, we propose a general Combined Ranking and Classification (CRC) framework to accurately conduct bipartite ranking. The framework unifies point-wise and pair-wise approaches and is simply based on the idea of treating each instance point as a pseudo-pair. Experiments on 14 real- word large-scale data sets demonstrate that the proposed algorithm of Active Sampling within CRC, when coupled with a linear Support Vector Machine, usually outperforms state-of-the-art point-wise and pair-wise ranking approaches in terms of both accuracy and efficiency.",
        "bibtex": "@InProceedings{pmlr-v29-Shen13,\n  title = \t {Active Sampling of Pairs and Points for Large-scale Linear Bipartite Ranking},\n  author = \t {Shen, Wei-Yuan and Lin, Hsuan-Tien},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {388--403},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Shen13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Shen13.html},\n  abstract = \t {Bipartite ranking is a fundamental ranking problem that learns to order relevant instances ahead of irrelevant ones. One major approach for bipartite ranking, called the pair-wise approach, tackles an equivalent binary classification problem of whether one instance out of a pair of instances should be ranked higher than the other. Nevertheless, the number of instance pairs constructed from the input data could be quadratic to the size of the input data, which makes pair-wise ranking generally infeasible on large-scale data sets. Another major approach for bipartite ranking, called the point-wise approach, directly solves a binary classification problem between relevant and irrelevant instance points. This approach is feasible for large-scale data sets, but the resulting ranking performance can be inferior. That is, it is difficult to conduct bipartite ranking accurately and efficiently at the same time. In this paper, we develop a novel scheme within the pair-wise approach to conduct bipartite ranking efficiently. The scheme, called Active Sampling, is inspired from the rich field of active learning and can reach a competitive ranking performance while focusing only on a small subset of the many pairs during training. Moreover, we propose a general Combined Ranking and Classification (CRC) framework to accurately conduct bipartite ranking. The framework unifies point-wise and pair-wise approaches and is simply based on the idea of treating each instance point as a pseudo-pair. Experiments on 14 real- word large-scale data sets demonstrate that the proposed algorithm of Active Sampling within CRC, when coupled with a linear Support Vector Machine, usually outperforms state-of-the-art point-wise and pair-wise ranking approaches in terms of both accuracy and efficiency.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Shen13.pdf",
        "supp": "",
        "pdf_size": 368397,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16217373722442913795&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan University; Department of Computer Science and Information Engineering, National Taiwan University",
        "aff_domain": "csie.ntu.edu.tw;csie.ntu.edu.tw",
        "email": "csie.ntu.edu.tw;csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "Department of Computer Science and Information Engineering",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2b2ba2fb74",
        "title": "Aggregating Predictions via Sequential Mini-Trading",
        "site": "https://proceedings.mlr.press/v29/Premachandra13.html",
        "author": "Mindika Premachandra; Mark Reid",
        "abstract": "Prediction markets which trade on contracts representing unknown future outcomes are designed specifically to aggregate expert predictions via the market price. While there are some existing machine learning interpretations for the market price and connections to Bayesian updating under the equilibrium analysis of such markets, there is less of an understanding of what the instantaneous price in sequentially traded markets means. In this paper we show that the prices generated in sequentially traded prediction markets are stochastic approximations to the price given by an equilibrium analysis. We do so by showing the equilibrium price is a solution to a stochastic optimisation problem which is solved by stochastic mirror descent (SMD) by a class of sequential pricing mechanisms. This connection leads us to propose a scheme called \u201cmini-trading\u201d which introduces a parameter related to the learning rate in SMD. We prove several properties of this scheme and show that it can improve the stability of prices in sequentially traded prediction markets.",
        "bibtex": "@InProceedings{pmlr-v29-Premachandra13,\n  title = \t {Aggregating Predictions via Sequential Mini-Trading},\n  author = \t {Premachandra, Mindika and Reid, Mark},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {373--387},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Premachandra13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Premachandra13.html},\n  abstract = \t {Prediction markets which trade on contracts representing unknown future outcomes are designed specifically to aggregate expert predictions via the market price. While there are some existing machine learning interpretations for the market price and connections to Bayesian updating under the equilibrium analysis of such markets, there is less of an understanding of what the instantaneous price in sequentially traded markets means. In this paper we show that the prices generated in sequentially traded prediction markets are stochastic approximations to the price given by an equilibrium analysis. We do so by showing the equilibrium price is a solution to a stochastic optimisation problem which is solved by stochastic mirror descent (SMD) by a class of sequential pricing mechanisms. This connection leads us to propose a scheme called \u201cmini-trading\u201d which introduces a parameter related to the learning rate in SMD. We prove several properties of this scheme and show that it can improve the stability of prices in sequentially traded prediction markets.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Premachandra13.pdf",
        "supp": "",
        "pdf_size": 369505,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13732164351403053046&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "The Australian National University & NICTA, Canberra, Australia; The Australian National University & NICTA, Canberra, Australia",
        "aff_domain": "anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "148ba0eb31",
        "title": "Co-Training with Insufficient Views",
        "site": "https://proceedings.mlr.press/v29/Wang13b.html",
        "author": "Wei Wang; Zhi-Hua Zhou",
        "abstract": "Co-training is a famous semi-supervised learning paradigm exploiting unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sufficient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sufficient. We define the diversity between the two views with respect to the confidence of prediction and prove that if the two views have large diversity, co-training is able to improve the learning performance by exploiting unlabeled data even with insufficient views. We also discuss the relationship between view insufficiency and diversity, and give some implications for understanding of the difference between co-training and co-regularization.",
        "bibtex": "@InProceedings{pmlr-v29-Wang13b,\n  title = \t {Co-Training with Insufficient Views},\n  author = \t {Wang, Wei and Zhou, Zhi-Hua},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {467--482},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Wang13b.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Wang13b.html},\n  abstract = \t {Co-training is a famous semi-supervised learning paradigm exploiting unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sufficient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sufficient. We define the diversity between the two views with respect to the confidence of prediction and prove that if the two views have large diversity, co-training is able to improve the learning performance by exploiting unlabeled data even with insufficient views. We also discuss the relationship between view insufficiency and diversity, and give some implications for understanding of the difference between co-training and co-regularization.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Wang13b.pdf",
        "supp": "",
        "pdf_size": 313427,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15770640141362228668&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China",
        "aff_domain": "lamda.nju.edu.cn;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "12fa07dbd1",
        "title": "Coinciding Walk Kernels: Parallel Absorbing Random Walks for Learning with Graphs and Few Labels",
        "site": "https://proceedings.mlr.press/v29/Neumann13.html",
        "author": "Marion Neumann; Roman Garnett; Kristian Kersting",
        "abstract": "Exploiting autocorrelation for node-label prediction in networked data has led to great success. However, when dealing with sparsely labeled networks, common in present-day tasks, the autocorrelation assumption is difficult to exploit. Taking a step beyond, we propose the coinciding walk kernel (cwk), a novel kernel leveraging label-structure similarity \u2013 the idea that nodes with similarly arranged labels in their local neighbourhoods are likely to have the same label \u2013 for learning problems on partially labeled graphs. Inspired by the success of random walk based schemes for the construction of graph kernels, cwk is defined in terms of the probability that the labels encountered during parallel random walks coincide. In addition to its intuitive probabilistic interpretation, coinciding walk kernels outperform existing kernel- and walk-based methods on the task of node-label prediction in sparsely labeled graphs with high label-structure similarity. We also show that computing cwks is faster than many state-of-the-art kernels on graphs. We evaluate cwks on several real- world networks, including cocitation and coauthor graphs, as well as a graph of interlinked populated places extracted from the dbpedia knowledge base.",
        "bibtex": "@InProceedings{pmlr-v29-Neumann13,\n  title = \t {Coinciding Walk Kernels: Parallel Absorbing Random Walks for Learning with Graphs and Few Labels},\n  author = \t {Neumann, Marion and Garnett, Roman and Kersting, Kristian},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {357--372},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Neumann13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Neumann13.html},\n  abstract = \t {Exploiting autocorrelation for node-label prediction in networked data has led to great success. However, when dealing with sparsely labeled networks, common in present-day tasks, the autocorrelation assumption is difficult to exploit. Taking a step beyond, we propose the coinciding walk kernel (cwk), a novel kernel leveraging label-structure similarity \u2013 the idea that nodes with similarly arranged labels in their local neighbourhoods are likely to have the same label \u2013 for learning problems on partially labeled graphs. Inspired by the success of random walk based schemes for the construction of graph kernels, cwk is defined in terms of the probability that the labels encountered during parallel random walks coincide. In addition to its intuitive probabilistic interpretation, coinciding walk kernels outperform existing kernel- and walk-based methods on the task of node-label prediction in sparsely labeled graphs with high label-structure similarity. We also show that computing cwks is faster than many state-of-the-art kernels on graphs. We evaluate cwks on several real- world networks, including cocitation and coauthor graphs, as well as a graph of interlinked populated places extracted from the dbpedia knowledge base.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Neumann13.pdf",
        "supp": "",
        "pdf_size": 1766278,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7656527200736037211&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of Bonn, Germany; University of Bonn, Germany; Technical University of Dortmund, Germany",
        "aff_domain": "uni-bonn.de;uni-bonn.de;cs.tu-dortmund.de",
        "email": "uni-bonn.de;uni-bonn.de;cs.tu-dortmund.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Bonn;Technical University of Dortmund",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-bonn.de;https://www.tu-dortmund.de",
        "aff_unique_abbr": "UBonn;TUDo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "f1b04c6374",
        "title": "EPMC: Every Visit Preference Monte Carlo for Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v29/Wirth13.html",
        "author": "Christian Wirth; Johannes F\u00fcrnkranz",
        "abstract": "Reinforcement learning algorithms are usually hard to use for non expert users. It is required to consider several aspects like the definition of state-, action- and reward-space as well as the algorithms hyperparameters. Preference based approaches try to address these problems by omitting the requirement for exact rewards, replacing them with preferences over solutions. Some algorithms have been proposed within this framework, but they are usually requiring parameterized policies which is again a hinderance for their application. Monte Carlo based approaches do not have this restriction and are also model free. Hence, we present a new preference-based reinforcement learning algorithm, utilizing Monte Carlo estimates. The main idea is to estimate the relative Q-value of two actions for the same state within a every-visit framework. This means, preferences are used to estimate the Q-value of state-action pairs within a trajectory, based on the feedback concerning the complete trajectory. The algorithm is evaluated on three common benchmark problems, namely mountain car, inverted pendulum and acrobot, showing its advantage over a closely related algorithm which is also using estimates for intermediate states, but based on a probability theorem. In comparison to SARSA(\u03bb), EPMC converges somewhat slower, but computes policies that are almost as good or better.",
        "bibtex": "@InProceedings{pmlr-v29-Wirth13,\n  title = \t {EPMC: Every Visit Preference Monte Carlo for Reinforcement Learning},\n  author = \t {Wirth, Christian and F\u00fcrnkranz, Johannes},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {483--497},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Wirth13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Wirth13.html},\n  abstract = \t {Reinforcement learning algorithms are usually hard to use for non expert users. It is required to consider several aspects like the definition of state-, action- and reward-space as well as the algorithms hyperparameters. Preference based approaches try to address these problems by omitting the requirement for exact rewards, replacing them with preferences over solutions. Some algorithms have been proposed within this framework, but they are usually requiring parameterized policies which is again a hinderance for their application. Monte Carlo based approaches do not have this restriction and are also model free. Hence, we present a new preference-based reinforcement learning algorithm, utilizing Monte Carlo estimates. The main idea is to estimate the relative Q-value of two actions for the same state within a every-visit framework. This means, preferences are used to estimate the Q-value of state-action pairs within a trajectory, based on the feedback concerning the complete trajectory. The algorithm is evaluated on three common benchmark problems, namely mountain car, inverted pendulum and acrobot, showing its advantage over a closely related algorithm which is also using estimates for intermediate states, but based on a probability theorem. In comparison to SARSA(\u03bb), EPMC converges somewhat slower, but computes policies that are almost as good or better.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Wirth13.pdf",
        "supp": "",
        "pdf_size": 431104,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7205270915551351555&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "TU Darmstadt, Germany; TU Darmstadt, Germany",
        "aff_domain": "ke.tu-darmstadt.de;ke.tu-darmstadt.de",
        "email": "ke.tu-darmstadt.de;ke.tu-darmstadt.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technische Universit\u00e4t Darmstadt",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tu-darmstadt.de",
        "aff_unique_abbr": "TU Darmstadt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "e9c268f109",
        "title": "Exploration vs Exploitation vs Safety: Risk-Aware Multi-Armed Bandits",
        "site": "https://proceedings.mlr.press/v29/Galichet13.html",
        "author": "Nicolas Galichet; Mich\u00e8le Sebag; Olivier Teytaud",
        "abstract": "Motivated by applications in energy management, this paper presents the Multi-Armed Risk-Aware Bandit (MaRaB) algorithm. With the goal of limiting the exploration of risky arms, MaRaB takes as arm quality its conditional value at risk. When the user-supplied risk level goes to 0, the arm quality tends toward the essential infimum of the arm distribution density, and MaRaB tends toward the MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal value. As a first contribution, this paper presents a theoretical analysis of the MIN algorithm under mild assumptions, establishing its robustness comparatively to UCB. The analysis is  supported by extensive experimental validation of MIN and MaRaB compared to UCB and  state-of-art risk-aware MAB algorithms on  artificial and real-world problems.",
        "bibtex": "@InProceedings{pmlr-v29-Galichet13,\n  title = \t {Exploration vs Exploitation vs Safety: Risk-Aware Multi-Armed Bandits},\n  author = \t {Galichet, Nicolas and Sebag, Mich\u00e8le and Teytaud, Olivier},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {245--260},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Galichet13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Galichet13.html},\n  abstract = \t {Motivated by applications in energy management, this paper presents the Multi-Armed Risk-Aware Bandit (MaRaB) algorithm. With the goal of limiting the exploration of risky arms, MaRaB takes as arm quality its conditional value at risk. When the user-supplied risk level goes to 0, the arm quality tends toward the essential infimum of the arm distribution density, and MaRaB tends toward the MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal value. As a first contribution, this paper presents a theoretical analysis of the MIN algorithm under mild assumptions, establishing its robustness comparatively to UCB. The analysis is  supported by extensive experimental validation of MIN and MaRaB compared to UCB and  state-of-art risk-aware MAB algorithms on  artificial and real-world problems. }\n}",
        "pdf": "http://proceedings.mlr.press/v29/Galichet13.pdf",
        "supp": "",
        "pdf_size": 962037,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4307850661283393916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "TAO, CNRS - INRIA - LRI, Universit`e Paris Sud, F-91405 Orsay; TAO, CNRS - INRIA - LRI, Universit`e Paris Sud, F-91405 Orsay; TAO, CNRS - INRIA - LRI, Universit`e Paris Sud, F-91405 Orsay",
        "aff_domain": "lri.fr;lri.fr;lri.fr",
        "email": "lri.fr;lri.fr;lri.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universit\u00e9 Paris Sud",
        "aff_unique_dep": "TAO, CNRS - INRIA - LRI",
        "aff_unique_url": "https://www.universite-paris-sud.fr",
        "aff_unique_abbr": "UPS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Orsay",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "de366c2b21",
        "title": "Generalized Aitchison Embeddings for Histograms",
        "site": "https://proceedings.mlr.press/v29/Le13.html",
        "author": "Tam Le; Marco Cuturi",
        "abstract": "Learning distances that are specifically designed to compare histograms in the probability simplex has recently attracted the attention of the community. Learning such distances is important because most machine learning problems involve bags of features rather than simple vectors. Ample empirical evidence suggests that the Euclidean distance in general and Mahalanobis metric learning in particular may not be suitable to quantify distances between points in the simplex. We propose in this paper a new contribution to address this problem by generalizing a family of embeddings proposed by Aitchison (1982) to map the probability simplex onto a suitable Euclidean space. We provide algorithms to estimate the parameters of such maps, and show that these algorithms lead to representations that outperform alternative approaches to compare histograms.",
        "bibtex": "@InProceedings{pmlr-v29-Le13,\n  title = \t {Generalized Aitchison Embeddings for Histograms},\n  author = \t {Le, Tam and Cuturi, Marco},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {293--308},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Le13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Le13.html},\n  abstract = \t {Learning distances that are specifically designed to compare histograms in the probability simplex has recently attracted the attention of the community. Learning such distances is important because most machine learning problems involve bags of features rather than simple vectors. Ample empirical evidence suggests that the Euclidean distance in general and Mahalanobis metric learning in particular may not be suitable to quantify distances between points in the simplex. We propose in this paper a new contribution to address this problem by generalizing a family of embeddings proposed by Aitchison (1982) to map the probability simplex onto a suitable Euclidean space. We provide algorithms to estimate the parameters of such maps, and show that these algorithms lead to representations that outperform alternative approaches to compare histograms.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Le13.pdf",
        "supp": "",
        "pdf_size": 1011368,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16218878876881179709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Graduate School of Informatics, Kyoto University; Graduate School of Informatics, Kyoto University",
        "aff_domain": "iip.ist.i.kyoto-u.ac.jp;i.kyoto-u.ac.jp",
        "email": "iip.ist.i.kyoto-u.ac.jp;i.kyoto-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Kyoto University",
        "aff_unique_dep": "Graduate School of Informatics",
        "aff_unique_url": "https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Kyoto U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "b06724ef54",
        "title": "Guided Monte Carlo Tree Search for Planning in Learned Environments",
        "site": "https://proceedings.mlr.press/v29/Eyck13.html",
        "author": "Jelle Van Eyck; Jan Ramon; Fabian Guiza; Geert MeyFroidt; Maurice Bruynooghe; Greet Van den Berghe",
        "abstract": "Monte Carlo tree search (MCTS) is a sampling and simulation based technique for searching in large search spaces containing both decision nodes and probabilistic events. This technique has recently become popular due to its successful application to games, e.g. Poker and Go. Such games have known rules and the alternation between self-moves and non-deterministic events or opponent moves can be used to prune uninteresting branches. In this paper we study a real-world setting where the processes in the domain have a high degree of uncertainty and the need for longer-term planning implies a sequence of (planning) decisions without any intermediate feedback. Fortunately, unlike the combinatorial complexity in strategic games, many real-world environments can be approximated by efficient algorithms on a short term. This paper proposes an MCTS variant using a new type of prior information based on estimating the effects of part of the world and explores its application to the problem of hospital planning, where machine learning algorithms can be used to predict the length of stay of patients for each of the different stages of their recovery.",
        "bibtex": "@InProceedings{pmlr-v29-Eyck13,\n  title = \t {Guided Monte Carlo Tree Search for Planning in Learned Environments},\n  author = \t {Eyck, Jelle Van and Ramon, Jan and Guiza, Fabian and MeyFroidt, Geert and Bruynooghe, Maurice and Berghe, Greet Van den},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {33--47},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Eyck13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Eyck13.html},\n  abstract = \t {Monte Carlo tree search (MCTS) is a sampling and simulation based technique for searching in large search spaces containing both decision nodes and probabilistic events. This technique has recently become popular due to its successful application to games, e.g. Poker and Go. Such games have known rules and the alternation between self-moves and non-deterministic events or opponent moves can be used to prune uninteresting branches. In this paper we study a real-world setting where the processes in the domain have a high degree of uncertainty and the need for longer-term planning implies a sequence of (planning) decisions without any intermediate feedback. Fortunately, unlike the combinatorial complexity in strategic games, many real-world environments can be approximated by efficient algorithms on a short term. This paper proposes an MCTS variant using a new type of prior information based on estimating the effects of part of the world and explores its application to the problem of hospital planning, where machine learning algorithms can be used to predict the length of stay of patients for each of the different stages of their recovery.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Eyck13.pdf",
        "supp": "",
        "pdf_size": 334071,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14686016014988448716&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, KULeuven; Department of Computer Science, KULeuven; Laboratory of Intensive Care Medicine, KULeuven; Laboratory of Intensive Care Medicine, KULeuven; Department of Computer Science, KULeuven; Laboratory of Intensive Care Medicine, KULeuven",
        "aff_domain": "cs.kuleuven.be;cs.kuleuven.be;med.kuleuven.be;med.kuleuven.be; maurice.bruynooghe.cs.kuleuven.be;med.kuleuven.be",
        "email": "cs.kuleuven.be;cs.kuleuven.be;med.kuleuven.be;med.kuleuven.be; maurice.bruynooghe.cs.kuleuven.be;med.kuleuven.be",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "KU Leuven",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.kuleuven.be",
        "aff_unique_abbr": "KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "98a0b106d4",
        "title": "Improving Predictive Specificity of Description Logic Learners by Fortification",
        "site": "https://proceedings.mlr.press/v29/Tran13.html",
        "author": "An Tran; Jens Dietrich; Hans Guesgen; Stephen Marsland",
        "abstract": "The predictive accuracy of a learning algorithm can be split into specificity and sensitivity, amongst other decompositions. Sensitivity, also known as completeness, is the ratio of true positives to the total number of positive examples, while specificity is the ratio of true negative to the total negative examples. In top-down learning methods of inductive logic programming, there is generally a bias towards sensitivity, since the learning starts from the most general rule (everything is positive) and specialises by excluding some of the negative examples. While this is often useful, it is not always the best choice: for example, in novelty detection, where the negative examples are rare and often varied, they may well be ignored by the learning. In this paper we introduce a method that attempts to remove the bias towards sensitivity by fortifying the model by computing and then including in the model some descriptions of the negative data even if they are considered redundant by the normal learning algorithm. We demonstrate the method on a set of standard datasets for description logic learning and show that the predictive accuracy increases.",
        "bibtex": "@InProceedings{pmlr-v29-Tran13,\n  title = \t {Improving Predictive Specificity of Description Logic Learners by Fortification},\n  author = \t {Tran, An and Dietrich, Jens and Guesgen, Hans and Marsland, Stephen},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {419--434},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Tran13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Tran13.html},\n  abstract = \t {The predictive accuracy of a learning algorithm can be split into specificity and sensitivity, amongst other decompositions. Sensitivity, also known as completeness, is the ratio of true positives to the total number of positive examples, while specificity is the ratio of true negative to the total negative examples. In top-down learning methods of inductive logic programming, there is generally a bias towards sensitivity, since the learning starts from the most general rule (everything is positive) and specialises by excluding some of the negative examples. While this is often useful, it is not always the best choice: for example, in novelty detection, where the negative examples are rare and often varied, they may well be ignored by the learning. In this paper we introduce a method that attempts to remove the bias towards sensitivity by fortifying the model by computing and then including in the model some descriptions of the negative data even if they are considered redundant by the normal learning algorithm. We demonstrate the method on a set of standard datasets for description logic learning and show that the predictive accuracy increases.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Tran13.pdf",
        "supp": "",
        "pdf_size": 840458,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:cZqVbGQuWXoJ:scholar.google.com/&scioq=Improving+Predictive+Specificity+of+Description+Logic+Learners+by+Fortification&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "School of Engineering and Advanced Technology, Massey University; School of Engineering and Advanced Technology, Massey University; School of Engineering and Advanced Technology, Massey University; School of Engineering and Advanced Technology, Massey University",
        "aff_domain": "gmail.com;massey.ac.nz;massey.ac.nz;massey.ac.nz",
        "email": "gmail.com;massey.ac.nz;massey.ac.nz;massey.ac.nz",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Massey University",
        "aff_unique_dep": "School of Engineering and Advanced Technology",
        "aff_unique_url": "https://www.massey.ac.nz",
        "aff_unique_abbr": "Massey",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "e39431377b",
        "title": "Information Retrieval Perspective to Meta-visualization",
        "site": "https://proceedings.mlr.press/v29/Peltonen13.html",
        "author": "Jaakko Peltonen; Ziyuan Lin",
        "abstract": "In visual data exploration with scatter plots, no single plot is sufficient to analyze complicated high-dimensional data sets. Given numerous visualizations created with different features or methods, meta-visualization is needed to analyze the visualizations together. We solve \\emphhow to arrange numerous visualizations onto a meta-visualization display, so that their similarities and differences can be analyzed. We introduce a machine learning approach to optimize the meta-visualization, based on an information retrieval perspective: two visualizations are similar if the analyst would retrieve similar neighborhoods between data samples from either visualization. Based on the approach, we introduce a nonlinear embedding method for meta-visualization: it optimizes locations of visualizations on a display, so that visualizations giving similar information about data are close to each other.",
        "bibtex": "@InProceedings{pmlr-v29-Peltonen13,\n  title = \t {Information Retrieval Perspective to Meta-visualization},\n  author = \t {Peltonen, Jaakko and Lin, Ziyuan},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {165--180},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Peltonen13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Peltonen13.html},\n  abstract = \t {In visual data exploration with scatter plots, no single plot is sufficient to analyze complicated high-dimensional data sets. Given numerous visualizations created with different features or methods, meta-visualization is needed to analyze the visualizations together. We solve \\emphhow to arrange numerous visualizations onto a meta-visualization display, so that their similarities and differences can be analyzed. We introduce a machine learning approach to optimize the meta-visualization, based on an information retrieval perspective: two visualizations are similar if the analyst would retrieve similar neighborhoods between data samples from either visualization. Based on the approach, we introduce a nonlinear embedding method for meta-visualization: it optimizes locations of visualizations on a display, so that visualizations giving similar information about data are close to each other.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Peltonen13.pdf",
        "supp": "",
        "pdf_size": 4939968,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=65692820836107420&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Information and Computer Science and Helsinki Institute for Information Technology HIIT, Aalto University, Finland; Department of Information and Computer Science and Helsinki Institute for Information Technology HIIT, Aalto University, Finland",
        "aff_domain": "aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Aalto University",
        "aff_unique_dep": "Department of Information and Computer Science",
        "aff_unique_url": "https://www.aalto.fi",
        "aff_unique_abbr": "Aalto",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "a973677e89",
        "title": "Learning Parts-based Representations with Nonnegative Restricted Boltzmann Machine",
        "site": "https://proceedings.mlr.press/v29/Nguyen13.html",
        "author": "Tu Dinh Nguyen; Truyen Tran; Dinh Phung; Svetha Venkatesh",
        "abstract": "The success of any machine learning system depends critically on effective representations of data. In many cases, especially those in vision, it is desirable that a representation scheme uncovers the parts-based, additive nature of the data. Of current representation learning schemes, restricted Boltzmann machines (RBMs) have proved to be highly effective in unsupervised settings. However, when it comes to parts-based discovery, RBMs do not usually produce satisfactory results. We enhance such capacity of RBMs by introducing nonnegativity into the model weights, resulting in a variant called \\emphnonnegative restricted Boltzmann machine (NRBM). The NRBM produces not only controllable decomposition of data into interpretable parts but also offers a way to estimate the intrinsic nonlinear dimensionality of data. We demonstrate the capacity of our model on well-known datasets of handwritten digits, faces and documents. The decomposition quality on images is comparable with or better than what produced by the nonnegative matrix factorisation (NMF), and the thematic features uncovered from text are qualitatively interpretable in a similar manner to that of the latent Dirichlet allocation (LDA). However, the learnt features, when used for classification, are more discriminative than those discovered by both NMF and LDA and comparable with those by RBM.",
        "bibtex": "@InProceedings{pmlr-v29-Nguyen13,\n  title = \t {Learning Parts-based Representations with Nonnegative Restricted Boltzmann Machine},\n  author = \t {Nguyen, Tu Dinh and Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {133--148},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Nguyen13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Nguyen13.html},\n  abstract = \t {The success of any machine learning system depends critically on effective representations of data. In many cases, especially those in vision, it is desirable that a representation scheme uncovers the parts-based, additive nature of the data. Of current representation learning schemes, restricted Boltzmann machines (RBMs) have proved to be highly effective in unsupervised settings. However, when it comes to parts-based discovery, RBMs do not usually produce satisfactory results. We enhance such capacity of RBMs by introducing nonnegativity into the model weights, resulting in a variant called \\emphnonnegative restricted Boltzmann machine (NRBM). The NRBM produces not only controllable decomposition of data into interpretable parts but also offers a way to estimate the intrinsic nonlinear dimensionality of data. We demonstrate the capacity of our model on well-known datasets of handwritten digits, faces and documents. The decomposition quality on images is comparable with or better than what produced by the nonnegative matrix factorisation (NMF), and the thematic features uncovered from text are qualitatively interpretable in a similar manner to that of the latent Dirichlet allocation (LDA). However, the learnt features, when used for classification, are more discriminative than those discovered by both NMF and LDA and comparable with those by RBM.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Nguyen13.pdf",
        "supp": "",
        "pdf_size": 539509,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12747768446995595097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Center for Pattern Recognition and Data Analytics, School of Information Technology, Deakin University, Geelong, Australia; Center for Pattern Recognition and Data Analytics, School of Information Technology, Deakin University, Geelong, Australia + Institute for Multi-Sensor Processing and Content Analysis, Curtin University, Australia; Center for Pattern Recognition and Data Analytics, School of Information Technology, Deakin University, Geelong, Australia; Center for Pattern Recognition and Data Analytics, School of Information Technology, Deakin University, Geelong, Australia",
        "aff_domain": "deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "email": "deakin.edu.au;deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Deakin University;Curtin University",
        "aff_unique_dep": "School of Information Technology;Institute for Multi-Sensor Processing and Content Analysis",
        "aff_unique_url": "https://www.deakin.edu.au;https://www.curtin.edu.au",
        "aff_unique_abbr": "Deakin;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Geelong;",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "c6023383ed",
        "title": "Learning a Metric Space for Neighbourhood Topology Estimation: Application to Manifold Learning",
        "site": "https://proceedings.mlr.press/v29/Moustafa13.html",
        "author": "Karim Abou- Moustafa; Dale Schuurmans; Frank Ferrie",
        "abstract": "Manifold learning algorithms rely on a neighbourhood graph to provide an estimate of the data\u2019s local topology. Unfortunately, current methods for estimating local topology assume local Euclidean geometry and locally uniform data density, which often leads to poor data embeddings. We address these shortcomings by proposing a framework that combines local learning with parametric density estimation for local topology estimation. Given a data set \\mathcalD \u2282\\mathcalX, we first estimate a new metric space (\\mathbbX,d_\\mathbbX) that characterizes the varying sample density of \\mathcalX in \\mathbbX, then use (\\mathbbX,d_\\mathbbX) as a new (pilot) input space for the graph construction step of the manifold learning process. The proposed framework results in significantly improved embeddings, which we demonstrated objectively by assessing clustering accuracy.",
        "bibtex": "@InProceedings{pmlr-v29-Moustafa13,\n  title = \t {Learning a Metric Space for Neighbourhood Topology Estimation: Application to Manifold Learning},\n  author = \t {Moustafa, Karim Abou- and Schuurmans, Dale and Ferrie, Frank},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {341--356},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Moustafa13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Moustafa13.html},\n  abstract = \t {Manifold learning algorithms rely on a neighbourhood graph to provide an estimate of the data\u2019s local topology. Unfortunately, current methods for estimating local topology assume local Euclidean geometry and locally uniform data density, which often leads to poor data embeddings. We address these shortcomings by proposing a framework that combines local learning with parametric density estimation for local topology estimation. Given a data set \\mathcalD \u2282\\mathcalX, we first estimate a new metric space (\\mathbbX,d_\\mathbbX) that characterizes the varying sample density of \\mathcalX in \\mathbbX, then use (\\mathbbX,d_\\mathbbX) as a new (pilot) input space for the graph construction step of the manifold learning process. The proposed framework results in significantly improved embeddings, which we demonstrated objectively by assessing clustering accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Moustafa13.pdf",
        "supp": "",
        "pdf_size": 1490368,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8568274447294954390&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computing Science, University of Alberta, Edmonton, Alberta T6G 2E8, Canada; Dept. of Computing Science, University of Alberta, Edmonton, Alberta T6G 2E8, Canada; Centre for Intelligent Machines, McGill University, Montr\u00b4eal, Qu\u00b4ebec H3A 0E9, Canada",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cim.mcgill.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cim.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Alberta;McGill University",
        "aff_unique_dep": "Dept. of Computing Science;Centre for Intelligent Machines",
        "aff_unique_url": "https://www.ualberta.ca;https://www.mcgill.ca",
        "aff_unique_abbr": "UAlberta;McGill",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Edmonton;Montr\u00e9al",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "1f51130ab4",
        "title": "Linear Approximation to ADMM for MAP inference",
        "site": "https://proceedings.mlr.press/v29/Forouzan13.html",
        "author": "Sholeh Forouzan; Alexander Ihler",
        "abstract": "Maximum a posteriori (MAP) inference is one of the fundamental inference tasks in graphical models.  MAP inference is in general NP-hard, making approximate methods of interest for many problems. One successful class of approximate inference algorithms is based on linear programming (LP) relaxations. The augmented Lagrangian method can be used to overcome a lack of strict convexity in LP relaxations, and the Alternating Direction Method of Multipliers (ADMM) provides an elegant algorithm for finding the saddle point of the augmented Lagrangian. Here we present an ADMM-based algorithm to solve the primal form of the MAP-LP whose closed form updates are based on a linear approximation technique. Our technique gives efficient, closed form updates that converge to the global optimum of the LP relaxation. We compare our algorithm to two existing ADMM-based MAP-LP methods, showing that our technique is faster on general, non-binary or non-pairwise models.",
        "bibtex": "@InProceedings{pmlr-v29-Forouzan13,\n  title = \t {Linear Approximation to ADMM for MAP inference},\n  author = \t {Forouzan, Sholeh and Ihler, Alexander},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {48--61},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Forouzan13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Forouzan13.html},\n  abstract = \t {Maximum a posteriori (MAP) inference is one of the fundamental inference tasks in graphical models.  MAP inference is in general NP-hard, making approximate methods of interest for many problems. One successful class of approximate inference algorithms is based on linear programming (LP) relaxations. The augmented Lagrangian method can be used to overcome a lack of strict convexity in LP relaxations, and the Alternating Direction Method of Multipliers (ADMM) provides an elegant algorithm for finding the saddle point of the augmented Lagrangian. Here we present an ADMM-based algorithm to solve the primal form of the MAP-LP whose closed form updates are based on a linear approximation technique. Our technique gives efficient, closed form updates that converge to the global optimum of the LP relaxation. We compare our algorithm to two existing ADMM-based MAP-LP methods, showing that our technique is faster on general, non-binary or non-pairwise models.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Forouzan13.pdf",
        "supp": "",
        "pdf_size": 303939,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9150067954846838066&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine",
        "aff_domain": "ICS.UCI.EDU;ICS.UCI.EDU",
        "email": "ICS.UCI.EDU;ICS.UCI.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a16be8a092",
        "title": "Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning",
        "site": "https://proceedings.mlr.press/v29/Liu13.html",
        "author": "Risheng Liu; Zhouchen Lin; Zhixun Su",
        "abstract": "Many problems in statistics and machine learning (e.g., probabilistic graphical model, feature extraction, clustering and classification, etc) can be (re)formulated as linearly constrained separable convex programs. The traditional alternating direction method (ADM) or its linearized version (LADM) is for the two-variable case and \\emphcannot be naively generalized to solve the multi-variable case. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-variable separable convex programs efficiently. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the \\emphsufficient and necessary conditions for global convergence. We further propose a simple optimality measure and reveal the convergence \\emphrate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, we devise a practical version of LADMPSAP for faster convergence. LADMPSAP is particularly suitable for sparse representation and low-rank recovery problems because its subproblems have closed form solutions and the sparsity and low-rankness of the iterates can be preserved during the iteration. It is also \\emphhighly parallelizable and hence fits for parallel or distributed computing. Numerical experiments testify to the speed and accuracy advantages of LADMPSAP.",
        "bibtex": "@InProceedings{pmlr-v29-Liu13,\n  title = \t {Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning},\n  author = \t {Liu, Risheng and Lin, Zhouchen and Su, Zhixun},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {116--132},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Liu13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Liu13.html},\n  abstract = \t {Many problems in statistics and machine learning (e.g., probabilistic graphical model, feature extraction, clustering and classification, etc) can be (re)formulated as linearly constrained separable convex programs. The traditional alternating direction method (ADM) or its linearized version (LADM) is for the two-variable case and \\emphcannot be naively generalized to solve the multi-variable case. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-variable separable convex programs efficiently. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the \\emphsufficient and necessary conditions for global convergence. We further propose a simple optimality measure and reveal the convergence \\emphrate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, we devise a practical version of LADMPSAP for faster convergence. LADMPSAP is particularly suitable for sparse representation and low-rank recovery problems because its subproblems have closed form solutions and the sparsity and low-rankness of the iterates can be preserved during the iteration. It is also \\emphhighly parallelizable and hence fits for parallel or distributed computing. Numerical experiments testify to the speed and accuracy advantages of LADMPSAP.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Liu13.pdf",
        "supp": "",
        "pdf_size": 4308873,
        "gs_citation": 216,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4078666043193680963&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology; Key Lab. of Machine Perception (MOE), School of EECS, Peking University; School of Mathematical Sciences, Dalian University of Technology",
        "aff_domain": "dlut.edu.cn;pku.edu.cn;dlut.edu.cn",
        "email": "dlut.edu.cn;pku.edu.cn;dlut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Dalian University of Technology;Peking University",
        "aff_unique_dep": "Faculty of Electronic Information and Electrical Engineering;School of EECS",
        "aff_unique_url": "http://en.dlut.edu.cn/;http://www.pku.edu.cn",
        "aff_unique_abbr": "DUT;PKU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dalian",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "fc15da1faf",
        "title": "Locally-Linear Learning Machines (L3M)",
        "site": "https://proceedings.mlr.press/v29/Wang13a.html",
        "author": "Joseph Wang; Venkatesh Saligrama",
        "abstract": "We present locally-linear learning machines (L3M) for multi-class classification. We formulate a global convex risk function to jointly learn linear feature space partitions and region-specific linear classifiers. L3M\u2019s features such as: (1) discriminative power similar to Kernel SVMs and Adaboost; (2) tight control on generalization error; (3) low training time cost due to on-line training; (4) low test-time costs due to local linearity; are all potentially well-suited for \u201cbig-data\u201d applications. We derive tight convex surrogates for the empirical risk function associated with space partitioning classifiers. These empirical risk functions are non-convex since they involve products of indicator functions. We obtain a global convex surrogate by first embedding empirical risk loss as an extremal point of an optimization problem and then convexifying this resulting problem. Using the proposed convex formulation, we demonstrate improvement in classification performance, test and training time relative to common discriminative learning methods on challenging multiclass data sets.",
        "bibtex": "@InProceedings{pmlr-v29-Wang13a,\n  title = \t {Locally-Linear Learning Machines (L3M)},\n  author = \t {Wang, Joseph and Saligrama, Venkatesh},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {451--466},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Wang13a.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Wang13a.html},\n  abstract = \t {We present locally-linear learning machines (L3M) for multi-class classification. We formulate a global convex risk function to jointly learn linear feature space partitions and region-specific linear classifiers. L3M\u2019s features such as: (1) discriminative power similar to Kernel SVMs and Adaboost; (2) tight control on generalization error; (3) low training time cost due to on-line training; (4) low test-time costs due to local linearity; are all potentially well-suited for \u201cbig-data\u201d applications. We derive tight convex surrogates for the empirical risk function associated with space partitioning classifiers. These empirical risk functions are non-convex since they involve products of indicator functions. We obtain a global convex surrogate by first embedding empirical risk loss as an extremal point of an optimization problem and then convexifying this resulting problem. Using the proposed convex formulation, we demonstrate improvement in classification performance, test and training time relative to common discriminative learning methods on challenging multiclass data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Wang13a.pdf",
        "supp": "",
        "pdf_size": 528952,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=393984263006486034&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical & Computer Engineering, Boston University; Department of Electrical & Computer Engineering, Boston University",
        "aff_domain": "bu.edu;bu.edu",
        "email": "bu.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f8a0c7109c",
        "title": "Multi-Label Classification with Unlabeled Data: An Inductive Approach",
        "site": "https://proceedings.mlr.press/v29/Wu13.html",
        "author": "Le Wu; Min-Ling Zhang",
        "abstract": "The problem of multi-label classification has attracted great interests in the last decade. Multi-label classification refers to the problems where an example that is represented by a \\emphsingle instance can be assigned to \\emphmore than one category. Until now, most of the researches on multi-label classification have focused on supervised settings whose assumption is that large amount of labeled training data is available. Unfortunately, labeling training example is expensive and time-consuming, especially when it has more than one label. However, in many cases abundant unlabeled data is easy to obtain. Current attempts toward exploiting unlabeled data for multi-label classification work under the \\emphtransductive setting, which aim at making predictions on existing unlabeled data while can not generalize to new unseen data. In this paper, the problem of \\emphinductive semi-supervised multi-label classification is studied, where a new approach named \\textsliMLCU, i.e. \\emphinductive Multi-Label Classification with Unlabeled data, is proposed. We formulate the inductive semi-supervised multi-label learning as an optimization problem of learning linear models and ConCave Convex Procedure \\textsl(CCCP) is applied to optimize the non-convex optimization problem. Empirical studies on twelve diversified real-word multi-label learning tasks clearly validate the superiority of \\textsliMLCU against the other well-established multi-label learning approaches.",
        "bibtex": "@InProceedings{pmlr-v29-Wu13,\n  title = \t {Multi-Label Classification with Unlabeled Data: An Inductive Approach},\n  author = \t {Wu, Le and Zhang, Min-Ling},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {197--212},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Wu13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Wu13.html},\n  abstract = \t {The problem of multi-label classification has attracted great interests in the last decade. Multi-label classification refers to the problems where an example that is represented by a \\emphsingle instance can be assigned to \\emphmore than one category. Until now, most of the researches on multi-label classification have focused on supervised settings whose assumption is that large amount of labeled training data is available. Unfortunately, labeling training example is expensive and time-consuming, especially when it has more than one label. However, in many cases abundant unlabeled data is easy to obtain. Current attempts toward exploiting unlabeled data for multi-label classification work under the \\emphtransductive setting, which aim at making predictions on existing unlabeled data while can not generalize to new unseen data. In this paper, the problem of \\emphinductive semi-supervised multi-label classification is studied, where a new approach named \\textsliMLCU, i.e. \\emphinductive Multi-Label Classification with Unlabeled data, is proposed. We formulate the inductive semi-supervised multi-label learning as an optimization problem of learning linear models and ConCave Convex Procedure \\textsl(CCCP) is applied to optimize the non-convex optimization problem. Empirical studies on twelve diversified real-word multi-label learning tasks clearly validate the superiority of \\textsliMLCU against the other well-established multi-label learning approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Wu13.pdf",
        "supp": "",
        "pdf_size": 507261,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2314867787628185925&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science and Engineering, MOE Key Laboratory of Computer Network and Information Integration, Southeast University, Nanjing 210096, China; School of Computer Science and Engineering, MOE Key Laboratory of Computer Network and Information Integration, Southeast University, Nanjing 210096, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "7e786441fc",
        "title": "Multi-armed Bandit Problem with Lock-up Periods",
        "site": "https://proceedings.mlr.press/v29/Komiyama13.html",
        "author": "Junpei Komiyama; Issei Sato; Hiroshi Nakagawa",
        "abstract": "We investigate a stochastic multi-armed bandit problem in which the forecaster\u2019s choice is restricted. In this problem, rounds are divided into lock-up periods and the forecaster must select the same arm throughout a period. While there has been much work on finding optimal algorithms for the stochastic multi-armed bandit problem,  their use under restricted conditions is not obvious. We extend the application ranges of these algorithms by proposing their natural conversion from ones for the stochastic bandit problem (index-based algorithms and greedy algorithms) to ones for the multi-armed bandit problem with lock-up periods. We prove that the regret of the converted algorithms is O(\\logT + L_max ), where T is the total number of rounds and L_max is the maximum size of the lock-up periods. The regret is preferable, except for the case when the maximum size of the lock-up periods is large. For these cases, we propose a meta-algorithm that results in a smaller regret by using a empirical best arm for large periods. We empirically compare and discuss these algorithms.",
        "bibtex": "@InProceedings{pmlr-v29-Komiyama13,\n  title = \t {Multi-armed Bandit Problem with Lock-up Periods},\n  author = \t {Komiyama, Junpei and Sato, Issei and Nakagawa, Hiroshi},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {100--115},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Komiyama13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Komiyama13.html},\n  abstract = \t {We investigate a stochastic multi-armed bandit problem in which the forecaster\u2019s choice is restricted. In this problem, rounds are divided into lock-up periods and the forecaster must select the same arm throughout a period. While there has been much work on finding optimal algorithms for the stochastic multi-armed bandit problem,  their use under restricted conditions is not obvious. We extend the application ranges of these algorithms by proposing their natural conversion from ones for the stochastic bandit problem (index-based algorithms and greedy algorithms) to ones for the multi-armed bandit problem with lock-up periods. We prove that the regret of the converted algorithms is O(\\logT + L_max ), where T is the total number of rounds and L_max is the maximum size of the lock-up periods. The regret is preferable, except for the case when the maximum size of the lock-up periods is large. For these cases, we propose a meta-algorithm that results in a smaller regret by using a empirical best arm for large periods. We empirically compare and discuss these algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Komiyama13.pdf",
        "supp": "",
        "pdf_size": 397296,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17529926918212318053&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Tokyo; The University of Tokyo; The University of Tokyo",
        "aff_domain": "gmail.com;r.dl.itc.u-tokyo.ac.jp;dl.itc.u-tokyo.ac.jp",
        "email": "gmail.com;r.dl.itc.u-tokyo.ac.jp;dl.itc.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "dae235b2d5",
        "title": "Multiclass Latent Locally Linear Support Vector Machines",
        "site": "https://proceedings.mlr.press/v29/Fornoni13.html",
        "author": "Marco Fornoni; Barbara Caputo; Francesco Orabona",
        "abstract": "Kernelized Support Vector Machines (SVM) have gained the status of off-the-shelf classifiers, able to deliver state of the art performance on almost any problem. Still, their practical use is constrained by their computational and memory complexity, which grows super-linearly with the number of training samples. In order to retain the low training and testing complexity of linear classifiers and the flexibility of non linear ones, a growing, promising alternative is represented by methods that learn non-linear classifiers through local combinations of linear ones. In this paper we propose a new multi class local classifier, based on a latent SVM formulation. The proposed classifier makes use of a set of linear models that are linearly combined using sample and class specific weights. Thanks to the latent formulation, the combination coefficients are modeled as latent variables. We allow soft combinations and we provide a closed-form solution for their estimation, resulting in an efficient prediction rule. This novel formulation allows to learn in a principled way the sample specific weights and the linear classifiers, in a unique optimization problem, using a CCCP optimization procedure. Extensive experiments on ten standard UCI machine learning datasets, one large binary dataset, three character and digit recognition databases, and a visual place categorization dataset show the power of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v29-Fornoni13,\n  title = \t {Multiclass Latent Locally Linear Support Vector Machines},\n  author = \t {Fornoni, Marco and Caputo, Barbara and Orabona, Francesco},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {229--244},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Fornoni13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Fornoni13.html},\n  abstract = \t {Kernelized Support Vector Machines (SVM) have gained the status of off-the-shelf classifiers, able to deliver state of the art performance on almost any problem. Still, their practical use is constrained by their computational and memory complexity, which grows super-linearly with the number of training samples. In order to retain the low training and testing complexity of linear classifiers and the flexibility of non linear ones, a growing, promising alternative is represented by methods that learn non-linear classifiers through local combinations of linear ones. In this paper we propose a new multi class local classifier, based on a latent SVM formulation. The proposed classifier makes use of a set of linear models that are linearly combined using sample and class specific weights. Thanks to the latent formulation, the combination coefficients are modeled as latent variables. We allow soft combinations and we provide a closed-form solution for their estimation, resulting in an efficient prediction rule. This novel formulation allows to learn in a principled way the sample specific weights and the linear classifiers, in a unique optimization problem, using a CCCP optimization procedure. Extensive experiments on ten standard UCI machine learning datasets, one large binary dataset, three character and digit recognition databases, and a visual place categorization dataset show the power of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Fornoni13.pdf",
        "supp": "",
        "pdf_size": 758095,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1609248206065514065&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Idiap Research Institute, Martigny, Switzerland + Ecole Polytechnique F\u00b4ed\u00b4erale (EPFL), Lausanne, Switzerland; Idiap Research Institute, Martigny, Switzerland + Sapienza - Universit`a di Roma, Italy; Toyota Technological Institute at Chicago, USA",
        "aff_domain": "idiap.ch;idiap.ch;ttic.edu",
        "email": "idiap.ch;idiap.ch;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2;3",
        "aff_unique_norm": "Idiap Research Institute;EPFL;Sapienza University of Rome;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.idiap.ch;https://www.epfl.ch;https://www.uniroma1.it;https://www.tti-chicago.org",
        "aff_unique_abbr": "Idiap;EPFL;Sapienza;TTI Chicago",
        "aff_campus_unique_index": "0+1;0;3",
        "aff_campus_unique": "Martigny;Lausanne;;Chicago",
        "aff_country_unique_index": "0+0;0+1;2",
        "aff_country_unique": "Switzerland;Italy;United States"
    },
    {
        "id": "af667d1daf",
        "title": "Multilabel Classification through Random Graph Ensembles",
        "site": "https://proceedings.mlr.press/v29/Su13.html",
        "author": "Hongyu Su; Juho Rousu",
        "abstract": "We present new methods for multilabel classification, relying on ensemble learning on a collection of random output graphs imposed on the multilabel and a kernel-based structured output learner as the base classifier. For ensemble learning, differences among the output graphs provide the required base classifier diversity and lead to improved performance in the increasing size of the ensemble. We study different methods of forming the ensemble prediction, including majority voting and two methods that perform inferences over the graph structures before or after combining the base models into the ensemble. We compare the methods against the state-of-the-art machine learning approaches on a set of heterogeneous multilabel benchmark problems, including multilabel AdaBoost, convex multitask feature learning, as well as single target learning approaches represented by Bagging and SVM. In our experiments, the random graph ensembles are very competitive and robust, ranking first or second on most of the datasets. Overall, our results show that random graph ensembles are viable alternatives to flat multilabel and multitask learners.",
        "bibtex": "@InProceedings{pmlr-v29-Su13,\n  title = \t {Multilabel Classification through Random Graph Ensembles},\n  author = \t {Su, Hongyu and Rousu, Juho},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {404--418},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Su13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Su13.html},\n  abstract = \t {We present new methods for multilabel classification, relying on ensemble learning on a collection of random output graphs imposed on the multilabel and a kernel-based structured output learner as the base classifier. For ensemble learning, differences among the output graphs provide the required base classifier diversity and lead to improved performance in the increasing size of the ensemble. We study different methods of forming the ensemble prediction, including majority voting and two methods that perform inferences over the graph structures before or after combining the base models into the ensemble. We compare the methods against the state-of-the-art machine learning approaches on a set of heterogeneous multilabel benchmark problems, including multilabel AdaBoost, convex multitask feature learning, as well as single target learning approaches represented by Bagging and SVM. In our experiments, the random graph ensembles are very competitive and robust, ranking first or second on most of the datasets. Overall, our results show that random graph ensembles are viable alternatives to flat multilabel and multitask learners.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Su13.pdf",
        "supp": "",
        "pdf_size": 327365,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=680043520568875622&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Helsinki Institute of Information Technology (HIIT) + Department of Information and Computer Science, Aalto University, Konemiehentie 2, 02150 Espoo, Finland; Helsinki Institute of Information Technology (HIIT) + Department of Information and Computer Science, Aalto University, Konemiehentie 2, 02150 Espoo, Finland",
        "aff_domain": "aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Helsinki Institute of Information Technology;Aalto University",
        "aff_unique_dep": "Institute of Information Technology;Department of Information and Computer Science",
        "aff_unique_url": "https://www.hiit.fi;https://www.aalto.fi",
        "aff_unique_abbr": "HIIT;Aalto",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Espoo",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "16142104c6",
        "title": "Novel Boosting Frameworks to Improve the Performance of Collaborative Filtering",
        "site": "https://proceedings.mlr.press/v29/Jiang13.html",
        "author": "Xiaotian Jiang; Zhendong Niu; Jiamin Guo; Ghulam Mustafa; Zihan Lin; Baomi Chen; Qian Zhou",
        "abstract": "Recommender systems are often based on collaborative filtering. Previous researches on collaborative filtering mainly focus on one single recommender or formulating hybrid with different approaches. In consideration of the problems of sparsity, recommender error rate, sample weight update, and potential, we adapt AdaBoost and propose two novel boosting frameworks for collaborative filtering. Each of the frameworks combines multiple homogeneous recommenders, which are based on the same collaborative filtering algorithm with different sample weights. We use seven popular collaborative filtering algorithms to evaluate the two frameworks with two MovieLens datasets of different scale. Experimental result shows the proposed frameworks improve the performance of collaborative filtering.",
        "bibtex": "@InProceedings{pmlr-v29-Jiang13,\n  title = \t {Novel Boosting Frameworks to Improve the Performance of Collaborative Filtering},\n  author = \t {Jiang, Xiaotian and Niu, Zhendong and Guo, Jiamin and Mustafa, Ghulam and Lin, Zihan and Chen, Baomi and Zhou, Qian},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {87--99},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Jiang13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Jiang13.html},\n  abstract = \t {Recommender systems are often based on collaborative filtering. Previous researches on collaborative filtering mainly focus on one single recommender or formulating hybrid with different approaches. In consideration of the problems of sparsity, recommender error rate, sample weight update, and potential, we adapt AdaBoost and propose two novel boosting frameworks for collaborative filtering. Each of the frameworks combines multiple homogeneous recommenders, which are based on the same collaborative filtering algorithm with different sample weights. We use seven popular collaborative filtering algorithms to evaluate the two frameworks with two MovieLens datasets of different scale. Experimental result shows the proposed frameworks improve the performance of collaborative filtering.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Jiang13.pdf",
        "supp": "",
        "pdf_size": 258740,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2478992898237446320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Beijing Institute of Technology; School of Computer Science, Beijing Institute of Technology; School of Computer Science, Beijing Institute of Technology; School of Computer Science, Beijing Institute of Technology; School of Computer Science, Beijing Institute of Technology; School of Computer Science, Beijing Institute of Technology; School of Computer Science, Beijing Institute of Technology",
        "aff_domain": "163.com;bit.edu.cn;hotmail.com;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn",
        "email": "163.com;bit.edu.cn;hotmail.com;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing Institute of Technology",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.bit.edu.cn",
        "aff_unique_abbr": "BIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "df2b3d869e",
        "title": "On multi-class classification through the minimization of the confusion matrix norm",
        "site": "https://proceedings.mlr.press/v29/Koco13.html",
        "author": "Sokol Ko\u00e7o; C\u00e9cile Capponi",
        "abstract": "In imbalanced multi-class classification problems, the misclassification rate as an error measure may not be a relevant choice. Several methods have been developed where the performance measure retained richer information than the mere misclassification rate: misclassification costs, ROC-based information, etc. Following this idea of dealing with alternate measures of performance, we propose to address imbalanced classification problems by using a new measure to be optimized: the norm of the confusion matrix. Indeed, recent results show that using the norm of the confusion matrix as an error measure can be quite interesting due to the fine-grain informations contained in the matrix,  especially in the case of imbalanced classes. Our first contribution then consists in showing that optimizing criterion based on the confusion matrix gives rise to a common background for cost-sensitive methods aimed at dealing with imbalanced classes learning problems. As our second contribution, we propose an extension of a recent multi-class boosting method \u2014 namely AdaBoost.MM \u2014 to the imbalanced class problem, by greedily minimizing  the empirical norm of the confusion matrix. A theoretical analysis of the properties of the proposed method is presented, while experimental results illustrate the behavior of the algorithm and show the relevancy of the approach compared to other methods.",
        "bibtex": "@InProceedings{pmlr-v29-Koco13,\n  title = \t {On multi-class classification through the minimization of the confusion matrix norm},\n  author = \t {Ko\u00e7o, Sokol and Capponi, C\u00e9cile},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {277--292},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Koco13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Koco13.html},\n  abstract = \t {In imbalanced multi-class classification problems, the misclassification rate as an error measure may not be a relevant choice. Several methods have been developed where the performance measure retained richer information than the mere misclassification rate: misclassification costs, ROC-based information, etc. Following this idea of dealing with alternate measures of performance, we propose to address imbalanced classification problems by using a new measure to be optimized: the norm of the confusion matrix. Indeed, recent results show that using the norm of the confusion matrix as an error measure can be quite interesting due to the fine-grain informations contained in the matrix,  especially in the case of imbalanced classes. Our first contribution then consists in showing that optimizing criterion based on the confusion matrix gives rise to a common background for cost-sensitive methods aimed at dealing with imbalanced classes learning problems. As our second contribution, we propose an extension of a recent multi-class boosting method \u2014 namely AdaBoost.MM \u2014 to the imbalanced class problem, by greedily minimizing  the empirical norm of the confusion matrix. A theoretical analysis of the properties of the proposed method is presented, while experimental results illustrate the behavior of the algorithm and show the relevancy of the approach compared to other methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Koco13.pdf",
        "supp": "",
        "pdf_size": 312423,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12524142181367702552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Aix-Marseille Universit\u00b4e, CNRS, LIF UMR 7279, 13000, Marseille, France; Aix-Marseille Universit\u00b4e, CNRS, LIF UMR 7279, 13000, Marseille, France",
        "aff_domain": "lif.univ-mrs.fr;lif.univ-mrs.fr",
        "email": "lif.univ-mrs.fr;lif.univ-mrs.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Aix-Marseille University",
        "aff_unique_dep": "LIF UMR 7279",
        "aff_unique_url": "https://www.univ-amu.fr",
        "aff_unique_abbr": "AMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Marseille",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "6aeb39d51b",
        "title": "Polynomial Runtime Bounds for Fixed-Rank Unsupervised Least-Squares Classification",
        "site": "https://proceedings.mlr.press/v29/Gieseke13.html",
        "author": "Fabian Gieseke; Tapio Pahikkala; Christian Igel",
        "abstract": "Maximum margin clustering can be regarded as the direct extension of support vector machines to unsupervised learning scenarios. The goal is to partition unlabeled data into two classes such that a subsequent application of a support vector machine would yield the overall best result (with respect to the optimization problem associated with support vector machines). While being very appealing from a conceptual point of view, the combinatorial nature of the induced optimization problem renders a direct application of this concept difficult. In order to obtain efficient optimization schemes, various surrogates of the original problem definition have been proposed in the literature. In this work, we consider one of these variants, called unsupervised regularized least-squares classification, which is based on the square loss, and develop polynomial upper runtime bounds for the induced combinatorial optimization task. In particular, we show that for n patterns and kernel matrix of fixed rank r (with given  eigendecomposition), one can obtain an optimal solution in \\mathcalO(n^r) time for r \u22642 and in \\mathcalO(n^r-1) time for r\u22653. The algorithmic framework is based on an interesting connection to the field of quadratic zero-one programming and permits the computation of exact solutions for the more general case of non-linear kernel functions in polynomial time.",
        "bibtex": "@InProceedings{pmlr-v29-Gieseke13,\n  title = \t {Polynomial Runtime Bounds for Fixed-Rank Unsupervised Least-Squares Classification},\n  author = \t {Gieseke, Fabian and Pahikkala, Tapio and Igel, Christian},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {62--71},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Gieseke13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Gieseke13.html},\n  abstract = \t {Maximum margin clustering can be regarded as the direct extension of support vector machines to unsupervised learning scenarios. The goal is to partition unlabeled data into two classes such that a subsequent application of a support vector machine would yield the overall best result (with respect to the optimization problem associated with support vector machines). While being very appealing from a conceptual point of view, the combinatorial nature of the induced optimization problem renders a direct application of this concept difficult. In order to obtain efficient optimization schemes, various surrogates of the original problem definition have been proposed in the literature. In this work, we consider one of these variants, called unsupervised regularized least-squares classification, which is based on the square loss, and develop polynomial upper runtime bounds for the induced combinatorial optimization task. In particular, we show that for n patterns and kernel matrix of fixed rank r (with given  eigendecomposition), one can obtain an optimal solution in \\mathcalO(n^r) time for r \u22642 and in \\mathcalO(n^r-1) time for r\u22653. The algorithmic framework is based on an interesting connection to the field of quadratic zero-one programming and permits the computation of exact solutions for the more general case of non-linear kernel functions in polynomial time.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Gieseke13.pdf",
        "supp": "",
        "pdf_size": 394183,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9075830174401889389&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Copenhagen, Denmark; Department of Information Technology and Turku Centre for Computer Science, University of Turku, Finland + Department of Computer Science, University of Copenhagen, Denmark; Department of Computer Science, University of Copenhagen, Denmark",
        "aff_domain": "DIKU.DK;UTU.FI;DIKU.DK",
        "email": "DIKU.DK;UTU.FI;DIKU.DK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "University of Copenhagen;University of Turku",
        "aff_unique_dep": "Department of Computer Science;Department of Information Technology",
        "aff_unique_url": "https://www.ku.dk;https://www.utu.fi",
        "aff_unique_abbr": "UCPH;UTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "Denmark;Finland"
    },
    {
        "id": "ecbe35ca70",
        "title": "Predictive Simulation Framework of Stochastic Diffusion Model for Identifying Top-K Influential Nodes",
        "site": "https://proceedings.mlr.press/v29/Ohara13.html",
        "author": "Kouzou Ohara; Kazumi Saito; Masahiro Kimura; Hiroshi Motoda",
        "abstract": "We address a problem of efficiently estimating the influence of a node in information diffusion over a social network. Since the information diffusion is a stochastic process, the influence degree of a node is quantified by the expectation, which is usually obtained by very time consuming many runs of simulation. Our contribution is that we proposed a framework for predictive simulation based on the leave-N-out cross validation technique that well approximates the error from the unknown ground truth for two target problems: one to estimate the influence degree of each node, and the other to identify top-K influential nodes. The method we proposed for the first problem estimates the approximation error of the influence degree of each node, and the method for the second problem estimates the precision of the derived top-K nodes, both without knowing the true influence degree. We experimentally evaluate the proposed methods using the three real world networks, and show that they can serve as a good measure to solve the target problems with far fewer runs of simulation ensuring the accuracy if N is appropriately chosen, and that estimating the top-K nodes is easier than estimating the influence degree, which means one can identify the influential nodes without knowing exactly their influence degree.",
        "bibtex": "@InProceedings{pmlr-v29-Ohara13,\n  title = \t {Predictive Simulation Framework of Stochastic Diffusion Model for Identifying Top-K Influential Nodes},\n  author = \t {Ohara, Kouzou and Saito, Kazumi and Kimura, Masahiro and Motoda, Hiroshi},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {149--164},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Ohara13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Ohara13.html},\n  abstract = \t {We address a problem of efficiently estimating the influence of a node in information diffusion over a social network. Since the information diffusion is a stochastic process, the influence degree of a node is quantified by the expectation, which is usually obtained by very time consuming many runs of simulation. Our contribution is that we proposed a framework for predictive simulation based on the leave-N-out cross validation technique that well approximates the error from the unknown ground truth for two target problems: one to estimate the influence degree of each node, and the other to identify top-K influential nodes. The method we proposed for the first problem estimates the approximation error of the influence degree of each node, and the method for the second problem estimates the precision of the derived top-K nodes, both without knowing the true influence degree. We experimentally evaluate the proposed methods using the three real world networks, and show that they can serve as a good measure to solve the target problems with far fewer runs of simulation ensuring the accuracy if N is appropriately chosen, and that estimating the top-K nodes is easier than estimating the influence degree, which means one can identify the influential nodes without knowing exactly their influence degree. }\n}",
        "pdf": "http://proceedings.mlr.press/v29/Ohara13.pdf",
        "supp": "",
        "pdf_size": 792771,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10117020876184603422&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Integrated Information Technology, Aoyama Gakuin University; School of Administration and Informatics, University of Shizuoka; Department of Electronics and Informatics, Ryukoku University; Institute of Scienti\ufb01c and Industrial Research, Osaka University+School of Computing, University of Tasmania",
        "aff_domain": "it.aoyama.ac.jp;u-shizuoka-ken.ac.jp;rins.ryukoku.ac.jp;ar.sanken.osaka-u.ac.jp",
        "email": "it.aoyama.ac.jp;u-shizuoka-ken.ac.jp;rins.ryukoku.ac.jp;ar.sanken.osaka-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3+4",
        "aff_unique_norm": "Aoyama Gakuin University;University of Shizuoka;Ryukoku University;Osaka University;University of Tasmania",
        "aff_unique_dep": "Department of Integrated Information Technology;School of Administration and Informatics;Department of Electronics and Informatics;Institute of Scienti\ufb01c and Industrial Research;School of Computing",
        "aff_unique_url": "https://www.aoyama.ac.jp;https://www.u-shizuoka.ac.jp;https://www.ryukoku.ac.jp;https://www.osaka-u.ac.jp;https://www.utas.edu.au",
        "aff_unique_abbr": "AGU;;Ryukoku U;Osaka U;UTAS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Osaka",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Japan;Australia"
    },
    {
        "id": "5cf0682208",
        "title": "Preface",
        "site": "https://proceedings.mlr.press/v29/Ong13.html",
        "author": "Cheng Soon Ong; Tu Bao Ho",
        "abstract": "",
        "bibtex": "@InProceedings{pmlr-v29-Ong13,\n  title = \t {Preface},\n  author = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {1--17},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Ong13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Ong13.html}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Ong13.pdf",
        "supp": "",
        "pdf_size": 136493,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141043608347785016&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4f673f7b19",
        "title": "Q-learning for history-based reinforcement learning",
        "site": "https://proceedings.mlr.press/v29/Daswani13.html",
        "author": "Mayank Daswani; Peter Sunehag; Marcus Hutter",
        "abstract": "We extend the Q-learning algorithm from the Markov Decision Process setting to problems where observations are non-Markov and do not reveal the full state of the world i.e. to POMDPs. We do this in a natural manner by adding \\ell_0 regularisation to the pathwise squared Q-learning objective function and then optimise this over both a choice of map from history to states and the resulting MDP parameters. The optimisation procedure involves a stochastic search over the map class nested with classical Q-learning of the parameters. This algorithm fits perfectly into the feature reinforcement learning framework, which chooses maps based on a cost criteria. The cost criterion used so far for feature reinforcement learning has been model-based and aimed at predicting future states and rewards. Instead we directly predict the return, which is what is needed for choosing optimal actions. Our Q-learning criteria also lends itself immediately to a function approximation setting where features are chosen based on the history. This algorithm is somewhat similar to the recent line of work on lasso temporal difference learning which aims at finding a small feature set with which one can perform policy evaluation. The distinction is that we aim directly for learning the Q-function of the optimal policy and we use \\ell_0 instead of \\ell_1 regularisation. We perform an experimental evaluation on classical benchmark domains and find improvement in convergence speed as well as in economy of the state representation. We also compare against MC-AIXI on the large Pocman domain and achieve competitive performance in average reward. We use less than half the CPU time and 36 times less memory. Overall, our algorithm hQL provides a better combination of computational, memory and  data efficiency than existing algorithms in this setting.",
        "bibtex": "@InProceedings{pmlr-v29-Daswani13,\n  title = \t {Q-learning for history-based reinforcement learning},\n  author = \t {Daswani, Mayank and Sunehag, Peter and Hutter, Marcus},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {213--228},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Daswani13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Daswani13.html},\n  abstract = \t {We extend the Q-learning algorithm from the Markov Decision Process setting to problems where observations are non-Markov and do not reveal the full state of the world i.e. to POMDPs. We do this in a natural manner by adding \\ell_0 regularisation to the pathwise squared Q-learning objective function and then optimise this over both a choice of map from history to states and the resulting MDP parameters. The optimisation procedure involves a stochastic search over the map class nested with classical Q-learning of the parameters. This algorithm fits perfectly into the feature reinforcement learning framework, which chooses maps based on a cost criteria. The cost criterion used so far for feature reinforcement learning has been model-based and aimed at predicting future states and rewards. Instead we directly predict the return, which is what is needed for choosing optimal actions. Our Q-learning criteria also lends itself immediately to a function approximation setting where features are chosen based on the history. This algorithm is somewhat similar to the recent line of work on lasso temporal difference learning which aims at finding a small feature set with which one can perform policy evaluation. The distinction is that we aim directly for learning the Q-function of the optimal policy and we use \\ell_0 instead of \\ell_1 regularisation. We perform an experimental evaluation on classical benchmark domains and find improvement in convergence speed as well as in economy of the state representation. We also compare against MC-AIXI on the large Pocman domain and achieve competitive performance in average reward. We use less than half the CPU time and 36 times less memory. Overall, our algorithm hQL provides a better combination of computational, memory and  data efficiency than existing algorithms in this setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Daswani13.pdf",
        "supp": "",
        "pdf_size": 306331,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3621798507607839681&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Research School of Computer Science, Australian National University, Canberra, ACT, 0200, Australia; Research School of Computer Science, Australian National University, Canberra, ACT, 0200, Australia; Research School of Computer Science, Australian National University, Canberra, ACT, 0200, Australia",
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "Research School of Computer Science",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "b45d8d355b",
        "title": "Random Projections as Regularizers: Learning a Linear Discriminant Ensemble from Fewer Observations than Dimensions",
        "site": "https://proceedings.mlr.press/v29/Durrant13.html",
        "author": "Robert Durrant; Ata Kaban",
        "abstract": "We examine the performance of an ensemble of randomly-projected Fisher Linear Discriminant classifiers, focusing on the case when there are fewer training observations than data dimensions. Our ensemble is learned from a sequence of randomly-projected representations of the original high dimensional data and therefore for this approach data can be collected, stored and processed in such a compressed form. The specific form and simplicity of this ensemble permits a direct and much more detailed analysis than existing generic tools in previous works.  In particular, we are able to derive the exact form of the generalization error of our ensemble, conditional on the training set, and based on this  we give theoretical guarantees which directly link the performance of the ensemble to that of the corresponding linear discriminant learned in the full data space. To the best of our knowledge these are the first theoretical results to prove such an explicit link for any classifier and classifier ensemble pair. Furthermore we show that the randomly-projected ensemble is equivalent to implementing a sophisticated regularization scheme to the linear discriminant learned in the original data space  and this prevents overfitting in conditions of small sample size where pseudo-inverse FLD learned in the data space is provably poor.",
        "bibtex": "@InProceedings{pmlr-v29-Durrant13,\n  title = \t {Random Projections as Regularizers: Learning a Linear Discriminant Ensemble from Fewer Observations than Dimensions},\n  author = \t {Durrant, Robert and Kaban, Ata},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {17--32},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Durrant13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Durrant13.html},\n  abstract = \t {We examine the performance of an ensemble of randomly-projected Fisher Linear Discriminant classifiers, focusing on the case when there are fewer training observations than data dimensions. Our ensemble is learned from a sequence of randomly-projected representations of the original high dimensional data and therefore for this approach data can be collected, stored and processed in such a compressed form. The specific form and simplicity of this ensemble permits a direct and much more detailed analysis than existing generic tools in previous works.  In particular, we are able to derive the exact form of the generalization error of our ensemble, conditional on the training set, and based on this  we give theoretical guarantees which directly link the performance of the ensemble to that of the corresponding linear discriminant learned in the full data space. To the best of our knowledge these are the first theoretical results to prove such an explicit link for any classifier and classifier ensemble pair. Furthermore we show that the randomly-projected ensemble is equivalent to implementing a sophisticated regularization scheme to the linear discriminant learned in the original data space  and this prevents overfitting in conditions of small sample size where pseudo-inverse FLD learned in the data space is provably poor.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Durrant13.pdf",
        "supp": "",
        "pdf_size": 459248,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12727422308068490276&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Statistics, University of Waikato, Private Bag 3105, Hamilton 3240, New Zealand; School of Computer Science, University of Birmingham, Edgbaston, B15 2TT, UK",
        "aff_domain": "waikato.ac.nz;cs.bham.ac.uk",
        "email": "waikato.ac.nz;cs.bham.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Waikato;University of Birmingham",
        "aff_unique_dep": "Department of Statistics;School of Computer Science",
        "aff_unique_url": "https://www.waikato.ac.nz;https://www.birmingham.ac.uk",
        "aff_unique_abbr": "UoW;UoB",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Hamilton;Edgbaston",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "New Zealand;United Kingdom"
    },
    {
        "id": "19663c90cb",
        "title": "Second Order Online Collaborative Filtering",
        "site": "https://proceedings.mlr.press/v29/Lu13.html",
        "author": "Jing Lu; Steven Hoi; Jialei Wang",
        "abstract": "Collaborative Filtering (CF) is one of the most successful learning techniques in building real-world recommender systems. Traditional CF algorithms are often based on batch machine learning methods which suffer from several critical drawbacks, e.g., extremely expensive model retraining cost whenever new samples arrive, unable to capture the latest change of user preferences over time, and high cost and slow reaction to new users or products extension. Such limitations make batch learning based CF methods unsuitable for real-world online applications where data often arrives sequentially and user preferences may change dynamically and rapidly. To address these limitations, we investigate online collaborative filtering techniques for building live recommender systems where the CF model can evolve on-the-fly over time. Unlike the regular first order CF algorithms (e.g., online gradient descent for CF) that converge slowly, in this paper, we present a new framework of second order online collaborative filtering, i.e., Confidence Weighted Online Collaborative Filtering (CWOCF), which applies the second order online optimization methodology to tackle the online collaborative filtering task. We conduct extensive experiments on several large-scale datasets, in which the encouraging results demonstrate that the proposed algorithms obtain significantly lower errors (both RMSE and MAE) than the state-of-the-art first order CF algorithms when receiving the same amount of training data in the online learning process.",
        "bibtex": "@InProceedings{pmlr-v29-Lu13,\n  title = \t {Second Order Online Collaborative Filtering},\n  author = \t {Lu, Jing and Hoi, Steven and Wang, Jialei},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {325--340},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Lu13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Lu13.html},\n  abstract = \t {Collaborative Filtering (CF) is one of the most successful learning techniques in building real-world recommender systems. Traditional CF algorithms are often based on batch machine learning methods which suffer from several critical drawbacks, e.g., extremely expensive model retraining cost whenever new samples arrive, unable to capture the latest change of user preferences over time, and high cost and slow reaction to new users or products extension. Such limitations make batch learning based CF methods unsuitable for real-world online applications where data often arrives sequentially and user preferences may change dynamically and rapidly. To address these limitations, we investigate online collaborative filtering techniques for building live recommender systems where the CF model can evolve on-the-fly over time. Unlike the regular first order CF algorithms (e.g., online gradient descent for CF) that converge slowly, in this paper, we present a new framework of second order online collaborative filtering, i.e., Confidence Weighted Online Collaborative Filtering (CWOCF), which applies the second order online optimization methodology to tackle the online collaborative filtering task. We conduct extensive experiments on several large-scale datasets, in which the encouraging results demonstrate that the proposed algorithms obtain significantly lower errors (both RMSE and MAE) than the state-of-the-art first order CF algorithms when receiving the same amount of training data in the online learning process.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Lu13.pdf",
        "supp": "",
        "pdf_size": 802193,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5667256992870117079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7632a27f20",
        "title": "Stability of Multi-Task Kernel Regression Algorithms",
        "site": "https://proceedings.mlr.press/v29/Audiffren13.html",
        "author": "Julien Audiffren; Hachem Kadri",
        "abstract": "We study the stability properties of nonlinear multi-task regression in reproducing Hilbert spaces with operator-valued kernels.  Such kernels, a.k.a. multi-task kernels, are appropriate for learning problems with nonscalar outputs like multi-task learning and structured output prediction. We show that multi-task kernel regression algorithms are uniformly stable in the general case of infinite-dimensional output spaces.  We then derive under mild assumption on the kernel generalization bounds of such algorithms, and we show their consistency even with non Hilbert-Schmidt operator-valued kernels. We demonstrate how to apply the results to various multi-task kernel regression methods such as vector-valued SVR and functional ridge regression.",
        "bibtex": "@InProceedings{pmlr-v29-Audiffren13,\n  title = \t {Stability of Multi-Task Kernel Regression Algorithms},\n  author = \t {Audiffren, Julien and Kadri, Hachem},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {1--16},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Audiffren13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Audiffren13.html},\n  abstract = \t {We study the stability properties of nonlinear multi-task regression in reproducing Hilbert spaces with operator-valued kernels.  Such kernels, a.k.a. multi-task kernels, are appropriate for learning problems with nonscalar outputs like multi-task learning and structured output prediction. We show that multi-task kernel regression algorithms are uniformly stable in the general case of infinite-dimensional output spaces.  We then derive under mild assumption on the kernel generalization bounds of such algorithms, and we show their consistency even with non Hilbert-Schmidt operator-valued kernels. We demonstrate how to apply the results to various multi-task kernel regression methods such as vector-valued SVR and functional ridge regression.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Audiffren13.pdf",
        "supp": "",
        "pdf_size": 361849,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=867905492000470197&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Aix-Marseille Universit\u00e9, CNRS, LIF UMR 7279, 13000, Marseille, France; Aix-Marseille Universit\u00e9, CNRS, LIF UMR 7279, 13000, Marseille, France",
        "aff_domain": "lif.univ-mrs.fr;lif.univ-mrs.fr",
        "email": "lif.univ-mrs.fr;lif.univ-mrs.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Aix-Marseille Universit\u00e9",
        "aff_unique_dep": "LIF UMR 7279",
        "aff_unique_url": "https://www.univ-amu.fr",
        "aff_unique_abbr": "AMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Marseille",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "f65533650f",
        "title": "The Multi-Task Learning View of Multimodal Data",
        "site": "https://proceedings.mlr.press/v29/Kadri13.html",
        "author": "Hachem Kadri; Stephane Ayache; C\u00e9cile Capponi; Sokol Ko\u00e7o; Fran\u00e7ois-Xavier Dup\u00e9; Emilie Morvant",
        "abstract": "We study the problem of learning from multiple views using kernel methods in a supervised setting.  We approach this  problem from a multi-task learning point of view and illustrate how to capture the interesting multimodal structure of the data using multi-task kernels.  Our analysis shows that the multi-task perspective offers the flexibility to design more efficient multiple-source learning algorithms, and hence the ability to exploit multiple descriptions of the data. In particular, we formulate the multimodal learning framework using vector-valued reproducing kernel Hilbert spaces, and we derive specific multi-task kernels that can operate over multiple modalities. Finally,  we analyze the vector-valued regularized least squares algorithm in this context, and demonstrate its potential in a series of experiments with a real-world multimodal data set.",
        "bibtex": "@InProceedings{pmlr-v29-Kadri13,\n  title = \t {The Multi-Task Learning View of Multimodal Data},\n  author = \t {Kadri, Hachem and Ayache, Stephane and Capponi, C\u00e9cile and Ko\u00e7o, Sokol and Dup\u00e9, Fran\u00e7ois-Xavier and Morvant, Emilie},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {261--276},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Kadri13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Kadri13.html},\n  abstract = \t {We study the problem of learning from multiple views using kernel methods in a supervised setting.  We approach this  problem from a multi-task learning point of view and illustrate how to capture the interesting multimodal structure of the data using multi-task kernels.  Our analysis shows that the multi-task perspective offers the flexibility to design more efficient multiple-source learning algorithms, and hence the ability to exploit multiple descriptions of the data. In particular, we formulate the multimodal learning framework using vector-valued reproducing kernel Hilbert spaces, and we derive specific multi-task kernels that can operate over multiple modalities. Finally,  we analyze the vector-valued regularized least squares algorithm in this context, and demonstrate its potential in a series of experiments with a real-world multimodal data set.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Kadri13.pdf",
        "supp": "",
        "pdf_size": 337732,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12596698381180990394&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Aix-Marseille Universit\u00b4 e, CNRS, LIF UMR 7279, 13000, Marseille, France; Aix-Marseille Universit\u00b4 e, CNRS, LIF UMR 7279, 13000, Marseille, France; Aix-Marseille Universit\u00b4 e, CNRS, LIF UMR 7279, 13000, Marseille, France; Aix-Marseille Universit\u00b4 e, CNRS, LIF UMR 7279, 13000, Marseille, France; Aix-Marseille Universit\u00b4 e, CNRS, LIF UMR 7279, 13000, Marseille, France; Aix-Marseille Universit\u00b4 e, CNRS, LIF UMR 7279, 13000, Marseille, France",
        "aff_domain": "lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr",
        "email": "lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Aix-Marseille Universit\u00e9",
        "aff_unique_dep": "LIF UMR 7279",
        "aff_unique_url": "https://www.univ-amu.fr",
        "aff_unique_abbr": "AMU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Marseille",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "b712c68cdd",
        "title": "Unconfused Ultraconservative Multiclass Algorithms",
        "site": "https://proceedings.mlr.press/v29/Louche13.html",
        "author": "Ugo Louche; Liva Ralaivola",
        "abstract": "We tackle the problem of learning linear classifiers from noisy datasets in a multiclass setting. The two-class version of this problem was studied a few years ago by, e.g. Bylander (1994) and Blum et al. (1996): in these contributions, the proposed approaches to fight the noise revolve around a Perceptron learning scheme fed with peculiar examples computed through a weighted average of points from the noisy training set. We propose to build upon these approaches and we introduce a new algorithm called \\uma (for Unconfused Multiclass additive Algorithm) which may be seen as a generalization to the multiclass setting of the previous approaches. In order to characterize the noise we use the \\em confusion matrix as a multiclass extension of the classification noise studied in the aforementioned literature. Theoretically well-founded, \\uma furthermore displays very good empirical noise robustness, as evidenced by numerical simulations conducted on both synthetic  and real data.",
        "bibtex": "@InProceedings{pmlr-v29-Louche13,\n  title = \t {Unconfused Ultraconservative Multiclass Algorithms},\n  author = \t {Louche, Ugo and Ralaivola, Liva},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {309--324},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Louche13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Louche13.html},\n  abstract = \t {We tackle the problem of learning linear classifiers from noisy datasets in a multiclass setting. The two-class version of this problem was studied a few years ago by, e.g. Bylander (1994) and Blum et al. (1996): in these contributions, the proposed approaches to fight the noise revolve around a Perceptron learning scheme fed with peculiar examples computed through a weighted average of points from the noisy training set. We propose to build upon these approaches and we introduce a new algorithm called \\uma (for Unconfused Multiclass additive Algorithm) which may be seen as a generalization to the multiclass setting of the previous approaches. In order to characterize the noise we use the \\em confusion matrix as a multiclass extension of the classification noise studied in the aforementioned literature. Theoretically well-founded, \\uma furthermore displays very good empirical noise robustness, as evidenced by numerical simulations conducted on both synthetic  and real data.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Louche13.pdf",
        "supp": "",
        "pdf_size": 380293,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13201583741892045309&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Qarma, Lab. d\u2019Informatique Fondamentale de Marseille, CNRS, Aix-Marseille University, France; Qarma, Lab. d\u2019Informatique Fondamentale de Marseille, CNRS, Aix-Marseille University, France",
        "aff_domain": "lif.univ-mrs.fr;lif.univ-mrs.fr",
        "email": "lif.univ-mrs.fr;lif.univ-mrs.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Aix-Marseille University",
        "aff_unique_dep": "Lab. d\u2019Informatique Fondamentale de Marseille",
        "aff_unique_url": "https://www.univ-amu.fr",
        "aff_unique_abbr": "AMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "10f3c5231c",
        "title": "Using Hyperbolic Cross Approximation to measure and compensate Covariate Shift",
        "site": "https://proceedings.mlr.press/v29/Vanck13.html",
        "author": "Thomas Vanck; Jochen Garcke",
        "abstract": "The concept of covariate shift in supervised data analysis describes a difference between the training and test distribution while the conditional distribution remains the same.  To improve the prediction performance one can address such a change by using individual weights for each training datapoint, which emphasizes the training points close to the test data set so that these get a higher significance. We propose a new method for calculating such weights by minimizing a Fourier series approximation of distance measures, in particular we consider the total variation distance, the Euclidean distance and Kullback-Leibler divergence. To be able to use the Fourier approach for higher dimensional data, we employ the so-called hyperbolic cross approximation. Results show that the new approach can compete with the latest methods and that on real life data an improved performance can be obtained.",
        "bibtex": "@InProceedings{pmlr-v29-Vanck13,\n  title = \t {Using Hyperbolic Cross Approximation to measure and compensate Covariate Shift},\n  author = \t {Vanck, Thomas and Garcke, Jochen},\n  booktitle = \t {Proceedings of the 5th Asian Conference on Machine Learning},\n  pages = \t {435--450},\n  year = \t {2013},\n  editor = \t {Ong, Cheng Soon and Ho, Tu Bao},\n  volume = \t {29},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Australian National University, Canberra, Australia},\n  month = \t {13--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v29/Vanck13.pdf},\n  url = \t {https://proceedings.mlr.press/v29/Vanck13.html},\n  abstract = \t {The concept of covariate shift in supervised data analysis describes a difference between the training and test distribution while the conditional distribution remains the same.  To improve the prediction performance one can address such a change by using individual weights for each training datapoint, which emphasizes the training points close to the test data set so that these get a higher significance. We propose a new method for calculating such weights by minimizing a Fourier series approximation of distance measures, in particular we consider the total variation distance, the Euclidean distance and Kullback-Leibler divergence. To be able to use the Fourier approach for higher dimensional data, we employ the so-called hyperbolic cross approximation. Results show that the new approach can compete with the latest methods and that on real life data an improved performance can be obtained.}\n}",
        "pdf": "http://proceedings.mlr.press/v29/Vanck13.pdf",
        "supp": "",
        "pdf_size": 445647,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7057212290497099541&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Institut f\u00a8 ur Mathematik, Technische Universit\u00a8 at Berlin, Berlin; Institut f\u00a8 ur Numerische Simulation, Universit\u00a8 at Bonn+Fraunhofer SCAI, Sankt Augustin",
        "aff_domain": "math.tu-berlin.de;ins.uni-bonn.de",
        "email": "math.tu-berlin.de;ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Technische Universit\u00a8 at Berlin;University of Bonn;Fraunhofer Society",
        "aff_unique_dep": "Institut f\u00a8 ur Mathematik;Institut f\u00a8 ur Numerische Simulation;SCAI (Simulation and Optimization)",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.uni-bonn.de;https://www.scai.fraunhofer.de",
        "aff_unique_abbr": "TU Berlin;;Fraunhofer SCAI",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Berlin;;Sankt Augustin",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Germany"
    }
]