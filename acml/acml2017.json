[
    {
        "id": "c39da4e01b",
        "title": "A Covariance Matrix Adaptation Evolution Strategy for Direct Policy Search in Reproducing Kernel Hilbert Space",
        "site": "https://proceedings.mlr.press/v77/vien17a.html",
        "author": "Ngo Anh Vien; Viet-Hung Dang; TaeChoong Chung",
        "abstract": "The covariance matrix adaptation evolution strategy (CMA-ES) is an efficient derivative-free optimization algorithm. It optimizes a black-box objective function over a well defined parameter space. In some problems, such parameter spaces are defined using function approximation in which feature functions are manually defined. Therefore, the performance of those techniques strongly depends on the quality of chosen features. Hence, enabling CMA-ES to optimize on a more complex and general function class of the objective has long been desired. Specifically, we consider modeling the input space for black-box optimization in reproducing kernel Hilbert spaces (RKHS). This modeling leads to a functional optimization problem whose domain is a function space that enables us to optimize in a very rich function class. In addition, we propose CMA-ES-RKHS, a generalized CMA-ES framework, that performs black-box functional optimization in RKHS. A search distribution, represented as a Gaussian process, is adapted by updating both its mean function and covariance operator. Adaptive representation of the mean function and the covariance operator is achieved by resorting to sparsification. CMA-ES-RKHS is evaluated on two simple functional optimization problems and two bench-mark reinforcement learning (RL) domains. For an application in RL, we model policies for MDPs in RKHS and transform a cumulative return objective as a functional of RKHS policies, which can be optimized via CMA-ES-RKHS. This formulation results in a black-box functional policy search framework.",
        "bibtex": "@InProceedings{pmlr-v77-vien17a,\n  title = \t {A Covariance Matrix Adaptation Evolution Strategy for Direct Policy Search in Reproducing Kernel Hilbert Space},\n  author = \t {Vien, Ngo Anh and Dang, Viet-Hung and Chung, TaeChoong},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {606--621},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/vien17a/vien17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/vien17a.html},\n  abstract = \t {The covariance matrix adaptation evolution strategy (CMA-ES) is an efficient derivative-free optimization algorithm. It optimizes a black-box objective function over a well defined parameter space. In some problems, such parameter spaces are defined using function approximation in which feature functions are manually defined. Therefore, the performance of those techniques strongly depends on the quality of chosen features. Hence, enabling CMA-ES to optimize on a more complex and general function class of the objective has long been desired. Specifically, we consider modeling the input space for black-box optimization in reproducing kernel Hilbert spaces (RKHS). This modeling leads to a functional optimization problem whose domain is a function space that enables us to optimize in a very rich function class. In addition, we propose CMA-ES-RKHS, a generalized CMA-ES framework, that performs black-box functional optimization in RKHS. A search distribution, represented as a Gaussian process, is adapted by updating both its mean function and covariance operator. Adaptive representation of the mean function and the covariance operator is achieved by resorting to sparsification. CMA-ES-RKHS is evaluated on two simple functional optimization problems and two bench-mark reinforcement learning (RL) domains. For an application in RL, we model policies for MDPs in RKHS and transform a cumulative return objective as a functional of RKHS policies, which can be optimized via CMA-ES-RKHS. This formulation results in a black-box functional policy search framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/vien17a/vien17a.pdf",
        "supp": "",
        "pdf_size": 380337,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5183855665043446243&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "EEECS, Queen\u2019s University Belfast, United Kingdom; Research and Development Center, Duy Tan University, Vietnam; Department of Computer Engineering, Kyung Hee University, Korea",
        "aff_domain": "qub.ac.uk;gmail.com;khu.ac.kr",
        "email": "qub.ac.uk;gmail.com;khu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Queen\u2019s University Belfast;Duy Tan University;Kyung Hee University",
        "aff_unique_dep": "EEECS;Research and Development Center;Department of Computer Engineering",
        "aff_unique_url": "https://www.qub.ac.uk;;http://www.khu.ac.kr",
        "aff_unique_abbr": "QUB;;KHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "United Kingdom;Vietnam;South Korea"
    },
    {
        "id": "4626e5376e",
        "title": "A Mutually-Dependent Hadamard Kernel for Modelling Latent Variable Couplings",
        "site": "https://proceedings.mlr.press/v77/remes17a.html",
        "author": "Sami Remes; Markus Heinonen; Samuel Kaski",
        "abstract": "We introduce a novel kernel that models input-dependent couplings across multiple latent processes. The pairwise joint kernel measures covariance along inputs and across different latent signals in a mutually-dependent fashion. A latent correlation Gaussian process (LCGP) model combines these non-stationary latent components into multiple outputs by an input-dependent mixing matrix. Probit classification and support for multiple observation sets are derived by Variational Bayesian inference. Results on several datasets indicate that the LCGP model can recover the correlations between latent signals while simultaneously achieving state-of-the-art performance. We highlight the latent covariances with an EEG classification dataset where latent brain processes and their couplings simultaneously emerge from the model.",
        "bibtex": "@InProceedings{pmlr-v77-remes17a,\n  title = \t {A Mutually-Dependent Hadamard Kernel for Modelling Latent Variable Couplings},\n  author = \t {Remes, Sami and Heinonen, Markus and Kaski, Samuel},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {455--470},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/remes17a/remes17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/remes17a.html},\n  abstract = \t {We introduce a novel kernel that models input-dependent couplings across multiple latent processes. The pairwise joint kernel measures covariance along inputs and across different latent signals in a mutually-dependent fashion. A latent correlation Gaussian process (LCGP) model combines these non-stationary latent components into multiple outputs by an input-dependent mixing matrix. Probit classification and support for multiple observation sets are derived by Variational Bayesian inference. Results on several datasets indicate that the LCGP model can recover the correlations between latent signals while simultaneously achieving state-of-the-art performance. We highlight the latent covariances with an EEG classification dataset where latent brain processes and their couplings simultaneously emerge from the model.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/remes17a/remes17a.pdf",
        "supp": "",
        "pdf_size": 1988438,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3091374452987525475&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Helsinki Institute for Information Technology HIIT + Department of Computer Science, Aalto University; Helsinki Institute for Information Technology HIIT + Department of Computer Science, Aalto University; Helsinki Institute for Information Technology HIIT + Department of Computer Science, Aalto University",
        "aff_domain": "aalto.fi;aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Helsinki Institute for Information Technology;Aalto University",
        "aff_unique_dep": "HIIT;Department of Computer Science",
        "aff_unique_url": "https://www.hiit.fi;https://www.aalto.fi",
        "aff_unique_abbr": "HIIT;Aalto",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "dd774581fd",
        "title": "A Quantum-Inspired Ensemble Method and Quantum-Inspired Forest Regressors",
        "site": "https://proceedings.mlr.press/v77/xie17a.html",
        "author": "Zeke Xie; Issei Sato",
        "abstract": "We propose a Quantum-Inspired Subspace(QIS) Ensemble Method for generating feature ensembles based on feature selections. We assign each principal component a Fraction Transition Probability as its probability weight based on Principal Component Analysis and quantum interpretations. In order to generate the feature subset for each base regressor, we select a feature subset from principal components based on Fraction Transition Probabilities. The idea originating from quantum mechanics can encourage ensemble diversity and the accuracy simultaneously. We incorporate Quantum-Inspired Subspace Method into Random Forest and propose Quantum-Inspired Forest. We theoretically prove that the quantum interpretation corresponds to the first order approximation of ensemble regression. We also evaluate the empirical performance of Quantum-Inspired Forest and Random Forest in multiple hyperparameter settings. Quantum-Inspired Forest proves the significant robustness of the default hyperparameters on most data sets. The contribution of this work is two-fold, a novel ensemble regression algorithm inspired by quantum mechanics and the theoretical connection between quantum interpretations and machine learning algorithms.",
        "bibtex": "@InProceedings{pmlr-v77-xie17a,\n  title = \t {A Quantum-Inspired Ensemble Method and Quantum-Inspired Forest Regressors},\n  author = \t {Xie, Zeke and Sato, Issei},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {81--96},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/xie17a/xie17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/xie17a.html},\n  abstract = \t {We propose a Quantum-Inspired Subspace(QIS) Ensemble Method for generating feature ensembles based on feature selections. We assign each principal component a Fraction Transition Probability as its probability weight based on Principal Component Analysis and quantum interpretations. In order to generate the feature subset for each base regressor, we select a feature subset from principal components based on Fraction Transition Probabilities. The idea originating from quantum mechanics can encourage ensemble diversity and the accuracy simultaneously. We incorporate Quantum-Inspired Subspace Method into Random Forest and propose Quantum-Inspired Forest. We theoretically prove that the quantum interpretation corresponds to the first order approximation of ensemble regression. We also evaluate the empirical performance of Quantum-Inspired Forest and Random Forest in multiple hyperparameter settings. Quantum-Inspired Forest proves the significant robustness of the default hyperparameters on most data sets. The contribution of this work is two-fold, a novel ensemble regression algorithm inspired by quantum mechanics and the theoretical connection between quantum interpretations and machine learning algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/xie17a/xie17a.pdf",
        "supp": "",
        "pdf_size": 288903,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16275052227976366144&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Tokyo, 7 Chome-3-1 Hongo, Bunkyo, Tokyo, Japan; The University of Tokyo, 7 Chome-3-1 Hongo, Bunkyo, Tokyo, Japan",
        "aff_domain": "k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "email": "k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "db0aeabd7d",
        "title": "A Study on Trust Region Update Rules in Newton Methods for Large-scale Linear Classification",
        "site": "https://proceedings.mlr.press/v77/hsia17a.html",
        "author": "Chih-Yang Hsia; Ya Zhu; Chih-Jen Lin",
        "abstract": "The main task in training a linear classifier is to solve an unconstrained minimization problem. To apply an optimization method typically we iteratively find a good direction and then decide a suitable step size. Past developments of extending optimization methods for large-scale linear classification focus on finding the direction, but little attention has been paid on adjusting the step size. In this work, we explain that inappropriate step-size adjustment may lead to serious slow convergence. Among the two major methods for step-size selection, line search and trust region, we focus on investigating the trust region methods. After presenting some detailed analysis, we develop novel and effective techniques to adjust the trust-region size. Experiments indicate that our new settings significantly outperform existing implementations for large-scale linear classification.",
        "bibtex": "@InProceedings{pmlr-v77-hsia17a,\n  title = \t {A Study on Trust Region Update Rules in Newton Methods for Large-scale Linear Classification},\n  author = \t {Hsia, Chih-Yang and Zhu, Ya and Lin, Chih-Jen},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {33--48},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/hsia17a/hsia17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/hsia17a.html},\n  abstract = \t {The main task in training a linear classifier is to solve an unconstrained minimization problem. To apply an optimization method typically we iteratively find a good direction and then decide a suitable step size. Past developments of extending optimization methods for large-scale linear classification focus on finding the direction, but little attention has been paid on adjusting the step size. In this work, we explain that inappropriate step-size adjustment may lead to serious slow convergence. Among the two major methods for step-size selection, line search and trust region, we focus on investigating the trust region methods. After presenting some detailed analysis, we develop novel and effective techniques to adjust the trust-region size. Experiments indicate that our new settings significantly outperform existing implementations for large-scale linear classification.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/hsia17a/hsia17a.pdf",
        "supp": "",
        "pdf_size": 975332,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1766149448079866867&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Dept. of Computer Science, National Taiwan University, Taipei, Taiwan; Computer Science Department, New York University, New York, USA; Dept. of Computer Science, National Taiwan University, Taipei, Taiwan",
        "aff_domain": "ntu.edu.tw;nyu.edu;csie.ntu.edu.tw",
        "email": "ntu.edu.tw;nyu.edu;csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "National Taiwan University;New York University",
        "aff_unique_dep": "Dept. of Computer Science;Computer Science Department",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.nyu.edu",
        "aff_unique_abbr": "NTU;NYU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Taiwan;New York",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "316bc3e30f",
        "title": "A Word Embeddings Informed Focused Topic Model",
        "site": "https://proceedings.mlr.press/v77/zhao17a.html",
        "author": "He Zhao; Lan Du; Wray Buntine",
        "abstract": "In natural language processing and related fields, it has been shown that the word embeddings can successfully capture both the semantic and syntactic features of words. They can serve as complementary information to topics models, especially for the cases where word co-occurrence data is insufficient, such as with short texts. In this paper, we propose a focused topic model where how a topic focuses on words is informed by word embeddings. Our models is able to discover more informed and focused topics with more representative words, leading to better modelling accuracy and topic quality. With the data argumentation technique, we can derive an efficient Gibbs sampling algorithm that benefits from the fully local conjugacy of the model. We conduct extensive experiments on several real world datasets, which demonstrate that our model achieves comparable or improved performance in terms of both perplexity and topic coherence, particularly in handling short text data.",
        "bibtex": "@InProceedings{pmlr-v77-zhao17a,\n  title = \t {A Word Embeddings Informed Focused Topic Model},\n  author = \t {Zhao, He and Du, Lan and Buntine, Wray},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {423--438},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/zhao17a/zhao17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/zhao17a.html},\n  abstract = \t {In natural language processing and related fields, it has been shown that the word embeddings can successfully capture both the semantic and syntactic features of words. They can serve as complementary information to topics models, especially for the cases where word co-occurrence data is insufficient, such as with short texts. In this paper, we propose a focused topic model where how a topic focuses on words is informed by word embeddings. Our models is able to discover more informed and focused topics with more representative words, leading to better modelling accuracy and topic quality. With the data argumentation technique, we can derive an efficient Gibbs sampling algorithm that benefits from the fully local conjugacy of the model. We conduct extensive experiments on several real world datasets, which demonstrate that our model achieves comparable or improved performance in terms of both perplexity and topic coherence, particularly in handling short text data.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/zhao17a/zhao17a.pdf",
        "supp": "",
        "pdf_size": 390961,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13901449122442698432&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Faculty of Information Technology, Monash University, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia",
        "aff_domain": "monash.edu;monash.edu;monash.edu",
        "email": "monash.edu;monash.edu;monash.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "Faculty of Information Technology",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "69e59f9733",
        "title": "Accumulated Gradient Normalization",
        "site": "https://proceedings.mlr.press/v77/hermans17a.html",
        "author": "Joeri R. Hermans; Gerasimos Spanakis; Rico M\u00f6ckel",
        "abstract": "This work addresses the instability in asynchronous data parallel optimization. It does so by introducing a novel distributed optimizer which is able to efficiently optimize a centralized model under communication constraints. The optimizer achieves this by pushing a normalized sequence of first-order gradients to a parameter server. This implies that the magnitude of a worker delta is smaller compared to an accumulated gradient, and provides a better direction towards a minimum compared to first-order gradients, which in turn also forces possible implicit momentum fluctuations to be more aligned since we make the assumption that all workers contribute towards a single minima. As a result, our approach mitigates the parameter staleness problem more effectively since staleness in asynchrony induces (implicit) momentum, and achieves a better convergence rate compared to other optimizers such as asynchronous \\textsceasgd and \\textscdynsgd, which we show empirically.",
        "bibtex": "@InProceedings{pmlr-v77-hermans17a,\n  title = \t {Accumulated Gradient Normalization},\n  author = \t {Hermans, Joeri R. and Spanakis, Gerasimos and M\u00f6ckel, Rico},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {439--454},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/hermans17a/hermans17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/hermans17a.html},\n  abstract = \t {This work addresses the instability in asynchronous data parallel optimization. It does so by introducing a novel distributed optimizer which is able to efficiently optimize a centralized model under communication constraints. The optimizer achieves this by pushing a normalized sequence of first-order gradients to a parameter server. This implies that the magnitude of a worker delta is smaller compared to an accumulated gradient, and provides a better direction towards a minimum compared to first-order gradients, which in turn also forces possible implicit momentum fluctuations to be more aligned since we make the assumption that all workers contribute towards a single minima. As a result, our approach mitigates the parameter staleness problem more effectively since staleness in asynchrony induces (implicit) momentum, and achieves a better convergence rate compared to other optimizers such as asynchronous \\textsceasgd and \\textscdynsgd, which we show empirically.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/hermans17a/hermans17a.pdf",
        "supp": "",
        "pdf_size": 4946098,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10115216682442228229&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical Engineering and Computer Science, Li\u00e8ge University, Belgium; Department of Data Science & Knowledge Engineering, Maastricht University, The Netherlands; Department of Data Science & Knowledge Engineering, Maastricht University, The Netherlands",
        "aff_domain": "doct.ulg.ac.be;maastrichtuniversity.nl;maastrichtuniversity.nl",
        "email": "doct.ulg.ac.be;maastrichtuniversity.nl;maastrichtuniversity.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Li\u00e8ge University;Maastricht University",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Department of Data Science & Knowledge Engineering",
        "aff_unique_url": "https://www.ulg.ac.be;https://www.maastrichtuniversity.nl",
        "aff_unique_abbr": "ULg;MU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Belgium;Netherlands"
    },
    {
        "id": "04c0006ba8",
        "title": "Adaptive Sampling Scheme for Learning in Severely Imbalanced Large Scale Data",
        "site": "https://proceedings.mlr.press/v77/zhang17b.html",
        "author": "Wei Zhang; Said Kobeissi; Scott Tomko; Chris Challis",
        "abstract": "Imbalanced data poses a serious challenge for many machine learning and data mining applications. It may significantly affect the performance of learning algorithms. In digital marketing applications, events of interest (positive instances for building predictive models) such as click and purchase are rare. A retail website can easily receive a million visits every day, yet only a small percentage of visits lead to purchase. The large amount of raw data and the small percentage of positive instances make it challenging to build decent predictive models in a timely fashion. In this paper, we propose an adaptive sampling strategy to deal with this problem. It efficiently returns high quality training data, ensures system responsiveness and improves predictive performances.",
        "bibtex": "@InProceedings{pmlr-v77-zhang17b,\n  title = \t {Adaptive Sampling Scheme for Learning in Severely Imbalanced Large Scale Data},\n  author = \t {Zhang, Wei and Kobeissi, Said and Tomko, Scott and Challis, Chris},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {240--247},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/zhang17b/zhang17b.pdf},\n  url = \t {https://proceedings.mlr.press/v77/zhang17b.html},\n  abstract = \t {Imbalanced data poses a serious challenge for many machine learning and data mining applications. It may significantly affect the performance of learning algorithms. In digital marketing applications, events of interest (positive instances for building predictive models) such as click and purchase are rare. A retail website can easily receive a million visits every day, yet only a small percentage of visits lead to purchase. The large amount of raw data and the small percentage of positive instances make it challenging to build decent predictive models in a timely fashion. In this paper, we propose an adaptive sampling strategy to deal with this problem. It efficiently returns high quality training data, ensures system responsiveness and improves predictive performances.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/zhang17b/zhang17b.pdf",
        "supp": "",
        "pdf_size": 182085,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1083193174344370621&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Adobe Systems, Mclean, USA; Adobe Systems, Mclean, USA; Adobe Systems, Mclean, USA; Adobe Systems, Mclean, USA",
        "aff_domain": "adobe.com;adobe.com;adobe.com;adobe.com",
        "email": "adobe.com;adobe.com;adobe.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Adobe",
        "aff_unique_dep": "Adobe Systems",
        "aff_unique_url": "https://www.adobe.com",
        "aff_unique_abbr": "Adobe",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mclean",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5c0353d1e8",
        "title": "Attentive Path Combination for Knowledge Graph Completion",
        "site": "https://proceedings.mlr.press/v77/jiang17a.html",
        "author": "Xiaotian Jiang; Quan Wang; Baoyuan Qi; Yongqin Qiu; Peng Li; Bin Wang",
        "abstract": "Knowledge graphs (KGs) are often significantly incomplete, necessitating a demand for KG completion. Path-based relation inference is one of the most important approaches to this task. Traditional methods treat each path between entity pairs as an atomic feature, thus inducing sparsity. Recently, neural network models solve this problem by decomposing a path as the sequence of relations in the path, before modelling path representations with Recurrent Neural Network (RNN) architectures. In cases there are multiple paths between an entity pair, state-of-the-art neural models either select only one path, or make usage of simple score pooling methods like Top-K, Average, LogSumExp. Unfortunately, none of these methods can model the scenario where relations can only be inferred by considering multiple informative paths collectively. In this paper, we propose a novel path-based relation inference model that learns entity pair representations with attentive path combination. Given an entity pair and a set of paths connecting the pair, our model allows for integrating information from each informative path, and form a dynamic entity pair representation for each query relation. We empirically evaluate the proposed method on a real-world dataset. Experimental results show that the proposed model achieves better performance than state-of-the-art path-based relation inference methods.",
        "bibtex": "@InProceedings{pmlr-v77-jiang17a,\n  title = \t {Attentive Path Combination for Knowledge Graph Completion},\n  author = \t {Jiang, Xiaotian and Wang, Quan and Qi, Baoyuan and Qiu, Yongqin and Li, Peng and Wang, Bin},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {590--605},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/jiang17a/jiang17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/jiang17a.html},\n  abstract = \t {Knowledge graphs (KGs) are often significantly incomplete, necessitating a demand for KG completion. Path-based relation inference is one of the most important approaches to this task. Traditional methods treat each path between entity pairs as an atomic feature, thus inducing sparsity. Recently, neural network models solve this problem by decomposing a path as the sequence of relations in the path, before modelling path representations with Recurrent Neural Network (RNN) architectures. In cases there are multiple paths between an entity pair, state-of-the-art neural models either select only one path, or make usage of simple score pooling methods like Top-K, Average, LogSumExp. Unfortunately, none of these methods can model the scenario where relations can only be inferred by considering multiple informative paths collectively. In this paper, we propose a novel path-based relation inference model that learns entity pair representations with attentive path combination. Given an entity pair and a set of paths connecting the pair, our model allows for integrating information from each informative path, and form a dynamic entity pair representation for each query relation. We empirically evaluate the proposed method on a real-world dataset. Experimental results show that the proposed model achieves better performance than state-of-the-art path-based relation inference methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/jiang17a/jiang17a.pdf",
        "supp": "",
        "pdf_size": 406867,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9748511218589080966&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, Chinese Academy of Sciences+State Key Laboratory of Information Security, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences+School of Cyber Security, Chinese Academy of Sciences",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0+0;0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering",
        "aff_unique_url": "http://www.cas.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "c323c3736d",
        "title": "Computer Assisted Composition with Recurrent Neural Networks",
        "site": "https://proceedings.mlr.press/v77/walder17a.html",
        "author": "Christian Walder; Dongwoo Kim",
        "abstract": "Sequence modeling with neural networks has lead to powerful models of symbolic music data. We address the problem of exploiting these models to reach creative musical goals, by combining with human input. To this end we generalise previous work, which sampled Markovian sequence models under the constraint that the sequence belong to the language of a given finite state machine provided by the human. We consider more expressive non-Markov models, thereby requiring approximate sampling which we provide in the form of an efficient sequential Monte Carlo method. In addition we provide and compare with a beam search strategy for conditional probability maximisation.\r Our algorithms are capable of convincingly re-harmonising famous musical works. To demonstrate this we provide visualisations, quantitative experiments, a human listening test and audio examples. We find both the sampling and optimisation procedures to be effective, yet complementary in character. For the case of highly permissive constraint sets, we find that sampling is to be preferred due to the overly regular nature of the optimisation based results. The generality of our algorithms permits countless other creative applications.",
        "bibtex": "@InProceedings{pmlr-v77-walder17a,\n  title = \t {Computer Assisted Composition with Recurrent Neural Networks},\n  author = \t {Walder, Christian and Kim, Dongwoo},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {359--374},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/walder17a/walder17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/walder17a.html},\n  abstract = \t {Sequence modeling with neural networks has lead to powerful models of symbolic music data. We address the problem of exploiting these models to reach creative musical goals, by combining with human input. To this end we generalise previous work, which sampled Markovian sequence models under the constraint that the sequence belong to the language of a given finite state machine provided by the human. We consider more expressive non-Markov models, thereby requiring approximate sampling which we provide in the form of an efficient sequential Monte Carlo method. In addition we provide and compare with a beam search strategy for conditional probability maximisation.\r Our algorithms are capable of convincingly re-harmonising famous musical works. To demonstrate this we provide visualisations, quantitative experiments, a human listening test and audio examples. We find both the sampling and optimisation procedures to be effective, yet complementary in character. For the case of highly permissive constraint sets, we find that sampling is to be preferred due to the overly regular nature of the optimisation based results. The generality of our algorithms permits countless other creative applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/walder17a/walder17a.pdf",
        "supp": "",
        "pdf_size": 631578,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6006246650453663095&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Data61 at CSIRO, Australia; Australian National University",
        "aff_domain": "data61.csiro.au;anu.edu.au",
        "email": "data61.csiro.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "CSIRO;Australian National University",
        "aff_unique_dep": "Data61;",
        "aff_unique_url": "https://www.csiro.au;https://www.anu.edu.au",
        "aff_unique_abbr": "CSIRO;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "d546a38e65",
        "title": "Data sparse nonparametric regression with $\u03b5$-insensitive losses",
        "site": "https://proceedings.mlr.press/v77/sangnier17a.html",
        "author": "Maxime Sangnier; Olivier Fercoq; Florence d\u2019Alch\u00e9-Buc",
        "abstract": "Leveraging the celebrated\tsupport vector regression (SVR) method, we propose a unifying framework in order to deliver regression machines in reproducing kernel Hilbert spaces (RKHSs) with data sparsity. The central point is a new definition of $\u03b5$-insensitivity, valid for many regression losses (including quantile and expectile regression) and their multivariate extensions.\tWe show that the dual optimization problem to empirical risk minimization with $\u03b5$-insensitivity involves a data sparse regularization. We also provide an analysis of the excess of risk as well as a randomized coordinate descent algorithm for solving the dual.\tNumerical experiments validate our approach.",
        "bibtex": "@InProceedings{pmlr-v77-sangnier17a,\n  title = \t {Data sparse nonparametric regression with $\u03b5$-insensitive losses},\n  author = \t {Sangnier, Maxime and Fercoq, Olivier and d\u2019Alch\u00e9-Buc, Florence},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {192--207},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/sangnier17a/sangnier17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/sangnier17a.html},\n  abstract = \t {Leveraging the celebrated\tsupport vector regression (SVR) method, we propose a unifying framework in order to deliver regression machines in reproducing kernel Hilbert spaces (RKHSs) with data sparsity. The central point is a new definition of $\u03b5$-insensitivity, valid for many regression losses (including quantile and expectile regression) and their multivariate extensions.\tWe show that the dual optimization problem to empirical risk minimization with $\u03b5$-insensitivity involves a data sparse regularization. We also provide an analysis of the excess of risk as well as a randomized coordinate descent algorithm for solving the dual.\tNumerical experiments validate our approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/sangnier17a/sangnier17a.pdf",
        "supp": "",
        "pdf_size": 686577,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6103120399378430468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Sorbonne Universit\u00b4 es, UPMC Univ Paris 06, CNRS, Paris, France; Universit\u00b4 e Paris-Saclay, T\u00b4 el\u00b4 ecom ParisTech, LTCI, Paris, France; Universit\u00b4 e Paris-Saclay, T\u00b4 el\u00b4 ecom ParisTech, LTCI, Paris, France",
        "aff_domain": "upmc.fr;telecom-paristech.fr;telecom-paristech.fr",
        "email": "upmc.fr;telecom-paristech.fr;telecom-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Sorbonne Universit\u00e9s;Universit\u00b4 e Paris-Saclay",
        "aff_unique_dep": ";T\u00b4 el\u00b4 ecom ParisTech, LTCI",
        "aff_unique_url": "https://www.sorbonne-universite.fr;https://www.universite-paris-saclay.fr",
        "aff_unique_abbr": "Sorbonne;UPS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "718b46969f",
        "title": "Deep Competitive Pathway Networks",
        "site": "https://proceedings.mlr.press/v77/chang17a.html",
        "author": "Jia-Ren Chang; Yong-Sheng Chen",
        "abstract": "In the design of deep neural architectures, recent studies have demonstrated the benefits of grouping subnetworks into a larger network. For examples, the Inception architecture integrates multi-scale subnetworks and the residual network can be regarded that a residual unit combines a residual subnetwork with an identity shortcut. In this work, we embrace this observation and propose the Competitive Pathway Network (CoPaNet). The CoPaNet comprises a stack of competitive pathway units and each unit contains multiple parallel residual-type subnetworks followed by a max operation for feature competition. This mechanism enhances the model capability by learning a variety of features in subnetworks. The proposed strategy explicitly shows that the features propagate through pathways in various routing patterns, which is referred to as pathway encoding of category information. Moreover, the cross-block shortcut can be added to the CoPaNet to encourage feature reuse. We evaluated the proposed CoPaNet on four object recognition benchmarks: CIFAR-10, CIFAR-100, SVHN, and ImageNet. CoPaNet obtained the state-of-the-art or comparable results using similar amounts of parameters. The code of CoPaNet is available at: \\urlhttps://github.com/JiaRenChang/CoPaNet.",
        "bibtex": "@InProceedings{pmlr-v77-chang17a,\n  title = \t {Deep Competitive Pathway Networks},\n  author = \t {Chang, Jia-Ren and Chen, Yong-Sheng},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {264--278},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/chang17a/chang17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/chang17a.html},\n  abstract = \t {In the design of deep neural architectures, recent studies have demonstrated the benefits of grouping subnetworks into a larger network. For examples, the Inception architecture integrates multi-scale subnetworks and the residual network can be regarded that a residual unit combines a residual subnetwork with an identity shortcut. In this work, we embrace this observation and propose the Competitive Pathway Network (CoPaNet). The CoPaNet comprises a stack of competitive pathway units and each unit contains multiple parallel residual-type subnetworks followed by a max operation for feature competition. This mechanism enhances the model capability by learning a variety of features in subnetworks. The proposed strategy explicitly shows that the features propagate through pathways in various routing patterns, which is referred to as pathway encoding of category information. Moreover, the cross-block shortcut can be added to the CoPaNet to encourage feature reuse. We evaluated the proposed CoPaNet on four object recognition benchmarks: CIFAR-10, CIFAR-100, SVHN, and ImageNet. CoPaNet obtained the state-of-the-art or comparable results using similar amounts of parameters. The code of CoPaNet is available at: \\urlhttps://github.com/JiaRenChang/CoPaNet.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/chang17a/chang17a.pdf",
        "supp": "",
        "pdf_size": 1148032,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8964423783217991553&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan",
        "aff_domain": "nctu.edu.tw;cs.nctu.edu.tw",
        "email": "nctu.edu.tw;cs.nctu.edu.tw",
        "github": "https://github.com/JiaRenChang/CoPaNet",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Chiao Tung University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nctu.edu.tw",
        "aff_unique_abbr": "NCTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "6a90ee95b9",
        "title": "Distributionally Robust Groupwise Regularization Estimator",
        "site": "https://proceedings.mlr.press/v77/blanchet17a.html",
        "author": "Jose Blanchet; Yang Kang",
        "abstract": "Regularized estimators in the context of group variables have been applied successfully in model and feature selection in order to preserve interpretability. We formulate a Distributionally Robust Optimization (DRO) problem which recovers popular estimators, such as Group Square Root Lasso (GSRL). Our DRO formulation allows us to interpret GSRL as a game, in which we learn a regression parameter while an adversary chooses a perturbation of the data. We wish to pick the parameter to minimize the expected loss under any plausible model chosen by the adversary - who, on the other hand, wishes to increase the expected loss. The regularization parameter turns out to be precisely determined by the amount of perturbation on the training data allowed by the adversary. In this paper, we introduce a data-driven (statistical) criterion for the optimal choice of regularization, which we evaluate asymptotically, in closed form, as the size of the training set increases. Our easy-to-evaluate regularization formula is compared against cross-validation, showing comparable performance.",
        "bibtex": "@InProceedings{pmlr-v77-blanchet17a,\n  title = \t {Distributionally Robust Groupwise Regularization Estimator},\n  author = \t {Blanchet, Jose and Kang, Yang},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {97--112},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/blanchet17a/blanchet17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/blanchet17a.html},\n  abstract = \t {Regularized estimators in the context of group variables have been applied successfully in model and feature selection in order to preserve interpretability. We formulate a Distributionally Robust Optimization (DRO) problem which recovers popular estimators, such as Group Square Root Lasso (GSRL). Our DRO formulation allows us to interpret GSRL as a game, in which we learn a regression parameter while an adversary chooses a perturbation of the data. We wish to pick the parameter to minimize the expected loss under any plausible model chosen by the adversary - who, on the other hand, wishes to increase the expected loss. The regularization parameter turns out to be precisely determined by the amount of perturbation on the training data allowed by the adversary. In this paper, we introduce a data-driven (statistical) criterion for the optimal choice of regularization, which we evaluate asymptotically, in closed form, as the size of the training set increases. Our easy-to-evaluate regularization formula is compared against cross-validation, showing comparable performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/blanchet17a/blanchet17a.pdf",
        "supp": "",
        "pdf_size": 622399,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18356782740119993408&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, Columbia University; Department of Statistics, Columbia University",
        "aff_domain": "columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8ec51e0764",
        "title": "Instance Specific Discriminative Modal Pursuit: A Serialized Approach",
        "site": "https://proceedings.mlr.press/v77/yang17a.html",
        "author": "Yang Yang; De-Chuan Zhan; Ying Fan; Yuan Jiang",
        "abstract": "With the fast development of data collection techniques, a huge amount of complex multi-modal data are generated, shared and stored on the Internet. The burden of extracting multi-modal features for test instances in data analysis becomes the main fact that hurts the efficiency of prediction.  In this paper, in order to reduce the modal extraction cost in serialized classification system, we propose a novel end-to-end serialized adaptive decision approach named Discriminative Modal Pursuit (\\sc Dmp), which can automatically extract instance-specifically discriminative modal sequence for reducing the cost of feature extraction in the test phase. Rather than jointly optimize a highly non-convex empirical risk minimization problem, we are inspired by LSTM, and the proposed \\sc Dmp can turn to learn the decision policies which predict the label information and decide the modalities to be extracted simultaneously within limited modal acquisition budget. Consequently, \\sc Dmp approach can balance the classification performance and modal feature extraction cost by utilizing different modalities for different test instances. Empirical studies show that \\sc Dmp is more efficient and effective than existing modal/feature extraction methods.",
        "bibtex": "@InProceedings{pmlr-v77-yang17a,\n  title = \t {Instance Specific Discriminative Modal Pursuit: A Serialized Approach},\n  author = \t {Yang, Yang and Zhan, De-Chuan and Fan, Ying and Jiang, Yuan},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {65--80},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/yang17a/yang17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/yang17a.html},\n  abstract = \t {With the fast development of data collection techniques, a huge amount of complex multi-modal data are generated, shared and stored on the Internet. The burden of extracting multi-modal features for test instances in data analysis becomes the main fact that hurts the efficiency of prediction.  In this paper, in order to reduce the modal extraction cost in serialized classification system, we propose a novel end-to-end serialized adaptive decision approach named Discriminative Modal Pursuit (\\sc Dmp), which can automatically extract instance-specifically discriminative modal sequence for reducing the cost of feature extraction in the test phase. Rather than jointly optimize a highly non-convex empirical risk minimization problem, we are inspired by LSTM, and the proposed \\sc Dmp can turn to learn the decision policies which predict the label information and decide the modalities to be extracted simultaneously within limited modal acquisition budget. Consequently, \\sc Dmp approach can balance the classification performance and modal feature extraction cost by utilizing different modalities for different test instances. Empirical studies show that \\sc Dmp is more efficient and effective than existing modal/feature extraction methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/yang17a/yang17a.pdf",
        "supp": "",
        "pdf_size": 1016779,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5035934703766079303&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China",
        "aff_domain": "LAMDA.NJU.EDU.CN;LAMDA.NJU.EDU.CN;LAMDA.NJU.EDU.CN;LAMDA.NJU.EDU.CN",
        "email": "LAMDA.NJU.EDU.CN;LAMDA.NJU.EDU.CN;LAMDA.NJU.EDU.CN;LAMDA.NJU.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "8e0870864a",
        "title": "Learning Convolutional Neural Networks using Hybrid Orthogonal Projection and Estimation",
        "site": "https://proceedings.mlr.press/v77/pan17a.html",
        "author": "Hengyue Pan; Hui Jiang",
        "abstract": "Convolutional neural networks (CNNs) have yielded the excellent performance in a variety of computer vision tasks, where CNNs typically adopt a similar structure consisting of convolution layers, pooling layers and fully connected layers. In this paper, we propose to apply a novel method, namely Hybrid Orthogonal Projection and Estimation (HOPE), to CNNs in order to introduce orthogonality into the CNN structure. The HOPE model can be viewed as a hybrid model to combine feature extraction using orthogonal linear projection with mixture models. It is an effective model to extract useful information from the original high-dimension feature vectors and meanwhile filter out irrelevant noises. In this work, we present three different ways to apply the HOPE models to CNNs, i.e., \\em HOPE-Input, \\em single-HOPE-Block and \\em multi-HOPE-Blocks. For \\em HOPE-Input CNNs, a HOPE layer is directly used right after the input to de-correlate high-dimension input feature vectors. Alternatively, in \\em single-HOPE-Block and \\em multi-HOPE-Blocks CNNs, we consider to use HOPE layers to replace one or more blocks in the CNNs, where one block may include several convolutional layers and one pooling layer. The experimental results on CIFAR-10, CIFAR-100 and ImageNet databases have shown that the orthogonal constraints imposed by the HOPE layers can significantly improve the performance of CNNs in these image classification tasks (we have achieved one of the best performance when image augmentation has not been applied, and top 5 performance with image augmentation).",
        "bibtex": "@InProceedings{pmlr-v77-pan17a,\n  title = \t {Learning Convolutional Neural Networks using Hybrid Orthogonal Projection and Estimation},\n  author = \t {Pan, Hengyue and Jiang, Hui},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {1--16},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/pan17a/pan17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/pan17a.html},\n  abstract = \t {Convolutional neural networks (CNNs) have yielded the excellent performance in a variety of computer vision tasks, where CNNs typically adopt a similar structure consisting of convolution layers, pooling layers and fully connected layers. In this paper, we propose to apply a novel method, namely Hybrid Orthogonal Projection and Estimation (HOPE), to CNNs in order to introduce orthogonality into the CNN structure. The HOPE model can be viewed as a hybrid model to combine feature extraction using orthogonal linear projection with mixture models. It is an effective model to extract useful information from the original high-dimension feature vectors and meanwhile filter out irrelevant noises. In this work, we present three different ways to apply the HOPE models to CNNs, i.e., \\em HOPE-Input, \\em single-HOPE-Block and \\em multi-HOPE-Blocks. For \\em HOPE-Input CNNs, a HOPE layer is directly used right after the input to de-correlate high-dimension input feature vectors. Alternatively, in \\em single-HOPE-Block and \\em multi-HOPE-Blocks CNNs, we consider to use HOPE layers to replace one or more blocks in the CNNs, where one block may include several convolutional layers and one pooling layer. The experimental results on CIFAR-10, CIFAR-100 and ImageNet databases have shown that the orthogonal constraints imposed by the HOPE layers can significantly improve the performance of CNNs in these image classification tasks (we have achieved one of the best performance when image augmentation has not been applied, and top 5 performance with image augmentation).}\n}",
        "pdf": "http://proceedings.mlr.press/v77/pan17a/pan17a.pdf",
        "supp": "",
        "pdf_size": 2761588,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9139297297047782810&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "iFLYTEK Laboratory for Neural Computing and Machine Learning (iNCML), Department of Electrical Engineering and Computer Science, York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, CANADA; iFLYTEK Laboratory for Neural Computing and Machine Learning (iNCML), Department of Electrical Engineering and Computer Science, York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, CANADA",
        "aff_domain": "CSE.YORKU.CA;CSE.YORKU.CA",
        "email": "CSE.YORKU.CA;CSE.YORKU.CA",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "York University",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.yorku.ca",
        "aff_unique_abbr": "York U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2b2f259cda",
        "title": "Learning Deep Semantic Embeddings for Cross-Modal Retrieval",
        "site": "https://proceedings.mlr.press/v77/kang17a.html",
        "author": "Cuicui Kang; Shengcai Liao; Zhen Li; Zigang Cao; Gang Xiong",
        "abstract": "Deep learning methods have been actively researched for cross-modal retrieval, with the softmax cross-entropy loss commonly applied for supervised learning. However, the softmax cross-entropy loss is known to result in large intra-class variances, which is not not very suited for cross-modal matching. In this paper, a deep architecture called Deep Semantic Embedding (DSE) is proposed, which is trained in an end-to-end manner for image-text cross-modal retrieval. With images and texts mapped to a feature embedding space, class labels are used to guide the embedding learning, so that the embedding space has a semantic meaning common for both images and texts. This way, the difference between different modalities is eliminated. Under this framework, the center loss is introduced beyond the commonly used softmax cross-entropy loss to achieve both inter-class separation and intra-class compactness. Besides, a distance based softmax cross-entropy loss is proposed to jointly consider the softmax cross-entropy and center losses in fully gradient based learning. Experiments have been done on three popular image-text cross-modal retrieval databases, showing that the proposed algorithms have achieved the best overall performances.",
        "bibtex": "@InProceedings{pmlr-v77-kang17a,\n  title = \t {Learning Deep Semantic Embeddings for Cross-Modal Retrieval},\n  author = \t {Kang, Cuicui and Liao, Shengcai and Li, Zhen and Cao, Zigang and Xiong, Gang},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {471--486},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/kang17a/kang17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/kang17a.html},\n  abstract = \t {Deep learning methods have been actively researched for cross-modal retrieval, with the softmax cross-entropy loss commonly applied for supervised learning. However, the softmax cross-entropy loss is known to result in large intra-class variances, which is not not very suited for cross-modal matching. In this paper, a deep architecture called Deep Semantic Embedding (DSE) is proposed, which is trained in an end-to-end manner for image-text cross-modal retrieval. With images and texts mapped to a feature embedding space, class labels are used to guide the embedding learning, so that the embedding space has a semantic meaning common for both images and texts. This way, the difference between different modalities is eliminated. Under this framework, the center loss is introduced beyond the commonly used softmax cross-entropy loss to achieve both inter-class separation and intra-class compactness. Besides, a distance based softmax cross-entropy loss is proposed to jointly consider the softmax cross-entropy and center losses in fully gradient based learning. Experiments have been done on three popular image-text cross-modal retrieval databases, showing that the proposed algorithms have achieved the best overall performances.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/kang17a/kang17a.pdf",
        "supp": "",
        "pdf_size": 723784,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4724497215833606771&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a8ad76e6da",
        "title": "Learning Predictive Leading Indicators for Forecasting Time Series Systems with Unknown Clusters of Forecast Tasks",
        "site": "https://proceedings.mlr.press/v77/gregorova17a.html",
        "author": "Magda Gregorov\u00e1; Alexandros Kalousis; St\u00e9phane Marchand-Maillet",
        "abstract": "We present a new method for forecasting systems of multiple interrelated time series. The method learns the forecast models together with discovering leading indicators from within the system that serve as good predictors improving the forecast accuracy and a cluster structure of the predictive tasks around these. The method is based on the classical linear vector autoregressive model (VAR) and links the discovery of the leading indicators to inferring sparse graphs of Granger causality. We formulate a new constrained optimisation problem to promote the desired sparse structures across the models and the sharing of information amongst the learning tasks in a multi-task manner. We propose an algorithm for solving the problem and document on a battery of synthetic and real-data experiments the advantages of our new method over baseline VAR models as well as the state-of-the-art sparse VAR learning methods.",
        "bibtex": "@InProceedings{pmlr-v77-gregorova17a,\n  title = \t {Learning Predictive Leading Indicators for Forecasting Time Series Systems with Unknown Clusters of Forecast Tasks},\n  author = \t {Gregorov\u00e1, Magda and Kalousis, Alexandros and Marchand-Maillet, St\u00e9phane},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {161--176},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/gregorova17a/gregorova17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/gregorova17a.html},\n  abstract = \t {We present a new method for forecasting systems of multiple interrelated time series. The method learns the forecast models together with discovering leading indicators from within the system that serve as good predictors improving the forecast accuracy and a cluster structure of the predictive tasks around these. The method is based on the classical linear vector autoregressive model (VAR) and links the discovery of the leading indicators to inferring sparse graphs of Granger causality. We formulate a new constrained optimisation problem to promote the desired sparse structures across the models and the sharing of information amongst the learning tasks in a multi-task manner. We propose an algorithm for solving the problem and document on a battery of synthetic and real-data experiments the advantages of our new method over baseline VAR models as well as the state-of-the-art sparse VAR learning methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/gregorova17a/gregorova17a.pdf",
        "supp": "",
        "pdf_size": 485467,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wlavL8J_VE8J:scholar.google.com/&scioq=Learning+Predictive+Leading+Indicators+for+Forecasting+Time+Series+Systems+with+Unknown+Clusters+of+Forecast+Tasks&hl=en&as_sdt=0,5",
        "gs_version_total": 9,
        "aff": "Geneva School of Business Administration, HES-SO University of Applied Sciences of Western Switzerland + University of Geneva, Switzerland; Geneva School of Business Administration, HES-SO University of Applied Sciences of Western Switzerland + University of Geneva, Switzerland; University of Geneva, Switzerland",
        "aff_domain": "hesge.ch;hesge.ch;unige.ch",
        "email": "hesge.ch;hesge.ch;unige.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;1",
        "aff_unique_norm": "HES-SO University of Applied Sciences of Western Switzerland;University of Geneva",
        "aff_unique_dep": "School of Business Administration;",
        "aff_unique_url": "https://www.hes-so.ch/en;https://www.unige.ch",
        "aff_unique_abbr": "HES-SO;UNIGE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Geneva;",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "4dc1cdd1f2",
        "title": "Learning RBM with a DC programming Approach",
        "site": "https://proceedings.mlr.press/v77/upadhya17a.html",
        "author": "Vidyadhar Upadhya; P. S. Sastry",
        "abstract": "By exploiting the property that the RBM log-likelihood function is the difference of convex functions, we formulate a stochastic variant of the difference of convex functions (DC) programming to minimize the negative log-likelihood. Interestingly, the traditional contrastive divergence algorithm is a special case of the above formulation and the hyperparameters of the two algorithms can be chosen such that the amount of computation per mini-batch is identical. We show that for a given computational budget the proposed algorithm almost always reaches a higher log-likelihood more rapidly, compared to the standard contrastive divergence algorithm. Further, we modify this algorithm to use the centered gradients and show that it is more efficient and effective compared to the standard centered gradient algorithm on benchmark datasets.",
        "bibtex": "@InProceedings{pmlr-v77-upadhya17a,\n  title = \t {Learning RBM with a DC programming Approach},\n  author = \t {Upadhya, Vidyadhar and Sastry, P. S.},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {498--513},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/upadhya17a/upadhya17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/upadhya17a.html},\n  abstract = \t {By exploiting the property that the RBM log-likelihood function is the difference of convex functions, we formulate a stochastic variant of the difference of convex functions (DC) programming to minimize the negative log-likelihood. Interestingly, the traditional contrastive divergence algorithm is a special case of the above formulation and the hyperparameters of the two algorithms can be chosen such that the amount of computation per mini-batch is identical. We show that for a given computational budget the proposed algorithm almost always reaches a higher log-likelihood more rapidly, compared to the standard contrastive divergence algorithm. Further, we modify this algorithm to use the centered gradients and show that it is more efficient and effective compared to the standard centered gradient algorithm on benchmark datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/upadhya17a/upadhya17a.pdf",
        "supp": "",
        "pdf_size": 476739,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=984834238300022375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Electrical Engineering Dept, Indian Institute of Science, Bangalore; Electrical Engineering. Dept, Indian Institute of Science, Bangalore",
        "aff_domain": "ee.iisc.ernet.in;ee.iisc.ernet.in",
        "email": "ee.iisc.ernet.in;ee.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Electrical Engineering Dept",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "da3374ddf2",
        "title": "Limits of End-to-End Learning",
        "site": "https://proceedings.mlr.press/v77/glasmachers17a.html",
        "author": "Tobias Glasmachers",
        "abstract": "End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning systems are specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all \u201cperipheral\u201d modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex.\r In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of \\emphscaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.",
        "bibtex": "@InProceedings{pmlr-v77-glasmachers17a,\n  title = \t {Limits of End-to-End Learning},\n  author = \t {Glasmachers, Tobias},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {17--32},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/glasmachers17a/glasmachers17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/glasmachers17a.html},\n  abstract = \t {End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning systems are specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all \u201cperipheral\u201d modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex.\r In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of \\emphscaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/glasmachers17a/glasmachers17a.pdf",
        "supp": "",
        "pdf_size": 293926,
        "gs_citation": 265,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8283636694822590738&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Neural Computation, Ruhr-University Bochum, Germany",
        "aff_domain": "ini.rub.de",
        "email": "ini.rub.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ruhr-University Bochum",
        "aff_unique_dep": "Institute for Neural Computation",
        "aff_unique_url": "https://www.ruhr-uni-bochum.de",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "20bc39f991",
        "title": "Locally Smoothed Neural Networks",
        "site": "https://proceedings.mlr.press/v77/pang17a.html",
        "author": "Liang Pang; Yanyan Lan; Jun Xu; Jiafeng Guo; Xueqi Cheng",
        "abstract": "Convolutional Neural Networks (CNN) and the locally connected layer are limited in capturing the importance and relations of different local receptive fields, which are often crucial for tasks such as face verification, visual question answering, and word sequence prediction. To tackle the issue, we propose a novel locally smoothed neural network (LSNN) in this paper. The main idea is to represent the weight matrix of the locally connected layer as the product of the kernel and the smoother, where the kernel is shared over different local receptive fields, and the smoother is for determining the importance and relations of different local receptive fields. Specifically, a multi-variate Gaussian function is utilized to generate the smoother, for modeling the location relations among different local receptive fields. Furthermore, the content information can also be leveraged by setting the mean and precision of the Gaussian function according to the content. Experiments on some variant of MNIST clearly show our advantages over CNN and locally connected layer.",
        "bibtex": "@InProceedings{pmlr-v77-pang17a,\n  title = \t {Locally Smoothed Neural Networks},\n  author = \t {Pang, Liang and Lan, Yanyan and Xu, Jun and Guo, Jiafeng and Cheng, Xueqi},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {177--191},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/pang17a/pang17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/pang17a.html},\n  abstract = \t {Convolutional Neural Networks (CNN) and the locally connected layer are limited in capturing the importance and relations of different local receptive fields, which are often crucial for tasks such as face verification, visual question answering, and word sequence prediction. To tackle the issue, we propose a novel locally smoothed neural network (LSNN) in this paper. The main idea is to represent the weight matrix of the locally connected layer as the product of the kernel and the smoother, where the kernel is shared over different local receptive fields, and the smoother is for determining the importance and relations of different local receptive fields. Specifically, a multi-variate Gaussian function is utilized to generate the smoother, for modeling the location relations among different local receptive fields. Furthermore, the content information can also be leveraged by setting the mean and precision of the Gaussian function according to the content. Experiments on some variant of MNIST clearly show our advantages over CNN and locally connected layer.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/pang17a/pang17a.pdf",
        "supp": "",
        "pdf_size": 533802,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1473622385775756174&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "gmail.com;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "gmail.com;ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "c2ec5a4615",
        "title": "Magnitude-Preserving Ranking for Structured Outputs",
        "site": "https://proceedings.mlr.press/v77/brouard17a.html",
        "author": "C\u00e9line Brouard; Eric Bach; Sebastian B\u00f6cker; Juho Rousu",
        "abstract": "In this paper, we present a novel method for solving structured prediction problems, based on combining Input Output Kernel Regression (IOKR) with an extension of magnitude-preserving ranking to structured output spaces. In particular, we concentrate on the case where a set of candidate outputs has been given, and the associated pre-image problem calls for ranking the set of candidate outputs. Our method, called magnitude-preserving IOKR, both aims to produce a good approximation of the output feature vectors, and to preserve the magnitude differences of the output features in the candidate sets. For the case where the candidate set does not contain corresponding \u2019correct\u2019 inputs, we propose a method for approximating the inputs through application of IOKR in the reverse direction. We apply our method to two learning problems: cross-lingual document retrieval and metabolite identification. Experiments show that the proposed approach improves performance over IOKR, and in the latter application obtains the current state-of-the-art accuracy.",
        "bibtex": "@InProceedings{pmlr-v77-brouard17a,\n  title = \t {Magnitude-Preserving Ranking for Structured Outputs},\n  author = \t {Brouard, C\u00e9line and Bach, Eric and B\u00f6cker, Sebastian and Rousu, Juho},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {407--422},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/brouard17a/brouard17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/brouard17a.html},\n  abstract = \t {In this paper, we present a novel method for solving structured prediction problems, based on combining Input Output Kernel Regression (IOKR) with an extension of magnitude-preserving ranking to structured output spaces. In particular, we concentrate on the case where a set of candidate outputs has been given, and the associated pre-image problem calls for ranking the set of candidate outputs. Our method, called magnitude-preserving IOKR, both aims to produce a good approximation of the output feature vectors, and to preserve the magnitude differences of the output features in the candidate sets. For the case where the candidate set does not contain corresponding \u2019correct\u2019 inputs, we propose a method for approximating the inputs through application of IOKR in the reverse direction. We apply our method to two learning problems: cross-lingual document retrieval and metabolite identification. Experiments show that the proposed approach improves performance over IOKR, and in the latter application obtains the current state-of-the-art accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/brouard17a/brouard17a.pdf",
        "supp": "",
        "pdf_size": 479705,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17046765693418175212&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Helsinki Institute for Information Technology, Department of Computer Science, Aalto University, Espoo, Finland; Helsinki Institute for Information Technology, Department of Computer Science, Aalto University, Espoo, Finland; Chair for Bioinformatics, Friedrich-Schiller University, Jena, Germany; Helsinki Institute for Information Technology, Department of Computer Science, Aalto University, Espoo, Finland",
        "aff_domain": "aalto.fi;aalto.fi;uni-jena.de;aalto.fi",
        "email": "aalto.fi;aalto.fi;uni-jena.de;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Aalto University;Friedrich-Schiller University",
        "aff_unique_dep": "Department of Computer Science;Chair for Bioinformatics",
        "aff_unique_url": "https://www.aalto.fi;https://www.uni-jena.de",
        "aff_unique_abbr": "Aalto;FSU",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Espoo;Jena",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Finland;Germany"
    },
    {
        "id": "9d082aca87",
        "title": "Mini-batch Block-coordinate based Stochastic Average Adjusted Gradient Methods to Solve Big Data Problems",
        "site": "https://proceedings.mlr.press/v77/chauhan17a.html",
        "author": "Vinod Kumar Chauhan; Kalpana Dahiya; Anuj Sharma",
        "abstract": "Big Data problems in Machine Learning have large number of data points or large number of features, or both, which make training of models difficult because of high computational complexities of single iteration of learning algorithms. To solve such learning problems, Stochastic Approximation offers an optimization approach to make complexity of each iteration independent of number of data points by taking only one data point or mini-batch of data points during each iteration and thereby helping to solve problems with large number of data points. Similarly, Coordinate Descent offers another optimization approach to make iteration complexity independent of the number of features/coordinates/variables by taking only one feature or block of features, instead of all, during an iteration and thereby helping to solve problems with large number of features. In this paper, an optimization framework, namely, Batch Block Optimization Framework has been developed to solve big data problems using the best of Stochastic Approximation as well as the best of Coordinate Descent approaches, independent of any solver. This framework is used to solve strongly convex and smooth empirical risk minimization problem with gradient descent (as a solver) and two novel Stochastic Average Adjusted Gradient methods have been proposed to reduce variance in mini-batch and block-coordinate setting of the developed framework. Theoretical analysis prove linear convergence of the proposed methods and empirical results with bench marked datasets prove the superiority of proposed methods against existing methods.",
        "bibtex": "@InProceedings{pmlr-v77-chauhan17a,\n  title = \t {Mini-batch Block-coordinate based Stochastic Average Adjusted Gradient Methods to Solve Big Data Problems},\n  author = \t {Chauhan, Vinod Kumar and Dahiya, Kalpana and Sharma, Anuj},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {49--64},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/chauhan17a/chauhan17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/chauhan17a.html},\n  abstract = \t {Big Data problems in Machine Learning have large number of data points or large number of features, or both, which make training of models difficult because of high computational complexities of single iteration of learning algorithms. To solve such learning problems, Stochastic Approximation offers an optimization approach to make complexity of each iteration independent of number of data points by taking only one data point or mini-batch of data points during each iteration and thereby helping to solve problems with large number of data points. Similarly, Coordinate Descent offers another optimization approach to make iteration complexity independent of the number of features/coordinates/variables by taking only one feature or block of features, instead of all, during an iteration and thereby helping to solve problems with large number of features. In this paper, an optimization framework, namely, Batch Block Optimization Framework has been developed to solve big data problems using the best of Stochastic Approximation as well as the best of Coordinate Descent approaches, independent of any solver. This framework is used to solve strongly convex and smooth empirical risk minimization problem with gradient descent (as a solver) and two novel Stochastic Average Adjusted Gradient methods have been proposed to reduce variance in mini-batch and block-coordinate setting of the developed framework. Theoretical analysis prove linear convergence of the proposed methods and empirical results with bench marked datasets prove the superiority of proposed methods against existing methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/chauhan17a/chauhan17a.pdf",
        "supp": "",
        "pdf_size": 2666867,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14147059146294684230&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science & Applications, Panjab University Chandigarh; University Institute of Engineering & Technology, Panjab University Chandigarh; Department of Computer Science & Applications, Panjab University Chandigarh",
        "aff_domain": "pu.ac.in;pu.ac.in;pu.ac.in",
        "email": "pu.ac.in;pu.ac.in;pu.ac.in",
        "github": "",
        "project": "https://sites.google.com/site/anujsharma25",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Panjab University",
        "aff_unique_dep": "Department of Computer Science & Applications",
        "aff_unique_url": "http://www.puchd.ac.in",
        "aff_unique_abbr": "PU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chandigarh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "127a3df140",
        "title": "Multi-Task Structured Prediction for Entity Analysis: Search-Based Learning Algorithms",
        "site": "https://proceedings.mlr.press/v77/ma17a.html",
        "author": "Chao Ma; Janardhan Rao Doppa; Prasad Tadepalli; Hamed Shahbazi; Xiaoli Fern",
        "abstract": "Entity analysis in natural language processing involves solving multiple structured prediction problems such as mention detection, coreference resolution, and entity linking. We explore the space of search-based learning approaches to solve the problem of \\em multi-task structured prediction (MTSP) in the context of entity analysis. In this paper, we study three different search architectures to solve MTSP problems that make different tradeoffs between speed and accuracy of training and inference. In all three architectures, we  learn one or more scoring functions that employ both intra-task and inter-task features. In the \u201cpipeline\u201d architecture, which is the fastest, we solve different tasks one after another in a pipelined fashion. In the \u201cjoint\u201d architecture, which is the most expensive,  we formulate MTSP as a single-task structured prediction, and search the joint space of multi-task structured outputs. To improve the speed of joint architecture, we introduce two different pruning methods and associated learning techniques. In the intermediate \u201ccyclic\u201d architecture, we cycle through the tasks multiple times in sequence until there is no performance improvement.  Results on two benchmark domains show that the joint architecture improves over the pipeline approach as well as the previous state-of-the-art approach based on graphical models. The cyclic architecture is faster than the joint approach and achieves competitive performance.",
        "bibtex": "@InProceedings{pmlr-v77-ma17a,\n  title = \t {Multi-Task Structured Prediction for Entity Analysis: Search-Based Learning Algorithms},\n  author = \t {Ma, Chao and Doppa, Janardhan Rao and Tadepalli, Prasad and Shahbazi, Hamed and Fern, Xiaoli},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {514--529},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/ma17a/ma17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/ma17a.html},\n  abstract = \t {Entity analysis in natural language processing involves solving multiple structured prediction problems such as mention detection, coreference resolution, and entity linking. We explore the space of search-based learning approaches to solve the problem of \\em multi-task structured prediction (MTSP) in the context of entity analysis. In this paper, we study three different search architectures to solve MTSP problems that make different tradeoffs between speed and accuracy of training and inference. In all three architectures, we  learn one or more scoring functions that employ both intra-task and inter-task features. In the \u201cpipeline\u201d architecture, which is the fastest, we solve different tasks one after another in a pipelined fashion. In the \u201cjoint\u201d architecture, which is the most expensive,  we formulate MTSP as a single-task structured prediction, and search the joint space of multi-task structured outputs. To improve the speed of joint architecture, we introduce two different pruning methods and associated learning techniques. In the intermediate \u201ccyclic\u201d architecture, we cycle through the tasks multiple times in sequence until there is no performance improvement.  Results on two benchmark domains show that the joint architecture improves over the pipeline approach as well as the previous state-of-the-art approach based on graphical models. The cyclic architecture is faster than the joint approach and achieves competitive performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/ma17a/ma17a.pdf",
        "supp": "",
        "pdf_size": 498427,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15441847491971566024&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d5c668a716",
        "title": "Multi-view Clustering with Adaptively Learned Graph",
        "site": "https://proceedings.mlr.press/v77/tao17a.html",
        "author": "Hong Tao; Chenping Hou; Jubo Zhu; Dongyun Yi",
        "abstract": "Multi-view clustering, which aims to improve the clustering performance by exploring the data\u2019s multiple representations, has become an important research direction. Graph based methods have been widely studied and achieve promising performance for multi-view clustering. However, most existing multi-view graph based methods perform clustering on the fixed input graphs, and the results are dependent on the quality of input graphs. In this paper, instead of fixing the input graphs, we propose Multi-view clustering with Adaptively Learned Graph (MALG), learning a new common similarity matrix. In our model, we not only consider the importance of multiple graphs from view level, but also focus on the performance of similarities within a view from sample-pair level. Sample-pair-specific weights are introduced to exploit the connection across views in more depth. In addition, the obtained optimal graph can be partitioned into specific clusters directly, according to its connected components. Experimental results on toy and real-world datasets demonstrate the efficacy of the proposed algorithm.",
        "bibtex": "@InProceedings{pmlr-v77-tao17a,\n  title = \t {Multi-view Clustering with Adaptively Learned Graph},\n  author = \t {Tao, Hong and Hou, Chenping and Zhu, Jubo and Yi, Dongyun},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {113--128},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/tao17a/tao17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/tao17a.html},\n  abstract = \t {Multi-view clustering, which aims to improve the clustering performance by exploring the data\u2019s multiple representations, has become an important research direction. Graph based methods have been widely studied and achieve promising performance for multi-view clustering. However, most existing multi-view graph based methods perform clustering on the fixed input graphs, and the results are dependent on the quality of input graphs. In this paper, instead of fixing the input graphs, we propose Multi-view clustering with Adaptively Learned Graph (MALG), learning a new common similarity matrix. In our model, we not only consider the importance of multiple graphs from view level, but also focus on the performance of similarities within a view from sample-pair level. Sample-pair-specific weights are introduced to exploit the connection across views in more depth. In addition, the obtained optimal graph can be partitioned into specific clusters directly, according to its connected components. Experimental results on toy and real-world datasets demonstrate the efficacy of the proposed algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/tao17a/tao17a.pdf",
        "supp": "",
        "pdf_size": 2140650,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12735598032956035858&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "College of Science, National University of Defense Technology, Changsha, Hunan, 410073, China; College of Science, National University of Defense Technology, Changsha, Hunan, 410073, China; College of Science, National University of Defense Technology, Changsha, Hunan, 410073, China; College of Science, National University of Defense Technology, Changsha, Hunan, 410073, China",
        "aff_domain": "hotmail.com;hotmail.com;aliyun.com;gmail.com",
        "email": "hotmail.com;hotmail.com;aliyun.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National University of Defense Technology",
        "aff_unique_dep": "College of Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Changsha",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2fe3853a44",
        "title": "Nested LSTMs",
        "site": "https://proceedings.mlr.press/v77/moniz17a.html",
        "author": "Joel Ruben Antony Moniz; David Krueger",
        "abstract": "We propose \\emphNested LSTMs (NLSTM), a novel RNN architecture with multiple levels of memory. Nested LSTMs add depth to LSTMs via nesting as opposed to stacking. The value of a memory cell in an NLSTM is computed by an LSTM cell, which has its own \\it inner memory cell. Specifically, instead of computing the value of the (outer) memory cell as $c^outer_t = f_t \u2299c_t-1 + i_t \u2299g_t$, NLSTM memory cells use the concatenation $(f_t \u2299c_t-1, i_t \u2299g_t)$ as input to an inner LSTM (or NLSTM) memory cell, and set $c^outer_t$ = $h^inner_t$. Nested LSTMs outperform both stacked and single-layer LSTMs with similar numbers of parameters in our experiments on various character-level language modeling tasks, and the inner memories of an LSTM learn longer term dependencies compared with the higher-level units of a stacked LSTM.",
        "bibtex": "@InProceedings{pmlr-v77-moniz17a,\n  title = \t {Nested LSTMs},\n  author = \t {Moniz, Joel Ruben Antony and Krueger, David},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {530--544},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/moniz17a/moniz17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/moniz17a.html},\n  abstract = \t {We propose \\emphNested LSTMs (NLSTM), a novel RNN architecture with multiple levels of memory. Nested LSTMs add depth to LSTMs via nesting as opposed to stacking. The value of a memory cell in an NLSTM is computed by an LSTM cell, which has its own \\it inner memory cell. Specifically, instead of computing the value of the (outer) memory cell as $c^outer_t = f_t \u2299c_t-1 + i_t \u2299g_t$, NLSTM memory cells use the concatenation $(f_t \u2299c_t-1, i_t \u2299g_t)$ as input to an inner LSTM (or NLSTM) memory cell, and set $c^outer_t$ = $h^inner_t$. Nested LSTMs outperform both stacked and single-layer LSTMs with similar numbers of parameters in our experiments on various character-level language modeling tasks, and the inner memories of an LSTM learn longer term dependencies compared with the higher-level units of a stacked LSTM.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/moniz17a/moniz17a.pdf",
        "supp": "",
        "pdf_size": 2076776,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8486839810885574798&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University\u2217; MILA, Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "ANDREW.CMU.EDU;UMONTREAL.CA",
        "email": "ANDREW.CMU.EDU;UMONTREAL.CA",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": ";MILA",
        "aff_unique_url": "https://www.cmu.edu;https://www.umontreal.ca",
        "aff_unique_abbr": "CMU;UdeM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Montr\u00e9al",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2bdc9cf03b",
        "title": "NeuralPower: Predict and Deploy Energy-Efficient Convolutional Neural Networks",
        "site": "https://proceedings.mlr.press/v77/cai17a.html",
        "author": "Ermao Cai; Da-Cheng Juan; Dimitrios Stamoulis; Diana Marculescu",
        "abstract": "\u201cHow much energy is consumed for an inference made by a convolutional neural network (CNN)?\u201d With the increased popularity of CNNs deployed on the wide-spectrum of platforms (from mobile devices to workstations), the answer to this question has drawn significant attention. From lengthening battery life of mobile devices to reducing the energy bill of a datacenter, it is important to understand the energy efficiency of CNNs during serving for making an inference, before actually training the model. In this work, we propose NeuralPower: a layer-wise predictive framework based on sparse polynomial regression, for predicting the serving energy consumption of a CNN deployed on any GPU platform. Given the architecture of a CNN, NeuralPower provides an accurate prediction and breakdown for power and runtime across all layers in the whole network, helping machine learners quickly identify the power, runtime, or energy bottlenecks. We also propose the \u201cenergy-precision ratio\u201d (EPR) metric to guide machine learners in selecting an energy-efficient CNN architecture that better trades off the energy consumption and prediction accuracy. The experimental results show that the prediction accuracy of the proposed NeuralPower outperforms the best published model to date, yielding an improvement in accuracy of up to 68.5%. We also assess the accuracy of predictions at the network level, by predicting the runtime, power, and energy of state-of-the-art CNN architectures, achieving an average accuracy of 88.24% in runtime, 88.34% in power, and 97.21% in energy. We comprehensively corroborate the effectiveness of NeuralPower as a powerful framework for machine learners by testing it on different GPU platforms and Deep Learning software tools.",
        "bibtex": "@InProceedings{pmlr-v77-cai17a,\n  title = \t {\\emph{NeuralPower}: Predict and Deploy Energy-Efficient Convolutional Neural Networks},\n  author = \t {Cai, Ermao and Juan, Da-Cheng and Stamoulis, Dimitrios and Marculescu, Diana},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {622--637},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/cai17a/cai17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/cai17a.html},\n  abstract = \t {\u201cHow much energy is consumed for an inference made by a convolutional neural network (CNN)?\u201d With the increased popularity of CNNs deployed on the wide-spectrum of platforms (from mobile devices to workstations), the answer to this question has drawn significant attention. From lengthening battery life of mobile devices to reducing the energy bill of a datacenter, it is important to understand the energy efficiency of CNNs during serving for making an inference, before actually training the model. In this work, we propose NeuralPower: a layer-wise predictive framework based on sparse polynomial regression, for predicting the serving energy consumption of a CNN deployed on any GPU platform. Given the architecture of a CNN, NeuralPower provides an accurate prediction and breakdown for power and runtime across all layers in the whole network, helping machine learners quickly identify the power, runtime, or energy bottlenecks. We also propose the \u201cenergy-precision ratio\u201d (EPR) metric to guide machine learners in selecting an energy-efficient CNN architecture that better trades off the energy consumption and prediction accuracy. The experimental results show that the prediction accuracy of the proposed NeuralPower outperforms the best published model to date, yielding an improvement in accuracy of up to 68.5%. We also assess the accuracy of predictions at the network level, by predicting the runtime, power, and energy of state-of-the-art CNN architectures, achieving an average accuracy of 88.24% in runtime, 88.34% in power, and 97.21% in energy. We comprehensively corroborate the effectiveness of NeuralPower as a powerful framework for machine learners by testing it on different GPU platforms and Deep Learning software tools.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/cai17a/cai17a.pdf",
        "supp": "",
        "pdf_size": 477244,
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17786095761583453172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of ECE, Carnegie Mellon University, Pittsburgh, PA, USA; Google Research, Mountain View, CA, USA + Department of ECE, Carnegie Mellon University, Pittsburgh, PA, USA; Department of ECE, Carnegie Mellon University, Pittsburgh, PA, USA; Department of ECE, Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": "cmu.edu;google.com;andrew.cmu.edu;cmu.edu",
        "email": "cmu.edu;google.com;andrew.cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Carnegie Mellon University;Google",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Google Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.google",
        "aff_unique_abbr": "CMU;Google",
        "aff_campus_unique_index": "0;1+0;0;0",
        "aff_campus_unique": "Pittsburgh;Mountain View",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eb25345bcc",
        "title": "On the Flatness of Loss Surface for Two-layered ReLU Networks",
        "site": "https://proceedings.mlr.press/v77/cao17a.html",
        "author": "Jiezhang Cao; Qingyao Wu; Yuguang Yan; Li Wang; Mingkui Tan",
        "abstract": "Deep learning has achieved unprecedented practical success in many applications. Despite its empirical success, however, the theoretical understanding of deep neural networks still remains a major open problem. In this paper, we explore properties of two-layered ReLU networks. For simplicity, we assume that the optimal model parameters (also called ground-truth parameters) are known. We then assume that a network receives Gaussian input and is trained by minimizing the expected squared loss between the prediction function of the network and a target function. To conduct the analysis, we propose a normal equation for critical points, and study the invariances under three kinds of transformations, namely, scale transformation, rotation transformation and perturbation transformation. We prove that these transformations can keep the loss of a critical point invariant, thus can incur flat regions. Consequently, how to escape from flat regions is vital in training neural networks.",
        "bibtex": "@InProceedings{pmlr-v77-cao17a,\n  title = \t {On the Flatness of Loss Surface for Two-layered ReLU Networks},\n  author = \t {Cao, Jiezhang and Wu, Qingyao and Yan, Yuguang and Wang, Li and Tan, Mingkui},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {545--560},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/cao17a/cao17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/cao17a.html},\n  abstract = \t {Deep learning has achieved unprecedented practical success in many applications. Despite its empirical success, however, the theoretical understanding of deep neural networks still remains a major open problem. In this paper, we explore properties of two-layered ReLU networks. For simplicity, we assume that the optimal model parameters (also called ground-truth parameters) are known. We then assume that a network receives Gaussian input and is trained by minimizing the expected squared loss between the prediction function of the network and a target function. To conduct the analysis, we propose a normal equation for critical points, and study the invariances under three kinds of transformations, namely, scale transformation, rotation transformation and perturbation transformation. We prove that these transformations can keep the loss of a critical point invariant, thus can incur flat regions. Consequently, how to escape from flat regions is vital in training neural networks.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/cao17a/cao17a.pdf",
        "supp": "",
        "pdf_size": 394816,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1181428022271205174&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Software Engineering, South China University of Technology; School of Software Engineering, South China University of Technology; School of Software Engineering, South China University of Technology; Department of Mathematics, University of Texas at Arlington; School of Software Engineering, South China University of Technology",
        "aff_domain": "mail.scut.edu.cn;scut.edu.cn;mail.scut.edu.cn;uta.edu;scut.edu.cn",
        "email": "mail.scut.edu.cn;scut.edu.cn;mail.scut.edu.cn;uta.edu;scut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "South China University of Technology;University of Texas at Arlington",
        "aff_unique_dep": "School of Software Engineering;Department of Mathematics",
        "aff_unique_url": "https://www.scut.edu.cn;https://www.uta.edu",
        "aff_unique_abbr": "SCUT;UTA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Arlington",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "1501993193",
        "title": "One Class Splitting Criteria for Random Forests",
        "site": "https://proceedings.mlr.press/v77/goix17a.html",
        "author": "Nicolas Goix; Nicolas Drougard; Romain Brault; Mael Chiapino",
        "abstract": "Random Forests (RFs) are strong machine learning tools for classification and regression. However, they remain supervised algorithms, and no extension of RFs to the one-class setting has been proposed, except for techniques based on second-class sampling. This work fills this gap by proposing a natural methodology to extend standard splitting criteria to the one-class setting, structurally generalizing RFs to one-class classification. An extensive benchmark of seven state-of-the-art anomaly detection algorithms is also presented. This empirically demonstrates the relevance of our approach.",
        "bibtex": "@InProceedings{pmlr-v77-goix17a,\n  title = \t {One Class Splitting Criteria for Random Forests},\n  author = \t {Goix, Nicolas and Drougard, Nicolas and Brault, Romain and Chiapino, Mael},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {343--358},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/goix17a/goix17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/goix17a.html},\n  abstract = \t {Random Forests (RFs) are strong machine learning tools for classification and regression. However, they remain supervised algorithms, and no extension of RFs to the one-class setting has been proposed, except for techniques based on second-class sampling. This work fills this gap by proposing a natural methodology to extend standard splitting criteria to the one-class setting, structurally generalizing RFs to one-class classification. An extensive benchmark of seven state-of-the-art anomaly detection algorithms is also presented. This empirically demonstrates the relevance of our approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/goix17a/goix17a.pdf",
        "supp": "",
        "pdf_size": 459224,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4131410417839192902&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "LTCI, T\u00b4 el\u00b4 ecom ParisTech, Universit\u00b4 e Paris-Saclay, France; ISAE-Supaero, Toulouse, France + LTCI, T\u00b4 el\u00b4 ecom ParisTech, Universit\u00b4 e Paris-Saclay, France; LTCI, T\u00b4 el\u00b4 ecom ParisTech, Universit\u00b4 e Paris-Saclay, France; LTCI, T\u00b4 el\u00b4 ecom ParisTech, Universit\u00b4 e Paris-Saclay, France",
        "aff_domain": "telecom-paristech.fr;isae-supaero.fr;telecom-paristech.fr;telecom-paristech.fr",
        "email": "telecom-paristech.fr;isae-supaero.fr;telecom-paristech.fr;telecom-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "T\u00e9l\u00e9com ParisTech;Institut Sup\u00e9rieur de l'A\u00e9ronautique et de l'Espace",
        "aff_unique_dep": "LTCI;",
        "aff_unique_url": "https://www.telecom-paris.fr;https://www.isae-supaero.fr",
        "aff_unique_abbr": "T\u00e9l\u00e9com ParisTech;ISAE-Supaero",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Toulouse",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "95db10de9e",
        "title": "PHD: A Probabilistic Model of Hybrid Deep Collaborative Filtering for Recommender Systems",
        "site": "https://proceedings.mlr.press/v77/liu17a.html",
        "author": "Jie Liu; Dong Wang; Yue Ding",
        "abstract": "Collaborative Filtering (CF), a well-known approach in producing recommender systems, has achieved wide use and excellent performance not only in research but also in industry. However, problems related to cold start and data sparsity have caused CF to attract an increasing amount of attention in efforts to solve these problems. Traditional approaches adopt side information to extract effective latent factors but still have some room for growth. Due to the strong characteristic of feature extraction in deep learning, many researchers have employed it with CF to extract effective representations and to enhance its performance in rating prediction. Based on this previous work, we propose a probabilistic model that combines a stacked denoising autoencoder and a convolutional neural network together with auxiliary side information (i.e, both from users and items) to extract users and items\u2019 latent factors, respectively. Extensive experiments for four datasets demonstrate that our proposed model outperforms other traditional approaches and deep learning models making it state of the art.",
        "bibtex": "@InProceedings{pmlr-v77-liu17a,\n  title = \t {PHD: A Probabilistic Model of Hybrid Deep Collaborative Filtering for Recommender Systems},\n  author = \t {Liu, Jie and Wang, Dong and Ding, Yue},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {224--239},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/liu17a/liu17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/liu17a.html},\n  abstract = \t {Collaborative Filtering (CF), a well-known approach in producing recommender systems, has achieved wide use and excellent performance not only in research but also in industry. However, problems related to cold start and data sparsity have caused CF to attract an increasing amount of attention in efforts to solve these problems. Traditional approaches adopt side information to extract effective latent factors but still have some room for growth. Due to the strong characteristic of feature extraction in deep learning, many researchers have employed it with CF to extract effective representations and to enhance its performance in rating prediction. Based on this previous work, we propose a probabilistic model that combines a stacked denoising autoencoder and a convolutional neural network together with auxiliary side information (i.e, both from users and items) to extract users and items\u2019 latent factors, respectively. Extensive experiments for four datasets demonstrate that our proposed model outperforms other traditional approaches and deep learning models making it state of the art.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/liu17a/liu17a.pdf",
        "supp": "",
        "pdf_size": 876781,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1505900603996845922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "aff_domain": "outlook.com;sjtu.edu.cn;sjtu.edu.cn",
        "email": "outlook.com;sjtu.edu.cn;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "5cf0682208",
        "title": "Preface",
        "site": "https://proceedings.mlr.press/v77/zhang17a.html",
        "author": "Min-Ling Zhang; Yung-Kyun Noh",
        "abstract": "",
        "bibtex": "@InProceedings{pmlr-v77-zhang17a,\n  title = \t {Preface},\n  author = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {i--xv},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/zhang17a/zhang17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/zhang17a.html}\n}",
        "pdf": "http://proceedings.mlr.press/v77/zhang17a/zhang17a.pdf",
        "supp": "",
        "pdf_size": 58976,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141043608347785016&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f4059d7ff8",
        "title": "Probability Calibration Trees",
        "site": "https://proceedings.mlr.press/v77/leathart17a.html",
        "author": "Tim Leathart; Eibe Frank; Geoffrey Holmes; Bernhard Pfahringer",
        "abstract": "Obtaining accurate and well calibrated probability estimates from classifiers is useful in many applications, for example, when minimising the expected cost of classifications. Existing methods of calibrating probability estimates are applied globally, ignoring the potential for improvements by applying a more fine-grained model. We propose probability calibration trees, a modification of logistic model trees that identifies regions of the input space in which different probability calibration models are learned to improve performance. We compare probability calibration trees to two widely used calibration methods\u2014isotonic regression and Platt scaling\u2014and show that our method results in lower root mean squared error on average than both methods, for estimates produced by a variety of base learners.",
        "bibtex": "@InProceedings{pmlr-v77-leathart17a,\n  title = \t {Probability Calibration Trees},\n  author = \t {Leathart, Tim and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {145--160},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/leathart17a/leathart17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/leathart17a.html},\n  abstract = \t {Obtaining accurate and well calibrated probability estimates from classifiers is useful in many applications, for example, when minimising the expected cost of classifications. Existing methods of calibrating probability estimates are applied globally, ignoring the potential for improvements by applying a more fine-grained model. We propose probability calibration trees, a modification of logistic model trees that identifies regions of the input space in which different probability calibration models are learned to improve performance. We compare probability calibration trees to two widely used calibration methods\u2014isotonic regression and Platt scaling\u2014and show that our method results in lower root mean squared error on average than both methods, for estimates produced by a variety of base learners.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/leathart17a/leathart17a.pdf",
        "supp": "",
        "pdf_size": 350457,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5529032258058791616&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Waikato; Department of Computer Science, University of Waikato; Department of Computer Science, University of Waikato; Department of Computer Science, University of Auckland",
        "aff_domain": "students.waikato.ac.nz;cs.waikato.ac.nz;cs.waikato.ac.nz;auckland.ac.nz",
        "email": "students.waikato.ac.nz;cs.waikato.ac.nz;cs.waikato.ac.nz;auckland.ac.nz",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Waikato;University of Auckland",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.waikato.ac.nz;https://www.auckland.ac.nz",
        "aff_unique_abbr": "UoW;UoA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "b4a27ebc11",
        "title": "Pyramid Person Matching Network for Person Re-identification",
        "site": "https://proceedings.mlr.press/v77/mao17a.html",
        "author": "Chaojie Mao; Yingming Li; Zhongfei Zhang; Yaqing Zhang; Xi Li",
        "abstract": "In this work, we present a deep convolutional pyramid person matching network (PPMN) with specially designed Pyramid Matching Module to address the problem of person re-identification. The architecture takes a pair of RGB images as input, and outputs a similiarity value indicating whether the two input images represent the same person or not. Based on deep convolutional neural networks, our approach first learns the discriminative semantic representation with the semantic-component-aware features for persons and then employs the Pyramid Matching Module to match the common semantic-components of persons, which is robust to the variation of spatial scales and misalignment of locations posed by viewpoint changes. The above two processes are jointly optimized via a unified end-to-end deep learning scheme. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our approach against the state-of-the-art approaches, especially on the rank-1 recognition rate.",
        "bibtex": "@InProceedings{pmlr-v77-mao17a,\n  title = \t {Pyramid Person Matching Network for Person Re-identification},\n  author = \t {Mao, Chaojie and Li, Yingming and Zhang, Zhongfei and Zhang, Yaqing and Li, Xi},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {487--497},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/mao17a/mao17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/mao17a.html},\n  abstract = \t {In this work, we present a deep convolutional pyramid person matching network (PPMN) with specially designed Pyramid Matching Module to address the problem of person re-identification. The architecture takes a pair of RGB images as input, and outputs a similiarity value indicating whether the two input images represent the same person or not. Based on deep convolutional neural networks, our approach first learns the discriminative semantic representation with the semantic-component-aware features for persons and then employs the Pyramid Matching Module to match the common semantic-components of persons, which is robust to the variation of spatial scales and misalignment of locations posed by viewpoint changes. The above two processes are jointly optimized via a unified end-to-end deep learning scheme. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our approach against the state-of-the-art approaches, especially on the rank-1 recognition rate.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/mao17a/mao17a.pdf",
        "supp": "",
        "pdf_size": 900726,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15010500173256510545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Information Science and Electronic Engineering",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2946082356",
        "title": "Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of Chinese and Japanese",
        "site": "https://proceedings.mlr.press/v77/ke17a.html",
        "author": "Yuanzhi Ke; Masafumi Hagiwara",
        "abstract": "The character vocabulary can be very large in non-alphabetic languages such as Chinese and Japanese, which makes neural network models huge to process such languages. We explored a model for sentiment classification that takes the embeddings of the radicals of the Chinese characters, i.e, hanzi of Chinese and kanji of Japanese. Our model is composed of a CNN word feature encoder and a bi-directional RNN document feature encoder. The results achieved are on par with the character embedding-based models, and close to the state-of-the-art word embedding-based models, with 90% smaller vocabulary, and at least 13% and 80% fewer parameters than the character embedding-based models and word embedding-based models respectively. The results suggest that the radical embeddingbased approach is cost-effective for machine learning on Chinese and Japanese.",
        "bibtex": "@InProceedings{pmlr-v77-ke17a,\n  title = \t {Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of Chinese and Japanese},\n  author = \t {Ke, Yuanzhi and Hagiwara, Masafumi},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {561--573},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/ke17a/ke17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/ke17a.html},\n  abstract = \t {The character vocabulary can be very large in non-alphabetic languages such as Chinese and Japanese, which makes neural network models huge to process such languages. We explored a model for sentiment classification that takes the embeddings of the radicals of the Chinese characters, i.e, hanzi of Chinese and kanji of Japanese. Our model is composed of a CNN word feature encoder and a bi-directional RNN document feature encoder. The results achieved are on par with the character embedding-based models, and close to the state-of-the-art word embedding-based models, with 90% smaller vocabulary, and at least 13% and 80% fewer parameters than the character embedding-based models and word embedding-based models respectively. The results suggest that the radical embeddingbased approach is cost-effective for machine learning on Chinese and Japanese.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/ke17a/ke17a.pdf",
        "supp": "",
        "pdf_size": 360246,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2821712333650424352&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Information and Computer Science, Faculty of Science and Engineering, Keio University; Department of Information and Computer Science, Faculty of Science and Engineering, Keio University",
        "aff_domain": "keio.jp;keio.jp",
        "email": "keio.jp;keio.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Keio University",
        "aff_unique_dep": "Department of Information and Computer Science",
        "aff_unique_url": "https://www.keio.ac.jp",
        "aff_unique_abbr": "Keio",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "146ff279f3",
        "title": "Rate Optimal Estimation for High Dimensional Spatial Covariance Matrices",
        "site": "https://proceedings.mlr.press/v77/li17a.html",
        "author": "Yi Li; Aidong Adam Ding; Jennifer Dy",
        "abstract": "Spatial covariance matrix estimation is of great significance in many applications in climatology, econometrics and many other fields with complex data structures involving spatial dependencies. High dimensionality brings new challenges to this problem, and no theoretical optimal estimator has been proved for the spatial high-dimensional covariance matrix. Over the past decade, the method of regularization has been introduced to high-dimensional covariance estimation for various structured matrices, to achieve rate optimal estimators. In this paper, we aim to bridge the gap in these two research areas. We use a structure of block bandable covariance matrices to incorporate spatial dependence information, and study rate optimal estimation of this type of structured high dimensional covariance matrices. A double tapering estimator is proposed, and is shown to achieve the asymptotic minimax error bound. Numerical studies on both synthetic and real data are conducted showing the improvement of the double tapering estimator over the sample covariance matrix estimator.",
        "bibtex": "@InProceedings{pmlr-v77-li17a,\n  title = \t {Rate Optimal Estimation for High Dimensional Spatial Covariance Matrices},\n  author = \t {Li, Yi and Ding, Aidong Adam and Dy, Jennifer},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {208--223},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/li17a/li17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/li17a.html},\n  abstract = \t {Spatial covariance matrix estimation is of great significance in many applications in climatology, econometrics and many other fields with complex data structures involving spatial dependencies. High dimensionality brings new challenges to this problem, and no theoretical optimal estimator has been proved for the spatial high-dimensional covariance matrix. Over the past decade, the method of regularization has been introduced to high-dimensional covariance estimation for various structured matrices, to achieve rate optimal estimators. In this paper, we aim to bridge the gap in these two research areas. We use a structure of block bandable covariance matrices to incorporate spatial dependence information, and study rate optimal estimation of this type of structured high dimensional covariance matrices. A double tapering estimator is proposed, and is shown to achieve the asymptotic minimax error bound. Numerical studies on both synthetic and real data are conducted showing the improvement of the double tapering estimator over the sample covariance matrix estimator.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/li17a/li17a.pdf",
        "supp": "",
        "pdf_size": 735907,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4217025094136173497&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Northeastern University; Northeastern University; Northeastern University",
        "aff_domain": "HUSKY.NEU.EDU;NEU.EDU;ECE.NEU.EDU",
        "email": "HUSKY.NEU.EDU;NEU.EDU;ECE.NEU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ad2f419e38",
        "title": "Recognizing Art Style Automatically in Painting with Deep Learning",
        "site": "https://proceedings.mlr.press/v77/lecoutre17a.html",
        "author": "Adrian Lecoutre; Benjamin Negrevergne; Florian Yger",
        "abstract": "The artistic style (or artistic movement) of a painting is a rich descriptor that captures both visual and historical information about the painting. Correctly identifying the artistic style of a paintings is crucial for indexing large artistic databases. In this paper, we investigate the use of deep residual neural to solve the problem of detecting the artistic style of a painting and outperform existing approaches to reach an accuracy of $62%$ on the Wikipaintings dataset (for 25 different style). To achieve this result, the network is first pre-trained on ImageNet, and deeply retrained for artistic style. We empirically evaluate that to achieve the best performance, one need to retrain about 20 layers. This suggests that the two tasks are as similar as expected, and explain the previous success of hand crafted features. We also demonstrate that the style detected on the Wikipaintings dataset are consistent with styles detected on an independent dataset and describe a number of experiments we conducted to validate this approach both qualitatively and quantitatively.",
        "bibtex": "@InProceedings{pmlr-v77-lecoutre17a,\n  title = \t {Recognizing Art Style Automatically in Painting with Deep Learning},\n  author = \t {Lecoutre, Adrian and Negrevergne, Benjamin and Yger, Florian},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {327--342},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/lecoutre17a/lecoutre17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/lecoutre17a.html},\n  abstract = \t {The artistic style (or artistic movement) of a painting is a rich descriptor that captures both visual and historical information about the painting. Correctly identifying the artistic style of a paintings is crucial for indexing large artistic databases. In this paper, we investigate the use of deep residual neural to solve the problem of detecting the artistic style of a painting and outperform existing approaches to reach an accuracy of $62%$ on the Wikipaintings dataset (for 25 different style). To achieve this result, the network is first pre-trained on ImageNet, and deeply retrained for artistic style. We empirically evaluate that to achieve the best performance, one need to retrain about 20 layers. This suggests that the two tasks are as similar as expected, and explain the previous success of hand crafted features. We also demonstrate that the style detected on the Wikipaintings dataset are consistent with styles detected on an independent dataset and describe a number of experiments we conducted to validate this approach both qualitatively and quantitatively.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/lecoutre17a/lecoutre17a.pdf",
        "supp": "",
        "pdf_size": 1072347,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16251596093826376840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "LAMSADE, INSA de Rouen,76800 Saint-\u00c9tienne-du-Rouvray, France; LAMSADE, CNRS, Universit\u00e9 Paris-Dauphine, PSL Research University, 75016 Paris, France; LAMSADE, CNRS, Universit\u00e9 Paris-Dauphine, PSL Research University, 75016 Paris, France",
        "aff_domain": "insa-rouen.fr;dauphine.fr;dauphine.fr",
        "email": "insa-rouen.fr;dauphine.fr;dauphine.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "INSA de Rouen;Universit\u00e9 Paris-Dauphine",
        "aff_unique_dep": "LAMSADE;LAMSADE",
        "aff_unique_url": ";https://www.univ-paris-dauphine.fr",
        "aff_unique_abbr": ";UPD",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Saint-\u00c9tienne-du-Rouvray;Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "754fa766af",
        "title": "Recovering Probability Distributions from Missing Data",
        "site": "https://proceedings.mlr.press/v77/tian17a.html",
        "author": "Jin Tian",
        "abstract": "A probabilistic query may not be estimable from observed data corrupted by missing values if the data are not missing at random (MAR). It is therefore of theoretical interest and practical importance to determine in principle whether a probabilistic query is estimable from missing data or not when the data are not MAR. We present algorithms that systematically determine whether the joint probability distribution or a target marginal distribution is estimable from observed data with missing values,  assuming that the data-generation model is represented as a Bayesian network, known as m-graphs, that not only encodes the dependencies among the variables but also explicitly portrays the mechanisms responsible for the missingness process. The results significantly advance the existing work.",
        "bibtex": "@InProceedings{pmlr-v77-tian17a,\n  title = \t {Recovering Probability Distributions from Missing Data},\n  author = \t {Tian, Jin},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {574--589},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/tian17a/tian17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/tian17a.html},\n  abstract = \t {A probabilistic query may not be estimable from observed data corrupted by missing values if the data are not missing at random (MAR). It is therefore of theoretical interest and practical importance to determine in principle whether a probabilistic query is estimable from missing data or not when the data are not MAR. We present algorithms that systematically determine whether the joint probability distribution or a target marginal distribution is estimable from observed data with missing values,  assuming that the data-generation model is represented as a Bayesian network, known as m-graphs, that not only encodes the dependencies among the variables but also explicitly portrays the mechanisms responsible for the missingness process. The results significantly advance the existing work.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/tian17a/tian17a.pdf",
        "supp": "",
        "pdf_size": 334035,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11972932283756057443&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Iowa State University",
        "aff_domain": "iastate.edu",
        "email": "iastate.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Iowa State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iastate.edu",
        "aff_unique_abbr": "ISU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4cf3b71f6a",
        "title": "Regret for Expected Improvement over the Best-Observed Value and Stopping Condition",
        "site": "https://proceedings.mlr.press/v77/nguyen17a.html",
        "author": "Vu Nguyen; Sunil Gupta; Santu Rana; Cheng Li; Svetha Venkatesh",
        "abstract": "Bayesian optimization (BO) is a sample-efficient method for global optimization of expensive, noisy, black-box functions using probabilistic methods. The performance of a BO method depends on its selection strategy through the acquisition function. Expected improvement (EI) is one of the most widely used acquisition functions for BO that finds the expectation of the improvement function over the incumbent. The incumbent is usually selected as the best-observed value so far, termed as $y^\\max$ (for the maximizing problem). Recent work has studied the convergence rate for EI under some mild assumptions or zero noise of observations. Especially, the work of Wang and de Freitas (2014) has derived the sublinear regret for EI under a stochastic noise. However, due to the difficulty in stochastic noise setting and to make the convergent proof feasible, they use an alternative choice for the incumbent as the maximum of the Gaussian process predictive mean, $\u03bc^\\max$. This modification makes the algorithm computationally inefficient because it requires an additional global optimization step to estimate $\u03bc^\\max$ that is costly and may be inaccurate. To address this issue, we derive a sublinear convergence rate for EI using the commonly used $y^\\max$. Moreover, our analysis is the first to study a stopping criteria for EI to prevent unnecessary evaluations. Our analysis complements the results of Wang and de Freitas (2014) to theoretically cover two incumbent settings for EI. Finally, we demonstrate empirically that EI using $y^\\max$ is both more computationally efficiency and more accurate than EI using $\u03bc^\\max$.",
        "bibtex": "@InProceedings{pmlr-v77-nguyen17a,\n  title = \t {Regret for Expected Improvement over the Best-Observed Value and Stopping Condition},\n  author = \t {Nguyen, Vu and Gupta, Sunil and Rana, Santu and Li, Cheng and Venkatesh, Svetha},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {279--294},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/nguyen17a/nguyen17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/nguyen17a.html},\n  abstract = \t {Bayesian optimization (BO) is a sample-efficient method for global optimization of expensive, noisy, black-box functions using probabilistic methods. The performance of a BO method depends on its selection strategy through the acquisition function. Expected improvement (EI) is one of the most widely used acquisition functions for BO that finds the expectation of the improvement function over the incumbent. The incumbent is usually selected as the best-observed value so far, termed as $y^\\max$ (for the maximizing problem). Recent work has studied the convergence rate for EI under some mild assumptions or zero noise of observations. Especially, the work of Wang and de Freitas (2014) has derived the sublinear regret for EI under a stochastic noise. However, due to the difficulty in stochastic noise setting and to make the convergent proof feasible, they use an alternative choice for the incumbent as the maximum of the Gaussian process predictive mean, $\u03bc^\\max$. This modification makes the algorithm computationally inefficient because it requires an additional global optimization step to estimate $\u03bc^\\max$ that is costly and may be inaccurate. To address this issue, we derive a sublinear convergence rate for EI using the commonly used $y^\\max$. Moreover, our analysis is the first to study a stopping criteria for EI to prevent unnecessary evaluations. Our analysis complements the results of Wang and de Freitas (2014) to theoretically cover two incumbent settings for EI. Finally, we demonstrate empirically that EI using $y^\\max$ is both more computationally efficiency and more accurate than EI using $\u03bc^\\max$.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/nguyen17a/nguyen17a.pdf",
        "supp": "",
        "pdf_size": 374621,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=170980630893126524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Deakin University, Geelong, Australia, Center for Pattern Recognition and Data Analytics; Deakin University, Geelong, Australia, Center for Pattern Recognition and Data Analytics; Deakin University, Geelong, Australia, Center for Pattern Recognition and Data Analytics; Deakin University, Geelong, Australia, Center for Pattern Recognition and Data Analytics; Deakin University, Geelong, Australia, Center for Pattern Recognition and Data Analytics",
        "aff_domain": "DEAKIN.EDU.AU;DEAKIN.EDU.AU;DEAKIN.EDU.AU;DEAKIN.EDU.AU;DEAKIN.EDU.AU",
        "email": "DEAKIN.EDU.AU;DEAKIN.EDU.AU;DEAKIN.EDU.AU;DEAKIN.EDU.AU;DEAKIN.EDU.AU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Deakin University",
        "aff_unique_dep": "Center for Pattern Recognition and Data Analytics",
        "aff_unique_url": "https://www.deakin.edu.au",
        "aff_unique_abbr": "Deakin",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Geelong",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "edc936be96",
        "title": "ST-GAN: Unsupervised Facial Image Semantic Transformation Using Generative Adversarial Networks",
        "site": "https://proceedings.mlr.press/v77/zhang17c.html",
        "author": "Jichao Zhang; Fan Zhong; Gongze Cao; Xueying Qin",
        "abstract": "Image semantic transformation aims to convert one image into another image with different semantic features (e.g., face pose, hairstyle). The previous methods, which learn the mapping function from one image domain to the other, require supervised information directly or indirectly. In this paper, we propose an unsupervised image semantic transformation method called semantic transformation generative adversarial networks (ST-GAN), and experimentally verify it on face dataset. We further improve ST-GAN with the Wasserstein distance to generate more realistic images and propose a method called local mutual information maximization to obtain a more explicit semantic transformation. ST-GAN has the ability to map the image semantic features into the latent vector and then perform transformation by controlling the latent vector.",
        "bibtex": "@InProceedings{pmlr-v77-zhang17c,\n  title = \t {ST-GAN: Unsupervised Facial Image Semantic Transformation Using Generative Adversarial Networks},\n  author = \t {Zhang, Jichao and Zhong, Fan and Cao, Gongze and Qin, Xueying},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {248--263},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/zhang17c/zhang17c.pdf},\n  url = \t {https://proceedings.mlr.press/v77/zhang17c.html},\n  abstract = \t {Image semantic transformation aims to convert one image into another image with different semantic features (e.g., face pose, hairstyle). The previous methods, which learn the mapping function from one image domain to the other, require supervised information directly or indirectly. In this paper, we propose an unsupervised image semantic transformation method called semantic transformation generative adversarial networks (ST-GAN), and experimentally verify it on face dataset. We further improve ST-GAN with the Wasserstein distance to generate more realistic images and propose a method called local mutual information maximization to obtain a more explicit semantic transformation. ST-GAN has the ability to map the image semantic features into the latent vector and then perform transformation by controlling the latent vector.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/zhang17c/zhang17c.pdf",
        "supp": "",
        "pdf_size": 8978141,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16297299171553641835&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Shandong university; School of Computer Science and Technology, Shandong university; School of Mathematics, Zhejiang University; College of Computer Science and Technology, Shandong university",
        "aff_domain": "gmail.com;sdu.edu.cn;gmail.com;sdu.edu.cn",
        "email": "gmail.com;sdu.edu.cn;gmail.com;sdu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Shandong University;Zhejiang University",
        "aff_unique_dep": "School of Computer Science and Technology;School of Mathematics",
        "aff_unique_url": "http://www.sdu.edu.cn;http://www.zju.edu.cn",
        "aff_unique_abbr": "SDU;ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "124b911315",
        "title": "Scale-Invariant Recognition by Weight-Shared CNNs in Parallel",
        "site": "https://proceedings.mlr.press/v77/takahashi17a.html",
        "author": "Ryo Takahashi; Takashi Matsubara; Kuniaki Uehara",
        "abstract": "Deep convolutional neural networks (CNNs) have become one of the most successful methods for image processing tasks in past few years. Recent studies on modern residual architectures, enabling CNNs to be much deeper, have achieved much better results thanks to their high expressive ability by numerous parameters. In general, CNNs are known to have the robustness to the small parallel shift of objects in images by their local receptive fields, weight parameters shared by each unit, and pooling layers sandwiching them. However, CNNs have a limited robustness to the other geometric transformations such as scaling and rotation, and this lack becomes an obstacle to performance improvement even now. This paper proposes a novel network architecture, the \\emphweight-shared multi-stage network (WSMS-Net), and focuses on acquiring the scale invariance by constructing of multiple stages of CNNs. The WSMS-Net is easily combined with existing deep CNNs, enables existing deep CNNs to acquire a robustness to the scaling, and therefore, achieves higher classification accuracy on CIFAR-10, CIFAR-100 and ImageNet datasets.",
        "bibtex": "@InProceedings{pmlr-v77-takahashi17a,\n  title = \t {Scale-Invariant Recognition by Weight-Shared CNNs in Parallel},\n  author = \t {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {295--310},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/takahashi17a/takahashi17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/takahashi17a.html},\n  abstract = \t {Deep convolutional neural networks (CNNs) have become one of the most successful methods for image processing tasks in past few years. Recent studies on modern residual architectures, enabling CNNs to be much deeper, have achieved much better results thanks to their high expressive ability by numerous parameters. In general, CNNs are known to have the robustness to the small parallel shift of objects in images by their local receptive fields, weight parameters shared by each unit, and pooling layers sandwiching them. However, CNNs have a limited robustness to the other geometric transformations such as scaling and rotation, and this lack becomes an obstacle to performance improvement even now. This paper proposes a novel network architecture, the \\emphweight-shared multi-stage network (WSMS-Net), and focuses on acquiring the scale invariance by constructing of multiple stages of CNNs. The WSMS-Net is easily combined with existing deep CNNs, enables existing deep CNNs to acquire a robustness to the scaling, and therefore, achieves higher classification accuracy on CIFAR-10, CIFAR-100 and ImageNet datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/takahashi17a/takahashi17a.pdf",
        "supp": "",
        "pdf_size": 3587517,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3281639148354607310&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Graduate School of System Informatics, Kobe University; Graduate School of System Informatics, Kobe University; Graduate School of System Informatics, Kobe University",
        "aff_domain": "ai.cs.kobe-u.ac.jp;phoenix.kobe-u.ac.jp;kobe-u.ac.jp",
        "email": "ai.cs.kobe-u.ac.jp;phoenix.kobe-u.ac.jp;kobe-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Kobe University",
        "aff_unique_dep": "Graduate School of System Informatics",
        "aff_unique_url": "https://www.kobe-u.ac.jp",
        "aff_unique_abbr": "Kobe U",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kobe",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "b3e028e719",
        "title": "Select-and-Evaluate: A Learning Framework for Large-Scale Knowledge Graph Search",
        "site": "https://proceedings.mlr.press/v77/chowdhury17a.html",
        "author": "F A Rezaur Rahman Chowdhury; Chao Ma; Md Rakibul Islam; Mohammad Hossein Namaki; Mohammad Omar Faruk; Janardhan Rao Doppa",
        "abstract": "Querying graph structured data is a fundamental operation that enables important applications including knowledge graph search, social network analysis, and cyber-network security. However, the growing size of real-world data graphs poses severe challenges for graph search to meet the response-time requirements of the applications. To address these scalability challenges, we develop a learning framework for graph search called \\bf Sele\\bf ct-and-Ev\\bf aluat\\bf e (SCALE). The key insight is to select a small part of the data graph that is sufficient to answer a given query in order to satisfy the specified constraints on time or accuracy. We formulate the problem of generating the candidate subgraph as a computational search process and induce search control knowledge from training queries using imitation learning. First, we define a search space over candidate selection plans, and identify target selection plans corresponding to the training queries by performing an expensive search. Subsequently, we learn greedy search control knowledge to imitate the search behavior of the target selection plans. Our experiments on large-scale knowledge graphs including DBpedia, YAGO, and Freebase show that using the learned selection plans, we can significantly improve the computational-efficiency of graph search to achieve high accuracy.",
        "bibtex": "@InProceedings{pmlr-v77-chowdhury17a,\n  title = \t {Select-and-Evaluate: A Learning Framework for Large-Scale Knowledge Graph Search},\n  author = \t {Chowdhury, F A Rezaur Rahman and Ma, Chao and Islam, Md Rakibul and Namaki, Mohammad Hossein and Faruk, Mohammad Omar and Doppa, Janardhan Rao},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {129--144},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/chowdhury17a/chowdhury17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/chowdhury17a.html},\n  abstract = \t {Querying graph structured data is a fundamental operation that enables important applications including knowledge graph search, social network analysis, and cyber-network security. However, the growing size of real-world data graphs poses severe challenges for graph search to meet the response-time requirements of the applications. To address these scalability challenges, we develop a learning framework for graph search called \\bf Sele\\bf ct-and-Ev\\bf aluat\\bf e (SCALE). The key insight is to select a small part of the data graph that is sufficient to answer a given query in order to satisfy the specified constraints on time or accuracy. We formulate the problem of generating the candidate subgraph as a computational search process and induce search control knowledge from training queries using imitation learning. First, we define a search space over candidate selection plans, and identify target selection plans corresponding to the training queries by performing an expensive search. Subsequently, we learn greedy search control knowledge to imitate the search behavior of the target selection plans. Our experiments on large-scale knowledge graphs including DBpedia, YAGO, and Freebase show that using the learned selection plans, we can significantly improve the computational-efficiency of graph search to achieve high accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/chowdhury17a/chowdhury17a.pdf",
        "supp": "",
        "pdf_size": 540447,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=176541776722094681&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4d56f543bf",
        "title": "Semi-supervised Convolutional Neural Networks for Identifying Wi-Fi Interference Sources",
        "site": "https://proceedings.mlr.press/v77/longi17a.html",
        "author": "Krista Longi; Teemu Pulkkinen; Arto Klami",
        "abstract": "We present a convolutional neural network for identifying radio frequency devices from signal data, in order to detect possible interference sources for wireless local area networks. Collecting training data for this problem is particularly challenging due to a high number of possible interfering devices, difficulty in obtaining precise timings, and the need to measure the devices in varying conditions. To overcome this challenge we focus on semi-supervised learning, aiming to minimize the need for reliable training samples while utilizing larger amounts of unsupervised labels to improve the accuracy. In particular, we propose a novel structured extension of the pseudo-label technique to take advantage of temporal continuity in the data and show that already a few seconds of training data for each device is sufficient for highly accurate recognition.",
        "bibtex": "@InProceedings{pmlr-v77-longi17a,\n  title = \t {Semi-supervised Convolutional Neural Networks for Identifying Wi-Fi Interference Sources},\n  author = \t {Longi, Krista and Pulkkinen, Teemu and Klami, Arto},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {391--406},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/longi17a/longi17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/longi17a.html},\n  abstract = \t {We present a convolutional neural network for identifying radio frequency devices from signal data, in order to detect possible interference sources for wireless local area networks. Collecting training data for this problem is particularly challenging due to a high number of possible interfering devices, difficulty in obtaining precise timings, and the need to measure the devices in varying conditions. To overcome this challenge we focus on semi-supervised learning, aiming to minimize the need for reliable training samples while utilizing larger amounts of unsupervised labels to improve the accuracy. In particular, we propose a novel structured extension of the pseudo-label technique to take advantage of temporal continuity in the data and show that already a few seconds of training data for each device is sufficient for highly accurate recognition.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/longi17a/longi17a.pdf",
        "supp": "",
        "pdf_size": 780135,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10900949928509052688&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Helsinki Institute for Information Technology HIIT, Department of Computer Science, University of Helsinki; Department of Computer Science, University of Helsinki + Ekahau Oy; Helsinki Institute for Information Technology HIIT, Department of Computer Science, University of Helsinki",
        "aff_domain": "helsinki.fi;ekahau.com;cs.helsinki.fi",
        "email": "helsinki.fi;ekahau.com;cs.helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University of Helsinki;Ekahau",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.helsinki.fi;https://www.ekahau.com",
        "aff_unique_abbr": "UH;Ekahau",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "f25b06751c",
        "title": "Using Deep Neural Networks to Automate Large Scale Statistical Analysis for Big Data Applications",
        "site": "https://proceedings.mlr.press/v77/zhang17d.html",
        "author": "Rongrong Zhang; Wei Deng; Michael Yu Zhu",
        "abstract": "Statistical analysis (SA) is a complex process to deduce population properties from analysis of data. It usually takes a well-trained analyst to successfully perform SA, and it becomes extremely challenging to apply SA to big data applications. We propose to use deep neural networks  to automate the SA process. In particular, we propose to construct convolutional neural networks (CNNs) to perform automatic model selection and parameter estimation, two most important SA tasks. We refer to the resulting CNNs as the neural model selector and the neural model estimator, respectively, which can be properly trained using labeled data systematically generated from candidate models. Simulation study shows that both the selector and estimator demonstrate excellent performances. The idea and proposed framework can be further extended to automate the entire SA process and have the potential to revolutionize how SA is performed in big data analytics.",
        "bibtex": "@InProceedings{pmlr-v77-zhang17d,\n  title = \t {Using Deep Neural Networks to Automate Large Scale Statistical Analysis for Big Data Applications},\n  author = \t {Zhang, Rongrong and Deng, Wei and Zhu, Michael Yu},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {311--326},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/zhang17d/zhang17d.pdf},\n  url = \t {https://proceedings.mlr.press/v77/zhang17d.html},\n  abstract = \t {Statistical analysis (SA) is a complex process to deduce population properties from analysis of data. It usually takes a well-trained analyst to successfully perform SA, and it becomes extremely challenging to apply SA to big data applications. We propose to use deep neural networks  to automate the SA process. In particular, we propose to construct convolutional neural networks (CNNs) to perform automatic model selection and parameter estimation, two most important SA tasks. We refer to the resulting CNNs as the neural model selector and the neural model estimator, respectively, which can be properly trained using labeled data systematically generated from candidate models. Simulation study shows that both the selector and estimator demonstrate excellent performances. The idea and proposed framework can be further extended to automate the entire SA process and have the potential to revolutionize how SA is performed in big data analytics.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/zhang17d/zhang17d.pdf",
        "supp": "",
        "pdf_size": 2634057,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17527952696380579668&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Statistics, Purdue University; Department of Mathematics, Purdue University; Department of Statistics, Purdue University + Center for Statistical Science, Department of Industrial Engineering, Tsinghua University",
        "aff_domain": "purdue.edu;purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Purdue University;Tsinghua University",
        "aff_unique_dep": "Department of Statistics;Department of Industrial Engineering",
        "aff_unique_url": "https://www.purdue.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Purdue;Tsinghua",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "811b311da7",
        "title": "Whitening-Free Least-Squares Non-Gaussian Component Analysis",
        "site": "https://proceedings.mlr.press/v77/shiino17a.html",
        "author": "Hiroaki Shiino; Hiroaki Sasaki; Gang Niu; Masashi Sugiyama",
        "abstract": "\\emphNon-Gaussian component analysis (NGCA) is an unsupervised linear dimension reduction method that extracts low-dimensional non-Gaussian \u201csignals\u201d from high-dimensional data contaminated with Gaussian noise. NGCA can be regarded as a generalization of \\emphprojection pursuit (PP) and \\emphindependent component analysis (ICA) to multi-dimensional and dependent non-Gaussian components. Indeed, seminal approaches to NGCA are based on PP and ICA. Recently, a novel NGCA approach called \\emphleast-squares NGCA (LSNGCA) has been developed, which gives a solution analytically through least-squares estimation of \\emphlog-density gradients and eigendecomposition. However, since \\emphpre-whitening of data is involved in LSNGCA, it performs unreliably when the data covariance matrix is ill-conditioned, which is often the case in high-dimensional data analysis. In this paper, we propose a \\emphwhitening-free variant of LSNGCA and experimentally demonstrate its superiority.",
        "bibtex": "@InProceedings{pmlr-v77-shiino17a,\n  title = \t {Whitening-Free Least-Squares Non-Gaussian Component Analysis},\n  author = \t {Shiino, Hiroaki and Sasaki, Hiroaki and Niu, Gang and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the Ninth Asian Conference on Machine Learning},\n  pages = \t {375--390},\n  year = \t {2017},\n  editor = \t {Zhang, Min-Ling and Noh, Yung-Kyun},\n  volume = \t {77},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Yonsei University, Seoul, Republic of Korea},\n  month = \t {15--17 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v77/shiino17a/shiino17a.pdf},\n  url = \t {https://proceedings.mlr.press/v77/shiino17a.html},\n  abstract = \t {\\emphNon-Gaussian component analysis (NGCA) is an unsupervised linear dimension reduction method that extracts low-dimensional non-Gaussian \u201csignals\u201d from high-dimensional data contaminated with Gaussian noise. NGCA can be regarded as a generalization of \\emphprojection pursuit (PP) and \\emphindependent component analysis (ICA) to multi-dimensional and dependent non-Gaussian components. Indeed, seminal approaches to NGCA are based on PP and ICA. Recently, a novel NGCA approach called \\emphleast-squares NGCA (LSNGCA) has been developed, which gives a solution analytically through least-squares estimation of \\emphlog-density gradients and eigendecomposition. However, since \\emphpre-whitening of data is involved in LSNGCA, it performs unreliably when the data covariance matrix is ill-conditioned, which is often the case in high-dimensional data analysis. In this paper, we propose a \\emphwhitening-free variant of LSNGCA and experimentally demonstrate its superiority.}\n}",
        "pdf": "http://proceedings.mlr.press/v77/shiino17a/shiino17a.pdf",
        "supp": "",
        "pdf_size": 697867,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=160059311890595634&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tokyo Institute of Technology + Yahoo Japan Corporation; Nara Institute of Science and Technology; The University of Tokyo + RIKEN Center for Advanced Intelligence Project; RIKEN Center for Advanced Intelligence Project + The University of Tokyo",
        "aff_domain": "yahoo-corp.jp;is.naist.jp;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "email": "yahoo-corp.jp;is.naist.jp;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3+4;4+3",
        "aff_unique_norm": "Tokyo Institute of Technology;Yahoo Japan Corporation;Nara Institute of Science and Technology;University of Tokyo;RIKEN",
        "aff_unique_dep": ";;;;Center for Advanced Intelligence Project",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.yahoo.co.jp;https://www.nist.go.jp;https://www.u-tokyo.ac.jp;https://www.riken.jp/en/",
        "aff_unique_abbr": "Titech;Yahoo Japan;NIST;UTokyo;RIKEN",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "Japan"
    }
]