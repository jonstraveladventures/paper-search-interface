[
    {
        "id": "261ead0fdf",
        "title": "A New Look at Nearest Neighbours: Identifying Benign Input Geometries via Random Projections",
        "site": "https://proceedings.mlr.press/v45/Kaban15b.html",
        "author": "Ata Kaban",
        "abstract": "It is well known that in general, the nearest neighbour rule (NN) has sample complexity that is exponential in the input space dimension d when only smoothness is assumed on the label posterior function. Here we consider NN on randomly projected  data, and we show that, if the input domain has a small \"metric size\", then the sample complexity becomes exponential in the metric entropy integral of the set of normalised chords of the input domain. This metric entropy integral measures the complexity of the input domain, and can be much smaller than d \u2013 for instance in cases when the data lies in a  linear or a smooth nonlinear subspace of the ambient space, or when it has a sparse representation. We then show that the guarantees we obtain for the compressive NN also hold for the dataspace NN in bounded domains; thus the random projection takes the role of an analytic tool to identify benign structures under which NN learning is possible from a small sample size. Numerical simulations on data designed to have intrinsically low complexity confirm our theoretical findings, and display a striking agreement in the empirical performances of compressive NN and dataspace NN. This suggests that high dimensional data sets that have a low complexity underlying structure are well suited for computationally cheap  compressive NN learning.",
        "bibtex": "@InProceedings{pmlr-v45-Kaban15b,\n  title = \t {A New Look at Nearest Neighbours: Identifying Benign Input Geometries via Random Projections},\n  author = \t {Kaban, Ata},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {65--80},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Kaban15b.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Kaban15b.html},\n  abstract = \t {It is well known that in general, the nearest neighbour rule (NN) has sample complexity that is exponential in the input space dimension d when only smoothness is assumed on the label posterior function. Here we consider NN on randomly projected  data, and we show that, if the input domain has a small \"metric size\", then the sample complexity becomes exponential in the metric entropy integral of the set of normalised chords of the input domain. This metric entropy integral measures the complexity of the input domain, and can be much smaller than d \u2013 for instance in cases when the data lies in a  linear or a smooth nonlinear subspace of the ambient space, or when it has a sparse representation. We then show that the guarantees we obtain for the compressive NN also hold for the dataspace NN in bounded domains; thus the random projection takes the role of an analytic tool to identify benign structures under which NN learning is possible from a small sample size. Numerical simulations on data designed to have intrinsically low complexity confirm our theoretical findings, and display a striking agreement in the empirical performances of compressive NN and dataspace NN. This suggests that high dimensional data sets that have a low complexity underlying structure are well suited for computationally cheap  compressive NN learning.   }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Kaban15b.pdf",
        "supp": "",
        "pdf_size": 837589,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14727684896392492515&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computer Science, The University of Birmingham, Edgbaston, B15 2TT, Birmingham, UK",
        "aff_domain": "cs.bham.ac.uk",
        "email": "cs.bham.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Birmingham",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.birmingham.ac.uk",
        "aff_unique_abbr": "Birmingham",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Birmingham",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "3fca448542",
        "title": "A Unified Framework for Jointly Learning Distributed Representations of Word and Attributes",
        "site": "https://proceedings.mlr.press/v45/Niu15.html",
        "author": "Liqiang Niu; Xin-Yu Dai; Shujian Huang; Jiajun Chen",
        "abstract": "Distributed word representations have achieved great success in natural language processing (NLP) area. However, most distributed models focus on local context properties and learn task-specific representations individually, therefore lack the ability to fuse multi-attributes and learn jointly. In this paper, we propose a unified framework which jointly learns distributed representations of word and attributes: characteristics of word. In our models, we consider three types of attributes: topic, lemma and document. Besides learning distributed attribute representations, we find that using additional attributes is beneficial to improve word representations. Several experiments are conducted to evaluate the performance of the learned topic representations, document representations, and improved word representations, respectively. The experimental results show that our models achieve significant and competitive results.",
        "bibtex": "@InProceedings{pmlr-v45-Niu15,\n  title = \t {A Unified Framework for Jointly Learning Distributed Representations of Word and Attributes},\n  author = \t {Niu, Liqiang and Dai, Xin-Yu and Huang, Shujian and Chen, Jiajun},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {143--156},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Niu15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Niu15.html},\n  abstract = \t {Distributed word representations have achieved great success in natural language processing (NLP) area. However, most distributed models focus on local context properties and learn task-specific representations individually, therefore lack the ability to fuse multi-attributes and learn jointly. In this paper, we propose a unified framework which jointly learns distributed representations of word and attributes: characteristics of word. In our models, we consider three types of attributes: topic, lemma and document. Besides learning distributed attribute representations, we find that using additional attributes is beneficial to improve word representations. Several experiments are conducted to evaluate the performance of the learned topic representations, document representations, and improved word representations, respectively. The experimental results show that our models achieve significant and competitive results. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Niu15.pdf",
        "supp": "",
        "pdf_size": 1388128,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6406821400420314916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "National Key Laboratory for Novel Software Techonology; National Key Laboratory for Novel Software Techonology; National Key Laboratory for Novel Software Techonology; National Key Laboratory for Novel Software Techonology",
        "aff_domain": "nlp.nju.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "email": "nlp.nju.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National Key Laboratory for Novel Software Technology",
        "aff_unique_dep": "Laboratory for Novel Software Technology",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "be1d81bb49",
        "title": "Autoencoder Trees",
        "site": "https://proceedings.mlr.press/v45/Irsoy15.html",
        "author": "Ozan \u0130rsoy; Ethem Alpaydin",
        "abstract": "We discuss an autoencoder model in which the encoding and decoding functions are implemented by decision trees. We use the soft decision tree where internal nodes realize soft multivariate splits given by a gating function and the overall output is the average of all leaves weighted by the gating values on their path. The encoder tree takes the input and generates a lower dimensional representation in the leaves and the decoder tree takes this and reconstructs the original input. Exploiting the continuity of the trees, autoencoder trees are trained with stochastic gradient-descent. On handwritten digit and news data, we see that the autoencoder trees yield good reconstruction error compared to traditional autoencoder perceptrons. We also see that the autoencoder tree captures hierarchical representations at different granularities of the data on its different levels and the leaves capture the localities in the input space.",
        "bibtex": "@InProceedings{pmlr-v45-Irsoy15,\n  title = \t {Autoencoder Trees},\n  author = \t {\u0130rsoy, Ozan and Alpaydin, Ethem},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {378--390},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Irsoy15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Irsoy15.html},\n  abstract = \t {We discuss an autoencoder model in which the encoding and decoding functions are implemented by decision trees. We use the soft decision tree where internal nodes realize soft multivariate splits given by a gating function and the overall output is the average of all leaves weighted by the gating values on their path. The encoder tree takes the input and generates a lower dimensional representation in the leaves and the decoder tree takes this and reconstructs the original input. Exploiting the continuity of the trees, autoencoder trees are trained with stochastic gradient-descent. On handwritten digit and news data, we see that the autoencoder trees yield good reconstruction error compared to traditional autoencoder perceptrons. We also see that the autoencoder tree captures hierarchical representations at different granularities of the data on its different levels and the leaves capture the localities in the input space. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Irsoy15.pdf",
        "supp": "",
        "pdf_size": 977421,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16940062313134725231&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Cornell University, Ithaca, NY 14853; Department of Computer Engineering, Bo\u02d8 gazi\u00b8 ci University, Bebek, \u02d9Istanbul 34342 Turkey",
        "aff_domain": "cs.cornell.edu;boun.edu.tr",
        "email": "cs.cornell.edu;boun.edu.tr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cornell University;Bo\u02d8 gazi\u00b8 ci University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Engineering",
        "aff_unique_url": "https://www.cornell.edu;https://www.boun.edu.tr",
        "aff_unique_abbr": "Cornell;Bogazici University",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Ithaca;Bebek",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;T\u00fcrkiye"
    },
    {
        "id": "f7b93b392b",
        "title": "Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias",
        "site": "https://proceedings.mlr.press/v45/Kondo15.html",
        "author": "Yohei Kondo; Shin-ichi Maeda; Kohei Hayashi",
        "abstract": "A common strategy for sparse linear regression is to introduce regularization, which eliminates irrelevant features by letting the corresponding weights be zeros. However, regularization often shrinks the estimator for relevant features, which leads to incorrect feature selection. Motivated by the above-mentioned issue, we propose Bayesian masking (BM), a sparse estimation method which imposes no regularization on the weights.  The key concept of BM is to introduce binary latent variables that randomly mask features. Estimating the masking rates determines the relevance of the features automatically. We derive a variational Bayesian inference algorithm that maximizes the lower bound of the factorized information criterion (FIC), which is a recently developed asymptotic criterion for evaluating the marginal log-likelihood. In addition, we propose reparametrization to accelerate the convergence of the derived algorithm. Finally, we show that BM outperforms Lasso and automatic relevance determination (ARD) in terms of the sparsity-shrinkage trade-off.",
        "bibtex": "@InProceedings{pmlr-v45-Kondo15,\n  title = \t {Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias},\n  author = \t {Kondo, Yohei and Maeda, Shin-ichi and Hayashi, Kohei},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {49--64},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Kondo15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Kondo15.html},\n  abstract = \t {A common strategy for sparse linear regression is to introduce regularization, which eliminates irrelevant features by letting the corresponding weights be zeros. However, regularization often shrinks the estimator for relevant features, which leads to incorrect feature selection. Motivated by the above-mentioned issue, we propose Bayesian masking (BM), a sparse estimation method which imposes no regularization on the weights.  The key concept of BM is to introduce binary latent variables that randomly mask features. Estimating the masking rates determines the relevance of the features automatically. We derive a variational Bayesian inference algorithm that maximizes the lower bound of the factorized information criterion (FIC), which is a recently developed asymptotic criterion for evaluating the marginal log-likelihood. In addition, we propose reparametrization to accelerate the convergence of the derived algorithm. Finally, we show that BM outperforms Lasso and automatic relevance determination (ARD) in terms of the sparsity-shrinkage trade-off. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Kondo15.pdf",
        "supp": "",
        "pdf_size": 1309394,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8338288278340512712&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Graduate School of Informatics, Kyoto University, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan; National Institute of Informatics, Tokyo, Japan + Kawarabayashi Large Graph Project, ERATO, JST",
        "aff_domain": "sys.i.kyoto-u.ac.jp;sys.i.kyoto-u.ac.jp;gmail.com",
        "email": "sys.i.kyoto-u.ac.jp;sys.i.kyoto-u.ac.jp;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2",
        "aff_unique_norm": "Kyoto University;National Institute of Informatics;Japan Science and Technology Agency",
        "aff_unique_dep": "Graduate School of Informatics;;Kawarabayashi Large Graph Project",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.nii.ac.jp;https://www.jst.go.jp",
        "aff_unique_abbr": "Kyoto U;NII;JST",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Kyoto;Tokyo;",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "66d60d60ce",
        "title": "Budgeted Bandit Problems with Continuous Random Costs",
        "site": "https://proceedings.mlr.press/v45/Xia15.html",
        "author": "Yingce Xia; Wenkui Ding; Xu-Dong Zhang; Nenghai Yu; Tao Qin",
        "abstract": "We study the budgeted bandit problem, where each arm is associated with both a reward and a cost. In a budgeted bandit problem, the objective is to design an arm pulling algorithm in order to maximize the total reward before the budget runs out. In this work, we study both multi-armed bandits and linear bandits, and focus on the setting with continuous random costs. We propose an upper confidence bound based algorithm for multi-armed bandits and a confidence ball based algorithm for linear bandits, and prove logarithmic regret bounds for both algorithms. We conduct simulations on the proposed algorithms, which verify the effectiveness of our proposed algorithms.",
        "bibtex": "@InProceedings{pmlr-v45-Xia15,\n  title = \t {Budgeted Bandit Problems with Continuous Random Costs},\n  author = \t {Xia, Yingce and Ding, Wenkui and Zhang, Xu-Dong and Yu, Nenghai and Qin, Tao},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {317--332},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Xia15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Xia15.html},\n  abstract = \t {We study the budgeted bandit problem, where each arm is associated with both a reward and a cost. In a budgeted bandit problem, the objective is to design an arm pulling algorithm in order to maximize the total reward before the budget runs out. In this work, we study both multi-armed bandits and linear bandits, and focus on the setting with continuous random costs. We propose an upper confidence bound based algorithm for multi-armed bandits and a confidence ball based algorithm for linear bandits, and prove logarithmic regret bounds for both algorithms. We conduct simulations on the proposed algorithms, which verify the effectiveness of our proposed algorithms. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Xia15.pdf",
        "supp": "",
        "pdf_size": 487759,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16915333026747701637&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1e465c51cc",
        "title": "Class-prior Estimation for Learning from Positive and Unlabeled Data",
        "site": "https://proceedings.mlr.press/v45/Christoffel15.html",
        "author": "Marthinus Christoffel; Gang Niu; Masashi Sugiyama",
        "abstract": "We consider the problem of estimating the \\emphclass prior in an unlabeled dataset. Under the assumption that an additional labeled dataset is available, the class prior can be estimated by fitting a mixture of class-wise data distributions to the unlabeled data distribution. However, in practice, such an additional labeled dataset is often not available. In this paper, we show that, with additional samples coming only from the positive class, the class prior of the unlabeled dataset can be estimated correctly. Our key idea is to use properly penalized divergences for model fitting to cancel the error caused by the absence of negative samples. We further show that the use of the penalized L_1-distance gives a computationally efficient algorithm with an analytic solution, and establish its uniform deviation bound and estimation error bound. Finally, we experimentally demonstrate the usefulness of the proposed method.",
        "bibtex": "@InProceedings{pmlr-v45-Christoffel15,\n  title = \t {Class-prior Estimation for Learning from Positive and Unlabeled Data},\n  author = \t {Christoffel, Marthinus and Niu, Gang and Sugiyama, Masashi},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {221--236},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Christoffel15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Christoffel15.html},\n  abstract = \t {We consider the problem of estimating the \\emphclass prior in an unlabeled dataset. Under the assumption that an additional labeled dataset is available, the class prior can be estimated by fitting a mixture of class-wise data distributions to the unlabeled data distribution. However, in practice, such an additional labeled dataset is often not available. In this paper, we show that, with additional samples coming only from the positive class, the class prior of the unlabeled dataset can be estimated correctly. Our key idea is to use properly penalized divergences for model fitting to cancel the error caused by the absence of negative samples. We further show that the use of the penalized L_1-distance gives a computationally efficient algorithm with an analytic solution, and establish its uniform deviation bound and estimation error bound. Finally, we experimentally demonstrate the usefulness of the proposed method.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Christoffel15.pdf",
        "supp": "",
        "pdf_size": 467667,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10047676413946205387&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "The University of Tokyo, Tokyo, 113-0033, Japan; The University of Tokyo, Tokyo, 113-0033, Japan; The University of Tokyo, Tokyo, 113-0033, Japan",
        "aff_domain": "ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "email": "ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "30e5457e8e",
        "title": "Consistency of structured output learning with missing labels",
        "site": "https://proceedings.mlr.press/v45/Antoniuk15.html",
        "author": "Kostiantyn Antoniuk; Vojtech Franc; Vaclav Hlavac",
        "abstract": "In this paper we study statistical consistency of partial losses suitable for learning structured output predictors from examples containing missing labels. We provide sufficient conditions on data generating distribution which admit to prove that the expected risk of the structured predictor learned by minimizing the partial loss converges to the optimal Bayes risk defined by an associated complete loss. We define a concept of surrogate classification calibrated partial losses which are easier to optimize yet their minimization preserves the statistical consistency. We give some concrete examples of surrogate partial losses which are classification calibrated. In particular, we show that the ramp-loss which is in the core of many existing algorithms  is classification calibrated.",
        "bibtex": "@InProceedings{pmlr-v45-Antoniuk15,\n  title = \t {Consistency of structured output learning with missing labels},\n  author = \t {Antoniuk, Kostiantyn and Franc, Vojtech and Hlavac, Vaclav},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {81--95},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Antoniuk15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Antoniuk15.html},\n  abstract = \t {In this paper we study statistical consistency of partial losses suitable for learning structured output predictors from examples containing missing labels. We provide sufficient conditions on data generating distribution which admit to prove that the expected risk of the structured predictor learned by minimizing the partial loss converges to the optimal Bayes risk defined by an associated complete loss. We define a concept of surrogate classification calibrated partial losses which are easier to optimize yet their minimization preserves the statistical consistency. We give some concrete examples of surrogate partial losses which are classification calibrated. In particular, we show that the ramp-loss which is in the core of many existing algorithms  is classification calibrated.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Antoniuk15.pdf",
        "supp": "",
        "pdf_size": 397924,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14486038287633055594&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Faculty of Electrical Engineering, Czech Technical University in Prague; Faculty of Electrical Engineering, Czech Technical University in Prague; Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague",
        "aff_domain": "CMP.FELK.CVUT.CZ;CMP.FELK.CVUT.CZ;FEL.CVUT.CZ",
        "email": "CMP.FELK.CVUT.CZ;CMP.FELK.CVUT.CZ;FEL.CVUT.CZ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Czech Technical University in Prague",
        "aff_unique_dep": "Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.cvut.cz",
        "aff_unique_abbr": "CTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Prague",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "id": "83be9fb493",
        "title": "Continuous Target Shift Adaptation in Supervised Learning",
        "site": "https://proceedings.mlr.press/v45/Nguyen15.html",
        "author": "Tuan Duong Nguyen; Marthinus Christoffel; Masashi Sugiyama",
        "abstract": "Supervised learning in machine learning concerns inferring an underlying relation between covariate \\bx and target y based on training covariate-target data. It is traditionally assumed that training data and test data, on which the generalization performance of a learning algorithm is measured, follow the same probability distribution. However, this standard assumption is often violated in many real-world applications such as computer vision, natural language processing, robot control, or survey design, due to intrinsic non-stationarity of the environment or inevitable sample selection bias. This situation is called \\emphdataset shift and has attracted a great deal of attention recently. In the paper, we consider supervised learning problems under the \\emphtarget shift scenario, where the target marginal distribution p(y) changes between the training and testing phases, while the target-conditioned covariate distribution p(\\bx|y) remains unchanged. Although various methods for mitigating target shift in classification (a.k.a. \\emphclass prior change) have been developed so far, few methods can be applied to continuous targets. In this paper, we propose methods for continuous target shift adaptation in regression and conditional density estimation. More specifically, our contribution is a novel importance weight estimator for continuous targets. Through experiments, the usefulness of the proposed method is demonstrated.",
        "bibtex": "@InProceedings{pmlr-v45-Nguyen15,\n  title = \t {Continuous Target Shift Adaptation in Supervised Learning},\n  author = \t {Nguyen, Tuan Duong and Christoffel, Marthinus and Sugiyama, Masashi},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {285--300},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Nguyen15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Nguyen15.html},\n  abstract = \t {Supervised learning in machine learning concerns inferring an underlying relation between covariate \\bx and target y based on training covariate-target data. It is traditionally assumed that training data and test data, on which the generalization performance of a learning algorithm is measured, follow the same probability distribution. However, this standard assumption is often violated in many real-world applications such as computer vision, natural language processing, robot control, or survey design, due to intrinsic non-stationarity of the environment or inevitable sample selection bias. This situation is called \\emphdataset shift and has attracted a great deal of attention recently. In the paper, we consider supervised learning problems under the \\emphtarget shift scenario, where the target marginal distribution p(y) changes between the training and testing phases, while the target-conditioned covariate distribution p(\\bx|y) remains unchanged. Although various methods for mitigating target shift in classification (a.k.a. \\emphclass prior change) have been developed so far, few methods can be applied to continuous targets. In this paper, we propose methods for continuous target shift adaptation in regression and conditional density estimation. More specifically, our contribution is a novel importance weight estimator for continuous targets. Through experiments, the usefulness of the proposed method is demonstrated.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Nguyen15.pdf",
        "supp": "",
        "pdf_size": 473453,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8296988944413759211&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Tokyo Institute of Technology, Tokyo, 152-8550, Japan; The University of Tokyo, Tokyo, 113-0033, Japan; The University of Tokyo, Tokyo, 113-0033, Japan",
        "aff_domain": "gmail.com;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "email": "gmail.com;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Tokyo Institute of Technology;University of Tokyo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "Titech;UTokyo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "6ebd600617",
        "title": "Curriculum Learning of Bayesian Network Structures",
        "site": "https://proceedings.mlr.press/v45/Zhao15a.html",
        "author": "Yanpeng Zhao; Yetian Chen; Kewei Tu; Jin Tian",
        "abstract": "Bayesian networks (BNs) are directed graphical models that have been widely used in various tasks for probabilistic reasoning and causal modeling.  One major challenge in these tasks is to learn the BN structures from data. In this paper, we propose a novel heuristic algorithm for BN structure learning that takes advantage of the idea of \\emphcurriculum learning. Our algorithm learns the BN structure by stages. At each stage a subnet is learned over a selected subset of the random variables conditioned on fixed values of the rest of the variables. The selected subset grows with stages and eventually includes all the variables. We prove theoretical advantages of our algorithm and also empirically show that it outperformed the state-of-the-art heuristic approach in learning BN structures.",
        "bibtex": "@InProceedings{pmlr-v45-Zhao15a,\n  title = \t {Curriculum Learning of Bayesian Network Structures},\n  author = \t {Zhao, Yanpeng and Chen, Yetian and Tu, Kewei and Tian, Jin},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {269--284},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Zhao15a.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Zhao15a.html},\n  abstract = \t {Bayesian networks (BNs) are directed graphical models that have been widely used in various tasks for probabilistic reasoning and causal modeling.  One major challenge in these tasks is to learn the BN structures from data. In this paper, we propose a novel heuristic algorithm for BN structure learning that takes advantage of the idea of \\emphcurriculum learning. Our algorithm learns the BN structure by stages. At each stage a subnet is learned over a selected subset of the random variables conditioned on fixed values of the rest of the variables. The selected subset grows with stages and eventually includes all the variables. We prove theoretical advantages of our algorithm and also empirically show that it outperformed the state-of-the-art heuristic approach in learning BN structures.\t }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Zhao15a.pdf",
        "supp": "",
        "pdf_size": 522001,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6618438265100075350&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Department of Computer Science, Iowa State University, Ames, IA, USA; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Department of Computer Science, Iowa State University, Ames, IA, USA",
        "aff_domain": "shanghaitech.edu.cn;iastate.edu;shanghaitech.edu.cn;iastate.edu",
        "email": "shanghaitech.edu.cn;iastate.edu;shanghaitech.edu.cn;iastate.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "ShanghaiTech University;Iowa State University",
        "aff_unique_dep": "School of Information Science and Technology;Department of Computer Science",
        "aff_unique_url": "https://www.shanghaitech.edu.cn;https://www.iastate.edu",
        "aff_unique_abbr": "ShanghaiTech;ISU",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Shanghai;Ames",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "702d5f5177",
        "title": "Data-Guided Approach for Learning and Improving User Experience in Computer Networks",
        "site": "https://proceedings.mlr.press/v45/Bao15.html",
        "author": "Yanan Bao; Xin Liu; Amit Pande",
        "abstract": "Machine learning algorithms have been traditionally used to understand user behavior or system performance. In computer networks, with a subset of input features as controllable network parameters, we envision developing a data-driven network resource allocation framework that can optimize user experience. In particular, we explore how to leverage a classifier learned from training instances to optimally guide network resource allocation to improve the overall performance on test instances. Based on logistic regression, we propose an optimal resource allocation algorithm, as well as heuristics with low-complexity. We evaluate the performance of the proposed algorithms using a synthetic Gaussian dataset, a real world dataset on video streaming over throttled networks, and a tier-one cellular operator\u2019s customer complaint traces. The evaluation demonstrates the effectiveness of the proposed algorithms; e.g., the optimal algorithm can have a 400% improvement compared with the baseline.",
        "bibtex": "@InProceedings{pmlr-v45-Bao15,\n  title = \t {Data-Guided Approach for Learning and Improving User Experience in Computer Networks},\n  author = \t {Bao, Yanan and Liu, Xin and Pande, Amit},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {127--142},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Bao15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Bao15.html},\n  abstract = \t {Machine learning algorithms have been traditionally used to understand user behavior or system performance. In computer networks, with a subset of input features as controllable network parameters, we envision developing a data-driven network resource allocation framework that can optimize user experience. In particular, we explore how to leverage a classifier learned from training instances to optimally guide network resource allocation to improve the overall performance on test instances. Based on logistic regression, we propose an optimal resource allocation algorithm, as well as heuristics with low-complexity. We evaluate the performance of the proposed algorithms using a synthetic Gaussian dataset, a real world dataset on video streaming over throttled networks, and a tier-one cellular operator\u2019s customer complaint traces. The evaluation demonstrates the effectiveness of the proposed algorithms; e.g., the optimal algorithm can have a 400% improvement compared with the baseline. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Bao15.pdf",
        "supp": "",
        "pdf_size": 837907,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12640048922268014444&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Davis, CA 95616, USA; University of California, Davis, CA 95616, USA; University of California, Davis, CA 95616, USA",
        "aff_domain": "ucdavis.edu;cs.ucdavis.edu;ucdavis.edu",
        "email": "ucdavis.edu;cs.ucdavis.edu;ucdavis.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Davis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucdavis.edu",
        "aff_unique_abbr": "UC Davis",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Davis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "335c99c501",
        "title": "Detecting Accounting Frauds in Publicly Traded U.S. Firms: A Machine Learning Approach",
        "site": "https://proceedings.mlr.press/v45/Li15.html",
        "author": "Bin Li; Julia Yu; Jie Zhang; Bin Ke",
        "abstract": "This paper studies how machine learning techniques can facilitate the detection of accounting fraud in publicly traded US firms. Existing studies often mimic human experts and employ the financial or nonfinancial ratios as the features for their systems. We depart from these studies by adopting raw accounting variables, which are directly available from a firm\u2019s financial statement and thereby can be easily applied to new firms at low cost. Further, we collected the most complete fraud dataset of US publicly traded firms and labeled the fraud and non-fraud firm-years. One key issue of the dataset is that the data is extremely imbalanced, in which the fraud firm-years are often less than one percent. Without re-sampling the data, we further propose to tackle the imbalance issue by adopting the techniques of imbalanced learning. In particular, we employ the linear and nonlinear Biased Penalty Support Vector Machine and the Ensemble Methods, both of which have been proved to successfully handle the imbalance issue in the machine learning literatures. We finally evaluate our approach by conducting extensive empirical studies. Empirical results show that the proposed schema can achieve much better performance, in terms of balanced accuracy, than the state of the art. Besides the performance, our approaches can also compute very fast, which further supports their practical deployment.",
        "bibtex": "@InProceedings{pmlr-v45-Li15,\n  title = \t {Detecting Accounting Frauds in Publicly Traded U.S. Firms: A Machine Learning Approach},\n  author = \t {Li, Bin and Yu, Julia and Zhang, Jie and Ke, Bin},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {173--188},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Li15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Li15.html},\n  abstract = \t {This paper studies how machine learning techniques can facilitate the detection of accounting fraud in publicly traded US firms. Existing studies often mimic human experts and employ the financial or nonfinancial ratios as the features for their systems. We depart from these studies by adopting raw accounting variables, which are directly available from a firm\u2019s financial statement and thereby can be easily applied to new firms at low cost. Further, we collected the most complete fraud dataset of US publicly traded firms and labeled the fraud and non-fraud firm-years. One key issue of the dataset is that the data is extremely imbalanced, in which the fraud firm-years are often less than one percent. Without re-sampling the data, we further propose to tackle the imbalance issue by adopting the techniques of imbalanced learning. In particular, we employ the linear and nonlinear Biased Penalty Support Vector Machine and the Ensemble Methods, both of which have been proved to successfully handle the imbalance issue in the machine learning literatures. We finally evaluate our approach by conducting extensive empirical studies. Empirical results show that the proposed schema can achieve much better performance, in terms of balanced accuracy, than the state of the art. Besides the performance, our approaches can also compute very fast, which further supports their practical deployment.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Li15.pdf",
        "supp": "",
        "pdf_size": 371364,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17224428419122379089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Economics and Management School, Wuhan University, Wuhan, P.R. China 430072; Division of Accounting, Nanyang Business School, Nanyang Technological University, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Singapore 639798; Division of Accounting, Business School, National University of Singapore, Singapore 119245",
        "aff_domain": "whu.edu.cn;ntu.edu.sg;ntu.edu.sg;nus.edu.sg",
        "email": "whu.edu.cn;ntu.edu.sg;ntu.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Wuhan University;Nanyang Technological University;National University of Singapore",
        "aff_unique_dep": "Economics and Management School;Division of Accounting;Division of Accounting",
        "aff_unique_url": "http://www.whu.edu.cn;https://www.ntu.edu.sg;https://www.nus.edu.sg",
        "aff_unique_abbr": "WHU;NTU;NUS",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Wuhan;Singapore",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "a770202625",
        "title": "Expectation Propagation for Rectified Linear Poisson Regression",
        "site": "https://proceedings.mlr.press/v45/Ko15.html",
        "author": "Young-Jun Ko; Matthias W. Seeger",
        "abstract": "The Poisson likelihood with rectified linear function as non-linearity is a physically plausible model to discribe the stochastic arrival process of photons or other particles at a detector.  At low emission rates the discrete nature of this process leads to measurement noise that behaves very differently from additive white Gaussian noise. To address the intractable inference problem for such models, we present a novel efficient and robust Expectation Propagation algorithm entirely based on analytically tractable computations operating reliably in regimes where quadrature based implementations can fail. Full posterior inference therefore becomes an attractive alternative in areas generally dominated by methods of point estimation. Moreover, we discuss the rectified linear function in the context of other common non-linearities and identify situations where it can serve as a robust alternative.",
        "bibtex": "@InProceedings{pmlr-v45-Ko15,\n  title = \t {Expectation Propagation for Rectified Linear Poisson Regression},\n  author = \t {Ko, Young-Jun and Seeger, Matthias W.},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {253--268},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Ko15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Ko15.html},\n  abstract = \t {The Poisson likelihood with rectified linear function as non-linearity is a physically plausible model to discribe the stochastic arrival process of photons or other particles at a detector.  At low emission rates the discrete nature of this process leads to measurement noise that behaves very differently from additive white Gaussian noise. To address the intractable inference problem for such models, we present a novel efficient and robust Expectation Propagation algorithm entirely based on analytically tractable computations operating reliably in regimes where quadrature based implementations can fail. Full posterior inference therefore becomes an attractive alternative in areas generally dominated by methods of point estimation. Moreover, we discuss the rectified linear function in the context of other common non-linearities and identify situations where it can serve as a robust alternative.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Ko15.pdf",
        "supp": "",
        "pdf_size": 726275,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17143104127317084714&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Ecole Polytechnique Federale de Lausanne, Switzerland; ",
        "aff_domain": "epfl.ch;gmail.com",
        "email": "epfl.ch;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ecole Polytechnique Federale de Lausanne",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "f0100aae42",
        "title": "Geometry-Aware Principal Component Analysis for Symmetric Positive Definite Matrices",
        "site": "https://proceedings.mlr.press/v45/Horev15.html",
        "author": "Inbal Horev; Florian Yger; Masashi Sugiyama",
        "abstract": "Symmetric positive definite (SPD) matrices, e.g. covariance matrices, are ubiquitous in machine learning applications. However, because their size grows as n^2 (where n is the number of variables) their high-dimensionality is a crucial point when working with them. Thus, it is often useful to apply to them dimensionality reduction techniques. Principal component analysis (PCA) is a canonical tool for dimensionality reduction, which for vector data reduces the dimension of the input data while maximizing the preserved variance. Yet, the commonly used, naive extensions of PCA to matrices result in sub-optimal variance retention. Moreover, when applied to SPD matrices, they ignore the geometric structure of the space of SPD matrices, further degrading the performance. In this paper we develop a new Riemannian geometry based formulation of PCA for SPD matrices that i) preserves more data variance by appropriately extending PCA to matrix data, and ii) extends the standard definition from the Euclidean to the Riemannian geometries. We experimentally demonstrate the usefulness of our approach as pre-processing for EEG signals.",
        "bibtex": "@InProceedings{pmlr-v45-Horev15,\n  title = \t {Geometry-Aware Principal Component Analysis for Symmetric Positive Definite Matrices},\n  author = \t {Horev, Inbal and Yger, Florian and Sugiyama, Masashi},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {1--16},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Horev15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Horev15.html},\n  abstract = \t {Symmetric positive definite (SPD) matrices, e.g. covariance matrices, are ubiquitous in machine learning applications. However, because their size grows as n^2 (where n is the number of variables) their high-dimensionality is a crucial point when working with them. Thus, it is often useful to apply to them dimensionality reduction techniques. Principal component analysis (PCA) is a canonical tool for dimensionality reduction, which for vector data reduces the dimension of the input data while maximizing the preserved variance. Yet, the commonly used, naive extensions of PCA to matrices result in sub-optimal variance retention. Moreover, when applied to SPD matrices, they ignore the geometric structure of the space of SPD matrices, further degrading the performance. In this paper we develop a new Riemannian geometry based formulation of PCA for SPD matrices that i) preserves more data variance by appropriately extending PCA to matrix data, and ii) extends the standard definition from the Euclidean to the Riemannian geometries. We experimentally demonstrate the usefulness of our approach as pre-processing for EEG signals.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Horev15.pdf",
        "supp": "",
        "pdf_size": 1448568,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10047193982482132832&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 18,
        "aff": "Tokyo Institute of Technology, Graduate School of Information Science and Engineering, Department of Computer Science; Tokyo Institute of Technology, Graduate School of Information Science and Engineering, Department of Computer Science; University of Tokyo, Graduate School of Frontier Sciences, Department of Complexity Science and Engineering",
        "aff_domain": "ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "email": "ms.k.u-tokyo.ac.jp;ms.k.u-tokyo.ac.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Tokyo Institute of Technology;University of Tokyo",
        "aff_unique_dep": "Department of Computer Science;Department of Complexity Science and Engineering",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "Titech;UTokyo",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "258d098021",
        "title": "Improving Sybil Detection via Graph Pruning and Regularization Techniques",
        "site": "https://proceedings.mlr.press/v45/Zhang15b.html",
        "author": "Huanhuan Zhang; Jie Zhang; Carol Fung; Chang Xu",
        "abstract": "Due to their open and anonymous nature, online social networks are particularly vulnerable to Sybil attacks. In recent years, there has been a rising interest in leveraging social network topological structures to combat Sybil attacks. Unfortunately, due to their strong dependency on unrealistic assumptions, existing graph-based Sybil defense mechanisms suffer from high false detection rates. In this paper, we focus on enhancing those mechanisms by considering additional graph structural information underlying social networks. Our solutions are based on our novel understanding and interpretation of Sybil detection as the problem of partially labeled classification. Specifically, we first propose an effective graph pruning technique to enhance the robustness of existing Sybil defense mechanisms against target attacks, by utilizing the local structural similarity between neighboring nodes in a social network. Second, we design a domain-specific graph regularization method to further improve the performance of those mechanisms by exploiting the relational property of the social network. Experimental results on four popular online social network datasets demonstrate that our proposed techniques can significantly improve the detection accuracy over the original Sybil defense mechanisms.",
        "bibtex": "@InProceedings{pmlr-v45-Zhang15b,\n  title = \t {Improving Sybil Detection via Graph Pruning and Regularization Techniques},\n  author = \t {Zhang, Huanhuan and Zhang, Jie and Fung, Carol and Xu, Chang},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {189--204},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Zhang15b.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Zhang15b.html},\n  abstract = \t {Due to their open and anonymous nature, online social networks are particularly vulnerable to Sybil attacks. In recent years, there has been a rising interest in leveraging social network topological structures to combat Sybil attacks. Unfortunately, due to their strong dependency on unrealistic assumptions, existing graph-based Sybil defense mechanisms suffer from high false detection rates. In this paper, we focus on enhancing those mechanisms by considering additional graph structural information underlying social networks. Our solutions are based on our novel understanding and interpretation of Sybil detection as the problem of partially labeled classification. Specifically, we first propose an effective graph pruning technique to enhance the robustness of existing Sybil defense mechanisms against target attacks, by utilizing the local structural similarity between neighboring nodes in a social network. Second, we design a domain-specific graph regularization method to further improve the performance of those mechanisms by exploiting the relational property of the social network. Experimental results on four popular online social network datasets demonstrate that our proposed techniques can significantly improve the detection accuracy over the original Sybil defense mechanisms.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Zhang15b.pdf",
        "supp": "",
        "pdf_size": 491021,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2091601568891788507&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Engineering, Nanyang Technological University, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Singapore 639798; Department of Computer Science, Virginia Commonwealth University, Richmond, VA; School of Computer Engineering, Nanyang Technological University, Singapore 639798",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;vcu.edu;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;vcu.edu;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Nanyang Technological University;Virginia Commonwealth University",
        "aff_unique_dep": "School of Computer Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.vcu.edu",
        "aff_unique_abbr": "NTU;VCU",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Singapore;Richmond",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "aabf81a876",
        "title": "Integration of Single-view Graphs with Diffusion of Tensor Product Graphs for Multi-view Spectral Clustering",
        "site": "https://proceedings.mlr.press/v45/Shu15.html",
        "author": "Le Shu; Longin Jan Latecki",
        "abstract": "Multi-view clustering takes diversity of multiple views (representations) into consideration. Multiple views may be obtained from various sources or different feature subsets and often provide complementary information to each other. In this paper, we propose a novel graph-based approach to integrate multiple representations to improve clustering performance. While original graphs have been widely used in many existing multi-view clustering approaches, the key idea of our approach is to integrate multiple views by exploring higher order information. In particular, given graphs constructed separately from single view data, we build cross-view tensor product graphs (TPGs), each of which is a Kronecker product of a pair of single-view graphs. Since each cross-view TPG captures higher order relationships of data under two different views, it is no surprise that we obtain more reliable similarities.  We linearly combine multiple cross-view TPGs to integrate higher order information. Efficient graph diffusion process on the fusion TPG helps to reveal the underlying cluster structure and boosts the clustering performance. Empirical study shows that the proposed approach outperforms state-of-the-art methods on benchmark datasets.",
        "bibtex": "@InProceedings{pmlr-v45-Shu15,\n  title = \t {Integration of Single-view Graphs with Diffusion of Tensor Product Graphs for Multi-view Spectral Clustering},\n  author = \t {Shu, Le and Latecki, Longin Jan},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {362--377},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Shu15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Shu15.html},\n  abstract = \t {Multi-view clustering takes diversity of multiple views (representations) into consideration. Multiple views may be obtained from various sources or different feature subsets and often provide complementary information to each other. In this paper, we propose a novel graph-based approach to integrate multiple representations to improve clustering performance. While original graphs have been widely used in many existing multi-view clustering approaches, the key idea of our approach is to integrate multiple views by exploring higher order information. In particular, given graphs constructed separately from single view data, we build cross-view tensor product graphs (TPGs), each of which is a Kronecker product of a pair of single-view graphs. Since each cross-view TPG captures higher order relationships of data under two different views, it is no surprise that we obtain more reliable similarities.  We linearly combine multiple cross-view TPGs to integrate higher order information. Efficient graph diffusion process on the fusion TPG helps to reveal the underlying cluster structure and boosts the clustering performance. Empirical study shows that the proposed approach outperforms state-of-the-art methods on benchmark datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Shu15.pdf",
        "supp": "",
        "pdf_size": 837539,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17390080887714259885&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer and Information Sciences, Temple University, 1925 N. 12 St. Philadelphia, PA, USA; Computer and Information Sciences, Temple University, 1925 N. 12 St. Philadelphia, PA, USA",
        "aff_domain": "temple.edu;temple.edu",
        "email": "temple.edu;temple.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Temple University",
        "aff_unique_dep": "Computer and Information Sciences",
        "aff_unique_url": "https://www.temple.edu",
        "aff_unique_abbr": "Temple",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d058f908f7",
        "title": "Largest Source Subset Selection for Instance Transfer",
        "site": "https://proceedings.mlr.press/v45/Zhou15.html",
        "author": "Shuang Zhou; Gijs Schoenmakers; Evgueni Smirnov; Ralf Peeters; Kurt Driessens; Siqi Chen",
        "abstract": "Instance-transfer learning has emerged as a promising  learning framework to boost performance of prediction models on newly-arrived tasks. The success of the framework depends on the relevance of the source data to the target data. This paper proposes a new approach to source data selection for instance-transfer learning.  The approach is capable of selecting the largest subset S^* of the source data which relevance to the target data is statistically guaranteed to be the highest among any superset of S^*. The approach is formally described and theoretically justified.  Experimental results on real-world data sets demonstrate that the approach outperforms existing instance selection methods.",
        "bibtex": "@InProceedings{pmlr-v45-Zhou15,\n  title = \t {Largest Source Subset Selection for Instance Transfer},\n  author = \t {Zhou, Shuang and Schoenmakers, Gijs and Smirnov, Evgueni and Peeters, Ralf and Driessens, Kurt and Chen, Siqi},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {423--438},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Zhou15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Zhou15.html},\n  abstract = \t {Instance-transfer learning has emerged as a promising  learning framework to boost performance of prediction models on newly-arrived tasks. The success of the framework depends on the relevance of the source data to the target data. This paper proposes a new approach to source data selection for instance-transfer learning.  The approach is capable of selecting the largest subset S^* of the source data which relevance to the target data is statistically guaranteed to be the highest among any superset of S^*. The approach is formally described and theoretically justified.  Experimental results on real-world data sets demonstrate that the approach outperforms existing instance selection methods. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Zhou15.pdf",
        "supp": "",
        "pdf_size": 1958670,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7315288786216585478&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Knowledge Engineering, Maastricht University, P.O. Box 616, 6200 MD, Maastricht, The Netherlands; Department of Knowledge Engineering, Maastricht University, P.O. Box 616, 6200 MD, Maastricht, The Netherlands; Department of Knowledge Engineering, Maastricht University, P.O. Box 616, 6200 MD, Maastricht, The Netherlands; Department of Knowledge Engineering, Maastricht University, P.O. Box 616, 6200 MD, Maastricht, The Netherlands; Department of Knowledge Engineering, Maastricht University, P.O. Box 616, 6200 MD, Maastricht, The Netherlands; School of Computer and Information Science, Southwest University, Chongqing, China",
        "aff_domain": "maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;gmail.com",
        "email": "maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;maastrichtuniversity.nl;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Maastricht University;Southwest University",
        "aff_unique_dep": "Department of Knowledge Engineering;School of Computer and Information Science",
        "aff_unique_url": "https://www.maastrichtuniversity.nl;http://www.swu.edu.cn",
        "aff_unique_abbr": "MU;",
        "aff_campus_unique_index": "0;0;0;0;0;1",
        "aff_campus_unique": "Maastricht;Chongqing",
        "aff_country_unique_index": "0;0;0;0;0;1",
        "aff_country_unique": "Netherlands;China"
    },
    {
        "id": "59826dae41",
        "title": "Maximum Margin Partial Label Learning",
        "site": "https://proceedings.mlr.press/v45/Yu15.html",
        "author": "Fei Yu; Min-Ling Zhang",
        "abstract": "Partial label learning deals with the problem that each training example is associated with a set of \\emphcandidate labels, and only one among the set is the ground-truth label. The basic strategy to learn from partial label examples is disambiguation, i.e. by trying to recover the ground-truth labeling information from the candidate label set. As one of the major machine learning techniques, maximum margin criterion has been employed to solve the partial label learning problem. Therein, disambiguation is performed by optimizing the margin between the maximum modeling output from candidate labels and that from non-candidate labels. However, in this formulation the margin between the ground-truth label and other candidate labels is not differentiated. In this paper, a new maximum margin formulation for partial label learning is proposed which aims to directly maximize the margin between the ground-truth label and all other labels. Specifically, an alternating optimization procedure is utilized to coordinate \\emphground-truth label identification and \\emphmargin maximization. Extensive experiments show that the derived partial label learning approach achieves competitive performance against other state-of-the-art comparing approaches.",
        "bibtex": "@InProceedings{pmlr-v45-Yu15,\n  title = \t {Maximum Margin Partial Label Learning},\n  author = \t {Yu, Fei and Zhang, Min-Ling},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {96--111},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Yu15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Yu15.html},\n  abstract = \t {Partial label learning deals with the problem that each training example is associated with a set of \\emphcandidate labels, and only one among the set is the ground-truth label. The basic strategy to learn from partial label examples is disambiguation, i.e. by trying to recover the ground-truth labeling information from the candidate label set. As one of the major machine learning techniques, maximum margin criterion has been employed to solve the partial label learning problem. Therein, disambiguation is performed by optimizing the margin between the maximum modeling output from candidate labels and that from non-candidate labels. However, in this formulation the margin between the ground-truth label and other candidate labels is not differentiated. In this paper, a new maximum margin formulation for partial label learning is proposed which aims to directly maximize the margin between the ground-truth label and all other labels. Specifically, an alternating optimization procedure is utilized to coordinate \\emphground-truth label identification and \\emphmargin maximization. Extensive experiments show that the derived partial label learning approach achieves competitive performance against other state-of-the-art comparing approaches. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Yu15.pdf",
        "supp": "",
        "pdf_size": 398852,
        "gs_citation": 201,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17255215160646079891&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "School of Computer Science and Engineering, Southeast University, Nanjing 210096, China + Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China; School of Computer Science and Engineering, Southeast University, Nanjing 210096, China + Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China",
        "aff_domain": "SEU.EDU.CN;SEU.EDU.CN",
        "email": "SEU.EDU.CN;SEU.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nanjing;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "58d8b4a8fc",
        "title": "Non-asymptotic Analysis of Compressive Fisher Discriminants in terms of the Effective Dimension",
        "site": "https://proceedings.mlr.press/v45/Kaban15a.html",
        "author": "Ata Kaban",
        "abstract": "We provide a non-asymptotic analysis of the generalisation error of compressive Fisher linear discriminant (FLD) classification that is dimension free under mild assumptions. Our analysis includes the effects that random projection has on classification performance under covariance model misspecification,  as well as various good and bad effects of random projections that contribute to the overall performance of compressive FLD. We also give an asymptotic bound as a corollary of our finite sample result. An important ingredient of our analysis is to develop new dimension-free bounds on the largest and smallest eigenvalue of the compressive covariance,  which may be of independent interest.",
        "bibtex": "@InProceedings{pmlr-v45-Kaban15a,\n  title = \t {Non-asymptotic Analysis of Compressive Fisher Discriminants in terms of the Effective Dimension},\n  author = \t {Kaban, Ata},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {17--32},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Kaban15a.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Kaban15a.html},\n  abstract = \t {We provide a non-asymptotic analysis of the generalisation error of compressive Fisher linear discriminant (FLD) classification that is dimension free under mild assumptions. Our analysis includes the effects that random projection has on classification performance under covariance model misspecification,  as well as various good and bad effects of random projections that contribute to the overall performance of compressive FLD. We also give an asymptotic bound as a corollary of our finite sample result. An important ingredient of our analysis is to develop new dimension-free bounds on the largest and smallest eigenvalue of the compressive covariance,  which may be of independent interest.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Kaban15a.pdf",
        "supp": "",
        "pdf_size": 587440,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1201946541937662067&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, University of Birmingham, Edgbaston, B15 2TT, Birmingham, UK",
        "aff_domain": "cs.bham.ac.uk",
        "email": "cs.bham.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Birmingham",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.birmingham.ac.uk",
        "aff_unique_abbr": "UoB",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Birmingham",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "606f13968a",
        "title": "One-Pass Multi-View Learning",
        "site": "https://proceedings.mlr.press/v45/Zhu15.html",
        "author": "Yue Zhu; Wei Gao; Zhi-Hua Zhou",
        "abstract": "Multi-view learning has been an important learning paradigm where data come from multiple channels or appear in multiple modalities. Many approaches have been developed in this field, and have achieved better performance than single-view ones. Those approaches, however, always work on small-size datasets with low dimensionality, owing to their high computational cost. In recent years, it has been witnessed that many applications involve large-scale multi-view data, e.g., hundreds of hours of video (including visual, audio and text views) is uploaded to YouTube every minute, bringing a big challenge to previous multi-view algorithms. This work  concentrates on the large-scale multi-view learning for classification and proposes the One-Pass Multi-View (OPMV) framework which goes through the training data only once without storing the entire training examples. This approach jointly optimizes the composite objective functions with consistency linear constraints for different views. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.",
        "bibtex": "@InProceedings{pmlr-v45-Zhu15,\n  title = \t {One-Pass Multi-View Learning},\n  author = \t {Zhu, Yue and Gao, Wei and Zhou, Zhi-Hua},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {407--422},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Zhu15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Zhu15.html},\n  abstract = \t {Multi-view learning has been an important learning paradigm where data come from multiple channels or appear in multiple modalities. Many approaches have been developed in this field, and have achieved better performance than single-view ones. Those approaches, however, always work on small-size datasets with low dimensionality, owing to their high computational cost. In recent years, it has been witnessed that many applications involve large-scale multi-view data, e.g., hundreds of hours of video (including visual, audio and text views) is uploaded to YouTube every minute, bringing a big challenge to previous multi-view algorithms. This work  concentrates on the large-scale multi-view learning for classification and proposes the One-Pass Multi-View (OPMV) framework which goes through the training data only once without storing the entire training examples. This approach jointly optimizes the composite objective functions with consistency linear constraints for different views. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.   }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Zhu15.pdf",
        "supp": "",
        "pdf_size": 1375837,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12048962361457437818&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization; National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization; National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization",
        "aff_domain": "lamda.nju.edu.cn;lamda.nju.edu.cn;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;lamda.nju.edu.cn;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Nanjing University;Collaborative Innovation Center of Novel Software Technology and Industrialization",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;",
        "aff_unique_url": "http://www.nju.edu.cn;",
        "aff_unique_abbr": "Nanjing University;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "5cf0682208",
        "title": "Preface",
        "site": "https://proceedings.mlr.press/v45/preface.html",
        "author": "Geoffrey Holmes; Tie-Yan Liu",
        "abstract": "Preface to ACML 2015.",
        "bibtex": "@InProceedings{pmlr-v45-preface,\n  title = \t {Preface},\n  author = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {i--xx},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/preface.pdf},\n  url = \t {https://proceedings.mlr.press/v45/preface.html},\n  abstract = \t {Preface to ACML 2015.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/preface.pdf",
        "supp": "",
        "pdf_size": 153112,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141043608347785016&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9a3e91a7f5",
        "title": "Preference Relation-based Markov Random Fields for Recommender Systems",
        "site": "https://proceedings.mlr.press/v45/Liu15.html",
        "author": "Shaowu Liu; Gang Li; Truyen Tran; Yuan Jiang",
        "abstract": "A \\emphpreference relation-based Top-N recommendation approach,  \\emphPrefMRF, is proposed to capture both the second-order and  the higher-order interactions among users and items.  Traditionally Top-N recommendation was achieved by predicting  the item ratings first, and then inferring the item rankings,  based on the assumption of availability of \\emphexplicit feedbacks  such as ratings, and the assumption that optimizing the ratings  is equivalent to optimizing the item rankings.  Nevertheless, both assumptions are not always true in real  world applications. The proposed \\emphPrefMRF approach drops these  assumptions by explicitly exploiting the preference relations,  a more practical user feedback.  Comparing to related work, the proposed \\emphPrefMRF approach has  the unique property of modeling both the second-order and the  higher-order interactions among users and items. To the best of our knowledge, this is the first time both types of interactions have been captured in \\emphpreference relation-based method. Experiment results on public datasets demonstrate that both  types of interactions have been properly captured,  and significantly improved Top-N recommendation performance  has been achieved.",
        "bibtex": "@InProceedings{pmlr-v45-Liu15,\n  title = \t {Preference Relation-based Markov Random Fields for Recommender Systems},\n  author = \t {Liu, Shaowu and Li, Gang and Tran, Truyen and Jiang, Yuan},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {157--172},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Liu15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Liu15.html},\n  abstract = \t {A \\emphpreference relation-based Top-N recommendation approach,  \\emphPrefMRF, is proposed to capture both the second-order and  the higher-order interactions among users and items.  Traditionally Top-N recommendation was achieved by predicting  the item ratings first, and then inferring the item rankings,  based on the assumption of availability of \\emphexplicit feedbacks  such as ratings, and the assumption that optimizing the ratings  is equivalent to optimizing the item rankings.  Nevertheless, both assumptions are not always true in real  world applications. The proposed \\emphPrefMRF approach drops these  assumptions by explicitly exploiting the preference relations,  a more practical user feedback.  Comparing to related work, the proposed \\emphPrefMRF approach has  the unique property of modeling both the second-order and the  higher-order interactions among users and items. To the best of our knowledge, this is the first time both types of interactions have been captured in \\emphpreference relation-based method. Experiment results on public datasets demonstrate that both  types of interactions have been properly captured,  and significantly improved Top-N recommendation performance  has been achieved.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Liu15.pdf",
        "supp": "",
        "pdf_size": 537860,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15561320875568596639&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "School of Information Technology, Deakin University, Geelong, Australia; School of Information Technology, Deakin University, Geelong, Australia; Pattern Recognition and Data Analytics, Deakin University, Geelong, Australia; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China",
        "aff_domain": "deakin.edu.au;deakin.edu.au;deakin.edu.au;nju.edu.cn",
        "email": "deakin.edu.au;deakin.edu.au;deakin.edu.au;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Deakin University;Nanjing University",
        "aff_unique_dep": "School of Information Technology;National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "https://www.deakin.edu.au;http://www.nju.edu.cn",
        "aff_unique_abbr": "Deakin;Nanjing U",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Geelong;Nanjing",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Australia;China"
    },
    {
        "id": "d508ae7810",
        "title": "Proximal Average Approximated Incremental Gradient Method for Composite Penalty Regularized Empirical Risk Minimization",
        "site": "https://proceedings.mlr.press/v45/Cheung15.html",
        "author": "Yiu-ming Cheung; Jian Lou",
        "abstract": "Proximal average (PA) is an approximation technique proposed recently to handle nonsmooth composite regularizer in empirical risk minimization problem. For nonsmooth composite regularizer, it is often difficult to directly derive the corresponding proximal update when solving with popular proximal update. While traditional approaches resort to complex splitting methods like ADMM, proximal average provides an alternative, featuring the tractability of implementation and theoretical analysis. Nevertheless, compared to SDCA-ADMM and SAG-ADMM which are examples of ADMM-based methods achieving faster convergence rate and low per-iteration complexity, existing PA-based approaches either converge slowly (e.g. PA-ASGD) or suffer from high per-iteration cost (e.g. PA-APG). In this paper, we therefore propose a new PA-based algorithm called PA-SAGA, which is optimal in both convergence rate and per-iteration cost, by incorporating into incremental gradient-based framework.",
        "bibtex": "@InProceedings{pmlr-v45-Cheung15,\n  title = \t {Proximal Average Approximated Incremental Gradient Method for Composite Penalty Regularized Empirical Risk Minimization},\n  author = \t {Cheung, Yiu-ming and Lou, Jian},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {205--220},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Cheung15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Cheung15.html},\n  abstract = \t {Proximal average (PA) is an approximation technique proposed recently to handle nonsmooth composite regularizer in empirical risk minimization problem. For nonsmooth composite regularizer, it is often difficult to directly derive the corresponding proximal update when solving with popular proximal update. While traditional approaches resort to complex splitting methods like ADMM, proximal average provides an alternative, featuring the tractability of implementation and theoretical analysis. Nevertheless, compared to SDCA-ADMM and SAG-ADMM which are examples of ADMM-based methods achieving faster convergence rate and low per-iteration complexity, existing PA-based approaches either converge slowly (e.g. PA-ASGD) or suffer from high per-iteration cost (e.g. PA-APG). In this paper, we therefore propose a new PA-based algorithm called PA-SAGA, which is optimal in both convergence rate and per-iteration cost, by incorporating into incremental gradient-based framework. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Cheung15.pdf",
        "supp": "",
        "pdf_size": 383383,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:KMDbbD_1mdgJ:scholar.google.com/&scioq=Proximal+Average+Approximated+Incremental+Gradient+Method+for+Composite+Penalty+Regularized+Empirical+Risk+Minimization&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China",
        "aff_domain": "comp.hkbu.edu.hk;comp.hkbu.edu.hk",
        "email": "comp.hkbu.edu.hk;comp.hkbu.edu.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong Baptist University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.hkbu.edu.hk",
        "aff_unique_abbr": "HKBU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "f8052dd491",
        "title": "Regularized Policy Gradients: Direct Variance Reduction in Policy Gradient Estimation",
        "site": "https://proceedings.mlr.press/v45/Zhao15b.html",
        "author": "Tingting Zhao; Gang Niu; Ning Xie; Jucheng Yang; Masashi Sugiyama",
        "abstract": "Policy gradient algorithms are widely used in reinforcement learning problems with continuous action spaces, which update the policy parameters along the steepest direction of the expected return. However, large variance of policy gradient estimation often causes instability of policy update. In this paper, we propose to suppress the variance of gradient estimation by directly employing the variance of policy gradients as a regularizer. Through experiments, we demonstrate that the proposed variance-regularization technique combined with parameter-based exploration and baseline subtraction provides more reliable policy updates than non-regularized counterparts.",
        "bibtex": "@InProceedings{pmlr-v45-Zhao15b,\n  title = \t {Regularized Policy Gradients: Direct Variance Reduction in Policy Gradient Estimation},\n  author = \t {Zhao, Tingting and Niu, Gang and Xie, Ning and Yang, Jucheng and Sugiyama, Masashi},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {333--348},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Zhao15b.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Zhao15b.html},\n  abstract = \t {Policy gradient algorithms are widely used in reinforcement learning problems with continuous action spaces, which update the policy parameters along the steepest direction of the expected return. However, large variance of policy gradient estimation often causes instability of policy update. In this paper, we propose to suppress the variance of gradient estimation by directly employing the variance of policy gradients as a regularizer. Through experiments, we demonstrate that the proposed variance-regularization technique combined with parameter-based exploration and baseline subtraction provides more reliable policy updates than non-regularized counterparts. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Zhao15b.pdf",
        "supp": "",
        "pdf_size": 779020,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7657498067755301797&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Tianjin University of Science and Technology, China; The University of Tokyo, Japan; Tongji University, China; Tianjin University of Science and Technology, China; The University of Tokyo, Japan",
        "aff_domain": "tust.edu.cn;ms.k.u-tokyo.ac.jp;tongji.edu.cn;tust.edu.cn;k.u-tokyo.ac.jp",
        "email": "tust.edu.cn;ms.k.u-tokyo.ac.jp;tongji.edu.cn;tust.edu.cn;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "Tianjin University of Science and Technology;University of Tokyo;Tongji University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.tjust.edu.cn;https://www.u-tokyo.ac.jp;https://www.tongji.edu.cn",
        "aff_unique_abbr": "TUST;UTokyo;Tongji",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "China;Japan"
    },
    {
        "id": "203cd828f3",
        "title": "Robust Multivariate Regression with Grossly Corrupted Observations and Its Application to Personality Prediction",
        "site": "https://proceedings.mlr.press/v45/Zhang15a.html",
        "author": "Xiaowei Zhang; Li Cheng; Tingshao Zhu",
        "abstract": "We consider the problem of multivariate linear regression with a small fraction of the responses being missing and grossly corrupted, where the magnitudes and locations of such occurrences are not known in priori. This is addressed in our approach by explicitly taking into account the error source and its sparseness nature. Moreover, our approach allows each regression task to possess its distinct noise level. We also propose a new algorithm that is theoretically shown to always converge to the optimal solution of its induced non-smooth optimization problem. Experiments on controlled simulations suggest the competitiveness of our algorithm comparing to existing multivariate regression models. In particular, we apply our model to predict the \\textitBig-Five personality from user behaviors at Social Network Sites (SNSs) and microblogs, an important yet difficult problem in psychology, where empirical results demonstrate its superior performance with respect to related learning methods.",
        "bibtex": "@InProceedings{pmlr-v45-Zhang15a,\n  title = \t {Robust Multivariate Regression with Grossly Corrupted Observations and Its Application to Personality Prediction},\n  author = \t {Zhang, Xiaowei and Cheng, Li and Zhu, Tingshao},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {112--126},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Zhang15a.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Zhang15a.html},\n  abstract = \t {We consider the problem of multivariate linear regression with a small fraction of the responses being missing and grossly corrupted, where the magnitudes and locations of such occurrences are not known in priori. This is addressed in our approach by explicitly taking into account the error source and its sparseness nature. Moreover, our approach allows each regression task to possess its distinct noise level. We also propose a new algorithm that is theoretically shown to always converge to the optimal solution of its induced non-smooth optimization problem. Experiments on controlled simulations suggest the competitiveness of our algorithm comparing to existing multivariate regression models. In particular, we apply our model to predict the \\textitBig-Five personality from user behaviors at Social Network Sites (SNSs) and microblogs, an important yet difficult problem in psychology, where empirical results demonstrate its superior performance with respect to related learning methods. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Zhang15a.pdf",
        "supp": "",
        "pdf_size": 403607,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17939300958674200380&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Bioinformatics Institute, Agency for Science, Technology and Research (A*STAR), Singapore; Bioinformatics Institute, Agency for Science, Technology and Research (A*STAR), Singapore; Institute of Psychology, Chinese Academy of Sciences, China",
        "aff_domain": "bii.a-star.edu.sg;bii.a-star.edu.sg;psych.ac.cn",
        "email": "bii.a-star.edu.sg;bii.a-star.edu.sg;psych.ac.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Agency for Science, Technology and Research;Chinese Academy of Sciences",
        "aff_unique_dep": "Bioinformatics Institute;Institute of Psychology",
        "aff_unique_url": "https://www.a-star.edu.sg;http://www.cas.cn",
        "aff_unique_abbr": "A*STAR;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "ca3744e3e6",
        "title": "Similarity-based Contrastive Divergence Methods for Energy-based Deep Learning Models",
        "site": "https://proceedings.mlr.press/v45/Sankar15.html",
        "author": "Adepu Ravi Sankar; Vineeth N Balasubramanian",
        "abstract": "Energy-based deep learning models like Restricted Boltzmann Machines are increasingly used for real-world applications. However, all these models inherently depend on the Contrastive Divergence (CD) method for training and maximization of log likelihood of generating the given data distribution. CD, which internally uses Gibbs sampling, often does not perform well due to issues such as biased samples, poor mixing of Markov chains and high-mass probability modes. Variants of CD such as PCD, Fast PCD and Tempered MCMC have been proposed to address this issue. In this work, we propose a new approach to CD-based methods, called Diss-CD, which uses dissimilar data to allow the Markov chain to explore new modes in the probability space. This method can be used with all variants of CD (or PCD), and across all energy-based deep learning models. Our experiments on using this approach on standard datasets including MNIST, Caltech-101 Silhouette and Synthetic Transformations, demonstrate the promise of this approach, showing fast convergence of error in learning and also a better approximation of log likelihood of the data.",
        "bibtex": "@InProceedings{pmlr-v45-Sankar15,\n  title = \t {Similarity-based Contrastive Divergence Methods for Energy-based Deep Learning Models},\n  author = \t {Sankar, Adepu Ravi and Balasubramanian, Vineeth N},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {391--406},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Sankar15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Sankar15.html},\n  abstract = \t {Energy-based deep learning models like Restricted Boltzmann Machines are increasingly used for real-world applications. However, all these models inherently depend on the Contrastive Divergence (CD) method for training and maximization of log likelihood of generating the given data distribution. CD, which internally uses Gibbs sampling, often does not perform well due to issues such as biased samples, poor mixing of Markov chains and high-mass probability modes. Variants of CD such as PCD, Fast PCD and Tempered MCMC have been proposed to address this issue. In this work, we propose a new approach to CD-based methods, called Diss-CD, which uses dissimilar data to allow the Markov chain to explore new modes in the probability space. This method can be used with all variants of CD (or PCD), and across all energy-based deep learning models. Our experiments on using this approach on standard datasets including MNIST, Caltech-101 Silhouette and Synthetic Transformations, demonstrate the promise of this approach, showing fast convergence of error in learning and also a better approximation of log likelihood of the data. }\n}",
        "pdf": "http://proceedings.mlr.press/v45/Sankar15.pdf",
        "supp": "",
        "pdf_size": 453711,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14365205134967115178&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Indian Institute of Technology Hyderabad, Sangareddy, Telangana, India 502285; Indian Institute of Technology Hyderabad, Sangareddy, Telangana, India 502285",
        "aff_domain": "iith.ac.in;iith.ac.in",
        "email": "iith.ac.in;iith.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Hyderabad",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iith.ac.in",
        "aff_unique_abbr": "IIT Hyderabad",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Sangareddy",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "1a0a7eddb6",
        "title": "Statistical Unfolded Logic Learning",
        "site": "https://proceedings.mlr.press/v45/Dai15.html",
        "author": "Wang-Zhou Dai; Zhi-Hua Zhou",
        "abstract": "During the past decade, Statistical Relational Learning (SRL) and Probabilistic Inductive Logic Programming (PILP), owing to their strength in capturing structure information, have attracted much attention for learning relational models such as weighted logic rules. Typically, a generative model is assumed for the structured joint distribution, and the learning process is accomplished in an enormous relational space. In this paper, we propose a new framework, i.e., Statistical Unfolded Logic (SUL) learning. In contrast to learning rules in the relational space directly, SUL propositionalizes the structure information into an attribute-value data set, and thus, statistical discriminative learning which is much more efficient than generative relational learning can be executed. In addition to achieving better generalization performance, SUL is able to conduct predicate invention that is hard to be realized by traditional SRL and PILP approaches. Experiments on real tasks show that our proposed approach is superior to state-of-the-art weighted rules learning approaches.",
        "bibtex": "@InProceedings{pmlr-v45-Dai15,\n  title = \t {Statistical Unfolded Logic Learning},\n  author = \t {Dai, Wang-Zhou and Zhou, Zhi-Hua},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {349--361},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Dai15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Dai15.html},\n  abstract = \t {During the past decade, Statistical Relational Learning (SRL) and Probabilistic Inductive Logic Programming (PILP), owing to their strength in capturing structure information, have attracted much attention for learning relational models such as weighted logic rules. Typically, a generative model is assumed for the structured joint distribution, and the learning process is accomplished in an enormous relational space. In this paper, we propose a new framework, i.e., Statistical Unfolded Logic (SUL) learning. In contrast to learning rules in the relational space directly, SUL propositionalizes the structure information into an attribute-value data set, and thus, statistical discriminative learning which is much more efficient than generative relational learning can be executed. In addition to achieving better generalization performance, SUL is able to conduct predicate invention that is hard to be realized by traditional SRL and PILP approaches. Experiments on real tasks show that our proposed approach is superior to state-of-the-art weighted rules learning approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Dai15.pdf",
        "supp": "",
        "pdf_size": 363959,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11238943248813556937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210046, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210046, China",
        "aff_domain": "lamda.nju.edu.cn;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "7fd3a98924",
        "title": "Streaming Variational Inference for Dirichlet Process Mixtures",
        "site": "https://proceedings.mlr.press/v45/Huynh15.html",
        "author": "Viet Huynh; Dinh Phung; Svetha Venkatesh",
        "abstract": "Bayesian nonparametric models are theoretically suitable to learn streaming data due to their complexity relaxation to the volume of observed data. However, most of the existing variational inference algorithms are not applicable to streaming applications since they require truncation on variational distributions. In this paper, we present two truncation-free variational algorithms, one for mix-membership inference called TFVB (truncation-free variational Bayes), and the other for hard clustering inference called TFME (truncation-free maximization expectation). With these algorithms, we further developed a streaming learning framework for the popular Dirichlet process mixture (DPM) models. Our experiments demonstrate the usefulness of our framework in both synthetic and real-world data.",
        "bibtex": "@InProceedings{pmlr-v45-Huynh15,\n  title = \t {Streaming Variational Inference for Dirichlet Process Mixtures},\n  author = \t {Huynh, Viet and Phung, Dinh and Venkatesh, Svetha},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {237--252},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Huynh15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Huynh15.html},\n  abstract = \t {Bayesian nonparametric models are theoretically suitable to learn streaming data due to their complexity relaxation to the volume of observed data. However, most of the existing variational inference algorithms are not applicable to streaming applications since they require truncation on variational distributions. In this paper, we present two truncation-free variational algorithms, one for mix-membership inference called TFVB (truncation-free variational Bayes), and the other for hard clustering inference called TFME (truncation-free maximization expectation). With these algorithms, we further developed a streaming learning framework for the popular Dirichlet process mixture (DPM) models. Our experiments demonstrate the usefulness of our framework in both synthetic and real-world data.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Huynh15.pdf",
        "supp": "",
        "pdf_size": 932922,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4226345665298697520&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Pattern Recognition and Data Analytics Centre, Deakin University, Australia; Pattern Recognition and Data Analytics Centre, Deakin University, Australia; Pattern Recognition and Data Analytics Centre, Deakin University, Australia",
        "aff_domain": "deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "email": "deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Deakin University",
        "aff_unique_dep": "Pattern Recognition and Data Analytics Centre",
        "aff_unique_url": "https://www.deakin.edu.au",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "8a7f0cf705",
        "title": "Sufficient Dimension Reduction via Direct Estimation of the Gradients of Logarithmic Conditional Densities",
        "site": "https://proceedings.mlr.press/v45/Sasaki15.html",
        "author": "Hiroaki Sasaki; Voot Tangkaratt; Masashi Sugiyama",
        "abstract": "Sufficient dimension reduction (SDR) is a framework of supervised linear dimension reduction, and is aimed at finding a low-dimensional orthogonal projection matrix for input data such that the projected input data retains maximal information on output data. A computationally efficient approach employs gradient estimates of the conditional density of the output given input data to find an appropriate projection matrix. However, since the gradients of the conditional densities are typically estimated by a local linear smoother, it does not perform well when the input dimensionality is high. In this paper, we propose a novel estimator of the gradients of logarithmic conditional densities called the \\emphleast-squares logarithmic conditional density gradients (LSLCG), which fits a gradient model \\emphdirectly to the true gradient without conditional density estimation under the squared loss. Thanks to the simple least-squares formulation, LSLCG gives a closed-form solution that can be computed efficiently. In addition, all the parameters can be automatically determined by cross-validation. Through experiments on a large variety of artificial and benchmark datasets, we demonstrate that the SDR method based on LSLCG outperforms existing SDR methods both in estimation accuracy and computational efficiency.",
        "bibtex": "@InProceedings{pmlr-v45-Sasaki15,\n  title = \t {Sufficient Dimension Reduction via Direct Estimation of the Gradients of Logarithmic Conditional Densities},\n  author = \t {Sasaki, Hiroaki and Tangkaratt, Voot and Sugiyama, Masashi},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {33--48},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Sasaki15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Sasaki15.html},\n  abstract = \t {Sufficient dimension reduction (SDR) is a framework of supervised linear dimension reduction, and is aimed at finding a low-dimensional orthogonal projection matrix for input data such that the projected input data retains maximal information on output data. A computationally efficient approach employs gradient estimates of the conditional density of the output given input data to find an appropriate projection matrix. However, since the gradients of the conditional densities are typically estimated by a local linear smoother, it does not perform well when the input dimensionality is high. In this paper, we propose a novel estimator of the gradients of logarithmic conditional densities called the \\emphleast-squares logarithmic conditional density gradients (LSLCG), which fits a gradient model \\emphdirectly to the true gradient without conditional density estimation under the squared loss. Thanks to the simple least-squares formulation, LSLCG gives a closed-form solution that can be computed efficiently. In addition, all the parameters can be automatically determined by cross-validation. Through experiments on a large variety of artificial and benchmark datasets, we demonstrate that the SDR method based on LSLCG outperforms existing SDR methods both in estimation accuracy and computational efficiency.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Sasaki15.pdf",
        "supp": "",
        "pdf_size": 406368,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18318689028235529876&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Graduate School of Frontier Sciences, The University of Tokyo, Chiba, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Frontier Sciences, The University of Tokyo, Chiba, Japan",
        "aff_domain": "MS.K.U-TOKYO.AC.JP;MS.K.U-TOKYO.AC.JP;K.U-TOKYO.AC.JP",
        "email": "MS.K.U-TOKYO.AC.JP;MS.K.U-TOKYO.AC.JP;K.U-TOKYO.AC.JP",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "Graduate School of Frontier Sciences",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Chiba;Tokyo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "8a8617de01",
        "title": "Surrogate regret bounds for generalized classification performance metrics",
        "site": "https://proceedings.mlr.press/v45/Kotlowski15.html",
        "author": "Wojciech Kotlowski; Krzysztof Dembczy\u0144ski",
        "abstract": "We consider optimization of generalized performance metrics for binary classification by means of surrogate loss.  We focus on a class of metrics, which are linear-fractional functions of the false positive and false negative rates (examples of which include $F_\\\\beta$-measure, Jaccard similarity coefficient, AM measure, and many others). Our analysis concerns the following two-step procedure. First, a real-valued function $f$ is learned by minimizing a surrogate loss for binary classification on the training sample. It is assumed that the surrogate loss is a strongly proper composite loss function (examples of which include logistic loss, squared-error loss, exponential loss, etc.). Then, given $f$, a threshold $\\\\hat{\\\\theta}$ is tuned on a separate validation sample, by direct optimization of the target performance measure. We show that the regret of the resulting classifier (obtained from thresholding $f$ on $\\\\hat{\\\\theta}$  measured with respect to the target metric is upperbounded by the regret of f measured with respect to the surrogate loss.  Our finding is further analyzed in\u00a0a\u00a0computational study on both synthetic and real data\u00a0sets.",
        "bibtex": "@InProceedings{pmlr-v45-Kotlowski15,\n  title = \t {Surrogate regret bounds for generalized classification performance metrics},\n  author = \t {Kotlowski, Wojciech and Dembczy\u0144ski, Krzysztof},\n  booktitle = \t {Asian Conference on Machine Learning},\n  pages = \t {301--316},\n  year = \t {2016},\n  editor = \t {Holmes, Geoffrey and Liu, Tie-Yan},\n  volume = \t {45},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hong Kong},\n  month = \t {20--22 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v45/Kotlowski15.pdf},\n  url = \t {https://proceedings.mlr.press/v45/Kotlowski15.html},\n  abstract = \t {We consider optimization of generalized performance metrics for binary classification by means of surrogate loss.  We focus on a class of metrics, which are linear-fractional functions of the false positive and false negative rates (examples of which include $F_\\\\beta$-measure, Jaccard similarity coefficient, AM measure, and many others). Our analysis concerns the following two-step procedure. First, a real-valued function $f$ is learned by minimizing a surrogate loss for binary classification on the training sample. It is assumed that the surrogate loss is a strongly proper composite loss function (examples of which include logistic loss, squared-error loss, exponential loss, etc.). Then, given $f$, a threshold $\\\\hat{\\\\theta}$ is tuned on a separate validation sample, by direct optimization of the target performance measure. We show that the regret of the resulting classifier (obtained from thresholding $f$ on $\\\\hat{\\\\theta}$  measured with respect to the target metric is upperbounded by the regret of f measured with respect to the surrogate loss.  Our finding is further analyzed in\u00a0a\u00a0computational study on both synthetic and real data\u00a0sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v45/Kotlowski15.pdf",
        "supp": "",
        "pdf_size": 415491,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7963078085856001383&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Poznan University of Technology, Poland; Poznan University of Technology, Poland",
        "aff_domain": "cs.put.poznan.pl;cs.put.poznan.pl",
        "email": "cs.put.poznan.pl;cs.put.poznan.pl",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Poznan University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.put.poznan.pl/",
        "aff_unique_abbr": "PUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Poland"
    }
]