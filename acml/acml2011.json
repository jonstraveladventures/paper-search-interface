[
    {
        "id": "25287ccb3c",
        "title": "A General Linear Non-Gaussian State-Space Model: Identifiability, Identification, and Applications",
        "site": "https://proceedings.mlr.press/v20/zhang11.html",
        "author": "Kun Zhang; Aapo Hyv\u00e4rinen",
        "abstract": "State-space modeling provides a powerful tool for system identification and prediction. In linear state-space models the data are usually assumed to be Gaussian and the models have certain structural constraints such that they are identifiable. In this paper we propose a non-Gaussian state-space model which does not have such constraints. We prove that this model is fully identifiable. We then propose an efficient two-step method for parameter estimation: one first extracts the subspace of the latent processes based on the temporal information of the data, and then performs multichannel blind deconvolution, making use of both the temporal information and non-Gaussianity. We conduct a series of simulations to illustrate the performance of the proposed method. Finally, we apply the proposed model and parameter estimation method on real data, including major world stock indices and magnetoencephalography (MEG) recordings. Experimental results are encouraging and show the practical usefulness of the proposed model and method.",
        "bibtex": "@InProceedings{pmlr-v20-zhang11,\n  title = \t {A General Linear Non-{G}aussian State-Space Model: Identifiability, Identification, and Applications},\n  author = \t {Zhang, Kun and Hyv\u00e4rinen, Aapo},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {113--128},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/zhang11/zhang11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/zhang11.html},\n  abstract = \t {State-space modeling provides a powerful tool for system identification and prediction. In linear state-space models the data are usually assumed to be Gaussian and the models have certain structural constraints such that they are identifiable. In this paper we propose a non-Gaussian state-space model which does not have such constraints. We prove that this model is fully identifiable. We then propose an efficient two-step method for parameter estimation: one first extracts the subspace of the latent processes based on the temporal information of the data, and then performs multichannel blind deconvolution, making use of both the temporal information and non-Gaussianity. We conduct a series of simulations to illustrate the performance of the proposed method. Finally, we apply the proposed model and parameter estimation method on real data, including major world stock indices and magnetoencephalography (MEG) recordings. Experimental results are encouraging and show the practical usefulness of the proposed model and method.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/zhang11/zhang11.pdf",
        "supp": "",
        "pdf_size": 869147,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12225867452640436769&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Max Planck Institute for Intelligent Systems; Dept of Computer Science, HIIT, and Dept of Mathematics and Statistics, University of Helsinki",
        "aff_domain": "tuebingen.mpg.de;helsinki.fi",
        "email": "tuebingen.mpg.de;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;University of Helsinki",
        "aff_unique_dep": "Intelligent Systems;Dept of Computer Science",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.helsinki.fi",
        "aff_unique_abbr": "MPI-IS;UH",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Helsinki",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Finland"
    },
    {
        "id": "fee27755e3",
        "title": "Acceleration Technique for Boosting Classification and its Application to Face Detection",
        "site": "https://proceedings.mlr.press/v20/kawakita11.html",
        "author": "Masanori Kawakita; Ryota Izumi; Jun'ichi Takeuchi; Yi Hu; Tetsuya Takamori; Hirokazu Kameyama",
        "abstract": "We propose an acceleration technique for boosting classification without any loss of classification accuracy and apply it to a face detection task. In classification task, much effort has been spent on improving the classification accuracy and the computational cost of training. In addition to them, the computational cost of classification itself can be critical in several applications including face detection. In face detection, a celebrating work by Viola and Jones\u00a0(2001) developed a significantly fast face detector achieving a competitive accuracy with all preceding face detectors. In their algorithm, the cascade structure of boosting classifier plays an important role. In this paper, we propose an acceleration technique for boosting classifier. The key idea of our proposal is the fact that one can determine the sign of discriminant function before all weak learners are evaluated in general. An advantage is that our algorithm has no loss in classification accuracy. Another advantage is that our proposal is a unsupervised learning so that it can treat a covariate shift situation. We also apply our proposal to each cascaded boosting classifier in Viola and Jones type face detector. As a result, our proposal succeeds in reducing the classification cost by 20%.",
        "bibtex": "@InProceedings{pmlr-v20-kawakita11,\n  title = \t {Acceleration Technique for Boosting Classification and its Application to Face Detection},\n  author = \t {Kawakita, Masanori and Izumi, Ryota and Takeuchi, Jun'ichi and Hu, Yi and Takamori, Tetsuya and Kameyama, Hirokazu},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {335--349},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/kawakita11/kawakita11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/kawakita11.html},\n  abstract = \t {We propose an acceleration technique for boosting classification without any loss of classification accuracy and apply it to a face detection task. In classification task, much effort has been spent on improving the classification accuracy and the computational cost of training. In addition to them, the computational cost of classification itself can be critical in several applications including face detection. In face detection, a celebrating work by Viola and Jones\u00a0(2001) developed a significantly fast face detector achieving a competitive accuracy with all preceding face detectors. In their algorithm, the cascade structure of boosting classifier plays an important role. In this paper, we propose an acceleration technique for boosting classifier. The key idea of our proposal is the fact that one can determine the sign of discriminant function before all weak learners are evaluated in general. An advantage is that our algorithm has no loss in classification accuracy. Another advantage is that our proposal is a unsupervised learning so that it can treat a covariate shift situation. We also apply our proposal to each cascaded boosting classifier in Viola and Jones type face detector. As a result, our proposal succeeds in reducing the classification cost by 20%.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/kawakita11/kawakita11.pdf",
        "supp": "",
        "pdf_size": 1235348,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1526127270995880269&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Graduate school of Information Science and Electrical Engineering, Kyushu University; Graduate school of Information Science and Electrical Engineering, Kyushu University; Graduate school of Information Science and Electrical Engineering, Kyushu University; Imaging Technology Center, Research & Development Management Headquarters, FUJIFILM Corporation; Imaging Technology Center, Research & Development Management Headquarters, FUJIFILM Corporation; Imaging Technology Center, Research & Development Management Headquarters, FUJIFILM Corporation",
        "aff_domain": "inf.kyushu-u.ac.jp;kairo.csce.kyushu-u.ac.jp;inf.kyushu-u.ac.jp;fujifilm.co.jp;fujifilm.co.jp;fujifilm.co.jp",
        "email": "inf.kyushu-u.ac.jp;kairo.csce.kyushu-u.ac.jp;inf.kyushu-u.ac.jp;fujifilm.co.jp;fujifilm.co.jp;fujifilm.co.jp",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1;1",
        "aff_unique_norm": "Kyushu University;FUJIFILM Corporation",
        "aff_unique_dep": "Graduate school of Information Science and Electrical Engineering;Imaging Technology Center, Research & Development Management Headquarters",
        "aff_unique_url": "https://www.kyushu-u.ac.jp;https://www.fujifilm.com",
        "aff_unique_abbr": "Kyushu U;Fujifilm",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "b85831b123",
        "title": "Approximate Model Selection for Large Scale LSSVM",
        "site": "https://proceedings.mlr.press/v20/ding11.html",
        "author": "Lizhong Ding; Shizhong Liao",
        "abstract": "Model selection is critical to least squares support vector machine (LSSVM). A major problem of existing model selection approaches of LSSVM is that the inverse of the kernel matrix need to be calculated with O(n^3) complexity for each iteration, where n is the number of training examples. It is prohibitive for the large scale application. In this paper, we propose an approximate approach to model selection of LSSVM. We use multilevel circulant matrices to approximate the kernel matrix so that the fast Fourier transform (FFT) can be applied to reduce the computational cost of matrix inverse. With such approximation, we first design an efficient LSSVM algorithm with O(nlog(n)) complexity and theoretically analyze the effect of kernel matrix approximation on the decision function of LSSVM. We further show that the approximate optimal model produced with the multilevel circulant matrix is consistent with the accurate one produced with the original kernel matrix. Under the guarantee of consistency, we present an approximate model selection scheme, whose complexity is significantly lower than the previous approaches. Experimental results on benchmark datasets demonstrate the effectiveness of approximate model selection.",
        "bibtex": "@InProceedings{pmlr-v20-ding11,\n  title = \t {Approximate Model Selection for Large Scale LSSVM},\n  author = \t {Ding, Lizhong and Liao, Shizhong},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {165--180},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/ding11/ding11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/ding11.html},\n  abstract = \t {Model selection is critical to least squares support vector machine (LSSVM). A major problem of existing model selection approaches of LSSVM is that the inverse of the kernel matrix need to be calculated with O(n^3) complexity for each iteration, where n is the number of training examples. It is prohibitive for the large scale application. In this paper, we propose an approximate approach to model selection of LSSVM. We use multilevel circulant matrices to approximate the kernel matrix so that the fast Fourier transform (FFT) can be applied to reduce the computational cost of matrix inverse. With such approximation, we first design an efficient LSSVM algorithm with O(nlog(n)) complexity and theoretically analyze the effect of kernel matrix approximation on the decision function of LSSVM. We further show that the approximate optimal model produced with the multilevel circulant matrix is consistent with the accurate one produced with the original kernel matrix. Under the guarantee of consistency, we present an approximate model selection scheme, whose complexity is significantly lower than the previous approaches. Experimental results on benchmark datasets demonstrate the effectiveness of approximate model selection.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/ding11/ding11.pdf",
        "supp": "",
        "pdf_size": 421713,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3665971648762747500&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science and Technology, Tianjin University, Tianjin 300072, P. R. China; School of Computer Science and Technology, Tianjin University, Tianjin 300072, P. R. China",
        "aff_domain": "gmail.com;tju.edu.cn",
        "email": "gmail.com;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "faeb1a6b6b",
        "title": "Bayesian Inference for Statistical Abduction Using Markov Chain Monte Carlo",
        "site": "https://proceedings.mlr.press/v20/ishihata11.html",
        "author": "Masakuzu Ishihata; Taisuke Sato",
        "abstract": "Abduction is one of the basic logical inferences (deduction, induction and abduction) and derives the best explanations for our observation. Statistical abduction attempts to define a probability distribution over explanations and to evaluate them by their probabilities. The framework of statistical abduction is general since many well-known probabilistic models, i.e., BNs, HMMs and PCFGs, are formulated as statistical abduction. Logic-based probabilistic models (LBPMs) have been developed as a way to combine probabilities and logic, and it enables us to perform statistical abduction. However, most of existing LBPMs impose restrictions on explanations (logical formulas) to realize efficient probability computation and learning. To relax those restrictions, we propose two MCMC (Markov chain Monte Carlo) methods for Bayesian inference on LBPMs using binary decision diagrams. The main advantage of our methods over existing methods is that it has no restriction on formulas. In the context of statistical abduction with Bayesian inference, whereas our deterministic knowledge can be described by logical formulas as rules and facts, our non-deterministic knowledge like frequency and preference can be reflected in a prior distribution in Bayesian inference. To illustrate our methods, we first formulate LDA (latent Dirichlet allocation) which is a well-known generative probabilistic model for bag-of-words as a form of statistical abduction, and compare the learning result of our methods with that of an MCMC method called collapsed Gibbs sampling specialized for LDA. We also apply our methods to diagnosis for failure in a logic circuit and evaluate explanations using a posterior distribution approximated by our method. The experiment shows Bayesian inference achieves better predicting accuracy than that of Maximum likelihood estimation.",
        "bibtex": "@InProceedings{pmlr-v20-ishihata11,\n  title = \t {Bayesian Inference for Statistical Abduction Using {M}arkov Chain {M}onte {C}arlo},\n  author = \t {Ishihata, Masakuzu and Sato, Taisuke},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {81--96},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/ishihata11/ishihata11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/ishihata11.html},\n  abstract = \t {Abduction is one of the basic logical inferences (deduction, induction and abduction) and derives the best explanations for our observation. Statistical abduction attempts to define a probability distribution over explanations and to evaluate them by their probabilities. The framework of statistical abduction is general since many well-known probabilistic models, i.e., BNs, HMMs and PCFGs, are formulated as statistical abduction. Logic-based probabilistic models (LBPMs) have been developed as a way to combine probabilities and logic, and it enables us to perform statistical abduction. However, most of existing LBPMs impose restrictions on explanations (logical formulas) to realize efficient probability computation and learning. To relax those restrictions, we propose two MCMC (Markov chain Monte Carlo) methods for Bayesian inference on LBPMs using binary decision diagrams. The main advantage of our methods over existing methods is that it has no restriction on formulas. In the context of statistical abduction with Bayesian inference, whereas our deterministic knowledge can be described by logical formulas as rules and facts, our non-deterministic knowledge like frequency and preference can be reflected in a prior distribution in Bayesian inference. To illustrate our methods, we first formulate LDA (latent Dirichlet allocation) which is a well-known generative probabilistic model for bag-of-words as a form of statistical abduction, and compare the learning result of our methods with that of an MCMC method called collapsed Gibbs sampling specialized for LDA. We also apply our methods to diagnosis for failure in a logic circuit and evaluate explanations using a posterior distribution approximated by our method. The experiment shows Bayesian inference achieves better predicting accuracy than that of Maximum likelihood estimation.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/ishihata11/ishihata11.pdf",
        "supp": "",
        "pdf_size": 976246,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12506649775565070610&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Graduate School of Information Science and Engineering, Tokyo Institute of Technology; Graduate School of Information Science and Engineering, Tokyo Institute of Technology",
        "aff_domain": "mi.cs.titech.ac.jp;mi.cs.titech.ac.jp",
        "email": "mi.cs.titech.ac.jp;mi.cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "Graduate School of Information Science and Engineering",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "aef496c6d5",
        "title": "Computationally Efficient Sufficient Dimension Reduction via Squared-Loss Mutual Information",
        "site": "https://proceedings.mlr.press/v20/yamada11.html",
        "author": "Makoto Yamada; Gang Niu; Jun Takagi; Masashi Sugiyama",
        "abstract": "The purpose of sufficient dimension reduction (SDR) is to find a low-dimensional expression of input features that is sufficient for predicting output values. In this paper, we propose a novel distribution-free SDR method called sufficient component analysis (SCA), which is computationally more efficient than existing methods. In our method, a solution is computed by iteratively performing dependence estimation and maximization: Dependence estimation is analytically carried out by recently-proposed least-squares mutual information (LSMI), and dependence maximization is also analytically carried out by utilizing the Epanechnikov kernel. Through large-scale experiments on real-world image classification and audio tagging problems, the proposed method is shown to compare favorably with existing dimension reduction approaches.",
        "bibtex": "@InProceedings{pmlr-v20-yamada11,\n  title = \t {Computationally Efficient Sufficient Dimension Reduction via Squared-Loss Mutual Information},\n  author = \t {Yamada, Makoto and Niu, Gang and Takagi, Jun and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {247--262},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/yamada11/yamada11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/yamada11.html},\n  abstract = \t {The purpose of sufficient dimension reduction (SDR) is to find a low-dimensional expression of input features that is sufficient for predicting output values. In this paper, we propose a novel distribution-free SDR method called sufficient component analysis (SCA), which is computationally more efficient than existing methods. In our method, a solution is computed by iteratively performing dependence estimation and maximization: Dependence estimation is analytically carried out by recently-proposed least-squares mutual information (LSMI), and dependence maximization is also analytically carried out by utilizing the Epanechnikov kernel. Through large-scale experiments on real-world image classification and audio tagging problems, the proposed method is shown to compare favorably with existing dimension reduction approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/yamada11/yamada11.pdf",
        "supp": "",
        "pdf_size": 778401,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18075819345211641196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Tokyo Institute of Technology; Department of Computer Science, Tokyo Institute of Technology; Department of Computer Science, Tokyo Institute of Technology; Department of Computer Science, Tokyo Institute of Technology",
        "aff_domain": "sg.cs.titech.ac.jp;sg.cs.titech.ac.jp;sg.cs.titech.ac.jp;cs.titech.ac.jp",
        "email": "sg.cs.titech.ac.jp;sg.cs.titech.ac.jp;sg.cs.titech.ac.jp;cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "c4d6ecd204",
        "title": "Continuous Rapid Action Value Estimates",
        "site": "https://proceedings.mlr.press/v20/couetoux11.html",
        "author": "Adrien Cou\u00ebtoux; Mario Milone; M\u00e1ty\u00e1s Brendel; Hassan Doghmen; Mich\u00e8le Sebag; Olivier Teytaud",
        "abstract": "In the last decade, Monte-Carlo Tree Search (MCTS) has revolutionized the domain of large-scale Markov Decision Process problems. MCTS most often uses the Upper Confidence Tree algorithm to handle the exploration versus exploitation trade-off, while a few heuristics are used to guide the exploration in large search spaces. Among these heuristics is Rapid Action Value Estimate (RAVE). This paper is concerned with extending the RAVE heuristics to continuous action and state spaces. The approach is experimentally validated on two artificial benchmark problems: the treasure hunt game, and a real-world energy management problem.",
        "bibtex": "@InProceedings{pmlr-v20-couetoux11,\n  title = \t {Continuous Rapid Action Value Estimates},\n  author = \t {Cou\u00ebtoux, Adrien and Milone, Mario and Brendel, M\u00e1ty\u00e1s and Doghmen, Hassan and Sebag, Mich\u00e8le and Teytaud, Olivier},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {19--31},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/couetoux11/couetoux11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/couetoux11.html},\n  abstract = \t {In the last decade, Monte-Carlo Tree Search (MCTS) has revolutionized the domain of large-scale Markov Decision Process problems. MCTS most often uses the Upper Confidence Tree algorithm to handle the exploration versus exploitation trade-off, while a few heuristics are used to guide the exploration in large search spaces. Among these heuristics is Rapid Action Value Estimate (RAVE). This paper is concerned with extending the RAVE heuristics to continuous action and state spaces. The approach is experimentally validated on two artificial benchmark problems: the treasure hunt game, and a real-world energy management problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/couetoux11/couetoux11.pdf",
        "supp": "",
        "pdf_size": 373583,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8042515414315376856&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "TAO, INRIA-CNRS-LRI, Univ. Paris-Sud, F-91405 Orsay; TAO, INRIA-CNRS-LRI, Univ. Paris-Sud, F-91405 Orsay; TAO, INRIA-CNRS-LRI, Univ. Paris-Sud, F-91405 Orsay; TAO, INRIA-CNRS-LRI, Univ. Paris-Sud, F-91405 Orsay; TAO, INRIA-CNRS-LRI, Univ. Paris-Sud, F-91405 Orsay; TAO, INRIA-CNRS-LRI, Univ. Paris-Sud, F-91405 Orsay+OASE Lab, National University of Tainan, Taiwan",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0+1",
        "aff_unique_norm": "INRIA-CNRS-LRI;National University of Tainan",
        "aff_unique_dep": "TAO;OASE Lab",
        "aff_unique_url": "https://www.inria.fr;https://www.nutn.edu.tw",
        "aff_unique_abbr": "INRIA;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Taiwan",
        "aff_country_unique_index": "0;0;0;0;0;0+1",
        "aff_country_unique": "France;China"
    },
    {
        "id": "2875aa0ce8",
        "title": "Estimating Diffusion Probability Changes for AsIC-SIS Model from Information Diffusion Results",
        "site": "https://proceedings.mlr.press/v20/koide11.html",
        "author": "Akihiro Koide; Kazumi Saito; Kouzou Ohara; Masahiro Kimura; Hiroshi Motoda",
        "abstract": "We address the problem of estimating changes in diffusion probability over a social network from the observed information diffusion results, which is possibly caused by an unknown external situation change. For this problem, we focused on the asynchronous independent cascade (AsIC) model in the SIS (Susceptible/Infected/Susceptible) setting in order to meet more realistic situations such as communication in a blogosphere. This model is referred to as the AsIC-SIS model. We assume that the diffusion parameter changes are approximated by a series of step functions, and their changes are reflected in the observed diffusion results. Thus, the problem is reduced to detecting how many step functions are needed, where in time each one starts and how long it lasts, and what the hight of each one is. The method employs the derivative of the likelihood function of the observed data that are assumed to be generated from the AsIC-SIS model, adopts a divide-and-conquer type greedy recursive partitioning, and utilizes an MDL model selection measure to determine the adequate number of step functions. The results obtained using real world network structures confirmed that the method works well as intended. The MDL criterion is useful to avoid overfitting, and the found pattern is not necessarily the same in terms of the number of step functions as the one assumed to be true, but the error is always reduced to a small value.",
        "bibtex": "@InProceedings{pmlr-v20-koide11,\n  title = \t {Estimating Diffusion Probability Changes for {AsIC-SIS} Model from Information Diffusion Results},\n  author = \t {Koide, Akihiro and Saito, Kazumi and Ohara, Kouzou and Kimura, Masahiro and Motoda, Hiroshi},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {297--313},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/koide11/koide11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/koide11.html},\n  abstract = \t {We address the problem of estimating changes in diffusion probability over a social network from the observed information diffusion results, which is possibly caused by an unknown external situation change. For this problem, we focused on the asynchronous independent cascade (AsIC) model in the SIS (Susceptible/Infected/Susceptible) setting in order to meet more realistic situations such as communication in a blogosphere. This model is referred to as the AsIC-SIS model. We assume that the diffusion parameter changes are approximated by a series of step functions, and their changes are reflected in the observed diffusion results. Thus, the problem is reduced to detecting how many step functions are needed, where in time each one starts and how long it lasts, and what the hight of each one is. The method employs the derivative of the likelihood function of the observed data that are assumed to be generated from the AsIC-SIS model, adopts a divide-and-conquer type greedy recursive partitioning, and utilizes an MDL model selection measure to determine the adequate number of step functions. The results obtained using real world network structures confirmed that the method works well as intended. The MDL criterion is useful to avoid overfitting, and the found pattern is not necessarily the same in terms of the number of step functions as the one assumed to be true, but the error is always reduced to a small value.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/koide11/koide11.pdf",
        "supp": "",
        "pdf_size": 443266,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11412948114796079815&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Graduate School of Management and Information of Innovation, University of Shizuoka; School of Management and Information, University of Shizuoka; Department of Integrated Information Technology, Aoyama Gakuin University; Department of Electronics and Informatics, Ryukoku University; Institute of Scienti\ufb01c and Industrial Research, Osaka University",
        "aff_domain": "u-shizuoka-ken.ac.jp;u-shizuoka-ken.ac.jp;it.aoyama.ac.jp;rins.ryukoku.ac.jp;ar.sanken.osaka-u.ac.jp",
        "email": "u-shizuoka-ken.ac.jp;u-shizuoka-ken.ac.jp;it.aoyama.ac.jp;rins.ryukoku.ac.jp;ar.sanken.osaka-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;3",
        "aff_unique_norm": "University of Shizuoka;Aoyama Gakuin University;Ryukoku University;Osaka University",
        "aff_unique_dep": "Graduate School of Management and Information of Innovation;Department of Integrated Information Technology;Department of Electronics and Informatics;Institute of Scienti\ufb01c and Industrial Research",
        "aff_unique_url": "https://www.u-shizuoka.ac.jp;https://www.aoyama.ac.jp;https://www.ryukoku.ac.jp;https://www.osaka-u.ac.jp",
        "aff_unique_abbr": ";AGU;Ryukoku U;Osaka U",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Osaka",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "115e87e9c9",
        "title": "Improving Policy Gradient Estimates with Influence Information",
        "site": "https://proceedings.mlr.press/v20/pinto11.html",
        "author": "Jervis Pinto; Alan Fern; Tim Bauer; Martin Erwig",
        "abstract": "In reinforcement learning (RL) it is often possible to obtain sound, but incomplete, information about influences and independencies among problem variables and rewards, even when an exact domain model is unknown. For example, such information can be computed based on a partial, qualitative domain model, or via domain-specific analysis techniques. While, intuitively, such information appears useful for RL, there are no algorithms that incorporate it in a sound way. In this work, we describe how to leverage such information for improving the estimation of policy gradients, which can be used to speedup gradient-based RL. We prove general conditions under which our estimator is unbiased and show that it will typically have reduced variance compared to standard unbiased gradient estimates. We evaluate the approach in the domain of Adaptation-Based Programming where RL is used to optimize the performance of programs and independence information can be computed via standard program analysis techniques. Incorporating independence information produces a large speedup in learning on a variety of adaptive programs.",
        "bibtex": "@InProceedings{pmlr-v20-pinto11,\n  title = \t {Improving Policy Gradient Estimates with Influence Information},\n  author = \t {Pinto, Jervis and Fern, Alan and Bauer, Tim and Erwig, Martin},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {1--18},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/pinto11/pinto11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/pinto11.html},\n  abstract = \t {In reinforcement learning (RL) it is often possible to obtain sound, but incomplete, information about influences and independencies among problem variables and rewards, even when an exact domain model is unknown. For example, such information can be computed based on a partial, qualitative domain model, or via domain-specific analysis techniques. While, intuitively, such information appears useful for RL, there are no algorithms that incorporate it in a sound way. In this work, we describe how to leverage such information for improving the estimation of policy gradients, which can be used to speedup gradient-based RL. We prove general conditions under which our estimator is unbiased and show that it will typically have reduced variance compared to standard unbiased gradient estimates. We evaluate the approach in the domain of Adaptation-Based Programming where RL is used to optimize the performance of programs and independence information can be computed via standard program analysis techniques. Incorporating independence information produces a large speedup in learning on a variety of adaptive programs.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/pinto11/pinto11.pdf",
        "supp": "",
        "pdf_size": 1243446,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5829687039122127223&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR 97331, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR 97331, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR 97331, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR 97331, USA",
        "aff_domain": "EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU",
        "email": "EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d74208bf7",
        "title": "Learning Attribute-weighted Voter Model over Social Networks",
        "site": "https://proceedings.mlr.press/v20/yamagishi11.html",
        "author": "Yuki Yamagishi; Kazumi Saito; Kouzou Ohara; Masahiro Kimura; Hiroshi Motoda",
        "abstract": "We propose an opinion formation model, an extension of the voter model that incorporates the strength of each node, which is modeled as a function of the node attributes. Then, we address the problem of estimating parameter values for these attributes that appear in the function from the observed opinion formation data and solve this by maximizing the likelihood using an iterative parameter value updating algorithm, which is efficient and is guaranteed to converge. We show that the proposed algorithm can correctly learn the dependency in our experiments on four real world networks for which we used the assumed attribute dependency. We further show that the influence degree of each node based on the extended voter model is substantially different from that obtained assuming a uniform strength (a naive model for which the influence degree is known to be proportional to the node degree), and is more sensitive to the node strength than the node degree even for a moderate value of the node strength.",
        "bibtex": "@InProceedings{pmlr-v20-yamagishi11,\n  title = \t {Learning Attribute-weighted Voter Model over Social Networks},\n  author = \t {Yamagishi, Yuki and Saito, Kazumi and Ohara, Kouzou and Kimura, Masahiro and Motoda, Hiroshi},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {263--280},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/yamagishi11/yamagishi11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/yamagishi11.html},\n  abstract = \t {We propose an opinion formation model, an extension of the voter model that incorporates the strength of each node, which is modeled as a function of the node attributes. Then, we address the problem of estimating parameter values for these attributes that appear in the function from the observed opinion formation data and solve this by maximizing the likelihood using an iterative parameter value updating algorithm, which is efficient and is guaranteed to converge. We show that the proposed algorithm can correctly learn the dependency in our experiments on four real world networks for which we used the assumed attribute dependency. We further show that the influence degree of each node based on the extended voter model is substantially different from that obtained assuming a uniform strength (a naive model for which the influence degree is known to be proportional to the node degree), and is more sensitive to the node strength than the node degree even for a moderate value of the node strength.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/yamagishi11/yamagishi11.pdf",
        "supp": "",
        "pdf_size": 695764,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8613183348217561571&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "School of Management and Information, University of Shizuoka; School of Management and Information, University of Shizuoka; Department of Integrated Information Technology, Aoyama Gakuin University; Department of Electronics and Informatics, Ryukoku University; Institute of Scientific and Industrial Research, Osaka University",
        "aff_domain": "u-shizuoka-ken.ac.jp;u-shizuoka-ken.ac.jp;it.aoyama.ac.jp;rins.ryukoku.ac.jp;ar.sanken.osaka-u.ac.jp",
        "email": "u-shizuoka-ken.ac.jp;u-shizuoka-ken.ac.jp;it.aoyama.ac.jp;rins.ryukoku.ac.jp;ar.sanken.osaka-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;3",
        "aff_unique_norm": "University of Shizuoka;Aoyama Gakuin University;Ryukoku University;Osaka University",
        "aff_unique_dep": "School of Management and Information;Department of Integrated Information Technology;Department of Electronics and Informatics;Institute of Scientific and Industrial Research",
        "aff_unique_url": "https://www.u-shizuoka.ac.jp;https://www.aoyama.ac.jp;https://www.ryukoku.ac.jp;https://www.osaka-u.ac.jp",
        "aff_unique_abbr": ";AGU;Ryukoku U;OSU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Osaka",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "10ee4d0474",
        "title": "Learning Rules from Incomplete Examples via Implicit Mention Models",
        "site": "https://proceedings.mlr.press/v20/doppa11.html",
        "author": "Janardhan Rao Doppa; Mohammad Shahed Sorower; Mohammad Nasresfahani; Jed Irvine; Walker Orr; Thomas G. Dietterich; Xiaoli Fern; Prasad Tadepalli",
        "abstract": "We study the problem of learning general rules from concrete facts extracted from natural data sources such as the newspaper stories and medical histories. Natural data sources present two challenges to automated learning, namely, radical incompleteness and systematic bias. In this paper, we propose an approach that combines simultaneous learning of multiple predictive rules with differential scoring of evidence which adapts to a presumed model of data generation. Learning multiple predicates simultaneously mitigates the problem of radical incompleteness, while the differential scoring would help reduce the effects of systematic bias. We evaluate our approach empirically on both textual and non-textual sources. We further present a theoretical analysis that elucidates our approach and explains the empirical results.",
        "bibtex": "@InProceedings{pmlr-v20-doppa11,\n  title = \t {Learning Rules from Incomplete Examples via Implicit Mention Models},\n  author = \t {Doppa, Janardhan Rao and Sorower, Mohammad Shahed and Nasresfahani, Mohammad and Irvine, Jed and Orr, Walker and Dietterich, Thomas G. and Fern, Xiaoli and Tadepalli, Prasad},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {197--212},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/doppa11/doppa11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/doppa11.html},\n  abstract = \t {We study the problem of learning general rules from concrete facts extracted from natural data sources such as the newspaper stories and medical histories. Natural data sources present two challenges to automated learning, namely, radical incompleteness and systematic bias. In this paper, we propose an approach that combines simultaneous learning of multiple predictive rules with differential scoring of evidence which adapts to a presumed model of data generation. Learning multiple predicates simultaneously mitigates the problem of radical incompleteness, while the differential scoring would help reduce the effects of systematic bias. We evaluate our approach empirically on both textual and non-textual sources. We further present a theoretical analysis that elucidates our approach and explains the empirical results.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/doppa11/doppa11.pdf",
        "supp": "",
        "pdf_size": 405085,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15815151285136692928&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Electrical Engineering and Computer Science; School of Electrical Engineering and Computer Science; School of Electrical Engineering and Computer Science; School of Electrical Engineering and Computer Science; School of Electrical Engineering and Computer Science; School of Electrical Engineering and Computer Science; School of Electrical Engineering and Computer Science; School of Electrical Engineering and Computer Science",
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Electrical Engineering and Computer Science",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7ed5e7e117",
        "title": "Learning low-rank output kernels",
        "site": "https://proceedings.mlr.press/v20/dinuzzo11.html",
        "author": "Francesco Dinuzzo; Kenji Fukumizu",
        "abstract": "Output kernel learning techniques allow to simultaneously learn a vector-valued function and a positive semidefinite matrix which describes the relationships between the outputs. In this paper, we introduce a new formulation that imposes a low-rank constraint on the output kernel and operates directly on a factor of the kernel matrix. First, we investigate the connection between output kernel learning and a regularization problem for an architecture with two layers. Then, we show that a variety of methods such as nuclear norm regularized regression, reduced-rank regression, principal component analysis, and low rank matrix approximation can be seen as special cases of the output kernel learning framework. Finally, we introduce a block coordinate descent strategy for learning low-rank output kernels.",
        "bibtex": "@InProceedings{pmlr-v20-dinuzzo11,\n  title = \t {Learning low-rank output kernels},\n  author = \t {Dinuzzo, Francesco and Fukumizu, Kenji},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {181--196},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/dinuzzo11/dinuzzo11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/dinuzzo11.html},\n  abstract = \t {Output kernel learning techniques allow to simultaneously learn a vector-valued function and a positive semidefinite matrix which describes the relationships between the outputs. In this paper, we introduce a new formulation that imposes a low-rank constraint on the output kernel and operates directly on a factor of the kernel matrix. First, we investigate the connection between output kernel learning and a regularization problem for an architecture with two layers. Then, we show that a variety of methods such as nuclear norm regularized regression, reduced-rank regression, principal component analysis, and low rank matrix approximation can be seen as special cases of the output kernel learning framework. Finally, we introduce a block coordinate descent strategy for learning low-rank output kernels.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/dinuzzo11/dinuzzo11.pdf",
        "supp": "",
        "pdf_size": 331338,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10011846218064750504&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Max Planck Institute for Intelligent Systems; The Institute of Statistical Mathematics",
        "aff_domain": "tuebingen.mpg.de;ism.ac.jp",
        "email": "tuebingen.mpg.de;ism.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Institute of Statistical Mathematics",
        "aff_unique_dep": "Intelligent Systems;",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.ism.ac.jp",
        "aff_unique_abbr": "MPI-IS;ISM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Japan"
    },
    {
        "id": "768592aba6",
        "title": "Learning to Locate Relative Outliers",
        "site": "https://proceedings.mlr.press/v20/li11.html",
        "author": "Shukai Li; Ivor W. Tsang",
        "abstract": "Outliers usually spread across regions of low density. However, due to the absence or scarcity of outliers, designing a robust detector to sift outliers from a given dataset is still very challenging. In this paper, we consider to identify relative outliers from the target dataset with respect to another reference dataset of normal data. Particularly, we employ Maximum Mean Discrepancy (MMD) for matching the distribution between these two datasets and present a novel learning framework to learn a relative outlier detector. The learning task is formulated as a Mixed Integer Programming (MIP) problem, which is computationally hard. To this end, we propose an effective procedure to find a largely violated labeling vector for identifying relative outliers from abundant normal patterns, and its convergence is also presented. Then, a set of largely violated labeling vectors are combined by multiple kernel learning methods to robustly locate relative outliers. Comprehensive empirical studies on real-world datasets verify that our proposed relative outlier detection outperforms existing methods.",
        "bibtex": "@InProceedings{pmlr-v20-li11,\n  title = \t {Learning to Locate Relative Outliers},\n  author = \t {Li, Shukai and Tsang, Ivor W.},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {47--62},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/li11/li11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/li11.html},\n  abstract = \t {Outliers usually spread across regions of low density. However, due to the absence or scarcity of outliers, designing a robust detector to sift outliers from a given dataset is still very challenging. In this paper, we consider to identify relative outliers from the target dataset with respect to another reference dataset of normal data. Particularly, we employ Maximum Mean Discrepancy (MMD) for matching the distribution between these two datasets and present a novel learning framework to learn a relative outlier detector. The learning task is formulated as a Mixed Integer Programming (MIP) problem, which is computationally hard. To this end, we propose an effective procedure to find a largely violated labeling vector for identifying relative outliers from abundant normal patterns, and its convergence is also presented. Then, a set of largely violated labeling vectors are combined by multiple kernel learning methods to robustly locate relative outliers. Comprehensive empirical studies on real-world datasets verify that our proposed relative outlier detection outperforms existing methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/li11/li11.pdf",
        "supp": "",
        "pdf_size": 471688,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10637490473798786249&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Engineering, Nanyang Technological University, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Singapore 639798",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Computer Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "76a9b5e0aa",
        "title": "Mapping Kernels Defined Over Countably Infinite Mapping Systems and their Application",
        "site": "https://proceedings.mlr.press/v20/shin11.html",
        "author": "Kilho Shin",
        "abstract": "The mapping kernel is a generalization of Haussler\u2019s convolution kernel, and has a wide range of application including kernels for higher degree structures such as trees. Like Haussler\u2019s convolution kernel, a mapping kernel is a finite sum of values of a primitive kernel. One of the major reasons to use the mapping kernel template in engineering novel kernels is because a strong theorem is known for positive definiteness of the resulting mapping kernels. If the mapping kernel meets the transitivity condition and if the primitive kernel is positive definite, the mapping kernel is also positive definite. In this paper, we generalize this theorem by showing, even when we extend the definition of mapping kernels so that a mapping kernel can be a converging sum of countably infinite primitive kernel values, the transitivity condition is still a criteria to determine positive definiteness of mapping kernels according to the extended definition. Interestingly, this result is also useful to investigate positive definiteness of mapping kernels determined as finite sums, when they do not meet the transitivity condition. For this purpose, we introduce a general method that we call covering technique.",
        "bibtex": "@InProceedings{pmlr-v20-shin11,\n  title = \t {Mapping kernels defined over countably infinite mapping systems and their application},\n  author = \t {Shin, Kilho},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {367--382},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/shin11/shin11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/shin11.html},\n  abstract = \t {The mapping kernel is a generalization of Haussler\u2019s convolution kernel, and has a wide range of application including kernels for higher degree structures such as trees. Like Haussler\u2019s convolution kernel, a mapping kernel is a finite sum of values of a primitive kernel. One of the major reasons to use the mapping kernel template in engineering novel kernels is because a strong theorem is known for positive definiteness of the resulting mapping kernels. If the mapping kernel meets the transitivity condition and if the primitive kernel is positive definite, the mapping kernel is also positive definite. In this paper, we generalize this theorem by showing, even when we extend the definition of mapping kernels so that a mapping kernel can be a converging sum of countably infinite primitive kernel values, the transitivity condition is still a criteria to determine positive definiteness of mapping kernels according to the extended definition. Interestingly, this result is also useful to investigate positive definiteness of mapping kernels determined as finite sums, when they do not meet the transitivity condition. For this purpose, we introduce a general method that we call covering technique.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/shin11/shin11.pdf",
        "supp": "",
        "pdf_size": 371106,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7890021322941312272&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Graduate School of Applied Informatics, University of Hyogo, Kobe, Japan",
        "aff_domain": "ai.u-hyogo.ac.jp",
        "email": "ai.u-hyogo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Hyogo",
        "aff_unique_dep": "Graduate School of Applied Informatics",
        "aff_unique_url": "https://www.u-hyogo.ac.jp",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Kobe",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "1f3dc6c5af",
        "title": "Microbagging Estimators: An Ensemble Approach to Distance-weighted Classifiers",
        "site": "https://proceedings.mlr.press/v20/nelson11.html",
        "author": "Blaine Nelson; Battista Biggio; Pavel Laskov",
        "abstract": "Support vector machines (SVMs) have been the predominate approach to kernel-based classification. While SVMs have demonstrated excellent performance in many application domains, they are known to be sensitive to noise in their training dataset. Motivated by the equalizing effect of bagging classifiers, we present a novel approach to kernel-based classification that we call microbagging. This method bags all possible maximal-margin estimators between pairs of training points to create a novel linear kernel classifier with weights defined directly as functions of the pairwise distance matrix induced by the kernel function. We derive relationships between linear and distance-based classifiers and empirically compare microbagging to the SVMs and robust SVMs on several datasets.",
        "bibtex": "@InProceedings{pmlr-v20-nelson11,\n  title = \t {Microbagging Estimators: An Ensemble Approach to Distance-weighted Classifiers},\n  author = \t {Nelson, Blaine and Biggio, Battista and Laskov, Pavel},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {63--79},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/nelson11/nelson11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/nelson11.html},\n  abstract = \t {Support vector machines (SVMs) have been the predominate approach to kernel-based classification. While SVMs have demonstrated excellent performance in many application domains, they are known to be sensitive to noise in their training dataset. Motivated by the equalizing effect of bagging classifiers, we present a novel approach to kernel-based classification that we call microbagging. This method bags all possible maximal-margin estimators between pairs of training points to create a novel linear kernel classifier with weights defined directly as functions of the pairwise distance matrix induced by the kernel function. We derive relationships between linear and distance-based classifiers and empirically compare microbagging to the SVMs and robust SVMs on several datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/nelson11/nelson11.pdf",
        "supp": "",
        "pdf_size": 790396,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=686060602798566837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Wilhelm Schickard Institute for Computer Science, University of T\u00fcbingen; Department of Electrical and Electronic Engineering, University of Cagliari; Wilhelm Schickard Institute for Computer Science, University of T\u00fcbingen",
        "aff_domain": "wsii.uni-tuebingen.de;diee.unica.it;uni-tuebingen.de",
        "email": "wsii.uni-tuebingen.de;diee.unica.it;uni-tuebingen.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of T\u00fcbingen;University of Cagliari",
        "aff_unique_dep": "Wilhelm Schickard Institute for Computer Science;Department of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.uni-tuebingen.de;https://www.unica.it",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;Italy"
    },
    {
        "id": "4057adbe51",
        "title": "Mixed-Variate Restricted Boltzmann Machines",
        "site": "https://proceedings.mlr.press/v20/tran11.html",
        "author": "Truyen Tran; Dinh Phung; Svetha Venkatesh",
        "abstract": "Modern datasets are becoming heterogeneous. To this end, we present in this paper Mixed-Variate Restricted Boltzmann Machines for simultaneously modelling variables of multiple types and modalities, including binary and continuous responses, categorical options, multicategorical choices, ordinal assessment and category-ranked preferences. Dependency among variables is modeled using latent binary variables, each of which can be interpreted as a particular hidden aspect of the data. The proposed model, similar to the standard RBMs, allows fast evaluation of the posterior for the latent variables. Hence, it is naturally suitable for many common tasks including, but not limited to, (a) as a pre-processing step to convert complex input data into a more convenient vectorial representation through the latent posteriors, thereby offering a dimensionality reduction capacity, (b) as a classifier supporting binary, multiclass, multilabel, and label-ranking outputs, or a regression tool for continuous outputs and (c) as a data completion tool for multimodal and heterogeneous data. We evaluate the proposed model on a large-scale dataset using the world opinion survey results on three tasks: feature extraction and visualization, data completion and prediction.",
        "bibtex": "@InProceedings{pmlr-v20-tran11,\n  title = \t {Mixed-Variate Restricted Boltzmann Machines},\n  author = \t {Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {213--229},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/tran11/tran11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/tran11.html},\n  abstract = \t {Modern datasets are becoming heterogeneous. To this end, we present in this paper Mixed-Variate Restricted Boltzmann Machines for simultaneously modelling variables of multiple types and modalities, including binary and continuous responses, categorical options, multicategorical choices, ordinal assessment and category-ranked preferences. Dependency among variables is modeled using latent binary variables, each of which can be interpreted as a particular hidden aspect of the data. The proposed model, similar to the standard RBMs, allows fast evaluation of the posterior for the latent variables. Hence, it is naturally suitable for many common tasks including, but not limited to, (a) as a pre-processing step to convert complex input data into a more convenient vectorial representation through the latent posteriors, thereby offering a dimensionality reduction capacity, (b) as a classifier supporting binary, multiclass, multilabel, and label-ranking outputs, or a regression tool for continuous outputs and (c) as a data completion tool for multimodal and heterogeneous data. We evaluate the proposed model on a large-scale dataset using the world opinion survey results on three tasks: feature extraction and visualization, data completion and prediction.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/tran11/tran11.pdf",
        "supp": "",
        "pdf_size": 477817,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8889154274309486051&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Curtin University; Curtin University; Curtin University",
        "aff_domain": "curtin.edu.au;curtin.edu.au;curtin.edu.au",
        "email": "curtin.edu.au;curtin.edu.au;curtin.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Curtin University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.curtin.edu.au",
        "aff_unique_abbr": "Curtin",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "1d5ef1e881",
        "title": "Multi-label Active Learning with Auxiliary Learner",
        "site": "https://proceedings.mlr.press/v20/hung11.html",
        "author": "Chen-Wei Hung; Hsuan-Tien Lin",
        "abstract": "Multi-label active learning is an important problem because of the expensive labeling cost in multi-label classification applications. A state-of-the-art approach for multi-label active learning, maximum loss reduction with maximum confidence (MMC), heavily depends on the binary relevance support vector machine in both learning and querying. Nevertheless, it is not clear whether the heavy dependence is necessary or unrivaled. In this work, we extend MMC to a more general framework that removes the heavy dependence and clarifies the roles of each component in MMC. In particular, the framework is characterized by a major learner for making predictions, an auxiliary learner for helping with query decisions and a query criterion based on the disagreement between the two learners. The framework takes MMC and several baseline multi-label active learning algorithms as special cases. With the flexibility of the general framework, we design two criteria other than the one used by MMC. We also explore the possibility of using learners other than the binary relevance support vector machine for multi-label active learning. Experimental results demonstrate that a new criterion, soft Hamming loss reduction, is usually better than the original MMC criterion across different pairs of major/auxiliary learners, and validate the usefulness of the proposed framework.",
        "bibtex": "@InProceedings{pmlr-v20-hung11,\n  title = \t {Multi-label Active Learning with Auxiliary Learner},\n  author = \t {Hung, Chen-Wei and Lin, Hsuan-Tien},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {315--332},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/hung11/hung11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/hung11.html},\n  abstract = \t {Multi-label active learning is an important problem because of the expensive labeling cost in multi-label classification applications. A state-of-the-art approach for multi-label active learning, maximum loss reduction with maximum confidence (MMC), heavily depends on the binary relevance support vector machine in both learning and querying. Nevertheless, it is not clear whether the heavy dependence is necessary or unrivaled. In this work, we extend MMC to a more general framework that removes the heavy dependence and clarifies the roles of each component in MMC. In particular, the framework is characterized by a major learner for making predictions, an auxiliary learner for helping with query decisions and a query criterion based on the disagreement between the two learners. The framework takes MMC and several baseline multi-label active learning algorithms as special cases. With the flexibility of the general framework, we design two criteria other than the one used by MMC. We also explore the possibility of using learners other than the binary relevance support vector machine for multi-label active learning. Experimental results demonstrate that a new criterion, soft Hamming loss reduction, is usually better than the original MMC criterion across different pairs of major/auxiliary learners, and validate the usefulness of the proposed framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/hung11/hung11.pdf",
        "supp": "",
        "pdf_size": 312027,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17328079145573475636&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan University; Department of Computer Science and Information Engineering, National Taiwan University",
        "aff_domain": "CSIE.NTU.EDU.TW;CSIE.NTU.EDU.TW",
        "email": "CSIE.NTU.EDU.TW;CSIE.NTU.EDU.TW",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "Department of Computer Science and Information Engineering",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "130a685f1e",
        "title": "Multi-label Classification with Error-correcting Codes",
        "site": "https://proceedings.mlr.press/v20/ferng11.html",
        "author": "Chung-Sung Ferng; Hsuan-Tien Lin",
        "abstract": "We formulate a framework for applying error-correcting codes (ECC) on multi-label classification problems. The framework treats some base learners as noisy channels and uses ECC to correct the prediction errors made by the learners. An immediate use of the framework is a novel ECC-based explanation of the popular random k-label-sets (RAKEL) algorithm using a simple repetition ECC. Using the framework, we empirically compare a broad spectrum of ECC designs for multi-label classification. The results not only demonstrate that RAKEL can be improved by applying some stronger ECC, but also show that the traditional Binary Relevance approach can be enhanced by learning more parity-checking labels. In addition, our study on different ECC helps understand the trade-off between the strength of ECC and the hardness of the base learning tasks.",
        "bibtex": "@InProceedings{pmlr-v20-ferng11,\n  title = \t {Multi-label Classification with Error-correcting Codes},\n  author = \t {Ferng, Chung-Sung and Lin, Hsuan-Tien},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {281--295},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/ferng11/ferng11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/ferng11.html},\n  abstract = \t {We formulate a framework for applying error-correcting codes (ECC) on multi-label classification problems. The framework treats some base learners as noisy channels and uses ECC to correct the prediction errors made by the learners. An immediate use of the framework is a novel ECC-based explanation of the popular random k-label-sets (RAKEL) algorithm using a simple repetition ECC. Using the framework, we empirically compare a broad spectrum of ECC designs for multi-label classification. The results not only demonstrate that RAKEL can be improved by applying some stronger ECC, but also show that the traditional Binary Relevance approach can be enhanced by learning more parity-checking labels. In addition, our study on different ECC helps understand the trade-off between the strength of ECC and the hardness of the base learning tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/ferng11/ferng11.pdf",
        "supp": "",
        "pdf_size": 400048,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2285217787392146957&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan University; Department of Computer Science and Information Engineering, National Taiwan University",
        "aff_domain": "csie.ntu.edu.tw;csie.ntu.edu.tw",
        "email": "csie.ntu.edu.tw;csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "Department of Computer Science and Information Engineering",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "22a016664e",
        "title": "Nonlinear Online Classification Algorithm with Probability Margin",
        "site": "https://proceedings.mlr.press/v20/chi11.html",
        "author": "Mingmin Chi; Huijun He; Wenqiang Zhang",
        "abstract": "Usually, it is necessary for nonlinear online learning algorithms to store a set of misclassified observed examples for computing kernel values. For large-scale problems, this is not only time consuming but leads also to an out-of-memory problem. In the paper, a nonlinear online classification algorithm is proposed with a probability margin to address the problem. In particular, the discriminant function is defined by the Gaussian mixture model with the statistical information of all the observed examples instead of data points. Then, the learnt model is used to train a nonlinear online classification algorithm with confidence such that the corresponding margin is defined by probability. When doing so, the internal memory is significantly reduced while the classification performance is kept. Also, we prove mistake bounds in terms of the generative model. Experiments carried out on one synthesis and two real large-scale data sets validate the effectiveness of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v20-chi11,\n  title = \t {Nonlinear Online Classification Algorithm with Probability Margin},\n  author = \t {Chi, Mingmin and He, Huijun and Zhang, Wenqiang},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {33--46},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/chi11/chi11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/chi11.html},\n  abstract = \t {Usually, it is necessary for nonlinear online learning algorithms to store a set of misclassified observed examples for computing kernel values. For large-scale problems, this is not only time consuming but leads also to an out-of-memory problem. In the paper, a nonlinear online classification algorithm is proposed with a probability margin to address the problem. In particular, the discriminant function is defined by the Gaussian mixture model with the statistical information of all the observed examples instead of data points. Then, the learnt model is used to train a nonlinear online classification algorithm with confidence such that the corresponding margin is defined by probability. When doing so, the internal memory is significantly reduced while the classification performance is kept. Also, we prove mistake bounds in terms of the generative model. Experiments carried out on one synthesis and two real large-scale data sets validate the effectiveness of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/chi11/chi11.pdf",
        "supp": "",
        "pdf_size": 426649,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11236944242386444450&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Fudan University, 825 Zhang Heng Road, Shanghai, 201203, China; School of Computer Science, Fudan University, 825 Zhang Heng Road, Shanghai, 201203, China; School of Computer Science, Fudan University, 825 Zhang Heng Road, Shanghai, 201203, China",
        "aff_domain": "fudan.edu.cn; ; ",
        "email": "fudan.edu.cn; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "5cf0682208",
        "title": "Preface",
        "site": "https://proceedings.mlr.press/v20/hsu11.html",
        "author": "Chun-Nan Hsu; Wee Sun Lee",
        "abstract": "Preface to the Proceedings of the 3rd Asian Conference on Machine Learning, November 13-15, Taoyuan, Taiwan.",
        "bibtex": "@InProceedings{pmlr-v20-hsu11,\n  title = \t {Preface},\n  author = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {i--xii},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/hsu11/hsu11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/hsu11.html},\n  abstract = \t {Preface to the Proceedings of the 3rd Asian Conference on Machine Learning, November 13-15, Taoyuan, Taiwan.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/hsu11/hsu11.pdf",
        "supp": "",
        "pdf_size": 0,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141043608347785016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0b61b68948",
        "title": "Quadratic Weighted Automata:Spectral Algorithm and Likelihood Maximization",
        "site": "https://proceedings.mlr.press/v20/bailly11.html",
        "author": "Raphael Bailly",
        "abstract": "In this paper, we address the problem of non-parametric density estimation on a set of strings $\\Sigma^*$. We introduce a probabilistic model - called quadratic weighted automaton, or QWA - and we present some methods which can be used in a density estimation task. A spectral analysis method leads to an effective regularization and a consistent estimate of the parameters. We provide a set of theoretical results on the convergence of this method. Experiments show that the combination of this method with likelihood maximization may be an interesting alternative to the well-known Baum-Welch algorithm.",
        "bibtex": "@InProceedings{pmlr-v20-bailly11,\n  title = \t {Quadratic Weighted Automata:Spectral Algorithm and Likelihood Maximization},\n  author = \t {Bailly, Raphael},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {147--163},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/bailly11/bailly11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/bailly11.html},\n  abstract = \t {In this paper, we address the problem of non-parametric density estimation on a set of strings $\\Sigma^*$. We introduce a probabilistic model - called quadratic weighted automaton, or QWA - and we present some methods which can be used in a density estimation task. A spectral analysis method leads to an effective regularization and a consistent estimate of the parameters. We provide a set of theoretical results on the convergence of this method. Experiments show that the combination of this method with likelihood maximization may be an interesting alternative to the well-known Baum-Welch algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/bailly11/bailly11.pdf",
        "supp": "",
        "pdf_size": 512400,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12700550643735043827&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Aix-Marseille Universit \u00b4e,UMR CNRS 6166, LIF , QARMA",
        "aff_domain": "LIF.UNIV-MRS.FR",
        "email": "LIF.UNIV-MRS.FR",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Aix-Marseille University",
        "aff_unique_dep": "Laboratoire d'Informatique Fondamentale",
        "aff_unique_url": "https://www.univ-amu.fr",
        "aff_unique_abbr": "AMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "id": "bc93ff0b12",
        "title": "Robust Generation of Dynamical Patterns in Human Motion by a Deep Belief Nets",
        "site": "https://proceedings.mlr.press/v20/sukhbaatar11.html",
        "author": "Sainbaya Sukhbaatar; Takaki Makino; Kazuyuki Aihara; Takashi Chikayama",
        "abstract": "We propose a Deep Belief Net model for robust motion generation, which consists of two layers of Restricted Boltzmann Machines (RBMs). The lower layer has multiple RBMs for encoding real-valued spatial patterns of motion frames into compact representations. The upper layer has one conditional RBM for learning temporal constraints on transitions between those compact representations. This separation of spatial and temporal learning makes it possible to reproduce many attractive dynamical behaviors such as walking by a stable limit cycle, a gait transition by bifurcation, synchronization of limbs by phase-locking, and easy top-down control. We trained the model with human motion capture data and the results of motion generation are reported here.",
        "bibtex": "@InProceedings{pmlr-v20-sukhbaatar11,\n  title = \t {Robust Generation of Dynamical Patterns in Human Motion by a Deep Belief Nets},\n  author = \t {Sukhbaatar, Sainbaya and Makino, Takaki and Aihara, Kazuyuki and Chikayama, Takashi},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {231--246},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/sukhbaatar11/sukhbaatar11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/sukhbaatar11.html},\n  abstract = \t {We propose a Deep Belief Net model for robust motion generation, which consists of two layers of Restricted Boltzmann Machines (RBMs). The lower layer has multiple RBMs for encoding real-valued spatial patterns of motion frames into compact representations. The upper layer has one conditional RBM for learning temporal constraints on transitions between those compact representations. This separation of spatial and temporal learning makes it possible to reproduce many attractive dynamical behaviors such as walking by a stable limit cycle, a gait transition by bifurcation, synchronization of limbs by phase-locking, and easy top-down control. We trained the model with human motion capture data and the results of motion generation are reported here.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/sukhbaatar11/sukhbaatar11.pdf",
        "supp": "",
        "pdf_size": 1323002,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9163498107276078320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematical Informatics, Graduate School of Information Science and Technology, The University of Tokyo; Collaborative Research Center for Innovative Mathematical Modelling, Institute of Industrial Science, The University of Tokyo; Institute of Industrial Science, The University of Tokyo; Department of Electrical Engineering and Information Systems, Graduate School of Engineering, The University of Tokyo",
        "aff_domain": "sat.t.u-tokyo.ac.jp;sat.t.u-tokyo.ac.jp;sat.t.u-tokyo.ac.jp;logos.ic.i.u-tokyo.ac.jp",
        "email": "sat.t.u-tokyo.ac.jp;sat.t.u-tokyo.ac.jp;sat.t.u-tokyo.ac.jp;logos.ic.i.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "Department of Mathematical Informatics, Graduate School of Information Science and Technology",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "51806c86b5",
        "title": "Summarization of Yes/No Questions Using a Feature Function Model",
        "site": "https://proceedings.mlr.press/v20/he11.html",
        "author": "Jing He; Decheng Dai",
        "abstract": "Answer summarization is an important problem in the study of Question and Answering. In this paper, we deal with the general questions with \u201cYes/No\u201d answers in English. We design 1) a model to score the relevance of the answers and the questions, and 2) a feature function combining the relevance and opinion scores to classify each answer to be \u201cYes\u201d, \u201cNo\u201d or \u201cNeutral\u201d. We combine the opinion features together with two weighting scores to solve this problem and conduct experiments on a real word dataset. Given an input question, the system firstly detects if it can be simply answered by \u201cYes/No\u201d or not, and then outputs the resulting voting numbers of \u201cYes\u201d answers and \u201cNo\u201d answers to this question. We also first proposed the accuracy, precision, and recall to the \u201cYes/No\u201d answer detection.",
        "bibtex": "@InProceedings{pmlr-v20-he11,\n  title = \t {Summarization of Yes/No Questions Using a Feature Function Model},\n  author = \t {He, Jing and Dai, Decheng},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {351--366},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/he11/he11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/he11.html},\n  abstract = \t {Answer summarization is an important problem in the study of Question and Answering. In this paper, we deal with the general questions with \u201cYes/No\u201d answers in English. We design 1) a model to score the relevance of the answers and the questions, and 2) a feature function combining the relevance and opinion scores to classify each answer to be \u201cYes\u201d, \u201cNo\u201d or \u201cNeutral\u201d. We combine the opinion features together with two weighting scores to solve this problem and conduct experiments on a real word dataset. Given an input question, the system firstly detects if it can be simply answered by \u201cYes/No\u201d or not, and then outputs the resulting voting numbers of \u201cYes\u201d answers and \u201cNo\u201d answers to this question. We also first proposed the accuracy, precision, and recall to the \u201cYes/No\u201d answer detection.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/he11/he11.pdf",
        "supp": "",
        "pdf_size": 328043,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7008446088848550881&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Google Inc.; Google Inc.",
        "aff_domain": "gmail.com;google.com",
        "email": "gmail.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a085a99ca2",
        "title": "Support Vector Machines Under Adversarial Label Noise",
        "site": "https://proceedings.mlr.press/v20/biggio11.html",
        "author": "Battista Biggio; Blaine Nelson; Pavel Laskov",
        "abstract": "In adversarial classification tasks like spam filtering and intrusion detection, malicious adversaries may manipulate data to thwart the outcome of an automatic analysis. Thus, besides achieving good classification performances, machine learning algorithms have to be robust against adversarial data manipulation to successfully operate in these tasks. While support vector machines (SVMs) have shown to be a very successful approach in classification problems, their effectiveness in adversarial classification tasks has not been extensively investigated yet. In this paper we present a preliminary investigation of the robustness of SVMs against adversarial data manipulation. In particular, we assume that the adversary has control over some training data, and aims to subvert the SVM learning process. Within this assumption, we show that this is indeed possible, and propose a strategy to improve the robustness of SVMs to training data manipulation based on a simple kernel matrix correction.",
        "bibtex": "@InProceedings{pmlr-v20-biggio11,\n  title = \t {Support Vector Machines Under Adversarial Label Noise},\n  author = \t {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {97--112},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/biggio11/biggio11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/biggio11.html},\n  abstract = \t {In adversarial classification tasks like spam filtering and intrusion detection, malicious adversaries may manipulate data to thwart the outcome of an automatic analysis. Thus, besides achieving good classification performances, machine learning algorithms have to be robust against adversarial data manipulation to successfully operate in these tasks. While support vector machines (SVMs) have shown to be a very successful approach in classification problems, their effectiveness in adversarial classification tasks has not been extensively investigated yet. In this paper we present a preliminary investigation of the robustness of SVMs against adversarial data manipulation. In particular, we assume that the adversary has control over some training data, and aims to subvert the SVM learning process. Within this assumption, we show that this is indeed possible, and propose a strategy to improve the robustness of SVMs to training data manipulation based on a simple kernel matrix correction.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/biggio11/biggio11.pdf",
        "supp": "",
        "pdf_size": 546771,
        "gs_citation": 553,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=409352622725846936&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Electrical and Electronic Engineering, University of Cagliari; Dept. of Mathematics and Natural Sciences, Eberhard-Karls-Universit\u00e4t T\u00fcbingen; Dept. of Mathematics and Natural Sciences, Eberhard-Karls-Universit\u00e4t T\u00fcbingen",
        "aff_domain": "diee.unica.it;wsii.uni-tuebingen.de;uni-tuebingen.de",
        "email": "diee.unica.it;wsii.uni-tuebingen.de;uni-tuebingen.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Cagliari;Eberhard-Karls-Universit\u00e4t T\u00fcbingen",
        "aff_unique_dep": "Dept. of Electrical and Electronic Engineering;Dept. of Mathematics and Natural Sciences",
        "aff_unique_url": "https://www.unica.it;https://www.uni-tuebingen.de/",
        "aff_unique_abbr": ";Uni T\u00fcbingen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Italy;Germany"
    },
    {
        "id": "18465486c4",
        "title": "Unsupervised Multiple Kernel Learning",
        "site": "https://proceedings.mlr.press/v20/zhuang11.html",
        "author": "Jinfeng Zhuang; Jialei Wang; Steven C. H. Hoi; Xiangyang Lan",
        "abstract": "Traditional multiple kernel learning (MKL) algorithms are essentially supervised learning in the sense that the kernel learning task requires the class labels of training data. However, class labels may not always be available prior to the kernel learning task in some real world scenarios, e.g., an early preprocessing step of a classification task or an unsupervised learning task such as dimension reduction. In this paper, we investigate a problem of Unsupervised Multiple Kernel Learning (UMKL), which does not require class labels of training data as needed in a conventional multiple kernel learning task. Since a kernel essentially defines pairwise similarity between any two examples, our unsupervised kernel learning method mainly follows two intuitive principles: (1) a good kernel should allow every example to be well reconstructed from its localized bases weighted by the kernel values; (2) a good kernel should induce kernel values that are coincided with the local geometry of the data. We formulate the unsupervised multiple kernel learning problem as an optimization task and propose an efficient alternating optimization algorithm to solve it. Empirical results on both classification and dimension reductions tasks validate the efficacy of the proposed UMKL algorithm.",
        "bibtex": "@InProceedings{pmlr-v20-zhuang11,\n  title = \t {Unsupervised Multiple Kernel Learning},\n  author = \t {Zhuang, Jinfeng and Wang, Jialei and Hoi, Steven C. H. and Lan, Xiangyang},\n  booktitle = \t {Proceedings of the Asian Conference on Machine Learning},\n  pages = \t {129--144},\n  year = \t {2011},\n  editor = \t {Hsu, Chun-Nan and Lee, Wee Sun},\n  volume = \t {20},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {South Garden Hotels and Resorts, Taoyuan, Taiwain},\n  month = \t {14--15 Nov},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v20/zhuang11/zhuang11.pdf},\n  url = \t {https://proceedings.mlr.press/v20/zhuang11.html},\n  abstract = \t {Traditional multiple kernel learning (MKL) algorithms are essentially supervised learning in the sense that the kernel learning task requires the class labels of training data. However, class labels may not always be available prior to the kernel learning task in some real world scenarios, e.g., an early preprocessing step of a classification task or an unsupervised learning task such as dimension reduction. In this paper, we investigate a problem of Unsupervised Multiple Kernel Learning (UMKL), which does not require class labels of training data as needed in a conventional multiple kernel learning task. Since a kernel essentially defines pairwise similarity between any two examples, our unsupervised kernel learning method mainly follows two intuitive principles: (1) a good kernel should allow every example to be well reconstructed from its localized bases weighted by the kernel values; (2) a good kernel should induce kernel values that are coincided with the local geometry of the data. We formulate the unsupervised multiple kernel learning problem as an optimization task and propose an efficient alternating optimization algorithm to solve it. Empirical results on both classification and dimension reductions tasks validate the efficacy of the proposed UMKL algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v20/zhuang11/zhuang11.pdf",
        "supp": "",
        "pdf_size": 367344,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17533001496835319357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; Department of Computer Science, Cornell University, USA",
        "aff_domain": "NTU.EDU.SG;NTU.EDU.SG;NTU.EDU.SG;CS.CORNELL.EDU",
        "email": "NTU.EDU.SG;NTU.EDU.SG;NTU.EDU.SG;CS.CORNELL.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Nanyang Technological University;Cornell University",
        "aff_unique_dep": "School of Computer Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.cornell.edu",
        "aff_unique_abbr": "NTU;Cornell",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Singapore;United States"
    }
]