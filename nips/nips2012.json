[
    {
        "id": "0c7ed8c242",
        "title": "3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/839ab46820b524afda05122893c2fe8e-Abstract.html",
        "author": "Sanja Fidler; Sven Dickinson; Raquel Urtasun",
        "abstract": "This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects  in 3D by enclosing them with tight  oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model[Felz.] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are  continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2D[Felz09] and 3D object detection[Hedau12].",
        "bibtex": "@inproceedings{NIPS2012_839ab468,\n author = {Fidler, Sanja and Dickinson, Sven and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/839ab46820b524afda05122893c2fe8e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/839ab46820b524afda05122893c2fe8e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/839ab46820b524afda05122893c2fe8e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 8823950,
        "gs_citation": 270,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=96153339602932455&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "TTI Chicago; University of Toronto; TTI Chicago",
        "aff_domain": "ttic.edu;cs.toronto.edu;ttic.edu",
        "email": "ttic.edu;cs.toronto.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Toronto",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.utoronto.ca",
        "aff_unique_abbr": "TTI;U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "698da44a65",
        "title": "3D Social Saliency from Head-mounted Cameras",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1bf2efbbe0c49b9f567c2e40f645279a-Abstract.html",
        "author": "Hyun S. Park; Eakta Jain; Yaser Sheikh",
        "abstract": "Yaser Sheikh",
        "bibtex": "@inproceedings{NIPS2012_1bf2efbb,\n author = {Park, Hyun and Jain, Eakta and Sheikh, Yaser},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {3D Social Saliency from Head-mounted Cameras},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1bf2efbbe0c49b9f567c2e40f645279a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1bf2efbbe0c49b9f567c2e40f645279a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1bf2efbbe0c49b9f567c2e40f645279a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1bf2efbbe0c49b9f567c2e40f645279a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3452326,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2531308731353934514&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Carnegie Mellon University; Texas Instruments; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;ti.com;cs.cmu.edu",
        "email": "cs.cmu.edu;ti.com;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Texas Instruments",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.ti.com",
        "aff_unique_abbr": "CMU;TI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3f4280bf40",
        "title": "A Bayesian Approach for Policy Learning from Trajectory Preference Queries",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/16c222aa19898e5058938167c8ab6c57-Abstract.html",
        "author": "Aaron Wilson; Alan Fern; Prasad Tadepalli",
        "abstract": "We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates the preferred trajectory. The agent's goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efficient than random selection.",
        "bibtex": "@inproceedings{NIPS2012_16c222aa,\n author = {Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian Approach for Policy Learning from Trajectory Preference Queries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/16c222aa19898e5058938167c8ab6c57-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/16c222aa19898e5058938167c8ab6c57-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/16c222aa19898e5058938167c8ab6c57-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 911223,
        "gs_citation": 233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1224343667038646716&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of EECS, Oregon State University; School of EECS, Oregon State University; School of EECS, Oregon State University",
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://eecs.oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9bb4d7b143",
        "title": "A Better Way to Pretrain Deep Boltzmann Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7d771e0e8f3633ab54856925ecdefc5d-Abstract.html",
        "author": "Geoffrey E. Hinton; Ruslan Salakhutdinov",
        "abstract": "We describe how the pre-training algorithm for Deep Boltzmann Machines (DBMs) is related to the pre-training algorithm for Deep Belief Networks and we show that under certain conditions, the pre-training procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pre-training DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pre-training algorithm allows us to learn better generative models.",
        "bibtex": "@inproceedings{NIPS2012_7d771e0e,\n author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Better Way to Pretrain Deep Boltzmann Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7d771e0e8f3633ab54856925ecdefc5d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7d771e0e8f3633ab54856925ecdefc5d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7d771e0e8f3633ab54856925ecdefc5d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 486775,
        "gs_citation": 188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9430001092226333900&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Statistics and Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Statistics and Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "ae985f0334",
        "title": "A Conditional Multinomial Mixture Model for Superset Label Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Abstract.html",
        "author": "Liping Liu; Thomas G. Dietterich",
        "abstract": "In the superset label learning problem (SLL), each training instance provides a set of candidate labels of which one is the true label of the instance. As in ordinary regression, the candidate label set is a noisy version of the true label. In this work, we solve the problem by maximizing the likelihood of the candidate label sets of training instances. We propose a probabilistic model, the Logistic Stick- Breaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSB- CMM is derived from the logistic stick-breaking process. It \ufb01rst maps data points to mixture components and then assigns to each mixture component a label drawn from a component-speci\ufb01c multinomial distribution. The mixture components can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art. The discovered underlying structures also provide improved explanations of the classi\ufb01cation predictions.",
        "bibtex": "@inproceedings{NIPS2012_aaebdb8b,\n author = {Liu, Liping and Dietterich, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Conditional Multinomial Mixture Model for Superset Label Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 288338,
        "gs_citation": 289,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10468328394659407633&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "EECS, Oregon State University; EECS, Oregon State University",
        "aff_domain": "eecs.oregonstate.edu;cs.orst.edu",
        "email": "eecs.oregonstate.edu;cs.orst.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "EECS",
        "aff_unique_url": "https://eecs.oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "64b5a7e414",
        "title": "A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/58d4d1e7b1e97b258c9ed0b37e02d087-Abstract.html",
        "author": "Aaron Defazio; Tib\u00e9rio S. Caetano",
        "abstract": "A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lovasz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.",
        "bibtex": "@inproceedings{NIPS2012_58d4d1e7,\n author = {Defazio, Aaron and Caetano, Tib\\'{e}rio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/58d4d1e7b1e97b258c9ed0b37e02d087-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/58d4d1e7b1e97b258c9ed0b37e02d087-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 373091,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15914744473690624137&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "NICTA/Australian National University; NICTA/ANU/University of Sydney",
        "aff_domain": "anu.edu.au;nicta.com.au",
        "email": "anu.edu.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "d6cd9674d0",
        "title": "A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/184260348236f9554fe9375772ff966e-Abstract.html",
        "author": "Cho-jui Hsieh; Arindam Banerjee; Inderjit S. Dhillon; Pradeep K. Ravikumar",
        "abstract": "In this paper, we consider the $\\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables. Even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field. Our proposed algorithm divides the problem into  smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. We derive a bound on the distance of the approximate solution to the true solution. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables. We further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure.  As an example, a recent state-of-the-art method, QUIC requires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, Divide and Conquer QUIC (DC-QUIC) only requires one hour to solve the problem.",
        "bibtex": "@inproceedings{NIPS2012_18426034,\n author = {Hsieh, Cho-jui and Banerjee, Arindam and Dhillon, Inderjit and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/184260348236f9554fe9375772ff966e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/184260348236f9554fe9375772ff966e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/184260348236f9554fe9375772ff966e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/184260348236f9554fe9375772ff966e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 351320,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8092942730636752054&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Computer Science, University of Texas, Austin; Dept. of Computer Science, University of Texas, Austin; Dept. of Computer Science, University of Texas; Dept. of Computer Science & Engineering, University of Minnesota, Twin Cities",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;cs.umn.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;cs.umn.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Texas at Austin;University of Texas;University of Minnesota",
        "aff_unique_dep": "Department of Computer Science;Dept. of Computer Science;Department of Computer Science & Engineering",
        "aff_unique_url": "https://www.utexas.edu;https://www.utexas.edu;https://www.umn.edu",
        "aff_unique_abbr": "UT Austin;UT;UMN",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Austin;;Twin Cities",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fc0fb6a766",
        "title": "A Generative Model for Parts-based Object Segmentation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/72b32a1f754ba1c09b3695e0cb6cde7f-Abstract.html",
        "author": "S. Eslami; Christopher Williams",
        "abstract": "The Shape Boltzmann Machine (SBM) has recently been introduced as a state-of-the-art model of foreground/background object shape. We extend the SBM to account for the foreground object's parts. Our model, the Multinomial SBM (MSBM), can capture both local and global statistics of part shapes accurately. We combine the MSBM with an appearance model to form a fully generative model of images of objects. Parts-based image segmentations are obtained simply by performing probabilistic inference in the model. We apply the model to two challenging datasets which exhibit significant shape and appearance variability, and find that it obtains results that are comparable to the state-of-the-art.",
        "bibtex": "@inproceedings{NIPS2012_72b32a1f,\n author = {Eslami, S. and Williams, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Generative Model for Parts-based Object Segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/72b32a1f754ba1c09b3695e0cb6cde7f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/72b32a1f754ba1c09b3695e0cb6cde7f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/72b32a1f754ba1c09b3695e0cb6cde7f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/72b32a1f754ba1c09b3695e0cb6cde7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2234401,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=945185816483740381&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;inf.ed.ac.uk",
        "email": "sms.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "319f698b25",
        "title": "A Geometric take on Metric Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html",
        "author": "S\u00f8ren Hauberg; Oren Freifeld; Michael J. Black",
        "abstract": "Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.",
        "bibtex": "@inproceedings{NIPS2012_ec5aa0b7,\n author = {Hauberg, S\\o ren and Freifeld, Oren and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Geometric take on Metric Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/ec5aa0b7846082a2415f0902f0da88f2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ec5aa0b7846082a2415f0902f0da88f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1155762,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8378354246660382202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "MPI for Intelligent Systems, T\u00a8ubingen, Germany; Brown University, Providence, US; MPI for Intelligent Systems, T\u00a8ubingen, Germany",
        "aff_domain": "tue.mpg.de;dam.brown.edu;tue.mpg.de",
        "email": "tue.mpg.de;dam.brown.edu;tue.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Brown University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.brown.edu",
        "aff_unique_abbr": "MPI-IS;Brown",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "T\u00fcbingen;Providence",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "d812c1db7d",
        "title": "A Linear Time Active Learning Algorithm for Link Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html",
        "author": "Nicol\u00f2 Cesa-bianchi; Claudio Gentile; Fabio Vitale; Giovanni Zappella",
        "abstract": "We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V,E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\\log|V|$.",
        "bibtex": "@inproceedings{NIPS2012_bf62768c,\n author = {Cesa-bianchi, Nicol\\`{o} and Gentile, Claudio and Vitale, Fabio and Zappella, Giovanni},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Linear Time Active Learning Algorithm for Link Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/bf62768ca46b6c3b5bea9515d1a1fc45-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 389205,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5d3ed1e958",
        "title": "A Marginalized Particle Gaussian Process Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html",
        "author": "Yali Wang; Brahim Chaib-draa",
        "abstract": "We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods.",
        "bibtex": "@inproceedings{NIPS2012_9ad6aaed,\n author = {Wang, Yali and Chaib-draa, Brahim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Marginalized Particle Gaussian Process Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9ad6aaed513b73148b7d49f70afcfb32-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 290214,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7494686398059059905&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Laval University; Department of Computer Science, Laval University",
        "aff_domain": "damas.ift.ulaval.ca;damas.ift.ulaval.ca",
        "email": "damas.ift.ulaval.ca;damas.ift.ulaval.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Laval University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.laval.ca",
        "aff_unique_abbr": "Laval",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "f1d3f4b574",
        "title": "A Neural Autoregressive Topic Model",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b495ce63ede0f4efc9eec62cb947c162-Abstract.html",
        "author": "Hugo Larochelle; Stanislas Lauly",
        "abstract": "We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm.",
        "bibtex": "@inproceedings{NIPS2012_b495ce63,\n author = {Larochelle, Hugo and Lauly, Stanislas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Neural Autoregressive Topic Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/b495ce63ede0f4efc9eec62cb947c162-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b495ce63ede0f4efc9eec62cb947c162-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 485014,
        "gs_citation": 303,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2858355605076963714&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "D\u00b4epartement d\u2019informatique, Universit \u00b4e de Sherbrooke; D\u00b4epartement d\u2019informatique, Universit \u00b4e de Sherbrooke",
        "aff_domain": "usherbrooke.ca;usherbrooke.ca",
        "email": "usherbrooke.ca;usherbrooke.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit u00e9 de Sherbrooke",
        "aff_unique_dep": "D u00e9partement d\u2019informatique",
        "aff_unique_url": "https://www.usherbrooke.ca",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "3063f2aec9",
        "title": "A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a9be4c2a4041cadbf9d61ae16dd1389e-Abstract.html",
        "author": "Pedro Ortega; Jordi Grau-moya; Tim Genewein; David Balduzzi; Daniel Braun",
        "abstract": "We propose a novel Bayesian approach to solve stochastic optimization problems that involve \ufb01nding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of \ufb01rst, doing inference over the function space and second, \ufb01nding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf\ufb01cient statistic is composed of the observed function values. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function.",
        "bibtex": "@inproceedings{NIPS2012_a9be4c2a,\n author = {Ortega, Pedro and Grau-moya, Jordi and Genewein, Tim and Balduzzi, David and Braun, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a9be4c2a4041cadbf9d61ae16dd1389e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a9be4c2a4041cadbf9d61ae16dd1389e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a9be4c2a4041cadbf9d61ae16dd1389e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 355667,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17699768765981428562&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Max Planck Institute for Intelligent Systems + Max Planck Institute for Biolog. Cybernetics; Max Planck Institute for Intelligent Systems + Max Planck Institute for Biolog. Cybernetics; Max Planck Institute for Intelligent Systems + Max Planck Institute for Biolog. Cybernetics; Max Planck Institute for Intelligent Systems; Max Planck Institute for Intelligent Systems + Max Planck Institute for Biolog. Cybernetics",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0;0+1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Intelligent Systems;Biological Cybernetics",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.biologie.kit.edu",
        "aff_unique_abbr": "MPI-IS;MPIBC",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "9251d82ae5",
        "title": "A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c042f4db68f23406c6cecf84a7ebb0fe-Abstract.html",
        "author": "Pieter-jan Kindermans; Hannes Verschore; David Verstraeten; Benjamin Schrauwen",
        "abstract": "The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session.",
        "bibtex": "@inproceedings{NIPS2012_c042f4db,\n author = {Kindermans, Pieter-jan and Verschore, Hannes and Verstraeten, David and Schrauwen, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c042f4db68f23406c6cecf84a7ebb0fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 569681,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11117883160045402084&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Ghent University, Electronics and Information Systems; Ghent University, Electronics and Information Systems; Ghent University, Electronics and Information Systems; Ghent University, Electronics and Information Systems",
        "aff_domain": "UGent.be; ; ; ",
        "email": "UGent.be; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Ghent University",
        "aff_unique_dep": "Electronics and Information Systems",
        "aff_unique_url": "https://www.ugent.be",
        "aff_unique_abbr": "UGent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "3178d953a3",
        "title": "A Polylog Pivot Steps Simplex Algorithm for Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d296c101daa88a51f6ca8cfc1ac79b50-Abstract.html",
        "author": "Elad Hazan; Zohar Karnin",
        "abstract": "We present a simplex algorithm for linear programming in a linear classification formulation. The paramount complexity parameter in linear classification problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear. This is in contrast to general linear programming, for which no sub-polynomial pivot rule is known.",
        "bibtex": "@inproceedings{NIPS2012_d296c101,\n author = {Hazan, Elad and Karnin, Zohar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Polylog Pivot Steps Simplex Algorithm for Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d296c101daa88a51f6ca8cfc1ac79b50-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 241755,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:aozSzIq68wgJ:scholar.google.com/&scioq=A+Polylog+Pivot+Steps+Simplex+Algorithm+for+Classification&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Technion - Israel Inst. of Tech.; Yahoo! Research",
        "aff_domain": "ie.technion.ac.il;ymail.com",
        "email": "ie.technion.ac.il;ymail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Yahoo!",
        "aff_unique_dep": ";Yahoo! Research",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://research.yahoo.com",
        "aff_unique_abbr": "Technion;Yahoo!",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "afb3e5c7a7",
        "title": "A Polynomial-time Form of Robust Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html",
        "author": "Yao-liang Yu; \u00d6zlem Aslan; Dale Schuurmans",
        "abstract": "Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach  compared to standard methods.",
        "bibtex": "@inproceedings{NIPS2012_ae5e3ce4,\n author = {Yu, Yao-liang and Aslan, \\\"{O}zlem and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Polynomial-time Form of Robust Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/ae5e3ce40e0404a45ecacaaf05e5f735-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ae5e3ce40e0404a45ecacaaf05e5f735-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 541170,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1702325019987214296&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada; Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada; Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "51fc5de3f4",
        "title": "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/34ed066df378efacc9b924ec161e7639-Abstract.html",
        "author": "Shusen Wang; Zhihua Zhang",
        "abstract": "The CUR matrix decomposition is an important extension of Nystr\u00f6m approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithms that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms.",
        "bibtex": "@inproceedings{NIPS2012_34ed066d,\n author = {Wang, Shusen and Zhang, Zhihua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/34ed066df378efacc9b924ec161e7639-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/34ed066df378efacc9b924ec161e7639-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/34ed066df378efacc9b924ec161e7639-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 199339,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5018928521867191305&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "College of Computer Science & Technology, Zhejiang University; College of Computer Science & Technology, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Computer Science & Technology",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "6629639fbb",
        "title": "A Simple and Practical Algorithm for Differentially Private Data Release",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/208e43f0e45c4c78cafadb83d2888cb6-Abstract.html",
        "author": "Moritz Hardt; Katrina Ligett; Frank Mcsherry",
        "abstract": "We present a new algorithm for differentially private data release, based on a simple combination of the Exponential Mechanism with the Multiplicative Weights update rule.  Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques.",
        "bibtex": "@inproceedings{NIPS2012_208e43f0,\n author = {Hardt, Moritz and Ligett, Katrina and Mcsherry, Frank},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Simple and Practical Algorithm for Differentially Private Data Release},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/208e43f0e45c4c78cafadb83d2888cb6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 477942,
        "gs_citation": 602,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5024449150180823454&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "IBM Almaden Research; Caltech + Computer Science Department, Cornell University; Microsoft Research SVC",
        "aff_domain": "us.ibm.com;caltech.edu;microsoft.com",
        "email": "us.ibm.com;caltech.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;3",
        "aff_unique_norm": "IBM;California Institute of Technology;Cornell University;Microsoft",
        "aff_unique_dep": "Research;;Computer Science Department;Microsoft Research SVC",
        "aff_unique_url": "https://www.ibm.com/research;https://www.caltech.edu;https://www.cornell.edu;https://www.microsoft.com/en-us/research/group/silicon-valley",
        "aff_unique_abbr": "IBM;Caltech;Cornell;MSR SVC",
        "aff_campus_unique_index": "0;1;3",
        "aff_campus_unique": "Almaden;Pasadena;;Silicon Valley",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "91a7051eaa",
        "title": "A Spectral Algorithm for Latent Dirichlet Allocation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/15d4e891d784977cacbfcbb00c48f133-Abstract.html",
        "author": "Anima Anandkumar; Dean P. Foster; Daniel J. Hsu; Sham M. Kakade; Yi-kai Liu",
        "abstract": "Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by \\emph{multiple} latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden.  This work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (\\emph{i.e.}, third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on $k \\times k$ matrices, where $k$ is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.",
        "bibtex": "@inproceedings{NIPS2012_15d4e891,\n author = {Anandkumar, Anima and Foster, Dean P and Hsu, Daniel J and Kakade, Sham M and Liu, Yi-kai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Spectral Algorithm for Latent Dirichlet Allocation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/15d4e891d784977cacbfcbb00c48f133-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/15d4e891d784977cacbfcbb00c48f133-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 233030,
        "gs_citation": 346,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2971709680458962730&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 31,
        "aff": "University of California, Irvine, CA; University of Pennsylvania, Philadelphia, PA; Microsoft Research, Cambridge, MA; Microsoft Research, Cambridge, MA; National Institute of Standards and Technology, Gaithersburg, MD",
        "aff_domain": "uci.edu;foster.net;microsoft.com;microsoft.com;nist.gov",
        "email": "uci.edu;foster.net;microsoft.com;microsoft.com;nist.gov",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;3",
        "aff_unique_norm": "University of California, Irvine;University of Pennsylvania;Microsoft;National Institute of Standards and Technology",
        "aff_unique_dep": ";;Microsoft Research;",
        "aff_unique_url": "https://www.uci.edu;https://www.upenn.edu;https://www.microsoft.com/en-us/research;https://www.nist.gov",
        "aff_unique_abbr": "UCI;UPenn;MSR;NIST",
        "aff_campus_unique_index": "0;1;2;2;3",
        "aff_campus_unique": "Irvine;Philadelphia;Cambridge;Gaithersburg",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b025264928",
        "title": "A Stochastic Gradient Method with an Exponential Convergence _Rate for Finite Training Sets",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/905056c1ac1dad141560467e0a99e1cf-Abstract.html",
        "author": "Nicolas L. Roux; Mark Schmidt; Francis R. Bach",
        "abstract": "We propose a new stochastic gradient method for optimizing the sum of\u2029 a finite set of smooth functions, where the sum is strongly convex.\u2029 While standard stochastic gradient methods\u2029 converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence \u2029rate.  In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard\u2029 algorithms, both in terms of optimizing the training error and reducing the test error quickly.",
        "bibtex": "@inproceedings{NIPS2012_905056c1,\n author = {Roux, Nicolas and Schmidt, Mark and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Stochastic Gradient Method with an Exponential Convergence \\_Rate for Finite Training Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/905056c1ac1dad141560467e0a99e1cf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/905056c1ac1dad141560467e0a99e1cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 329419,
        "gs_citation": 1122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1209759207862565895&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "SIERRA Project-Team, INRIA - ENS, Paris, France; SIERRA Project-Team, INRIA - ENS, Paris, France; SIERRA Project-Team, INRIA - ENS, Paris, France",
        "aff_domain": "le-roux.name;inria.fr;ens.fr",
        "email": "le-roux.name;inria.fr;ens.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA - ENS",
        "aff_unique_dep": "SIERRA Project-Team",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "dd02e25ee2",
        "title": "A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/17326d10d511828f6b34fa6d751739e2-Abstract.html",
        "author": "Thomas Furmston; David Barber",
        "abstract": "Parametric policy search algorithms are one of the methods of choice for the optimisation of Markov Decision Processes, with Expectation Maximisation and natural gradient ascent being considered the current state of the art in the field. In this article we provide a unifying perspective of these two algorithms by showing that their step-directions in the parameter space are closely related to the search direction of an approximate Newton method. This analysis leads naturally to the consideration of this approximate Newton method as an alternative gradient-based method for Markov Decision Processes. We are able show that the algorithm has numerous desirable properties, absent in the naive application of Newton's method, that make it a viable alternative to either Expectation Maximisation or natural gradient ascent. Empirical results suggest that the algorithm has excellent convergence and robustness properties, performing strongly in comparison to both Expectation Maximisation and natural gradient ascent.",
        "bibtex": "@inproceedings{NIPS2012_17326d10,\n author = {Furmston, Thomas and Barber, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/17326d10d511828f6b34fa6d751739e2-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/17326d10d511828f6b34fa6d751739e2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/17326d10d511828f6b34fa6d751739e2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/17326d10d511828f6b34fa6d751739e2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1392017,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6996051743623249397&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University College London; Department of Computer Science, University College London",
        "aff_domain": "cs.ucl.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "674224d71f",
        "title": "A dynamic excitatory-inhibitory network in a VLSI chip for spiking information reregistrations",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/12780ea688a71dabc284b064add459a4-Abstract.html",
        "author": "Juan Huo",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS2012_12780ea6,\n author = {Huo, Juan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A dynamic excitatory-inhibitory network in a VLSI chip for spiking information reregistrations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/12780ea688a71dabc284b064add459a4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/12780ea688a71dabc284b064add459a4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 0,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=95910173007137055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e4c1dd58ae",
        "title": "A latent factor model for highly multi-relational data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0a1bf96b7165e962e90cb14648c9462d-Abstract.html",
        "author": "Rodolphe Jenatton; Nicolas L. Roux; Antoine Bordes; Guillaume R. Obozinski",
        "abstract": "Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations.",
        "bibtex": "@inproceedings{NIPS2012_0a1bf96b,\n author = {Jenatton, Rodolphe and Roux, Nicolas and Bordes, Antoine and Obozinski, Guillaume R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A latent factor model for highly multi-relational data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0a1bf96b7165e962e90cb14648c9462d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 330288,
        "gs_citation": 535,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1447473044289475803&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "CMAP, UMR CNRS 7641, Ecole Polytechnique, Palaiseau, France; INRIA - SIERRA Project Team, Ecole Normale Sup\u00e9rieure, Paris, France; Heudiasyc, UMR CNRS 7253, Universit\u00e9 de Technologie de Compi\u00e8gne, France; INRIA - SIERRA Project Team, Ecole Normale Sup\u00e9rieure, Paris, France",
        "aff_domain": "cmap.polytechnique.fr;le-roux.name;utc.fr;ens.fr",
        "email": "cmap.polytechnique.fr;le-roux.name;utc.fr;ens.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Ecole Polytechnique;INRIA;Universit\u00e9 de Technologie de Compi\u00e8gne",
        "aff_unique_dep": "CMAP, UMR CNRS 7641;SIERRA Project Team;Heudiasyc, UMR CNRS 7253",
        "aff_unique_url": "https://www.polytechnique.edu;https://www.inria.fr;https://www.utc.fr",
        "aff_unique_abbr": "Polytechnique;INRIA;",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Palaiseau;Paris;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "d9a5e35f0e",
        "title": "A lattice filter model of the visual pathway",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/31839b036f63806cba3f47b93af8ccb5-Abstract.html",
        "author": "Karol Gregor; Dmitri B. Chklovskii",
        "abstract": "Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (LGN) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons. We propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals. The stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> LGN) and its filter weights can be learned using Hebbian rules in a stage-wise sequential manner. Moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in LGN neurons and fruit fly second-order visual neurons. Therefore, the lattice filter model is a useful abstraction that may help unravel visual system function.",
        "bibtex": "@inproceedings{NIPS2012_31839b03,\n author = {Gregor, Karol and Chklovskii, Dmitri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A lattice filter model of the visual pathway},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/31839b036f63806cba3f47b93af8ccb5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 694177,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=93999019731370292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "123673e191",
        "title": "A mechanistic model of early sensory processing based on subtracting sparse representations",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/92977ae4d2ba21425a59afb269c2a14e-Abstract.html",
        "author": "Shaul Druckmann; Tao Hu; Dmitri B. Chklovskii",
        "abstract": "Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics.",
        "bibtex": "@inproceedings{NIPS2012_92977ae4,\n author = {Druckmann, Shaul and Hu, Tao and Chklovskii, Dmitri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A mechanistic model of early sensory processing based on subtracting sparse representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/92977ae4d2ba21425a59afb269c2a14e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/92977ae4d2ba21425a59afb269c2a14e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 7550279,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2635092619045614782&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Janelia Farm Research Campus; Janelia Farm Research Campus; Janelia Farm Research Campus",
        "aff_domain": "janelia.hhmi.org;janelia.hhmi.org;janelia.hhmi.org",
        "email": "janelia.hhmi.org;janelia.hhmi.org;janelia.hhmi.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "HHMI Janelia Research Campus",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.janelia.org",
        "aff_unique_abbr": "Janelia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ashburn",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b571ce9d54",
        "title": "A new metric on the manifold of kernel matrices with application to matrix geometric means",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/98dce83da57b0395e163467c9dae521b-Abstract.html",
        "author": "Suvrit Sra",
        "abstract": "Symmetric positive definite (spd) matrices are remarkably pervasive in a multitude of scientific disciplines, including machine learning and optimization. We consider the fundamental task of measuring distances between two spd matrices; a task that is often nontrivial whenever an application demands the distance function to respect the non-Euclidean geometry of spd matrices. Unfortunately, typical non-Euclidean distance measures such as the Riemannian metric $\\riem(X,Y)=\\frob{\\log(X\\inv{Y})}$, are computationally demanding and also complicated to use. To allay some of these difficulties, we introduce a new metric on spd matrices: this metric not only respects non-Euclidean geometry, it also offers faster computation than $\\riem$ while being less complicated to use. We support our claims theoretically via a series of theorems that relate our metric to $\\riem(X,Y)$, and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances.",
        "bibtex": "@inproceedings{NIPS2012_98dce83d,\n author = {Sra, Suvrit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A new metric on the manifold of kernel matrices with application to matrix geometric means},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/98dce83da57b0395e163467c9dae521b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/98dce83da57b0395e163467c9dae521b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 367266,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3015523146374203722&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Max Planck Institute for Intelligent Systems",
        "aff_domain": "tuebingen.mpg.de",
        "email": "tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Intelligent Systems",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2d0a4c1e68",
        "title": "A nonparametric variable clustering model",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/af4732711661056eadbf798ba191272a-Abstract.html",
        "author": "Konstantina Palla; Zoubin Ghahramani; David A. Knowles",
        "abstract": "Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a clustering, of observed variables so that variables in a cluster are highly correlated. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date.",
        "bibtex": "@inproceedings{NIPS2012_af473271,\n author = {Palla, Konstantina and Ghahramani, Zoubin and Knowles, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A nonparametric variable clustering model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/af4732711661056eadbf798ba191272a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/af4732711661056eadbf798ba191272a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/af4732711661056eadbf798ba191272a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/af4732711661056eadbf798ba191272a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 779728,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7106311817540743647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Cambridge; Stanford University; University of Cambridge",
        "aff_domain": "cam.ac.uk;cs.stanford.edu;eng.cam.ac.uk",
        "email": "cam.ac.uk;cs.stanford.edu;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Cambridge;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.stanford.edu",
        "aff_unique_abbr": "Cambridge;Stanford",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Stanford",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "4c516306e0",
        "title": "A quasi-Newton proximal splitting method",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e034fb6b66aacc1d48f445ddfb08da98-Abstract.html",
        "author": "Stephen Becker; Jalal Fadili",
        "abstract": "We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification.",
        "bibtex": "@inproceedings{NIPS2012_e034fb6b,\n author = {Becker, Stephen and Fadili, Jalal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A quasi-Newton proximal splitting method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e034fb6b66aacc1d48f445ddfb08da98-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 310527,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13748136039948426771&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "LJLL, CNRS-UPMC, Paris France; GREYC, CNRS-ENSICAEN-Univ. of Caen, Caen France",
        "aff_domain": "upmc.fr;greyc.ensicaen.fr",
        "email": "upmc.fr;greyc.ensicaen.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "UPMC;GREYC",
        "aff_unique_dep": "LJLL;",
        "aff_unique_url": "https://www.upmc.fr;",
        "aff_unique_abbr": "UPMC;",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Paris;Caen",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "b752c86e25",
        "title": "A systematic approach to extracting semantic information from functional MRI data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c60d060b946d6dd6145dcbad5c4ccf6f-Abstract.html",
        "author": "Francisco Pereira; Matthew Botvinick",
        "abstract": "This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure.",
        "bibtex": "@inproceedings{NIPS2012_c60d060b,\n author = {Pereira, Francisco and Botvinick, Matthew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A systematic approach to extracting semantic information from functional MRI data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c60d060b946d6dd6145dcbad5c4ccf6f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 411292,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12111863588936127839&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Siemens Corporation, Corporate Technology, Princeton, NJ 08540; Princeton Neuroscience Institute and Department of Psychology, Princeton University, Princeton NJ 08540",
        "aff_domain": "gmail.com;princeton.edu",
        "email": "gmail.com;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Siemens Corporation;Princeton University",
        "aff_unique_dep": "Corporate Technology;Princeton Neuroscience Institute and Department of Psychology",
        "aff_unique_url": "https://www.siemens.com;https://www.princeton.edu",
        "aff_unique_abbr": "Siemens;Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "af6813e18f",
        "title": "Accelerated Training for Matrix-norm Regularization: A Boosting Approach",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1ee3dfcd8a0645a25a35977997223d22-Abstract.html",
        "author": "Xinhua Zhang; Dale Schuurmans; Yao-liang Yu",
        "abstract": "Sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm. Although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper, we propose a boosting method for regularized learning that guarantees $\\epsilon$ accuracy within $O(1/\\epsilon)$ iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle.",
        "bibtex": "@inproceedings{NIPS2012_1ee3dfcd,\n author = {Zhang, Xinhua and Schuurmans, Dale and Yu, Yao-liang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accelerated Training for Matrix-norm Regularization: A Boosting Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1ee3dfcd8a0645a25a35977997223d22-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1ee3dfcd8a0645a25a35977997223d22-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 470553,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15762838188973860521&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada + National ICT Australia (NICTA), Machine Learning Group; Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada; Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "University of Alberta;National ICT Australia",
        "aff_unique_dep": "Department of Computing Science;Machine Learning Group",
        "aff_unique_url": "https://www.ualberta.ca;https://www.nicta.com.au",
        "aff_unique_abbr": "UAlberta;NICTA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edmonton;",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "Canada;Australia"
    },
    {
        "id": "de4b57a92a",
        "title": "Accuracy at the Top",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7fe1f8abaad094e0b5cb1b01d712f708-Abstract.html",
        "author": "Stephen Boyd; Corinna Cortes; Mehryar Mohri; Ana Radovanovic",
        "abstract": "We introduce a new notion of classification accuracy based on the top $\\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines. We define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems. We also present margin-based guarantees for this algorithm based on the $\\tau$-quantile of the functions in the hypothesis set. Finally, we report the results of several experiments evaluating the performance of our algorithm. In a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top.",
        "bibtex": "@inproceedings{NIPS2012_7fe1f8ab,\n author = {Boyd, Stephen and Cortes, Corinna and Mohri, Mehryar and Radovanovic, Ana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accuracy at the Top},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/7fe1f8abaad094e0b5cb1b01d712f708-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7fe1f8abaad094e0b5cb1b01d712f708-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 820223,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9176233908512249914&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Google Research; Courant Institute and Google; Google Research",
        "aff_domain": "stanford.edu;google.com;cims.nyu.edu;google.com",
        "email": "stanford.edu;google.com;cims.nyu.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Stanford University;Google;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": ";Google Research;Mathematical Sciences",
        "aff_unique_url": "https://www.stanford.edu;https://research.google;https://courant.nyu.edu",
        "aff_unique_abbr": "Stanford;Google Research;Courant",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Stanford;Mountain View;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d603a685da",
        "title": "Action-Model Based Multi-agent Plan Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a597e50502f5ff68e3e25b9114205d4a-Abstract.html",
        "author": "Hankz H. Zhuo; Qiang Yang; Subbarao Kambhampati",
        "abstract": "Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. Previous MAPR approaches required a library of team activity sequences (team plans) be given as input. However, collecting a library of team plans to ensure adequate coverage is often difficult and costly. In this paper, we relax this constraint, so that team plans are not required to be provided beforehand. We assume instead that a set of action models are available. Such models are often already created to describe domain physics; i.e., the  preconditions and effects of effects actions. We propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans. We encode the resulting MAPR problem as a \\emph{satisfiability problem} and solve the problem using a state-of-the-art weighted MAX-SAT solver. Our approach also allows for incompleteness in the observed plan traces. Our empirical studies demonstrate that our algorithm is both effective and efficient in comparison to state-of-the-art MAPR methods based on plan libraries.",
        "bibtex": "@inproceedings{NIPS2012_a597e505,\n author = {Zhuo, Hankz and Yang, Qiang and Kambhampati, Subbarao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Action-Model Based Multi-agent Plan Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a597e50502f5ff68e3e25b9114205d4a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a597e50502f5ff68e3e25b9114205d4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a597e50502f5ff68e3e25b9114205d4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 377659,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1336395078014644691&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Sun Yat-sen University, Guangzhou, China 510006; Huawei Noah\u2019s Ark Research Lab, Core Building 2, Hong Kong Science Park, Shatin, Hong Kong; Department of Computer Science and Engineering, Arizona State University, Tempe, Arizona, US 85287-5406",
        "aff_domain": "mail.sysu.edu.cn;cse.ust.hk;asu.edu",
        "email": "mail.sysu.edu.cn;cse.ust.hk;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Sun Yat-sen University;Huawei;Arizona State University",
        "aff_unique_dep": "Department of Computer Science;Huawei Noah\u2019s Ark Research Lab;Department of Computer Science and Engineering",
        "aff_unique_url": "http://www.sysu.edu.cn;https://www.huawei.com/en/research;https://www.asu.edu",
        "aff_unique_abbr": "SYSU;Huawei Noah\u2019s Ark Lab;ASU",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Guangzhou;Hong Kong SAR;Tempe",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2b04644188",
        "title": "Active Comparison of Prediction Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/92fb0c6d1758261f10d052e6e2c1123c-Abstract.html",
        "author": "Christoph Sawade; Niels Landwehr; Tobias Scheffer",
        "abstract": "We address the problem of comparing the risks of two given predictive models - for instance, a baseline model and a challenger - as confidently as possible on a fixed labeling budget. This problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution. In this case, new test instances have to be drawn and labeled at a cost. We devise an active comparison method that selects instances according to an instrumental sampling distribution. We derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model. Empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values.",
        "bibtex": "@inproceedings{NIPS2012_92fb0c6d,\n author = {Sawade, Christoph and Landwehr, Niels and Scheffer, Tobias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Comparison of Prediction Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/92fb0c6d1758261f10d052e6e2c1123c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/92fb0c6d1758261f10d052e6e2c1123c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/92fb0c6d1758261f10d052e6e2c1123c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/92fb0c6d1758261f10d052e6e2c1123c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 798638,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5405840733813721353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Potsdam, Department of Computer Science; University of Potsdam, Department of Computer Science; University of Potsdam, Department of Computer Science",
        "aff_domain": "cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de",
        "email": "cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Potsdam",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uni-potsdam.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "b1f1ec3408",
        "title": "Active Learning of Model Evidence Using Bayesian Quadrature",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Abstract.html",
        "author": "Michael Osborne; Roman Garnett; Zoubin Ghahramani; David K. Duvenaud; Stephen J. Roberts; Carl E. Rasmussen",
        "abstract": "Numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy.",
        "bibtex": "@inproceedings{NIPS2012_6364d3f0,\n author = {Osborne, Michael and Garnett, Roman and Ghahramani, Zoubin and Duvenaud, David K and Roberts, Stephen J and Rasmussen, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Learning of Model Evidence Using Bayesian Quadrature},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 356546,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3634410860518272879&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Oxford; University of Cambridge; Carnegie Mellon University; University of Cambridge; University of Oxford; University of Cambridge",
        "aff_domain": "robots.ox.ac.uk;cam.ac.uk;cs.cmu.edu;cam.ac.uk;robots.ox.ac.uk;eng.cam.ac.uk",
        "email": "robots.ox.ac.uk;cam.ac.uk;cs.cmu.edu;cam.ac.uk;robots.ox.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;0;1",
        "aff_unique_norm": "University of Oxford;University of Cambridge;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.cam.ac.uk;https://www.cmu.edu",
        "aff_unique_abbr": "Oxford;Cambridge;CMU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "36883c5c59",
        "title": "Active Learning of Multi-Index Function Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b4a528955b84f584974e92d025a75d1f-Abstract.html",
        "author": "Tyagi Hemant; Volkan Cevher",
        "abstract": "We consider the problem of actively learning \\textit{multi-index} functions of the form $f(\\vecx) = g(\\matA\\vecx)= \\sum_{i=1}^k g_i(\\veca_i^T\\vecx)$ from point evaluations of $f$. We assume that the function $f$ is defined on an $\\ell_2$-ball in $\\Real^d$, $g$ is twice continuously differentiable almost everywhere, and $\\matA \\in \\mathbb{R}^{k \\times d}$ is a rank $k$ matrix, where $k \\ll d$.  We propose a randomized, active sampling scheme for estimating such functions with uniform approximation guarantees. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive an estimator of the function $f$ along with sample complexity bounds. We also characterize the noise robustness of the scheme, and provide empirical evidence that the high-dimensional scaling of our sample complexity bounds are quite accurate.",
        "bibtex": "@inproceedings{NIPS2012_b4a52895,\n author = {Hemant, Tyagi and Cevher, Volkan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Learning of Multi-Index Function Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b4a528955b84f584974e92d025a75d1f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b4a528955b84f584974e92d025a75d1f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b4a528955b84f584974e92d025a75d1f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 399704,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13483913777563326812&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "LIONS \u2013 EPFL; LIONS \u2013 EPFL",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "e84f57f7db",
        "title": "Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b571ecea16a9824023ee1af16897a582-Abstract.html",
        "author": "Amadou Ba; Mathieu Sinn; Yannig Goude; Pascal Pompey",
        "abstract": "This paper proposes an efficient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) filter. In order to quickly track changes in the model and put more weight on recent data, the RLS filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricite de France (EDF). Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy.",
        "bibtex": "@inproceedings{NIPS2012_b571ecea,\n author = {Ba, Amadou and Sinn, Mathieu and Goude, Yannig and Pompey, Pascal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b571ecea16a9824023ee1af16897a582-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b571ecea16a9824023ee1af16897a582-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b571ecea16a9824023ee1af16897a582-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1805781,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7049153844673329330&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "IBM Research - Ireland; IBM Research - Ireland; EDF R&D; IBM Research - Ireland",
        "aff_domain": "ie.ibm.com;ie.ibm.com;edf.fr;ie.ibm.com",
        "email": "ie.ibm.com;ie.ibm.com;edf.fr;ie.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "IBM;EDF Research and Development",
        "aff_unique_dep": "IBM Research;",
        "aff_unique_url": "https://www.ibm.com/research;https://www.edf.com/research-and-development",
        "aff_unique_abbr": "IBM;EDF R&D",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Ireland;France"
    },
    {
        "id": "817fbdd517",
        "title": "Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/42a0e188f5033bc65bf8d78622277c4e-Abstract.html",
        "author": "Alexandra Carpentier; R\u00e9mi Munos",
        "abstract": "We consider the problem of adaptive stratified sampling for Monte Carlo integration of a differentiable function given a finite number of evaluations to the function. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost as accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and we provide a finite-sample analysis.",
        "bibtex": "@inproceedings{NIPS2012_42a0e188,\n author = {Carpentier, Alexandra and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/42a0e188f5033bc65bf8d78622277c4e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/42a0e188f5033bc65bf8d78622277c4e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 320140,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11857568772905722400&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "StatisticalLaboratory,CMS WilberforceRoad,Cambridge CB30WBUK; INRIALille-NordEurope 40,avenueHalley 59000Villeneuved\u2019ascq,France",
        "aff_domain": "statslab.cam.ac.uk;inria.fr",
        "email": "statslab.cam.ac.uk;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Statistical Laboratory;INRIA Lille-Nord Europe",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.inria.fr/en/lille-nord-europe",
        "aff_unique_abbr": ";INRIA",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Lille",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "id": "a2ce3fd1a6",
        "title": "Affine Independent Variational Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d91d1b4d82419de8a614abce9cc0e6d4-Abstract.html",
        "author": "Edward Challis; David Barber",
        "abstract": "We present a method for approximate inference for a broad class of non-conjugate probabilistic models. In particular, for the family of generalized linear model target densities we describe a rich class of variational approximating densities which can be best fit to the target by minimizing the Kullback-Leibler divergence.  Our approach is based on using the Fourier representation which we show results in efficient and scalable inference.",
        "bibtex": "@inproceedings{NIPS2012_d91d1b4d,\n author = {Challis, Edward and Barber, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Affine Independent Variational Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d91d1b4d82419de8a614abce9cc0e6d4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d91d1b4d82419de8a614abce9cc0e6d4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/d91d1b4d82419de8a614abce9cc0e6d4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d91d1b4d82419de8a614abce9cc0e6d4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 571963,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17887971334536625678&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University College London, UK; Department of Computer Science, University College London, UK",
        "aff_domain": "cs.ucl.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "82e3df1b78",
        "title": "Algorithms for Learning Markov Field Policies",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9f36407ead0629fc166f14dde7970f68-Abstract.html",
        "author": "Abdeslam Boularias; Jan R. Peters; Oliver B. Kroemer",
        "abstract": "We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications. The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects.",
        "bibtex": "@inproceedings{NIPS2012_9f36407e,\n author = {Boularias, Abdeslam and Peters, Jan and Kroemer, Oliver},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Algorithms for Learning Markov Field Policies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9f36407ead0629fc166f14dde7970f68-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9f36407ead0629fc166f14dde7970f68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9f36407ead0629fc166f14dde7970f68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5468093,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VoBi1DS4M6YJ:scholar.google.com/&scioq=Algorithms+for+Learning+Markov+Field+Policies&hl=en&as_sdt=0,33",
        "gs_version_total": 12,
        "aff": "Max Planck Institute for Intelligent Systems; Technische Universit\u00e4t Darmstadt; Technische Universit\u00e4t Darmstadt",
        "aff_domain": "tuebingen.mpg.de;robot-learning.de;robot-learning.de",
        "email": "tuebingen.mpg.de;robot-learning.de;robot-learning.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Technische Universit\u00e4t Darmstadt",
        "aff_unique_dep": "Intelligent Systems;",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.tu-darmstadt.de",
        "aff_unique_abbr": "MPI-IS;TUD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "017eb1dadd",
        "title": "An Integer Optimization Approach to Associative Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9e7ba617ad9e69b39bd0c29335b79629-Abstract.html",
        "author": "Dimitris Bertsimas; Allison Chang; Cynthia Rudin",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS2012_9e7ba617,\n author = {Bertsimas, Dimitris and Chang, Allison and Rudin, Cynthia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Integer Optimization Approach to Associative Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9e7ba617ad9e69b39bd0c29335b79629-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9e7ba617ad9e69b39bd0c29335b79629-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 0,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8538487291887720457&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "65f131fd4e",
        "title": "Analog readout for optical reservoir computers",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html",
        "author": "Anteo Smerieri; Fran\u00e7ois Duport; Yvon Paquot; Benjamin Schrauwen; Marc Haelterman; Serge Massar",
        "abstract": "Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers.",
        "bibtex": "@inproceedings{NIPS2012_250cf8b5,\n author = {Smerieri, Anteo and Duport, Fran\\c{c}ois and Paquot, Yvon and Schrauwen, Benjamin and Haelterman, Marc and Massar, Serge},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analog readout for optical reservoir computers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/250cf8b51c773f3f8dc8b4be867a9a02-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 446394,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13373764778036977004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c0dde3e8e3",
        "title": "Analyzing 3D Objects in Cluttered Images",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/918317b57931b6b7a7d29490fe5ec9f9-Abstract.html",
        "author": "Mohsen Hejrati; Deva Ramanan",
        "abstract": "We present an approach to detecting and analyzing the 3D configuration of objects in real-world images with heavy occlusion and clutter. We focus on the application of finding and analyzing cars. We do so with a two-stage model; the first stage reasons about 2D shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then refined by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset.",
        "bibtex": "@inproceedings{NIPS2012_918317b5,\n author = {Hejrati, Mohsen and Ramanan, Deva},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analyzing 3D Objects in Cluttered Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/918317b57931b6b7a7d29490fe5ec9f9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/918317b57931b6b7a7d29490fe5ec9f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/918317b57931b6b7a7d29490fe5ec9f9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3943664,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15438618299668588950&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "UC Irvine; UC Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8581773f0c",
        "title": "Ancestor Sampling for Particle Gibbs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4122cb13c7a474c1976c9706ae36521d-Abstract.html",
        "author": "Fredrik Lindsten; Thomas Sch\u00f6n; Michael I. Jordan",
        "abstract": "We present a novel method in the family of particle MCMC methods that we refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the existing PG with backward simulation (PG-BS) procedure, we use backward sampling to (considerably) improve the mixing of the PG kernel. Instead of using separate forward and backward sweeps as in PG-BS, however, we achieve the same effect in a single forward sweep. We apply the PG-AS framework to the challenging class of non-Markovian state-space models. We develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the PG-AS framework. In particular, as we show in a simulation study, PG-AS can yield an order-of-magnitude improved accuracy relative to PG-BS due to its robustness to the truncation error. Several application examples are discussed, including Rao-Blackwellized particle smoothing and inference in degenerate state-space models.",
        "bibtex": "@inproceedings{NIPS2012_4122cb13,\n author = {Lindsten, Fredrik and Sch\\\"{o}n, Thomas and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ancestor Sampling for Particle Gibbs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4122cb13c7a474c1976c9706ae36521d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4122cb13c7a474c1976c9706ae36521d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/4122cb13c7a474c1976c9706ae36521d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4122cb13c7a474c1976c9706ae36521d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 482666,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6464509029903553054&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Div. of Automatic Control, Link\u00f6ping University; Dept. of EECS and Statistics, University of California, Berkeley; Div. of Automatic Control, Link\u00f6ping University",
        "aff_domain": "isy.liu.se;cs.berkeley.edu;isy.liu.se",
        "email": "isy.liu.se;cs.berkeley.edu;isy.liu.se",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Link\u00f6ping University;University of California, Berkeley",
        "aff_unique_dep": "Division of Automatic Control;Dept. of EECS and Statistics",
        "aff_unique_url": "https://www.liu.se;https://www.berkeley.edu",
        "aff_unique_abbr": "LiU;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Sweden;United States"
    },
    {
        "id": "5a73a0f97e",
        "title": "Angular Quantization-based Binary Codes for Fast Similarity Search",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f5deaeeae1538fb6c45901d524ee2f98-Abstract.html",
        "author": "Yunchao Gong; Sanjiv Kumar; Vishal Verma; Svetlana Lazebnik",
        "abstract": "This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2012_f5deaeea,\n author = {Gong, Yunchao and Kumar, Sanjiv and Verma, Vishal and Lazebnik, Svetlana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Angular Quantization-based Binary Codes for Fast Similarity Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f5deaeeae1538fb6c45901d524ee2f98-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 495828,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8489753741012406219&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Computer Science Department, University of North Carolina at Chapel Hill, NC 27599, USA; Google Research, New York, NY 10011, USA; Computer Science Department, University of North Carolina at Chapel Hill, NC 27599, USA; Computer Science Department, University of Illinois at Urbana-Champaign, IL 61801, USA",
        "aff_domain": "cs.unc.edu;google.com;cs.unc.edu;uiuc.edu",
        "email": "cs.unc.edu;google.com;cs.unc.edu;uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Google;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Computer Science Department;Google Research;Computer Science Department",
        "aff_unique_url": "https://www.unc.edu;https://research.google;https://illinois.edu",
        "aff_unique_abbr": "UNC Chapel Hill;Google Research;UIUC",
        "aff_campus_unique_index": "0;1;0;2",
        "aff_campus_unique": "Chapel Hill;New York;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "86dbfaa5d4",
        "title": "Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/83f97f4825290be4cb794ec6a234595f-Abstract.html",
        "author": "Ulugbek Kamilov; Sundeep Rangan; Michael Unser; Alyson K. Fletcher",
        "abstract": "We consider the estimation of an i.i.d.\\ vector $\\xbf \\in \\R^n$ from measurements $\\ybf \\in \\R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. We present a method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\\xbf$. The proposed algorithm is a generalization of a recently-developed method by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriors in the E-steps are computed via approximate message passing. The techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.  We prove that for large i.i.d.\\ Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations.  This analysis shows that the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.  The adaptive GAMP methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.",
        "bibtex": "@inproceedings{NIPS2012_83f97f48,\n author = {Kamilov, Ulugbek and Rangan, Sundeep and Unser, Michael and Fletcher, Alyson K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/83f97f4825290be4cb794ec6a234595f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 332174,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3566458324023880447&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "EPFL; Polytechnic Institute of New York University; University of California, Santa Cruz; EPFL",
        "aff_domain": "epfl.ch;poly.edu;soe.ucsc.edu;epfl.ch",
        "email": "epfl.ch;poly.edu;soe.ucsc.edu;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "EPFL;New York University;University of California, Santa Cruz",
        "aff_unique_dep": ";Polytechnic Institute;",
        "aff_unique_url": "https://www.epfl.ch;https://www.poly.nyu.edu;https://www.ucsc.edu",
        "aff_unique_abbr": "EPFL;NYU Poly;UCSC",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Brooklyn;Santa Cruz",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "7188099c34",
        "title": "Approximating Concavely Parameterized Optimization Problems",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/bdb106a0560c4e46ccc488ef010af787-Abstract.html",
        "author": "Joachim Giesen; Jens Mueller; Soeren Laue; Sascha Swiercy",
        "abstract": "We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\\varepsilon >0$ by a set of size $O(1/\\sqrt{\\varepsilon})$. A lower bound of size $\\Omega (1/\\sqrt{\\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\\sqrt{\\varepsilon})$. Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion.",
        "bibtex": "@inproceedings{NIPS2012_bdb106a0,\n author = {Giesen, Joachim and Mueller, Jens and Laue, Soeren and Swiercy, Sascha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximating Concavely Parameterized Optimization Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/bdb106a0560c4e46ccc488ef010af787-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 228311,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17441894469854223198&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Friedrich-Schiller-Universit\u00e4t Jena; Friedrich-Schiller-Universit\u00e4t Jena; Friedrich-Schiller-Universit\u00e4t Jena; Friedrich-Schiller-Universit\u00e4t Jena",
        "aff_domain": "uni-jena.de;uni-jena.de;informatik.uni-jena.de;googlemail.com",
        "email": "uni-jena.de;uni-jena.de;informatik.uni-jena.de;googlemail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Friedrich-Schiller-Universit\u00e4t",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-jena.de",
        "aff_unique_abbr": "FSU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Jena",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "1785bae9d8",
        "title": "Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/801c14f07f9724229175b8ef8b4585a8-Abstract.html",
        "author": "Amy Greenwald; Jiacui Li; Eric Sodomka",
        "abstract": "In many large economic markets, goods are sold through sequential auctions. Such domains include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions.   In this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown.",
        "bibtex": "@inproceedings{NIPS2012_801c14f0,\n author = {Greenwald, Amy and Li, Jiacui and Sodomka, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/801c14f07f9724229175b8ef8b4585a8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/801c14f07f9724229175b8ef8b4585a8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/801c14f07f9724229175b8ef8b4585a8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/801c14f07f9724229175b8ef8b4585a8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1900072,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9174643812608227146&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Brown University; Department of Computer Science, Brown University; Department of Applied Math/Economics, Brown University",
        "aff_domain": "cs.brown.edu;cs.brown.edu;alumni.brown.edu",
        "email": "cs.brown.edu;cs.brown.edu;alumni.brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8cd4ece55d",
        "title": "Assessing Blinding in Clinical Trials",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d6baf65e0b240ce177cf70da146c8dc8-Abstract.html",
        "author": "Ognjen Arandjelovic",
        "abstract": "The interaction between the patient's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants' feedback.",
        "bibtex": "@inproceedings{NIPS2012_d6baf65e,\n author = {Arandjelovic, Ognjen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Assessing Blinding in Clinical Trials},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d6baf65e0b240ce177cf70da146c8dc8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d6baf65e0b240ce177cf70da146c8dc8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d6baf65e0b240ce177cf70da146c8dc8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 495595,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7386229075024436764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Deakin University, Australia",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Deakin University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.deakin.edu.au",
        "aff_unique_abbr": "Deakin",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "18313833e0",
        "title": "Augment-and-Conquer Negative Binomial Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6a61d423d02a1c56250dc23ae7ff12f3-Abstract.html",
        "author": "Mingyuan Zhou; Lawrence Carin",
        "abstract": "By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the  gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.",
        "bibtex": "@inproceedings{NIPS2012_6a61d423,\n author = {Zhou, Mingyuan and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Augment-and-Conquer Negative Binomial Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/6a61d423d02a1c56250dc23ae7ff12f3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6a61d423d02a1c56250dc23ae7ff12f3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 721671,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14096103897025324035&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Dept. of Electrical and Computer Engineering, Duke University, Durham, NC 27708; Dept. of Electrical and Computer Engineering, Duke University, Durham, NC 27708",
        "aff_domain": "ee.duke.edu;ee.duke.edu",
        "email": "ee.duke.edu;ee.duke.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Durham",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "aaf21b6d53",
        "title": "Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1be3bc32e6564055d5ca3e5a354acbef-Abstract.html",
        "author": "Ashwini Shukla; Aude Billard",
        "abstract": "Non-linear dynamical systems (DS) have been used extensively for building generative models of human behavior. Its applications range from modeling brain dynamics  to encoding motor commands. Many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space. Although these enable the robots to react against sudden perturbations without any re-planning, the motions are always directed towards a single target. In this work, we focus on combining several such DS with distinct attractors, resulting in a multi-stable DS. We show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object. While exploiting multiple attractors provides more flexibility in recovering from unseen perturbations, it also increases the complexity of the underlying learning problem. Here we present the Augmented-SVM (A-SVM) model which inherits region partitioning ability of the well known SVM classifier and is augmented with novel constraints derived from the individual DS. The new constraints modify the original SVM dual whose optimal solution then results in a new class of support vectors (SV). These new SV ensure that the resulting multi-stable DS incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction. We show, via implementations on a simulated 10 degrees of freedom mobile robotic platform, that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations.",
        "bibtex": "@inproceedings{NIPS2012_1be3bc32,\n author = {Shukla, Ashwini and Billard, Aude},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1be3bc32e6564055d5ca3e5a354acbef-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1be3bc32e6564055d5ca3e5a354acbef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 7377006,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1762257367169556226&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Learning Algorithms and Systems Laboratory (LASA), \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL); Learning Algorithms and Systems Laboratory (LASA), \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)",
        "aff_domain": "epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "Learning Algorithms and Systems Laboratory (LASA)",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "fc16186c45",
        "title": "Automatic Feature Induction for Stagewise Collaborative Filtering",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8f85517967795eeef66c225f7883bdcb-Abstract.html",
        "author": "Joonseok Lee; Mingxuan Sun; Seungyeon Kim; Guy Lebanon",
        "abstract": "Recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms.",
        "bibtex": "@inproceedings{NIPS2012_8f855179,\n author = {Lee, Joonseok and Sun, Mingxuan and Kim, Seungyeon and Lebanon, Guy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Automatic Feature Induction for Stagewise Collaborative Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8f85517967795eeef66c225f7883bdcb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 352802,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16268005168671745518&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology + Google Research",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;cc.gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Georgia Institute of Technology;Google",
        "aff_unique_dep": "College of Computing;Google Research",
        "aff_unique_url": "https://www.gatech.edu;https://research.google",
        "aff_unique_abbr": "Georgia Tech;Google Research",
        "aff_campus_unique_index": "0;0;0;0+1",
        "aff_campus_unique": "Atlanta;Mountain View",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e75aa6cfab",
        "title": "Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/57aeee35c98205091e18d1140e9f38cf-Abstract.html",
        "author": "Joan Fruitet; Alexandra Carpentier; Maureen Clerc; R\u00e9mi Munos",
        "abstract": "A brain-computer interface (BCI) allows users to \u201ccommunicate\u201d with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue. This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm UCB-classif based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session. Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%.",
        "bibtex": "@inproceedings{NIPS2012_57aeee35,\n author = {Fruitet, Joan and Carpentier, Alexandra and Clerc, Maureen and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/57aeee35c98205091e18d1140e9f38cf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/57aeee35c98205091e18d1140e9f38cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 464407,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1689456158504625513&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "INRIA, Sophia Antipolis; Statistical Laboratory, CMS; INRIA Lille-Nord Europe; INRIA, Sophia Antipolis",
        "aff_domain": "inria.fr;statslab.cam.ac.uk;inria.fr;inria.fr",
        "email": "inria.fr;statslab.cam.ac.uk;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "INRIA;CMS",
        "aff_unique_dep": ";Statistical Laboratory",
        "aff_unique_url": "https://www.inria.fr;",
        "aff_unique_abbr": "INRIA;CMS",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Sophia Antipolis;;Lille-Nord Europe",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France;"
    },
    {
        "id": "4c7f06c7d8",
        "title": "Bayesian Hierarchical Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/642e92efb79421734881b53e1e1b18b6-Abstract.html",
        "author": "Feng Cao; Soumya Ray",
        "abstract": "We describe an approach to incorporating Bayesian priors in the maxq framework for hierarchical reinforcement learning (HRL). We define priors on the primitive environment model and on task pseudo-rewards. Since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy. We show empirically that (i) our approach results in improved convergence over non-Bayesian baselines, given sensible priors, (ii) task hierarchies and Bayesian priors can be complementary sources of information, and using both sources is better than either alone, (iii) taking advantage of the structural decomposition induced by the task hierarchy significantly reduces the computational cost of Bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to automatic learning of hierarchically optimal rather than recursively optimal policies.",
        "bibtex": "@inproceedings{NIPS2012_642e92ef,\n author = {Cao, Feng and Ray, Soumya},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Hierarchical Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/642e92efb79421734881b53e1e1b18b6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/642e92efb79421734881b53e1e1b18b6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 479327,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8383243263867189052&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of EECS, Case Western Reserve University, Cleveland, OH 44106; Department of EECS, Case Western Reserve University, Cleveland, OH 44106",
        "aff_domain": "case.edu;case.edu",
        "email": "case.edu;case.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Case Western Reserve University",
        "aff_unique_dep": "Department of EECS",
        "aff_unique_url": "https://www.case.edu",
        "aff_unique_abbr": "CWRU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cleveland",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ea5d1968ce",
        "title": "Bayesian Nonparametric Modeling of Suicide Attempts",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/430c3626b879b4005d41b8a46172e0c0-Abstract.html",
        "author": "Francisco Ruiz; Isabel Valera; Carlos Blanco; Fernando P\u00e9rez-Cruz",
        "abstract": "The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the U.S. population. In the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts.",
        "bibtex": "@inproceedings{NIPS2012_430c3626,\n author = {Ruiz, Francisco and Valera, Isabel and Blanco, Carlos and P\\'{e}rez-Cruz, Fernando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Nonparametric Modeling of Suicide Attempts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/430c3626b879b4005d41b8a46172e0c0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 387768,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3730057439322732665&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University Carlos III in Madrid; University Carlos III in Madrid; Columbia University College of Physicians and Surgeons; University Carlos III in Madrid",
        "aff_domain": "tsc.uc3m.es;tsc.uc3m.es;nyspi.columbia.edu;tsc.uc3m.es",
        "email": "tsc.uc3m.es;tsc.uc3m.es;nyspi.columbia.edu;tsc.uc3m.es",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Carlos III University of Madrid;Columbia University",
        "aff_unique_dep": ";College of Physicians and Surgeons",
        "aff_unique_url": "https://www.uc3m.es;https://www.columbia.edu",
        "aff_unique_abbr": "UC3M;Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Madrid;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Spain;United States"
    },
    {
        "id": "cd3520c0e1",
        "title": "Bayesian Pedigree Analysis using Measure Factorization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/dfd7468ac613286cdbb40872c8ef3b06-Abstract.html",
        "author": "Bonnie Kirkpatrick; Alexandre Bouchard-c\u00f4t\u00e9",
        "abstract": "Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease.  With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites.  Some pedigrees number in the thousands of individuals.  Meanwhile, analysis methods have remained limited to pedigrees of <100 individuals which limits analyses to many small independent pedigrees.  Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited.  This is because linkage anlysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order.  LODs are difficult to interpret and nontrivial to extend to consider interactions among sites.  These developments and difficulties call for the creation of modern methods of pedigree analysis.  Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models.   We show that these disease models can be turned into accurate and efficient estimators.  The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models.  This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction.",
        "bibtex": "@inproceedings{NIPS2012_dfd7468a,\n author = {Kirkpatrick, Bonnie and Bouchard-c\\^{o}t\\'{e}, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Pedigree Analysis using Measure Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/dfd7468ac613286cdbb40872c8ef3b06-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/dfd7468ac613286cdbb40872c8ef3b06-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 399271,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6718618500718801411&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Statistics Department, University of British Columbia; Computer Science Department, University of British Columbia",
        "aff_domain": "stat.ubc.ca;cs.ubc.ca",
        "email": "stat.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Statistics Department",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "3d0733316d",
        "title": "Bayesian Probabilistic Co-Subspace Addition",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9f396fe44e7c05c16873b05ec425cbad-Abstract.html",
        "author": "Lei Shi",
        "abstract": "For modeling data matrices, this paper introduces Probabilistic Co-Subspace Addition (PCSA) model by simultaneously capturing the dependent structures among both rows and columns. Briefly, PCSA assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two features, which distribute in the row-wise and column-wise latent subspaces. Consequently, it captures the dependencies among entries intricately, and is able to model the non-Gaussian and heteroscedastic density. Variational inference is proposed on PCSA for  approximate Bayesian learning, where the updating for posteriors is formulated into the problem of solving Sylvester equations. Furthermore, PCSA is extended to tackling and filling missing values, to adapting its sparseness, and to modelling tensor data. In comparison with several state-of-art approaches, experiments demonstrate the effectiveness and efficiency of Bayesian (sparse) PCSA on modeling matrix (tensor) data and filling missing values.",
        "bibtex": "@inproceedings{NIPS2012_9f396fe4,\n author = {Shi, Lei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Probabilistic Co-Subspace Addition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9f396fe44e7c05c16873b05ec425cbad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 651077,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qzJhBsOk6LkJ:scholar.google.com/&scioq=Bayesian+Probabilistic+Co-Subspace+Addition&hl=en&as_sdt=0,5",
        "gs_version_total": 7,
        "aff": "Baidu.com, Inc",
        "aff_domain": "baidu.com",
        "email": "baidu.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Baidu",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "66d4c23ab3",
        "title": "Bayesian Warped Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d840cc5d906c3e9c84374c8919d2074e-Abstract.html",
        "author": "Miguel L\u00e1zaro-Gredilla",
        "abstract": "Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classification, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets.",
        "bibtex": "@inproceedings{NIPS2012_d840cc5d,\n author = {L\\'{a}zaro-Gredilla, Miguel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Warped Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d840cc5d906c3e9c84374c8919d2074e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d840cc5d906c3e9c84374c8919d2074e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d840cc5d906c3e9c84374c8919d2074e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 192718,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11938384393169749396&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. Signal Processing & Communications, Universidad Carlos III de Madrid - Spain",
        "aff_domain": "tsc.uc3m.es",
        "email": "tsc.uc3m.es",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Universidad Carlos III de Madrid",
        "aff_unique_dep": "Dept. Signal Processing & Communications",
        "aff_unique_url": "https://www.uc3m.es",
        "aff_unique_abbr": "UC3M",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "a25a5733d1",
        "title": "Bayesian active learning with localized priors for fast receptive field characterization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4588e674d3f0faf985047d4c3f13ed0d-Abstract.html",
        "author": "Mijung Park; Jonathan W. Pillow",
        "abstract": "Active learning can substantially improve the yield of neurophysiology experiments by adaptively selecting stimuli to probe a neuron's receptive field (RF) in real time. Bayesian active learning methods maintain a posterior distribution over the RF, and select stimuli to maximally reduce posterior entropy on each time step.  However, existing methods tend to rely on simple Gaussian priors, and do not exploit uncertainty at the level of hyperparameters when determining an optimal stimulus.  This uncertainty can play a substantial role in RF characterization, particularly when RFs are smooth, sparse, or local in space and time.  In this paper, we describe a novel framework for active learning under hierarchical, conditionally Gaussian priors.  Our algorithm uses sequential Markov Chain Monte Carlo sampling (''particle filtering'' with MCMC) over hyperparameters to construct a mixture-of-Gaussians representation of the RF posterior, and selects optimal stimuli using an approximate infomax criterion.  The core elements of this algorithm are parallelizable, making it computationally efficient for real-time experiments.  We apply our algorithm to simulated and real neural data, and show that it can provide highly accurate receptive field estimates from very limited data, even with a small number of hyperparameter samples.",
        "bibtex": "@inproceedings{NIPS2012_4588e674,\n author = {Park, Mijung and Pillow, Jonathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian active learning with localized priors for fast receptive field characterization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4588e674d3f0faf985047d4c3f13ed0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 949733,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13601635547372204373&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Electrical and Computer Engineering, The University of Texas at Austin; Center For Perceptual Systems, The University of Texas at Austin",
        "aff_domain": "mail.utexas.edu;mail.utexas.edu",
        "email": "mail.utexas.edu;mail.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "14ea8d1ef7",
        "title": "Bayesian estimation of discrete entropy with mixtures of stick-breaking priors",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0b8aff0438617c055eb55f0ba5d226fa-Abstract.html",
        "author": "Evan Archer; Ill Memming Park; Jonathan W. Pillow",
        "abstract": "We consider the problem of estimating Shannon's entropy H in the   under-sampled regime, where the number of possible symbols may be   unknown or countably infinite.  Pitman-Yor processes (a   generalization of Dirichlet processes) provide tractable prior   distributions over the space of countably infinite discrete   distributions, and have found major applications in Bayesian   non-parametric statistics and machine learning. Here we show that   they also provide natural priors for Bayesian entropy estimation,   due to the remarkable fact that the moments of the induced posterior   distribution over H can be computed analytically. We derive   formulas for the posterior mean (Bayes' least squares estimate) and   variance under such priors.  Moreover, we show that a fixed   Dirichlet or Pitman-Yor process prior implies a narrow prior on H,   meaning the prior strongly determines the entropy estimate in the   under-sampled regime. We derive a family of continuous mixing   measures such that the resulting mixture of Pitman-Yor processes   produces an approximately flat (improper) prior over H.  We   explore the theoretical properties of the resulting estimator, and   show that it performs well on data sampled from both exponential and   power-law tailed distributions.",
        "bibtex": "@inproceedings{NIPS2012_0b8aff04,\n author = {Archer, Evan and Park, Il Memming and Pillow, Jonathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian estimation of discrete entropy with mixtures of stick-breaking priors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/0b8aff0438617c055eb55f0ba5d226fa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0b8aff0438617c055eb55f0ba5d226fa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2213987,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11625038213118471288&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Institute for Computational and Engineering Sciences+Center for Perceptual Systems+Dept. of Psychology+Division of Statistics & Scientific Computation; Center for Perceptual Systems+Dept. of Psychology+Division of Statistics & Scientific Computation; Center for Perceptual Systems+Dept. of Psychology+Division of Statistics & Scientific Computation",
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu",
        "email": "utexas.edu;utexas.edu;utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2+3;1+2+3;1+2+3",
        "aff_unique_norm": "Institute for Computational and Engineering Sciences;Center for Perceptual Systems;University Affiliation Not Specified;University of Texas at Austin",
        "aff_unique_dep": ";;Department of Psychology;Division of Statistics & Scientific Computation",
        "aff_unique_url": ";;;https://www.utexas.edu",
        "aff_unique_abbr": ";;;UT Austin",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0+0+0;0+0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "0aeab93c12",
        "title": "Bayesian models for Large-scale Hierarchical Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Abstract.html",
        "author": "Siddharth Gopal; Yiming Yang; Bing Bai; Alexandru Niculescu-mizil",
        "abstract": "A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods.",
        "bibtex": "@inproceedings{NIPS2012_a3fb4fbf,\n author = {Gopal, Siddharth and Yang, Yiming and Bai, Bing and Niculescu-mizil, Alexandru},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian models for Large-scale Hierarchical Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 360794,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10657737037011840977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; NEC Laboratories America, Princeton; NEC Laboratories America, Princeton",
        "aff_domain": "andrew.cmu.edu;cs.cmu.edu;nec-labs.com;nec-labs.com",
        "email": "andrew.cmu.edu;cs.cmu.edu;nec-labs.com;nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Carnegie Mellon University;NEC Laboratories America",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "CMU;NEC Labs",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Princeton",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "201beb452c",
        "title": "Bayesian nonparametric models for bipartite graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0768281a05da9f27df178b5c39a51263-Abstract.html",
        "author": "Francois Caron",
        "abstract": "We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, an Indian Buffet-like generative process for network growth, and a simple and efficient Gibbs sampler for posterior simulation. Our model is shown to be well fitted to several real-world social networks.",
        "bibtex": "@inproceedings{NIPS2012_0768281a,\n author = {Caron, Francois},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian nonparametric models for bipartite graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0768281a05da9f27df178b5c39a51263-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/0768281a05da9f27df178b5c39a51263-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0768281a05da9f27df178b5c39a51263-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 371299,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1464368413254176150&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ba87562879",
        "title": "Bayesian nonparametric models for ranked data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7f1171a78ce0780a2142a6eb7bc4f3c8-Abstract.html",
        "author": "Francois Caron; Yee W. Teh",
        "abstract": "We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items.   Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process.  We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation.  We then develop a time-varying extension of our model, and apply our model to the New York Times lists of weekly bestselling books.",
        "bibtex": "@inproceedings{NIPS2012_7f1171a7,\n author = {Caron, Francois and Teh, Yee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian nonparametric models for ranked data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 477707,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1261523831480251675&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "INRIA; Department of Statistics",
        "aff_domain": "inria.fr;stats.ox.ac.uk",
        "email": "inria.fr;stats.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "INRIA;University Affiliation Not Specified",
        "aff_unique_dep": ";Department of Statistics",
        "aff_unique_url": "https://www.inria.fr;",
        "aff_unique_abbr": "INRIA;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France;"
    },
    {
        "id": "9b75bc5313",
        "title": "Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8b0d268963dd0cfb808aac48a549829f-Abstract.html",
        "author": "Victor Gabillon; Mohammad Ghavamzadeh; Alessandro Lazaric",
        "abstract": "We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms.",
        "bibtex": "@inproceedings{NIPS2012_8b0d2689,\n author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8b0d268963dd0cfb808aac48a549829f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8b0d268963dd0cfb808aac48a549829f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8b0d268963dd0cfb808aac48a549829f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 306456,
        "gs_citation": 394,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5597743079697150672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "47ba426d87",
        "title": "Burn-in, bias, and the rationality of anchoring",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/81e5f81db77c596492e6f1a5a792ed53-Abstract.html",
        "author": "Falk Lieder; Tom Griffiths; Noah Goodman",
        "abstract": "Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as burn-in. Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks. Our theoretical and empirical results suggest that the anchoring bias is consistent with approximate Bayesian inference.",
        "bibtex": "@inproceedings{NIPS2012_81e5f81d,\n author = {Lieder, Falk and Griffiths, Tom and Goodman, Noah},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Burn-in, bias, and the rationality of anchoring},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/81e5f81db77c596492e6f1a5a792ed53-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1373267,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4014842505286628901&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "771e559e7f",
        "title": "CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3a066bda8c96b9478bb0512f0a43028c-Abstract.html",
        "author": "Henrik Ohlsson; Allen Yang; Roy Dong; Shankar Sastry",
        "abstract": "While compressive sensing (CS) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models. This limits its application and excludes many areas where CS ideas could make a difference. This paper presents a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose a novel solution using a lifting technique -- CPRL, which relaxes the NP-hard problem to a nonsmooth semidefinite program. Our analysis shows that CPRL inherits many desirable properties from CS, such as guarantees for exact recovery. We further provide scalable numerical solvers to accelerate its implementation. The source code of our algorithms will be provided to the public.",
        "bibtex": "@inproceedings{NIPS2012_3a066bda,\n author = {Ohlsson, Henrik and Yang, Allen and Dong, Roy and Sastry, Shankar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/3a066bda8c96b9478bb0512f0a43028c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3a066bda8c96b9478bb0512f0a43028c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 334713,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9921466705107514706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Division of Automatic Control, Department of Electrical Engineering, Link\u00f6ping University, Sweden + Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, CA, USA",
        "aff_domain": "eecs.berkeley.edu; ; ; ",
        "email": "eecs.berkeley.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "Link\u00f6ping University;University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering;Department of Electrical Engineering and Computer Sciences",
        "aff_unique_url": "https://www.liu.se;https://www.berkeley.edu",
        "aff_unique_abbr": "LiU;UC Berkeley",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+1;1;1;1",
        "aff_country_unique": "Sweden;United States"
    },
    {
        "id": "43f0580515",
        "title": "Calibrated Elastic Regularization in Matrix Completion",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html",
        "author": "Tingni Sun; Cun-hui Zhang",
        "abstract": "This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a unified analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones.",
        "bibtex": "@inproceedings{NIPS2012_e46de7e1,\n author = {Sun, Tingni and Zhang, Cun-hui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Calibrated Elastic Regularization in Matrix Completion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 242642,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5931404931573371171&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Statistics Department, The Wharton School, University of Pennsylvania; Department of Statistics and Biostatistics, Rutgers University",
        "aff_domain": "wharton.upenn.edu;stat.rutgers.edu",
        "email": "wharton.upenn.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Pennsylvania;Rutgers University",
        "aff_unique_dep": "Statistics Department;Department of Statistics and Biostatistics",
        "aff_unique_url": "https://www.wharton.upenn.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "UPenn;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "064d0a3e29",
        "title": "Cardinality Restricted Boltzmann Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/99adff456950dd9629a5260c4de21858-Abstract.html",
        "author": "Kevin Swersky; Ilya Sutskever; Daniel Tarlow; Richard S. Zemel; Ruslan Salakhutdinov; Ryan P. Adams",
        "abstract": "The Restricted Boltzmann Machine (RBM) is a popular density model that is also good for extracting features. A main source of tractability in RBM models is the model's assumption that given an input, hidden units activate independently from one another. Sparsity and competition in the hidden representation is believed to be beneficial, and while an RBM with competition among its hidden units would acquire some of the attractive properties of sparse coding, such constraints are not added  due to the widespread belief that the resulting model would become intractable.  In this work, we show how a dynamic programming algorithm developed in 1981 can be used to implement exact sparsity in the RBM's hidden units. We then expand on this and show how to pass derivatives through a layer of exact sparsity, which makes it possible to fine-tune a deep belief network (DBN) consisting of RBMs with sparse hidden layers.  We show that sparsity in the RBM's hidden layer improves the performance of both the pre-trained representations and of the fine-tuned model.",
        "bibtex": "@inproceedings{NIPS2012_99adff45,\n author = {Swersky, Kevin and Sutskever, Ilya and Tarlow, Daniel and Zemel, Richard and Salakhutdinov, Russ R and Adams, Ryan P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cardinality Restricted Boltzmann Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/99adff456950dd9629a5260c4de21858-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/99adff456950dd9629a5260c4de21858-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/99adff456950dd9629a5260c4de21858-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 754243,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6979072430722394652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Dept. of Computer Science, University of Toronto; Dept. of Computer Science, University of Toronto; Dept. of Computer Science, University of Toronto; Dept. of Computer Science\u2020and Statistics\u2021, University of Toronto; Dept. of Computer Science\u2020and Statistics\u2021, University of Toronto; School of Eng. and Appl. Sciences, Harvard University",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;seas.harvard.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "University of Toronto;Harvard University",
        "aff_unique_dep": "Department of Computer Science;School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.utoronto.ca;https://www.seas.harvard.edu",
        "aff_unique_abbr": "U of T;SEAS",
        "aff_campus_unique_index": "0;0;0;0;0;1",
        "aff_campus_unique": "Toronto;Cambridge",
        "aff_country_unique_index": "0;0;0;0;0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "37ca135f56",
        "title": "Causal discovery with scale-mixture model for spatiotemporal variance dependencies",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4d5b995358e7798bc7e9d9db83c612a5-Abstract.html",
        "author": "Zhitang Chen; Kun Zhang; Laiwan Chan",
        "abstract": "In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be represented as a function of the direct causes themselves. However, in many real world problems, there are significant dependencies in the variances or energies, which indicates that causality may possibly take place at the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporal variance dependencies to represent a specific type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneous and temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR). We prove the identifiability of this model under the non-Gaussian assumption on the innovation processes. We also propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthesis and real world data are conducted to show the applicability of the proposed model and algorithms.",
        "bibtex": "@inproceedings{NIPS2012_4d5b9953,\n author = {Chen, Zhitang and Zhang, Kun and Chan, Laiwan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Causal discovery with scale-mixture model for spatiotemporal variance dependencies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/4d5b995358e7798bc7e9d9db83c612a5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4d5b995358e7798bc7e9d9db83c612a5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1366871,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11612333696252928296&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong",
        "aff_domain": "cse.cuhk.edu.hk;tuebingen.mpg.de;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;tuebingen.mpg.de;cse.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "CUHK;MPI-IS",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Hong Kong SAR;T\u00fcbingen",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;Germany"
    },
    {
        "id": "c5952c230c",
        "title": "Classification Calibration Dimension for General Multiclass Losses",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/24146db4eb48c718b84cae0a0799dcfc-Abstract.html",
        "author": "Harish G. Ramaswamy; Shivani Agarwal",
        "abstract": "We study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of \\emph{classification calibration dimension} of a multiclass loss matrix, which measures the smallest",
        "bibtex": "@inproceedings{NIPS2012_24146db4,\n author = {Ramaswamy, Harish G and Agarwal, Shivani},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Classification Calibration Dimension for General Multiclass Losses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/24146db4eb48c718b84cae0a0799dcfc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/24146db4eb48c718b84cae0a0799dcfc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 435151,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2119244067454384394&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore 560012, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore 560012, India",
        "aff_domain": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "email": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "ab1fd0f419",
        "title": "Clustering Aggregation as Maximum-Weight Independent Set",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html",
        "author": "Nan Li; Longin J. Latecki",
        "abstract": "We formulate clustering aggregation as a special instance of Maximum-Weight Independent Set (MWIS) problem. For a given dataset, an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters. The vertices, which represent the distinct clusters, are weighted by an internal index measuring both cohesion and separation. The edges connect the vertices whose corresponding clusters overlap. Intuitively, an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together. We formalize this intuition as the MWIS problem on the attributed graph, i.e., finding the heaviest subset of mutually non-adjacent vertices.  This MWIS problem exhibits a special structure. Since the clusters of each input clustering form a partition of the dataset, the vertices corresponding to each clustering form a maximal independent set (MIS) in the attributed graph. We propose a variant of simulated annealing method that takes advantage of this special structure. Our algorithm starts from each MIS, which is close to a distinct local optimum of the MWIS problem, and utilizes a local search heuristic to explore its neighborhood in order to find the MWIS. Extensive experiments on many challenging datasets show that: 1. our approach to clustering aggregation automatically decides the optimal number of clusters; 2. it does not require any parameter tuning for the underlying clustering algorithms; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance; 4. it is robust against moderate or even bad input clusterings.",
        "bibtex": "@inproceedings{NIPS2012_01386bd6,\n author = {Li, Nan and Latecki, Longin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering Aggregation as Maximum-Weight Independent Set},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/01386bd6d8e091c2ab4c7c7de644d37b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 551298,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9379904830551265998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer and Information Sciences, Temple University, Philadelphia, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, USA",
        "aff_domain": "temple.edu;temple.edu",
        "email": "temple.edu;temple.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Temple University",
        "aff_unique_dep": "Department of Computer and Information Sciences",
        "aff_unique_url": "https://www.temple.edu",
        "aff_unique_abbr": "Temple",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5763161ee3",
        "title": "Clustering Sparse Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html",
        "author": "Yudong Chen; Sujay Sanghavi; Huan Xu",
        "abstract": "We develop a new algorithm to cluster sparse unweighted graphs -- i.e. partition the nodes into disjoint clusters so that there is higher density within clusters, and low across clusters. By sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small, possibly vanishing in the size of the graph. Sparsity makes the problem noisier, and hence more difficult to solve.   Any clustering involves a tradeoff between minimizing two kinds of errors: missing edges within clusters and present edges across clusters. Our insight is that in the sparse case, these must be {\\em penalized differently}. We analyze our algorithm's performance on the natural, classical and widely studied ``planted partition'' model (also called the stochastic block model); we show that our algorithm can cluster sparser graphs, and with smaller clusters, than all previous methods. This is seen empirically as well.",
        "bibtex": "@inproceedings{NIPS2012_1e6e0a04,\n author = {Chen, Yudong and Sanghavi, Sujay and Xu, Huan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering Sparse Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1e6e0a04d20f50967c64dac2d639a577-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1e6e0a04d20f50967c64dac2d639a577-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 288343,
        "gs_citation": 209,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1766408903279403263&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Electrical and Computer Engineering, The University of Texas at Austin; Mechanical Engineering Department, National University of Singapore",
        "aff_domain": "utexas.edu;mail.utexas.edu;nus.edu.sg",
        "email": "utexas.edu;mail.utexas.edu;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Texas at Austin;National University of Singapore",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Mechanical Engineering Department",
        "aff_unique_url": "https://www.utexas.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "UT Austin;NUS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "5322fd8a1c",
        "title": "Clustering by Nonnegative Matrix Factorization Using Graph Random Walk",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ba2fd310dcaa8781a9a652a31baf3c68-Abstract.html",
        "author": "Zhirong Yang; Tele Hao; Onur Dikmen; Xi Chen; Erkki Oja",
        "abstract": "Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis.  However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.  Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk.  Our method can thus accommodate farther relationships between data samples.  Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.  The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix.  Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity.",
        "bibtex": "@inproceedings{NIPS2012_ba2fd310,\n author = {Yang, Zhirong and Hao, Tele and Dikmen, Onur and Chen, Xi and Oja, Erkki},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering by Nonnegative Matrix Factorization Using Graph Random Walk},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/ba2fd310dcaa8781a9a652a31baf3c68-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ba2fd310dcaa8781a9a652a31baf3c68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 330833,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7578102971800114903&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Information and Computer Science, Aalto University, 00076, Finland; Department of Information and Computer Science, Aalto University, 00076, Finland; Department of Information and Computer Science, Aalto University, 00076, Finland; Department of Information and Computer Science, Aalto University, 00076, Finland; Department of Information and Computer Science, Aalto University, 00076, Finland",
        "aff_domain": "aalto.fi;aalto.fi;aalto.fi;aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi;aalto.fi;aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Aalto University",
        "aff_unique_dep": "Department of Information and Computer Science",
        "aff_unique_url": "https://www.aalto.fi",
        "aff_unique_abbr": "Aalto",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "07f22e2119",
        "title": "Co-Regularized Hashing for Multimodal Data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html",
        "author": "Yi Zhen; Dit-Yan Yeung",
        "abstract": "Hashing-based methods provide a very promising approach to large-scale similarity search.  To obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically.  In this paper, we study hash function learning in the context of multimodal data.  We propose a novel multimodal hash function learning method, called Co-Regularized Hashing (CRH), based on a boosted co-regularization framework.  The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized.  We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets.",
        "bibtex": "@inproceedings{NIPS2012_5c049256,\n author = {Zhen, Yi and Yeung, Dit-Yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Co-Regularized Hashing for Multimodal Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/5c04925674920eb58467fb52ce4ef728-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 930969,
        "gs_citation": 238,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16463982531768755913&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology",
        "aff_domain": "cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "79345f84eb",
        "title": "Cocktail Party Processing via Structured Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html",
        "author": "Yuxuan Wang; Deliang Wang",
        "abstract": "While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random fields (CRFs) to classify speech dominance within each time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises.",
        "bibtex": "@inproceedings{NIPS2012_76dc611d,\n author = {Wang, Yuxuan and Wang, Deliang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cocktail Party Processing via Structured Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/76dc611d6ebaafc66cc0879c71b5db5c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 175109,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1540512681771727565&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "DepartmentofComputerScienceandEngineering; DepartmentofComputerScienceandEngineering+CenterforCognitiveScience",
        "aff_domain": "cse.ohio-state.edu;cse.ohio-state.edu",
        "email": "cse.ohio-state.edu;cse.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Department of Computer Science and Engineering;Center for Cognitive Science",
        "aff_unique_dep": "Computer Science and Engineering;Center for Cognitive Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6d84fd5d56",
        "title": "Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/5fd0b37cd7dbbb00f97ba6ce92bf5add-Abstract.html",
        "author": "Shinsuke Koyama",
        "abstract": "Statistical features of neuronal spike trains are known to be non-Poisson. Here, we investigate the extent to which the non-Poissonian feature affects the efficiency of transmitting information on fluctuating firing rates. For this purpose, we introduce the Kullbuck-Leibler (KL) divergence as a measure of the efficiency of information encoding, and assume that spike trains are generated by time-rescaled renewal processes. We show that the KL divergence determines the lower bound of the degree of rate fluctuations below which the temporal variation of the firing rates is undetectable from sparse data. We also show that the KL divergence, as well as the lower bound, depends not only on the variability of spikes in terms of the coefficient of variation, but also significantly on the higher-order moments of interspike interval (ISI) distributions. We examine three specific models that are commonly used for describing the stochastic nature of spikes (the gamma, inverse Gaussian (IG) and lognormal ISI distributions), and find that the time-rescaled renewal process with the IG distribution achieves the largest KL divergence, followed by the lognormal and gamma distributions.",
        "bibtex": "@inproceedings{NIPS2012_5fd0b37c,\n author = {Koyama, Shinsuke},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 453789,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2740325651979983246&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "http://skoyama.blogspot.jp",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "080fac9db1",
        "title": "Collaborative Gaussian Processes for Preference Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html",
        "author": "Neil Houlsby; Ferenc Huszar; Zoubin Ghahramani; Jose M. Hern\u00e1ndez-lobato",
        "abstract": "We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a \\emph{preference kernel} for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms.",
        "bibtex": "@inproceedings{NIPS2012_afdec700,\n author = {Houlsby, Neil and Huszar, Ferenc and Ghahramani, Zoubin and Hern\\'{a}ndez-lobato, Jose},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collaborative Gaussian Processes for Preference Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/afdec7005cc9f14302cd0474fd0f3c96-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/afdec7005cc9f14302cd0474fd0f3c96-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 396657,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16628944590085217122&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5911e31bfa",
        "title": "Collaborative Ranking With 17 Parameters",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3b712de48137572f3849aabd5666a4e3-Abstract.html",
        "author": "Maksims Volkovs; Richard S. Zemel",
        "abstract": "The primary application of collaborate filtering (CF) is to recommend a small set of items to a user, which entails ranking. Most approaches, however, formulate the CF problem as rating prediction, overlooking the ranking perspective. In this work we present a method for collaborative ranking that leverages the strengths of the two main CF approaches, neighborhood- and model-based. Our novel method is highly efficient, with only seventeen parameters to optimize and a single hyperparameter to tune, and beats the state-of-the-art collaborative ranking methods. We also show that parameters learned on one dataset yield excellent results on a very different dataset, without any retraining.",
        "bibtex": "@inproceedings{NIPS2012_3b712de4,\n author = {Volkovs, Maksims and Zemel, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collaborative Ranking With 17 Parameters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3b712de48137572f3849aabd5666a4e3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 469828,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2408822156866812985&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Toronto; University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "c24ca2c9bb",
        "title": "Communication-Efficient Algorithms for Statistical Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e7f8a7fb0b77bcb3b283af5be021448f-Abstract.html",
        "author": "Yuchen Zhang; Martin J. Wainwright; John C. Duchi",
        "abstract": "We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\\order(N^{-1}+(N/m)^{-2})$. Whenever $m \\le \\sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset.",
        "bibtex": "@inproceedings{NIPS2012_e7f8a7fb,\n author = {Zhang, Yuchen and Wainwright, Martin J and Duchi, John C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Communication-Efficient Algorithms for Statistical Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e7f8a7fb0b77bcb3b283af5be021448f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 194988,
        "gs_citation": 698,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16522544821790879034&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science+Department of Statistics",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Department of Statistics",
        "aff_unique_url": "https://web.mit.edu;",
        "aff_unique_abbr": "MIT;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "1df01cecb0",
        "title": "Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d240e3d38a8882ecad8633c8f9c78c9b-Abstract.html",
        "author": "Konstantinos Tsianos; Sean Lawlor; Michael G. Rabbat",
        "abstract": "We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\\cite{kRegExpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice.",
        "bibtex": "@inproceedings{NIPS2012_d240e3d3,\n author = {Tsianos, Konstantinos and Lawlor, Sean and Rabbat, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Communication/Computation Tradeoffs in Consensus-Based Distributed Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/d240e3d38a8882ecad8633c8f9c78c9b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d240e3d38a8882ecad8633c8f9c78c9b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 299179,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12609224135643190033&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c4417930a8",
        "title": "Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b7087c1f4f89e63af8d46f3b20271153-Abstract.html",
        "author": "Jeff Beck; Alexandre Pouget; Katherine A. Heller",
        "abstract": "Recent experiments have demonstrated that humans and animals typically reason probabilistically about their environment. This ability requires a neural code that represents probability distributions and neural circuits that are capable of implementing the operations of probabilistic inference. The proposed probabilistic population coding (PPC) framework provides a statistically efficient neural representation of probability distributions that is both broadly consistent with physiological measurements and capable of implementing some of the basic operations of probabilistic inference in a biologically plausible way. However, these experiments and the corresponding neural models have largely focused on simple (tractable) probabilistic computations such as cue combination, coordinate transformations, and decision making. As a result it remains unclear how to generalize this framework to more complex probabilistic computations. Here we address this short coming by showing that a very general approximate inference algorithm known as Variational Bayesian Expectation Maximization can be implemented within the linear PPC framework. We apply this approach to a generic problem faced by any given layer of cortex, namely the identification of latent causes of complex mixtures of spikes. We identify a formal equivalent between this spike pattern demixing problem and topic models used for document classification, in particular Latent Dirichlet Allocation (LDA). We then construct a neural network implementation of variational inference and learning for LDA that utilizes a linear PPC. This network relies critically on two non-linear operations: divisive normalization and super-linear facilitation, both of which are ubiquitously observed in neural circuits. We also demonstrate how online learning can be achieved using a variation of Hebb\u2019s rule and describe an extesion of this work which allows us to deal with time varying and correlated latent causes.",
        "bibtex": "@inproceedings{NIPS2012_b7087c1f,\n author = {Beck, Jeff and Pouget, Alexandre and Heller, Katherine A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b7087c1f4f89e63af8d46f3b20271153-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b7087c1f4f89e63af8d46f3b20271153-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b7087c1f4f89e63af8d46f3b20271153-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2229841,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3472259749250461355&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Brain and Cognitive Sciences, University of Rochester; Department of Statistical Science, Duke University; Department of Neuroscience, University of Geneva",
        "aff_domain": "bcs.rochester.edu;stat.duke.edu;unige.ch",
        "email": "bcs.rochester.edu;stat.duke.edu;unige.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Rochester;Duke University;University of Geneva",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences;Department of Statistical Science;Department of Neuroscience",
        "aff_unique_url": "https://www.rochester.edu;https://www.duke.edu;https://www.unige.ch",
        "aff_unique_abbr": "U of R;Duke;UNIGE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "5ea8c69e2f",
        "title": "Compressive Sensing MRI with Wavelet Tree Sparsity",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/65658fde58ab3c2b6e5132a39fae7cb9-Abstract.html",
        "author": "Chen Chen; Junzhou Huang",
        "abstract": "In Compressive Sensing Magnetic Resonance Imaging (CS-MRI), one can reconstruct a MR image with good quality from only a small number of measurements. This can significantly reduce MR scanning time. According to structured sparsity theory, the measurements can be further reduced to $\\mathcal{O}(K+\\log n)$ for tree-sparse data instead of $\\mathcal{O}(K+K\\log n)$ for standard $K$-sparse data with length $n$. However, few of existing algorithms has utilized this for CS-MRI, while most of them use Total Variation and wavelet sparse regularization. On the other side, some algorithms have been proposed for tree sparsity regularization, but few of them has validated   the benefit of tree structure in CS-MRI. In this paper, we propose a fast convex optimization algorithm to improve CS-MRI.  Wavelet sparsity, gradient sparsity and tree sparsity are all considered in our model for real MR images. The original complex problem is decomposed to three simpler subproblems then each of the subproblems can be efficiently solved with an iterative scheme. Numerous experiments have been conducted and show that the proposed algorithm outperforms the state-of-the-art CS-MRI algorithms, and gain better reconstructions results on real MR images than general tree based solvers or algorithms.",
        "bibtex": "@inproceedings{NIPS2012_65658fde,\n author = {Chen, Chen and Huang, Junzhou},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Compressive Sensing MRI with Wavelet Tree Sparsity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/65658fde58ab3c2b6e5132a39fae7cb9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1070756,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10038707696509518853&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Engineering, University of Texas at Arlington; Department of Computer Science and Engineering, University of Texas at Arlington",
        "aff_domain": "mavs.uta.edu;uta.edu",
        "email": "mavs.uta.edu;uta.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Arlington",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.uta.edu",
        "aff_unique_abbr": "UT Arlington",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Arlington",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0a6ce30c4c",
        "title": "Compressive neural representation of sparse, high-dimensional probabilities",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/26408ffa703a72e8ac0117e74ad46f33-Abstract.html",
        "author": "Zachary Pitkow",
        "abstract": "This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a high-dimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain.",
        "bibtex": "@inproceedings{NIPS2012_26408ffa,\n author = {Pitkow, Zachary},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Compressive neural representation of sparse, high-dimensional probabilities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/26408ffa703a72e8ac0117e74ad46f33-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/26408ffa703a72e8ac0117e74ad46f33-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/26408ffa703a72e8ac0117e74ad46f33-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1287747,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11029106555262917023&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Brain and Cognitive Sciences, University of Rochester, Rochester, NY 14607",
        "aff_domain": "bcs.rochester.edu",
        "email": "bcs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Rochester",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cabb46744f",
        "title": "Confusion-Based Online Learning and a Passive-Aggressive Scheme",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/bc7316929fe1545bf0b98d114ee3ecb8-Abstract.html",
        "author": "Liva Ralaivola",
        "abstract": "This paper provides the first ---to the best of our knowledge--- analysis of online learning algorithms for multiclass problems when the {\\em confusion} matrix is taken as a performance measure. The work builds upon recent and elegant results on noncommutative concentration inequalities, i.e. concentration inequalities that apply to matrices, and more precisely to matrix martingales.  We do establish generalization bounds for online learning algorithm and show how the theoretical study motivate the proposition of a new confusion-friendly learning procedure. This learning algorithm, called \\copa (for COnfusion Passive-Aggressive) is a passive-aggressive learning algorithm; it is shown that the update equations for \\copa can be computed analytically, thus allowing the user from having to recours to any optimization package to implement it.",
        "bibtex": "@inproceedings{NIPS2012_bc731692,\n author = {Ralaivola, Liva},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Confusion-Based Online Learning and a Passive-Aggressive Scheme},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/bc7316929fe1545bf0b98d114ee3ecb8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/bc7316929fe1545bf0b98d114ee3ecb8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 292674,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10832164063056805250&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "67075322c4",
        "title": "Context-Sensitive Decision Forests for Object Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/bcbe3365e6ac95ea2c0343a2395834dd-Abstract.html",
        "author": "Peter Kontschieder; Samuel R. Bul\u00f2; Antonio Criminisi; Pushmeet Kohli; Marcello Pelillo; Horst Bischof",
        "abstract": "In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2012_bcbe3365,\n author = {Kontschieder, Peter and Bul\\`{o}, Samuel and Criminisi, Antonio and Kohli, Pushmeet and Pelillo, Marcello and Bischof, Horst},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Context-Sensitive Decision Forests for Object Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/bcbe3365e6ac95ea2c0343a2395834dd-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/bcbe3365e6ac95ea2c0343a2395834dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/bcbe3365e6ac95ea2c0343a2395834dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2402846,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1064921770486942307&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d44270dd18",
        "title": "Continuous Relaxations for Discrete Hamiltonian Monte Carlo",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c913303f392ffc643f7240b180602652-Abstract.html",
        "author": "Yichuan Zhang; Zoubin Ghahramani; Amos J. Storkey; Charles A. Sutton",
        "abstract": "Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference,  results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difficult discrete systems. We demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems.",
        "bibtex": "@inproceedings{NIPS2012_c913303f,\n author = {Zhang, Yichuan and Ghahramani, Zoubin and Storkey, Amos J and Sutton, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Continuous Relaxations for Discrete Hamiltonian Monte Carlo},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c913303f392ffc643f7240b180602652-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c913303f392ffc643f7240b180602652-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c913303f392ffc643f7240b180602652-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 473273,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3103439545954917945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "School of Informatics, University of Edinburgh, United Kingdom; School of Informatics, University of Edinburgh, United Kingdom; School of Informatics, University of Edinburgh, United Kingdom; Department of Engineering, University of Cambridge, United Kingdom",
        "aff_domain": "sms.ed.ac.uk;inf.ed.ac.uk;ed.ac.uk;eng.cam.ac.uk",
        "email": "sms.ed.ac.uk;inf.ed.ac.uk;ed.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Edinburgh;University of Cambridge",
        "aff_unique_dep": "School of Informatics;Department of Engineering",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.cam.ac.uk",
        "aff_unique_abbr": "Edinburgh;Cambridge",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Edinburgh;Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "5b2e805cdc",
        "title": "Controlled Recognition Bounds for Visual Learning and Exploration",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2a50e9c2d6b89b95bcb416d6857f8b45-Abstract.html",
        "author": "Vasiliy Karasev; Alessandro Chiuso; Stefano Soatto",
        "abstract": "We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of \u201cvisual search\u201d of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a \u201cpassive\u201d agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an \u201comnipotent\u201d agent, capable of infinite control authority, can achieve arbitrarily good performance (asymptotically).",
        "bibtex": "@inproceedings{NIPS2012_2a50e9c2,\n author = {Karasev, Vasiliy and Chiuso, Alessandro and Soatto, Stefano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Controlled Recognition Bounds for Visual Learning and Exploration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2a50e9c2d6b89b95bcb416d6857f8b45-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2a50e9c2d6b89b95bcb416d6857f8b45-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2a50e9c2d6b89b95bcb416d6857f8b45-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1535751,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7171865683939775546&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9f1a47f1ad",
        "title": "Convergence Rate Analysis of MAP Coordinate Minimization Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/bad5f33780c42f2588878a9d07405083-Abstract.html",
        "author": "Ofer Meshi; Amir Globerson; Tommi S. Jaakkola",
        "abstract": "Finding maximum aposteriori (MAP) assignments in graphical models is an important task in many applications. Since the problem is generally hard, linear programming (LP) relaxations are often used. Solving these relaxations efficiently is thus an important practical problem. In recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual LP. However,these are generally not guaranteed to converge to a global optimum. One approach to remedy this is to smooth the LP, and perform coordinate descent on the smoothed dual. However, little is known about the convergence rate of this procedure. Here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates. We also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence. Empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima.",
        "bibtex": "@inproceedings{NIPS2012_bad5f337,\n author = {Meshi, Ofer and Globerson, Amir and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergence Rate Analysis of MAP Coordinate Minimization Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/bad5f33780c42f2588878a9d07405083-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/bad5f33780c42f2588878a9d07405083-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/bad5f33780c42f2588878a9d07405083-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/bad5f33780c42f2588878a9d07405083-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 341847,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10996335092445941037&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel; CSAIL, MIT, Cambridge, MA; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel",
        "aff_domain": "cs.huji.ac.il;csail.mit.edu;cs.huji.ac.il",
        "email": "cs.huji.ac.il;csail.mit.edu;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Hebrew University of Jerusalem;Massachusetts Institute of Technology",
        "aff_unique_dep": "School of Computer Science and Engineering;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "http://www.huji.ac.il;https://www.csail.mit.edu",
        "aff_unique_abbr": "HUJI;MIT",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Jerusalem;Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "3054640fb9",
        "title": "Convergence and Energy Landscape for Cheeger Cut Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/17c276c8e723eb46aef576537e9d56d0-Abstract.html",
        "author": "Xavier Bresson; Thomas Laurent; David Uminsky; James V. Brecht",
        "abstract": "Unsupervised clustering of scattered, noisy and high-dimensional data points is an important and difficult problem. Continuous relaxations of balanced cut problems  yield excellent clustering results. This paper provides rigorous convergence results for two algorithms that solve the relaxed Cheeger Cut minimization.  The first algorithm is a new steepest descent algorithm and the second one is a slight modification of the Inverse Power Method algorithm \\cite{pro:HeinBuhler10OneSpec}. While the steepest descent algorithm has better theoretical convergence properties,  in practice both algorithm perform equally.  We also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem, and relate this characterization to the convergence of the algorithms.",
        "bibtex": "@inproceedings{NIPS2012_17c276c8,\n author = {Bresson, Xavier and Laurent, Thomas and Uminsky, David and Brecht, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergence and Energy Landscape for Cheeger Cut Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/17c276c8e723eb46aef576537e9d56d0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/17c276c8e723eb46aef576537e9d56d0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 267797,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10018127702752400420&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "City University of Hong Kong; University of California, Riversize; University of San Francisco; University of California, Los Angeles",
        "aff_domain": "cityu.edu.hk;math.ucr.edu;usfca.edu;math.ucla.edu",
        "email": "cityu.edu.hk;math.ucr.edu;usfca.edu;math.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "City University of Hong Kong;University of California, Riverside;University of San Francisco;University of California, Los Angeles",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.cityu.edu.hk;https://www.ucr.edu;https://www.usfca.edu;https://www.ucla.edu",
        "aff_unique_abbr": "CityU;UCR;USF;UCLA",
        "aff_campus_unique_index": "0;1;3",
        "aff_campus_unique": "Hong Kong SAR;Riverside;;Los Angeles",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "1b29c9607d",
        "title": "Convex Multi-view Subspace Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1141938ba2c2b13f5505d7c424ebae5f-Abstract.html",
        "author": "Martha White; Xinhua Zhang; Dale Schuurmans; Yao-liang Yu",
        "abstract": "Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.  In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.  Experiments illustrate that the proposed method produces high quality results.",
        "bibtex": "@inproceedings{NIPS2012_1141938b,\n author = {White, Martha and Zhang, Xinhua and Schuurmans, Dale and Yu, Yao-liang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convex Multi-view Subspace Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1141938ba2c2b13f5505d7c424ebae5f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1141938ba2c2b13f5505d7c424ebae5f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 696718,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=266104993118503020&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada; Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada; Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada + National ICT Australia (NICTA), Machine Learning Group; Department of Computing Science, University of Alberta, Edmonton AB T6G 2E8, Canada",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "University of Alberta;National ICT Australia",
        "aff_unique_dep": "Department of Computing Science;Machine Learning Group",
        "aff_unique_url": "https://www.ualberta.ca;https://www.nicta.com.au",
        "aff_unique_abbr": "UAlberta;NICTA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Edmonton;",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "Canada;Australia"
    },
    {
        "id": "6a7b8bb5c8",
        "title": "Convolutional-Recursive Deep Learning for 3D Object Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3eae62bba9ddf64f69d49dc48e2dd214-Abstract.html",
        "author": "Richard Socher; Brody Huval; Bharath Bath; Christopher D. Manning; Andrew Y. Ng",
        "abstract": "Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We in- troduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, \ufb01xed-tree RNNs in order to compose higher order fea- tures. RNNs can be seen as combining convolution and pooling into one ef\ufb01cient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during train- ing and testing than comparable architectures such as two-layer CNNs.",
        "bibtex": "@inproceedings{NIPS2012_3eae62bb,\n author = {Socher, Richard and Huval, Brody and Bath, Bharath and Manning, Christopher D and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convolutional-Recursive Deep Learning for 3D Object Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3eae62bba9ddf64f69d49dc48e2dd214-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3eae62bba9ddf64f69d49dc48e2dd214-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3eae62bba9ddf64f69d49dc48e2dd214-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 754450,
        "gs_citation": 863,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6693756062088563335&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Computer Science Department, Stanford University, Stanford, CA 94305, USA; Computer Science Department, Stanford University, Stanford, CA 94305, USA; Computer Science Department, Stanford University, Stanford, CA 94305, USA; Computer Science Department, Stanford University, Stanford, CA 94305, USA; Computer Science Department, Stanford University, Stanford, CA 94305, USA",
        "aff_domain": "socher.org;stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu",
        "email": "socher.org;stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "www.socher.org",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d3fde9463",
        "title": "Cost-Sensitive Exploration in Bayesian Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6d9c547cf146054a5a720606a7694467-Abstract.html",
        "author": "Dongho Kim; Kee-eung Kim; Pascal Poupart",
        "abstract": "In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems.",
        "bibtex": "@inproceedings{NIPS2012_6d9c547c,\n author = {Kim, Dongho and Kim, Kee-eung and Poupart, Pascal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cost-Sensitive Exploration in Bayesian Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6d9c547cf146054a5a720606a7694467-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6d9c547cf146054a5a720606a7694467-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6d9c547cf146054a5a720606a7694467-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 185463,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5702967092796135935&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Engineering, University of Cambridge, UK; Dept of Computer Science, KAIST, Korea; School of Computer Science, University of Waterloo, Canada",
        "aff_domain": "cam.ac.uk;cs.kaist.ac.kr;cs.uwaterloo.ca",
        "email": "cam.ac.uk;cs.kaist.ac.kr;cs.uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Cambridge;KAIST;University of Waterloo",
        "aff_unique_dep": "Department of Engineering;Dept of Computer Science;School of Computer Science",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.kaist.ac.kr;https://uwaterloo.ca",
        "aff_unique_abbr": "Cambridge;KAIST;UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "United Kingdom;South Korea;Canada"
    },
    {
        "id": "3e142302cc",
        "title": "Coupling Nonparametric Mixtures via Latent Dirichlet Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1c383cd30b7c298ab50293adfecb7b18-Abstract.html",
        "author": "Dahua Lin; John W. Fisher",
        "abstract": "Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Specifically, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures, allowing mixture models to be coupled more flexibly. In addition, we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling.",
        "bibtex": "@inproceedings{NIPS2012_1c383cd3,\n author = {Lin, Dahua and Fisher, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Coupling Nonparametric Mixtures via Latent Dirichlet Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1c383cd30b7c298ab50293adfecb7b18-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1c383cd30b7c298ab50293adfecb7b18-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1c383cd30b7c298ab50293adfecb7b18-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1c383cd30b7c298ab50293adfecb7b18-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2338072,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14167613947870437317&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "MIT CSAIL; MIT CSAIL",
        "aff_domain": "mit.edu;csail.mit.edu",
        "email": "mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT CSAIL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6476eee1ef",
        "title": "Deep Learning of Invariant Features via Simulated Fixations in Video",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/13d7dc096493e1f77fb4ccf3eaf79df1-Abstract.html",
        "author": "Will Zou; Shenghuo Zhu; Kai Yu; Andrew Y. Ng",
        "abstract": "We apply salient feature detection and tracking in videos to simulate \ufb01xations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classi\ufb01cation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset.",
        "bibtex": "@inproceedings{NIPS2012_13d7dc09,\n author = {Zou, Will and Zhu, Shenghuo and Yu, Kai and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Learning of Invariant Features via Simulated Fixations in Video},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/13d7dc096493e1f77fb4ccf3eaf79df1-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/13d7dc096493e1f77fb4ccf3eaf79df1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/13d7dc096493e1f77fb4ccf3eaf79df1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 557073,
        "gs_citation": 209,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14358293240518169689&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering, Stanford University, CA; NECLaboratories America, Inc., Cupertino, CA; Department of Computer Science, Stanford University, CA; NECLaboratories America, Inc., Cupertino, CA",
        "aff_domain": "cs.stanford.edu;sv.nec-labs.com;cs.stanford.edu;sv.nec-labs.com",
        "email": "cs.stanford.edu;sv.nec-labs.com;cs.stanford.edu;sv.nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Stanford University;NEC Laboratories America, Inc.",
        "aff_unique_dep": "Department of Electrical Engineering;",
        "aff_unique_url": "https://www.stanford.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "Stanford;NEC Labs America",
        "aff_campus_unique_index": "0;1;2;1",
        "aff_campus_unique": "California;Cupertino;Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c991a433de",
        "title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html",
        "author": "Dan Ciresan; Alessandro Giusti; Luca M. Gambardella; J\u00fcrgen Schmidhuber",
        "abstract": "We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures  depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment {\\em biological} neuron membranes, we use a special type of deep {\\em artificial} neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a $512 \\times 512 \\times 30$ stack with known ground truth, and tested on a stack of the same  size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \\emph{rand error}, \\emph{warping error} and \\emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer.",
        "bibtex": "@inproceedings{NIPS2012_459a4ddc,\n author = {Ciresan, Dan and Giusti, Alessandro and Gambardella, Luca and Schmidhuber, J\\\"{u}rgen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/459a4ddcb586f24efd9395aa7662bc7c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1390206,
        "gs_citation": 2211,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=750234041877790810&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "IDSIA+USI-SUPSI; IDSIA+USI-SUPSI; IDSIA+USI-SUPSI; IDSIA+USI-SUPSI",
        "aff_domain": "idsia.ch;idsia.ch;idsia.ch;idsia.ch",
        "email": "idsia.ch;idsia.ch;idsia.ch;idsia.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Institute of Digital Technologies;Universit\u00e0 della Svizzera italiana",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.idsia.ch;https://www.usi.ch",
        "aff_unique_abbr": "IDSIA;USI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "5c9610b4cc",
        "title": "Deep Representations and Codes for Image Auto-Annotation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3c7781a36bcd6cf08c11a970fbe0e2a6-Abstract.html",
        "author": "Ryan Kiros; Csaba Szepesv\u00e1ri",
        "abstract": "The task of assigning a set of relevant tags to an image is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and fix an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of full sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we outperform or compete with existing annotation approaches that use over a dozen distinct image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efficient storage and fast comparisons. In our experiments, using deeper architectures always outperform shallow ones.",
        "bibtex": "@inproceedings{NIPS2012_3c7781a3,\n author = {Kiros, Ryan and Szepesv\\'{a}ri, Csaba},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Representations and Codes for Image Auto-Annotation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2356685,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13892927585369718303&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Department of Computing Science, University of Alberta, Edmonton, AB, Canada",
        "aff_domain": "ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "4c20c9c4f5",
        "title": "Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8c19f571e251e61cb8dd3612f26d5ecf-Abstract.html",
        "author": "Pietro D. Lena; Ken Nagata; Pierre F. Baldi",
        "abstract": "Residue-residue contact prediction is a fundamental problem in protein structure prediction. Hower, despite considerable research efforts, contact prediction methods are still largely unreliable. Here we introduce a novel deep machine-learning architecture which consists of a multidimensional stack of learning modules. For contact prediction, the idea is implemented as a three-dimensional stack of Neural Networks NN^k_{ij}, where i and j index the spatial coordinates of the contact map and k indexes ''time''. The temporal dimension is introduced to capture the fact that protein folding is not an instantaneous process, but rather a progressive refinement. Networks at level k in the stack can be trained in supervised fashion to refine the predictions produced by the previous level, hence addressing the problem of vanishing gradients, typical of deep architectures. Increased accuracy and generalization capabilities of this approach are established by rigorous comparison with other classical machine learning approaches for contact prediction. The deep approach leads to an accuracy for difficult long-range contacts of about 30%, roughly 10% above the state-of-the-art. Many variations in the architectures and the training algorithms are possible, leaving room for further improvements. Furthermore, the approach is applicable to other problems with strong underlying spatial and temporal components.",
        "bibtex": "@inproceedings{NIPS2012_8c19f571,\n author = {Lena, Pietro and Nagata, Ken and Baldi, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8c19f571e251e61cb8dd3612f26d5ecf-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8c19f571e251e61cb8dd3612f26d5ecf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8c19f571e251e61cb8dd3612f26d5ecf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 229597,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5555870477681257925&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Institute for Genomics and Bioinformatics, University of California, Irvine; Department of Computer Science, Institute for Genomics and Bioinformatics, University of California, Irvine; Department of Computer Science, Institute for Genomics and Bioinformatics, University of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ac266c23c7",
        "title": "Delay Compensation with Dynamical Synapses",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/85422afb467e9456013a2a51d4dff702-Abstract.html",
        "author": "Chi Fung; K. Wong; Si Wu",
        "abstract": "Time delay is pervasive in neural information processing. To achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system. In the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner. The state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents. The parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network. We also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude.",
        "bibtex": "@inproceedings{NIPS2012_85422afb,\n author = {Fung, Chi and Wong, K. and Wu, Si},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Delay Compensation with Dynamical Synapses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/85422afb467e9456013a2a51d4dff702-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/85422afb467e9456013a2a51d4dff702-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/85422afb467e9456013a2a51d4dff702-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/85422afb467e9456013a2a51d4dff702-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 143231,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17405684531761220700&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing 100875, China",
        "aff_domain": "ust.hk;ust.hk;bnu.edu.cn",
        "email": "ust.hk;ust.hk;bnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Beijing Normal University",
        "aff_unique_dep": ";State Key Laboratory of Cognitive Neuroscience and Learning",
        "aff_unique_url": "https://www.ust.hk;http://www.bnu.edu.cn",
        "aff_unique_abbr": "HKUST;BNU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Hong Kong;Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "8270e6923e",
        "title": "Density Propagation and Improved Bounds on the Partition Function",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e00406144c1e7e35240afed70f34166a-Abstract.html",
        "author": "Stefano Ermon; Ashish Sabharwal; Bart Selman; Carla P. Gomes",
        "abstract": "Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds.",
        "bibtex": "@inproceedings{NIPS2012_e0040614,\n author = {Ermon, Stefano and Sabharwal, Ashish and Selman, Bart and Gomes, Carla P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Density Propagation and Improved Bounds on the Partition Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e00406144c1e7e35240afed70f34166a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e00406144c1e7e35240afed70f34166a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 218392,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13525396772944672831&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6a81f9dc0e",
        "title": "Density-Difference Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html",
        "author": "Masashi Sugiyama; Takafumi Kanamori; Taiji Suzuki; Marthinus D. Plessis; Song Liu; Ichiro Takeuchi",
        "abstract": "We address the problem of estimating the difference between two probability densities. A naive approach  is a two-step procedure of first estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small estimation error incurred in the first stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure  for directly estimating the density difference without separately estimating two densities. We derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. We then show how the proposed density-difference estimator can be utilized in L2-distance approximation. Finally, we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection.",
        "bibtex": "@inproceedings{NIPS2012_f2fc9902,\n author = {Sugiyama, Masashi and Kanamori, Takafumi and Suzuki, Taiji and Plessis, Marthinus and Liu, Song and Takeuchi, Ichiro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Density-Difference Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f2fc990265c712c49d51a18a32b39f0c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 296257,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8536981210544403952&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d45e4783f1",
        "title": "Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6395ebd0f4b478145ecfbaf939454fa4-Abstract.html",
        "author": "Yung-kyun Noh; Frank Park; Daniel D. Lee",
        "abstract": "This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification.  We show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation.  Applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification.  Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose five different criteria for adaptively acquiring nearest neighbors.  Experiments with both synthetic and real datasets demonstrate the effectivness of our classification criteria.",
        "bibtex": "@inproceedings{NIPS2012_6395ebd0,\n author = {Noh, Yung-kyun and Park, Frank and Lee, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6395ebd0f4b478145ecfbaf939454fa4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 319885,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15252795462078543926&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Schl. of Mechanical and Aerospace Engineering, Seoul National University; Schl. of Mechanical and Aerospace Engineering, Seoul National University; Dept. of Electrical and Systems Engineering, University of Pennsylvania",
        "aff_domain": "snu.ac.kr;snu.ac.kr;seas.upenn.edu",
        "email": "snu.ac.kr;snu.ac.kr;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Seoul National University;University of Pennsylvania",
        "aff_unique_dep": "School of Mechanical and Aerospace Engineering;Dept. of Electrical and Systems Engineering",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.upenn.edu",
        "aff_unique_abbr": "SNU;UPenn",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seoul;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "3039b752b1",
        "title": "Dimensionality Dependent PAC-Bayes Margin Bound",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7380ad8a673226ae47fce7bff88e9c33-Abstract.html",
        "author": "Chi Jin; Liwei Wang",
        "abstract": "Margin is one of the most important concepts in machine learning. Previous margin bounds, both for SVM and for boosting, are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or infinite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors fixed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of finite dimension; and the two bounds tend to be equivalent as the dimension goes to infinity. In addition, we show that the VC bound for linear classifiers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classifiers.",
        "bibtex": "@inproceedings{NIPS2012_7380ad8a,\n author = {Jin, Chi and Wang, Liwei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dimensionality Dependent PAC-Bayes Margin Bound},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7380ad8a673226ae47fce7bff88e9c33-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7380ad8a673226ae47fce7bff88e9c33-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/7380ad8a673226ae47fce7bff88e9c33-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7380ad8a673226ae47fce7bff88e9c33-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 143737,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12632128357713455129&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Key Laboratory of Machine Perception, MOE, School of Physics, Peking University; Key Laboratory of Machine Perception, MOE, School of EECS, Peking University",
        "aff_domain": "gmail.com;cis.pku.edu.cn",
        "email": "gmail.com;cis.pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Physics",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "b9d6bef547",
        "title": "Dip-means: an incremental clustering method for estimating the number of clusters",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a8240cb8235e9c493a0c30607586166c-Abstract.html",
        "author": "Argyris Kalogeratos; Aristidis Likas",
        "abstract": "Learning the number of clusters is a key problem in data clustering. We present dip-means, a novel robust incremental method to learn the number of data clusters that may be used as a wrapper around any iterative clustering algorithm of the k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as a ''viewer'' and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of the distances between the viewer and the cluster members. Two important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations. Experimental results on artificial and real datasets indicate the effectiveness of our method and its superiority over analogous approaches.",
        "bibtex": "@inproceedings{NIPS2012_a8240cb8,\n author = {Kalogeratos, Argyris and Likas, Aristidis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dip-means: an incremental clustering method for estimating the number of clusters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a8240cb8235e9c493a0c30607586166c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a8240cb8235e9c493a0c30607586166c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a8240cb8235e9c493a0c30607586166c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 740883,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2587638004362513106&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Ioannina, Ioannina, Greece 45110; Department of Computer Science, University of Ioannina, Ioannina, Greece 45110",
        "aff_domain": "cs.uoi.gr;cs.uoi.gr",
        "email": "cs.uoi.gr;cs.uoi.gr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Ioannina",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uoi.gr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ioannina",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Greece"
    },
    {
        "id": "2c1122ddb7",
        "title": "Discriminative Learning of Sum-Product Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/573f7f25b7b1eb79a4ec6ba896debefd-Abstract.html",
        "author": "Robert Gens; Pedro Domingos",
        "abstract": "Sum-product networks are a new deep architecture that can perform fast, exact in- ference on high-treewidth models. Only generative methods for training SPNs have been proposed to date. In this paper, we present the \ufb01rst discriminative training algorithms for SPNs, combining the high accuracy of the former with the representational power and tractability of the latter. We show that the class of tractable discriminative SPNs is broader than the class of tractable generative ones, and propose an ef\ufb01cient backpropagation-style algorithm for computing the gradient of the conditional log likelihood. Standard gradient descent suffers from the diffusion problem, but networks with many layers can be learned reliably us- ing \u201chard\u201d gradient descent, where marginal inference is replaced by MPE infer- ence (i.e., inferring the most probable state of the non-evidence variables). The resulting updates have a simple and intuitive form. We test discriminative SPNs on standard image classi\ufb01cation tasks. We obtain the best results to date on the CIFAR-10 dataset, using fewer features than prior methods with an SPN architec- ture that learns local image structure discriminatively. We also report the highest published test accuracy on STL-10 even though we only use the labeled portion of the dataset.",
        "bibtex": "@inproceedings{NIPS2012_573f7f25,\n author = {Gens, Robert and Domingos, Pedro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative Learning of Sum-Product Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/573f7f25b7b1eb79a4ec6ba896debefd-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/573f7f25b7b1eb79a4ec6ba896debefd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/573f7f25b7b1eb79a4ec6ba896debefd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 871653,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6246363843297084595&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "982b88f1b7",
        "title": "Discriminatively Trained Sparse Code Gradients for Contour Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/16a5cdae362b8d27a1d8f8c7b78b4330-Abstract.html",
        "author": "Ren Xiaofeng; Liefeng Bo",
        "abstract": "Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most existing approaches including the state-of-the-art Global Pb (gPb) operator.  In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding.  We use K-SVD and Orthogonal Matching Pursuit for efficient dictionary learning and encoding, and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear SVM. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and find contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours).  Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth images and surface normals lead to promising contour detection using depth and depth+color, as verified on the NYU Depth Dataset.  Our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation.",
        "bibtex": "@inproceedings{NIPS2012_16a5cdae,\n author = {Xiaofeng, Ren and Bo, Liefeng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminatively Trained Sparse Code Gradients for Contour Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/16a5cdae362b8d27a1d8f8c7b78b4330-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/16a5cdae362b8d27a1d8f8c7b78b4330-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1783029,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3176523018087901353&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4d36e03b12",
        "title": "Distributed Non-Stochastic Experts",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html",
        "author": "Varun Kanade; Zhenming Liu; Bozidar Radunovic",
        "abstract": "We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator. At each time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon T, while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal O(\\sqrt{log(n)T}) regret bound at the cost of T communication. (ii) No communication: Each site runs an independent copy \u2013 the regret is O(\\sqrt{log(n)kT}) and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than \\sqrt{kT} and communication better than T. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret O(\\sqrt{k^{5(1+\\epsilon)/6} T}) and communication O(T/k^\\epsilon), for any value of \\epsilon in (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off.",
        "bibtex": "@inproceedings{NIPS2012_1385974e,\n author = {Kanade, Varun and Liu, Zhenming and Radunovic, Bozidar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Non-Stochastic Experts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1385974ed5904a438616ff7bdb3f7439-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1385974ed5904a438616ff7bdb3f7439-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 330946,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9726312677713175492&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "UC Berkeley + Harvard University; Princeton University + Harvard University; Microsoft Research",
        "aff_domain": "eecs.berkeley.edu;cs.princeton.edu;microsoft.com",
        "email": "eecs.berkeley.edu;cs.princeton.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;3",
        "aff_unique_norm": "University of California, Berkeley;Harvard University;Princeton University;Microsoft",
        "aff_unique_dep": ";;;Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.harvard.edu;https://www.princeton.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;Harvard;Princeton;MSR",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "19958eabf6",
        "title": "Distributed Probabilistic Learning for Camera Networks with Missing Data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/bb7946e7d85c81a9e69fee1cea4a087c-Abstract.html",
        "author": "Sejong Yoon; Vladimir Pavlovic",
        "abstract": "Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points.  However, many problems in wide-area surveillance can benefit from distributed modeling, either because of physical or computational constraints.  Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data.  In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing.  In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors.  We demonstrate the utility of this approach on the problem of distributed affine structure from motion.  Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations.",
        "bibtex": "@inproceedings{NIPS2012_bb7946e7,\n author = {Yoon, Sejong and Pavlovic, Vladimir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Probabilistic Learning for Camera Networks with Missing Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/bb7946e7d85c81a9e69fee1cea4a087c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/bb7946e7d85c81a9e69fee1cea4a087c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 548551,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8434291071575806804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Rutgers University; Department of Computer Science, Rutgers University",
        "aff_domain": "cs.rutgers.edu;cs.rutgers.edu",
        "email": "cs.rutgers.edu;cs.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rutgers University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rutgers.edu",
        "aff_unique_abbr": "Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "249bc83691",
        "title": "Dual-Space Analysis of the Sparse Linear Model",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/362e80d4df43b03ae6d3f8540cd63626-Abstract.html",
        "author": "Yi Wu; David P. Wipf",
        "abstract": "Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients.  These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters.  Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation.  The underlying cost functions can be related via a dual-space framework from Wipf et al. (2011), which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space.  This perspective is useful because some analyses or extensions are more conducive to development in one space or the other.  Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit.  As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I.  In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II.  For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular L1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations.",
        "bibtex": "@inproceedings{NIPS2012_362e80d4,\n author = {Wu, Yi and Wipf, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dual-Space Analysis of the Sparse Linear Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/362e80d4df43b03ae6d3f8540cd63626-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/362e80d4df43b03ae6d3f8540cd63626-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/362e80d4df43b03ae6d3f8540cd63626-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 154566,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12118815346898807235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Visual Computing Group, Microsoft Research Asia; Visual Computing Group, Microsoft Research Asia",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Visual Computing Group",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "MSRA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Asia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "e7abb601e2",
        "title": "Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c0c7c76d30bd3dcaefc96f40275bdc0a-Abstract.html",
        "author": "Christoph H. Lampert",
        "abstract": "We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable's marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufficiently certain about any individual decision. Whenever this is the case, we dynamically prune the variable we are confident about from the underlying factor graph. Consequently, at any time only samples of variable whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classification and image inpainting, shows that adaptive sampling can drastically accelerate MMP without sacrificing prediction accuracy.",
        "bibtex": "@inproceedings{NIPS2012_c0c7c76d,\n author = {Lampert, Christoph H},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1150579,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18422572132088892671&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "IST Austria (Institute of Science and Technology Austria)",
        "aff_domain": "ist.ac.at",
        "email": "ist.ac.at",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institute of Science and Technology Austria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ist.ac.at",
        "aff_unique_abbr": "IST Austria",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "685969bc84",
        "title": "Dynamical And-Or Graph Learning for Object Shape Modeling and Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1afa34a7f984eeabdbb0a7d494132ee5-Abstract.html",
        "author": "Xiaolong Wang; Liang Lin",
        "abstract": "This paper studies a novel discriminative part-based model to represent and recognize object shapes with an \u201cAnd-Or graph\u201d. We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches.",
        "bibtex": "@inproceedings{NIPS2012_1afa34a7,\n author = {Wang, Xiaolong and Lin, Liang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamical And-Or Graph Learning for Object Shape Modeling and Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1afa34a7f984eeabdbb0a7d494132ee5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 8165999,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7254937481691802588&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Sun Yat-Sen University; Sun Yat-Sen University",
        "aff_domain": "gmail.com;ieee.org",
        "email": "gmail.com;ieee.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.sysu.edu.cn/",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "074b0f51ad",
        "title": "Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c5ab0bc60ac7929182aadd08703f1ec6-Abstract.html",
        "author": "Michael C Hughes; Emily B. Fox; Erik B. Sudderth",
        "abstract": "Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences.",
        "bibtex": "@inproceedings{NIPS2012_c5ab0bc6,\n author = {Hughes, Michael C and Fox, Emily and Sudderth, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c5ab0bc60ac7929182aadd08703f1ec6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c5ab0bc60ac7929182aadd08703f1ec6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c5ab0bc60ac7929182aadd08703f1ec6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c5ab0bc60ac7929182aadd08703f1ec6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5700521,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3177761952123241027&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, Brown University; Department of Statistics, University of Washington; Department of Computer Science, Brown University",
        "aff_domain": "cs.brown.edu;stat.washington.edu;cs.brown.edu",
        "email": "cs.brown.edu;stat.washington.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Brown University;University of Washington",
        "aff_unique_dep": "Department of Computer Science;Department of Statistics",
        "aff_unique_url": "https://www.brown.edu;https://www.washington.edu",
        "aff_unique_abbr": "Brown;UW",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9f88839ff1",
        "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/35051070e572e47d2c26c241ab88307f-Abstract.html",
        "author": "Arthur Guez; David Silver; Peter Dayan",
        "abstract": "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.",
        "bibtex": "@inproceedings{NIPS2012_35051070,\n author = {Guez, Arthur and Silver, David and Dayan, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/35051070e572e47d2c26c241ab88307f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/35051070e572e47d2c26c241ab88307f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/35051070e572e47d2c26c241ab88307f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/35051070e572e47d2c26c241ab88307f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 700762,
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6002988384188863543&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;cs.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;cs.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "468881b525",
        "title": "Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3df1d4b96d8976ff5986393e8767f5b2-Abstract.html",
        "author": "Neil Burch; Marc Lanctot; Duane Szafron; Richard G. Gibson",
        "abstract": "Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player's actions according to the player's average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff.",
        "bibtex": "@inproceedings{NIPS2012_3df1d4b9,\n author = {Burch, Neil and Lanctot, Marc and Szafron, Duane and Gibson, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/3df1d4b96d8976ff5986393e8767f5b2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3df1d4b96d8976ff5986393e8767f5b2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 331748,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7961624022940765706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e6b32027db",
        "title": "Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a9eb812238f753132652ae09963a05e9-Abstract.html",
        "author": "Morteza Ibrahimi; Adel Javanmard; Benjamin V. Roy",
        "abstract": "We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system.  Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes. More recently, an asymptotic regret bound of $\\tilde{O}(\\sqrt{T})$ was shown for $T \\gg p$ where $p$ is the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present an adaptive control scheme that for $p \\gg 1$ and $T \\gg \\polylog(p)$ achieves a regret bound of $\\tilde{O}(p \\sqrt{T})$. In particular, our algorithm has an average cost of $(1+\\eps)$ times the optimum cost after $T = \\polylog(p) O(1/\\eps^2)$. This is in comparison to previous work on the dense dynamics where the algorithm needs $\\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy. We believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks.",
        "bibtex": "@inproceedings{NIPS2012_a9eb8122,\n author = {Ibrahimi, Morteza and Javanmard, Adel and Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a9eb812238f753132652ae09963a05e9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a9eb812238f753132652ae09963a05e9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a9eb812238f753132652ae09963a05e9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 202492,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9999194088128292622&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9c4135b02a",
        "title": "Efficient Sampling for Bipartite Matching Problems",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4c27cea8526af8cfee3be5e183ac9605-Abstract.html",
        "author": "Maksims Volkovs; Richard S. Zemel",
        "abstract": "Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in real-world applications of these problems is intractable, making efficient approximation methods essential for learning and inference. In this paper we propose a novel {\\it sequential matching} sampler based on the generalization of the Plackett-Luce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems - ranking and image correspondence - which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches.",
        "bibtex": "@inproceedings{NIPS2012_4c27cea8,\n author = {Volkovs, Maksims and Zemel, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Sampling for Bipartite Matching Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/4c27cea8526af8cfee3be5e183ac9605-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4c27cea8526af8cfee3be5e183ac9605-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1406045,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5472222313812076325&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Toronto; University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "46ac360a35",
        "title": "Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a4300b002bcfb71f291dac175d52df94-Abstract.html",
        "author": "Sander M. Bohte",
        "abstract": "Neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli. While adaptation is an intrinsic feature of neuronal models like the Hodgkin-Huxley model, the challenge is to integrate adaptation in models of neural computation.  Recent computational models like the Adaptive Spike Response Model implement adaptation as spike-based addition of fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents. Such adaptation has been shown to accurately model neural spiking behavior over a limited dynamic range. Taking a cue from kinetic models of adaptation, we propose a multiplicative Adaptive Spike Response Model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking. We show that unlike the additive adaptation model, the firing rate in the multiplicative adaptation model saturates to a maximum spike-rate. When simulating variance switching experiments, the model also quantitatively fits the experimental data over a wide dynamic range. Furthermore, dynamic threshold models of adaptation suggest a straightforward interpretation of neural activity in terms of dynamic signal encoding with shifted and weighted exponential kernels. We show that when thus encoding rectified filtered stimulus signals, the multiplicative Adaptive Spike Response Model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude, without changing model parameters.",
        "bibtex": "@inproceedings{NIPS2012_a4300b00,\n author = {Bohte, Sander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a4300b002bcfb71f291dac175d52df94-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a4300b002bcfb71f291dac175d52df94-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a4300b002bcfb71f291dac175d52df94-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2621501,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8534415500130816575&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d7fb370743",
        "title": "Efficient and direct estimation of a neural subunit model for sensory coding",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/56468d5607a5aaf1604ff5e15593b003-Abstract.html",
        "author": "Brett Vintch; Andrew Zaharia; J Movshon; Eero P. Simoncelli",
        "abstract": "Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a 'subunit' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (\"convolutional\") copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the 'subunits' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency.",
        "bibtex": "@inproceedings{NIPS2012_56468d56,\n author = {Vintch, Brett and Zaharia, Andrew and Movshon, J and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient and direct estimation of a neural subunit model for sensory coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/56468d5607a5aaf1604ff5e15593b003-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/56468d5607a5aaf1604ff5e15593b003-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/56468d5607a5aaf1604ff5e15593b003-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 492193,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8836197668004937530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Center for Neural Science; Center for Neural Science; Center for Neural Science; Center for Neural Science + Howard Hughes Medical Institute",
        "aff_domain": "cns.nyu.edu; ; ; ",
        "email": "cns.nyu.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "New York University;Howard Hughes Medical Institute",
        "aff_unique_dep": "Center for Neural Science;",
        "aff_unique_url": "https://www.cns.nyu.edu;https://www.hhmi.org",
        "aff_unique_abbr": "CNS;HHMI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5155c3eb85",
        "title": "Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ef0eff6088e2ed94f6caf720239f40d5-Abstract.html",
        "author": "Xue-xin Wei; Alan Stocker",
        "abstract": "A common challenge for Bayesian models of perception is the fact that the two fundamental Bayesian components, the prior distribution and the likelihood func- tion, are formally unconstrained. Here we argue that a neural system that emulates Bayesian inference is naturally constrained by the way it represents sensory infor- mation in populations of neurons. More speci\ufb01cally, we show that an ef\ufb01cient coding principle creates a direct link between prior and likelihood based on the underlying stimulus distribution. The resulting Bayesian estimates can show bi- ases away from the peaks of the prior distribution, a behavior seemingly at odds with the traditional view of Bayesian estimation, yet one that has been reported in human perception. We demonstrate that our framework correctly accounts for the repulsive biases previously reported for the perception of visual orientation, and show that the predicted tuning characteristics of the model neurons match the reported orientation tuning properties of neurons in primary visual cortex. Our results suggest that ef\ufb01cient coding is a promising hypothesis in constrain- ing Bayesian models of perceptual inference.",
        "bibtex": "@inproceedings{NIPS2012_ef0eff60,\n author = {Wei, Xue-xin and Stocker, Alan A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ef0eff6088e2ed94f6caf720239f40d5-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ef0eff6088e2ed94f6caf720239f40d5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ef0eff6088e2ed94f6caf720239f40d5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2332222,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8551376060539849377&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Departments of Psychology and Electrical and Systems Engineering, University of Pennsylvania; Departments of Psychology and Electrical and Systems Engineering, University of Pennsylvania",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "http://www.sas.upenn.edu/ astocker/lab",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Departments of Psychology and Electrical and Systems Engineering",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b49dd98378",
        "title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0e01938fc48a2cfb5f2217fbfb00722d-Abstract.html",
        "author": "Paul Vernaza; Drew Bagnell",
        "abstract": "The application of the maximum entropy principle to sequence   modeling has been popularized by methods such as Conditional Random   Fields (CRFs).  However, these approaches are generally limited to   modeling paths in discrete spaces of low dimensionality.  We   consider the problem of modeling distributions over paths in   continuous spaces of high dimensionality---a problem for which   inference is generally intractable.  Our main contribution is to   show that maximum entropy modeling of high-dimensional, continuous   paths is tractable as long as the constrained features    possess a certain kind of low dimensional structure.   In this case, we show that the associated {\\em partition function} is   symmetric and that this symmetry can be exploited to compute the   partition function efficiently in a compressed form.  Empirical   results are given showing an application of our method to maximum   entropy modeling of high dimensional human motion capture data.",
        "bibtex": "@inproceedings{NIPS2012_0e01938f,\n author = {Vernaza, Paul and Bagnell, Drew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient high dimensional maximum entropy modeling via symmetric partition functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0e01938fc48a2cfb5f2217fbfb00722d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0e01938fc48a2cfb5f2217fbfb00722d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/0e01938fc48a2cfb5f2217fbfb00722d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0e01938fc48a2cfb5f2217fbfb00722d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2368858,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5532015345420970972&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "cmu.edu;ri.cmu.edu",
        "email": "cmu.edu;ri.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "The Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4ed0382f4e",
        "title": "Emergence of Object-Selective Features in Unsupervised Feature Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/39e4973ba3321b80f37d9b55f63ed8b8-Abstract.html",
        "author": "Adam Coates; Andrej Karpathy; Andrew Y. Ng",
        "abstract": "Recent work in unsupervised feature learning has focused on the goal   of discovering high-level features from unlabeled images.  Much   progress has been made in this direction, but in most cases it is   still standard to use a large amount of labeled data in order to   construct detectors sensitive to object classes or other complex   patterns in the data.  In this paper, we aim to test the hypothesis   that unsupervised feature learning methods, provided with only   unlabeled data, can learn high-level, invariant features that are   sensitive to commonly-occurring objects.  Though a handful of prior   results suggest that this is possible when each object class   accounts for a large fraction of the data (as in many labeled   datasets), it is unclear whether something similar can be   accomplished when dealing with completely unlabeled data.  A major   obstacle to this test, however, is scale: we cannot expect to   succeed with small datasets or with small numbers of learned   features.  Here, we propose a large-scale feature learning system   that enables us to carry out this experiment, learning 150,000   features from tens of millions of unlabeled images.  Based on two   scalable clustering algorithms (K-means and agglomerative   clustering), we find that our simple system can discover features   sensitive to a commonly occurring object class (human faces) and can   also combine these into detectors invariant to significant global   distortions like large translations and scale.",
        "bibtex": "@inproceedings{NIPS2012_39e4973b,\n author = {Coates, Adam and Karpathy, Andrej and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Emergence of Object-Selective Features in Unsupervised Feature Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/39e4973ba3321b80f37d9b55f63ed8b8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1373484,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10794633353847907100&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "696a47a6b1",
        "title": "Ensemble weighted kernel estimators for multivariate entropy estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/92c8c96e4c37100777c7190b76d28233-Abstract.html",
        "author": "Kumar Sricharan; Alfred O. Hero",
        "abstract": "The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, kernel plug-in estimators suffer from the curse of dimensionality, wherein the MSE rate of convergence is glacially slow - of order  $O(T^{-{\\gamma}/{d}})$, where $T$ is the number of samples, and $\\gamma>0$ is a rate parameter. In this paper, it is shown that for sufficiently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order $O(T^{-1})$. Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed offline. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates.",
        "bibtex": "@inproceedings{NIPS2012_92c8c96e,\n author = {Sricharan, Kumar and Hero, Alfred},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ensemble weighted kernel estimators for multivariate entropy estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/92c8c96e4c37100777c7190b76d28233-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 184716,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14143456899450458033&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of EECS, University of Michigan; Department of EECS, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c577bd2ddb",
        "title": "Entangled Monte Carlo",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/dc4c44f624d600aa568390f1f1104aa0-Abstract.html",
        "author": "Seong-hwan Jun; Liangliang Wang; Alexandre Bouchard-c\u00f4t\u00e9",
        "abstract": "We propose a novel method for scalable parallelization of SMC algorithms, Entangled Monte Carlo simulation (EMC).  EMC avoids the transmission of particles between  nodes, and instead reconstructs them from the particle genealogy. In particular, we show that we can reduce the communication to the particle weights for each machine while efficiently maintaining implicit global coherence of the parallel simulation. We explain methods to efficiently maintain a genealogy of particles from which any particle can be reconstructed. We demonstrate using examples from Bayesian phylogenetic that the computational gain from parallelization using EMC significantly outweighs the cost of particle reconstruction. The timing experiments show that reconstruction of particles is indeed much more efficient as compared to transmission of particles.",
        "bibtex": "@inproceedings{NIPS2012_dc4c44f6,\n author = {Jun, Seong-hwan and Wang, Liangliang and Bouchard-c\\^{o}t\\'{e}, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Entangled Monte Carlo},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/dc4c44f624d600aa568390f1f1104aa0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/dc4c44f624d600aa568390f1f1104aa0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 988692,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4878565737153682102&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistics, University of British Columbia; Department of Statistics, University of British Columbia; Department of Statistics, University of British Columbia",
        "aff_domain": "stat.ubc.ca;stat.ubc.ca;stat.ubc.ca",
        "email": "stat.ubc.ca;stat.ubc.ca;stat.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "36e642485f",
        "title": "Entropy Estimations Using Correlated Symmetric Stable Random Projections",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/5e76bef6e019b2541ff53db39f407a98-Abstract.html",
        "author": "Ping Li; Cun-hui Zhang",
        "abstract": "Methods for efficiently estimating the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Our experiments confirm that this method is able to substantially better approximate the Shannon entropy compared to the prior state-of-the-art.",
        "bibtex": "@inproceedings{NIPS2012_5e76bef6,\n author = {Li, Ping and Zhang, Cun-hui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Entropy Estimations Using Correlated Symmetric Stable Random Projections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/5e76bef6e019b2541ff53db39f407a98-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/5e76bef6e019b2541ff53db39f407a98-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/5e76bef6e019b2541ff53db39f407a98-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 394337,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6496842862936128487&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistical Science, Cornell University; Department of Statistics and Biostatistics, Rutgers University",
        "aff_domain": "cornell.edu;stat.rutgers.edu",
        "email": "cornell.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cornell University;Rutgers University",
        "aff_unique_dep": "Department of Statistical Science;Department of Statistics and Biostatistics",
        "aff_unique_url": "https://www.cornell.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "Cornell;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3ac4dac2bb",
        "title": "Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential _1-Minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7bccfde7714a1ebadf06c5f4cea752c1-Abstract.html",
        "author": "Demba Ba; Behtash Babadi; Patrick Purdon; Emery Brown",
        "abstract": "We consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^K$, for which the increments $x_k-x_{k-1}$ are $S_k$-sparse (with $S_k$ typically smaller than $S_1$), based on linear measurements $(y_k = A_k x_k + e_k)_{k=1}^K$, where $A_k$ and $e_k$ denote the measurement matrix and noise, respectively. Assuming each $A_k$ obeys the restricted isometry property (RIP) of a certain order---depending only on $S_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^K$ \\emph{exactly}. This is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.",
        "bibtex": "@inproceedings{NIPS2012_7bccfde7,\n author = {Ba, Demba and Babadi, Behtash and Purdon, Patrick and Brown, Emery},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential \\_1-Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7bccfde7714a1ebadf06c5f4cea752c1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 148294,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3889601882781732921&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "MIT Department of BCS, Cambridge, MA 02139 + MGH Department of Anesthesia, Critical Care and Pain Medicine, Boston, MA 02114; MIT Department of BCS, Cambridge, MA 02139 + MGH Department of Anesthesia, Critical Care and Pain Medicine, Boston, MA 02114; MGH Department of Anesthesia, Critical Care and Pain Medicine, Boston, MA 02114; MIT Department of BCS, Cambridge, MA 02139 + MGH Department of Anesthesia, Critical Care and Pain Medicine, Boston, MA 02114",
        "aff_domain": "mit.edu;nmr.mgh.harvard.edu;nmr.mgh.harvard.edu;neurostat.mit.edu",
        "email": "mit.edu;nmr.mgh.harvard.edu;nmr.mgh.harvard.edu;neurostat.mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;1;0+1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Massachusetts General Hospital",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences;Department of Anesthesia, Critical Care and Pain Medicine",
        "aff_unique_url": "https://www.mit.edu;https://www.mgh.harvard.edu",
        "aff_unique_abbr": "MIT;MGH",
        "aff_campus_unique_index": "0+1;0+1;1;0+1",
        "aff_campus_unique": "Cambridge;Boston",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "56e814aeb5",
        "title": "Expectation Propagation in Gaussian Process Dynamical Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html",
        "author": "Marc Deisenroth; Shakir Mohamed",
        "abstract": "Rich and complex time-series data, such as those generated from engineering sys- tems, financial markets, videos or neural recordings are now a common feature of modern data analysis. Explaining the phenomena underlying these diverse data sets requires flexible and accurate models. In this paper, we promote Gaussian process dynamical systems as a rich model class appropriate for such analysis. In particular, we present a message passing algorithm for approximate inference in GPDSs based on expectation propagation. By phrasing inference as a general mes- sage passing problem, we iterate forward-backward smoothing. We obtain more accurate posterior distributions over latent structures, resulting in improved pre- dictive performance compared to state-of-the-art GPDS smoothers, which are spe- cial cases of our general iterative message passing algorithm. Hence, we provide a unifying approach within which to contextualize message passing in GPDSs.",
        "bibtex": "@inproceedings{NIPS2012_e53a0a29,\n author = {Deisenroth, Marc and Mohamed, Shakir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Expectation Propagation in Gaussian Process Dynamical Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e53a0a2978c28872a4505bdb51db06dc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 627212,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14003130235477583828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Technische Universit \u00a8at Darmstadt, Germany; Department of Computer Science, University of British Columbia, Canada",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technische Universit\u00e4t Darmstadt;University of British Columbia",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.tu-darmstadt.de;https://www.ubc.ca",
        "aff_unique_abbr": "TUD;UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "id": "81300a738b",
        "title": "Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a0a080f42e6f13b3a2df133f073095dd-Abstract.html",
        "author": "Manuel Lopes; Tobias Lang; Marc Toussaint; Pierre-yves Oudeyer",
        "abstract": "Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as Rmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.",
        "bibtex": "@inproceedings{NIPS2012_a0a080f4,\n author = {Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-yves},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/a0a080f42e6f13b3a2df133f073095dd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a0a080f42e6f13b3a2df133f073095dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 318532,
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11201671690048983205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b97aafe076",
        "title": "Exponential Concentration for Mutual Information Estimation with Application to Forests",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c8ba76c279269b1c6bc8a07e38e78fa4-Abstract.html",
        "author": "Han Liu; Larry Wasserman; John D. Lafferty",
        "abstract": "We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph.",
        "bibtex": "@inproceedings{NIPS2012_c8ba76c2,\n author = {Liu, Han and Wasserman, Larry and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exponential Concentration for Mutual Information Estimation with Application to Forests},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c8ba76c279269b1c6bc8a07e38e78fa4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c8ba76c279269b1c6bc8a07e38e78fa4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c8ba76c279269b1c6bc8a07e38e78fa4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 245104,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15502344060877206131&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Operations Research and Financial Engineering, Princeton University, NJ 08544; Department of Computer Science+Department of Statistics, University of Chicago, IL 60637; Department of Statistics+Machine Learning Department, Carnegie Mellon University, PA 15231",
        "aff_domain": "princeton.edu;galton.uchicago.edu;stat.cmu.edu",
        "email": "princeton.edu;galton.uchicago.edu;stat.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;3+4",
        "aff_unique_norm": "Princeton University;Unknown Institution;University of Chicago;University Affiliation Not Specified;Carnegie Mellon University",
        "aff_unique_dep": "Department of Operations Research and Financial Engineering;Department of Computer Science;Department of Statistics;Department of Statistics;Machine Learning Department",
        "aff_unique_url": "https://www.princeton.edu;;https://www.uchicago.edu;;https://www.cmu.edu",
        "aff_unique_abbr": "Princeton;;UChicago;;CMU",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Chicago;Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "0ab6e587e1",
        "title": "Factorial LDA: Sparse Multi-Dimensional Text Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/68d13cf26c4b4f4f932e3eff990093ba-Abstract.html",
        "author": "Michael Paul; Mark Dredze",
        "abstract": "Multi-dimensional latent variable models can capture the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional latent variable model in which a document is influenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientific discipline, and focus (e.g. methods vs. applications.) Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors.",
        "bibtex": "@inproceedings{NIPS2012_68d13cf2,\n author = {Paul, Michael and Dredze, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Factorial LDA: Sparse Multi-Dimensional Text Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/68d13cf26c4b4f4f932e3eff990093ba-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/68d13cf26c4b4f4f932e3eff990093ba-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/68d13cf26c4b4f4f932e3eff990093ba-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 573450,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4544913976084646973&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Human Language Technology Center of Excellence (HLTCOE) + Center for Language and Speech Processing (CLSP), Johns Hopkins University; Human Language Technology Center of Excellence (HLTCOE) + Center for Language and Speech Processing (CLSP), Johns Hopkins University",
        "aff_domain": "cs.jhu.edu;cs.jhu.edu",
        "email": "cs.jhu.edu;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Human Language Technology Center of Excellence;Johns Hopkins University",
        "aff_unique_dep": "Human Language Technology;Center for Language and Speech Processing (CLSP)",
        "aff_unique_url": ";https://www.jhu.edu",
        "aff_unique_abbr": "HLTCOE;JHU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fba2388de4",
        "title": "Factoring nonnegative matrices with linear programs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/08c5433a60135c32e34f46a71175850c-Abstract.html",
        "author": "Ben Recht; Christopher Re; Joel Tropp; Victor Bittorf",
        "abstract": "This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes.",
        "bibtex": "@inproceedings{NIPS2012_08c5433a,\n author = {Recht, Ben and Re, Christopher and Tropp, Joel and Bittorf, Victor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Factoring nonnegative matrices with linear programs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/08c5433a60135c32e34f46a71175850c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/08c5433a60135c32e34f46a71175850c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/08c5433a60135c32e34f46a71175850c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 360004,
        "gs_citation": 285,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9202886442821712266&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "Computer Sciences, University of Wisconsin; Computer Sciences, University of Wisconsin; Computer Sciences, University of Wisconsin; Computing and Mathematical Sciences, California Institute of Technology",
        "aff_domain": "cs.wisc.edu;cs.wisc.edu;cs.wisc.edu;cms.caltech.edu",
        "email": "cs.wisc.edu;cs.wisc.edu;cs.wisc.edu;cms.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Wisconsin;California Institute of Technology",
        "aff_unique_dep": "Computer Sciences;Computing and Mathematical Sciences",
        "aff_unique_url": "https://www.wisc.edu;https://www.caltech.edu",
        "aff_unique_abbr": "UW;Caltech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pasadena",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "363293a55e",
        "title": "Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e94f63f579e05cb49c05c2d050ead9c0-Abstract.html",
        "author": "Mohammad Emtiyaz Khan; Shakir Mohamed; Kevin P. Murphy",
        "abstract": "We present a new variational inference algorithm for Gaussian processes with non-conjugate likelihood functions. This includes binary and multi-class classification, as well as ordinal regression. Our method constructs a convex lower bound, which can be optimized by using an efficient fixed point update method. We then show empirically that our new approach is much faster than existing methods without any degradation in performance.",
        "bibtex": "@inproceedings{NIPS2012_e94f63f5,\n author = {Khan, Mohammad Emtiyaz and Mohamed, Shakir and Murphy, Kevin P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e94f63f579e05cb49c05c2d050ead9c0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 311316,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5343430892785636573&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "68b2026a90",
        "title": "Fast Resampling Weighted v-Statistics",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html",
        "author": "Chunxiao Zhou; Jiseong Park; Yun Fu",
        "abstract": "In this paper, a novel, computationally fast, and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed. To avoid any real resampling, we have linked this problem with finite group action and converted it into a problem of orbit enumeration. For further computational cost reduction, an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively. The computational complexity analysis shows reduction in the computational cost from n! or nn level to low-order polynomial level.",
        "bibtex": "@inproceedings{NIPS2012_2b24d495,\n author = {Zhou, Chunxiao and Park, Jiseong and Fu, Yun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Resampling Weighted v-Statistics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/2b24d495052a8ce66358eb576b8912c8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2b24d495052a8ce66358eb576b8912c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 421484,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:a7yUdV-cvZwJ:scholar.google.com/&scioq=Fast+Resampling+Weighted+v-Statistics&hl=en&as_sdt=0,33",
        "gs_version_total": 7,
        "aff": "Mark O. Hatfield Clinical Research Center, National Institutes of Health, Bethesda, MD 20892; Dept of Math, George Mason Univ, Fairfax, VA 22030; Dept of ECE, Northeastern Univ, Boston, MA 02115",
        "aff_domain": "nih.gov;gmail.com;ece.neu.edu",
        "email": "nih.gov;gmail.com;ece.neu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "National Institutes of Health;George Mason University;Northeastern University",
        "aff_unique_dep": "Mark O. Hatfield Clinical Research Center;Department of Mathematics;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.nih.gov;https://www.gmu.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "NIH;GMU;NU",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Bethesda;Fairfax;Boston",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "58182ba48e",
        "title": "Fast Variational Inference in the Conjugate Exponential Family",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/50905d7b2216bfeccb5b41016357176b-Abstract.html",
        "author": "James Hensman; Magnus Rattray; Neil D. Lawrence",
        "abstract": "We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.",
        "bibtex": "@inproceedings{NIPS2012_50905d7b,\n author = {Hensman, James and Rattray, Magnus and Lawrence, Neil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Variational Inference in the Conjugate Exponential Family},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/50905d7b2216bfeccb5b41016357176b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/50905d7b2216bfeccb5b41016357176b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 288527,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16279163189456061717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, The University of Sheffield + Shef\ufb01eld Institute for Translational Neuroscience, SITraN; Faculty of Life Science, The University of Manchester; Department of Computer Science, The University of Sheffield + Shef\ufb01eld Institute for Translational Neuroscience, SITraN",
        "aff_domain": "sheffield.ac.uk;manchester.ac.uk;sheffield.ac.uk",
        "email": "sheffield.ac.uk;manchester.ac.uk;sheffield.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "University of Sheffield;Sheffield Institute for Translational Neuroscience;University of Manchester",
        "aff_unique_dep": "Department of Computer Science;Translational Neuroscience;Faculty of Life Science",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.sheffield.ac.uk/sitran;https://www.manchester.ac.uk",
        "aff_unique_abbr": "Sheffield;SITraN;UoM",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "5ceb1bd46f",
        "title": "FastEx: Hash Clustering with Exponential Families",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/018b59ce1fd616d874afad0f44ba338d-Abstract.html",
        "author": "Amr Ahmed; Sujith Ravi; Alex J. Smola; Shravan M. Narayanamurthy",
        "abstract": "Clustering is a key component in data analysis toolbox. Despite its   importance, scalable algorithms often eschew rich statistical models   in favor of simpler descriptions such as $k$-means clustering. In   this paper we present a sampler, capable of estimating   mixtures of exponential families. At its heart lies a novel proposal distribution using random   projections to achieve high throughput in generating proposals, which is crucial   for clustering models with large numbers of clusters.",
        "bibtex": "@inproceedings{NIPS2012_018b59ce,\n author = {Ahmed, Amr and Ravi, Sujith and Smola, Alex and Narayanamurthy, Shravan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {FastEx: Hash Clustering with Exponential Families},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/018b59ce1fd616d874afad0f44ba338d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 355803,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15882808823012975705&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Research at Google, Mountain View, CA; Research at Google, Mountain View, CA; Microsoft Research, Bangalore, India; Research at Google, Mountain View, CA",
        "aff_domain": "google.com;google.com;gmail.com;smola.org",
        "email": "google.com;google.com;gmail.com;smola.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Google;Microsoft",
        "aff_unique_dep": "Google Research;Microsoft Research",
        "aff_unique_url": "https://research.google;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "Google;MSR",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Mountain View;Bangalore",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "9495493606",
        "title": "Feature Clustering for Accelerating Parallel Coordinate Descent",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4e732ced3463d06de0ca9a15b6153677-Abstract.html",
        "author": "Chad Scherrer; Ambuj Tewari; Mahantesh Halappanavar; David Haglin",
        "abstract": "Large scale $\\ell_1$-regularized loss minimization problems arise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\\ell_1$ regularized problems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\\ell_1$-regularization problems.",
        "bibtex": "@inproceedings{NIPS2012_4e732ced,\n author = {Scherrer, Chad and Tewari, Ambuj and Halappanavar, Mahantesh and Haglin, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feature Clustering for Accelerating Parallel Coordinate Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4e732ced3463d06de0ca9a15b6153677-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4e732ced3463d06de0ca9a15b6153677-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/4e732ced3463d06de0ca9a15b6153677-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4e732ced3463d06de0ca9a15b6153677-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 921525,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10644254060876021627&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Independent Consultant; Department of Statistics, University of Michigan; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory",
        "aff_domain": "gmail.com;umich.edu;pnnl.gov;pnnl.gov",
        "email": "gmail.com;umich.edu;pnnl.gov;pnnl.gov",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Independent Consultant;University of Michigan;Pacific Northwest National Laboratory",
        "aff_unique_dep": ";Department of Statistics;",
        "aff_unique_url": ";https://www.umich.edu;https://www.pnnl.gov",
        "aff_unique_abbr": ";UM;PNNL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ann Arbor",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "a972200c4d",
        "title": "Feature-aware Label Space Dimension Reduction for Multi-label Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d4c2e4a3297fe25a71d030b67eb83bfc-Abstract.html",
        "author": "Yao-nan Chen; Hsuan-tien Lin",
        "abstract": "Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets.",
        "bibtex": "@inproceedings{NIPS2012_d4c2e4a3,\n author = {Chen, Yao-nan and Lin, Hsuan-tien},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feature-aware Label Space Dimension Reduction for Multi-label Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d4c2e4a3297fe25a71d030b67eb83bfc-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d4c2e4a3297fe25a71d030b67eb83bfc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d4c2e4a3297fe25a71d030b67eb83bfc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 268994,
        "gs_citation": 318,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11434593577161813602&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science & Information Engineering, National Taiwan University; Department of Computer Science & Information Engineering, National Taiwan University",
        "aff_domain": "csie.ntu.edu.tw;csie.ntu.edu.tw",
        "email": "csie.ntu.edu.tw;csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "Department of Computer Science & Information Engineering",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "e2f4c8d879",
        "title": "Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d2ed45a52bc0edfa11c2064e9edee8bf-Abstract.html",
        "author": "Antonino Freno; Mikaela Keller; Marc Tommasi",
        "abstract": "Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches.",
        "bibtex": "@inproceedings{NIPS2012_d2ed45a5,\n author = {Freno, Antonino and Keller, Mikaela and Tommasi, Marc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/d2ed45a52bc0edfa11c2064e9edee8bf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d2ed45a52bc0edfa11c2064e9edee8bf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 123367,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12475938592032027079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "INRIALille \u2013NordEurope; INRIALille \u2013NordEurope + Universit\u00b4e Charles de Gaulle \u2013 Lille 3; INRIALille \u2013NordEurope + Universit\u00b4e Charles de Gaulle \u2013 Lille 3",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0+1",
        "aff_unique_norm": "INRIA Lille - Nord Europe;Universit\u00e9 Charles de Gaulle",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.inria.fr/lille;https://www.univ-lille.fr",
        "aff_unique_abbr": "INRIA Lille;UCDG",
        "aff_campus_unique_index": "0;0+0;0+0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "8f76a5c367",
        "title": "Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8e296a067a37563370ded05f5a3bf3ec-Abstract.html",
        "author": "Ehsan Elhamifar; Guillermo Sapiro; Ren\u00e9 Vidal",
        "abstract": "Given pairwise dissimilarities between data points, we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection. We formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming. The solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives. We obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives. When data points are distributed around multiple clusters according to the dissimilarities, we show that the data in each cluster select only representatives from that cluster. Unlike metric-based methods, our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality. We demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text.",
        "bibtex": "@inproceedings{NIPS2012_8e296a06,\n author = {Elhamifar, Ehsan and Sapiro, Guillermo and Vidal, Ren\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8e296a067a37563370ded05f5a3bf3ec-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8e296a067a37563370ded05f5a3bf3ec-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8e296a067a37563370ded05f5a3bf3ec-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1401911,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11366837345959905477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a281d13b44",
        "title": "Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e555ebe0ce426f7f9b2bef0706315e0c-Abstract.html",
        "author": "Andre Wibisono; Martin J. Wainwright; Michael I. Jordan; John C. Duchi",
        "abstract": "We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\\sqrt{\\dim}$ in convergence rate over traditional stochastic gradient methods, where $\\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors.",
        "bibtex": "@inproceedings{NIPS2012_e555ebe0,\n author = {Wibisono, Andre and Wainwright, Martin J and Jordan, Michael and Duchi, John C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/e555ebe0ce426f7f9b2bef0706315e0c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e555ebe0ce426f7f9b2bef0706315e0c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 224629,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17200423017139696850&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley + Department of Statistics, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley + Department of Statistics, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+0;0+0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0+0;0+0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c6ab8ee2bf",
        "title": "Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a50abba8132a77191791390c3eb19fe7-Abstract.html",
        "author": "Dijun Luo; Heng Huang; Feiping Nie; Chris H. Ding",
        "abstract": "In many graph-based machine learning and data mining approaches, the quality of the graph is critical. However, in real-world applications, especially in semi-supervised learning and unsupervised learning, the evaluation of the quality of a graph is often expensive and sometimes even impossible, due the cost or the unavailability of ground truth. In this paper, we proposed a robust approach with convex optimization to ``forge'' a graph: with an input of a graph, to learn a graph with higher quality. Our major concern is that an ideal graph shall satisfy all the following constraints: non-negative, symmetric, low rank, and positive semidefinite. We develop a graph learning algorithm by solving a convex optimization problem and further develop an efficient optimization to obtain global optimal solutions with theoretical guarantees. With only one non-sensitive parameter, our method is shown by experimental results to be robust and achieve higher accuracy in semi-supervised learning and clustering under various settings. As a preprocessing of graphs, our method has a wide range of potential applications machine learning and data mining.",
        "bibtex": "@inproceedings{NIPS2012_a50abba8,\n author = {Luo, Dijun and Huang, Heng and Nie, Feiping and Ding, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a50abba8132a77191791390c3eb19fe7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 379997,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4641142824239370735&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, The University of Texas at Arlington; Department of Computer Science and Engineering, The University of Texas at Arlington; Department of Computer Science and Engineering, The University of Texas at Arlington; Department of Computer Science and Engineering, The University of Texas at Arlington",
        "aff_domain": "gmail.com;uta.edu;uta.edu;gmail.com",
        "email": "gmail.com;uta.edu;uta.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Texas at Arlington",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.uta.edu",
        "aff_unique_abbr": "UT Arlington",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Arlington",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "90e7d38278",
        "title": "Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8df707a948fac1b4a0f97aa554886ec8-Abstract.html",
        "author": "Kei Wakabayashi; Takao Miura",
        "abstract": "Hierarchical Hidden Markov Models (HHMMs) are sophisticated stochastic models that enable us to capture a hierarchical context characterization of sequence data. However, existing HHMM parameter estimation methods require large computations of time complexity O(TN^{2D}) at least for model inference, where D is the depth of the hierarchy, N is the number of states in each level, and T is the sequence length. In this paper, we propose a new inference method of HHMMs for which the time complexity is O(TN^{D+1}). A key idea of our algorithm is application of the forward-backward algorithm to ''state activation probabilities''. The notion of a state activation, which offers a simple formalization of the hierarchical transition behavior of HHMMs, enables us to conduct model inference efficiently. We present some experiments to demonstrate that our proposed method works more efficiently to estimate HHMM parameters than do some existing methods such as the flattening method and Gibbs sampling method.",
        "bibtex": "@inproceedings{NIPS2012_8df707a9,\n author = {Wakabayashi, Kei and Miura, Takao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8df707a948fac1b4a0f97aa554886ec8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 250298,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9817373087264120542&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Faculty of Library, Information and Media Science, University of Tsukuba, Japan; Department of Engineering, Hosei University, Japan",
        "aff_domain": "slis.tsukuba.ac.jp;hosei.ac.jp",
        "email": "slis.tsukuba.ac.jp;hosei.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Tsukuba;Hosei University",
        "aff_unique_dep": "Faculty of Library, Information and Media Science;Department of Engineering",
        "aff_unique_url": "https://www.tsukuba.ac.jp;https://www.hosei.ac.jp",
        "aff_unique_abbr": ";Hosei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "67cb9ae2eb",
        "title": "From Deformations to Parts: Motion-based Segmentation of 3D Objects",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html",
        "author": "Soumya Ghosh; Matthew Loper; Erik B. Sudderth; Michael J. Black",
        "abstract": "We develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3D) poses.  We adapt the distance dependent Chinese restaurant process (ddCRP) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.  To allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.  By placing a matrix normal-inverse-Wishart prior on these affine transformations, we develop a ddCRP Gibbs sampler which tractably marginalizes over transformation uncertainty.  Analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods.",
        "bibtex": "@inproceedings{NIPS2012_a1140a3d,\n author = {Ghosh, Soumya and Loper, Matthew and Sudderth, Erik and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From Deformations to Parts: Motion-based Segmentation of 3D Objects},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/a1140a3d0df1c81e24ae954d935e8926-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a1140a3d0df1c81e24ae954d935e8926-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4056859,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9609917685091258180&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science, Brown University; Department of Computer Science, Brown University; Perceiving Systems Department, Max Planck Institute for Intelligent Systems; Perceiving Systems Department, Max Planck Institute for Intelligent Systems",
        "aff_domain": "cs.brown.edu;cs.brown.edu;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "cs.brown.edu;cs.brown.edu;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Brown University;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Department of Computer Science;Perceiving Systems Department",
        "aff_unique_url": "https://www.brown.edu;https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "Brown;MPI-IS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "0a920bb281",
        "title": "Fully Bayesian inference for neural models with negative-binomial spiking",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b55ec28c52d5f6205684a473a2193564-Abstract.html",
        "author": "Jonathan W. Pillow; James Scott",
        "abstract": "Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses.  The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability.  Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals.  This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efficient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models.  We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains.",
        "bibtex": "@inproceedings{NIPS2012_b55ec28c,\n author = {Pillow, Jonathan and Scott, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fully Bayesian inference for neural models with negative-binomial spiking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b55ec28c52d5f6205684a473a2193564-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1432321,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16702266177842779803&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Center for Perceptual Systems, Department of Psychology, The University of Texas at Austin; Division of Statistics and Scienti\ufb01c Computation, McCombs School of Business, The University of Texas at Austin",
        "aff_domain": "mail.utexas.edu;mccombs.utexas.edu",
        "email": "mail.utexas.edu;mccombs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3c925e1f87",
        "title": "Fused sparsity and robust estimation for linear models with unknown variance",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7750ca3559e5b8e1f44210283368fc16-Abstract.html",
        "author": "Arnak Dalalyan; Yin Chen",
        "abstract": "In this paper, we develop a novel approach to the problem of learning sparse representations in the context of fused sparsity and unknown noise level. We propose an algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes the aforementioned learning task by means of a second-order cone program. A special  emphasize is put on the particular instance of fused sparsity corresponding to  the learning in presence of outliers. We establish finite sample risk bounds and  carry out an experimental evaluation on both synthetic and real data.",
        "bibtex": "@inproceedings{NIPS2012_7750ca35,\n author = {Dalalyan, Arnak and Chen, Yin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fused sparsity and robust estimation for linear models with unknown variance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7750ca3559e5b8e1f44210283368fc16-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7750ca3559e5b8e1f44210283368fc16-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/7750ca3559e5b8e1f44210283368fc16-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7750ca3559e5b8e1f44210283368fc16-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1330180,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9315038164829032020&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University Paris Est, LIGM; ENSAE-CREST-GENES",
        "aff_domain": "eleves.enpc.fr;ensae.fr",
        "email": "eleves.enpc.fr;ensae.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University Paris Est;ENSAE-CREST",
        "aff_unique_dep": "LIGM;GENES",
        "aff_unique_url": "https://www.univ-paris-est.fr;https://www.ensae.fr",
        "aff_unique_abbr": ";ENSAE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "f492ab16a0",
        "title": "Fusion with Diffusion for Robust Visual Tracking",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3e313b9badf12632cdae5452d20e1af6-Abstract.html",
        "author": "Yu Zhou; Xiang Bai; Wenyu Liu; Longin J. Latecki",
        "abstract": "A weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering. The edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations. In this paper, a novel approach to integrate multiple similarity measures is pro-posed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the dif-fusion on the TPG is the same as the diffusion process on each of the original graphs, Moreover, it is not necessary to explicitly construct the TPG in our frame-work. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods.",
        "bibtex": "@inproceedings{NIPS2012_3e313b9b,\n author = {Zhou, Yu and Bai, Xiang and Liu, Wenyu and Latecki, Longin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fusion with Diffusion for Robust Visual Tracking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3e313b9badf12632cdae5452d20e1af6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3e313b9badf12632cdae5452d20e1af6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/3e313b9badf12632cdae5452d20e1af6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3e313b9badf12632cdae5452d20e1af6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 362628,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14805505249690456189&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Electronics and Information Engineering, Huazhong Univ. of Science and Technology, P. R. China; Dept. of Electronics and Information Engineering, Huazhong Univ. of Science and Technology, P. R. China; Dept. of Electronics and Information Engineering, Huazhong Univ. of Science and Technology, P. R. China; Dept. of Computer and Information Sciences, Temple Univ., Philadelphia, USA",
        "aff_domain": "gmail.com;gmail.com;hust.edu.cn;temple.edu",
        "email": "gmail.com;gmail.com;hust.edu.cn;temple.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Huazhong University of Science and Technology;Temple University",
        "aff_unique_dep": "Department of Electronics and Information Engineering;Department of Computer and Information Sciences",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.temple.edu",
        "aff_unique_abbr": "HUST;Temple",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Philadelphia",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "3daab55725",
        "title": "GenDeR: A Generic Diversified Ranking Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7f24d240521d99071c93af3917215ef7-Abstract.html",
        "author": "Jingrui He; Hanghang Tong; Qiaozhu Mei; Boleslaw Szymanski",
        "abstract": "Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling, product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a large volume of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm.",
        "bibtex": "@inproceedings{NIPS2012_7f24d240,\n author = {He, Jingrui and Tong, Hanghang and Mei, Qiaozhu and Szymanski, Boleslaw},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {GenDeR: A Generic Diversified Ranking Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7f24d240521d99071c93af3917215ef7-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7f24d240521d99071c93af3917215ef7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7f24d240521d99071c93af3917215ef7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 119847,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13524750967153834790&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "IBM T.J. Watson Research; IBM T.J. Watson Research; University of Michigan; Rensselaer Polytechnic Institute",
        "aff_domain": "us.ibm.com;us.ibm.com;umich.edu;rpi.edu",
        "email": "us.ibm.com;us.ibm.com;umich.edu;rpi.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "IBM;University of Michigan;Rensselaer Polytechnic Institute",
        "aff_unique_dep": "IBM T.J. Watson Research Center;;",
        "aff_unique_url": "https://www.ibm.com/research/watson;https://www.umich.edu;https://www.rpi.edu",
        "aff_unique_abbr": "IBM Watson;UM;RPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b346f83add",
        "title": "Generalization Bounds for Domain Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ca8155f4d27f205953f9d3d7974bdd70-Abstract.html",
        "author": "Chao Zhang; Lei Zhang; Jieping Ye",
        "abstract": "In this paper, we provide a new framework to study the generalization bound of the learning process for domain adaptation. Without loss of generality, we consider two kinds of representative domain adaptation settings: one is domain adaptation with multiple sources and the other is domain adaptation combining source and target data. In particular, we introduce two quantities that capture the inherent characteristics of domains. For either kind of domain adaptation, based on the two quantities, we then develop the specific Hoeffding-type deviation inequality and symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number. By using the resultant generalization bound, we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the learning process. The numerical experiments support our results.",
        "bibtex": "@inproceedings{NIPS2012_ca8155f4,\n author = {Zhang, Chao and Zhang, Lei and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalization Bounds for Domain Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/ca8155f4d27f205953f9d3d7974bdd70-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ca8155f4d27f205953f9d3d7974bdd70-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 292856,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18205378726424465844&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Center for Evolutionary Medicine and Informatics, The Biodesign Institute, and Computer Science and Engineering, Arizona State University, Tempe, USA; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, P.R. China; Center for Evolutionary Medicine and Informatics, The Biodesign Institute, and Computer Science and Engineering, Arizona State University, Tempe, USA",
        "aff_domain": "asu.edu;yahoo.com.cn;asu.edu",
        "email": "asu.edu;yahoo.com.cn;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Arizona State University;Nanjing University of Science and Technology",
        "aff_unique_dep": "Computer Science and Engineering;School of Computer Science and Technology",
        "aff_unique_url": "https://www.asu.edu;http://www.nust.edu.cn",
        "aff_unique_abbr": "ASU;NUST",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Tempe;Nanjing",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "96bc6e8d3f",
        "title": "Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html",
        "author": "Alex Schwing; Tamir Hazan; Marc Pollefeys; Raquel Urtasun",
        "abstract": "While finding the exact solution for the MAP inference problem is intractable for many real-world tasks, MAP LP relaxations have been shown to be very effective in practice.  However, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent. In this work we propose to augment these algorithms with an $\\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the Fenchel-Young duality theorem. Furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart. We demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers.",
        "bibtex": "@inproceedings{NIPS2012_f197002b,\n author = {Schwing, Alex and Hazan, Tamir and Pollefeys, Marc and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f197002b9a0853eca5e046d9ca4663d5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 332808,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5362481860614971504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "ETH Zurich; TTI Chicago; ETH Zurich; TTI Chicago",
        "aff_domain": "inf.ethz.ch;ttic.edu;inf.ethz.ch;ttic.edu",
        "email": "inf.ethz.ch;ttic.edu;inf.ethz.ch;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "ETH Zurich;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.tti-chicago.org",
        "aff_unique_abbr": "ETHZ;TTI",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "efaf1b6bea",
        "title": "Gradient Weights help Nonparametric Regressors",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/286674e3082feb7e5afb92777e48821f-Abstract.html",
        "author": "Samory Kpotufe; Abdeslam Boularias",
        "abstract": "In regression problems over $\\real^d$, the unknown function $f$ often varies more in some coordinates than in others. We show that weighting each coordinate $i$ with the estimated norm of the $i$th derivative of $f$  is an efficient way to significantly improve the performance of distance-based regressors, e.g. kernel and $k$-NN regressors.  We propose a simple estimator of these derivative norms and prove its consistency. Moreover, the proposed  estimator is efficiently learned online.",
        "bibtex": "@inproceedings{NIPS2012_286674e3,\n author = {Kpotufe, Samory and Boularias, Abdeslam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gradient Weights help Nonparametric Regressors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/286674e3082feb7e5afb92777e48821f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/286674e3082feb7e5afb92777e48821f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/286674e3082feb7e5afb92777e48821f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 343217,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15498203689514497971&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Max Planck Institute for Intelligent Systems + Toyota Technological Institute Chicago; Max Planck Institute for Intelligent Systems",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Intelligent Systems;",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.tti-chicago.org",
        "aff_unique_abbr": "MPI-IS;TTI Chicago",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "6c7ca52b37",
        "title": "Gradient-based kernel method for feature extraction and variable selection",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html",
        "author": "Kenji Fukumizu; Chenlei Leng",
        "abstract": "We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter finds a subset of predictors. First, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method.  In comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets.  Second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by Chen et al. (2010).  Experimental results show that the proposed methods successfully find effective features and variables without parametric models.",
        "bibtex": "@inproceedings{NIPS2012_83fa5a43,\n author = {Fukumizu, Kenji and Leng, Chenlei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gradient-based kernel method for feature extraction and variable selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/83fa5a432ae55c253d0e60dbfa716723-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/83fa5a432ae55c253d0e60dbfa716723-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 164789,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13629284792301010414&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "The Institute of Statistical Mathematics; National University of Singapore",
        "aff_domain": "ism.ac.jp;nus.edu.sg",
        "email": "ism.ac.jp;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Institute of Statistical Mathematics;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ism.ac.jp;https://www.nus.edu.sg",
        "aff_unique_abbr": "ISM;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Japan;Singapore"
    },
    {
        "id": "f45ac61cb6",
        "title": "Graphical Gaussian Vector for Image Categorization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ba3866600c3540f67c1e9575e213be0a-Abstract.html",
        "author": "Tatsuya Harada; Yasuo Kuniyoshi",
        "abstract": "This paper proposes a novel image representation called a Graphical Gaussian Vector, which is a counterpart of the codebook and local feature matching approaches. In our method, we model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. We consider the parameter of GMRF as a feature vector of the image. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Finally we define a new image feature by embedding the metric into the parameters, which can be directly applied to scalable linear classifiers. Our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. As the proposed method simply calculates the local auto-correlations of local features, it is able to achieve both high classification accuracy and high efficiency.",
        "bibtex": "@inproceedings{NIPS2012_ba386660,\n author = {Harada, Tatsuya and Kuniyoshi, Yasuo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Graphical Gaussian Vector for Image Categorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ba3866600c3540f67c1e9575e213be0a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ba3866600c3540f67c1e9575e213be0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ba3866600c3540f67c1e9575e213be0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 717918,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8531388547128115584&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The University of Tokyo/JST PRESTO; The University of Tokyo",
        "aff_domain": "isi.imi.i.u-tokyo.ac.jp;isi.imi.i.u-tokyo.ac.jp",
        "email": "isi.imi.i.u-tokyo.ac.jp;isi.imi.i.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "d637fe8c54",
        "title": "Graphical Models via Generalized Linear Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0ff8033cf9437c213ee13937b1c4c455-Abstract.html",
        "author": "Eunho Yang; Genevera Allen; Zhandong Liu; Pradeep K. Ravikumar",
        "abstract": "Undirected graphical models, or Markov networks, such as Gaussian graphical models and Ising models enjoy popularity in a variety of applications.  In many settings, however, data may not follow a Gaussian or binomial distribution assumed by these models. We introduce a new class of graphical models based on generalized linear models (GLM) by assuming that node-wise conditional distributions arise from exponential families.  Our models allow one to estimate networks for a wide class of exponential distributions, such as the Poisson, negative binomial, and exponential, by fitting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We provide examples of high-throughput genomic networks learned via our GLM graphical models for multinomial and Poisson distributed data.",
        "bibtex": "@inproceedings{NIPS2012_0ff8033c,\n author = {Yang, Eunho and Allen, Genevera and Liu, Zhandong and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Graphical Models via Generalized Linear Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/0ff8033cf9437c213ee13937b1c4c455-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0ff8033cf9437c213ee13937b1c4c455-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 318297,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9771653600538851196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Statistics, Rice University; Department of Pediatrics-Neurology, Baylor College of Medicine",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;rice.edu;bcm.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;rice.edu;bcm.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Texas at Austin;Rice University;Baylor College of Medicine",
        "aff_unique_dep": "Department of Computer Science;Department of Statistics;Department of Pediatrics-Neurology",
        "aff_unique_url": "https://www.utexas.edu;https://www.rice.edu;https://www.bcm.edu",
        "aff_unique_abbr": "UT Austin;Rice;BCM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "21a19d28fa",
        "title": "Hamming Distance Metric Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/59b90e1005a220e2ebc542eb9d950b1e-Abstract.html",
        "author": "Mohammad Norouzi; David J Fleet; Ruslan Salakhutdinov",
        "abstract": "Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs.  We develop a new loss-augmented inference algorithm that is quadratic in the code length.  We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes.",
        "bibtex": "@inproceedings{NIPS2012_59b90e10,\n author = {Norouzi, Mohammad and Fleet, David J and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hamming Distance Metric Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/59b90e1005a220e2ebc542eb9d950b1e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/59b90e1005a220e2ebc542eb9d950b1e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 547607,
        "gs_citation": 971,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13638489687562961488&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Departments of Computer Science\u2020; Departments of Computer Science\u2020; Departments of Computer Science\u2020and Statistics\u2021",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University Affiliation Not Specified;University of California, Berkeley",
        "aff_unique_dep": "Departments of Computer Science;Departments of Computer Science and Statistics",
        "aff_unique_url": ";https://www.berkeley.edu",
        "aff_unique_abbr": ";UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "110ddab657",
        "title": "Hierarchical Optimistic Region Selection driven by Curiosity",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8a0e1141fd37fa5b98d5bb769ba1a7cc-Abstract.html",
        "author": "Odalric-ambrym Maillard",
        "abstract": "This paper aims to take a step forwards making the term ``intrinsic motivation'' from reinforcement learning theoretically well founded, focusing on curiosity-driven learning. To that end, we consider the setting where, a fixed partition P of a continuous space X being given, and a process \\nu defined on X being unknown, we are asked to sequentially decide which cell of the partition to select as well as where to sample \\nu in that cell, in order to minimize a loss function that is inspired from previous work on curiosity-driven learning. The loss on each cell consists of one term measuring a simple worst case quadratic sampling error, and a penalty term proportional to the range of the variance in that cell. The corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region, and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization can be used in order to solve this problem. The resulting procedure, called Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C) is provided together with a finite-time regret analysis.",
        "bibtex": "@inproceedings{NIPS2012_8a0e1141,\n author = {Maillard, Odalric-ambrym},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Optimistic Region Selection driven by Curiosity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 266029,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8225931777704987070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Lehrstuhl f \u00a8ur Informationstechnologie, Montanuniversit \u00a8at Leoben, Leoben, A-8700, Austria",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Montanuniversit\u00e4t Leoben",
        "aff_unique_dep": "Lehrstuhl f\u00fcr Informationstechnologie",
        "aff_unique_url": "https://www.montanuni-leoben.ac.at",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Leoben",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "453898938f",
        "title": "Hierarchical spike coding of sound",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ad3019b856147c17e82a5bead782d2a8-Abstract.html",
        "author": "Yan Karklin; Chaitanya Ekanadham; Eero P. Simoncelli",
        "abstract": "We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task.",
        "bibtex": "@inproceedings{NIPS2012_ad3019b8,\n author = {Karklin, Yan and Ekanadham, Chaitanya and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical spike coding of sound},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ad3019b856147c17e82a5bead782d2a8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ad3019b856147c17e82a5bead782d2a8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ad3019b856147c17e82a5bead782d2a8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 984601,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17025361066883164463&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Howard Hughes Medical Institute, Center for Neural Science, New York University; Courant Institute of Mathematical Sciences, New York University; Howard Hughes Medical Institute, Center for Neural Science, and Courant Institute of Mathematical Sciences, New York University",
        "aff_domain": "nyu.edu;math.nyu.edu;nyu.edu",
        "email": "nyu.edu;math.nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "New York University;Howard Hughes Medical Institute",
        "aff_unique_dep": "Center for Neural Science;Center for Neural Science",
        "aff_unique_url": "https://www.nyu.edu;https://www.hhmi.org",
        "aff_unique_abbr": "NYU;HHMI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d5eae3d5c1",
        "title": "High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html",
        "author": "Hua Wang; Feiping Nie; Heng Huang; Jingwen Yan; Sungeun Kim; Shannon Risacher; Andrew Saykin; Li Shen",
        "abstract": "Alzheimer disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms. In addition, the sparsity of the model enables the selection of a small number of MRI measures while maintaining high prediction accuracy. The empirical studies, using the baseline MRI and serial cognitive data of the ADNI cohort, have yielded promising results.",
        "bibtex": "@inproceedings{NIPS2012_85fc37b1,\n author = {Wang, Hua and Nie, Feiping and Huang, Heng and Yan, Jingwen and Kim, Sungeun and Risacher, Shannon and Saykin, Andrew and Shen, Li},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer\\textquotesingle s Disease Progression Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/85fc37b18c57097425b52fc7afbb6969-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/85fc37b18c57097425b52fc7afbb6969-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 492158,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6754558333916573304&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX 76019; Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX 76019; Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX 76019; Department of Radiology and Imaging Sciences, Indiana University School of Medicine, Indianapolis, IN 46202; Department of Radiology and Imaging Sciences, Indiana University School of Medicine, Indianapolis, IN 46202; Department of Radiology and Imaging Sciences, Indiana University School of Medicine, Indianapolis, IN 46202; Department of Radiology and Imaging Sciences, Indiana University School of Medicine, Indianapolis, IN 46202; Department of Radiology and Imaging Sciences, Indiana University School of Medicine, Indianapolis, IN 46202",
        "aff_domain": "gmail.com;gmail.com;uta.edu;iupui.edu;iupui.edu;iupui.edu;iupui.edu;iupui.edu",
        "email": "gmail.com;gmail.com;uta.edu;iupui.edu;iupui.edu;iupui.edu;iupui.edu;iupui.edu",
        "github": "",
        "project": "http://adni.loni.ucla.edu/wp-content/uploads/how toapply/ADNI Acknowledgement List.pdf",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1;1;1;1",
        "aff_unique_norm": "University of Texas at Arlington;Indiana University School of Medicine",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Radiology and Imaging Sciences",
        "aff_unique_url": "https://www.uta.edu;https://medicine.iu.edu",
        "aff_unique_abbr": "UTA;IUSM",
        "aff_campus_unique_index": "0;0;0;1;1;1;1;1",
        "aff_campus_unique": "Arlington;Indianapolis",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "258e532b0f",
        "title": "Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/13f9896df61279c928f19721878fac41-Abstract.html",
        "author": "Stefan Habenschuss; Johannes Bill; Bernhard Nessler",
        "abstract": "Recent spiking network models of Bayesian inference and unsupervised learning frequently assume either inputs to arrive in a special format or employ complex computations in neuronal activation functions and synaptic plasticity rules. Here we show in a rigorous mathematical treatment how homeostatic processes, which have previously received little attention in this context, can overcome common theoretical limitations and facilitate the neural implementation and performance of existing models. In particular, we show that homeostatic plasticity can be understood as the enforcement of a 'balancing' posterior constraint during probabilistic inference and learning with Expectation Maximization. We link homeostatic dynamics to the theory of variational inference, and show that nontrivial terms, which typically appear during probabilistic inference in a large class of models, drop out. We demonstrate the feasibility of our approach in a spiking Winner-Take-All architecture of Bayesian inference and learning. Finally, we sketch how the mathematical framework can be extended to richer recurrent network architectures. Altogether, our theory provides a novel perspective on the interplay of homeostatic processes and synaptic plasticity in cortical microcircuits, and points to an essential role of homeostasis during inference and learning in spiking networks.",
        "bibtex": "@inproceedings{NIPS2012_13f9896d,\n author = {Habenschuss, Stefan and Bill, Johannes and Nessler, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/13f9896df61279c928f19721878fac41-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/13f9896df61279c928f19721878fac41-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/13f9896df61279c928f19721878fac41-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 560935,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3404387141335517445&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Institute for Theoretical Computer Science, Graz University of Technology; Institute for Theoretical Computer Science, Graz University of Technology; Institute for Theoretical Computer Science, Graz University of Technology",
        "aff_domain": "igi.tugraz.at;igi.tugraz.at;igi.tugraz.at",
        "email": "igi.tugraz.at;igi.tugraz.at;igi.tugraz.at",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Institute for Theoretical Computer Science",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TU Graz",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "338a571277",
        "title": "How Prior Probability Influences Decision Making: A Unifying Probabilistic Model",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html",
        "author": "Yanping Huang; Timothy Hanks; Mike Shadlen; Abram L. Friesen; Rajesh P. Rao",
        "abstract": "How does the brain combine prior knowledge with sensory evidence when making decisions under uncertainty? Two competing descriptive models have been proposed based on experimental data.  The first posits an additive offset to a decision variable, implying a static effect of the prior. However, this model is inconsistent with recent data from a motion discrimination task involving temporal integration of uncertain sensory evidence. To explain this data, a second model has been proposed which assumes a time-varying influence of the prior. Here we present a normative model of decision making that incorporates prior knowledge in a principled way.  We show that the additive offset model and the time-varying prior model emerge naturally when decision making is viewed within the framework of partially observable Markov decision processes (POMDPs).  Decision making in the model reduces to (1) computing beliefs given observations and prior information in a Bayesian manner, and (2) selecting actions based on these beliefs to maximize the  expected sum of future rewards. We show that the model can explain both  data previously explained using the additive offset model as well as more  recent data on the time-varying influence of prior knowledge on decision making.",
        "bibtex": "@inproceedings{NIPS2012_5d44ee6f,\n author = {Huang, Yanping and Hanks, Timothy and Shadlen, Mike and Friesen, Abram L and Rao, Rajesh PN},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {How Prior Probability Influences Decision Making: A Unifying Probabilistic Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/5d44ee6f2c3f71b73125876103c8f6c4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2383503,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3959834385046068988&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Washington; University of Washington; Princeton University; Columbia University + Howard Hughes Medical Institute; University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu;princeton.edu;columbia.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;princeton.edu;columbia.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2+3;0",
        "aff_unique_norm": "University of Washington;Princeton University;Columbia University;Howard Hughes Medical Institute",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.washington.edu;https://www.princeton.edu;https://www.columbia.edu;https://www.hhmi.org",
        "aff_unique_abbr": "UW;Princeton;Columbia;HHMI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cff8dd8dc9",
        "title": "How They Vote: Issue-Adjusted Models of Legislative Behavior",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/193002e668758ea9762904da1a22337c-Abstract.html",
        "author": "Sean Gerrish; David M. Blei",
        "abstract": "We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues.  Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space.",
        "bibtex": "@inproceedings{NIPS2012_193002e6,\n author = {Gerrish, Sean and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {How They Vote: Issue-Adjusted Models of Legislative Behavior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/193002e668758ea9762904da1a22337c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/193002e668758ea9762904da1a22337c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/193002e668758ea9762904da1a22337c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/193002e668758ea9762904da1a22337c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 544843,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3238158759885131990&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9eb439a981",
        "title": "Human memory search as a random walk in a semantic network",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/14d9e8007c9b41f57891c48e07c23f57-Abstract.html",
        "author": "Joseph L. Austerweil; Joshua T. Abbott; Thomas L. Griffiths",
        "abstract": "The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters.",
        "bibtex": "@inproceedings{NIPS2012_14d9e800,\n author = {Austerweil, Joseph and Abbott, Joshua T and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Human memory search as a random walk in a semantic network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/14d9e8007c9b41f57891c48e07c23f57-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/14d9e8007c9b41f57891c48e07c23f57-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/14d9e8007c9b41f57891c48e07c23f57-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 620192,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1394062196016703251&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Psychology, University of California, Berkeley; Department of Psychology, University of California, Berkeley; Department of Psychology, University of California, Berkeley",
        "aff_domain": "berkeley.edu;gmail.com;berkeley.edu",
        "email": "berkeley.edu;gmail.com;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "79e113a756",
        "title": "Identifiability and Unmixing of Latent Parse Trees",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/50c3d7614917b24303ee6a220679dab3-Abstract.html",
        "author": "Daniel J. Hsu; Sham M. Kakade; Percy Liang",
        "abstract": "This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models.",
        "bibtex": "@inproceedings{NIPS2012_50c3d761,\n author = {Hsu, Daniel J and Kakade, Sham M and Liang, Percy S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identifiability and Unmixing of Latent Parse Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/50c3d7614917b24303ee6a220679dab3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/50c3d7614917b24303ee6a220679dab3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/50c3d7614917b24303ee6a220679dab3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/50c3d7614917b24303ee6a220679dab3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 345669,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18331484645552999107&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Microsoft Research; Microsoft Research; Stanford University",
        "aff_domain": "microsoft.com;microsoft.com;cs.stanford.edu",
        "email": "microsoft.com;microsoft.com;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Microsoft;Stanford University",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.stanford.edu",
        "aff_unique_abbr": "MSR;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e4cbc3b460",
        "title": "Identification of Recurrent Patterns in the Activation of Brain Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ad13a2a07ca4b7642959dc0c4c740ab6-Abstract.html",
        "author": "Firdaus Janoos; Weichang Li; Niranjan Subrahmanya; Istvan Morocz; William Wells",
        "abstract": "Identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series.  In this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.  While the application presented here is for identifying distinct brain activity patterns from fMRI, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks.",
        "bibtex": "@inproceedings{NIPS2012_ad13a2a0,\n author = {Janoos, Firdaus and Li, Weichang and Subrahmanya, Niranjan and Morocz, Istvan and Wells, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identification of Recurrent Patterns in the Activation of Brain Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/ad13a2a07ca4b7642959dc0c4c740ab6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ad13a2a07ca4b7642959dc0c4c740ab6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 877931,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11896514099158985165&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "ExxonMobil Corporate Strategic Research; ExxonMobil Corporate Strategic Research; ExxonMobil Corporate Strategic Research; Harvard Medical School; Harvard Medical School",
        "aff_domain": "exxonmobil.com; ; ; ; ",
        "email": "exxonmobil.com; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "ExxonMobil;Harvard University",
        "aff_unique_dep": "Corporate Strategic Research;Medical School",
        "aff_unique_url": "https://www.exxonmobil.com;https://hms.harvard.edu",
        "aff_unique_abbr": "ExxonMobil;HMS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Boston",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8cf7cea8f9",
        "title": "Image Denoising and Inpainting with Deep Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6cdd60ea0045eb7a6ec44c54d29ed402-Abstract.html",
        "author": "Junyuan Xie; Linli Xu; Enhong Chen",
        "abstract": "We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method achieves state-of-the-art performance in the image denoising task. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.",
        "bibtex": "@inproceedings{NIPS2012_6cdd60ea,\n author = {Xie, Junyuan and Xu, Linli and Chen, Enhong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Image Denoising and Inpainting with Deep Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6cdd60ea0045eb7a6ec44c54d29ed402-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3386035,
        "gs_citation": 1921,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6118112247085297920&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computer Science and Technology, University of Science and Technology of China; School of Computer Science and Technology, University of Science and Technology of China; School of Computer Science and Technology, University of Science and Technology of China",
        "aff_domain": "gmail.com;ustc.edu.cn;ustc.edu.cn",
        "email": "gmail.com;ustc.edu.cn;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "4f7f13d4e5",
        "title": "ImageNet Classification with Deep Convolutional Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
        "author": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E. Hinton",
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\\% and 18.9\\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.",
        "bibtex": "@inproceedings{NIPS2012_c399862d,\n author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {ImageNet Classification with Deep Convolutional Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1418820,
        "gs_citation": 37554,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1036029811429977192&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Toronto; University of Toronto; University of Toronto",
        "aff_domain": "cs.utoronto.ca;cs.utoronto.ca;cs.utoronto.ca",
        "email": "cs.utoronto.ca;cs.utoronto.ca;cs.utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "10694b3e63",
        "title": "Imitation Learning by Coaching",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2dffbc474aa176b6dc957938c15d0c8b-Abstract.html",
        "author": "He He; Jason Eisner; Hal Daume",
        "abstract": "Imitation Learning has been shown to be successful in solving many challenging real-world problems. Some recent approaches give strong performance guarantees by training the policy iteratively. However, it is important to note that these guarantees depend on   how well the policy we found can imitate the oracle on the training data.  When there is a substantial difference between the oracle's ability  and the learner's policy space, we may fail to find a policy that has low error on the training set. In such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner  and gradually approaches the oracle. By a reduction of learning by demonstration to online learning,  we prove that coaching can yield a lower regret bound than using the oracle. We apply our algorithm to a novel cost-sensitive dynamic feature selection problem, a hard decision problem that considers a user-specified accuracy-cost trade-off.  Experimental results on UCI datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods.",
        "bibtex": "@inproceedings{NIPS2012_2dffbc47,\n author = {He, He and Eisner, Jason and Daume, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Imitation Learning by Coaching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2dffbc474aa176b6dc957938c15d0c8b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 419123,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2854109801515764382&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland; Department of Computer Science, Johns Hopkins University",
        "aff_domain": "cs.umd.edu;cs.umd.edu;cs.jhu.edu",
        "email": "cs.umd.edu;cs.umd.edu;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Maryland;Johns Hopkins University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu;https://www.jhu.edu",
        "aff_unique_abbr": "UMD;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fe51552a2f",
        "title": "Interpreting prediction markets: a stochastic approach",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/41a60377ba920919939d83326ebee5a1-Abstract.html",
        "author": "Rafael M. Frongillo; Nicolas Della Penna; Mark D. Reid",
        "abstract": "We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution. This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved. In particular, we show that the stationary point of the stochastic process of prices generated by the market is equal to the market's Walrasian equilibrium of classic market analysis. Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour.",
        "bibtex": "@inproceedings{NIPS2012_41a60377,\n author = {Frongillo, Rafael and Della Penna, Nichol\\'{a}s and Reid, Mark D},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Interpreting prediction markets: a stochastic approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/41a60377ba920919939d83326ebee5a1-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/41a60377ba920919939d83326ebee5a1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/41a60377ba920919939d83326ebee5a1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1369742,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6554044336757925172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Division, University of California, Berkeley; Research School of Computer Science, The Australian National University; Research School of Computer Science, The Australian National University + NICTA",
        "aff_domain": "cs.berkeley.edu;nikete.com;anu.edu.au",
        "email": "cs.berkeley.edu;nikete.com;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1+2",
        "aff_unique_norm": "University of California, Berkeley;Australian National University;National Information and Communications Technology Australia",
        "aff_unique_dep": "Computer Science Division;Research School of Computer Science;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.anu.edu.au;https://www.nicta.com.au",
        "aff_unique_abbr": "UC Berkeley;ANU;NICTA",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;1;1+1",
        "aff_country_unique": "United States;Australia"
    },
    {
        "id": "5b03b2a57e",
        "title": "Inverse Reinforcement Learning through Structured Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/559cb990c9dffd8675f6bc2186971dc2-Abstract.html",
        "author": "Edouard Klein; Matthieu Geist; Bilal Piot; Olivier Pietquin",
        "abstract": "This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called feature expectation of the expert as the parameterization of the score function of a multi-class classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving the direct RL problem. Moreover, with an appropriate heuristic, it can succeed with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator.",
        "bibtex": "@inproceedings{NIPS2012_559cb990,\n author = {Klein, Edouard and Geist, Matthieu and Piot, Bilal and Pietquin, Olivier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inverse Reinforcement Learning through Structured Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/559cb990c9dffd8675f6bc2186971dc2-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/559cb990c9dffd8675f6bc2186971dc2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/559cb990c9dffd8675f6bc2186971dc2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 469164,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3357637167761781499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "LORIA \u2013 team ABC; Sup\u00e9lec \u2013 IMS-MaLIS Research Group; Sup\u00e9lec \u2013 IMS-MaLIS Research Group + UMI 2958 (GeorgiaTech-CNRS); Sup\u00e9lec \u2013 IMS-MaLIS Research Group + UMI 2958 (GeorgiaTech-CNRS)",
        "aff_domain": "supelec.fr;supelec.fr;supelec.fr;supelec.fr",
        "email": "supelec.fr;supelec.fr;supelec.fr;supelec.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1+2;1+2",
        "aff_unique_norm": "LORIA;Sup\u00e9lec;Georgia Institute of Technology",
        "aff_unique_dep": "team ABC;IMS-MaLIS Research Group;",
        "aff_unique_url": "https://www.loria.fr;https://www.suplec.fr;https://www.gatech.edu",
        "aff_unique_abbr": ";;GeorgiaTech",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0+1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "04790aab1d",
        "title": "Isotropic Hashing",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8c6744c9d42ec2cb9e8885b54ff744d0-Abstract.html",
        "author": "Weihao Kong; Wu-jun Li",
        "abstract": "Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.",
        "bibtex": "@inproceedings{NIPS2012_8c6744c9,\n author = {Kong, Weihao and Li, Wu-jun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Isotropic Hashing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8c6744c9d42ec2cb9e8885b54ff744d0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 306658,
        "gs_citation": 385,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17689627301220300829&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Shanghai Key Laboratory of Scalable Computing and Systems; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China",
        "aff_domain": "cs.sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "cs.sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Shanghai Key Laboratory of Scalable Computing and Systems;Shanghai Jiao Tong University",
        "aff_unique_dep": ";Department of Computer Science and Engineering",
        "aff_unique_url": ";https://www.sjtu.edu.cn",
        "aff_unique_abbr": ";SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "07b39cc4f0",
        "title": "Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/edfbe1afcf9246bb0d40eb4d8027d90f-Abstract.html",
        "author": "Benjamin Rolfs; Bala Rajaratnam; Dominique Guillot; Ian Wong; Arian Maleki",
        "abstract": "Sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years. A major focus has been on methods which perform model selection in high dimensions. To this end, numerous convex $\\ell_1$ regularization approaches have been proposed in the literature. It is not however clear which of these methods are optimal in any well-defined sense. A major gap in this regard pertains to the rate of convergence of proposed optimization methods. To address this, an iterative thresholding algorithm for numerically solving the $\\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented. The proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem. The convergence rate is provided in closed form, and is related to the condition number of the optimal point. Numerical results demonstrating the proven rate of convergence are presented.",
        "bibtex": "@inproceedings{NIPS2012_edfbe1af,\n author = {Rolfs, Benjamin and Rajaratnam, Bala and Guillot, Dominique and Wong, Ian and Maleki, Arian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/edfbe1afcf9246bb0d40eb4d8027d90f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/edfbe1afcf9246bb0d40eb4d8027d90f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 322396,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14903091718590312378&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Statistics, Stanford University; Dept. of Statistics, Stanford University; ICME, Stanford University; Dept. of ECE, Rice University; Dept. of EE and Statistics, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;rice.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;rice.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Stanford University;Rice University",
        "aff_unique_dep": "Dept. of Statistics;Dept. of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.rice.edu",
        "aff_unique_abbr": "Stanford;Rice",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "864c0bdf0b",
        "title": "Iterative ranking from pair-wise comparisons",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9adeb82fffb5444e81fa0ce8ad8afe7a-Abstract.html",
        "author": "Sahand Negahban; Sewoong Oh; Devavrat Shah",
        "abstract": "The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR\u2019s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, finding \u2018scores\u2019 for each object (e.g. player\u2019s rating) is of interest to understanding the intensity of the preferences.   In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efficacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1].",
        "bibtex": "@inproceedings{NIPS2012_9adeb82f,\n author = {Negahban, Sahand and Oh, Sewoong and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Iterative ranking from pair-wise comparisons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 257495,
        "gs_citation": 568,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13801630835660299136&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 25,
        "aff": "Department of EECS, Massachusetts Institute of Technology; Department of IESE, University of Illinois at Urbana Champaign; Department of EECS, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;illinois.edu;mit.edu",
        "email": "mit.edu;illinois.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Department of IESE",
        "aff_unique_url": "https://web.mit.edu;https://www.illinois.edu",
        "aff_unique_abbr": "MIT;UIUC",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1b5a3c684f",
        "title": "Joint Modeling of a Matrix with Associated Text via Latent Binary Features",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6c29793a140a811d0c45ce03c1c93a28-Abstract.html",
        "author": "Xianxing Zhang; Lawrence Carin",
        "abstract": "A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring latent binary features (topics) for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents defined by the legislation. State-of-the-art results are manifested for prediction of votes on a new piece of legislation, based only on the observed text legislation. The coupling of the text and legislation is also demonstrated to yield insight into the properties of the matrix decomposition for roll-call data.",
        "bibtex": "@inproceedings{NIPS2012_6c29793a,\n author = {Zhang, Xianxing and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Joint Modeling of a Matrix with Associated Text via Latent Binary Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6c29793a140a811d0c45ce03c1c93a28-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6c29793a140a811d0c45ce03c1c93a28-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/6c29793a140a811d0c45ce03c1c93a28-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6c29793a140a811d0c45ce03c1c93a28-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 804112,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5188756281464588481&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Duke University; Duke University",
        "aff_domain": "duke.edu;duke.edu",
        "email": "duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a715817e0b",
        "title": "Kernel Hyperalignment",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/170c944978496731ba71f34c25826a34-Abstract.html",
        "author": "Alexander Lorbert; Peter J. Ramadge",
        "abstract": "We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We conducted experiments using real-world, multi-subject fMRI data.",
        "bibtex": "@inproceedings{NIPS2012_170c9449,\n author = {Lorbert, Alexander and Ramadge, Peter J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Hyperalignment},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/170c944978496731ba71f34c25826a34-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/170c944978496731ba71f34c25826a34-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/170c944978496731ba71f34c25826a34-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/170c944978496731ba71f34c25826a34-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 386941,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14036310378461651450&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering, Princeton University; Department of Electrical Engineering, Princeton University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "79a4fd3eb3",
        "title": "Kernel Latent SVM for Visual Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/beed13602b9b0e6ecb5b568ff5058f07-Abstract.html",
        "author": "Weilong Yang; Yang Wang; Arash Vahdat; Greg Mori",
        "abstract": "Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning.",
        "bibtex": "@inproceedings{NIPS2012_beed1360,\n author = {Yang, Weilong and Wang, Yang and Vahdat, Arash and Mori, Greg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Latent SVM for Visual Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/beed13602b9b0e6ecb5b568ff5058f07-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6709604,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15486104067327608602&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computing Science, Simon Fraser University; Department of Computer Science, University of Manitoba; School of Computing Science, Simon Fraser University; School of Computing Science, Simon Fraser University",
        "aff_domain": "sfu.ca;cs.umanitoba.ca;sfu.ca;cs.sfu.ca",
        "email": "sfu.ca;cs.umanitoba.ca;sfu.ca;cs.sfu.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Simon Fraser University;University of Manitoba",
        "aff_unique_dep": "School of Computing Science;Department of Computer Science",
        "aff_unique_url": "https://www.sfu.ca;https://umanitoba.ca",
        "aff_unique_abbr": "SFU;U of M",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Burnaby;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "6f0021a97c",
        "title": "Label Ranking with Partial Abstention based on Thresholded Probabilistic Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/fe2d010308a6b3799a3d9c728ee74244-Abstract.html",
        "author": "Weiwei Cheng; Eyke H\u00fcllermeier; Willem Waegeman; Volkmar Welker",
        "abstract": "Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classification, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach.",
        "bibtex": "@inproceedings{NIPS2012_fe2d0103,\n author = {Cheng, Weiwei and H\\\"{u}llermeier, Eyke and Waegeman, Willem and Welker, Volkmar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Label Ranking with Partial Abstention based on Thresholded Probabilistic Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/fe2d010308a6b3799a3d9c728ee74244-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/fe2d010308a6b3799a3d9c728ee74244-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 252321,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7315216455181418717&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Mathematics and Computer Science, Philipps-Universit \u00a8at Marburg, Marburg, Germany; Mathematics and Computer Science, Philipps-Universit \u00a8at Marburg, Marburg, Germany; Mathematical Modeling, Statistics and Bioinformatics, Ghent University, Ghent, Belgium; Mathematics and Computer Science, Philipps-Universit \u00a8at Marburg, Marburg, Germany",
        "aff_domain": "mathematik.uni-marburg.de;mathematik.uni-marburg.de;ugent.be;mathematik.uni-marburg.de",
        "email": "mathematik.uni-marburg.de;mathematik.uni-marburg.de;ugent.be;mathematik.uni-marburg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Philipps-Universit\u00e4t Marburg;Ghent University",
        "aff_unique_dep": "Mathematics and Computer Science;Mathematical Modeling, Statistics and Bioinformatics",
        "aff_unique_url": "https://www.uni-marburg.de;https://www.ugent.be",
        "aff_unique_abbr": "Uni Marburg;UGent",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Marburg;Ghent",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Germany;Belgium"
    },
    {
        "id": "98eb97d0c9",
        "title": "Large Scale Distributed Deep Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html",
        "author": "Jeffrey Dean; Greg Corrado; Rajat Monga; Kai Chen; Matthieu Devin; Mark Mao; Marc'aurelio Ranzato; Andrew Senior; Paul Tucker; Ke Yang; Quoc V. Le; Andrew Y. Ng",
        "abstract": "Recent work in unsupervised feature learning and deep learning has shown that  being able to train large models can dramatically improve performance.  In this  paper, we consider the problem of training a deep network with billions of  parameters using tens of thousands of CPU cores.  We have developed a  software framework called DistBelief that can utilize computing clusters  with thousands of machines to train large models.  Within this framework, we  have developed two algorithms for large-scale distributed training: (i) Downpour  SGD, an asynchronous stochastic gradient descent procedure supporting a  large number of model replicas, and (ii) Sandblaster, a framework that supports  for a variety of distributed batch optimization procedures, including a distributed  implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both  increase the scale and speed of deep network training.  We have successfully  used our system to train a deep network 100x larger than previously reported in  the literature, and achieves state-of-the-art performance on ImageNet, a visual  object recognition task with 16 million images and 21k categories.  We show that  these same techniques dramatically accelerate the training of a more modestly  sized deep network for a commercial speech recognition service. Although we  focus on and report performance of these methods as applied to training large  neural networks, the underlying algorithms are applicable to any gradient-based  machine learning algorithm.",
        "bibtex": "@inproceedings{NIPS2012_6aca9700,\n author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large Scale Distributed Deep Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/6aca97005c68f1206823815f66102863-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6aca97005c68f1206823815f66102863-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 269557,
        "gs_citation": 5206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9220704513857531974&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA; Google Inc., Mountain View, CA",
        "aff_domain": "google.com;google.com; ; ; ; ; ; ; ; ; ; ",
        "email": "google.com;google.com; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 12,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Inc.",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "24128ab565",
        "title": "Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ab541d874c7bc19ab77642849e02b89f-Abstract.html",
        "author": "Matthew Der; Lawrence K. Saul",
        "abstract": "We describe a latent variable model for supervised dimensionality reduction and distance metric learning. The model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones. The model\u2019s continuous latent variables locate pairs of examples in a latent space of lower dimensionality. The model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate Gaussian. Nevertheless we show that inference is completely tractable and derive an Expectation-Maximization (EM) algorithm for parameter estimation. We also compare the model to other approaches in distance metric learning. The model\u2019s main advantage is its simplicity: at each iteration of the EM algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem. Experiments show that these simple updates are highly effective.",
        "bibtex": "@inproceedings{NIPS2012_ab541d87,\n author = {Der, Matthew and Saul, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ab541d874c7bc19ab77642849e02b89f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2732437,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10315307949924558682&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, University of California, San Diego; Department of Computer Science and Engineering, University of California, San Diego",
        "aff_domain": "cs.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "36806aebb5",
        "title": "Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2d6cc4b2d139a53512fb8cbb3086ae2e-Abstract.html",
        "author": "Anima Anandkumar; Ragupathyraj Valluvan",
        "abstract": "Graphical model selection refers to the problem of estimating the unknown graph structure given observations at the nodes in the model. We consider a challenging instance of this problem when some of the nodes are latent or hidden.  We  characterize  conditions for tractable graph estimation and develop efficient methods with provable guarantees. We consider the class of Ising models Markov on  locally tree-like graphs, which are in the regime of correlation decay. We  propose an efficient method for graph estimation, and establish its structural consistency when the number of samples $n$ scales as $n = \\Omega(\\theta_{\\min}^{-\\delta \\eta(\\eta+1)-2}\\log p)$, where $\\theta_{\\min}$ is the minimum edge potential, $\\delta$ is the depth (i.e., distance from a hidden node to the nearest  observed nodes), and $\\eta$ is a parameter which depends on the minimum and maximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides  flexibility to control  the number of latent variables and the cycle lengths in the output graph.  We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound  on sample requirements.",
        "bibtex": "@inproceedings{NIPS2012_2d6cc4b2,\n author = {Anandkumar, Anima and Valluvan, Ragupathyraj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 287989,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-0jvWhzCtw4J:scholar.google.com/&scioq=Latent+Graphical+Model+Selection:+Efficient+Methods+for+Locally+Tree-like+Graphs&hl=en&as_sdt=0,5",
        "gs_version_total": 11,
        "aff": "UC Irvine; UC Irvine",
        "aff_domain": "uci.edu;uci.edu",
        "email": "uci.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f26b74a5d",
        "title": "Learned Prioritization for Trading Off Accuracy and Speed",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/884d247c6f65a96a7da4d1105d584ddd-Abstract.html",
        "author": "Jiarong Jiang; Adam Teichert; Jason Eisner; Hal Daume",
        "abstract": "Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.",
        "bibtex": "@inproceedings{NIPS2012_884d247c,\n author = {Jiang, Jiarong and Teichert, Adam and Eisner, Jason and Daume, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learned Prioritization for Trading Off Accuracy and Speed},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/884d247c6f65a96a7da4d1105d584ddd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 317078,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6345507921106699695&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, Johns Hopkins University; Department of Computer Science, University of Maryland; Department of Computer Science, Johns Hopkins University",
        "aff_domain": "umiacs.umd.edu;jhu.edu;umiacs.umd.edu;jhu.edu",
        "email": "umiacs.umd.edu;jhu.edu;umiacs.umd.edu;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Maryland;Johns Hopkins University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu;https://www.jhu.edu",
        "aff_unique_abbr": "UMD;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b056e48c03",
        "title": "Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html",
        "author": "Aharon Birnbaum; Shai S. Shwartz",
        "abstract": "Given $\\alpha,\\epsilon$, we study the time complexity   required to improperly learn a halfspace with misclassification   error rate of at most $(1+\\alpha)\\,L^*_\\gamma + \\epsilon$, where   $L^*_\\gamma$ is the optimal $\\gamma$-margin error rate. For $\\alpha   = 1/\\gamma$, polynomial time and sample complexity is achievable   using the hinge-loss. For $\\alpha = 0$, \\cite{ShalevShSr11} showed   that $\\poly(1/\\gamma)$ time is impossible, while learning is   possible in time $\\exp(\\tilde{O}(1/\\gamma))$.  An immediate   question, which this paper tackles, is what is achievable if $\\alpha   \\in (0,1/\\gamma)$.  We derive positive results interpolating between   the polynomial time for $\\alpha = 1/\\gamma$ and the exponential   time for $\\alpha=0$. In particular, we show that there are cases in   which $\\alpha = o(1/\\gamma)$ but the problem is still solvable in   polynomial time. Our results naturally extend to the adversarial   online learning model and to the PAC learning with malicious noise   model.",
        "bibtex": "@inproceedings{NIPS2012_67f7fb87,\n author = {Birnbaum, Aharon and Shwartz, Shai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/67f7fb873eaf29526a11a9b7ac33bfac-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/67f7fb873eaf29526a11a9b7ac33bfac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 151544,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3710879822665375375&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel; School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "669004fa16",
        "title": "Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6855456e2fe46a9d49d3d3af4f57443d-Abstract.html",
        "author": "Assaf Glazer; Michael Lindenbaum; Shaul Markovitch",
        "abstract": "We propose an efficient, generalized, nonparametric, statistical Kolmogorov-Smirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efficient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods.",
        "bibtex": "@inproceedings{NIPS2012_6855456e,\n author = {Glazer, Assaf and Lindenbaum, Michael and Markovitch, Shaul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6855456e2fe46a9d49d3d3af4f57443d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6855456e2fe46a9d49d3d3af4f57443d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6855456e2fe46a9d49d3d3af4f57443d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 876406,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5021665623808004143&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Technion \u2013 Israel Institute of Technology, Haifa 32000, Israel; Department of Computer Science, Technion \u2013 Israel Institute of Technology, Haifa 32000, Israel; Department of Computer Science, Technion \u2013 Israel Institute of Technology, Haifa 32000, Israel",
        "aff_domain": "cs.technion.ac.il;cs.technion.ac.il;cs.technion.ac.il",
        "email": "cs.technion.ac.il;cs.technion.ac.il;cs.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion \u2013 Israel Institute of Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "9c0523751a",
        "title": "Learning Image Descriptors with the Boosting-Trick",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0a09c8844ba8f0936c20bd791130d6b6-Abstract.html",
        "author": "Tomasz Trzcinski; Mario Christoudias; Vincent Lepetit; Pascal Fua",
        "abstract": "In this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection. The main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes. This representation can  be improved using machine learning, however, past approaches  have been mostly limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.  While kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive. We propose  to use the boosting-trick  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space. The non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance.",
        "bibtex": "@inproceedings{NIPS2012_0a09c884,\n author = {Trzcinski, Tomasz and Christoudias, Mario and Lepetit, Vincent and Fua, Pascal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Image Descriptors with the Boosting-Trick},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/0a09c8844ba8f0936c20bd791130d6b6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0a09c8844ba8f0936c20bd791130d6b6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 349506,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3670402760912234021&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "CVLab, EPFL, Lausanne, Switzerland; CVLab, EPFL, Lausanne, Switzerland; CVLab, EPFL, Lausanne, Switzerland; CVLab, EPFL, Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "CVLab",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lausanne",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "8ef927e434",
        "title": "Learning Invariant Representations of Molecules for Atomization Energy Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/115f89503138416a242f40fb7d7f338e-Abstract.html",
        "author": "Gr\u00e9goire Montavon; Katja Hansen; Siamac Fazli; Matthias Rupp; Franziska Biegler; Andreas Ziehe; Alexandre Tkatchenko; Anatole V. Lilienfeld; Klaus-Robert M\u00fcller",
        "abstract": "The accurate prediction of molecular energetics in chemical compound space is a crucial ingredient for rational compound design.  The inherently graph-like, non-vectorial nature of molecular data gives rise to a unique and difficult machine learning problem. In this paper, we adopt a learning-from-scratch approach where quantum-mechanical molecular energies are predicted directly from the raw molecular geometry. The study suggests a benefit from setting flexible priors and enforcing invariance stochastically rather than structurally. Our results improve the state-of-the-art by a factor of almost three, bringing statistical methods one step closer to the holy grail of ''chemical accuracy''.",
        "bibtex": "@inproceedings{NIPS2012_115f8950,\n author = {Montavon, Gr\\'{e}goire and Hansen, Katja and Fazli, Siamac and Rupp, Matthias and Biegler, Franziska and Ziehe, Andreas and Tkatchenko, Alexandre and Lilienfeld, Anatole and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Invariant Representations of Molecules for Atomization Energy Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/115f89503138416a242f40fb7d7f338e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/115f89503138416a242f40fb7d7f338e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/115f89503138416a242f40fb7d7f338e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 221221,
        "gs_citation": 214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12626725478560197702&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Machine Learning Group, TU Berlin; Fritz-Haber-Institut der Max-Planck-Gesellschaft, Berlin; Machine Learning Group, TU Berlin; Institute of Pharmaceutical Sciences, ETH Zurich; Machine Learning Group, TU Berlin; Machine Learning Group, TU Berlin; Fritz-Haber-Institut der Max-Planck-Gesellschaft, Berlin; Argonne Leadership Computing Facility, Argonne National Laboratory, Lemont, IL; Machine Learning Group, TU Berlin+Dept. of Brain and Cognitive Engineering, Korea University",
        "aff_domain": "tu-berlin.de; ; ; ; ; ; ; ;tu-berlin.de",
        "email": "tu-berlin.de; ; ; ; ; ; ; ;tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;0;0;1;3;0+4",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;Fritz-Haber-Institut der Max-Planck-Gesellschaft;ETH Zurich;Argonne National Laboratory;Korea University",
        "aff_unique_dep": "Machine Learning Group;;Institute of Pharmaceutical Sciences;Argonne Leadership Computing Facility;Dept. of Brain and Cognitive Engineering",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.fhi.mpg.de;https://www.ethz.ch;https://www.anl.gov;http://www.korea.ac.kr",
        "aff_unique_abbr": "TU Berlin;FHI MPG;ETHZ;ANL;KU",
        "aff_campus_unique_index": "0;0;0;0;0;0;2;0",
        "aff_campus_unique": "Berlin;;Lemont",
        "aff_country_unique_index": "0;0;0;1;0;0;0;2;0+3",
        "aff_country_unique": "Germany;Switzerland;United States;South Korea"
    },
    {
        "id": "fe95f8b898",
        "title": "Learning Label Trees for Probabilistic Modelling of Implicit Feedback",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0829424ffa0d3a2547b6c9622c77de03-Abstract.html",
        "author": "Andriy Mnih; Yee W. Teh",
        "abstract": "User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. In the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data.",
        "bibtex": "@inproceedings{NIPS2012_0829424f,\n author = {Mnih, Andriy and Teh, Yee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Label Trees for Probabilistic Modelling of Implicit Feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0829424ffa0d3a2547b6c9622c77de03-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0829424ffa0d3a2547b6c9622c77de03-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0829424ffa0d3a2547b6c9622c77de03-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 218242,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14120631015672695788&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f15e9021c5",
        "title": "Learning Manifolds with K-Means and K-Flats",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b20bb95ab626d93fd976af958fbc61ba-Abstract.html",
        "author": "Guillermo Canas; Tomaso Poggio; Lorenzo Rosasco",
        "abstract": "We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-\ufb02ats, and analyze their performance. We extend previous results  for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-\ufb02ats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-\ufb02ats, both the results and the mathematical tools are  new.",
        "bibtex": "@inproceedings{NIPS2012_b20bb95a,\n author = {Canas, Guillermo and Poggio, Tomaso and Rosasco, Lorenzo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Manifolds with K-Means and K-Flats},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/b20bb95ab626d93fd976af958fbc61ba-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b20bb95ab626d93fd976af958fbc61ba-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 380416,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2865453627342091310&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Laboratory for Computational and Statistical Learning - MIT-IIT+CBCL, McGovern Institute - Massachusetts Institute of Technology; Laboratory for Computational and Statistical Learning - MIT-IIT+CBCL, McGovern Institute - Massachusetts Institute of Technology; Laboratory for Computational and Statistical Learning - MIT-IIT+CBCL, McGovern Institute - Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;ai.mit.edu;mit.edu",
        "email": "mit.edu;ai.mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Laboratory for Computational and Statistical Learning",
        "aff_unique_url": "https://web.mit.edu/",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a11d5aba88",
        "title": "Learning Mixtures of Tree Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/10a7cdd970fe135cf4f7bb55c0e3b59f-Abstract.html",
        "author": "Anima Anandkumar; Daniel J. Hsu; Furong Huang; Sham M. Kakade",
        "abstract": "We consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with  provable guarantees.   Our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The   sample and computational requirements for our method scale as $\\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs.",
        "bibtex": "@inproceedings{NIPS2012_10a7cdd9,\n author = {Anandkumar, Anima and Hsu, Daniel J and Huang, Furong and Kakade, Sham M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Mixtures of Tree Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 134401,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "UC Irvine; Microsoft Research New England; UC Irvine; Microsoft Research New England",
        "aff_domain": "uci.edu;microsoft.com;uci.edu;microsoft.com",
        "email": "uci.edu;microsoft.com;uci.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of California, Irvine;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.uci.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-new-england",
        "aff_unique_abbr": "UCI;MSR NE",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Irvine;New England",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f916925b1f",
        "title": "Learning Multiple Tasks using Shared Hypotheses",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9c82c7143c102b71c593d98d96093fde-Abstract.html",
        "author": "Koby Crammer; Yishay Mansour",
        "abstract": "In this work we consider a setting where we have a very large number   of related tasks with few examples from each individual task. Rather   than either learning each task individually (and having a large   generalization error) or learning all the tasks together using a   single hypothesis (and suffering a potentially large inherent   error), we consider learning a small pool of {\\em shared     hypotheses}. Each task is then mapped to a single hypothesis in   the pool (hard association). We derive VC dimension generalization   bounds for our model, based on the number of tasks, shared   hypothesis and the VC dimension of the hypotheses   class. We conducted experiments with both synthetic problems and   sentiment of reviews, which strongly support our approach.",
        "bibtex": "@inproceedings{NIPS2012_9c82c714,\n author = {Crammer, Koby and Mansour, Yishay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Multiple Tasks using Shared Hypotheses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9c82c7143c102b71c593d98d96093fde-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 533901,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6316592999784277345&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical Enginering, The Technion - Israel Institute of Technology, Haifa, 32000 Israel; School of Computer Science, Tel Aviv University, Tel - Aviv 69978",
        "aff_domain": "ee.technion.ac.il;tau.ac.il",
        "email": "ee.technion.ac.il;tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Tel Aviv University",
        "aff_unique_dep": "Department of Electrical Engineering;School of Computer Science",
        "aff_unique_url": "https://www.technion.ac.il;https://www.tau.ac.il",
        "aff_unique_abbr": "Technion;TAU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Haifa;Tel Aviv",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "db94adfa69",
        "title": "Learning Networks of Heterogeneous Influence",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d759175de8ea5b1d9a2660e45554894f-Abstract.html",
        "author": "Nan Du; Le Song; Ming Yuan; Alex J. Smola",
        "abstract": "Information, disease, and influence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.  The structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities.",
        "bibtex": "@inproceedings{NIPS2012_d759175d,\n author = {Du, Nan and Song, Le and Yuan, Ming and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Networks of Heterogeneous Influence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d759175de8ea5b1d9a2660e45554894f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 473558,
        "gs_citation": 205,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7974641597750725498&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Georgia Institute of Technology\u2217; Georgia Institute of Technology\u2217; Google Research\u2020; Georgia Institute of Technology\u2217",
        "aff_domain": "gatech.edu;cc.gatech.edu;smola.org;isye.gatech.edu",
        "email": "gatech.edu;cc.gatech.edu;smola.org;isye.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.gatech.edu;https://research.google",
        "aff_unique_abbr": "Georgia Tech;Google Research",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f2d339a907",
        "title": "Learning Partially Observable Models Using Temporally Abstract Decision Trees",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/39461a19e9eddfb385ea76b26521ea48-Abstract.html",
        "author": "Erik Talvitie",
        "abstract": "Part of",
        "bibtex": "@inproceedings{NIPS2012_39461a19,\n author = {Talvitie, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Partially Observable Models Using Temporally Abstract Decision Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/39461a19e9eddfb385ea76b26521ea48-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 196917,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15832339951073692455&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Mathematics and Computer Science, Franklin & Marshall College, Lancaster, PA 17604",
        "aff_domain": "fandm.edu",
        "email": "fandm.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Franklin & Marshall College",
        "aff_unique_dep": "Department of Mathematics and Computer Science",
        "aff_unique_url": "https://www.fandm.edu",
        "aff_unique_abbr": "F&M",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lancaster",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "020c114901",
        "title": "Learning Probability Measures with respect to Optimal Transport Metrics",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c54e7837e0cd0ced286cb5995327d1ab-Abstract.html",
        "author": "Guillermo Canas; Lorenzo Rosasco",
        "abstract": "We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.",
        "bibtex": "@inproceedings{NIPS2012_c54e7837,\n author = {Canas, Guillermo and Rosasco, Lorenzo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Probability Measures with respect to Optimal Transport Metrics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c54e7837e0cd0ced286cb5995327d1ab-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c54e7837e0cd0ced286cb5995327d1ab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 799471,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11825573887530703624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Laboratory for Computational and Statistical Learning - MIT-IIT + CBCL, McGovern Institute - Massachusetts Institute of Technology; Laboratory for Computational and Statistical Learning - MIT-IIT + CBCL, McGovern Institute - Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Laboratory for Computational and Statistical Learning",
        "aff_unique_url": "https://web.mit.edu/",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ba4937c789",
        "title": "Learning about Canonical Views from Internet Image Collections",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/310dcbbf4cce62f762a2aaa148d556bd-Abstract.html",
        "author": "Elad Mezuman; Yair Weiss",
        "abstract": "Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or \u201ccanonical\u201d view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views? We start by manually finding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to find the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories.",
        "bibtex": "@inproceedings{NIPS2012_310dcbbf,\n author = {Mezuman, Elad and Weiss, Yair},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning about Canonical Views from Internet Image Collections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/310dcbbf4cce62f762a2aaa148d556bd-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/310dcbbf4cce62f762a2aaa148d556bd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/310dcbbf4cce62f762a2aaa148d556bd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/310dcbbf4cce62f762a2aaa148d556bd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3363576,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5998031355126857107&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Interdisciplinary Center for Neural Computation + Edmond & Lily Safra Center for Brain Sciences + Hebrew University of Jerusalem; School of Computer Science and Engineering + Edmond & Lily Safra Center for Brain Sciences + Hebrew University of Jerusalem",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;3+1+2",
        "aff_unique_norm": "Interdisciplinary Center for Neural Computation;Edmond & Lily Safra Center for Brain Sciences;Hebrew University of Jerusalem;University Affiliation Not Specified",
        "aff_unique_dep": "Neural Computation;Center for Brain Sciences;;School of Computer Science and Engineering",
        "aff_unique_url": ";;https://www.huji.ac.il;",
        "aff_unique_abbr": ";;HUJI;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Jerusalem",
        "aff_country_unique_index": "1+2;1+2",
        "aff_country_unique": ";United States;Israel"
    },
    {
        "id": "0d4ef0f2b7",
        "title": "Learning as MAP Inference in Discrete Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html",
        "author": "Xianghang Liu; James Petterson; Tib\u00e9rio S. Caetano",
        "abstract": "We present a new formulation for attacking binary classification problems. Instead of relying on convex losses and regularisers such as in SVMs, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but \\emph{discrete} formulation, where estimation amounts to finding a MAP configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss. We argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms, or both. By reducing the learning problem to a MAP inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself. We empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees. Due to the discrete nature of the formulation, it also allows for \\emph{direct} regularisation through cardinality-based penalties, such as the $\\ell_0$ pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner. We also outline a number of open problems arising from the formulation.",
        "bibtex": "@inproceedings{NIPS2012_89fcd07f,\n author = {Liu, Xianghang and Petterson, James and Caetano, Tib\\'{e}rio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning as MAP Inference in Discrete Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/89fcd07f20b6785b92134bd6c1d0fa42-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 460718,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9206237502921897224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "NICTA/UNSW; NICTA/ANU; NICTA/ANU/University of Sydney",
        "aff_domain": "nicta.com.au;nicta.com.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of New South Wales;Australian National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unsw.edu.au;https://www.anu.edu.au",
        "aff_unique_abbr": "UNSW;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "ca710af922",
        "title": "Learning curves for multi-task Gaussian process regression",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/dd8eb9f23fbd362da0e3f4e70b878c16-Abstract.html",
        "author": "Peter Sollich; Simon Ashton",
        "abstract": "We study the average case performance of multi-task Gaussian process (GP)   regression as captured in the learning curve, i.e.\\ the average Bayes error   for a chosen task versus the total number of examples $n$ for all   tasks. For GP covariances that are the product of an   input-dependent covariance function and a free-form inter-task   covariance matrix, we   show that accurate approximations for the learning curve can be   obtained for an arbitrary number of tasks $T$.  We use   these to study the asymptotic learning behaviour for large   $n$. Surprisingly, multi-task learning can be asymptotically essentially   useless: examples from other tasks only help when the   degree of inter-task correlation, $\\rho$, is near its maximal value   $\\rho=1$. This effect is most extreme for learning of smooth target   functions as described by e.g.\\ squared exponential kernels. We also   demonstrate that when learning {\\em many} tasks, the learning curves   separate into an initial phase, where the Bayes error on each task   is reduced down to a plateau value by ``collective learning''    even though most tasks have not seen examples,   and a final decay that occurs only once the number of examples is   proportional to the number of tasks.",
        "bibtex": "@inproceedings{NIPS2012_dd8eb9f2,\n author = {Sollich, Peter and Ashton, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning curves for multi-task Gaussian process regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/dd8eb9f23fbd362da0e3f4e70b878c16-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 359148,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4089325599618662348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f5d8316ea1",
        "title": "Learning from Distributions via Support Measure Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract.html",
        "author": "Krikamol Muandet; Kenji Fukumizu; Francesco Dinuzzo; Bernhard Sch\u00f6lkopf",
        "abstract": "This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.",
        "bibtex": "@inproceedings{NIPS2012_9bf31c7f,\n author = {Muandet, Krikamol and Fukumizu, Kenji and Dinuzzo, Francesco and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning from Distributions via Support Measure Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 198805,
        "gs_citation": 240,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8595944455330607478&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "MPI for Intelligent Systems, T\u00fcbingen; The Institute of Statistical Mathematics, Tokyo; MPI for Intelligent Systems, T\u00fcbingen; MPI for Intelligent Systems, T\u00fcbingen",
        "aff_domain": "tuebingen.mpg.de;ism.ac.jp;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;ism.ac.jp;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Institute of Statistical Mathematics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.ism.ac.jp",
        "aff_unique_abbr": "MPI-IS;ISM",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "T\u00fcbingen;Tokyo",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Germany;Japan"
    },
    {
        "id": "27a594c92e",
        "title": "Learning from the Wisdom of Crowds by Minimax Entropy",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/46489c17893dfdcf028883202cefd6d1-Abstract.html",
        "author": "Dengyong Zhou; Sumit Basu; Yi Mao; John C. Platt",
        "abstract": "An important way to make large training sets is to gather noisy labels from crowds of nonexperts. We propose a minimax entropy principle to improve the quality of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem.",
        "bibtex": "@inproceedings{NIPS2012_46489c17,\n author = {Zhou, Dengyong and Basu, Sumit and Mao, Yi and Platt, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning from the Wisdom of Crowds by Minimax Entropy},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/46489c17893dfdcf028883202cefd6d1-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/46489c17893dfdcf028883202cefd6d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/46489c17893dfdcf028883202cefd6d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 825444,
        "gs_citation": 447,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8467997124371898756&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "70ab8c32b6",
        "title": "Learning optimal spike-based representations",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html",
        "author": "Ralph Bourdoukan; David Barrett; Sophie Deneve; Christian K. Machens",
        "abstract": "How do neural networks learn to represent information? Here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder. We define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss. The dynamical equations yield a network of integrate-and-fire neurons undergoing Hebbian plasticity. We show that, through learning, initially regular and highly correlated spike trains evolve towards Poisson-distributed and independent spike trains with much lower firing rates. The learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder. We show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs. In turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme. Our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code.",
        "bibtex": "@inproceedings{NIPS2012_3a15c7d0,\n author = {Bourdoukan, Ralph and Barrett, David and Deneve, Sophie and Machens, Christian K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning optimal spike-based representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3a15c7d0bbe60300a39f76f8a5ba6896-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1109896,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15386407630253325362&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Group for Neural Theory, \u00b4Ecole Normale Sup \u00b4erieure, Paris, France; Group for Neural Theory, \u00b4Ecole Normale Sup \u00b4erieure, Paris, France; Champalimaud Neuroscience Programme, Champalimaud Centre for the Unknown, Lisbon, Portugal; Group for Neural Theory, \u00b4Ecole Normale Sup \u00b4erieure, Paris, France",
        "aff_domain": "ens.fr;ens.fr;neuro.fchampalimaud.org;ens.fr",
        "email": "ens.fr;ens.fr;neuro.fchampalimaud.org;ens.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Ecole Normale Sup\u00e9rieure;Champalimaud Centre for the Unknown",
        "aff_unique_dep": "Group for Neural Theory;Champalimaud Neuroscience Programme",
        "aff_unique_url": "https://www.ens.fr;https://www.champalimaud.org",
        "aff_unique_abbr": "ENS;",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Paris;Lisbon",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "France;Portugal"
    },
    {
        "id": "6dcc0afa5a",
        "title": "Learning the Architecture of Sum-Product Networks Using Clustering on Variables",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html",
        "author": "Aaron Dennis; Dan Ventura",
        "abstract": "The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture significantly improves its performance compared to using a previously-proposed static architecture.",
        "bibtex": "@inproceedings{NIPS2012_f33ba15e,\n author = {Dennis, Aaron and Ventura, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning the Architecture of Sum-Product Networks Using Clustering on Variables},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f33ba15effa5c10e873bf3842afb46a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 405766,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5207150684177982602&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Brigham Young University; Department of Computer Science, Brigham Young University",
        "aff_domain": "byu.edu;cs.byu.edu",
        "email": "byu.edu;cs.byu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brigham Young University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.byu.edu",
        "aff_unique_abbr": "BYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9dcf658e6b",
        "title": "Learning the Dependency Structure of Latent Factors",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/df0aab058ce179e4f7ab135ed4e641a9-Abstract.html",
        "author": "Yunlong He; Yanjun Qi; Koray Kavukcuoglu; Haesun Park",
        "abstract": "In this paper, we study latent factor models with the dependency structure in the latent space.  We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance.",
        "bibtex": "@inproceedings{NIPS2012_df0aab05,\n author = {He, Yunlong and Qi, Yanjun and Kavukcuoglu, Koray and Park, Haesun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning the Dependency Structure of Latent Factors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/df0aab058ce179e4f7ab135ed4e641a9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/df0aab058ce179e4f7ab135ed4e641a9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1232988,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14478569618408169675&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Georgia Institute of Technology; NEC Labs America; NEC Labs America; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;nec-labs.com;nec-labs.com;cc.gatech.edu",
        "email": "gatech.edu;nec-labs.com;nec-labs.com;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;NEC Labs America",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "Georgia Tech;NEC LA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1d34c9ef29",
        "title": "Learning to Align from Scratch",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d81f9c1be2e08964bf9f24b15f0e4900-Abstract.html",
        "author": "Gary Huang; Marwan Mattar; Honglak Lee; Erik G. Learned-miller",
        "abstract": "Unsupervised joint alignment of images has been demonstrated to   improve performance on recognition tasks such as face verification.   Such alignment reduces undesired variability due to factors such as   pose, while only requiring weak supervision in the form of poorly   aligned examples.  However, prior work on unsupervised alignment of   complex, real world images has required the careful selection of   feature representation based on hand-crafted image descriptors, in   order to achieve an appropriate, smooth optimization landscape.    In this paper, we instead propose a novel combination of   unsupervised joint alignment with unsupervised feature learning.   Specifically, we incorporate deep learning into the {\\em congealing}   alignment framework.  Through deep learning, we obtain features that   can represent the image at differing resolutions based on network   depth, and that are tuned to the statistics of the specific data   being aligned.  In addition, we modify the learning algorithm for   the restricted Boltzmann machine by incorporating a group sparsity   penalty, leading to a topographic organization on the learned   filters and improving subsequent alignment results.    We apply our method to the Labeled Faces in the Wild database   (LFW). Using the aligned images produced by our proposed   unsupervised algorithm, we achieve a significantly higher accuracy   in face verification than obtained using the original face images,   prior work in unsupervised alignment, and prior work in supervised   alignment.  We also match the accuracy for the best available, but   unpublished method.",
        "bibtex": "@inproceedings{NIPS2012_d81f9c1b,\n author = {Huang, Gary and Mattar, Marwan and Lee, Honglak and Learned-miller, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Align from Scratch},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d81f9c1be2e08964bf9f24b15f0e4900-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2104457,
        "gs_citation": 443,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8080465866779262823&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University of Massachusetts, Amherst, MA; University of Massachusetts, Amherst, MA; University of Michigan, Ann Arbor, MI; University of Massachusetts, Amherst, MA",
        "aff_domain": "cs.umass.edu;cs.umass.edu;eecs.umich.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;eecs.umich.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Massachusetts Amherst;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umass.edu;https://www.umich.edu",
        "aff_unique_abbr": "UMass Amherst;UM",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Amherst;Ann Arbor",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "70e8f62c32",
        "title": "Learning to Discover Social Circles in Ego Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7a614fd06c325499f1680b9896beedeb-Abstract.html",
        "author": "Jure Leskovec; Julian J. Mcauley",
        "abstract": "Our personal social networks are big and cluttered, and currently there is no good way to organize them. Social networking sites allow users to manually categorize their friends into social circles (e.g.",
        "bibtex": "@inproceedings{NIPS2012_7a614fd0,\n author = {Leskovec, Jure and Mcauley, Julian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Discover Social Circles in Ego Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7a614fd06c325499f1680b9896beedeb-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7a614fd06c325499f1680b9896beedeb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7a614fd06c325499f1680b9896beedeb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 712911,
        "gs_citation": 2804,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2087091374559854147&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 25,
        "aff": "Stanford, USA; Stanford, USA",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1d6de2403e",
        "title": "Learning visual motion in recurrent neural networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0f96613235062963ccde717b18f97592-Abstract.html",
        "author": "Marius Pachitariu; Maneesh Sahani",
        "abstract": "We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model.",
        "bibtex": "@inproceedings{NIPS2012_0f966132,\n author = {Pachitariu, Marius and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning visual motion in recurrent neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0f96613235062963ccde717b18f97592-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0f96613235062963ccde717b18f97592-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/0f96613235062963ccde717b18f97592-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0f96613235062963ccde717b18f97592-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1179595,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1467000606392355912&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Gatsby Computational Neuroscience Unit, University College London, UK; Gatsby Computational Neuroscience Unit, University College London, UK",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "44f5b6b8b6",
        "title": "Learning with Partially Absorbing Random Walks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/512c5cad6c37edb98ae91c8a76c3a291-Abstract.html",
        "author": "Xiao-ming Wu; Zhenguo Li; Anthony M. So; John Wright; Shih-fu Chang",
        "abstract": "We propose a novel stochastic process that is with probability $\\alpha_i$ being absorbed at current state $i$, and with probability $1-\\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates, a random walk starting from a set $\\mathcal{S}$ of low conductance will be mostly absorbed in $\\mathcal{S}$. Moreover, the absorption probabilities vary slowly inside $\\mathcal{S}$, while dropping sharply outside $\\mathcal{S}$, thus implementing the desirable cluster assumption for graph-based learning. Remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning.",
        "bibtex": "@inproceedings{NIPS2012_512c5cad,\n author = {Wu, Xiao-ming and Li, Zhenguo and So, Anthony and Wright, John and Chang, Shih-fu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Partially Absorbing Random Walks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/512c5cad6c37edb98ae91c8a76c3a291-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/512c5cad6c37edb98ae91c8a76c3a291-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 789678,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18136721116300294143&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical Engineering, Columbia University + Department of Computer Science, Columbia University; Department of Electrical Engineering, Columbia University + Department of Computer Science, Columbia University; Department of SEEM, The Chinese University of Hong Kong; Department of Electrical Engineering, Columbia University + Department of Computer Science, Columbia University; Department of Electrical Engineering, Columbia University + Department of Computer Science, Columbia University",
        "aff_domain": "ee.columbia.edu;ee.columbia.edu;se.cuhk.edu.hk;ee.columbia.edu;ee.columbia.edu",
        "email": "ee.columbia.edu;ee.columbia.edu;se.cuhk.edu.hk;ee.columbia.edu;ee.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0;1;0+0;0+0",
        "aff_unique_norm": "Columbia University;Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Electrical Engineering;Department of SEEM",
        "aff_unique_url": "https://www.columbia.edu;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "Columbia;CUHK",
        "aff_campus_unique_index": ";;1;;",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0;0+0;1;0+0;0+0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "8ce0ed1dbf",
        "title": "Learning with Recursive Perceptual Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/70222949cc0db89ab32c9969754d4758-Abstract.html",
        "author": "Oriol Vinyals; Yangqing Jia; Li Deng; Trevor Darrell",
        "abstract": "Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance. Deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem. We propose a deep non-linear classifier whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks.",
        "bibtex": "@inproceedings{NIPS2012_70222949,\n author = {Vinyals, Oriol and Jia, Yangqing and Deng, Li and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Recursive Perceptual Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/70222949cc0db89ab32c9969754d4758-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/70222949cc0db89ab32c9969754d4758-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/70222949cc0db89ab32c9969754d4758-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1292809,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7692641079042066083&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1d92398f55",
        "title": "Learning with Target Prior",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c9f95a0a5af052bffce5c89917335f67-Abstract.html",
        "author": "Zuoguan Wang; Siwei Lyu; Gerwin Schalk; Qiang Ji",
        "abstract": "In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\\y$ can be modeled with a prior model $p(\\y)$ and the relations between data and target variables are estimated through $p(\\y)$ and a set of uncorresponded data $\\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\\t$ that maximizes the log likelihood of $f_\\t(\\x)$ on a uncorresponded training set with regards to $p(\\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video.",
        "bibtex": "@inproceedings{NIPS2012_c9f95a0a,\n author = {Wang, Zuoguan and Lyu, Siwei and Schalk, Gerwin and Ji, Qiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Target Prior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c9f95a0a5af052bffce5c89917335f67-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 668847,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18157789395886589857&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of ECSE, Rensselaer Polytechnic Inst., Troy, NY 12180; Computer Science, Univ. at Albany, SUNY, Albany, NY 12222; Wadsworth Center, NYS Dept. of Health, Albany, NY 12201; Dept. of ECSE, Rensselaer Polytechnic Inst., Troy, NY 12180",
        "aff_domain": "rpi.edu;cs.albany.edu;wadsworth.org;rpi.edu",
        "email": "rpi.edu;cs.albany.edu;wadsworth.org;rpi.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;University at Albany, State University of New York;New York State Department of Health",
        "aff_unique_dep": "Department of Electrical, Computer, and Systems Engineering;Computer Science;Wadsworth Center",
        "aff_unique_url": "https://www.rpi.edu;https://www.albany.edu;https://www.health.ny.gov/",
        "aff_unique_abbr": "RPI;UAlbany;NYS Dept. of Health",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Troy;Albany",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "443153428b",
        "title": "Link Prediction in Graphs with Autoregressive Features",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html",
        "author": "Emile Richard; Stephane Gaiffas; Nicolas Vayatis",
        "abstract": "In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices which takes into account both sparsity and low rank properties of the matrices. Oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. The estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm.",
        "bibtex": "@inproceedings{NIPS2012_71f6278d,\n author = {Richard, Emile and Gaiffas, Stephane and Vayatis, Nicolas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Link Prediction in Graphs with Autoregressive Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/71f6278d140af599e06ad9bf1ba03cb0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/71f6278d140af599e06ad9bf1ba03cb0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 257436,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9059901476114289226&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7ff8155175",
        "title": "Local Supervised Learning through Space Partitioning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a684eceee76fc522773286a895bc8436-Abstract.html",
        "author": "Joseph Wang; Venkatesh Saligrama",
        "abstract": "We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers. We formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error. We train locally linear classifiers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also show improved robustness to label noise.",
        "bibtex": "@inproceedings{NIPS2012_a684ecee,\n author = {Wang, Joseph and Saligrama, Venkatesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Local Supervised Learning through Space Partitioning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a684eceee76fc522773286a895bc8436-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a684eceee76fc522773286a895bc8436-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/a684eceee76fc522773286a895bc8436-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a684eceee76fc522773286a895bc8436-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 460923,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2984031989340509163&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Electrical and Computer Engineering, Boston University; Dept. of Electrical and Computer Engineering, Boston University",
        "aff_domain": "bu.edu;bu.edu",
        "email": "bu.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Dept. of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2f1e6f4155",
        "title": "Localizing 3D cuboids in single-view images",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/58238e9ae2dd305d79c2ebc8c1883422-Abstract.html",
        "author": "Jianxiong Xiao; Bryan Russell; Antonio Torralba",
        "abstract": "In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners.",
        "bibtex": "@inproceedings{NIPS2012_58238e9a,\n author = {Xiao, Jianxiong and Russell, Bryan and Torralba, Antonio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Localizing 3D cuboids in single-view images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/58238e9ae2dd305d79c2ebc8c1883422-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5728305,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12706941962114147995&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4a9a30a0bb",
        "title": "Locally Uniform Comparison Image Descriptor",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c20ad4d76fe97759aa27a0c99bff6710-Abstract.html",
        "author": "Andrew Ziegler; Eric Christiansen; David Kriegman; Serge J. Belongie",
        "abstract": "Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a random sampling of pairwise comparisons of pixel intensities in an image patch. Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a simple description method based on permutation distances between the ordering of intensities of RGB values between two patches. LUCID is computable in linear time with respect to patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster.",
        "bibtex": "@inproceedings{NIPS2012_c20ad4d7,\n author = {Ziegler, Andrew and Christiansen, Eric and Kriegman, David and Belongie, Serge},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Locally Uniform Comparison Image Descriptor},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c20ad4d76fe97759aa27a0c99bff6710-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c20ad4d76fe97759aa27a0c99bff6710-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c20ad4d76fe97759aa27a0c99bff6710-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 852231,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5003979979967626267&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Engineering, University of California, San Diego; Department of Computer Science and Engineering, University of California, San Diego; Department of Computer Science and Engineering, University of California, San Diego; Department of Computer Science and Engineering, University of California, San Diego",
        "aff_domain": "gatech.edu;cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu",
        "email": "gatech.edu;cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c0dce3536c",
        "title": "Locating Changes in Highly Dependent Data with Unknown Number of Change Points",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/299fb2142d7de959380f91c01c3a293c-Abstract.html",
        "author": "Azadeh Khaleghi; Daniil Ryabko",
        "abstract": "The problem of multiple change point estimation is  considered for sequences  with unknown number of change points.  A consistency framework is suggested that is suitable for highly dependent time-series, and an asymptotically consistent algorithm is proposed.   In order for the consistency to be established the only assumption  required is that the data is generated by stationary ergodic time-series distributions. No modeling, independence or parametric assumptions are made; the data are allowed to be dependent and the dependence can be of arbitrary form.  The theoretical results are complemented with experimental evaluations.",
        "bibtex": "@inproceedings{NIPS2012_299fb214,\n author = {Khaleghi, Azadeh and Ryabko, Daniil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Locating Changes in Highly Dependent Data with Unknown Number of Change Points},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/299fb2142d7de959380f91c01c3a293c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 368295,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9431824585596010495&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "SequeL-INRIA/LIFL-CNRS, Universit \u00b4e de Lille, France; SequeL-INRIA/LIFL-CNRS",
        "aff_domain": "inria.fr;ryabko.net",
        "email": "inria.fr;ryabko.net",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universit\u00e9 de Lille;INRIA",
        "aff_unique_dep": "SequeL-INRIA/LIFL-CNRS;SequeL",
        "aff_unique_url": "https://www.univ-lille.fr;https://www.inria.fr",
        "aff_unique_abbr": ";INRIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "c3b059370b",
        "title": "MAP Inference in Chains using Column Generation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html",
        "author": "David Belanger; Alexandre Passos; Sebastian Riedel; Andrew McCallum",
        "abstract": "Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning.",
        "bibtex": "@inproceedings{NIPS2012_7634ea65,\n author = {Belanger, David and Passos, Alexandre and Riedel, Sebastian and McCallum, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {MAP Inference in Chains using Column Generation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/7634ea65a4e6d9041cfd3f7de18e334a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7634ea65a4e6d9041cfd3f7de18e334a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 309088,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15450321125316065235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Massachusetts, Amherst; Department of Computer Science, University of Massachusetts, Amherst; Department of Computer Science, University College London; Department of Computer Science, University of Massachusetts, Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.ucl.ac.uk;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.ucl.ac.uk;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Massachusetts Amherst;University College London",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.umass.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "UMass Amherst;UCL",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Amherst;London",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "4900d4ec4b",
        "title": "MCMC for continuous-time discrete-state systems",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html",
        "author": "Vinayak Rao; Yee W. Teh",
        "abstract": "We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process, while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler, and show its advantages.",
        "bibtex": "@inproceedings{NIPS2012_6da37dd3,\n author = {Rao, Vinayak and Teh, Yee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {MCMC for continuous-time discrete-state systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 382703,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10109580270060141032&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f19e3219e7",
        "title": "Majorization for CRFs and Latent Likelihoods",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d395771085aab05244a4fb8fd91bf4ee-Abstract.html",
        "author": "Tony Jebara; Anna Choromanska",
        "abstract": "The partition function plays a key role in probabilistic modeling including conditional random fields, graphical models, and maximum likelihood estimation. To optimize partition functions, this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Such bounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems, low-rank versions of the bound are provided and outperform LBFGS as well as first-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods.",
        "bibtex": "@inproceedings{NIPS2012_d3957710,\n author = {Jebara, Tony and Choromanska, Anna},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Majorization for CRFs and Latent Likelihoods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d395771085aab05244a4fb8fd91bf4ee-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d395771085aab05244a4fb8fd91bf4ee-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/d395771085aab05244a4fb8fd91bf4ee-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d395771085aab05244a4fb8fd91bf4ee-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 744376,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6927785777374472692&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Columbia University; Department of Electrical Engineering, Columbia University",
        "aff_domain": "cs.columbia.edu;columbia.edu",
        "email": "cs.columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2f1dd2a8b3",
        "title": "Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f899139df5e1059396431415e770c6dd-Abstract.html",
        "author": "Wei Bi; James T. Kwok",
        "abstract": "In hierarchical classification, the prediction paths may be required to always end at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes. However, while there have been a lot of MLNP methods in hierarchical multiclass classification, performing MLNP in hierarchical multilabel classification is much more difficult. In this paper, we propose a novel MLNP algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and DAGs. We show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm. Moreover, this can be further extended to the minimization of the expected symmetric loss. Experiments are performed on a number of real-world data sets with tree- and DAG-structured label hierarchies. The  proposed method consistently outperforms other hierarchical and flat multilabel classification methods.",
        "bibtex": "@inproceedings{NIPS2012_f899139d,\n author = {Bi, Wei and Kwok, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f899139df5e1059396431415e770c6dd-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f899139df5e1059396431415e770c6dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f899139df5e1059396431415e770c6dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 926172,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8420248237240250149&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology",
        "aff_domain": "cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "c6f3ee15ac",
        "title": "Matrix reconstruction with the local max norm",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f5f8590cd58a54e94377e6ae2eded4d9-Abstract.html",
        "author": "Rina Foygel; Nathan Srebro; Ruslan Salakhutdinov",
        "abstract": "We introduce a new family of matrix norms, the ''local max'' norms, generalizing existing methods such as the max norm, the trace norm (nuclear norm), and the weighted or smoothed weighted trace norms, which have been extensively used in the literature as regularizers for matrix reconstruction problems. We show that this new family can be used to interpolate between the (weighted or unweighted) trace norm and the more conservative max norm. We test this interpolation on simulated data and on the large-scale Netflix and MovieLens ratings data, and find improved accuracy relative to the existing matrix norms. We also provide theoretical results showing learning guarantees for some of the new norms.",
        "bibtex": "@inproceedings{NIPS2012_f5f8590c,\n author = {Foygel, Rina and Srebro, Nathan and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Matrix reconstruction with the local max norm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/f5f8590cd58a54e94377e6ae2eded4d9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f5f8590cd58a54e94377e6ae2eded4d9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 306084,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17802160122936387670&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Statistics, Stanford University; Toyota Technological Institute at Chicago; Dept. of Statistics and Dept. of Computer Science, University of Toronto",
        "aff_domain": "stanford.edu;ttic.edu;utstat.toronto.edu",
        "email": "stanford.edu;ttic.edu;utstat.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Stanford University;Toyota Technological Institute at Chicago;University of Toronto",
        "aff_unique_dep": "Department of Statistics;;Dept. of Statistics",
        "aff_unique_url": "https://www.stanford.edu;https://www.tti-chicago.org;https://www.utoronto.ca",
        "aff_unique_abbr": "Stanford;TTI Chicago;U of T",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Stanford;Chicago;Toronto",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "1a6c33659e",
        "title": "Max-Margin Structured Output Regression for Spatio-Temporal Action Localization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9872ed9fc22fc182d371c3e9ed316094-Abstract.html",
        "author": "Du Tran; Junsong Yuan",
        "abstract": "Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because one needs to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efficient Max-Path search method, thus makes it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2012_9872ed9f,\n author = {Tran, Du and Yuan, Junsong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Max-Margin Structured Output Regression for Spatio-Temporal Action Localization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9872ed9fc22fc182d371c3e9ed316094-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 8212406,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10876493848856302959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "aff_domain": "gmail.com;ntu.edu.sg",
        "email": "gmail.com;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "8f3afb5af2",
        "title": "Memorability of Image Regions",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e9dae45ec08b498f7e1af247757c9b35-Abstract.html",
        "author": "Aditya Khosla; Jianxiong Xiao; Antonio Torralba; Aude Oliva",
        "abstract": "While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten using a data-driven approach that combines local and global images features. The model automatically discov- ers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works.",
        "bibtex": "@inproceedings{NIPS2012_e9dae45e,\n author = {Khosla, Aditya and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Memorability of Image Regions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e9dae45ec08b498f7e1af247757c9b35-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e9dae45ec08b498f7e1af247757c9b35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e9dae45ec08b498f7e1af247757c9b35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3338084,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9113047679991038558&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "47eb8b3cae",
        "title": "Meta-Gaussian Information Bottleneck",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3cef96dcc9b8035d23f69e30bb19218a-Abstract.html",
        "author": "Melanie Rey; Volker Roth",
        "abstract": "We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula  entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers.",
        "bibtex": "@inproceedings{NIPS2012_3cef96dc,\n author = {Rey, Melanie and Roth, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Meta-Gaussian Information Bottleneck},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/3cef96dcc9b8035d23f69e30bb19218a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3cef96dcc9b8035d23f69e30bb19218a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 932617,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8812498874666706725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Mathematics and Computer Science, University of Basel; Department of Mathematics and Computer Science, University of Basel",
        "aff_domain": "unibas.ch;unibas.ch",
        "email": "unibas.ch;unibas.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Basel",
        "aff_unique_dep": "Department of Mathematics and Computer Science",
        "aff_unique_url": "https://www.unibas.ch",
        "aff_unique_abbr": "UniBas",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "e866a23fdf",
        "title": "Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/dc58e3a306451c9d670adcd37004f48f-Abstract.html",
        "author": "Nishant Mehta; Dongryeol Lee; Alexander G. Gray",
        "abstract": "Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.   We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.",
        "bibtex": "@inproceedings{NIPS2012_dc58e3a3,\n author = {Mehta, Nishant and Lee, Dongryeol and Gray, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/dc58e3a306451c9d670adcd37004f48f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 263146,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3034651762779610116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "83065e3c58",
        "title": "Minimization of Continuous Bethe Approximations: A Positive Variation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2715518c875999308842e3455eda2fe3-Abstract.html",
        "author": "Jason Pacheco; Erik B. Sudderth",
        "abstract": "We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions.  While existing message passing algorithms define fixed point iterations corresponding to stationary points of the Bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge. For continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as Gaussians with negative variance. Conversely, our approach leverages multiplier methods with well-understood convergence properties, and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random fields, showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation.",
        "bibtex": "@inproceedings{NIPS2012_2715518c,\n author = {Pacheco, Jason and Sudderth, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimization of Continuous Bethe Approximations: A Positive Variation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2715518c875999308842e3455eda2fe3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2715518c875999308842e3455eda2fe3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/2715518c875999308842e3455eda2fe3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2715518c875999308842e3455eda2fe3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 479449,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=316018765451478221&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, Brown University, Providence, RI; Department of Computer Science, Brown University, Providence, RI",
        "aff_domain": "cs.brown.edu;cs.brown.edu",
        "email": "cs.brown.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Providence",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "644b5ee6bb",
        "title": "Minimizing Sparse High-Order Energies by Submodular Vertex-Cover",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/98b297950041a42470269d56260243a1-Abstract.html",
        "author": "Andrew Delong; Olga Veksler; Anton Osokin; Yuri Boykov",
        "abstract": "Inference on high-order graphical models has become increasingly important in recent years.  We consider energies with simple 'sparse' high-order potentials.  Previous work in this area uses either specialized message-passing  or transforms each high-order potential to the pairwise case. We take a fundamentally different approach, transforming the entire original problem into a comparatively small instance of a submodular vertex-cover problem. These vertex-cover instances can then be attacked by standard pairwise methods, where they run much faster (4--15 times) and are often more effective than on the original problem. We evaluate our approach on synthetic data, and we show that our algorithm can be useful in a fast hierarchical clustering and model estimation framework.",
        "bibtex": "@inproceedings{NIPS2012_98b29795,\n author = {Delong, Andrew and Veksler, Olga and Osokin, Anton and Boykov, Yuri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimizing Sparse High-Order Energies by Submodular Vertex-Cover},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/98b297950041a42470269d56260243a1-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/98b297950041a42470269d56260243a1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/98b297950041a42470269d56260243a1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2438481,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6137769066830594171&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of Toronto; Western University; Moscow State University; Western University",
        "aff_domain": "gmail.com;csd.uwo.ca;gmail.com;csd.uwo.ca",
        "email": "gmail.com;csd.uwo.ca;gmail.com;csd.uwo.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "University of Toronto;Western University;Moscow State University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.uwo.ca;https://www.msu.ru",
        "aff_unique_abbr": "U of T;Western;MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Canada;Russian Federation"
    },
    {
        "id": "ca3bc3ea33",
        "title": "Minimizing Uncertainty in Pipelines",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9cb67ffb59554ab1dabb65bcb370ddd9-Abstract.html",
        "author": "Nilesh Dalvi; Aditya Parameswaran; Vibhor Rastogi",
        "abstract": "In this paper, we consider the problem of debugging large pipelines by human labeling. We represent the execution of a pipeline using a directed acyclic graph of AND and OR nodes, where each node represents a data item produced by some operator in the pipeline. We assume that each operator assigns a confidence to each of its output data. We want to reduce the uncertainty in the output by issuing queries to a human expert, where a query consists of checking if a given data item is correct. In this paper, we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty. We perform a detailed evaluation of the complexity of the problem for various classes of graphs. We give efficient algorithms for the problem for trees, and show that, for a general dag, the problem is intractable.",
        "bibtex": "@inproceedings{NIPS2012_9cb67ffb,\n author = {Dalvi, Nilesh and Parameswaran, Aditya and Rastogi, Vibhor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimizing Uncertainty in Pipelines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9cb67ffb59554ab1dabb65bcb370ddd9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9cb67ffb59554ab1dabb65bcb370ddd9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9cb67ffb59554ab1dabb65bcb370ddd9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 245545,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10502971094404815536&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Facebook, Inc.; Stanford University; Google, Inc.",
        "aff_domain": "fb.com;cs.stanford.edu;gmail.com",
        "email": "fb.com;cs.stanford.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Meta;Stanford University;Google",
        "aff_unique_dep": "Facebook;;Google",
        "aff_unique_url": "https://www.facebook.com;https://www.stanford.edu;https://www.google.com",
        "aff_unique_abbr": "FB;Stanford;Google",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Stanford;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6d4f980445",
        "title": "Mirror Descent Meets Fixed Share (and feels no regret)",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8e6b42f1644ecb1327dc03ab345e618b-Abstract.html",
        "author": "Nicol\u00f2 Cesa-bianchi; Pierre Gaillard; Gabor Lugosi; Gilles Stoltz",
        "abstract": "Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters.",
        "bibtex": "@inproceedings{NIPS2012_8e6b42f1,\n author = {Cesa-bianchi, Nicol\\`{o} and Gaillard, Pierre and Lugosi, Gabor and Stoltz, Gilles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mirror Descent Meets Fixed Share (and feels no regret)},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/8e6b42f1644ecb1327dc03ab345e618b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8e6b42f1644ecb1327dc03ab345e618b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 329258,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15997120010813184399&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Universit\u00e0 degli Studi di Milano; Ecole Normale Sup\u00e9rieure\u2217, Paris; ICREA & Universitat Pompeu Fabra, Barcelona; Ecole Normale Sup\u00e9rieure\u2217, Paris & HEC Paris, Jouy-en-Josas, France",
        "aff_domain": "unimi.it;ens.fr;upf.edu;ens.fr",
        "email": "unimi.it;ens.fr;upf.edu;ens.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Universit\u00e0 degli Studi di Milano;Ecole Normale Sup\u00e9rieure;Universitat Pompeu Fabra",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unimi.it;https://www.ens.fr;https://www.upf.edu",
        "aff_unique_abbr": "UniMi;ENS;UPF",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Paris;Barcelona",
        "aff_country_unique_index": "0;1;2;1",
        "aff_country_unique": "Italy;France;Spain"
    },
    {
        "id": "f52174dde2",
        "title": "Mixability in Statistical Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/670e8a43b246801ca1eaca97b3e19189-Abstract.html",
        "author": "Tim V. Erven; Peter Gr\u00fcnwald; Mark D. Reid; Robert C. Williamson",
        "abstract": "Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability.",
        "bibtex": "@inproceedings{NIPS2012_670e8a43,\n author = {Erven, Tim and Gr\\\"{u}nwald, Peter and Reid, Mark D and Williamson, Robert C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mixability in Statistical Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/670e8a43b246801ca1eaca97b3e19189-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/670e8a43b246801ca1eaca97b3e19189-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/670e8a43b246801ca1eaca97b3e19189-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/670e8a43b246801ca1eaca97b3e19189-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 422462,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5238739248453732643&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Universit\u00e9 Paris-Sud, France; CWI and Leiden University, the Netherlands; ANU and NICTA, Australia; ANU and NICTA, Australia",
        "aff_domain": "timvanerven.nl;cwi.nl;anu.edu.au;anu.edu.au",
        "email": "timvanerven.nl;cwi.nl;anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Universit\u00e9 Paris-Sud;Centrum Wiskunde & Informatica;Australian National University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.universite-paris-sud.fr;https://www.cwi.nl/;https://www.anu.edu.au",
        "aff_unique_abbr": "UPS;CWI;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;2",
        "aff_country_unique": "France;Netherlands;Australia"
    },
    {
        "id": "840e3d0cd4",
        "title": "Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html",
        "author": "Mathieu Sinn; Bei Chen",
        "abstract": "Conditional Markov Chains (also known as Linear-Chain Conditional Random Fields  in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables. Large-sample properties of Conditional Markov Chains have been first studied by Sinn and Poupart [1]. The paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given.",
        "bibtex": "@inproceedings{NIPS2012_c2aee861,\n author = {Sinn, Mathieu and Chen, Bei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c2aee86157b4a40b78132f1e71a9e6f1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c2aee86157b4a40b78132f1e71a9e6f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 282991,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17331788231940850080&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IBM Research - Ireland, Mulhuddart, Dublin 15; McMaster University, Hamilton, Ontario, Canada",
        "aff_domain": "ie.ibm.com;math.mcmaster.ca",
        "email": "ie.ibm.com;math.mcmaster.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IBM;McMaster University",
        "aff_unique_dep": "IBM Research - Ireland;",
        "aff_unique_url": "https://www.ibm.com/research;https://www.mcmaster.ca",
        "aff_unique_abbr": "IBM;McMaster",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Dublin;Hamilton",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Ireland;Canada"
    },
    {
        "id": "5fd20d33e9",
        "title": "Modelling Reciprocating Relationships with Hawkes Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/40cb228987243c91b2dd0b7c9c4a0856-Abstract.html",
        "author": "Charles Blundell; Jeff Beck; Katherine A. Heller",
        "abstract": "We present a Bayesian nonparametric model that discovers implicit social structure from interaction time-series data. Social groups are often formed implicitly, through actions among members of groups. Yet many models of social networks use explicitly declared relationships to infer social structure. We consider a particular class of Hawkes processes, a doubly stochastic point process, that is able to model reciprocity between groups of individuals. We then extend the Infinite Relational Model by using these reciprocating Hawkes processes to parameterise its edges, making events associated with edges co-dependent through time. Our model outperforms general, unstructured Hawkes processes as well as structured Poisson process-based models at predicting verbal and email turn-taking, and military conflicts among nations.",
        "bibtex": "@inproceedings{NIPS2012_40cb2289,\n author = {Blundell, Charles and Beck, Jeff and Heller, Katherine A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modelling Reciprocating Relationships with Hawkes Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/40cb228987243c91b2dd0b7c9c4a0856-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/40cb228987243c91b2dd0b7c9c4a0856-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/40cb228987243c91b2dd0b7c9c4a0856-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 931165,
        "gs_citation": 259,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=602848896589555042&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom; Duke University, Durham, NC, USA; University of Rochester, Rochester, NY, USA",
        "aff_domain": "gatsby.ucl.ac.uk;stat.duke.edu;bcs.rochester.edu",
        "email": "gatsby.ucl.ac.uk;stat.duke.edu;bcs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University College London;Duke University;University of Rochester",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.duke.edu;https://www.rochester.edu",
        "aff_unique_abbr": "UCL;Duke;U of R",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "London;Durham;Rochester",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "606bc1ccb1",
        "title": "Monte Carlo Methods for Maximum Margin Supervised Topic Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/912d2b1c7b2826caf99687388d2e8f7c-Abstract.html",
        "author": "Qixia Jiang; Jun Zhu; Maosong Sun; Eric P. Xing",
        "abstract": "An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihood-based supervised topic models, of which posterior inference can be carried out using the Bayes' rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions. In this paper, we develop two efficient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency.",
        "bibtex": "@inproceedings{NIPS2012_912d2b1c,\n author = {Jiang, Qixia and Zhu, Jun and Sun, Maosong and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Monte Carlo Methods for Maximum Margin Supervised Topic Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/912d2b1c7b2826caf99687388d2e8f7c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1216551,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11784303610766816895&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science & Technology, Tsinghua National TNList Lab+State Key Lab of Intelligent Tech. & Sys., Tsinghua University, Beijing 100084, China; Department of Computer Science & Technology, Tsinghua National TNList Lab+State Key Lab of Intelligent Tech. & Sys., Tsinghua University, Beijing 100084, China; Department of Computer Science & Technology, Tsinghua National TNList Lab, Tsinghua University, Beijing 100084, China; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "mail.tsinghua.edu.cn;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn;cs.cmu.edu",
        "email": "mail.tsinghua.edu.cn;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0;0;1",
        "aff_unique_norm": "Tsinghua University;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science & Technology;School of Computer Science",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.cmu.edu",
        "aff_unique_abbr": "THU;CMU",
        "aff_campus_unique_index": "1;1;1;2",
        "aff_campus_unique": ";Beijing;Pittsburgh",
        "aff_country_unique_index": "0+0;0+0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "e721858914",
        "title": "Multi-Stage Multi-Task Feature Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2ab56412b1163ee131e1246da0955bd1-Abstract.html",
        "author": "Pinghua Gong; Jieping Ye; Chang-shui Zhang",
        "abstract": "Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an $\\ell_0$-type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms.",
        "bibtex": "@inproceedings{NIPS2012_2ab56412,\n author = {Gong, Pinghua and Ye, Jieping and Zhang, Chang-shui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Stage Multi-Task Feature Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/2ab56412b1163ee131e1246da0955bd1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2ab56412b1163ee131e1246da0955bd1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 159443,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17977048708724954916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "State Key Laboratory on Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Automation, Tsinghua University, Beijing 100084, China; Computer Science and Engineering, Center for Evolutionary Medicine and Informatics, The Biodesign Institute, Arizona State University, Tempe, AZ 85287, USA; State Key Laboratory on Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Automation, Tsinghua University, Beijing 100084, China",
        "aff_domain": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;asu.edu",
        "email": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Tsinghua University;Arizona State University",
        "aff_unique_dep": "Department of Automation;Computer Science and Engineering",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.asu.edu",
        "aff_unique_abbr": "Tsinghua;ASU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Beijing;Tempe",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "bcbc7804b1",
        "title": "Multi-Task Averaging",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1728efbda81692282ba642aafd57be3a-Abstract.html",
        "author": "Sergey Feldman; Maya Gupta; Bela Frigyik",
        "abstract": "We present a multi-task learning approach to jointly estimate the means of multiple independent data sets. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the single-task averages. We derive the optimal amount of regularization, and show that it can be effectively estimated. Simulations and real data experiments demonstrate that MTA  both maximum likelihood and James-Stein estimators, and that our approach to estimating the amount of regularization rivals cross-validation in performance but is more computationally efficient.",
        "bibtex": "@inproceedings{NIPS2012_1728efbd,\n author = {Feldman, Sergey and Gupta, Maya and Frigyik, Bela},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Task Averaging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1728efbda81692282ba642aafd57be3a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1728efbda81692282ba642aafd57be3a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1728efbda81692282ba642aafd57be3a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 302240,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5320290076878866684&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b568de04e7",
        "title": "Multi-criteria Anomaly Detection using Pareto Depth Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html",
        "author": "Ko-jen Hsiao; Kevin Xu; Jeff Calder; Alfred O. Hero",
        "abstract": "We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.",
        "bibtex": "@inproceedings{NIPS2012_1543843a,\n author = {Hsiao, Ko-jen and Xu, Kevin and Calder, Jeff and Hero, Alfred},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-criteria Anomaly Detection using Pareto Depth Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1543843a4723ed2ab08e18053ae6dc5b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1543843a4723ed2ab08e18053ae6dc5b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 526316,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6357569762815641066&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Michigan, Ann Arbor, MI, USA 48109; University of Michigan, Ann Arbor, MI, USA 48109; University of Michigan, Ann Arbor, MI, USA 48109; University of Michigan, Ann Arbor, MI, USA 48109",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b64da2d3a4",
        "title": "Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/33e75ff09dd601bbe69f351039152189-Abstract.html",
        "author": "C. M. Niu; Sirish Nandyala; Won J. Sohn; Terence Sanger",
        "abstract": "Our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia. To this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics. The models also need to be evaluated in hyper-time, i.e. significantly faster than real-time, for producing useful predictions. We designed a platform with digital VLSI hardware for multi-scale hyper-time emulations of human motor nervous systems. The platform is constructed on a scalable, distributed array of Field Programmable Gate Array (FPGA) devices. All devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time. Each physiological component is implemented using models from well documented studies and can be flexibly modified. Thus the validity of emulation can be easily advised by neurophysiologists and clinicians. For maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits. This paper presents the methodology of building FPGA modules in correspondence to components of a monosynaptic spinal loop. Results of emulated activities are shown. The paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections. In conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed. It compels us to test the origins of childhood motor disorders and predict their long-term progressions.",
        "bibtex": "@inproceedings{NIPS2012_33e75ff0,\n author = {Niu, C. and Nandyala, Sirish and Sohn, Won and Sanger, Terence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-scale Hyper-time Hardware Emulation of Human Motor Nervous System Based on Spiking Neurons using FPGA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/33e75ff09dd601bbe69f351039152189-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/33e75ff09dd601bbe69f351039152189-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/33e75ff09dd601bbe69f351039152189-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1718911,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6164920160634941349&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Biomedical Engineering, University of Southern California, Los Angeles, CA 90089; Department of Biomedical Engineering, University of Southern California, Los Angeles, CA 90089; Department of Biomedical Engineering, University of Southern California, Los Angeles, CA 90089; Department of Biomedical Engineering+Department of Neurology+Department of Biokinesiology, University of Southern California, Los Angeles, CA 90089",
        "aff_domain": "sangerlab.net;usc.edu;gmail.com;sangerlab.net",
        "email": "sangerlab.net;usc.edu;gmail.com;sangerlab.net",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1+2+0",
        "aff_unique_norm": "University of Southern California;Department of Biomedical Engineering;Unknown Institution",
        "aff_unique_dep": "Department of Biomedical Engineering;Biomedical Engineering;Department of Neurology",
        "aff_unique_url": "https://www.usc.edu;;",
        "aff_unique_abbr": "USC;;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "357e969725",
        "title": "Multi-task Vector Field Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html",
        "author": "Binbin Lin; Sen Yang; Chiyuan Zhang; Jieping Ye; Xiaofei He",
        "abstract": "Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach.",
        "bibtex": "@inproceedings{NIPS2012_a5e00132,\n author = {Lin, Binbin and Yang, Sen and Zhang, Chiyuan and Ye, Jieping and He, Xiaofei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-task Vector Field Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a5e00132373a7031000fd987a3c9f87b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1168270,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4585108238283010945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "State Key Lab of CAD &CG, Zhejiang University, Hangzhou 310058, China; The Biodesign Institute, Arizona State University, Tempe, AZ, 85287; State Key Lab of CAD &CG, Zhejiang University, Hangzhou 310058, China; The Biodesign Institute, Arizona State University, Tempe, AZ, 85287; State Key Lab of CAD &CG, Zhejiang University, Hangzhou 310058, China",
        "aff_domain": "gmail.com;asu.edu;gmail.com;asu.edu;gmail.com",
        "email": "gmail.com;asu.edu;gmail.com;asu.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "Zhejiang University;Arizona State University",
        "aff_unique_dep": "State Key Lab of CAD &CG;The Biodesign Institute",
        "aff_unique_url": "http://www.zju.edu.cn;https://asu.edu",
        "aff_unique_abbr": "ZJU;ASU",
        "aff_campus_unique_index": "0;1;0;1;0",
        "aff_campus_unique": "Hangzhou;Tempe",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "a07f78cf57",
        "title": "Multiclass Learning Approaches: A Theoretical Comparison with Implications",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/19f3cd308f1455b3fa09a282e0d496f4-Abstract.html",
        "author": "Amit Daniely; Sivan Sabato; Shai S. Shwartz",
        "abstract": "We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \\emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.",
        "bibtex": "@inproceedings{NIPS2012_19f3cd30,\n author = {Daniely, Amit and Sabato, Sivan and Shwartz, Shai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiclass Learning Approaches: A Theoretical Comparison with Implications},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/19f3cd308f1455b3fa09a282e0d496f4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/19f3cd308f1455b3fa09a282e0d496f4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 255551,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1059252489798015100&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "31716bd71e",
        "title": "Multiclass Learning with Simplex Coding",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html",
        "author": "Youssef Mroueh; Tomaso Poggio; Lorenzo Rosasco; Jean-jeacques Slotine",
        "abstract": "In this paper we dicuss a novel  framework for multiclass learning,  defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class.  Moreover, we show that in this setting it is possible to derive the first provably consistent  regularized methods with training/tuning complexity which is {\\em independent} to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper.",
        "bibtex": "@inproceedings{NIPS2012_1cecc7a7,\n author = {Mroueh, Youssef and Poggio, Tomaso and Rosasco, Lorenzo and Slotine, Jean-jeacques},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiclass Learning with Simplex Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1cecc7a77928ca8133fa24680a88d2f9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1cecc7a77928ca8133fa24680a88d2f9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 424429,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6560048631001706734&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CBCL, McGovern Institute, MIT; CBCL, McGovern Institute, MIT; CBCL, McGovern Institute, MIT; LCSL, MIT- IIT + ME, BCS, MIT",
        "aff_domain": "mit.edu;ai.mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;ai.mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://web.mit.edu/",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "660b28f6fe",
        "title": "Multilabel Classification using Bayesian Compressed Sensing",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e1d5be1c7f2f456670de3d53c7b54f4a-Abstract.html",
        "author": "Ashish Kapoor; Raajay Viswanathan; Prateek Jain",
        "abstract": "In this paper, we present a Bayesian framework for multilabel classification using compressed sensing. The key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key benefits of the model are that a) it can naturally handle datasets that have  missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case.  Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model.",
        "bibtex": "@inproceedings{NIPS2012_e1d5be1c,\n author = {Kapoor, Ashish and Viswanathan, Raajay and Jain, Prateek},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multilabel Classification using Bayesian Compressed Sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e1d5be1c7f2f456670de3d53c7b54f4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 495052,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13270909989814549762&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft Research, Redmond, USA; Microsoft Research, Bangalore, INDIA; Microsoft Research, Bangalore, INDIA",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Redmond;Bangalore",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "a15dfdb2a5",
        "title": "Multimodal Learning with Deep Boltzmann Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html",
        "author": "Nitish Srivastava; Ruslan Salakhutdinov",
        "abstract": "We propose a Deep Boltzmann Machine for learning a generative model of multimodal data. We show how to use the model to extract a meaningful representation of multimodal data. We find that the learned representation is useful for classification and information retreival tasks, and hence conforms to some notion of semantic similarity. The model defines a probability density over the space of multimodal inputs. By sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that our model can significantly outperform SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains.",
        "bibtex": "@inproceedings{NIPS2012_af21d0c9,\n author = {Srivastava, Nitish and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multimodal Learning with Deep Boltzmann Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/af21d0c97db2e27e13572cbf59eb343d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/af21d0c97db2e27e13572cbf59eb343d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1229697,
        "gs_citation": 2235,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6558128843129001214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 29,
        "aff": "Department of Computer Science, University of Toronto; Department of Statistics and Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "d29f6a3c80",
        "title": "Multiple Choice Learning: Learning to Produce Multiple Structured Outputs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html",
        "author": "Abner Guzm\u00e1n-rivera; Dhruv Batra; Pushmeet Kohli",
        "abstract": "The paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade. Given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one. The standard approach for handling this scenario is to learn a single model and then produce M-best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we formulate this multiple {\\em choice} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem. We present a max-margin formulation  that minimizes an upper-bound on this loss-function. Experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this  scenario and leads to substantial improvements in prediction accuracy.",
        "bibtex": "@inproceedings{NIPS2012_cfbce4c1,\n author = {Guzm\\'{a}n-rivera, Abner and Batra, Dhruv and Kohli, Pushmeet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiple Choice Learning: Learning to Produce Multiple Structured Outputs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/cfbce4c1d7c425baf21d6b6f2babe6be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1631626,
        "gs_citation": 249,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16138143721581950981&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Illinois; Virginia Tech; Microsoft Research Cambridge",
        "aff_domain": "illinois.edu;vt.edu;microsoft.com",
        "email": "illinois.edu;vt.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Illinois;Virginia Tech;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www.illinois.edu;https://www.vt.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-cambridge",
        "aff_unique_abbr": "UIUC;VT;MSR Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "1fa4c0b529",
        "title": "Multiple Operator-valued Kernel Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/36a1694bce9815b7e38a9dad05ad42e0-Abstract.html",
        "author": "Hachem Kadri; Alain Rakotomamonjy; Philippe Preux; Francis R. Bach",
        "abstract": "Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.",
        "bibtex": "@inproceedings{NIPS2012_36a1694b,\n author = {Kadri, Hachem and Rakotomamonjy, Alain and Preux, Philippe and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiple Operator-valued Kernel Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/36a1694bce9815b7e38a9dad05ad42e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 365699,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9576545840303971455&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "LIF - CNRS / INRIA Lille - Sequel Project, Universit \u00b4e Aix-Marseille, Marseille, France; LITIS EA 4108, Universit \u00b4e de Rouen, St Etienne du Rouvray, France; INRIA - Sierra Project, Ecole Normale Sup \u00b4erieure, Paris, France; INRIA Lille - Sequel Project, LIFL - CNRS, Universit \u00b4e de Lille, Villeneuve d\u2019Ascq, France",
        "aff_domain": "lif.univ-mrs.fr;insa-rouen.fr;inria.fr;inria.fr",
        "email": "lif.univ-mrs.fr;insa-rouen.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "CNRS;Universit\u00e9 de Rouen;INRIA;INRIA Lille",
        "aff_unique_dep": "LIF - Sequel Project;LITIS EA 4108;Sierra Project;Sequel Project",
        "aff_unique_url": "https://www.cnrs.fr;https://www.univ-rouen.fr;https://www.inria.fr;https://www.inria.fr",
        "aff_unique_abbr": "CNRS;;INRIA;INRIA",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Lille;St Etienne du Rouvray;Paris",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "8900ee4b69",
        "title": "Multiplicative Forests for Continuous-Time Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html",
        "author": "Jeremy Weiss; Sriraam Natarajan; David Page",
        "abstract": "Learning temporal dependencies between variables over continuous time is an important and challenging task. Continuous-time Bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices, which grows exponentially in the number of parents per variable. We develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits. Using a multiplicative assumption we show how to update the forest likelihood in closed form, producing efficient model updates. Our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability.",
        "bibtex": "@inproceedings{NIPS2012_cb70ab37,\n author = {Weiss, Jeremy and Natarajan, Sriraam and Page, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiplicative Forests for Continuous-Time Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/cb70ab375662576bd1ac5aaf16b3fca4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 143430,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13555239743043086556&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Wisconsin; Wake Forest University; University of Wisconsin",
        "aff_domain": "cs.wisc.edu;wakehealth.edu;biostat.wisc.edu",
        "email": "cs.wisc.edu;wakehealth.edu;biostat.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Wisconsin;Wake Forest University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wisc.edu;https://www.wfu.edu",
        "aff_unique_abbr": "UW;WFU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a1c0ca1ea6",
        "title": "Multiresolution Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/819f46e52c25763a55cc642422644317-Abstract.html",
        "author": "Emily B. Fox; David B. Dunson",
        "abstract": "We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity.",
        "bibtex": "@inproceedings{NIPS2012_819f46e5,\n author = {Fox, Emily and Dunson, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiresolution Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/819f46e52c25763a55cc642422644317-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/819f46e52c25763a55cc642422644317-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/819f46e52c25763a55cc642422644317-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/819f46e52c25763a55cc642422644317-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 935423,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14726823655215444121&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept of Statistics, University of Washington; Dept of Statistical Science, Duke University",
        "aff_domain": "stat.washington.edu;stat.duke.edu",
        "email": "stat.washington.edu;stat.duke.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Washington;Duke University",
        "aff_unique_dep": "Department of Statistics;Dept of Statistical Science",
        "aff_unique_url": "https://www.washington.edu;https://www.duke.edu",
        "aff_unique_abbr": "UW;Duke",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "133f38594b",
        "title": "Multiresolution analysis on the symmetric group",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/86b122d4358357d834a87ce618a55de0-Abstract.html",
        "author": "Risi Kondor; Walter Dempsey",
        "abstract": "There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of O(n^p) complexity with small p for sparse signals (in contrast to the O(n^q n!) complexity typical of FFTs). We discuss potential applications in ranking, sparse approximation, and multi-object tracking.",
        "bibtex": "@inproceedings{NIPS2012_86b122d4,\n author = {Kondor, Risi and Dempsey, Walter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiresolution analysis on the symmetric group},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/86b122d4358357d834a87ce618a55de0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/86b122d4358357d834a87ce618a55de0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/86b122d4358357d834a87ce618a55de0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/86b122d4358357d834a87ce618a55de0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 221224,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6127868197201939294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics and Department of Computer Science, The University of Chicago; Department of Statistics and Department of Computer Science, The University of Chicago",
        "aff_domain": "uchicago.edu;uchicago.edu",
        "email": "uchicago.edu;uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Chicago",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.uchicago.edu",
        "aff_unique_abbr": "UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a2a7950d70",
        "title": "Natural Images, Gaussian Mixtures and Dead Leaves",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e97ee2054defb209c35fe4dc94599061-Abstract.html",
        "author": "Daniel Zoran; Yair Weiss",
        "abstract": "Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components --- including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models.",
        "bibtex": "@inproceedings{NIPS2012_e97ee205,\n author = {Zoran, Daniel and Weiss, Yair},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Natural Images, Gaussian Mixtures and Dead Leaves},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e97ee2054defb209c35fe4dc94599061-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2056575,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15834512633691196140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Interdisciplinary Center for Neural Computation, Hebrew University of Jerusalem, Israel; School of Computer Science and Engineering, Hebrew University of Jerusalem, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "Interdisciplinary Center for Neural Computation",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2418642848",
        "title": "Near-Optimal MAP Inference for Determinantal Point Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6c8dba7d0df1c4a79dd07646be9a26c8-Abstract.html",
        "author": "Jennifer Gillenwater; Alex Kulesza; Ben Taskar",
        "abstract": "Determinantal point processes (DPPs) have recently been proposed as   computationally efficient probabilistic models of diverse sets for a   variety of applications, including document summarization, image   search, and pose estimation.  Many DPP inference operations,   including normalization and sampling, are tractable; however,   finding the most likely configuration (MAP), which is often required   in practice for decoding, is NP-hard, so we must resort to   approximate inference.  Because DPP probabilities are   log-submodular, greedy algorithms have been used in the past with   some empirical success; however, these methods only give   approximation guarantees in the special case of DPPs with monotone   kernels.  In this paper we propose a new algorithm for approximating   the MAP problem based on continuous techniques for submodular   function maximization.  Our method involves a novel continuous   relaxation of the log-probability function, which, in contrast to   the multilinear extension used for general submodular functions, can   be evaluated and differentiated exactly and efficiently.  We obtain   a practical algorithm with a 1/4-approximation guarantee for a   general class of non-monotone DPPs.  Our algorithm also extends to   MAP inference under complex polytope constraints, making it possible   to combine DPPs with Markov random fields, weighted matchings, and   other models.  We demonstrate that our approach outperforms greedy   methods on both synthetic and real-world data.",
        "bibtex": "@inproceedings{NIPS2012_6c8dba7d,\n author = {Gillenwater, Jennifer and Kulesza, Alex and Taskar, Ben},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-Optimal MAP Inference for Determinantal Point Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/6c8dba7d0df1c4a79dd07646be9a26c8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6c8dba7d0df1c4a79dd07646be9a26c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 407217,
        "gs_citation": 157,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5030588453905143535&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a66ee82d7f",
        "title": "Near-optimal Differentially Private Principal Components",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f770b62bc8f42a0b66751fe636fc6eb0-Abstract.html",
        "author": "Kamalika Chaudhuri; Anand Sarwate; Kaushik Sinha",
        "abstract": "Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension.  Many current data sets of interest contain private or sensitive information about individuals.  Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs.  Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs.  In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output.  We demonstrate that on real data, there this a large performance gap between the existing methods and our method.  We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling.",
        "bibtex": "@inproceedings{NIPS2012_f770b62b,\n author = {Chaudhuri, Kamalika and Sarwate, Anand and Sinha, Kaushik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-optimal Differentially Private Principal Components},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/f770b62bc8f42a0b66751fe636fc6eb0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f770b62bc8f42a0b66751fe636fc6eb0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 497819,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1813779945159210915&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "UC San Diego; TTI-Chicago; UC San Diego",
        "aff_domain": "ucsd.edu;ttic.edu;cs.ucsd.edu",
        "email": "ucsd.edu;ttic.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UCSD;TTI",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "San Diego;Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3c4a1915a0",
        "title": "Neurally Plausible Reinforcement Learning of Working Memory Tasks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4daa3db355ef2b0e64b472968cb70f0d-Abstract.html",
        "author": "Jaldert Rombouts; Pieter Roelfsema; Sander M. Bohte",
        "abstract": "A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity. It is however not well known how these neurons acquire their task-relevant tuning. Here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error. We propose that the action selection stage feeds back attentional signals to earlier processing levels. These feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. A globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to (1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and (2) learn to optimally integrate probabilistic evidence for perceptual decision making.",
        "bibtex": "@inproceedings{NIPS2012_4daa3db3,\n author = {Rombouts, Jaldert and Roelfsema, Pieter and Bohte, Sander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neurally Plausible Reinforcement Learning of Working Memory Tasks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4daa3db355ef2b0e64b472968cb70f0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1406114,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13371462339860780181&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CWI, Life Sciences, Amsterdam, The Netherlands; CWI, Life Sciences, Amsterdam, The Netherlands; Netherlands Institute for Neuroscience, Amsterdam, The Netherlands",
        "aff_domain": "cwi.nl;cwi.nl;nin.knaw.nl",
        "email": "cwi.nl;cwi.nl;nin.knaw.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Centrum Wiskunde & Informatica;Netherlands Institute for Neuroscience",
        "aff_unique_dep": "Life Sciences;",
        "aff_unique_url": "https://www.cwi.nl;https://www.nin.nl",
        "aff_unique_abbr": "CWI;NIN",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amsterdam",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "1952c54046",
        "title": "Neuronal Spike Generation Mechanism as an Oversampling, Noise-shaping A-to-D converter",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/36660e59856b4de58a219bcf4e27eba3-Abstract.html",
        "author": "Dmitri B. Chklovskii; Daniel Soudry",
        "abstract": "We explore the hypothesis that the neuronal spike generation mechanism is an analog-to-digital converter, which rectifies low-pass filtered summed synaptic currents and encodes them into spike trains linearly decodable in post-synaptic neurons. To digitally encode an analog current waveform, the sampling rate of the spike generation mechanism must exceed its Nyquist rate. Such oversampling is consistent with the experimental observation that the precision of the spike-generation mechanism is an order of magnitude greater than the cut-off frequency of dendritic low-pass filtering. To achieve additional reduction in the error of analog-to-digital conversion, electrical engineers rely on noise-shaping. If noise-shaping were used in neurons, it would introduce correlations in spike timing to reduce low-frequency (up to Nyquist) transmission error at the cost of high-frequency one (from Nyquist to sampling rate). Using experimental data from three different classes of neurons, we demonstrate that biological neurons utilize noise-shaping. We also argue that rectification by the spike-generation mechanism may improve energy efficiency and carry out de-noising. Finally, the zoo of ion channels in neurons may be viewed as a set of predictors, various subsets of which are activated depending on the statistics of the input current.",
        "bibtex": "@inproceedings{NIPS2012_36660e59,\n author = {Chklovskii, Dmitri and Soudry, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neuronal Spike Generation Mechanism as an Oversampling, Noise-shaping A-to-D converter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/36660e59856b4de58a219bcf4e27eba3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/36660e59856b4de58a219bcf4e27eba3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/36660e59856b4de58a219bcf4e27eba3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 538027,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11203752306193465990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Janelia Farm Research Campus + Howard Hughes Medical Institute; Department of Electrical Engineering + Technion",
        "aff_domain": "janelia.hhmi.org;gmail.com",
        "email": "janelia.hhmi.org;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+3",
        "aff_unique_norm": "HHMI Janelia Research Campus;Howard Hughes Medical Institute;Institution not specified;Technion - Israel Institute of Technology",
        "aff_unique_dep": ";;Department of Electrical Engineering;",
        "aff_unique_url": "https://www.janelia.org;https://www.hhmi.org;;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Janelia;HHMI;;Technion",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Ashburn;",
        "aff_country_unique_index": "0+0;2",
        "aff_country_unique": "United States;;Israel"
    },
    {
        "id": "04b9b8392e",
        "title": "Newton-Like Methods for Sparse Inverse Covariance Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b3967a0e938dc2a6340e258630febd5a-Abstract.html",
        "author": "Figen Oztoprak; Jorge Nocedal; Steven Rennie; Peder A. Olsen",
        "abstract": "We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimation problem. Comparisons with the method implemented in the QUIC software package are presented.",
        "bibtex": "@inproceedings{NIPS2012_b3967a0e,\n author = {Oztoprak, Figen and Nocedal, Jorge and Rennie, Steven and Olsen, Peder A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Newton-Like Methods for Sparse Inverse Covariance Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b3967a0e938dc2a6340e258630febd5a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 324783,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14368325216883319780&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "IBM, T. J. Watson Research Center; Sabanci University; Northwestern University; IBM, T. J. Watson Research Center",
        "aff_domain": "us.ibm.com;sabanciuniv.edu;eecs.northwestern.edu;us.ibm.com",
        "email": "us.ibm.com;sabanciuniv.edu;eecs.northwestern.edu;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "IBM;Sabanci University;Northwestern University",
        "aff_unique_dep": "IBM;;",
        "aff_unique_url": "https://www.ibm.com;https://www.sabanciuniv.edu/;https://www.northwestern.edu",
        "aff_unique_abbr": "IBM;SU;NU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T. J. Watson Research Center;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;T\u00fcrkiye"
    },
    {
        "id": "7351bc264c",
        "title": "No-Regret Algorithms for Unconstrained Online Convex Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/38ca89564b2259401518960f7a06f94b-Abstract.html",
        "author": "Brendan Mcmahan; Matthew Streeter",
        "abstract": "Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is R^n.  Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x* are known in advance.  We present an algorithm that, without such prior knowledge, offers near-optimal regret bounds with respect to",
        "bibtex": "@inproceedings{NIPS2012_38ca8956,\n author = {Mcmahan, Brendan and Streeter, Matthew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {No-Regret Algorithms for Unconstrained Online Convex Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/38ca89564b2259401518960f7a06f94b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/38ca89564b2259401518960f7a06f94b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/38ca89564b2259401518960f7a06f94b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/38ca89564b2259401518960f7a06f94b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 267570,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=50355650063716826&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Duolingo, Inc.*; Google, Inc.",
        "aff_domain": "duolingo.com;google.com",
        "email": "duolingo.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Duolingo;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.duolingo.com;https://www.google.com",
        "aff_unique_abbr": "Duolingo;Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e37601d3f4",
        "title": "Non-linear Metric Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/43cca4b3de2097b9558efefd0ecc3588-Abstract.html",
        "author": "Dor Kedem; Stephen Tyree; Fei Sha; Gert R. Lanckriet; Kilian Q. Weinberger",
        "abstract": "In this paper, we introduce two novel metric learning algorithms, \u03c72-LMNN and GB-LMNN, which are explicitly designed to be non-linear and  easy-to-use. The two approaches achieve this goal in fundamentally different ways: \u03c72-LMNN inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear \u03c72-distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter. On various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of kNN classification error, but in the case of \u03c72-LMNN, obtain best results in 19 out of 20 learning settings.",
        "bibtex": "@inproceedings{NIPS2012_43cca4b3,\n author = {Kedem, Dor and Tyree, Stephen and Sha, Fei and Lanckriet, Gert and Weinberger, Kilian Q},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-linear Metric Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/43cca4b3de2097b9558efefd0ecc3588-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/43cca4b3de2097b9558efefd0ecc3588-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/43cca4b3de2097b9558efefd0ecc3588-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/43cca4b3de2097b9558efefd0ecc3588-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 615446,
        "gs_citation": 276,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1028869531208356855&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Comp. Sci. & Engi., Washington U., St. Louis, MO 63130; Dept. of Comp. Sci. & Engi., Washington U., St. Louis, MO 63130; Dept. of Comp. Sci. & Engi., Washington U., St. Louis, MO 63130; Dept. of Comp. Sci., U. of Southern California, Los Angeles, CA 90089; Dept. of Elec. & Comp. Engineering, U. of California, La Jolla, CA 92093",
        "aff_domain": "wustl.edu;wustl.edu;wustl.edu;usc.edu;ece.ucsd.edu",
        "email": "wustl.edu;wustl.edu;wustl.edu;usc.edu;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Washington University in St. Louis;University of Southern California;University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science & Engineering;Department of Computer Science;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://wustl.edu;https://www.usc.edu;https://ucsd.edu",
        "aff_unique_abbr": "WashU;USC;UCSD",
        "aff_campus_unique_index": "0;0;0;1;2",
        "aff_campus_unique": "St. Louis;Los Angeles;La Jolla",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f312fcffe5",
        "title": "Non-parametric Approximate Dynamic Programming via the Kernel Method",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/84d9ee44e457ddef7f2c4f25dc8fa865-Abstract.html",
        "author": "Nikhil Bhat; Vivek Farias; Ciamac C. Moallemi",
        "abstract": "This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful, dimension-independent approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our non-parametric procedure is competitive with parametric ADP approaches.",
        "bibtex": "@inproceedings{NIPS2012_84d9ee44,\n author = {Bhat, Nikhil and Farias, Vivek and Moallemi, Ciamac C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-parametric Approximate Dynamic Programming via the Kernel Method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/84d9ee44e457ddef7f2c4f25dc8fa865-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/84d9ee44e457ddef7f2c4f25dc8fa865-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/84d9ee44e457ddef7f2c4f25dc8fa865-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/84d9ee44e457ddef7f2c4f25dc8fa865-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 310759,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7462649743639755196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Graduate School of Business, Columbia University; Sloan School of Management, Massachusetts Institute of Technology; Graduate School of Business, Columbia University",
        "aff_domain": "gsb.columbai.edu;mit.edu;gsb.columbai.edu",
        "email": "gsb.columbai.edu;mit.edu;gsb.columbai.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Columbia University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Graduate School of Business;Sloan School of Management",
        "aff_unique_url": "https://www.columbia.edu;https://mitsloan.mit.edu/",
        "aff_unique_abbr": "Columbia;MIT",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "New York;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7386303991",
        "title": "Nonconvex Penalization Using Laplace Exponents and Concave Conjugates",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/191c62d342811d1a0d3d0528ec35cd2d-Abstract.html",
        "author": "Zhihua Zhang; Bojun Tu",
        "abstract": "In this paper we study sparsity-inducing nonconvex penalty functions using L\u00b4evy processes. We de\ufb01ne such a penalty as the Laplace exponent of a subordina- tor. Accordingly, we propose a novel approach for the construction of sparsity- inducing nonconvex penalties. Particularly, we show that the nonconvex logarith- mic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionally, we explore the concave conjugate of nonconvex penalties. We \ufb01nd that the LOG and EXP penalties are the concave conjugates of negative Kullback-Leiber (KL) dis- tance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance.",
        "bibtex": "@inproceedings{NIPS2012_191c62d3,\n author = {Zhang, Zhihua and Tu, Bojun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonconvex Penalization Using Laplace Exponents and Concave Conjugates},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/191c62d342811d1a0d3d0528ec35cd2d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/191c62d342811d1a0d3d0528ec35cd2d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/191c62d342811d1a0d3d0528ec35cd2d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 125519,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7394004130449820481&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "College of Computer Science & Technology, Zhejiang University; College of Computer Science & Technology, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Computer Science & Technology",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "9fb2b51a35",
        "title": "Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html",
        "author": "Jaedeug Choi; Kee-eung Kim",
        "abstract": "We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to be met in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efficient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains.",
        "bibtex": "@inproceedings{NIPS2012_140f6969,\n author = {Choi, Jaedeug and Kim, Kee-eung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/140f6969d5213fd0ece03148e62e461e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/140f6969d5213fd0ece03148e62e461e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 183928,
        "gs_citation": 192,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11125389771260527334&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Korea Advanced Institute of Science and Technology; Department of Computer Science, Korea Advanced Institute of Science and Technology",
        "aff_domain": "ai.kaist.ac.kr;cs.kaist.ac.kr",
        "email": "ai.kaist.ac.kr;cs.kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "6f0478f276",
        "title": "Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c78c347465f4775425c059ea101c131f-Abstract.html",
        "author": "Minjie Xu; Jun Zhu; Bo Zhang",
        "abstract": "We present a probabilistic formulation of max-margin matrix factorization and build accordingly a nonparametric Bayesian model which automatically resolves the unknown number of latent factors. Our work demonstrates a successful example that integrates Bayesian nonparametrics and max-margin learning, which are conventionally two separate paradigms and enjoy complementary advantages. We develop an efcient variational algorithm for posterior inference, and our extensive empirical studies on large-scale MovieLens and EachMovie data sets appear to justify the aforementioned dual advantages.",
        "bibtex": "@inproceedings{NIPS2012_c78c3474,\n author = {Xu, Minjie and Zhu, Jun and Zhang, Bo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c78c347465f4775425c059ea101c131f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c78c347465f4775425c059ea101c131f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c78c347465f4775425c059ea101c131f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c78c347465f4775425c059ea101c131f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 403871,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=149488039601861965&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "State Key Laboratory of Intelligent Technology and Systems (LITS) + Tsinghua National Laboratory for Information Science and Technology (TNList) + Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; State Key Laboratory of Intelligent Technology and Systems (LITS) + Tsinghua National Laboratory for Information Science and Technology (TNList) + Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; State Key Laboratory of Intelligent Technology and Systems (LITS) + Tsinghua National Laboratory for Information Science and Technology (TNList) + Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China",
        "aff_domain": "gmail.com;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "gmail.com;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+1;0+1+1;0+1+1",
        "aff_unique_norm": "State Key Laboratory of Intelligent Technology and Systems;Tsinghua University",
        "aff_unique_dep": "Intelligent Technology and Systems;National Laboratory for Information Science and Technology",
        "aff_unique_url": ";http://www.tnlist.org/",
        "aff_unique_abbr": "LITS;TNList",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "65a0207340",
        "title": "Nonparametric Reduced Rank Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f2201f5191c4e92cc5af043eebfd0946-Abstract.html",
        "author": "Rina Foygel; Michael Horrell; Mathias Drton; John D. Lafferty",
        "abstract": "We propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models.  An additive model is estimated for each dimension of a $q$-dimensional response, with a shared $p$-dimensional predictor variable.  To control the complexity of the model, we employ a functional form of the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low rank.  Backfitting algorithms are derived and justified using a nonparametric form of the nuclear norm subdifferential.  Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting.  The methods are illustrated on gene expression data.",
        "bibtex": "@inproceedings{NIPS2012_f2201f51,\n author = {Foygel, Rina and Horrell, Michael and Drton, Mathias and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Reduced Rank Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f2201f5191c4e92cc5af043eebfd0946-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4221644,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13818462776601068473&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d8d67c8a8c",
        "title": "Nonparanormal Belief Propagation (NPNBP)",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/41ae36ecb9b3eee609d05b90c14222fb-Abstract.html",
        "author": "Gal Elidan; Cobi Cario",
        "abstract": "The empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances. Yet, for continuous non-Gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models.  In this work we present Nonparanormal BP  for performing efficient inference on distributions parameterized by  a Gaussian copulas network and any univariate marginals. For  tree structured networks, our approach is guaranteed to be exact for  this powerful class of non-Gaussian models. Importantly, the method  is as efficient as standard Gaussian BP, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used.",
        "bibtex": "@inproceedings{NIPS2012_41ae36ec,\n author = {Elidan, Gal and Cario, Cobi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparanormal Belief Propagation (NPNBP)},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/41ae36ecb9b3eee609d05b90c14222fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2121062,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14122000485965986444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistics, Hebrew University; School of Computer Science and Engineering, Hebrew University",
        "aff_domain": "huji.ac.il;mail.huji.ac.il",
        "email": "huji.ac.il;mail.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "cb66c204f2",
        "title": "Nystr\u00f6m Method vs Random Fourier Features: A Theoretical and Empirical Comparison",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html",
        "author": "Tianbao Yang; Yu-feng Li; Mehrdad Mahdavi; Rong Jin; Zhi-Hua Zhou",
        "abstract": "Both random Fourier features and the Nystr\u00f6m method have been successfully applied to efficient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features  where the basis functions (i.e., cosine and sine functions) are sampled from a distribution  {\\it independent} from the training data, basis functions used by the Nystr\u00f6m method are randomly sampled from the training examples and are therefore {\\it data dependent}. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based the Nystr\u00f6m method can yield  impressively  better generalization error bound than random Fourier features based approach. We empirically verify our theoretical findings on a wide range of large data sets.",
        "bibtex": "@inproceedings{NIPS2012_621bf66d,\n author = {Yang, Tianbao and Li, Yu-feng and Mahdavi, Mehrdad and Jin, Rong and Zhou, Zhi-Hua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nystr\\\"{o}m Method vs Random Fourier Features: A Theoretical and Empirical Comparison},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/621bf66ddb7c962aa0d22ac97d69b793-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1227295,
        "gs_citation": 458,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18146089413183711027&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Machine Learning Lab, GE Global Research, San Ramon, CA 94583; National Key Laboratory for Novel Software Technology, Nanjing University, 210023, China; Michigan State University, East Lansing, MI 48824 + National Key Laboratory for Novel Software Technology, Nanjing University, 210023, China; Michigan State University, East Lansing, MI 48824; National Key Laboratory for Novel Software Technology, Nanjing University, 210023, China",
        "aff_domain": "ge.com;lamda.nju.edu.cn;msu.edu;msu.edu;lamda.nju.edu.cn",
        "email": "ge.com;lamda.nju.edu.cn;msu.edu;msu.edu;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+1;2;1",
        "aff_unique_norm": "GE Global Research;Nanjing University;Michigan State University",
        "aff_unique_dep": "Machine Learning Lab;National Key Laboratory for Novel Software Technology;",
        "aff_unique_url": "https://www.ge.com/research;http://www.nju.edu.cn;https://www.msu.edu",
        "aff_unique_abbr": "GE Global Research;Nanjing U;MSU",
        "aff_campus_unique_index": "0;2;2",
        "aff_campus_unique": "San Ramon;;East Lansing",
        "aff_country_unique_index": "0;1;0+1;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "8250e652be",
        "title": "On Lifting the Gibbs Sampling Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/fc8001f834f6a5f0561080d134d53d29-Abstract.html",
        "author": "Deepak Venugopal; Vibhav Gogate",
        "abstract": "Statistical relational learning models combine the power of first-order logic, the de facto tool for handling relational structure, with that of probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the speed, accuracy and scalability of existing graphical models' inference algorithms by exploiting symmetry in the first-order representation. In this paper, we consider blocked Gibbs sampling, an advanced variation of the classic Gibbs sampling algorithm and lift it to the first-order level. We propose to achieve this by partitioning the first-order atoms in the relational model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing such clusters and determining their complexity and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy and convergence.",
        "bibtex": "@inproceedings{NIPS2012_fc8001f8,\n author = {Venugopal, Deepak and Gogate, Vibhav},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Lifting the Gibbs Sampling Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/fc8001f834f6a5f0561080d134d53d29-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 140239,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14014181708868061341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, The University of Texas at Dallas, Richardson, TX, 75080, USA; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, 75080, USA",
        "aff_domain": "utdallas.edu;hlt.utdallas.edu",
        "email": "utdallas.edu;hlt.utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Dallas",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utdallas.edu",
        "aff_unique_abbr": "UT Dallas",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Richardson",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "37c25a9217",
        "title": "On Multilabel Classification and Ranking with Partial Feedback",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/11b921ef080f7736089c757404650e40-Abstract.html",
        "author": "Claudio Gentile; Francesco Orabona",
        "abstract": "We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation.  We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show $O(T^{1/2}\\log T)$ regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance.",
        "bibtex": "@inproceedings{NIPS2012_11b921ef,\n author = {Gentile, Claudio and Orabona, Francesco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Multilabel Classification and Ranking with Partial Feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/11b921ef080f7736089c757404650e40-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/11b921ef080f7736089c757404650e40-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/11b921ef080f7736089c757404650e40-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/11b921ef080f7736089c757404650e40-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 460035,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1452180948439513956&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "DiSTA, Universit `a dell\u2019Insubria, Italy; TTI Chicago, USA",
        "aff_domain": "uninsubria.it;orabona.com",
        "email": "uninsubria.it;orabona.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universit\u00e0 dell\u2019Insubria;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "DiSTA;",
        "aff_unique_url": "https://www.uninsubria.it;https://tti-chicago.org",
        "aff_unique_abbr": ";TTI Chicago",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Italy;United States"
    },
    {
        "id": "df01f98bae",
        "title": "On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7cce53cf90577442771720a370c3c723-Abstract.html",
        "author": "Qirong Ho; Junming Yin; Eric P. Xing",
        "abstract": "In this paper, we argue for representing networks as a bag of {\\it triangular motifs}, particularly for important network problems that current model-based approaches handle poorly due to computational bottlenecks incurred by using edge representations. Such approaches require both 1-edges and 0-edges (missing edges) to be provided as input, and as a consequence, approximate inference algorithms for these models usually require $\\Omega(N^2)$ time per iteration, precluding their application to larger real-world networks. In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality. A triangular motif is a vertex triple containing 2 or 3 edges, and the number of such motifs is $\\Theta(\\sum_{i}D_{i}^{2})$ (where $D_i$ is the degree of vertex $i$), which is much smaller than $N^2$ for low-maximum-degree networks. Using this representation, we develop a novel mixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networks with high maximum degree, the triangular motifs can be naturally subsampled in a {\\it node-centric} fashion, allowing for much faster inference at a small cost in accuracy. Empirically, we demonstrate that our approach, when compared to that of an edge-based model, has faster runtime and improved accuracy for mixed-membership community detection. We conclude with a large-scale demonstration on an $N\\approx 280,000$-node network, which is infeasible for network models with $\\Omega(N^2)$ inference cost.",
        "bibtex": "@inproceedings{NIPS2012_7cce53cf,\n author = {Ho, Qirong and Yin, Junming and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7cce53cf90577442771720a370c3c723-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7cce53cf90577442771720a370c3c723-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/7cce53cf90577442771720a370c3c723-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7cce53cf90577442771720a370c3c723-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1182016,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17862477704949869930&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f96c3957c0",
        "title": "On the (Non-)existence of Convex, Calibrated Surrogate Losses for Ranking",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/50f3f8c42b998a48057e9d33f4144b8b-Abstract.html",
        "author": "Cl\u00e9ment Calauz\u00e8nes; Nicolas Usunier; Patrick Gallinari",
        "abstract": "We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the sur- rogate risk imply near-optimal values of the risk de\ufb01ned by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pair- wise preferences. Our results cast lights on the intrinsic dif\ufb01culty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk.",
        "bibtex": "@inproceedings{NIPS2012_50f3f8c4,\n author = {Calauz\\`{e}nes, Cl\\'{e}ment and Usunier, Nicolas and Gallinari, Patrick},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the (Non-)existence of Convex, Calibrated Surrogate Losses for Ranking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/50f3f8c42b998a48057e9d33f4144b8b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/50f3f8c42b998a48057e9d33f4144b8b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/50f3f8c42b998a48057e9d33f4144b8b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 366979,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16232419394841565625&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8c3951dcd6",
        "title": "On the Sample Complexity of Robust PCA",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/78b9cab19959e4af8ff46156ee460c74-Abstract.html",
        "author": "Matthew Coudron; Gilad Lerman",
        "abstract": "We estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix. This estimator is used in a convex algorithm for robust subspace recovery (i.e., robust PCA). Our model assumes a sub-Gaussian underlying distribution and an i.i.d.~sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $N$ is of order $O(N^{-0.5+\\eps})$ for arbitrarily small $\\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation, i.e., $O(N^{-0.5})$. Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is $O(D^{2+\\delta})$ for arbitrarily small $\\delta>0$ (whereas the sample complexity of direct covariance estimation with Frobenius norm is $O(D^{2})$). These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm, which are close to those of PCA. To the best of our knowledge, this is the only work analyzing the sample complexity of any robust PCA algorithm.",
        "bibtex": "@inproceedings{NIPS2012_78b9cab1,\n author = {Coudron, Matthew and Lerman, Gilad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Sample Complexity of Robust PCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/78b9cab19959e4af8ff46156ee460c74-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/78b9cab19959e4af8ff46156ee460c74-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/78b9cab19959e4af8ff46156ee460c74-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 285123,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15029694540286373500&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology; School of Mathematics, University of Minnesota",
        "aff_domain": "mit.edu;umn.edu",
        "email": "mit.edu;umn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Minnesota",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;School of Mathematics",
        "aff_unique_url": "https://web.mit.edu;https://www.math.umn.edu",
        "aff_unique_abbr": "MIT;UMN",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Minneapolis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ae4754bfad",
        "title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8b6dd7db9af49e67306feb59a8bdc52c-Abstract.html",
        "author": "Bruno Scherrer; Boris Lesner",
        "abstract": "We consider infinite-horizon stationary $\\gamma$-discounted Markov   Decision Processes, for which it is known that there exists a   stationary optimal policy. Using Value and Policy Iteration with   some error $\\epsilon$ at each iteration, it is well-known that one   can compute stationary policies that are $\\frac{2\\gamma{(1-\\gamma)^2}\\epsilon$-optimal. After arguing that this   guarantee is tight, we develop variations of Value and Policy   Iteration for computing non-stationary policies that can be up to   $\\frac{2\\gamma}{1-\\gamma}\\epsilon$-optimal, which constitutes a significant   improvement in the usual situation when $\\gamma$ is close to   $1$. Surprisingly, this shows that the problem of ``computing near-optimal non-stationary policies'' is much simpler than that   of ``computing near-optimal stationary policies''.",
        "bibtex": "@inproceedings{NIPS2012_8b6dd7db,\n author = {Scherrer, Bruno and Lesner, Boris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8b6dd7db9af49e67306feb59a8bdc52c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 264500,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11204358216800039854&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Inria, Villers-l `es-Nancy, F-54600, France; Inria, Villers-l `es-Nancy, F-54600, France",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Villers-l\u00e8s-Nancy",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "e8af2904c4",
        "title": "On the connections between saliency and tracking",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2dace78f80bc92e6d7493423d729448e-Abstract.html",
        "author": "Vijay Mahadevan; Nuno Vasconcelos",
        "abstract": "A model connecting visual tracking and saliency has recently been proposed. This model is based on the saliency hypothesis for tracking which postulates that tracking is achieved by the top-down tuning, based on target features, of discriminant center-surround saliency mechanisms over time. In this work, we identify three main predictions that must hold if the hypothesis were true: 1) tracking reliability should be larger for salient than for non-salient targets, 2) tracking reliability should have a dependence on the defining variables of saliency, namely feature contrast and distractor heterogeneity, and must replicate the dependence of saliency on these variables, and 3) saliency and tracking can be implemented with common low level neural mechanisms. We confirm that the first two predictions hold by reporting results from a set of human behavior studies on the connection between saliency and tracking. We also show that the third prediction holds by constructing a common neurophysiologically plausible architecture that can computationally solve both saliency and tracking. This architecture is fully compliant with the standard physiological models of V1 and MT, and with what is known about attentional control in area LIP, while explaining the results of the human behavior experiments.",
        "bibtex": "@inproceedings{NIPS2012_2dace78f,\n author = {Mahadevan, Vijay and Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the connections between saliency and tracking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/2dace78f80bc92e6d7493423d729448e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2dace78f80bc92e6d7493423d729448e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 261590,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4301812786767339832&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Yahoo! Labs, Bangalore, India; Statistical Visual Computing Laboratory, UC San Diego, La Jolla, CA 92092",
        "aff_domain": "yahoo-inc.com;ece.ucsd.edu",
        "email": "yahoo-inc.com;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yahoo!;University of California, San Diego",
        "aff_unique_dep": "Yahoo! Labs;Statistical Visual Computing Laboratory",
        "aff_unique_url": "https://labs.yahoo.com;https://ucsd.edu",
        "aff_unique_abbr": "Yahoo! Labs;UCSD",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Bangalore;La Jolla",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "9c4fb19eae",
        "title": "On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1ecfb463472ec9115b10c292ef8bc986-Abstract.html",
        "author": "Andre Barreto; Doina Precup; Joelle Pineau",
        "abstract": "The ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge. This paper presents a new reinforcement-learning algorithm, called iKBSF, which extends the benefits of kernel-based learning to the on-line scenario. As a kernel-based method, the proposed algorithm is stable and has good convergence properties. However, unlike other similar algorithms,iKBSF's space complexity is independent of the number of sample transitions, and as a result it can process an arbitrary amount of data. We present theoretical results showing that iKBSF can approximate (to any level of accuracy) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator. In order to show the effectiveness of the proposed algorithm in practice, we apply iKBSF to the challenging three-pole balancing task, where the ability to process a large number of transitions is crucial for achieving a high success rate.",
        "bibtex": "@inproceedings{NIPS2012_1ecfb463,\n author = {Barreto, Andre and Precup, Doina and Pineau, Joelle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1ecfb463472ec9115b10c292ef8bc986-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1ecfb463472ec9115b10c292ef8bc986-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/1ecfb463472ec9115b10c292ef8bc986-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1ecfb463472ec9115b10c292ef8bc986-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 134044,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18168441591409949823&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "c92d7dcdae",
        "title": "One Permutation Hashing",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/eaa32c96f620053cf442ad32258076b9-Abstract.html",
        "author": "Ping Li; Art Owen; Cun-hui Zhang",
        "abstract": "While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.  In this paper, we develop a simple \\textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing $k$ permutations to just one  would be much more \\textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \\& logistic regression also confirm the theoretical results.",
        "bibtex": "@inproceedings{NIPS2012_eaa32c96,\n author = {Li, Ping and Owen, Art and Zhang, Cun-hui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {One Permutation Hashing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/eaa32c96f620053cf442ad32258076b9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/eaa32c96f620053cf442ad32258076b9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/eaa32c96f620053cf442ad32258076b9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 346338,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5819268922080061842&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b0249e03a9",
        "title": "Online L1-Dictionary Learning with Application to Novel Document Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/dd77279f7d325eec933f05b1672f6a1f-Abstract.html",
        "author": "Shiva P. Kasiviswanathan; Huahua Wang; Arindam Banerjee; Prem Melville",
        "abstract": "Given their pervasive use, social media, such as Twitter, have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge, we introduce the problem of online L1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the L1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data, shows that this online L1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results. Our algorithm for online L1-dictionary learning could be of independent interest.",
        "bibtex": "@inproceedings{NIPS2012_dd77279f,\n author = {Kasiviswanathan, Shiva and Wang, Huahua and Banerjee, Arindam and Melville, Prem},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online L1-Dictionary Learning with Application to Novel Document Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/dd77279f7d325eec933f05b1672f6a1f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/dd77279f7d325eec933f05b1672f6a1f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/dd77279f7d325eec933f05b1672f6a1f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/dd77279f7d325eec933f05b1672f6a1f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 371662,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11337683307893087456&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "General Electric Global Research; University of Minnesota; University of Minnesota; IBM T.J. Watson Research Center",
        "aff_domain": "gmail.com;cs.umn.edu;cs.umn.edu;us.ibm.com",
        "email": "gmail.com;cs.umn.edu;cs.umn.edu;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "General Electric;University of Minnesota;IBM",
        "aff_unique_dep": "Global Research;;Research Center",
        "aff_unique_url": "https://www.ge.com/research;https://www.minnesota.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "GE;UMN;IBM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";T.J. Watson",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7af1b27785",
        "title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f9a40a4780f5e1306c46f1c8daecee3b-Abstract.html",
        "author": "Ronald Ortner; Daniil Ryabko",
        "abstract": "We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty.  Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Hoelder continuity of rewards and transition probabilities.",
        "bibtex": "@inproceedings{NIPS2012_f9a40a47,\n author = {Ortner, Ronald and Ryabko, Daniil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Regret Bounds for Undiscounted Continuous Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f9a40a4780f5e1306c46f1c8daecee3b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 178096,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14271315532189502702&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Montanuniversitaet Leoben; INRIA Lille-Nord Europe, \u00b4equipe SequeL",
        "aff_domain": "unileoben.ac.at;ryabko.net",
        "email": "unileoben.ac.at;ryabko.net",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Montanuniversitaet Leoben;INRIA Lille-Nord Europe",
        "aff_unique_dep": ";\u00b4equipe SequeL",
        "aff_unique_url": "https://www.montanuni-leoben.at;https://www.inria.fr",
        "aff_unique_abbr": "MUL;INRIA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Austria;France"
    },
    {
        "id": "d8914fcdcd",
        "title": "Online Sum-Product Computation Over Trees",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e2c4a40d50b47094f571e40efead3900-Abstract.html",
        "author": "Mark Herbster; Stephen Pasteris; Fabio Vitale",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS2012_e2c4a40d,\n author = {Herbster, Mark and Pasteris, Stephen and Vitale, Fabio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Sum-Product Computation Over Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e2c4a40d50b47094f571e40efead3900-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e2c4a40d50b47094f571e40efead3900-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/e2c4a40d50b47094f571e40efead3900-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e2c4a40d50b47094f571e40efead3900-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 433982,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10891497478129972693&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University College London, London W12 3QT, England, UK; Department of Computer Science, University College London, London W12 3QT, England, UK; Department of Computer Science, University of Milan, 20122 Milan, Italy",
        "aff_domain": "csw.ac.uk;csw.ac.uk;unimiwit",
        "email": "csw.ac.uk;csw.ac.uk;unimiwit",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University College London;University of Milan",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.unimi.it",
        "aff_unique_abbr": "UCL;UniMi",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "London;Milan",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United Kingdom;Italy"
    },
    {
        "id": "17f5dc2fbc",
        "title": "Online allocation and homogeneous partitioning for piecewise constant mean-approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/eeb69a3cb92300456b6a5f4162093851-Abstract.html",
        "author": "Alexandra Carpentier; Odalric-ambrym Maillard",
        "abstract": "In the setting of active learning for the multi-armed bandit, where the goal of a learner is to estimate with equal precision the mean of a finite number of arms, recent results show that it is possible to derive strategies based on finite-time confidence bounds that are competitive with the best possible strategy. We here consider an extension of this problem to the case when the arms are the cells of a finite partition P of a continuous sampling space X \\subset \\Real^d. Our goal is now to build a piecewise constant approximation of a noisy function (where each piece is one region of P and P is fixed beforehand) in order to maintain the local quadratic error of approximation on each cell equally low. Although this extension is not trivial, we show that a simple algorithm based on upper confidence bounds can be proved to be adaptive to the function itself in a near-optimal way, when |P| is chosen to be of minimax-optimal order  on the class of \\alpha-H\u00f6lder functions.",
        "bibtex": "@inproceedings{NIPS2012_eeb69a3c,\n author = {Carpentier, Alexandra and Maillard, Odalric-ambrym},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online allocation and homogeneous partitioning for piecewise constant mean-approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/eeb69a3cb92300456b6a5f4162093851-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/eeb69a3cb92300456b6a5f4162093851-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 291215,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11934037337302087457&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Montanuniversit \u00a8atLeoben; StatisticalLaboratory,CMS",
        "aff_domain": "gmail.com;statslab.cam.ac.uk",
        "email": "gmail.com;statslab.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Montanuniversit\u00e4t Leoben;CMS",
        "aff_unique_dep": ";Statistical Laboratory",
        "aff_unique_url": "https://www.montanuni-leoben.at;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Austria;"
    },
    {
        "id": "1e50731891",
        "title": "Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L_p$ Loss",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html",
        "author": "Zhuo Wang; Alan Stocker; Daniel D Lee",
        "abstract": "In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general $L_p$ norm.  We generalize the Cramer-Rao lower bound and show how the $L_p$ loss can be written as a functional of the Fisher Information in the asymptotic limit, by proving the moment convergence of certain functions of Poisson random variables.  In this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing $L_p$ loss in the limit as $p$ goes to zero.",
        "bibtex": "@inproceedings{NIPS2012_062ddb6c,\n author = {Wang, Zhuo and Stocker, Alan A and Lee, Daniel D},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum L\\_p Loss},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/062ddb6c727310e76b6200b7c71f63b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 288284,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1740039098109252615&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Mathematics, University of Pennsylvania; Department of Psychology, University of Pennsylvania; Department of Electrical and Systems Engineering, University of Pennsylvania",
        "aff_domain": "sas.upenn.edu;sas.upenn.edu;seas.upenn.edu",
        "email": "sas.upenn.edu;sas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d9d2b8d0d5",
        "title": "Optimal Regularized Dual Averaging Methods for Stochastic Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/274ad4786c3abca69fa097b85867d9a4-Abstract.html",
        "author": "Xi Chen; Qihang Lin; Javier Pena",
        "abstract": "This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth.  We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the optimal  rate of $O(\\frac{1}{N}+\\frac{1}{N^2})$ for $N$  iterations, which improves the best known rate $O(\\frac{\\log N }{N})$ of previous stochastic dual averaging algorithms. In addition, our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., $\\ell_1$-norm), it has the advantage of encouraging sparser solutions. We further develop a multi-stage extension using the proposed algorithm as a subroutine, which achieves the uniformly-optimal rate $O(\\frac{1}{N}+\\exp\\{-N\\})$ for strongly convex loss.",
        "bibtex": "@inproceedings{NIPS2012_274ad478,\n author = {Chen, Xi and Lin, Qihang and Pena, Javier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Regularized Dual Averaging Methods for Stochastic Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/274ad4786c3abca69fa097b85867d9a4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/274ad4786c3abca69fa097b85867d9a4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/274ad4786c3abca69fa097b85867d9a4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/274ad4786c3abca69fa097b85867d9a4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 350410,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7932640729308229902&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Machine Learning Department, Carnegie Mellon University; Tepper School of Business, Carnegie Mellon University; Tepper School of Business, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "cs.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3f1f12d0fb",
        "title": "Optimal kernel choice for large-scale two-sample tests",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/dbe272bab69f8e13f14b405e038deb64-Abstract.html",
        "author": "Arthur Gretton; Dino Sejdinovic; Heiko Strathmann; Sivaraman Balakrishnan; Massimiliano Pontil; Kenji Fukumizu; Bharath K. Sriperumbudur",
        "abstract": "Abstract Given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.",
        "bibtex": "@inproceedings{NIPS2012_dbe272ba,\n author = {Gretton, Arthur and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Fukumizu, Kenji and Sriperumbudur, Bharath K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal kernel choice for large-scale two-sample tests},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/dbe272bab69f8e13f14b405e038deb64-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/dbe272bab69f8e13f14b405e038deb64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 919759,
        "gs_citation": 885,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8862676228731505220&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Gatsby Unit and CSD, CSML, UCL, UK + MPI for Intelligent Systems, Germany; Gatsby Unit and CSD, CSML, UCL, UK; Gatsby Unit and CSD, CSML, UCL, UK; Gatsby Unit and CSD, CSML, UCL, UK; LTI, CMU, USA; CSD, CSML, UCL, UK; ISM, Japan",
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;cs.cmu.edu;cs.ucl.ac.uk;ism.ac.jp",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;cs.cmu.edu;cs.ucl.ac.uk;ism.ac.jp",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0;2;0;3",
        "aff_unique_norm": "University College London;Max Planck Institute for Intelligent Systems;Carnegie Mellon University;ISM",
        "aff_unique_dep": "Gatsby Unit and Centre for Speech and Language Processing;;Language Technologies Institute;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.mpi-is.mpg.de;https://www.cmu.edu;",
        "aff_unique_abbr": "UCL;MPI-IS;CMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;0;2;0;3",
        "aff_country_unique": "United Kingdom;Germany;United States;Japan"
    },
    {
        "id": "8d1c9cf460",
        "title": "Parametric Local Metric Learning for Nearest Neighbor Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/470e7a4f017a5476afb7eeb3f8b96f9b-Abstract.html",
        "author": "Jun Wang; Alexandros Kalousis; Adam Woznica",
        "abstract": "We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this ''independence'' approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space.  We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several large-scale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner.",
        "bibtex": "@inproceedings{NIPS2012_470e7a4f,\n author = {Wang, Jun and Kalousis, Alexandros and Woznica, Adam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parametric Local Metric Learning for Nearest Neighbor Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/470e7a4f017a5476afb7eeb3f8b96f9b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2877967,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4713919039853288237&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Geneva, Switzerland; Department of Computer Science, University of Geneva, Switzerland; Department of Business Informatics, University of Applied Sciences, Western Switzerland",
        "aff_domain": "unige.ch;unige.ch;hesge.ch",
        "email": "unige.ch;unige.ch;hesge.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Geneva;University of Applied Sciences Western Switzerland",
        "aff_unique_dep": "Department of Computer Science;Department of Business Informatics",
        "aff_unique_url": "https://www.unige.ch;https://www.hes-so.ch/en",
        "aff_unique_abbr": "UNIGE;HES-SO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "99d68f85c1",
        "title": "Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3cec07e9ba5f5bb252d13f5f431e4bbb-Abstract.html",
        "author": "Jenna Wiens; Eric Horvitz; John V. Guttag",
        "abstract": "A patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state. We explore representing patient risk as a time series. In doing so, patient risk stratification becomes a time-series classification task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted. Thus, we begin by defining and extracting approximate \\textit{risk processes}, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns. We apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \\textit{Clostridium Difficile}. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05).",
        "bibtex": "@inproceedings{NIPS2012_3cec07e9,\n author = {Wiens, Jenna and Horvitz, Eric and Guttag, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 512031,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16781172320030264729&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "MIT; MIT; Microsoft",
        "aff_domain": "mit.edu;mit.edu;microsoft.com",
        "email": "mit.edu;mit.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://web.mit.edu;https://www.microsoft.com",
        "aff_unique_abbr": "MIT;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dcac7d8987",
        "title": "Perceptron Learning of SAT",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html",
        "author": "Alex Flint; Matthew Blaschko",
        "abstract": "Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science.  In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem.  In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space.  Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm.  Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates.  We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT.  Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task.",
        "bibtex": "@inproceedings{NIPS2012_fb60d411,\n author = {Flint, Alex and Blaschko, Matthew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Perceptron Learning of SAT},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 334724,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11480941502891827685&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Engineering Science, University of Oxford; Center for Visual Computing, Ecole Centrale Paris",
        "aff_domain": "robots.ox.ac.uk;inria.fr",
        "email": "robots.ox.ac.uk;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Oxford;Ecole Centrale Paris",
        "aff_unique_dep": "Department of Engineering Science;Center for Visual Computing",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.ecp.fr",
        "aff_unique_abbr": "Oxford;ECP",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Oxford;Paris",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "id": "13800bdbad",
        "title": "Perfect Dimensionality Recovery by Variational Bayesian PCA",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/26337353b7962f533d78c762373b3318-Abstract.html",
        "author": "Shinichi Nakajima; Ryota Tomioka; Masashi Sugiyama; S. D. Babacan",
        "abstract": "The variational Bayesian (VB) approach is one of the best tractable approximations to the Bayesian estimation, and it was demonstrated to perform well in many applications. However, its good performance was not fully understood theoretically. For example, VB sometimes produces a sparse solution, which is regarded as a practical advantage of VB, but such sparsity is hardly observed in the rigorous Bayesian estimation. In this paper, we focus on probabilistic PCA and give more theoretical insight into the empirical success of VB. More specifically, for the situation where the noise variance is unknown, we derive a sufficient condition for perfect recovery of the true PCA dimensionality in the large-scale limit when the size of an observed matrix goes to infinity. In our analysis, we obtain bounds for a noise variance estimator and simple closed-form solutions for other parameters, which themselves are actually very useful for better implementation of VB-PCA.",
        "bibtex": "@inproceedings{NIPS2012_26337353,\n author = {Nakajima, Shinichi and Tomioka, Ryota and Sugiyama, Masashi and Babacan, S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Perfect Dimensionality Recovery by Variational Bayesian PCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/26337353b7962f533d78c762373b3318-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/26337353b7962f533d78c762373b3318-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/26337353b7962f533d78c762373b3318-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 323604,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1736652212280063377&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Nikon Corporation, Tokyo, 140-8601, Japan; The University of Tokyo, Tokyo 113-8685, Japan; Tokyo Institute of Technology, Tokyo 152-8552, Japan; University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA",
        "aff_domain": "nikon.co.jp;mist.i.u-tokyo.ac.jp;cs.titech.ac.jp;illinois.edu",
        "email": "nikon.co.jp;mist.i.u-tokyo.ac.jp;cs.titech.ac.jp;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Nikon Corporation;University of Tokyo;Tokyo Institute of Technology;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.nikon.com;https://www.u-tokyo.ac.jp;https://www.titech.ac.jp;https://illinois.edu",
        "aff_unique_abbr": "Nikon;UTokyo;Titech;UIUC",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Tokyo;Urbana",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "1f07f892c7",
        "title": "Persistent Homology for Learning Densities with Bounded Support",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/20aee3a5f4643755a79ee5f6a73050ac-Abstract.html",
        "author": "Florian T. Pokorny; Hedvig Kjellstr\u00f6m; Danica Kragic; Carl Ek",
        "abstract": "We present a novel method for learning densities with bounded support which enables us to incorporate `hard' topological constraints. In particular, we show how emerging techniques from computational algebraic topology and the notion of Persistent Homology can be combined with kernel based  methods from Machine Learning for the purpose of density estimation. The proposed formalism facilitates learning of models with bounded support in a principled way, and -- by incorporating Persistent Homology techniques in our approach -- we are able to encode algebraic-topological constraints which are not addressed in current state-of the art probabilistic models. We study the behaviour of our method on two synthetic examples for various sample sizes and exemplify the benefits of the proposed approach on a real-world data-set by learning a motion model for a racecar. We show how to learn a model which respects the underlying topological structure of the racetrack, constraining the trajectories of the car.",
        "bibtex": "@inproceedings{NIPS2012_20aee3a5,\n author = {Pokorny, Florian and Kjellstr\\\"{o}m, Hedvig and Kragic, Danica and Ek, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Persistent Homology for Learning Densities with Bounded Support},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/20aee3a5f4643755a79ee5f6a73050ac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2013049,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1880233675635445136&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computer Vision and Active Perception Lab, Centre for Autonomous Systems, School of Computer Science and Communication, KTH Royal Institute of Technology, Stockholm, Sweden; Computer Vision and Active Perception Lab, Centre for Autonomous Systems, School of Computer Science and Communication, KTH Royal Institute of Technology, Stockholm, Sweden; Computer Vision and Active Perception Lab, Centre for Autonomous Systems, School of Computer Science and Communication, KTH Royal Institute of Technology, Stockholm, Sweden; Computer Vision and Active Perception Lab, Centre for Autonomous Systems, School of Computer Science and Communication, KTH Royal Institute of Technology, Stockholm, Sweden",
        "aff_domain": "csc.kth.se;csc.kth.se;csc.kth.se;csc.kth.se",
        "email": "csc.kth.se;csc.kth.se;csc.kth.se;csc.kth.se",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "KTH Royal Institute of Technology",
        "aff_unique_dep": "School of Computer Science and Communication",
        "aff_unique_url": "https://www.kth.se",
        "aff_unique_abbr": "KTH",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stockholm",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "id": "a051e7672a",
        "title": "Phoneme Classification using Constrained Variational Gaussian Process Dynamical System",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4fac9ba115140ac4f1c22da82aa0bc7f-Abstract.html",
        "author": "Hyunsin Park; Sungrack Yun; Sanghyuk Park; Jongmin Kim; Chang D. Yoo",
        "abstract": "This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets.",
        "bibtex": "@inproceedings{NIPS2012_4fac9ba1,\n author = {Park, Hyunsin and Yun, Sungrack and Park, Sanghyuk and Kim, Jongmin and Yoo, Chang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Phoneme Classification using Constrained Variational Gaussian Process Dynamical System},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4fac9ba115140ac4f1c22da82aa0bc7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 306861,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4414338493242903105&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of EE, KAIST; Qualcomm Korea; Department of EE, KAIST; Department of EE, KAIST; Department of EE, KAIST",
        "aff_domain": "kaist.ac.kr;qualcomm.com;kaist.ac.kr;gmail.com;ee.kaist.ac.kr",
        "email": "kaist.ac.kr;qualcomm.com;kaist.ac.kr;gmail.com;ee.kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "KAIST;Qualcomm",
        "aff_unique_dep": "Department of EE;",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.qualcomm.com",
        "aff_unique_abbr": "KAIST;QCOM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "9a84fa0717",
        "title": "Pointwise Tracking the Optimal Regression Function",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ef50c335cca9f340bde656363ebd02fd-Abstract.html",
        "author": "Yair Wiener; Ran El-Yaniv",
        "abstract": "This paper examines the possibility of a `reject option' in the context of least squares regression. It is shown that using rejection it is theoretically possible to learn `selective' regressors that can $\\epsilon$-pointwise track the best regressor in hindsight from the same hypothesis class, while rejecting only a bounded portion of the domain. Moreover, the rejected volume vanishes with the training set size, under certain conditions. We then develop efficient and exact implementation of these selective regressors for the case of linear regression. Empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error.",
        "bibtex": "@inproceedings{NIPS2012_ef50c335,\n author = {Wiener, Yair and El-Yaniv, Ran},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Pointwise Tracking the Optimal Regression Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ef50c335cca9f340bde656363ebd02fd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 158273,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=64737866375898562&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science Department, Technion\u2013Israel Institute of Technology; Computer Science Department, Technion\u2013Israel Institute of Technology",
        "aff_domain": "cs.technion.ac.il;tx.technion.ac.il",
        "email": "cs.technion.ac.il;tx.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion\u2013Israel Institute of Technology",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "1f6b81de1f",
        "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html",
        "author": "Jasper Snoek; Hugo Larochelle; Ryan P. Adams",
        "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \u201cblack art\u201d requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm\u2019s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.",
        "bibtex": "@inproceedings{NIPS2012_05311655,\n author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Practical Bayesian Optimization of Machine Learning Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 521672,
        "gs_citation": 11616,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14442949298925775705&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 25,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Sherbrooke; School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": "cs.toronto.edu;usherbrooke.edu;seas.harvard.edu",
        "email": "cs.toronto.edu;usherbrooke.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Toronto;University of Sherbrooke;Harvard University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.utoronto.ca;https://www.usherbrooke.ca;https://www.harvard.edu",
        "aff_unique_abbr": "U of T;;Harvard",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Toronto;;Cambridge",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "adf6aa442b",
        "title": "Predicting Action Content On-Line and in Real Time before Action Onset \u2013 an Intracranial Human Study",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/21c52f533c0c585bab4f075bf08d7104-Abstract.html",
        "author": "Uri Maoz; Shengxuan Ye; Ian Ross; Adam Mamelak; Christof Koch",
        "abstract": "The ability to predict action content from neural signals in real time before the ac- tion occurs has been long sought in the neuroscienti\ufb01c study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for under- standing the relation between neural correlates of decision-making and conscious, voluntary action as well as for brain-machine interfaces. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a \u201cmatching-pennies\u201d game against an opponent. In each trial, subjects were given a 5 s countdown, after which they had to raise their left or right hand immediately as the \u201cgo\u201d signal appeared on a computer screen. They won a \ufb01xed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The question we here studied was the extent to which neural precursors of the subjects\u2019 decisions can be detected in intracranial local \ufb01eld potentials (LFP) prior to the onset of the action. We found that combined low-frequency (0.1\u20135 Hz) LFP signals from 10 electrodes were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5 s before the go signal with 68\u00b13% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 electrodes simultaneously, and tested it on retrospective data from 7 patients. On average, we could predict the correct hand choice in 83% of the trials, which rose to 92% if we let the system drop 3/10 of the trials on which it was less con\ufb01dent. Our system demonstrates\u2014 for the \ufb01rst time\u2014the feasibility of accurately predicting a binary action on single trials in real time for patients with intracranial recordings, well before the action occurs.",
        "bibtex": "@inproceedings{NIPS2012_21c52f53,\n author = {Maoz, Uri and Ye, Shengxuan and Ross, Ian and Mamelak, Adam and Koch, Christof},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting Action Content On-Line and in Real Time before Action Onset \\textendash  an Intracranial Human Study},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/21c52f533c0c585bab4f075bf08d7104-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/21c52f533c0c585bab4f075bf08d7104-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/21c52f533c0c585bab4f075bf08d7104-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/21c52f533c0c585bab4f075bf08d7104-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 555040,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1132965186339671993&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "California Institute of Technology; California Institute of Technology; Huntington Hospital; Cedars-Sinai Medical Center; California Institute of Technology + Allen Institute for Brain Science",
        "aff_domain": "caltech.edu;caltech.edu;aol.com;cshs.org;klab.caltech.edu",
        "email": "caltech.edu;caltech.edu;aol.com;cshs.org;klab.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0+3",
        "aff_unique_norm": "California Institute of Technology;Huntington Hospital;Cedars-Sinai Medical Center;Allen Institute for Brain Science",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.caltech.edu;https://www.huntingtonhospital.com;https://www.cedars-sinai.org;https://www.alleninstitute.org",
        "aff_unique_abbr": "Caltech;;;Allen Institute",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pasadena;",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "38bb93ee9b",
        "title": "Priors for Diversity in Generative Latent Variable Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c26820b8a4c1b3c2aa868d6d57e14a79-Abstract.html",
        "author": "James T. Kwok; Ryan P. Adams",
        "abstract": "Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization, for building density models of data, and for   providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions   on internal parameters.  For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the   underlying i.i.d.\\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions, we are able to define a DPP on probability measures.  We show how to perform MAP inference   with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model.",
        "bibtex": "@inproceedings{NIPS2012_c26820b8,\n author = {Kwok, James and Adams, Ryan P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Priors for Diversity in Generative Latent Variable Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c26820b8a4c1b3c2aa868d6d57e14a79-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 551644,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8013203285522108324&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "School of Engineering and Applied Sciences, Harvard University; School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": "fas.harvard.edu;seas.harvard.edu",
        "email": "fas.harvard.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2997412e55",
        "title": "Privacy Aware Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/08d98638c6fcd194a4b1e6992063e944-Abstract.html",
        "author": "Martin J. Wainwright; Michael I. Jordan; John C. Duchi",
        "abstract": "We study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.  In this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator.",
        "bibtex": "@inproceedings{NIPS2012_08d98638,\n author = {Wainwright, Martin J and Jordan, Michael and Duchi, John C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Privacy Aware Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/08d98638c6fcd194a4b1e6992063e944-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/08d98638c6fcd194a4b1e6992063e944-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 213268,
        "gs_citation": 325,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9407990721521472397&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley + Department of Statistics, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley + Department of Statistics, University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+0;0+0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0+0;0+0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c10bae9858",
        "title": "Probabilistic Event Cascades for Alzheimer's disease",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0663a4ddceacb40b095eda264a85f15c-Abstract.html",
        "author": "Jonathan Huang; Daniel Alexander",
        "abstract": "Accurate and detailed models of the progression of neurodegenerative diseases such as  Alzheimer's (AD) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments. In this paper, we introduce the ALPACA (Alzheimer's disease Probabilistic Cascades) model, a generative model linking latent Alzheimer's progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a fixed ordering of events, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed disease progression models. We describe efficient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer's patients from the  Alzheimer's Disease Neuroimaging Initiative.",
        "bibtex": "@inproceedings{NIPS2012_0663a4dd,\n author = {Huang, Jonathan and Alexander, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Event Cascades for Alzheimer\\textquotesingle s disease},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0663a4ddceacb40b095eda264a85f15c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0663a4ddceacb40b095eda264a85f15c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0663a4ddceacb40b095eda264a85f15c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1732884,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14679917589131252938&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; University College London",
        "aff_domain": "stanford.edu;cs.ucl.ac.uk",
        "email": "stanford.edu;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;University College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Stanford;UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "b4dfbc2f1b",
        "title": "Probabilistic Low-Rank Subspace Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b51a15f382ac914391a58850ab343b00-Abstract.html",
        "author": "S. D. Babacan; Shinichi Nakajima; Minh Do",
        "abstract": "In this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we first develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in clustering and identifying outliers.",
        "bibtex": "@inproceedings{NIPS2012_b51a15f3,\n author = {Babacan, S. and Nakajima, Shinichi and Do, Minh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Low-Rank Subspace Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b51a15f382ac914391a58850ab343b00-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b51a15f382ac914391a58850ab343b00-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/b51a15f382ac914391a58850ab343b00-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b51a15f382ac914391a58850ab343b00-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 422323,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15817982997372403893&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Illinois at Urbana-Champaign; Nikon Corporation; University of Illinois at Urbana-Champaign",
        "aff_domain": "gmail.com;nikon.co.jp;illinois.edu",
        "email": "gmail.com;nikon.co.jp;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Nikon Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.nikon.com",
        "aff_unique_abbr": "UIUC;Nikon",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "3657a42d68",
        "title": "Probabilistic n-Choose-k Models for Classification and Ranking",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/84fdbc3ac902561c00871c9b0c226756-Abstract.html",
        "author": "Kevin Swersky; Brendan J. Frey; Daniel Tarlow; Richard S. Zemel; Ryan P. Adams",
        "abstract": "In categorical data there is often structure in the number of variables that take on each label. For example, the total number of objects in an image and the number of highly relevant documents per query in web search both tend to follow a structured distribution. In this paper, we study a probabilistic model that explicitly includes a prior distribution over such counts, along with a count-conditional likelihood that de\ufb01nes probabilities over all subsets of a given size. When labels are binary and the prior over counts is a Poisson-Binomial distribution, a standard logistic regression model is recovered, but for other count distributions, such priors induce global dependencies and combinatorics that appear to complicate learning and inference. However, we demonstrate that simple, ef\ufb01cient learning procedures can be derived for more general forms of this model. We illustrate the utility of the formulation by exploring applications to multi-object classi\ufb01cation, learning to rank, and top-K classi\ufb01cation.",
        "bibtex": "@inproceedings{NIPS2012_84fdbc3a,\n author = {Swersky, Kevin and Frey, Brendan J and Tarlow, Daniel and Zemel, Richard and Adams, Ryan P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic n-Choose-k Models for Classification and Ranking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/84fdbc3ac902561c00871c9b0c226756-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/84fdbc3ac902561c00871c9b0c226756-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/84fdbc3ac902561c00871c9b0c226756-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/84fdbc3ac902561c00871c9b0c226756-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 328694,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14702237655841830172&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Dept. of Computer Science, University of Toronto; Dept. of Computer Science, University of Toronto; School of Eng. and Appl. Sciences, Harvard University; Dept. of Computer Science, University of Toronto; Prob. and Stat. Inf. Group, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;seas.harvard.edu;cs.toronto.edu;psi.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;seas.harvard.edu;cs.toronto.edu;psi.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Toronto;Harvard University",
        "aff_unique_dep": "Department of Computer Science;School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.utoronto.ca;https://www.seas.harvard.edu",
        "aff_unique_abbr": "U of T;SEAS",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Toronto;Cambridge;",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "305911dcfa",
        "title": "Projection Retrieval for Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html",
        "author": "Madalina Fiterau; Artur Dubrawski",
        "abstract": "In many applications classification systems often require in the loop human intervention. In such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution. To tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users.",
        "bibtex": "@inproceedings{NIPS2012_ea8fcd92,\n author = {Fiterau, Madalina and Dubrawski, Artur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Projection Retrieval for Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ea8fcd92d59581717e06eb187f10666d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 433389,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18120007268932734195&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Machine Learning Department, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fea0887f69",
        "title": "Proper losses for learning from partial labels",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/217eedd1ba8c592db97d0dbe54c7adfc-Abstract.html",
        "author": "Jes\u00fas Cid-sueiro",
        "abstract": "This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, establish a necessary and sufficient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. An interesting result is that the full knowledge of this matrix is not required, and losses can be constructed that are proper in a subset of the probability simplex.",
        "bibtex": "@inproceedings{NIPS2012_217eedd1,\n author = {Cid-sueiro, Jes\\'{u}s},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Proper losses for learning from partial labels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/217eedd1ba8c592db97d0dbe54c7adfc-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/217eedd1ba8c592db97d0dbe54c7adfc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/217eedd1ba8c592db97d0dbe54c7adfc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 295319,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5760095001972783319&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Legans-Madrid, 28911 Spain",
        "aff_domain": "tsc.uc3m.es",
        "email": "tsc.uc3m.es",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Universidad Carlos III de Madrid",
        "aff_unique_dep": "Department of Signal Theory and Communications",
        "aff_unique_url": "https://www.uc3m.es",
        "aff_unique_abbr": "UC3M",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Legans-Madrid",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "c68822acaa",
        "title": "Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html",
        "author": "Sanjeev Arora; Rong Ge; Ankur Moitra; Sushant Sachdeva",
        "abstract": "We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \\eta$ where $A$ is an unknown $n \\times n$ matrix and $x$ is chosen uniformly at random from $\\{+1, -1\\}^n$, $\\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\\Sigma$: We give an algorithm that provable recovers $A$ and $\\Sigma$ up to an additive $\\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \\epsilon$. To accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search.",
        "bibtex": "@inproceedings{NIPS2012_09c6c378,\n author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur and Sachdeva, Sushant},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/09c6c3783b4a70054da74f2538ed47c6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/09c6c3783b4a70054da74f2538ed47c6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 325349,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10984644368565659585&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; School of Mathematics, Institute for Advanced Study; Department of Computer Science, Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu;ias.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu;ias.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Princeton University;Institute for Advanced Study",
        "aff_unique_dep": "Department of Computer Science;School of Mathematics",
        "aff_unique_url": "https://www.princeton.edu;https://www.ias.edu",
        "aff_unique_abbr": "Princeton;IAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "43cb571969",
        "title": "Proximal Newton-type methods for convex optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0f304eddb4ad6007a3093fd6d963a1d2-Abstract.html",
        "author": "Jason Lee; Yuekai Sun; Michael Saunders",
        "abstract": "We seek to solve convex optimization problems in composite form:",
        "bibtex": "@inproceedings{NIPS2012_0f304edd,\n author = {Lee, Jason D and Sun, Yuekai and Saunders, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Proximal Newton-type methods for convex optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0f304eddb4ad6007a3093fd6d963a1d2-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0f304eddb4ad6007a3093fd6d963a1d2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/0f304eddb4ad6007a3093fd6d963a1d2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0f304eddb4ad6007a3093fd6d963a1d2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 253819,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17700185636313237991&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Institute for Computational and Mathematical Engineering, Stanford University, Stanford, CA; Institute for Computational and Mathematical Engineering, Stanford University, Stanford, CA; Department of Management Science and Engineering, Stanford University, Stanford, CA",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Institute for Computational and Mathematical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5905364324",
        "title": "Putting Bayes to sleep",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/43ec517d68b6edd3015b3edc9a11367b-Abstract.html",
        "author": "Dmitry Adamskiy; Manfred K. Warmuth; Wouter M. Koolen",
        "abstract": "We consider sequential prediction algorithms that are given the predictions from a set of models as inputs. If the nature of the data is changing over time in that different models predict well on different segments of the data, then adaptivity is typically achieved by mixing into the weights in each round a bit of the initial prior (kind of like a weak restart). However, what if the favored models in each segment are from a small subset, i.e. the data is likely to be predicted well by models that predicted well before? Curiously, fitting such ''sparse composite models'' is achieved by mixing in a bit of all the past posteriors. This self-referential updating method is rather peculiar, but it is efficient and gives superior performance on many natural data sets. Also it is important because it introduces a long-term memory: any model that has done well in the past can be recovered quickly. While Bayesian interpretations can be found for mixing in a bit of the initial prior, no Bayesian interpretation is known for mixing in past posteriors.  We build atop the ''specialist'' framework from the online learning literature to give the Mixing Past Posteriors update a proper Bayesian foundation. We apply our method to a well-studied multitask learning problem and obtain a new intriguing efficient update that achieves a significantly better bound.",
        "bibtex": "@inproceedings{NIPS2012_43ec517d,\n author = {Adamskiy, Dmitry and Warmuth, Manfred K. K and Koolen, Wouter M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Putting Bayes to sleep},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/43ec517d68b6edd3015b3edc9a11367b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/43ec517d68b6edd3015b3edc9a11367b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/43ec517d68b6edd3015b3edc9a11367b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 293504,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18367774860393968951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "73bb99cd3d",
        "title": "Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/fccb3cdc9acc14a6e70a12f74560c026-Abstract.html",
        "author": "Chris Hinrichs; Vikas Singh; Jiming Peng; Sterling Johnson",
        "abstract": "Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients. Existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q \\succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept. This formulation significantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model\u2019s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject\u2019s conversion to Alzheimer\u2019s Disease (AD) by exploiting aggregate information from several distinct imaging modalities. Here, our new model outperforms the state of the art (p-values << 10\u22123 ). We briefly discuss ramifications in terms of learning bounds (Rademacher complexity).",
        "bibtex": "@inproceedings{NIPS2012_fccb3cdc,\n author = {Hinrichs, Chris and Singh, Vikas and Peng, Jiming and Johnson, Sterling},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/fccb3cdc9acc14a6e70a12f74560c026-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/fccb3cdc9acc14a6e70a12f74560c026-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 519011,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2243797945823725861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Wisconsin + Geriatric Research Education & Clinical Center; University of Wisconsin + Geriatric Research Education & Clinical Center; University of Illinois; University of Wisconsin + Geriatric Research Education & Clinical Center",
        "aff_domain": "cs.wisc.edu;biostat.wisc.edu;illinois.edu;medicine.wisc.edu",
        "email": "cs.wisc.edu;biostat.wisc.edu;illinois.edu;medicine.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2;0+1",
        "aff_unique_norm": "University of Wisconsin;Geriatric Research Education & Clinical Center;University of Illinois",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.wisc.edu;;https://www.illinois.edu",
        "aff_unique_abbr": "UW;;UIUC",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c4172bbd6d",
        "title": "Query Complexity of Derivative-Free Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html",
        "author": "Kevin G. Jamieson; Robert Nowak; Ben Recht",
        "abstract": "Derivative Free Optimization (DFO) is attractive when the objective function's derivatives are not available and evaluations are costly.   Moreover, if the function evaluations are noisy, then approximating gradients by finite differences is difficult.  This paper gives quantitative lower bounds on the performance of DFO with noisy function evaluations, exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients. This challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient.  However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions.  A distinctive feature of the algorithm is that it only uses Boolean-valued function comparisons, rather than evaluations.  This makes the algorithm useful in an even wider range of applications, including optimization based on paired comparisons from human subjects, for example.  Remarkably, we show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same.",
        "bibtex": "@inproceedings{NIPS2012_e6d8545d,\n author = {Jamieson, Kevin G and Nowak, Robert and Recht, Ben},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Query Complexity of Derivative-Free Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/e6d8545daa42d5ced125a4bf747b3688-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e6d8545daa42d5ced125a4bf747b3688-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 399238,
        "gs_citation": 212,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7255922303736331334&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "University of Wisconsin; University of Wisconsin; University of Wisconsin",
        "aff_domain": "wisc.edu;engr.wisc.edu;cs.wisc.edu",
        "email": "wisc.edu;engr.wisc.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Wisconsin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "76f463dd60",
        "title": "Random Utility Theory for Social Choice",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a512294422de868f8474d22344636f16-Abstract.html",
        "author": "Hossein Azari; David Parks; Lirong Xia",
        "abstract": "Random utility theory models an agents preferences on alternatives by drawing a real-valued score on each alternative (typically independently) from a parameterized distribution, and then ranking the alternatives according to scores. A special case that has received signicant attention is the Plackett-Luce model, for which fast inference methods for maximum likelihood estimators are available. This paper develops conditions on general random utility models that enable fast inference within a Bayesian framework through MC-EM, providing concave loglikelihood functions and bounded sets of global maxima solutions. Results on both real-world and simulated data provide support for the scalability of the approach and capability for model selection among general random utility models including Plackett-Luce.",
        "bibtex": "@inproceedings{NIPS2012_a5122944,\n author = {Azari, Hossein and Parks, David and Xia, Lirong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Utility Theory for Social Choice},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a512294422de868f8474d22344636f16-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a512294422de868f8474d22344636f16-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a512294422de868f8474d22344636f16-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3459038,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3499000903622861143&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "SEAS, Harvard University; SEAS, Harvard University; SEAS, Harvard University",
        "aff_domain": "fas.harvard.edu;eecs.harvard.edu;seas.harvard.edu",
        "email": "fas.harvard.edu;eecs.harvard.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.seas.harvard.edu",
        "aff_unique_abbr": "SEAS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2cb21e17d1",
        "title": "Random function priors for exchangeable arrays with applications to graphs and relational data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/df6c9756b2334cc5008c115486124bfe-Abstract.html",
        "author": "James Lloyd; Peter Orbanz; Zoubin Ghahramani; Daniel M. Roy",
        "abstract": "A fundamental problem in the analysis of structured relational data like graphs, networks, databases, and matrices is to extract a summary of the common struc- ture underlying relations between individual entities. Relational data are typically encoded in the form of arrays; invariance to the ordering of rows and columns corresponds to exchangeable arrays. Results in probability theory due to Aldous, Hoover and Kallenberg show that exchangeable arrays can be represented in terms of a random measurable function which constitutes the natural model parameter in a Bayesian model. We obtain a \ufb02exible yet simple Bayesian nonparametric model by placing a Gaussian process prior on the parameter function. Ef\ufb01cient inference utilises elliptical slice sampling combined with a random sparse approximation to the Gaussian process. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases.",
        "bibtex": "@inproceedings{NIPS2012_df6c9756,\n author = {Lloyd, James and Orbanz, Peter and Ghahramani, Zoubin and Roy, Daniel M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random function priors for exchangeable arrays with applications to graphs and relational data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/df6c9756b2334cc5008c115486124bfe-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/df6c9756b2334cc5008c115486124bfe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/df6c9756b2334cc5008c115486124bfe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 495453,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14160689498894879887&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b349d7d1e3",
        "title": "Rational inference of relative preferences",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2cfd4560539f887a5e420412b370b361-Abstract.html",
        "author": "Nisheeth Srivastava; Paul R. Schrater",
        "abstract": "Statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions. However, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented. In this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects. Complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function.",
        "bibtex": "@inproceedings{NIPS2012_2cfd4560,\n author = {Srivastava, Nisheeth and Schrater, Paul R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rational inference of relative preferences},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2cfd4560539f887a5e420412b370b361-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2cfd4560539f887a5e420412b370b361-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/2cfd4560539f887a5e420412b370b361-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2cfd4560539f887a5e420412b370b361-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 321012,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10195480017829955723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dept of Computer Science, University of Minnesota; Dept of Psychology, University of Minnesota",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "Dept of Computer Science",
        "aff_unique_url": "https://www.umn.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8157424319",
        "title": "Recognizing Activities by Attribute Dynamics",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html",
        "author": "Weixin Li; Nuno Vasconcelos",
        "abstract": "In this work, we consider the problem of modeling the dynamic structure of human activities in  the attributes space. A video sequence is first represented in a semantic feature space, where each feature encodes the probability of  occurrence of an activity attribute at a given time. A generative model,  denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this  space. The BDS is a non-linear dynamic system, which extends both the binary  principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining binary observation variables with a hidden Gauss-Markov state  process. In this way, it integrates the representation power of semantic  modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS  parameters, inspired by a popular LDS  learning method from dynamic textures, is proposed. A similarity measure between BDSs, which generalizes  the Binet-Cauchy kernel for LDS, is then introduced and used to design  activity classifiers. The proposed method is shown to outperform similar classifiers  derived from the kernel dynamic system (KDS) and state-of-the-art approaches for  dynamics-based or attribute-based action recognition.",
        "bibtex": "@inproceedings{NIPS2012_33e8075e,\n author = {Li, Weixin and Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recognizing Activities by Attribute Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/33e8075e9970de0cfea955afd4644bb2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 836222,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8117618218181286161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4bcd339cd2",
        "title": "Recovery of Sparse Probability Measures via Convex Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0f2c9a93eea6f38fabb3acb1c31488c6-Abstract.html",
        "author": "Mert Pilanci; Laurent E. Ghaoui; Venkat Chandrasekaran",
        "abstract": "We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. It's well-known that the classical L1 regularizer fails to promote sparsity on the probability simplex since L1 norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming. As a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently. A sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known heuristics based on L1 norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm.",
        "bibtex": "@inproceedings{NIPS2012_0f2c9a93,\n author = {Pilanci, Mert and Ghaoui, Laurent and Chandrasekaran, Venkat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recovery of Sparse Probability Measures via Convex Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0f2c9a93eea6f38fabb3acb1c31488c6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 517728,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12888805918340526342&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Electrical Engineering and Computer Science, University of California Berkeley; Electrical Engineering and Computer Science, University of California Berkeley; Department of Computing and Mathematical Sciences, California Institute of Technology",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;caltech.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Berkeley;California Institute of Technology",
        "aff_unique_dep": "Electrical Engineering and Computer Science;Department of Computing and Mathematical Sciences",
        "aff_unique_url": "https://www.berkeley.edu;https://www.caltech.edu",
        "aff_unique_abbr": "UC Berkeley;Caltech",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Berkeley;Pasadena",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b0c39b3519",
        "title": "Reducing statistical time-series problems to binary classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/93d65641ff3f1586614cf2c1ad240b6c-Abstract.html",
        "author": "Daniil Ryabko; Jeremie Mary",
        "abstract": "We  show how binary classification methods developed to work on i.i.d. data can be  used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series.  Specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solving  these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods.  Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.",
        "bibtex": "@inproceedings{NIPS2012_93d65641,\n author = {Ryabko, Daniil and Mary, Jeremie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reducing statistical time-series problems to binary classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/93d65641ff3f1586614cf2c1ad240b6c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/93d65641ff3f1586614cf2c1ad240b6c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 299474,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9498171137129458109&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "SequeL-INRIA/LIFL-CNRS, Universit\u00e9 de Lille, France; SequeL-INRIA/LIFL-CNRS, Universit\u00e9 de Lille, France",
        "aff_domain": "ryabko.net;inria.fr",
        "email": "ryabko.net;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 de Lille",
        "aff_unique_dep": "SequeL-INRIA/LIFL-CNRS",
        "aff_unique_url": "https://www.univ-lille.fr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "16e6b2ebe1",
        "title": "Regularized Off-Policy TD-Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/28f0b864598a1291557bed248a998d4e-Abstract.html",
        "author": "Bo Liu; Sridhar Mahadevan; Ji Liu",
        "abstract": "We present a novel $l_1$ regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying RO-TD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization. A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm.",
        "bibtex": "@inproceedings{NIPS2012_28f0b864,\n author = {Liu, Bo and Mahadevan, Sridhar and Liu, Ji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regularized Off-Policy TD-Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/28f0b864598a1291557bed248a998d4e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/28f0b864598a1291557bed248a998d4e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/28f0b864598a1291557bed248a998d4e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 206634,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14486796567651303831&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Computer Science Department, University of Massachusetts; Computer Science Department, University of Massachusetts; Computer Science Department, University of Wisconsin",
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.wisc.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Massachusetts;University of Wisconsin",
        "aff_unique_dep": "Computer Science Department;Computer Science Department",
        "aff_unique_url": "https://www.umass.edu;https://www.wisc.edu",
        "aff_unique_abbr": "UMass;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "809acea7dc",
        "title": "Relax and Randomize : From Value to Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/53adaf494dc89ef7196d73636eb2451b-Abstract.html",
        "author": "Sasha Rakhlin; Ohad Shamir; Karthik Sridharan",
        "abstract": "We show a principled way of deriving online learning algorithms from a minimax analysis. Various upper bounds on the minimax value, previously thought to be non-constructive, are shown to yield algorithms. This allows us to seamlessly recover known methods and to derive new ones, also capturing such ''unorthodox'' methods as Follow the Perturbed Leader and the R^2 forecaster. Understanding the inherent complexity of the learning problem thus leads to the development of algorithms. To illustrate our approach, we present several new algorithms, including a family of randomized methods that use the idea of a ''random play out''. New versions of the Follow-the-Perturbed-Leader algorithms are presented, as well as methods based on the Littlestone's dimension, efficient methods for matrix completion with trace norm, and algorithms for the problems of transductive learning and prediction with static experts.",
        "bibtex": "@inproceedings{NIPS2012_53adaf49,\n author = {Rakhlin, Sasha and Shamir, Ohad and Sridharan, Karthik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Relax and Randomize : From Value to Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/53adaf494dc89ef7196d73636eb2451b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/53adaf494dc89ef7196d73636eb2451b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1457509,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12116314394970254095&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "48d9b4ce37",
        "title": "Repulsive Mixtures",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8d6dc35e506fc23349dd10ee68dabb64-Abstract.html",
        "author": "Francesca Petralia; Vinayak Rao; David B. Dunson",
        "abstract": "Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.   Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set.",
        "bibtex": "@inproceedings{NIPS2012_8d6dc35e,\n author = {Petralia, Francesca and Rao, Vinayak and Dunson, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Repulsive Mixtures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/8d6dc35e506fc23349dd10ee68dabb64-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8d6dc35e506fc23349dd10ee68dabb64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 352901,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16670387850049284865&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Statistical Science, Duke University; Gatsby Computational Neuroscience Unit, University College London; Department of Statistical Science, Duke University",
        "aff_domain": "duke.edu;gatsby.ucl.ac.uk;stat.duke.edu",
        "email": "duke.edu;gatsby.ucl.ac.uk;stat.duke.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Duke University;University College London",
        "aff_unique_dep": "Department of Statistical Science;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.duke.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Duke;UCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "7abccd391c",
        "title": "Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e2f374c3418c50bc30d67d5f7454a5b4-Abstract.html",
        "author": "Teodor M. Moldovan; Pieter Abbeel",
        "abstract": "The expected return is a widely used objective in decision making under uncer- tainty. Many algorithms, such as value iteration, have been proposed to optimize it. In risk-aware settings, however, the expected return is often not an appropriate objective to optimize. We propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties. We also draw con- nections to previously proposed objectives for risk-aware planing: minmax, ex- ponential utility, percentile and mean minus variance. Our method applies to an extended class of Markov decision processes: we allow costs to be stochastic as long as they are bounded. Additionally, we present an ef\ufb01cient algorithm for op- timizing the proposed objective. Synthetic and real-world experiments illustrate the effectiveness of our method, at scale.",
        "bibtex": "@inproceedings{NIPS2012_e2f374c3,\n author = {Moldovan, Teodor and Abbeel, Pieter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e2f374c3418c50bc30d67d5f7454a5b4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e2f374c3418c50bc30d67d5f7454a5b4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/e2f374c3418c50bc30d67d5f7454a5b4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e2f374c3418c50bc30d67d5f7454a5b4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 283081,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8471616274210291537&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of California at Berkeley; Department of Computer Science, University of California at Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e08b139c2a",
        "title": "Risk-Aversion in Multi-armed Bandits",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/83f2550373f2f19492aa30fbd5b57512-Abstract.html",
        "author": "Amir Sani; Alessandro Lazaric; R\u00e9mi Munos",
        "abstract": "In stochastic multi--armed bandits the objective is to solve the exploration--exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk--aversion where the objective is to compete against the arm with the best risk--return trade--off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, we investigate their theoretical guarantees, and we report preliminary empirical results.",
        "bibtex": "@inproceedings{NIPS2012_83f25503,\n author = {Sani, Amir and Lazaric, Alessandro and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Risk-Aversion in Multi-armed Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/83f2550373f2f19492aa30fbd5b57512-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/83f2550373f2f19492aa30fbd5b57512-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/83f2550373f2f19492aa30fbd5b57512-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 336968,
        "gs_citation": 207,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3335453019786235960&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "INRIA Lille - Nord Europe, Team SequeL; INRIA Lille - Nord Europe, Team SequeL; INRIA Lille - Nord Europe, Team SequeL",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "Team SequeL",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lille - Nord Europe",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "76b11441f1",
        "title": "Robustness and risk-sensitivity in Markov decision processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract.html",
        "author": "Takayuki Osogami",
        "abstract": "We uncover relations between robust MDPs and risk-sensitive MDPs.  The objective of a robust MDP is to minimize a function, such as the expectation of cumulative cost, for the worst case when the parameters have uncertainties.  The objective of a risk-sensitive MDP is to minimize a risk measure of the cumulative cost when the parameters are known.  We show that a risk-sensitive MDP of minimizing the expected exponential utility is equivalent to a robust MDP of minimizing the worst-case expectation with a penalty for the deviation of the uncertain parameters from their nominal values, which is measured with the Kullback-Leibler divergence.  We also show that a risk-sensitive MDP of minimizing an iterated risk measure that is composed of certain coherent risk measures is equivalent to a robust MDP of minimizing the worst-case expectation when the possible deviations of uncertain parameters from their nominal values are characterized with a concave function.",
        "bibtex": "@inproceedings{NIPS2012_d1f491a4,\n author = {Osogami, Takayuki},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robustness and risk-sensitivity in Markov decision processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/d1f491a404d6854880943e5c3cd9ca25-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d1f491a404d6854880943e5c3cd9ca25-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 337017,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3620013290870945127&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "IBM Research - Tokyo",
        "aff_domain": "jp.ibm.com",
        "email": "jp.ibm.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "a0057b2f3e",
        "title": "Scalable Inference of Overlapping Communities",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d6ef5f7fa914c19931a55bb262ec879c-Abstract.html",
        "author": "Prem Gopalan; Sean Gerrish; Michael Freedman; David M. Blei; David M. Mimno",
        "abstract": "We develop a scalable algorithm for posterior inference of overlapping communities in large networks.  Our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel. It naturally interleaves subsampling the network with estimating its community structure.  We apply our algorithm on ten large, real-world networks with up to 60,000 nodes. It converges several orders of magnitude faster than the state-of-the-art algorithm for MMSB, finds hundreds of communities in large real-world networks, and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms.",
        "bibtex": "@inproceedings{NIPS2012_d6ef5f7f,\n author = {Gopalan, Prem K and Gerrish, Sean and Freedman, Michael and Blei, David and Mimno, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Inference of Overlapping Communities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/d6ef5f7fa914c19931a55bb262ec879c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d6ef5f7fa914c19931a55bb262ec879c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 873528,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3817141423408688095&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Princeton University, Princeton, NJ 08540; Department of Computer Science, Princeton University, Princeton, NJ 08540; Department of Computer Science, Princeton University, Princeton, NJ 08540; Department of Computer Science, Princeton University, Princeton, NJ 08540; Department of Computer Science, Princeton University, Princeton, NJ 08540",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8579ea2282",
        "title": "Scalable imputation of genetic data with a discrete fragmentation-coagulation process",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/84438b7aae55a0638073ef798e50b4ef-Abstract.html",
        "author": "Lloyd Elliott; Yee W. Teh",
        "abstract": "We present a Bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a Markov model of  partitions. The partitions at consecutive locations in the genome are related by their clusters first splitting and then merging.  Our model can be thought of as a discrete time analogue of continuous time fragmentation-coagulation processes [Teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable. We apply this model to the problem of genotype imputation, showing improved computational efficiency while maintaining the same accuracies as in [Teh et al 2011].",
        "bibtex": "@inproceedings{NIPS2012_84438b7a,\n author = {Elliott, Lloyd and Teh, Yee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable imputation of genetic data with a discrete fragmentation-coagulation process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/84438b7aae55a0638073ef798e50b4ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 445394,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15503184557873643836&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Department of Statistics, University of Oxford",
        "aff_domain": "gatsby.ucl.ac.uk;stats.ox.ac.uk",
        "email": "gatsby.ucl.ac.uk;stats.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;University of Oxford",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Department of Statistics",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.ox.ac.uk",
        "aff_unique_abbr": "UCL;Oxford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "London;Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "7d12e51fcd",
        "title": "Scalable nonconvex inexact proximal splitting",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7f100b7b36092fb9b06dfb4fac360931-Abstract.html",
        "author": "Suvrit Sra",
        "abstract": "We study large-scale, nonsmooth, nonconconvex optimization problems. In particular, we focus on nonconvex problems with \\emph{composite} objectives. This class of problems includes the extensively studied convex, composite objective problems as a special case. To tackle composite nonconvex problems, we introduce a powerful new framework based on asymptotically \\emph{nonvanishing} errors, avoiding the common convenient assumption of eventually vanishing errors. Within our framework we derive both batch and incremental nonconvex proximal splitting algorithms. To our knowledge, our framework is first to develop and analyze incremental \\emph{nonconvex} proximal-splitting algorithms, even if we disregard the ability to handle nonvanishing errors. We illustrate our theoretical framework by showing how it applies to difficult large-scale, nonsmooth, and nonconvex problems.",
        "bibtex": "@inproceedings{NIPS2012_7f100b7b,\n author = {Sra, Suvrit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable nonconvex inexact proximal splitting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7f100b7b36092fb9b06dfb4fac360931-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7f100b7b36092fb9b06dfb4fac360931-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7f100b7b36092fb9b06dfb4fac360931-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 362622,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17457535599500999602&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Max Planck Institute for Intelligent Systems",
        "aff_domain": "tuebingen.mpg.de",
        "email": "tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Intelligent Systems",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "5598c2b287",
        "title": "Scaled Gradients on Grassmann Manifolds for Matrix Completion",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/71a3cb155f8dc89bf3d0365288219936-Abstract.html",
        "author": "Thanh Ngo; Yousef Saad",
        "abstract": "This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices, while maintaining established global convegence and exact recovery guarantees. A connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure is also established. The proposed conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods.",
        "bibtex": "@inproceedings{NIPS2012_71a3cb15,\n author = {Ngo, Thanh and Saad, Yousef},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scaled Gradients on Grassmann Manifolds for Matrix Completion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/71a3cb155f8dc89bf3d0365288219936-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/71a3cb155f8dc89bf3d0365288219936-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/71a3cb155f8dc89bf3d0365288219936-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 256241,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1229526080411434531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "91accbc7d5",
        "title": "Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html",
        "author": "Stephen Bach; Matthias Broecheler; Lise Getoor; Dianne O'leary",
        "abstract": "Probabilistic graphical models are powerful tools for analyzing constrained, continuous domains. However, finding most-probable explanations (MPEs) in these models can be computationally expensive. In this paper, we improve the scalability of MPE inference in a class of graphical models with piecewise-linear and piecewise-quadratic dependencies and linear constraints over continuous domains. We derive algorithms based on a consensus-optimization framework and demonstrate their superior performance over state of the art. We show empirically that in a large-scale voter-preference modeling problem our algorithms scale linearly in the number of dependencies and constraints.",
        "bibtex": "@inproceedings{NIPS2012_c5d73680,\n author = {Bach, Stephen and Broecheler, Matthias and Getoor, Lise and O\\textquotesingle leary, Dianne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c5d736809766d46260d816d8dbc9eb44-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c5d736809766d46260d816d8dbc9eb44-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 285483,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9424940627013392125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "University of Maryland, College Park; Aurelius LLC; University of Maryland, College Park; University of Maryland, College Park",
        "aff_domain": "cs.umd.edu;thinkaurelius.com;cs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;thinkaurelius.com;cs.umd.edu;cs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Maryland;Aurelius LLC",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;",
        "aff_unique_abbr": "UMD;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "83446b20be",
        "title": "Searching for objects driven by context",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/1068c6e4c8051cfd4e9ea8072e3189e2-Abstract.html",
        "author": "Bogdan Alexe; Nicolas Heess; Yee W. Teh; Vittorio Ferrari",
        "abstract": "The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired. We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image. Their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set. In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy.",
        "bibtex": "@inproceedings{NIPS2012_1068c6e4,\n author = {Alexe, Bogdan and Heess, Nicolas and Teh, Yee and Ferrari, Vittorio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Searching for objects driven by context},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/1068c6e4c8051cfd4e9ea8072e3189e2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6850307,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12964127729519829707&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2a1fa05ade",
        "title": "Selecting Diverse Features via Spectral Regularization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e94550c93cd70fe748e6982b3439ad3b-Abstract.html",
        "author": "Abhimanyu Das; Anirban Dasgupta; Ravi Kumar",
        "abstract": "We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective. Diversity is useful for several reasons such as interpretability, robustness to noise, etc.  We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized approximately by efficient greedy and local search algorithms, with provable guarantees.  We compare our algorithms to traditional greedy and $\\ell_1$-regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations.",
        "bibtex": "@inproceedings{NIPS2012_e94550c9,\n author = {Das, Abhimanyu and Dasgupta, Anirban and Kumar, Ravi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Selecting Diverse Features via Spectral Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e94550c93cd70fe748e6982b3439ad3b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 307725,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2723489259698187590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft Research Mountain View; Yahoo! Labs Sunnyvale; Google Mountain View",
        "aff_domain": "microsoft.com;yahoo-inc.com;gmail.com",
        "email": "microsoft.com;yahoo-inc.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft;Yahoo!;Google",
        "aff_unique_dep": "Microsoft Research;Yahoo! Labs;Google",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://yahoo.com;https://www.google.com",
        "aff_unique_abbr": "MSR;Yahoo!;Google",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Mountain View;Sunnyvale",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e5d854b871",
        "title": "Selective Labeling via Error Bound Minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/045117b0e0a11a242b9765e79cbf113f-Abstract.html",
        "author": "Quanquan Gu; Tong Zhang; Jiawei Han; Chris H. Ding",
        "abstract": "In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2012_045117b0,\n author = {Gu, Quanquan and Zhang, Tong and Han, Jiawei and Ding, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Selective Labeling via Error Bound Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/045117b0e0a11a242b9765e79cbf113f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/045117b0e0a11a242b9765e79cbf113f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 295781,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9194463560143181292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign; Department. of Statistics, Rutgers University; Department. of Computer Science & Engineering, University of Texas at Arlington; Department of Computer Science, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;stat.rutgers.edu;uta.edu;cs.uiuc.edu",
        "email": "illinois.edu;stat.rutgers.edu;uta.edu;cs.uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Rutgers University;University of Texas at Arlington",
        "aff_unique_dep": "Department of Computer Science;Department of Statistics;Department of Computer Science & Engineering",
        "aff_unique_url": "https://illinois.edu;https://www.rutgers.edu;https://www.uta.edu",
        "aff_unique_abbr": "UIUC;Rutgers;UTA",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Urbana-Champaign;;Arlington",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "72c76902c8",
        "title": "Semantic Kernel Forests from Multiple Taxonomies",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d554f7bb7be44a7267068a7df88ddd20-Abstract.html",
        "author": "Sung Ju Hwang; Kristen Grauman; Fei Sha",
        "abstract": "When learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient.  While an \\emph{object taxonomy} specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that \\emph{are} relevant.  In light of these issues, we propose a discriminative feature learning approach that leverages \\emph{multiple} hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenic ties, while another could reflect their habitats).  For each taxonomy, we first learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes.  Then, using the resulting \\emph{semantic kernel forest}, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class.  To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure.  We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements.",
        "bibtex": "@inproceedings{NIPS2012_d554f7bb,\n author = {Hwang, Sung and Grauman, Kristen and Sha, Fei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semantic Kernel Forests from Multiple Taxonomies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d554f7bb7be44a7267068a7df88ddd20-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 236642,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12950720507845668417&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "University of Texas; University of Texas; University of Southern California",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;usc.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Texas;University of Southern California",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.usc.edu",
        "aff_unique_abbr": "UT;USC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bef60b0dd1",
        "title": "Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/dd45045f8c68db9f54e70c67048d32e8-Abstract.html",
        "author": "Jinfeng Yi; Rong Jin; Shaili Jain; Tianbao Yang; Anil K. Jain",
        "abstract": "One of the main challenges in data clustering is to define an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called \\textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered. One difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difficulty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency.",
        "bibtex": "@inproceedings{NIPS2012_dd45045f,\n author = {Yi, Jinfeng and Jin, Rong and Jain, Shaili and Yang, Tianbao and Jain, Anil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/dd45045f8c68db9f54e70c67048d32e8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/dd45045f8c68db9f54e70c67048d32e8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2961616,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=389118205821247109&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Michigan State University; Michigan State University; Michigan State University; Yale University; Machine Learning Lab, GE Global Research",
        "aff_domain": "cse.msu.edu;cse.msu.edu;cse.msu.edu;yale.edu;ge.com",
        "email": "cse.msu.edu;cse.msu.edu;cse.msu.edu;yale.edu;ge.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Michigan State University;Yale University;GE Global Research",
        "aff_unique_dep": ";;Machine Learning Lab",
        "aff_unique_url": "https://www.msu.edu;https://www.yale.edu;https://www.ge.com/research",
        "aff_unique_abbr": "MSU;Yale;GE Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "81499d682a",
        "title": "Semi-Supervised Domain Adaptation with Non-Parametric Copulas",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8e98d81f8217304975ccb23337bb5761-Abstract.html",
        "author": "David Lopez-paz; Jose M. Hern\u00e1ndez-lobato; Bernhard Sch\u00f6lkopf",
        "abstract": "A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model across different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques.",
        "bibtex": "@inproceedings{NIPS2012_8e98d81f,\n author = {Lopez-paz, David and Hern\\'{a}ndez-lobato, Jose and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-Supervised Domain Adaptation with Non-Parametric Copulas},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8e98d81f8217304975ccb23337bb5761-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8e98d81f8217304975ccb23337bb5761-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/8e98d81f8217304975ccb23337bb5761-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8e98d81f8217304975ccb23337bb5761-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 542648,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4753151448784736814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "MPI for Intelligent Systems; University of Cambridge; MPI for Intelligent Systems",
        "aff_domain": "tue.mpg.de;cam.ac.uk;tue.mpg.de",
        "email": "tue.mpg.de;cam.ac.uk;tue.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.cam.ac.uk",
        "aff_unique_abbr": "MPI-IS;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "a8071dd5af",
        "title": "Semi-supervised Eigenvectors for Locally-biased Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7e7e69ea3384874304911625ac34321c-Abstract.html",
        "author": "Toke Hansen; Michael W. Mahoney",
        "abstract": "In many applications, one has information, e.g., labels that are  provided in a semi-supervised manner, about a specific target region of a  large data set, and one wants to perform machine learning and data analysis  tasks nearby that pre-specified target region.   Locally-biased problems of this sort are particularly challenging for  popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities. In this paper, we address this issue by providing a methodology to construct  semi-supervised eigenvectors of a graph Laplacian, and we illustrate  how these locally-biased eigenvectors can be used to perform  locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized  directions of maximum variance, conditioned on being well-correlated with an  input seed set of nodes that is assumed to be provided in a semi-supervised  manner. We also provide several empirical examples demonstrating how these  semi-supervised eigenvectors can be used to perform locally-biased learning.",
        "bibtex": "@inproceedings{NIPS2012_7e7e69ea,\n author = {Hansen, Toke and Mahoney, Michael W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-supervised Eigenvectors for Locally-biased Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7e7e69ea3384874304911625ac34321c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7e7e69ea3384874304911625ac34321c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7e7e69ea3384874304911625ac34321c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5015043,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11123840202520720702&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Section for Cognitive Systems, DTU Informatics, Technical University of Denmark; Department of Mathematics, Stanford University, Stanford, CA 94305",
        "aff_domain": "imm.dtu.dk;cs.stanford.edu",
        "email": "imm.dtu.dk;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technical University of Denmark;Stanford University",
        "aff_unique_dep": "Section for Cognitive Systems, DTU Informatics;Department of Mathematics",
        "aff_unique_url": "https://www.dtu.dk;https://www.stanford.edu",
        "aff_unique_abbr": "DTU;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Denmark;United States"
    },
    {
        "id": "d87af32192",
        "title": "Semiparametric Principal Component Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a5bad363fc47f424ddf5091c8471480a-Abstract.html",
        "author": "Fang Han; Han Liu",
        "abstract": "We propose two new principal component analysis methods in this paper utilizing a semiparametric model. The according methods are named Copula Component Analysis (COCA) and Copula PCA. The semiparametric model assumes that, af- ter unspeci\ufb01ed marginally monotone transformations, the distributions are multi- variate Gaussian. The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian dis- tribution. The robust nonparametric rank-based correlation coef\ufb01cient estimator, Spearman\u2019s rho, is exploited in estimation. We prove that, under suitable condi- tions, although the marginal distributions can be arbitrarily continuous, the COCA and Copula PCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results. We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012).",
        "bibtex": "@inproceedings{NIPS2012_a5bad363,\n author = {Han, Fang and Liu, Han},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semiparametric Principal Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a5bad363fc47f424ddf5091c8471480a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a5bad363fc47f424ddf5091c8471480a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a5bad363fc47f424ddf5091c8471480a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5719032,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17260664787074831370&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Biostatistics, Johns Hopkins University; Department of Operations Research and Financial Engineering, Princeton University",
        "aff_domain": "jhsph.edu;princeton.edu",
        "email": "jhsph.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Johns Hopkins University;Princeton University",
        "aff_unique_dep": "Department of Biostatistics;Department of Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.jhu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "JHU;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f29563cf9a",
        "title": "Shifting Weights: Adapting Object Detectors from Image to Video",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/26e359e83860db1d11b6acca57d8ea88-Abstract.html",
        "author": "Kevin Tang; Vignesh Ramanathan; Li Fei-fei; Daphne Koller",
        "abstract": "Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection and LabelMe Video datasets that illustrate the benefit of our approach to adapt object detectors to video.",
        "bibtex": "@inproceedings{NIPS2012_26e359e8,\n author = {Tang, Kevin and Ramanathan, Vignesh and Fei-fei, Li and Koller, Daphne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Shifting Weights: Adapting Object Detectors from Image to Video},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/26e359e83860db1d11b6acca57d8ea88-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1627951,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13632135440925357838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computer Science Department, Stanford University; Department of Electrical Engineering, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0e0147ebed",
        "title": "Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4dcae38ee11d3a6606cc6cd636a3628b-Abstract.html",
        "author": "Piyush Rai; Abhishek Kumar; Hal Daume",
        "abstract": "Multiple-output regression models require estimating multiple functions, one for each output. To improve parameter estimation in such models, methods based on structural regularization of the model parameters are usually needed. In this paper, we present a multiple-output regression model that leverages the covariance structure of the functions (i.e., how the multiple functions are related with each other) as well as the conditional covariance structure of the outputs. This is in contrast with existing methods that usually take into account only one of these structures. More importantly, unlike most of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data. Several previously proposed structural regularization based  multiple-output regression models turn out to be special cases of our model. Moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs). Experimental results on both synthetic and real datasets demonstrate the effectiveness of our method.",
        "bibtex": "@inproceedings{NIPS2012_4dcae38e,\n author = {Rai, Piyush and Kumar, Abhishek and Daume, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4dcae38ee11d3a6606cc6cd636a3628b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4dcae38ee11d3a6606cc6cd636a3628b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4dcae38ee11d3a6606cc6cd636a3628b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 129102,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1327911741659285267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Science, University of Texas at Austin; Dept. of Computer Science, University of Maryland; Dept. of Computer Science, University of Maryland",
        "aff_domain": "cs.utexas.edu;cs.umd.edu;umiacs.umd.edu",
        "email": "cs.utexas.edu;cs.umd.edu;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Texas at Austin;University of Maryland",
        "aff_unique_dep": "Department of Computer Science;Dept. of Computer Science",
        "aff_unique_url": "https://www.utexas.edu;https://www/umd.edu",
        "aff_unique_abbr": "UT Austin;UMD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "52a1915056",
        "title": "Sketch-Based Linear Value Function Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c667d53acd899a97a85de0c201ba99be-Abstract.html",
        "author": "Marc Bellemare; Joel Veness; Michael Bowling",
        "abstract": "Hashing is a common method to reduce large, potentially infinite feature vectors to a fixed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and fifty-five Atari 2600 games to highlight the superior learning performance of tug-of-war hashing.",
        "bibtex": "@inproceedings{NIPS2012_c667d53a,\n author = {Bellemare, Marc and Veness, Joel and Bowling, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sketch-Based Linear Value Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c667d53acd899a97a85de0c201ba99be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 412995,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15664728805167722968&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Alberta; University of Alberta; University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "aa11361949",
        "title": "Slice Normalized Dynamic Markov Logic Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c8c41c4a18675a74e01c8a20e8a0f662-Abstract.html",
        "author": "Tivadar Papai; Henry Kautz; Daniel Stefankovic",
        "abstract": "Markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a Markov random field (MRF) or a conditional random field (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the domain of time points typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional random field for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues. It supports efficient online inference, and can directly model influences between variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks.",
        "bibtex": "@inproceedings{NIPS2012_c8c41c4a,\n author = {Papai, Tivadar and Kautz, Henry and Stefankovic, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Slice Normalized Dynamic Markov Logic Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c8c41c4a18675a74e01c8a20e8a0f662-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 166547,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9997510979558490022&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Rochester; Department of Computer Science, University of Rochester; Department of Computer Science, University of Rochester",
        "aff_domain": "cs.rochester.edu;cs.rochester.edu;cs.rochester.edu",
        "email": "cs.rochester.edu;cs.rochester.edu;cs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d2d6dfc39f",
        "title": "Slice sampling normalized kernel-weighted completely random measure mixture models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html",
        "author": "Nicholas Foti; Sinead Williamson",
        "abstract": "A number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality.  However, the inference algorithms are often slow and unwieldy, and are in general highly specific to a given model formulation. In this paper, we describe a wide class of nonparametric processes, including several existing models, and present a slice sampler that allows efficient inference across this class of models.",
        "bibtex": "@inproceedings{NIPS2012_8f1d4362,\n author = {Foti, Nick and Williamson, Sinead},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Slice sampling normalized kernel-weighted completely random measure mixture models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/8f1d43620bc6bb580df6e80b0dc05c48-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8f1d43620bc6bb580df6e80b0dc05c48-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 500069,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18235591050641318641&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Dartmouth College; Department of Machine Learning, Carnegie Mellon University",
        "aff_domain": "cs.dartmouth.edu;cs.cmu.edu",
        "email": "cs.dartmouth.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Dartmouth College;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science;Department of Machine Learning",
        "aff_unique_url": "https://dartmouth.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Dartmouth;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "46a15e6694",
        "title": "Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3bbfdde8842a5c44a0323518eec97cbe-Abstract.html",
        "author": "Ke Jiang; Brian Kulis; Michael I. Jordan",
        "abstract": "Links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics, i.e., letting the variance of particular distributions in a graphical model go to zero. For instance, in the context of clustering, such an approach yields precise connections between the k-means and EM algorithms.  In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models.  Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of Bayesian nonparametric models.  We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis.",
        "bibtex": "@inproceedings{NIPS2012_3bbfdde8,\n author = {Jiang, Ke and Kulis, Brian and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3bbfdde8842a5c44a0323518eec97cbe-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3bbfdde8842a5c44a0323518eec97cbe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3bbfdde8842a5c44a0323518eec97cbe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 779526,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2638391489198467686&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of CSE, The Ohio State University; Department of CSE, The Ohio State University; Departments of EECS and Statistics, University of California at Berkeley",
        "aff_domain": "cse.ohio-state.edu;cse.ohio-state.edu;cs.berkeley.edu",
        "email": "cse.ohio-state.edu;cse.ohio-state.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Ohio State University;University of California, Berkeley",
        "aff_unique_dep": "Department of CSE;Departments of EECS and Statistics",
        "aff_unique_url": "https://www.osu.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "OSU;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4b5829c1a6",
        "title": "Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/b112ca4087d668785e947a57493d1740-Abstract.html",
        "author": "Tuo Zhao; Kathryn Roeder; Han Liu",
        "abstract": "Many statistical methods gain robustness and exibility by sacricing convenient computational structure. In this paper, we illustrate this fundamental tradeoff by studying a semiparametric graphical model estimation problem. We explain how new computational techniques help to solve this type of problem. In particularly, we propose a smooth-projected neighborhood pursuit method for efciently estimating high dimensional nonparanormal graphs with theoretical guarantees. Besides new computational and theoretical analysis, we also provide an alternative view to analyze the tradeoff between computational efciency and statistical error under a smoothing optimization framework. We also report experimental results on text and stock datasets.",
        "bibtex": "@inproceedings{NIPS2012_b112ca40,\n author = {Zhao, Tuo and Roeder, Kathryn and Liu, Han},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/b112ca4087d668785e947a57493d1740-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/b112ca4087d668785e947a57493d1740-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/b112ca4087d668785e947a57493d1740-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1909900,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11799395486801266938&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7db0e62eab",
        "title": "Sparse Approximate Manifolds for Differential Geometric MCMC",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4476b929e30dd0c4e8bdbcc82c6ba23a-Abstract.html",
        "author": "Ben Calderhead; M\u00e1ty\u00e1s A. Sustik",
        "abstract": "One of the enduring challenges in Markov chain Monte Carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost. The recent introduction of locally adaptive MCMC methods based on the natural underlying Riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration. In this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid MCMC algorithm that extends the applicability of Riemannian Manifold MCMC methods to statistical models that do not admit an analytically computable metric tensor. Secondly, we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made. We demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust student-t error model, for which the expected Fisher Information is analytically intractable.",
        "bibtex": "@inproceedings{NIPS2012_4476b929,\n author = {Calderhead, Ben and Sustik, M\\'{a}ty\\'{a}s},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Approximate Manifolds for Differential Geometric MCMC},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 275884,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9006163574188164145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "CoMPLEX, University College London, London, WC1E 6BT, UK; Department of Computer Sciences, University of Texas at Austin, Austin, TX 78712, USA",
        "aff_domain": "ucl.ac.uk;cs.utexas.edu",
        "email": "ucl.ac.uk;cs.utexas.edu",
        "github": "",
        "project": "http://www.2020science.net/people/ben-calderhead",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;University of Texas at Austin",
        "aff_unique_dep": "CoMPLEX;Department of Computer Sciences",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.utexas.edu",
        "aff_unique_abbr": "UCL;UT Austin",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "London;Austin",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "aa6ba4edc5",
        "title": "Sparse Prediction with the $k$-Support Norm",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/99bcfcd754a98ce89cb86f73acc04645-Abstract.html",
        "author": "Andreas Argyriou; Rina Foygel; Nathan Srebro",
        "abstract": "We derive a novel norm that corresponds to the tightest convex   relaxation of sparsity combined with an $\\ell_2$ penalty. We show   that this new norm provides a tighter relaxation than the elastic   net, and is thus a good replacement for the Lasso or the elastic net   in sparse prediction problems.  But through studying our new norm,   we also bound the looseness of the elastic net, thus shedding new   light on it and providing justification for its use.",
        "bibtex": "@inproceedings{NIPS2012_99bcfcd7,\n author = {Argyriou, Andreas and Foygel, Rina and Srebro, Nathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Prediction with the k-Support Norm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/99bcfcd754a98ce89cb86f73acc04645-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/99bcfcd754a98ce89cb86f73acc04645-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/99bcfcd754a98ce89cb86f73acc04645-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 374085,
        "gs_citation": 196,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8058173355533771214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "\u00b4Ecole Centrale Paris; Department of Statistics, Stanford University; Toyota Technological Institute at Chicago",
        "aff_domain": "ecp.fr;stanford.edu;ttic.edu",
        "email": "ecp.fr;stanford.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Ecole Centrale Paris;Stanford University;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";Department of Statistics;",
        "aff_unique_url": "https://www.ecp.fr;https://www.stanford.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "ECP;Stanford;TTI Chicago",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Stanford;Chicago",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "7520a33d37",
        "title": "Spectral Learning of General Weighted Automata via Constrained Matrix Completion",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/700fdb2ba62d4554dc268c65add4b16e-Abstract.html",
        "author": "Borja Balle; Mehryar Mohri",
        "abstract": "Many tasks in text and speech processing and computational biology require es- timating functions mapping strings to real numbers. A broad class of such func- tions can be de\ufb01ned by weighted automata. Spectral methods based on the sin- gular value decomposition of a Hankel matrix have been recently proposed for learning a probability distribution represented by a weighted automaton from a training sample drawn according to this same target distribution. In this paper, we show how spectral methods can be extended to the problem of learning a general weighted automaton from a sample generated by an arbitrary distribution. The main obstruction to this approach is that, in general, some entries of the Hankel matrix may be missing. We present a solution to this problem based on solving a constrained matrix completion problem. Combining these two ingredients, matrix completion and spectral method, a whole new family of algorithms for learning general weighted automata is obtained. We present generalization bounds for a particular algorithm in this family. The proofs rely on a joint stability analysis of matrix completion and spectral learning.",
        "bibtex": "@inproceedings{NIPS2012_700fdb2b,\n author = {Balle, Borja and Mohri, Mehryar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Learning of General Weighted Automata via Constrained Matrix Completion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/700fdb2ba62d4554dc268c65add4b16e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/700fdb2ba62d4554dc268c65add4b16e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/700fdb2ba62d4554dc268c65add4b16e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/700fdb2ba62d4554dc268c65add4b16e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 307877,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12137788152862028876&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Universitat Polit `ecnica de Catalunya; Courant Institute and Google Research",
        "aff_domain": "lsi.upc.edu;cims.nyu.edu",
        "email": "lsi.upc.edu;cims.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universitat Polit\u00e8cnica de Catalunya;Courant Institute",
        "aff_unique_dep": ";Courant Institute",
        "aff_unique_url": "https://www.upc.edu;https://courant.nyu.edu",
        "aff_unique_abbr": "UPC;Courant",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Spain;United States"
    },
    {
        "id": "9e2d5d5175",
        "title": "Spectral learning of linear dynamics from generalised-linear observations with application to neural population data",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/d58072be2820e8682c0a27c0518e805e-Abstract.html",
        "author": "Lars Buesing; Jakob H. Macke; Maneesh Sahani",
        "abstract": "Latent linear dynamical systems with generalised-linear observation models arise in a variety of applications, for example when modelling the spiking activity of populations of neurons.  Here, we show how  spectral learning methods for linear systems with Gaussian observations   (usually called subspace identification in this context) can be extended to estimate the parameters of dynamical system models observed through non-Gaussian noise models. We use this approach to obtain estimates of parameters for a dynamical model of neural population data, where the observed spike-counts are Poisson-distributed with log-rates determined by the latent dynamical process, possibly driven by external inputs. We show that the extended system identification algorithm is consistent and accurately recovers the correct parameters on large simulated data sets with much smaller computational cost than approximate expectation-maximisation (EM) due to the non-iterative nature of subspace identification. Even on smaller data sets, it provides an effective initialization for EM, leading to more robust performance and faster convergence. These benefits are shown to extend to real neural data.",
        "bibtex": "@inproceedings{NIPS2012_d58072be,\n author = {Buesing, Lars and Macke, Jakob H and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral learning of linear dynamics from generalised-linear observations with application to neural population data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/d58072be2820e8682c0a27c0518e805e-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/d58072be2820e8682c0a27c0518e805e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/d58072be2820e8682c0a27c0518e805e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/d58072be2820e8682c0a27c0518e805e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 205316,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8792622705543783545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Gatsby Computational Neuroscience Unit, University College London, London, UK; Gatsby Computational Neuroscience Unit, University College London, London, UK + Max Planck Institute for Biological Cybernetics and Bernstein Center for Computational Neuroscience T\u00fcbingen; Gatsby Computational Neuroscience Unit, University College London, London, UK",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University College London;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Biological Cybernetics",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "UCL;MPIBC",
        "aff_campus_unique_index": "0;0+1;0",
        "aff_campus_unique": "London;T\u00fcbingen",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "60f7634e81",
        "title": "Spiking and saturating dendrites differentially expand single neuron computation capacity",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/63538fe6ef330c13a05a3ed7e599d5f7-Abstract.html",
        "author": "Romain Caz\u00e9; Mark Humphries; Boris S. Gutkin",
        "abstract": "The integration of excitatory inputs in dendrites is non-linear: multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input's response taken separately. If this depolarization is bigger than the arithmetic sum, the dendrite is spiking; if the depolarization is smaller, the dendrite is saturating. Decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity, as the neuron then maps onto a two layer neural network, enabling it to compute linearly non-separable Boolean functions (lnBFs). How can these lnBFs be implemented by dendritic architectures in practise? And can saturating dendrites equally expand computational capacity? To adress these questions we use a binary neuron model and Boolean algebra. First, we confirm that spiking dendrites enable a neuron to compute lnBFs using an architecture based on the disjunctive normal form (DNF). Second, we prove that saturating dendrites as well as spiking dendrites also enable a neuron to compute lnBFs using an architecture based on the conjunctive normal form (CNF). Contrary to the DNF-based architecture, a CNF-based architecture leads to a dendritic unit tuning that does not imply the neuron tuning, as has been observed experimentally. Third, we show that one cannot use a DNF-based architecture with saturating dendrites. Consequently, we show that an important family of lnBFs implemented with a CNF-architecture can require an exponential number of saturating dendritic units, whereas the same family implemented with either a DNF-architecture or a CNF-architecture always require a linear number of spiking dendritic unit. This minimization could explain why a neuron spends energetic resources to make its dendrites spike.",
        "bibtex": "@inproceedings{NIPS2012_63538fe6,\n author = {Caz\\'{e}, Romain and Humphries, Mark and Gutkin, Boris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spiking and saturating dendrites differentially expand single neuron computation capacity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/63538fe6ef330c13a05a3ed7e599d5f7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 341672,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11281580151815089321&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "INSERM U960, Paris Diderot, Paris 7, ENS; INSERM U960+University of Manchester; INSERM U960, CNRS, ENS",
        "aff_domain": "ens.fr;manchester.ac.uk;ens.fr",
        "email": "ens.fr;manchester.ac.uk;ens.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;3",
        "aff_unique_norm": "Paris Diderot University;Institut National de la Sant\u00e9 et de la Recherche M\u00e9dicale;University of Manchester;Institut National de la Sant\u00e9 et de la Recherche M\u00e9dicale (INSERM)",
        "aff_unique_dep": "INSERM U960;Unit\u00e9 960;;Unit\u00e9 960",
        "aff_unique_url": "https://www.parisdescartes.fr;https://www.inserm.fr;https://www.manchester.ac.uk;https://www.inserm.fr",
        "aff_unique_abbr": "Paris Diderot;INSERM;UoM;INSERM",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "France;United Kingdom"
    },
    {
        "id": "698d57e971",
        "title": "Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3435c378bb76d4357324dd7e69f3cd18-Abstract.html",
        "author": "Yanyan Lan; Jiafeng Guo; Xueqi Cheng; Tie-yan Liu",
        "abstract": "This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications.",
        "bibtex": "@inproceedings{NIPS2012_3435c378,\n author = {Lan, Yanyan and Guo, Jiafeng and Cheng, Xueqi and Liu, Tie-yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3435c378bb76d4357324dd7e69f3cd18-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 202461,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3538423410535404243&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Microsoft Research Asia",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;microsoft.com",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Chinese Academy of Sciences;Microsoft",
        "aff_unique_dep": "Institute of Computing Technology;Research",
        "aff_unique_url": "http://www.ict.ac.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "CAS;MSR Asia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "0a8acced57",
        "title": "Stochastic Gradient Descent with Only One Projection",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c52f1bd66cc19d05628bd8bf27af3ad6-Abstract.html",
        "author": "Mehrdad Mahdavi; Tianbao Yang; Rong Jin; Shenghuo Zhu; Jinfeng Yi",
        "abstract": "Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\\it each} iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $O(1/\\sqrt{T})$ convergence rate for general convex optimization, and an $O(\\ln T/T)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function.",
        "bibtex": "@inproceedings{NIPS2012_c52f1bd6,\n author = {Mahdavi, Mehrdad and Yang, Tianbao and Jin, Rong and Zhu, Shenghuo and Yi, Jinfeng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Gradient Descent with Only One Projection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c52f1bd66cc19d05628bd8bf27af3ad6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c52f1bd66cc19d05628bd8bf27af3ad6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 282193,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7693167522736087202&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a831191bd8",
        "title": "Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html",
        "author": "Alekh Agarwal; Sahand Negahban; Martin J. Wainwright",
        "abstract": "We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a $\\order(\\pdim/T)$ convergence rate for strongly convex objectives in $\\pdim$ dimensions and $\\order(\\sqrt{\\spindex( \\log\\pdim)/T})$ convergence rate when the optimum is $\\spindex$-sparse. Our algorithm is based on successively solving a series of $\\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error of our solution after $T$ iterations is at most $\\order(\\spindex(\\log\\pdim)/T)$, with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem.",
        "bibtex": "@inproceedings{NIPS2012_5751ec3e,\n author = {Agarwal, Alekh and Negahban, Sahand and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/5751ec3e9a4feab575962e78e006250d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 257694,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14392520029194155947&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Microsoft Research, New York NY; Dept. of EECS, MIT; Dept. of EECS and Statistics, UCBerkeley",
        "aff_domain": "microsoft.com;mit.edu;stat.berkeley.edu",
        "email": "microsoft.com;mit.edu;stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft;Massachusetts Institute of Technology;University of California, Berkeley",
        "aff_unique_dep": "Microsoft Research;Department of Electrical Engineering and Computer Science;Department of Electrical Engineering and Computer Sciences and Department of Statistics",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://web.mit.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "MSR;MIT;UC Berkeley",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "New York;Cambridge;Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ab577921eb",
        "title": "Strategic Impatience in Go/NoGo versus Forced-Choice Decision-Making",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6d70cb65d15211726dcce4c0e971e21c-Abstract.html",
        "author": "Pradeep Shenoy; Angela J. Yu",
        "abstract": "Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes.",
        "bibtex": "@inproceedings{NIPS2012_6d70cb65,\n author = {Shenoy, Pradeep and Yu, Angela J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Strategic Impatience in Go/NoGo versus Forced-Choice Decision-Making},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6d70cb65d15211726dcce4c0e971e21c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 259954,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8326992758501570518&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ffd7b14721",
        "title": "Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6ba1085b788407963fe0e89c699a7396-Abstract.html",
        "author": "Po-ling Loh; Martin J. Wainwright",
        "abstract": "We investigate a curious relationship between the structure of a discrete graphical model and the support of the inverse of a generalized covariance matrix. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph re\ufb02ects the conditional independence structure of the graph. Our work extends results that have previously been es- tablished only in the context of multivariate Gaussian graphical models, thereby addressing an open question about the signi\ufb01cance of the inverse covariance ma- trix of a non-Gaussian distribution. Based on our population-level results, we show how the graphical Lasso may be used to recover the edge structure of cer- tain classes of discrete graphical models, and present simulations to verify our theoretical results.",
        "bibtex": "@inproceedings{NIPS2012_6ba1085b,\n author = {Loh, Po-ling and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6ba1085b788407963fe0e89c699a7396-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6ba1085b788407963fe0e89c699a7396-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6ba1085b788407963fe0e89c699a7396-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 347015,
        "gs_citation": 229,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17208740070338258967&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Statistics, University of California, Berkeley; Departments of Statistics and EECS, University of California, Berkeley",
        "aff_domain": "berkeley.edu;stat.berkeley.edu",
        "email": "berkeley.edu;stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6d4b509ba0",
        "title": "Structured Learning of Gaussian Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/5e51eeda0422de44a7cc260b4239d4f9-Abstract.html",
        "author": "Karthik Mohan; Mike Chung; Seungyeop Han; Daniela Witten; Su-in Lee; Maryam Fazel",
        "abstract": "We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data.",
        "bibtex": "@inproceedings{NIPS2012_5e51eeda,\n author = {Mohan, Karthik and Chung, Mike and Han, Seungyeop and Witten, Daniela and Lee, Su-in and Fazel, Maryam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured Learning of Gaussian Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/5e51eeda0422de44a7cc260b4239d4f9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/5e51eeda0422de44a7cc260b4239d4f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/5e51eeda0422de44a7cc260b4239d4f9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1887993,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3257586432378973752&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Electrical Engineering, Univ. of Washington; Computer Science and Engineering, Univ. of Washington; Computer Science and Engineering, Univ. of Washington; Biostatistics, Univ. of Washington; Computer Science and Engineering, and Genome Sciences, Univ. of Washington; Electrical Engineering, Univ. of Washington",
        "aff_domain": "uw.edu;cs.washington.edu;cs.washington.edu;uw.edu;uw.edu;uw.edu",
        "email": "uw.edu;cs.washington.edu;cs.washington.edu;uw.edu;uw.edu;uw.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Electrical Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "865a73d6a7",
        "title": "Submodular-Bregman and the Lov\u00e1sz-Bregman Divergences with Applications",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/dba1cdfcf6359389d170caadb3223ad2-Abstract.html",
        "author": "Rishabh Iyer; Jeff A. Bilmes",
        "abstract": "We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular-Bregman divergences. We consider two kinds, de\ufb01ned either from tight modular upper or tight modular lower bounds of a submodular function. We show that the properties of these divergences are analogous to the (standard continuous) Bregman divergence. We demonstrate how they generalize many useful divergences, including the weighted Hamming distance, squared weighted Hamming, weighted precision, recall, conditional mutual information, and a generalized KL-divergence on sets. We also show that the generalized Bregman divergence on the Lov\u00b4asz extension of a submodular function, which we call the Lov\u00b4asz-Bregman divergence, is a continuous extension of a submodular Bregman divergence. We point out a number of applications, and in particular show that a proximal algorithm de\ufb01ned through the submodular Bregman divergence pro- vides a framework for many mirror-descent style algorithms related to submodular function optimization. We also show that a generalization of the k-means algorithm using the Lov\u00b4asz Bregman divergence is natural in clustering scenarios where ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely ef\ufb01cient unlike other order based distance measures.",
        "bibtex": "@inproceedings{NIPS2012_dba1cdfc,\n author = {Iyer, Rishabh and Bilmes, Jeff A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Submodular-Bregman and the Lov\\'{a}sz-Bregman Divergences with Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/dba1cdfcf6359389d170caadb3223ad2-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/dba1cdfcf6359389d170caadb3223ad2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/dba1cdfcf6359389d170caadb3223ad2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 384531,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7012356716789055485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering, University of Washington; Department of Electrical Engineering, University of Washington",
        "aff_domain": "u.washington.edu;uw.edu",
        "email": "u.washington.edu;uw.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "15eb2e6a75",
        "title": "Super-Bit Locality-Sensitive Hashing",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html",
        "author": "Jianqiu Ji; Jianmin Li; Shuicheng Yan; Bo Zhang; Qi Tian",
        "abstract": "Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\\pi/2]$. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments.",
        "bibtex": "@inproceedings{NIPS2012_072b030b,\n author = {Ji, Jianqiu and Li, Jianmin and Yan, Shuicheng and Zhang, Bo and Tian, Qi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Super-Bit Locality-Sensitive Hashing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/072b030ba126b2f4b2374f342be9ed44-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/072b030ba126b2f4b2374f342be9ed44-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 475958,
        "gs_citation": 166,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2795291381060175702&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, 117576; State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Department of Computer Science, University of Texas at San Antonio, One UTSA Circle, University of Texas at San Antonio, San Antonio, TX 78249-1644",
        "aff_domain": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn;nus.edu.sg;cs.utsa.edu",
        "email": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn;nus.edu.sg;cs.utsa.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "Tsinghua University;National University of Singapore;University of Texas at San Antonio",
        "aff_unique_dep": "Department of Computer Science and Technology;Department of Electrical and Computer Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.nus.edu.sg;https://www.utsa.edu",
        "aff_unique_abbr": "THU;NUS;UTSA",
        "aff_campus_unique_index": "0;0;0;2",
        "aff_campus_unique": "Beijing;;San Antonio",
        "aff_country_unique_index": "0;0;1;0;2",
        "aff_country_unique": "China;Singapore;United States"
    },
    {
        "id": "f5316d0cd6",
        "title": "Supervised Learning with Similarity Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/202cb962ac59075b964b07152d234b70-Abstract.html",
        "author": "Purushottam Kar; Prateek Jain",
        "abstract": "We address the problem of general supervised learning when data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multiclass classification problems. We propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classification. We give a ''goodness'' criterion for similarity functions w.r.t. a given supervised learning task and then adapt a well-known landmarking technique to provide efficient algorithms for supervised learning using ''good'' similarity functions. We demonstrate the effectiveness of our model on three important supervised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error. Furthermore, for the case of real-valued regression, we give a natural goodness definition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs.",
        "bibtex": "@inproceedings{NIPS2012_202cb962,\n author = {Kar, Purushottam and Jain, Prateek},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Supervised Learning with Similarity Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/202cb962ac59075b964b07152d234b70-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/202cb962ac59075b964b07152d234b70-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/202cb962ac59075b964b07152d234b70-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/202cb962ac59075b964b07152d234b70-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 711028,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15626287845335150140&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Indian Institute of Technology; Microsoft Research Lab",
        "aff_domain": "cse.iitk.ac.in;microsoft.com",
        "email": "cse.iitk.ac.in;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Indian Institute of Technology;Microsoft",
        "aff_unique_dep": ";Research Lab",
        "aff_unique_url": "https://www.iit.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "IIT;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "db59391dee",
        "title": "Symbolic Dynamic Programming for Continuous State and Observation POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/2dea61eed4bceec564a00115c4d21334-Abstract.html",
        "author": "Zahra Zamani; Scott Sanner; Pascal Poupart; Kristian Kersting",
        "abstract": "Partially-observable Markov decision processes (POMDPs) provide a powerful model for real-world sequential decision-making problems. In recent years, point- based value iteration methods have proven to be extremely effective techniques for \ufb01nding (approximately) optimal dynamic programming solutions to POMDPs when an initial set of belief states is known. However, no point-based work has provided exact point-based backups for both continuous state and observation spaces, which we tackle in this paper. Our key insight is that while there may be an in\ufb01nite number of possible observations, there are only a \ufb01nite number of observation partitionings that are relevant for optimal decision-making when a \ufb01nite, \ufb01xed set of reachable belief states is known. To this end, we make two important contributions: (1) we show how previous exact symbolic dynamic pro- gramming solutions for continuous state MDPs can be generalized to continu- ous state POMDPs with discrete observations, and (2) we show how this solution can be further extended via recently developed symbolic methods to continuous state and observations to derive the minimal relevant observation partitioning for potentially correlated, multivariate observation spaces. We demonstrate proof-of- concept results on uni- and multi-variate state and observation steam plant control.",
        "bibtex": "@inproceedings{NIPS2012_2dea61ee,\n author = {Zamani, Zahra and Sanner, Scott and Poupart, Pascal and Kersting, Kristian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Symbolic Dynamic Programming for Continuous State and Observation POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/2dea61eed4bceec564a00115c4d21334-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/2dea61eed4bceec564a00115c4d21334-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/2dea61eed4bceec564a00115c4d21334-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 408507,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15651957064149748459&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "ANU & NICTA; NICTA & ANU; U. of Waterloo; Fraunhofer IAIS & U. of Bonn",
        "aff_domain": "anu.edu.au;nicta.com.au;uwaterloo.ca;iais.fraunhofer.de",
        "email": "anu.edu.au;nicta.com.au;uwaterloo.ca;iais.fraunhofer.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Australian National University;National Information and Communications Technology Australia;University of Waterloo;Fraunhofer IAIS",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au;https://uwaterloo.ca;https://www.iais.fraunhofer.de/",
        "aff_unique_abbr": "ANU;NICTA;UW;Fraunhofer IAIS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2",
        "aff_country_unique": "Australia;Canada;Germany"
    },
    {
        "id": "584ff00078",
        "title": "Symmetric Correspondence Topic Models for Multilingual Text Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/6766aa2750c19aad2fa1b32f36ed4aee-Abstract.html",
        "author": "Kosuke Fukumasu; Koji Eguchi; Eric P. Xing",
        "abstract": "Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models.",
        "bibtex": "@inproceedings{NIPS2012_6766aa27,\n author = {Fukumasu, Kosuke and Eguchi, Koji and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Symmetric Correspondence Topic Models for Multilingual Text Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/6766aa2750c19aad2fa1b32f36ed4aee-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 221105,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=139379296307064281&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Graduate School of System Informatics, Kobe University, Kobe 657-8501, Japan; Graduate School of System Informatics, Kobe University, Kobe 657-8501, Japan; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA",
        "aff_domain": "cs25.scitec.kobe-u.ac.jp;port.kobe-u.ac.jp;cs.cmu.edu",
        "email": "cs25.scitec.kobe-u.ac.jp;port.kobe-u.ac.jp;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Kobe University;Carnegie Mellon University",
        "aff_unique_dep": "Graduate School of System Informatics;School of Computer Science",
        "aff_unique_url": "https://www.kobe-u.ac.jp;https://www.cmu.edu",
        "aff_unique_abbr": "Kobe U;CMU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Kobe;Pittsburgh",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "f239c970e4",
        "title": "Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Abstract.html",
        "author": "Jake Bouvrie; Jean-jeacques Slotine",
        "abstract": "To learn reliable rules that can generalize to novel situations, the brain must be capable of imposing some form of regularization. Here we suggest, through theoretical and computational arguments, that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system. The functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise. Noise on the inputs is shown to impose regularization, and when synchronization upstream induces time-varying correlations across noise variables, the degree of regularization can be calibrated over time. The resulting qualitative behavior matches experimental data from visual cortex.",
        "bibtex": "@inproceedings{NIPS2012_f8c1f23d,\n author = {Bouvrie, Jake and Slotine, Jean-jeacques},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 354261,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7517584705109143010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Mathematics, Duke University; Nonlinear Systems Laboratory, Massachusetts Institute of Technology",
        "aff_domain": "math.duke.edu;mit.edu",
        "email": "math.duke.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Duke University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Mathematics;Nonlinear Systems Laboratory",
        "aff_unique_url": "https://www.duke.edu;https://web.mit.edu",
        "aff_unique_abbr": "Duke;MIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fde1c84b46",
        "title": "Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a58149d355f02887dfbe55ebb2b64ba3-Abstract.html",
        "author": "Michael Collins; Shay B. Cohen",
        "abstract": "We describe an approach to speed-up inference with latent variable PCFGs, which have been shown to  be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature.  We also describe an error bound for this approximation, which bounds the difference between the probabilities calculated by the algorithm and the true probabilities that the approximated model gives. Empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance.",
        "bibtex": "@inproceedings{NIPS2012_a58149d3,\n author = {Collins, Michael and Cohen, Shay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a58149d355f02887dfbe55ebb2b64ba3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a58149d355f02887dfbe55ebb2b64ba3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a58149d355f02887dfbe55ebb2b64ba3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 310270,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8583349244987890116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science, Columbia University; Department of Computer Science, Columbia University",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "60f10f164f",
        "title": "The Bethe Partition Function of Log-supermodular Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/03afdbd66e7929b125f8597834fa83a4-Abstract.html",
        "author": "Nicholas Ruozzi",
        "abstract": "Sudderth, Wainwright, and Willsky conjectured that the Bethe approximation corresponding to any fixed point of the belief propagation algorithm over an attractive, pairwise binary graphical model provides a lower bound on the true partition function. In this work, we resolve this conjecture in the affirmative by demonstrating that, for any graphical model with binary variables whose potential functions (not necessarily pairwise) are all log-supermodular, the Bethe partition function always lower bounds the true partition function.  The proof of this result follows from a new variant of the \u201cfour functions\u201d theorem that may be of independent interest.",
        "bibtex": "@inproceedings{NIPS2012_03afdbd6,\n author = {Ruozzi, Nicholas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Bethe Partition Function of Log-supermodular Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/03afdbd66e7929b125f8597834fa83a4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 243180,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6141720196960580663&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Communication Theory Laboratory, EPFL, Lausanne, Switzerland",
        "aff_domain": "epfl.ch",
        "email": "epfl.ch",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "Communication Theory Laboratory",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lausanne",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "edfc657066",
        "title": "The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/5c936263f3428a40227908d5a3847c0b-Abstract.html",
        "author": "Simon Lyons; Amos J. Storkey; Simo S\u00e4rkk\u00e4",
        "abstract": "Stochastic differential equations (SDE) are a natural tool for modelling systems that are inherently noisy or contain uncertainties that can be modelled as stochastic processes. Crucial to the process of using SDE to build mathematical models is the ability to estimate parameters of those models from observed data. Over the past few decades, significant progress has been made on this problem, but we are still far from having a definitive solution. We describe a novel method of approximating a diffusion process that we show to be useful in Markov chain Monte-Carlo (MCMC) inference algorithms. We take the \u2018white\u2019 noise that drives a diffusion process and decompose it into two terms. The first is a \u2018coloured noise\u2019 term that can be deterministically controlled by a set of auxilliary variables. The second term is small and enables us to form a linear Gaussian \u2018small noise\u2019 approximation. The decomposition allows us to take a diffusion process of interest and cast it in a form that is amenable to sampling by MCMC methods. We explain why many state-of-the-art inference methods fail on highly nonlinear inference problems. We demonstrate experimentally that our method performs well in such situations. Our results show that this method is a promising new tool for use in inference and parameter estimation problems.",
        "bibtex": "@inproceedings{NIPS2012_5c936263,\n author = {Lyons, Simon and Storkey, Amos J and S\\\"{a}rkk\\\"{a}, Simo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/5c936263f3428a40227908d5a3847c0b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/5c936263f3428a40227908d5a3847c0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 364453,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14246788420902542383&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Informatics, University of Edinburgh; Aalto University, Department of Biomedical Engineering and Computational Science; School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;aalto.fi;ed.ac.uk",
        "email": "sms.ed.ac.uk;aalto.fi;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Edinburgh;Aalto University",
        "aff_unique_dep": "School of Informatics;Department of Biomedical Engineering and Computational Science",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.aalto.fi",
        "aff_unique_abbr": "Edinburgh;Aalto",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;Finland"
    },
    {
        "id": "4b75a387ab",
        "title": "The Lov\u00e1sz \u03d1 function, SVMs and finding large dense subgraphs",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8eefcfdf5990e441f0fb6f3fad709e21-Abstract.html",
        "author": "Vinay Jethava; Anders Martinsson; Chiranjib Bhattacharyya; Devdatt Dubhashi",
        "abstract": "The Lovasz $\\theta$ function of a graph, is a fundamental tool in combinatorial optimization and approximation algorithms.  Computing $\\theta$ involves solving a SDP  and is extremely expensive even for moderately sized graphs.  In this paper we establish that the Lovasz $\\theta$ function is equivalent to  a kernel learning problem related to one class SVM. This interesting connection opens up many opportunities  bridging graph theoretic algorithms and machine learning.   We show that there exist graphs, which we call $SVM-\\theta$ graphs, on which the Lovasz $\\theta$ function can be approximated well by a one-class  SVM.    This leads to a novel use of SVM techniques to solve algorithmic problems in large graphs e.g. identifying a planted clique  of size $\\Theta({\\sqrt{n}})$ in a random graph $G(n,\\frac{1}{2})$. A classic approach for this problem involves computing  the $\\theta$ function, however it is not scalable due to SDP computation. We show that the random graph with a planted clique is an example of $SVM-\\theta$ graph, and as a consequence a SVM based approach  easily identifies the clique in large graphs and is competitive with the  state-of-the-art.    Further, we introduce  the notion of a ''common orthogonal labeling'' which extends the notion  of a ''orthogonal labelling of a single  graph (used in defining the $\\theta$ function)  to multiple graphs.  The problem of finding the optimal common orthogonal labelling is cast as a  Multiple Kernel Learning problem and is used to identify a large common dense region in multiple graphs.  The proposed algorithm achieves an order of magnitude scalability compared to the state of the art.",
        "bibtex": "@inproceedings{NIPS2012_8eefcfdf,\n author = {Jethava, Vinay and Martinsson, Anders and Bhattacharyya, Chiranjib and Dubhashi, Devdatt},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Lov\\'{a}sz \\textvartheta  function, SVMs and finding large dense subgraphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8eefcfdf5990e441f0fb6f3fad709e21-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 361486,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5212387686622159729&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science & Engineering Department, Chalmers University of Technology; Department of Mathematics, Chalmers University of Technology; Department of CSA, Indian Institute of Science; Computer Science & Engineering Department, Chalmers University of Technology",
        "aff_domain": "chalmers.se;student.chalmers.se;csa.iisc.ernet.in;chalmers.se",
        "email": "chalmers.se;student.chalmers.se;csa.iisc.ernet.in;chalmers.se",
        "github": "",
        "project": "http://www.cse.chalmers.se/~jethava/svm-theta.html",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Chalmers University of Technology;Indian Institute of Science",
        "aff_unique_dep": "Computer Science & Engineering Department;Department of CSA",
        "aff_unique_url": "https://www.chalmers.se;https://www.iisc.ac.in",
        "aff_unique_abbr": "Chalmers;IISc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Sweden;India"
    },
    {
        "id": "a9cbf8ccc3",
        "title": "The Perturbed Variation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/168908dd3227b8358eababa07fcaf091-Abstract.html",
        "author": "Maayan Harel; Shie Mannor",
        "abstract": "We introduce a new discrepancy score between two distributions that gives an indication on their \\emph{similarity}. While much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions. The new score gives an intuitive interpretation of similarity;  it optimally perturbs the distributions so that they best fit each other. The score is defined between distributions, and can be efficiently estimated from samples. We provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions. The statistical power of this procedures is presented in simulations. We also compare the score's capacity to detect similarity with that of other known measures on real data.",
        "bibtex": "@inproceedings{NIPS2012_168908dd,\n author = {Harel, Maayan and Mannor, Shie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Perturbed Variation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/168908dd3227b8358eababa07fcaf091-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/168908dd3227b8358eababa07fcaf091-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/168908dd3227b8358eababa07fcaf091-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/168908dd3227b8358eababa07fcaf091-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 402785,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13433061072921885748&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Technion, Haifa, Israel; Technion, Haifa, Israel",
        "aff_domain": "tx.technion.ac.il;ee.technion.ac.il",
        "email": "tx.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "3c7c0b47c5",
        "title": "The Time-Marginalized Coalescent Prior for Hierarchical Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c73dfe6c630edb4c1692db67c510f65c-Abstract.html",
        "author": "Levi Boyles; Max Welling",
        "abstract": "We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clustering. The prior is constructed by marginalizing out the time information of Kingman\u2019s coalescent, providing a prior over tree structures which we call the Time-Marginalized Coalescent (TMC). This allows for models which factorize the tree structure and times, providing two benefits: more flexible priors may be constructed and more efficient Gibbs type inference can be used. We demonstrate this on an example model for density estimation and show the TMC achieves competitive experimental results.",
        "bibtex": "@inproceedings{NIPS2012_c73dfe6c,\n author = {Boyles, Levi and Welling, Max},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Time-Marginalized Coalescent Prior for Hierarchical Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c73dfe6c630edb4c1692db67c510f65c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c73dfe6c630edb4c1692db67c510f65c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/c73dfe6c630edb4c1692db67c510f65c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c73dfe6c630edb4c1692db67c510f65c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 718361,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8914417108233754251&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine",
        "aff_domain": "uci.edu;uci.edu",
        "email": "uci.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6d620e2a34",
        "title": "The representer theorem for Hilbert spaces: a necessary and sufficient condition",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/eb160de1de89d9058fcb0b968dbbbd68-Abstract.html",
        "author": "Francesco Dinuzzo; Bernhard Sch\u00f6lkopf",
        "abstract": "The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the finite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufficiently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data.",
        "bibtex": "@inproceedings{NIPS2012_eb160de1,\n author = {Dinuzzo, Francesco and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The representer theorem for Hilbert spaces: a necessary and sufficient condition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/eb160de1de89d9058fcb0b968dbbbd68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 207537,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1026473944260752675&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a09f20e343",
        "title": "The topographic unsupervised learning of natural sounds in the auditory cortex",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/47a658229eb2368a99f1d032c8848542-Abstract.html",
        "author": "Hiroki Terashima; Masato Okada",
        "abstract": "The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efficient coding strategy and that the disorder in A1 reflects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reflected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.",
        "bibtex": "@inproceedings{NIPS2012_47a65822,\n author = {Terashima, Hiroki and Okada, Masato},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The topographic unsupervised learning of natural sounds in the auditory cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/47a658229eb2368a99f1d032c8848542-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/47a658229eb2368a99f1d032c8848542-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/47a658229eb2368a99f1d032c8848542-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6155087,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4682836295204897443&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "The University of Tokyo/ JSPS; The University of Tokyo/ RIKEN BSI",
        "aff_domain": "teratti.jp;k.u-tokyo.ac.jp",
        "email": "teratti.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "e0a25a5b1d",
        "title": "The variational hierarchical EM algorithm for clustering hidden Markov models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7eabe3a1649ffa2b3ff8c02ebfd5659f-Abstract.html",
        "author": "Emanuele Coviello; Gert R. Lanckriet; Antoni B. Chan",
        "abstract": "In this paper, we derive a novel algorithm to cluster  hidden Markov models (HMMs) according to their probability distributions. We propose a variational hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ``cluster center'', i.e., a novel HMM that is representative for the group. We illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging.",
        "bibtex": "@inproceedings{NIPS2012_7eabe3a1,\n author = {Coviello, Emanuele and Lanckriet, Gert and Chan, Antoni},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The variational hierarchical EM algorithm for clustering hidden Markov models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 371971,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17164609347765314758&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "ECE Dept., UC San Diego; CS Dept., CityU of Hong Kong; ECE Dept., UC San Diego",
        "aff_domain": "ucsd.edu;cityu.edu.hk;ece.ucsd.edu",
        "email": "ucsd.edu;cityu.edu.hk;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;City University of Hong Kong",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.ucsd.edu;https://www.cityu.edu.hk",
        "aff_unique_abbr": "UCSD;CityU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "San Diego;Hong Kong SAR",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "36b4fb6904",
        "title": "Tight Bounds on Profile Redundancy and Distinguishability",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/7c33e57e3dbd8a52940fa1a963aa4a4a-Abstract.html",
        "author": "Jayadev Acharya; Hirakendu Das; Alon Orlitsky",
        "abstract": "The minimax KL-divergence of any distribution from all distributions in a collection P has several practical implications. In compression, it is called redundancy and represents the least additional number of bits over the entropy needed to encode the output of any distribution in P. In online es- timation and learning, it is the lowest expected log-loss regret when guessing a sequence of random values generated by a distribution in P. In hypothesis testing, it upper bounds the largest number of distinguishable distributions in P. Motivated by problems ranging from population estimation to text classi\ufb01cation and speech recognition, several machine-learning and information-theory researchers have recently considered label-invariant observations and properties induced by i.i.d. distributions. A suf\ufb01cient statistic for all these properties is the data\u2019s pro\ufb01le, the multiset of the number of times each data element appears. Improving on a sequence of previous works, we show that the redun- dancy of the collection of distributions induced over pro\ufb01les by length-n i.i.d. sequences is between 0.3 \u00b7 n1/3 and n1/3 log2 n, in particular, establishing its exact growth power.",
        "bibtex": "@inproceedings{NIPS2012_7c33e57e,\n author = {Acharya, Jayadev and Das, Hirakendu and Orlitsky, Alon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tight Bounds on Profile Redundancy and Distinguishability},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/7c33e57e3dbd8a52940fa1a963aa4a4a-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/7c33e57e3dbd8a52940fa1a963aa4a4a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/7c33e57e3dbd8a52940fa1a963aa4a4a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/7c33e57e3dbd8a52940fa1a963aa4a4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 253111,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15951747247738093115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "ECE, UCSD; Yahoo!; ECE & CSE, UCSD",
        "aff_domain": "ucsd.edu;yahoo-inc.com;ucsd.edu",
        "email": "ucsd.edu;yahoo-inc.com;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;Yahoo!",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.yahoo.com",
        "aff_unique_abbr": "UCSD;Yahoo!",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c28ce84f56",
        "title": "Timely Object Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0deb1c54814305ca9ad266f53bc82511-Abstract.html",
        "author": "Sergey Karayev; Tobias Baumgartner; Mario Fritz; Trevor Darrell",
        "abstract": "In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the eminent PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains $66\\%$ better AP than a random ordering, and $14\\%$ better performance than an intelligent baseline. On the timeliness measure, our method obtains at least $11\\%$ better performance. Our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning.",
        "bibtex": "@inproceedings{NIPS2012_0deb1c54,\n author = {Karayev, Sergey and Baumgartner, Tobias and Fritz, Mario and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Timely Object Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0deb1c54814305ca9ad266f53bc82511-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 7977719,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11937597298171821850&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c54e19a37b",
        "title": "Topic-Partitioned Multinetwork Embeddings",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/0e9fa1f3e9e66792401a6972d477dcc3-Abstract.html",
        "author": "Peter Krafft; Juston Moore; Bruce Desmarais; Hanna M. Wallach",
        "abstract": "We introduce a joint model of network content and context designed for exploratory analysis of email networks via visualization of topic-specific communication patterns. Our model is an admixture model for text and network attributes which uses multinomial distributions over words as mixture components for explaining text and latent Euclidean positions of actors as mixture components for explaining network attributes.  We validate the appropriateness of our model by achieving state-of-the-art performance on a link prediction task and by achieving semantic coherence equivalent to that of latent Dirichlet allocation. We demonstrate the capability of our model for descriptive, explanatory, and exploratory analysis by investigating the inferred topic-specific communication patterns of a new government email dataset, the New Hanover County email corpus.",
        "bibtex": "@inproceedings{NIPS2012_0e9fa1f3,\n author = {Krafft, Peter and Moore, Juston and Desmarais, Bruce and Wallach, Hanna},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Topic-Partitioned Multinetwork Embeddings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/0e9fa1f3e9e66792401a6972d477dcc3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/0e9fa1f3e9e66792401a6972d477dcc3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1118849,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10550717280801752904&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "CSAIL, MIT; Department of Computer Science, University of Massachusetts Amherst + Department of Political Science, University of Massachusetts Amherst; Department of Political Science, University of Massachusetts Amherst; Department of Computer Science, University of Massachusetts Amherst",
        "aff_domain": "mit.edu;cs.umass.edu;polsci.umass.edu;cs.umass.edu",
        "email": "mit.edu;cs.umass.edu;polsci.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+1;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Massachusetts Amherst",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;Department of Computer Science",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.umass.edu",
        "aff_unique_abbr": "MIT;UMass Amherst",
        "aff_campus_unique_index": "0;1+1;1;1",
        "aff_campus_unique": "Cambridge;Amherst",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "77a07e2c12",
        "title": "Topology Constraints in Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html",
        "author": "Marcelo Fiori; Pablo Mus\u00e9; Guillermo Sapiro",
        "abstract": "Graphical models are a very useful tool to describe and understand natural phenomena, from gene expression to climate change and social interactions. The topological structure of these graphs/networks is a fundamental part of the analysis, and in many cases the main goal of the study. However, little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data. In this work we propose extensions to the basic joint regression model for network estimation, which explicitly incorporate graph-topological constraints into the corresponding optimization approach. The first proposed extension includes an eigenvector centrality constraint, thereby promoting this important prior topological property. The second developed extension promotes the formation of certain motifs, triangle-shaped ones in particular, which are known to exist for example in genetic regulatory networks. The presentation of the underlying formulations, which serve as examples of the introduction of topological constraints in network estimation, is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge.",
        "bibtex": "@inproceedings{NIPS2012_00411460,\n author = {Fiori, Marcelo and Mus\\'{e}, Pablo and Sapiro, Guillermo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Topology Constraints in Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/00411460f7c92d2124a67ea0f4cb5f85-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1569103,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2488523281650130719&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Universidad de la Rep\u00b4ublica, Uruguay; Universidad de la Rep\u00b4ublica, Uruguay; Duke University, Durham, NC 27708",
        "aff_domain": "fing.edu.uy;fing.edu.uy;duke.edu",
        "email": "fing.edu.uy;fing.edu.uy;duke.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Universidad de la Rep\u00fablica;Duke University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unorte.edu.uy;https://www.duke.edu",
        "aff_unique_abbr": "Udelar;Duke",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Durham",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Uruguay;United States"
    },
    {
        "id": "7666bd4c2f",
        "title": "Towards a learning-theoretic analysis of spike-timing dependent plasticity",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/f47330643ae134ca204bf6b2481fec47-Abstract.html",
        "author": "David Balduzzi; Michel Besserve",
        "abstract": "This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli.",
        "bibtex": "@inproceedings{NIPS2012_f4733064,\n author = {Balduzzi, David and Besserve, Michel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Towards a learning-theoretic analysis of spike-timing dependent plasticity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/f47330643ae134ca204bf6b2481fec47-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/f47330643ae134ca204bf6b2481fec47-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 444065,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9519405670469149921&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "MPI for Intelligent Systems, T\u00fcbingen, Germany+ETH Zurich, Switzerland; MPI for Intelligent Systems and MPI for Biological Cybernetics T\u00fcbingen, Germany",
        "aff_domain": "inf.ethz.ch;tuebingen.mpg.de",
        "email": "inf.ethz.ch;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://www.ethz.ch",
        "aff_unique_abbr": "MPI-IS;ETHZ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "id": "6a2a5c6f9e",
        "title": "Tractable Objectives for Robust Policy Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ce5140df15d046a66883807d18d0264b-Abstract.html",
        "author": "Katherine Chen; Michael Bowling",
        "abstract": "Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance.  One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations.   In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty.  Instead we focus on identifying optimization objectives for which solutions can be efficiently approximated.  We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efficiently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP.",
        "bibtex": "@inproceedings{NIPS2012_ce5140df,\n author = {Chen, Katherine and Bowling, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tractable Objectives for Robust Policy Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ce5140df15d046a66883807d18d0264b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ce5140df15d046a66883807d18d0264b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/ce5140df15d046a66883807d18d0264b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ce5140df15d046a66883807d18d0264b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 620172,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16709652900525240335&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Alberta; University of Alberta",
        "aff_domain": "ualberta.ca;cs.ualberta.ca",
        "email": "ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "b3e138e8e3",
        "title": "Training sparse natural image models with a fast Gibbs sampler of an extended state space",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/9b72e31dac81715466cd580a448cf823-Abstract.html",
        "author": "Lucas Theis; Jascha Sohl-dickstein; Matthias Bethge",
        "abstract": "We present a new learning strategy based on an efficient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models.",
        "bibtex": "@inproceedings{NIPS2012_9b72e31d,\n author = {Theis, Lucas and Sohl-dickstein, Jascha and Bethge, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Training sparse natural image models with a fast Gibbs sampler of an extended state space},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/9b72e31dac81715466cd580a448cf823-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/9b72e31dac81715466cd580a448cf823-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/9b72e31dac81715466cd580a448cf823-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/9b72e31dac81715466cd580a448cf823-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 443638,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2798333654873317610&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Werner Reichardt Centre for Integrative Neuroscience; Redwood Center for Theoretical Neuroscience; Werner Reichardt Centre for Integrative Neuroscience",
        "aff_domain": "bethgelab.org;berkeley.edu;bethgelab.org",
        "email": "bethgelab.org;berkeley.edu;bethgelab.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Werner Reichardt Centre for Integrative Neuroscience;University of California, Berkeley",
        "aff_unique_dep": "Centre for Integrative Neuroscience;Center for Theoretical Neuroscience",
        "aff_unique_url": "https://www.cin.uni-tuebingen.de;https://www.berkeley.edu",
        "aff_unique_abbr": "CIN;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "971cb11549",
        "title": "Trajectory-Based Short-Sighted Probabilistic Planning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/8a146f1a3da4700cbf03cdc55e2daae6-Abstract.html",
        "author": "Felipe Trevizan; Manuela Veloso",
        "abstract": "Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [ref] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs.  We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately $10^{70}$ states.",
        "bibtex": "@inproceedings{NIPS2012_8a146f1a,\n author = {Trevizan, Felipe and Veloso, Manuela},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Trajectory-Based Short-Sighted Probabilistic Planning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/8a146f1a3da4700cbf03cdc55e2daae6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/8a146f1a3da4700cbf03cdc55e2daae6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/8a146f1a3da4700cbf03cdc55e2daae6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 145355,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12282887347239398896&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Machine Learning Department; Computer Science Department",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Computer Science Department",
        "aff_unique_dep": "Machine Learning Department;Computer Science",
        "aff_unique_url": "https://www.cs.cmu.edu/ml;",
        "aff_unique_abbr": "CMU ML;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "00985fd9ec",
        "title": "Transelliptical Component Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/de3f712d1a02c5fb481a7a99b0da7fa3-Abstract.html",
        "author": "Fang Han; Han Liu",
        "abstract": "We propose a high dimensional semiparametric scale-invariant principle compo- nent analysis, named TCA, by utilize the natural connection between the ellipti- cal distribution family and the principal component analysis. Elliptical distribu- tion family includes many well-known multivariate distributions like multivari- ate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that",
        "bibtex": "@inproceedings{NIPS2012_de3f712d,\n author = {Han, Fang and Liu, Han},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Transelliptical Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/de3f712d1a02c5fb481a7a99b0da7fa3-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/de3f712d1a02c5fb481a7a99b0da7fa3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/de3f712d1a02c5fb481a7a99b0da7fa3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 228747,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6563859098268728231&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Biostatistics, Johns Hopkins University; Department of Operations Research and Financial Engineering, Princeton University",
        "aff_domain": "jhsph.edu;princeton.edu",
        "email": "jhsph.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Johns Hopkins University;Princeton University",
        "aff_unique_dep": "Department of Biostatistics;Department of Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.jhu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "JHU;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "57e71b2ea1",
        "title": "Transelliptical Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/af8d1eb220186400c494db7091e402b0-Abstract.html",
        "author": "Han Liu; Fang Han; Cun-hui Zhang",
        "abstract": "We advocate the use of a new distribution family\u2014the transelliptical\u2014for robust inference of high dimensional graphical models. The transelliptical family is an extension of the nonparanormal family proposed by Liu et al. (2009). Just as the nonparanormal extends the normal by transforming the variables using univariate functions, the transelliptical extends the elliptical family in the same way. We propose a nonparametric rank-based regularization estimator which achieves the parametric rates of convergence for both graph recovery and parameter estima- tion. Such a result suggests that the extra robustness and \ufb02exibility obtained by the semiparametric transelliptical modeling incurs almost no ef\ufb01ciency loss. We also discuss the relationship between this work with the transelliptical component analysis proposed by Han and Liu (2012).",
        "bibtex": "@inproceedings{NIPS2012_af8d1eb2,\n author = {Liu, Han and Han, Fang and Zhang, Cun-hui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Transelliptical Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/af8d1eb220186400c494db7091e402b0-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/af8d1eb220186400c494db7091e402b0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/af8d1eb220186400c494db7091e402b0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 561311,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10084084142066629902&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Operations Research and Financial Engineering, Princeton University; Department of Biostatistics, Johns Hopkins University; Department of Statistics, Rutgers University",
        "aff_domain": "princeton.edu;jhsph.edu;stat.rutgers.edu",
        "email": "princeton.edu;jhsph.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Princeton University;Johns Hopkins University;Rutgers University",
        "aff_unique_dep": "Department of Operations Research and Financial Engineering;Department of Biostatistics;Department of Statistics",
        "aff_unique_url": "https://www.princeton.edu;https://www.jhu.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "Princeton;JHU;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5ce755f21b",
        "title": "Transferring Expectations in Model-based Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/3f67fd97162d20e6fe27748b5b372509-Abstract.html",
        "author": "Trung Nguyen; Tomi Silander; Tze Y. Leong",
        "abstract": "We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without pre-defined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains.",
        "bibtex": "@inproceedings{NIPS2012_3f67fd97,\n author = {Nguyen, Trung and Silander, Tomi and Leong, Tze},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Transferring Expectations in Model-based Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/3f67fd97162d20e6fe27748b5b372509-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/3f67fd97162d20e6fe27748b5b372509-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/3f67fd97162d20e6fe27748b5b372509-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 255647,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=246233596722614397&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computing, National University of Singapore, Singapore, 117417; School of Computing, National University of Singapore, Singapore, 117417; School of Computing, National University of Singapore, Singapore, 117417",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "fa7f0aa63a",
        "title": "Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/838e8afb1ca34354ac209f53d90c3a43-Abstract.html",
        "author": "Michael Bryant; Erik B. Sudderth",
        "abstract": "Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning.  In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.",
        "bibtex": "@inproceedings{NIPS2012_838e8afb,\n author = {Bryant, Michael and Sudderth, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/838e8afb1ca34354ac209f53d90c3a43-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/838e8afb1ca34354ac209f53d90c3a43-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/838e8afb1ca34354ac209f53d90c3a43-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/838e8afb1ca34354ac209f53d90c3a43-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 198772,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14515059374670415109&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Brown University, Providence, RI; Department of Computer Science, Brown University, Providence, RI",
        "aff_domain": "gmail.com;cs.brown.edu",
        "email": "gmail.com;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Providence",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "906ee2ccf9",
        "title": "Truncation-free Online Variational Inference for Bayesian Nonparametric Models",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html",
        "author": "Chong Wang; David M. Blei",
        "abstract": "We present a truncation-free online variational inference algorithm for Bayesian nonparametric models. Unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly. Our experiments for Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms.",
        "bibtex": "@inproceedings{NIPS2012_091d584f,\n author = {Wang, Chong and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Truncation-free Online Variational Inference for Bayesian Nonparametric Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/091d584fced301b442654dd8c23b3fc9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/091d584fced301b442654dd8c23b3fc9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2917868,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5219442788193198&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Machine Learning Department, Carnegie Mellon University + Princeton University; Computer Science Department, Princeton University",
        "aff_domain": "cs.cmu.edu;cs.princeton.edu",
        "email": "cs.cmu.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "Carnegie Mellon University;Princeton University",
        "aff_unique_dep": "Machine Learning Department;",
        "aff_unique_url": "https://www.cmu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "CMU;Princeton",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Princeton",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "67dc34f423",
        "title": "Unsupervised Structure Discovery for Semantic Analysis of Audio",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html",
        "author": "Sourish Chaudhuri; Bhiksha Raj",
        "abstract": "Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines.",
        "bibtex": "@inproceedings{NIPS2012_c6e19e83,\n author = {Chaudhuri, Sourish and Raj, Bhiksha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Structure Discovery for Semantic Analysis of Audio},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2950678,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12032314514508558047&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d35772043d",
        "title": "Unsupervised Template Learning for Fine-Grained Object Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/a8f8f60264024dca151f164729b76c0b-Abstract.html",
        "author": "Shulin Yang; Liefeng Bo; Jue Wang; Linda G. Shapiro",
        "abstract": "Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms.",
        "bibtex": "@inproceedings{NIPS2012_a8f8f602,\n author = {Yang, Shulin and Bo, Liefeng and Wang, Jue and Shapiro, Linda},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Template Learning for Fine-Grained Object Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/a8f8f60264024dca151f164729b76c0b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/a8f8f60264024dca151f164729b76c0b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/a8f8f60264024dca151f164729b76c0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1062475,
        "gs_citation": 221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9536786400586491101&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "University of Washington, Seattle, WA 98195; ISTC-PC Intel labs, Seattle, WA 98195; Adobe ATL Labs, Seattle, WA 98103; University of Washington, Seattle, WA 98195",
        "aff_domain": "cs.washington.edu;intel.com;adobe.com;cs.washington.edu",
        "email": "cs.washington.edu;intel.com;adobe.com;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Washington;Intel;Adobe",
        "aff_unique_dep": ";ISTC-PC;Adobe ATL Labs",
        "aff_unique_url": "https://www.washington.edu;https://www.intel.com;https://www.adobe.com",
        "aff_unique_abbr": "UW;Intel;Adobe",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ea4141c811",
        "title": "Value Pursuit Iteration",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/ab233b682ec355648e7891e66c54191b-Abstract.html",
        "author": "Amir M. Farahmand; Doina Precup",
        "abstract": "Value Pursuit Iteration (VPI) is an approximate value iteration algorithm that finds a close to optimal policy for  reinforcement learning and planning problems with large state spaces. VPI has two main features: First, it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features. The algorithm is almost insensitive to the number of irrelevant features. Second, after each iteration of VPI, the algorithm adds a set of functions based on the currently learned value function to the dictionary. This increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function. We theoretically study VPI and provide a finite-sample error upper bound for it.",
        "bibtex": "@inproceedings{NIPS2012_ab233b68,\n author = {Farahmand, Amir and Precup, Doina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Value Pursuit Iteration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/ab233b682ec355648e7891e66c54191b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/ab233b682ec355648e7891e66c54191b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/ab233b682ec355648e7891e66c54191b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/ab233b682ec355648e7891e66c54191b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 345187,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7907268257008898179&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "http://Academic.SoloGen.net",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2c6ede0a8a",
        "title": "Variational Inference for Crowdsourcing",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/cd00692c3bfe59267d5ecfac5310286c-Abstract.html",
        "author": "Qiang Liu; Jian Peng; Alex Ihler",
        "abstract": "Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al, while our MF method is closely related to a commonly used EM algorithm. In both cases, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state-of-the-art algorithms based on more complicated modeling assumptions.",
        "bibtex": "@inproceedings{NIPS2012_cd00692c,\n author = {Liu, Qiang and Peng, Jian and Ihler, Alexander T},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Inference for Crowdsourcing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/cd00692c3bfe59267d5ecfac5310286c-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/cd00692c3bfe59267d5ecfac5310286c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/cd00692c3bfe59267d5ecfac5310286c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/cd00692c3bfe59267d5ecfac5310286c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1217926,
        "gs_citation": 424,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17221955537328683936&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "ICS, UC Irvine; TTI-C & CSAIL, MIT; ICS, UC Irvine",
        "aff_domain": "ics.uci.edu;csail.mit.edu;ics.uci.edu",
        "email": "ics.uci.edu;csail.mit.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Irvine;Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Computer Science;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.uci.edu;https://www.csail.mit.edu",
        "aff_unique_abbr": "UCI;MIT",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Irvine;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "93720f1336",
        "title": "Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html",
        "author": "Angela Eigenstetter; Bjorn Ommer",
        "abstract": "Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality  like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular  approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability,  the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated  learning algorithms such as the popular kernel methods are not able to suppress noisy or superfluous dimensions of such high-dimensional data. Consequently, there is a natural need for  feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that  reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded  feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach.",
        "bibtex": "@inproceedings{NIPS2012_85d8ce59,\n author = {Eigenstetter, Angela and Ommer, Bjorn},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/85d8ce590ad8981ca2c8286f79f59954-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3448366,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8875978236125344110&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "HCI & IWR, University of Heidelberg; HCI & IWR, University of Heidelberg",
        "aff_domain": "iwr.uni-heidelberg.de;uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;uni-heidelberg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Heidelberg",
        "aff_unique_dep": "Human-Computer Interaction and Institute for Visualization and Data Analysis",
        "aff_unique_url": "https://www.uni-heidelberg.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "c9df4a9f27",
        "title": "Volume Regularization for Binary Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/4c5bde74a8f110656874902f07378009-Abstract.html",
        "author": "Koby Crammer; Tal Wagner",
        "abstract": "We introduce a large-volume box classification for binary   prediction, which maintains a subset of weight vectors, and   specifically axis-aligned boxes. Our learning algorithm seeks for a   box of large volume that contains ``simple'' weight vectors which   most of are accurate on the training set. Two versions of the   learning process are cast as convex optimization problems, and it   is shown how to solve them efficiently.  The formulation yields a   natural PAC-Bayesian performance bound and it is shown to minimize a   quantity directly aligned with it. The algorithm outperforms SVM and   the recently proposed AROW algorithm on a majority of $30$ NLP   datasets and binarized USPS optical character recognition datasets.",
        "bibtex": "@inproceedings{NIPS2012_4c5bde74,\n author = {Crammer, Koby and Wagner, Tal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Volume Regularization for Binary Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/4c5bde74a8f110656874902f07378009-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/4c5bde74a8f110656874902f07378009-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/4c5bde74a8f110656874902f07378009-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 295144,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7905470734298775724&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Electrical Enginering, The Technion - Israel Institute of Technology; Faculty of Mathematics and Computer Science, Weizmann Institute of Science + Department of Electrical Enginering, The Technion - Israel Institute of Technology",
        "aff_domain": "ee.technion.ac.il;gmail.com",
        "email": "ee.technion.ac.il;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Weizmann Institute of Science",
        "aff_unique_dep": "Department of Electrical Engineering;Faculty of Mathematics and Computer Science",
        "aff_unique_url": "https://www.technion.ac.il;https://www.weizmann.ac.il",
        "aff_unique_abbr": "Technion;Weizmann",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "ee351ec911",
        "title": "Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/e0cf1f47118daebc5b16269099ad7347-Abstract.html",
        "author": "Christian Mayr; Paul St\u00e4rke; Johannes Partzsch; Love Cederstroem; Rene Sch\u00fcffny; Yao Shuai; Nan Du; Heidemarie Schmidt",
        "abstract": "Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems. The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device.",
        "bibtex": "@inproceedings{NIPS2012_e0cf1f47,\n author = {Mayr, Christian and St\\\"{a}rke, Paul and Partzsch, Johannes and Cederstroem, Love and Sch\\\"{u}ffny, Rene and Shuai, Yao and Du, Nan and Schmidt, Heidemarie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/e0cf1f47118daebc5b16269099ad7347-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/e0cf1f47118daebc5b16269099ad7347-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3278832,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7506145522051462912&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Institute of Circuits and Systems, TU Dresden, Dresden, Germany; Institute of Circuits and Systems, TU Dresden, Dresden, Germany; Institute of Circuits and Systems, TU Dresden, Dresden, Germany; Institute of Circuits and Systems, TU Dresden, Dresden, Germany; Zentrum Mikroelektronik Dresden AG, Dresden, Germany; Inst. of Ion Beam Physics and Materials Res., Helmholtz-Zentrum Dresden-Rossendorf e.V., Dresden, Germany; Professur Materialsysteme der Nanoelektronik, TU Chemnitz, Chemnitz, Germany; Professur Materialsysteme der Nanoelektronik, TU Chemnitz, Chemnitz, Germany",
        "aff_domain": "tu-dresden.de;tu-dresden.de;tu-dresden.de;tu-dresden.de;zmdi.com;hzdr.de;s2012.tu-chemnitz.de;etit.tu-chemnitz.de",
        "email": "tu-dresden.de;tu-dresden.de;tu-dresden.de;tu-dresden.de;zmdi.com;hzdr.de;s2012.tu-chemnitz.de;etit.tu-chemnitz.de",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;2;3;3",
        "aff_unique_norm": "Technische Universit\u00e4t Dresden;Zentrum Mikroelektronik Dresden AG;Helmholtz-Zentrum Dresden-Rossendorf;Technische Universit\u00e4t Chemnitz",
        "aff_unique_dep": "Institute of Circuits and Systems;;Institute of Ion Beam Physics and Materials Research;Professur Materialsysteme der Nanoelektronik",
        "aff_unique_url": "https://www.tu-dresden.de;https://www.zmd.de;https://www.hzdr.de;https://www.tu-chemnitz.de",
        "aff_unique_abbr": "TU Dresden;ZMD;HZDR;TU Chemnitz",
        "aff_campus_unique_index": "0;0;0;0;0;0;1;1",
        "aff_campus_unique": "Dresden;Chemnitz",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "ff8d0846a1",
        "title": "Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/996a7fa078cc36c46d02f9af3bef918b-Abstract.html",
        "author": "Won H. Kim; Deepti Pachauri; Charles Hatt; Moo. K. Chung; Sterling Johnson; Vikas Singh",
        "abstract": "Hypothesis testing on signals de\ufb01ned on surfaces (such as the cortical surface) is a fundamental component of a variety of studies in Neuroscience. The goal here is to identify regions that exhibit changes as a function of the clinical condition under study. As the clinical questions of interest move towards identifying very early signs of diseases, the corresponding statistical differences at the group level invariably become weaker and increasingly hard to identify. Indeed, after a multiple comparisons correction is adopted (to account for correlated statistical tests over all surface points), very few regions may survive. In contrast to hypothesis tests on point-wise measurements, in this paper, we make the case for performing statistical analysis on multi-scale shape descriptors that characterize the local topological context of the signal around each surface vertex. Our descriptors are based on recent results from harmonic analysis, that show how wavelet theory extends to non-Euclidean settings (i.e., irregular weighted graphs). We provide strong evidence that these descriptors successfully pick up group-wise differences, where traditional methods either fail or yield unsatisfactory results. Other than this primary application, we show how the framework allows performing cortical surface smoothing in the native space without mappint to a unit sphere.",
        "bibtex": "@inproceedings{NIPS2012_996a7fa0,\n author = {Kim, Won and Pachauri, Deepti and Hatt, Charles and Chung, Moo. and Johnson, Sterling and Singh, Vikas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/996a7fa078cc36c46d02f9af3bef918b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6823454,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10945247972801132926&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Dept. of Computer Sciences, University of Wisconsin, Madison, WI+Wisconsin Alzheimer\u2019s Disease Research Center, University of Wisconsin, Madison, WI+GRECC, William S. Middleton V A Hospital, Madison, WI; Dept. of Computer Sciences, University of Wisconsin, Madison, WI+Wisconsin Alzheimer\u2019s Disease Research Center, University of Wisconsin, Madison, WI+GRECC, William S. Middleton V A Hospital, Madison, WI; Dept. of Biomedical Engineering, University of Wisconsin, Madison, WI; Dept. of Biostatistics & Med. Informatics, University of Wisconsin, Madison, WI; Wisconsin Alzheimer\u2019s Disease Research Center, University of Wisconsin, Madison, WI+GRECC, William S. Middleton V A Hospital, Madison, WI+Dept. of Computer Sciences, University of Wisconsin, Madison, WI+Dept. of Biostatistics & Med. Informatics, University of Wisconsin, Madison, WI; Dept. of Computer Sciences, University of Wisconsin, Madison, WI+Dept. of Biostatistics & Med. Informatics, University of Wisconsin, Madison, WI+Wisconsin Alzheimer\u2019s Disease Research Center, University of Wisconsin, Madison, WI+GRECC, William S. Middleton V A Hospital, Madison, WI",
        "aff_domain": "cs.wisc.edu;cs.wisc.edu;wisc.edu;wisc.edu;medicine.wisc.edu;biostat.wisc.edu",
        "email": "cs.wisc.edu;cs.wisc.edu;wisc.edu;wisc.edu;medicine.wisc.edu;biostat.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+1+2;0;1;1+2+0+1;0+1+1+2",
        "aff_unique_norm": "University of Wisconsin-Madison;University of Wisconsin;William S. Middleton Memorial Veterans Hospital",
        "aff_unique_dep": "Department of Computer Sciences;Wisconsin Alzheimer\u2019s Disease Research Center;GRECC (Geriatric Research, Education, and Clinical Center)",
        "aff_unique_url": "https://www.wisc.edu;https://www.wisc.edu;https://www.wisconsin.va.gov/",
        "aff_unique_abbr": "UW-Madison;UW;",
        "aff_campus_unique_index": "0+0+0;0+0+0;0;0;0+0+0+0;0+0+0+0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0;0+0+0+0;0+0+0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6b012aeb32",
        "title": "Weighted Likelihood Policy Search with Model Selection",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/feab05aa91085b7a8012516bc3533958-Abstract.html",
        "author": "Tsuyoshi Ueno; Kohei Hayashi; Takashi Washio; Yoshinobu Kawahara",
        "abstract": "Reinforcement learning (RL) methods based on direct policy search (DPS) have been actively discussed to achieve an efficient approach to complicated Markov decision processes (MDPs). Although they have brought much progress in practical applications of RL, there still remains an unsolved problem in DPS related to model  selection for the policy. In this paper, we propose a novel DPS method, {\\it  weighted likelihood policy search (WLPS)}, where a policy is efficiently learned through the weighted likelihood estimation. WLPS naturally connects DPS to the statistical inference problem and thus various sophisticated techniques in statistics can be applied to DPS problems directly. Hence, by following the idea of the {\\it information criterion}, we develop a new measurement for model comparison in DPS based on the weighted log-likelihood.",
        "bibtex": "@inproceedings{NIPS2012_feab05aa,\n author = {Ueno, Tsuyoshi and Hayashi, Kohei and Washio, Takashi and Kawahara, Yoshinobu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Weighted Likelihood Policy Search with Model Selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/feab05aa91085b7a8012516bc3533958-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/feab05aa91085b7a8012516bc3533958-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/feab05aa91085b7a8012516bc3533958-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 147009,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17323468225492770322&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Japan Science and Technology Agency; University of Tokyo; Osaka University; Osaka University",
        "aff_domain": "ar.sanken.osaka-u.ac.jp;gmail.com;ar.sanken.osaka-u.ac.jp;ar.sanken.osaka-u.ac.jp",
        "email": "ar.sanken.osaka-u.ac.jp;gmail.com;ar.sanken.osaka-u.ac.jp;ar.sanken.osaka-u.ac.jp",
        "github": "",
        "project": "https://sites.google.com/site/tsuyoshiueno/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Japan Science and Technology Agency;University of Tokyo;Osaka University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.jst.go.jp;https://www.u-tokyo.ac.jp;https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "JST;UTokyo;Osaka U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2aea79128d",
        "title": "Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding",
        "site": "https://papers.nips.cc/paper_files/paper/2012/hash/cd61a580392a70389e27b0bc2b439f49-Abstract.html",
        "author": "Philip Sterne; Joerg Bornschein; Abdul-saboor Sheikh; J\u00f6rg L\u00fccke; Jacquelyn A. Shelton",
        "abstract": "Modelling natural images with sparse coding (SC) has faced two main challenges: \ufb02exibly representing varying pixel intensities and realistically representing low- level image components. This paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard SC model in two crucial points: (1) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity, and (2) the model uses the highly nonlinear combination rule of maximal causes analysis (MCA) instead of a lin- ear combination. The major challenge is parameter optimization because a model with either (1) or (2) results in strongly multimodal posteriors. We show for the \ufb01rst time that a model combining both improvements can be trained ef\ufb01ciently while retaining the rich structure of the posteriors. We design an exact piece- wise Gibbs sampling method and combine this with a variational method based on preselection of latent dimensions. This combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions. Applying the model to image patches we study the optimal encoding of images by simple cells in V1 and compare the model\u2019s predictions with in vivo neural recordings. In contrast to standard SC, we \ufb01nd that the optimal prior favors asymmetric and bimodal ac- tivity of simple cells. Testing our model for consistency we \ufb01nd that the average posterior is approximately equal to the prior. Furthermore, we \ufb01nd that the model predicts a high percentage of globular receptive \ufb01elds alongside Gabor-like \ufb01elds. Similarly high percentages are observed in vivo. Our results thus argue in favor of improvements of the standard sparse coding model for simple cells by using \ufb02exible priors and nonlinear combinations.",
        "bibtex": "@inproceedings{NIPS2012_cd61a580,\n author = {Sterne, Philip and Bornschein, Joerg and Sheikh, Abdul-saboor and L\\\"{u}cke, J\\\"{o}rg and Shelton, Jacquelyn},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/cd61a580392a70389e27b0bc2b439f49-Paper.pdf},\n volume = {25},\n year = {2012}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2012/file/cd61a580392a70389e27b0bc2b439f49-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2012/file/cd61a580392a70389e27b0bc2b439f49-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2012/file/cd61a580392a70389e27b0bc2b439f49-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1709171,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17825311668540000365&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Frankfurt Institute for Advanced Studies; Frankfurt Institute for Advanced Studies; Frankfurt Institute for Advanced Studies; Frankfurt Institute for Advanced Studies; Frankfurt Institute for Advanced Studies + Physics Dept., Goethe-University Frankfurt, Germany",
        "aff_domain": "fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de",
        "email": "fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Frankfurt Institute for Advanced Studies;Goethe University Frankfurt",
        "aff_unique_dep": ";Physics Department",
        "aff_unique_url": "https://www.fias.uni-frankfurt.de/;https://www.uni-frankfurt.de",
        "aff_unique_abbr": "FIAS;GU Frankfurt",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Frankfurt",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "Germany"
    }
]