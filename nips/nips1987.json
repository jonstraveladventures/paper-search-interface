[
    {
        "id": "bb31aa9271",
        "title": "A 'Neural' Network that Learns to Play Backgammon",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/d59a1dc497cf2773637256f50f492723-Abstract.html",
        "author": "G. Tesauro; T. J. Sejnowski",
        "abstract": "We describe a class of connectionist networks that have learned to play back(cid:173)",
        "bibtex": "@inproceedings{NIPS1987_d59a1dc4,\n author = {Tesauro, G. and Sejnowski, T. J.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A \\textquotesingle Neural\\textquotesingle  Network that Learns to Play Backgammon},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/d59a1dc497cf2773637256f50f492723-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/d59a1dc497cf2773637256f50f492723-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/d59a1dc497cf2773637256f50f492723-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2424225,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7568023020161386311&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Complex Systems Research, University of Illinois at Urbana-Champaign; Biophysics Dept., Johns Hopkins University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Johns Hopkins University",
        "aff_unique_dep": "Center for Complex Systems Research;Biophysics Department",
        "aff_unique_url": "https://illinois.edu;https://www.jhu.edu",
        "aff_unique_abbr": "UIUC;JHU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "db72591e4b",
        "title": "A Computer Simulation of Cerebral Neocortex: Computational Capabilities of Nonlinear Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/71996f80223a3e89a5bd0139908097db-Abstract.html",
        "author": "Alexander Singer; John P. Donoghue",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1987_71996f80,\n author = {Singer, Alexander and Donoghue, John},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A Computer Simulation of Cerebral Neocortex: Computational Capabilities of Nonlinear Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/71996f80223a3e89a5bd0139908097db-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/71996f80223a3e89a5bd0139908097db-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/71996f80223a3e89a5bd0139908097db-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2360385,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:cHlUcmAf-qcJ:scholar.google.com/&scioq=A+Computer+Simulation+of+Cerebral+Neocortex:+Computational+Capabilities+of+Nonlinear+Neural+Networks&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Department of Biophysics, Johns Hopkins University; Center for Neural Science, Brown University",
        "aff_domain": "jhu.edu;brown.edu",
        "email": "jhu.edu;brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Johns Hopkins University;Brown University",
        "aff_unique_dep": "Department of Biophysics;Center for Neural Science",
        "aff_unique_url": "https://www.jhu.edu;https://www.brown.edu",
        "aff_unique_abbr": "JHU;Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "abe4a95d4c",
        "title": "A Computer Simulation of Olfactory Cortex with Functional Implications for Storage and Retrieval of Olfactory Information",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/20a5e2274bcb5ea174f03f9ce1487900-Abstract.html",
        "author": "Matthew A. Wilson; James M. Bower",
        "abstract": "Based  on  anatomical  and  physiological  data,  we  have  developed  a  computer  simulation  of piri(cid:173) form  (olfactory)  cortex  which  is  capable  of reproducing  spatial  and  temporal  patterns  of actual  cortical  activity  under  a  variety  of conditions.  Using  a  simple  Hebb-type  learning  rule  in  conjunc(cid:173) tion  with  the  cortical  dynamics  which  emerge  from  the  anatomical  and  physiological  organiza(cid:173) tion  of  the  model,  the  simulations  are  capable  of establishing  cortical  representations  for  differ(cid:173) ent  input  patterns.  The  basis  of  these  representations  lies  in  the  interaction  of  sparsely  distribut(cid:173) ed,  highly  divergent/convergent  interconnections  between  modeled  neurons.  We  have  shown  that  different  representations  can  be  stored  with  minimal  interference.  and  that  following  learning  these  representations  are  resistant  to  input  degradation,  allowing  reconstruction  of  a  representa(cid:173) tion  following  only  a  partial  presentation  of  an  original  training  stimulus.  Further,  we  have  demonstrated  that  the  degree  of  overlap  of  cortical  representations  for  different  stimuli  can  also  be  modulated.  For  instance  similar  input  patterns  can  be  induced  to generate  distinct  cortical  representations  (discrimination).  while  dissimilar  inputs  can  be  induced  to  generate  overlapping  representations  (accommodation).  Both  features  are  presumably  important  in  classifying  olfacto(cid:173) ry stimuli.",
        "bibtex": "@inproceedings{NIPS1987_20a5e227,\n author = {Wilson, Matthew A. and Bower, James M.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A Computer Simulation of Olfactory Cortex with Functional Implications for Storage and Retrieval of Olfactory Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/20a5e2274bcb5ea174f03f9ce1487900-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/20a5e2274bcb5ea174f03f9ce1487900-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/20a5e2274bcb5ea174f03f9ce1487900-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2866921,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16104164987967610429&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computation and Neural Systems Program, Division of Biology, California Institute of Technology, Pasadena, CA 91125; Computation and Neural Systems Program, Division of Biology, California Institute of Technology, Pasadena, CA 91125",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Division of Biology",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6fd8619da3",
        "title": "A Dynamical Approach to Temporal Pattern Processing",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/d9e5bd751997cffa6bc2d0e31ebdc048-Abstract.html",
        "author": "W. Scott Stornetta; Tad Hogg; B. A. Huberman",
        "abstract": "Recognizing  patterns  with  temporal  context  is  important for  such  tasks  as  speech  recognition,  motion  detection  and  signature  verification.  We  propose  an  architecture  in  which  time  serves as its  own representation, and temporal context is encoded in the state of the  nodes. We contrast this with the approach of replicating portions of the  architecture to represent time.",
        "bibtex": "@inproceedings{NIPS1987_d9e5bd75,\n author = {Stornetta, W. Scott and Hogg, Tad and Huberman, B. A.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A Dynamical Approach to Temporal Pattern Processing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/d9e5bd751997cffa6bc2d0e31ebdc048-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/d9e5bd751997cffa6bc2d0e31ebdc048-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/d9e5bd751997cffa6bc2d0e31ebdc048-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1710864,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8626864216573267998&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "55388f6824",
        "title": "A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Artificial Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/afe8a4577080504b8bec07bbe4b2b9cc-Abstract.html",
        "author": "Christopher L. Scofield",
        "abstract": "A  single  cell  theory  for  the  development  of  selectivity  and  ocular  dominance  in  visual  cortex  has  been  presented  previously  by  Bienenstock,  Cooper  and  Munrol.  This  has  been  extended  to  a  network  applicable  to  layer  IV  of  visual  cortex2 .  In  this  paper  we  present  a  mean  field  approximation  that  captures  in  a  fairly  transparent  manner  the  quantitative,  results  of  the  network  theory.  Finally,  we  consider  the  application  of  this  theory  to  artificial  neural  networks  and  show  that  a  significant  reduction  in  architectural  complexity  is  possible.",
        "bibtex": "@inproceedings{NIPS1987_afe8a457,\n author = {Scofield, Christopher L.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Artificial Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/afe8a4577080504b8bec07bbe4b2b9cc-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/afe8a4577080504b8bec07bbe4b2b9cc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/afe8a4577080504b8bec07bbe4b2b9cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1728939,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13876271313501510222&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2c0fd7c8c5",
        "title": "A Method for the Design of Stable Lateral Inhibition Networks that is Robust in the Presence of Circuit Parasitics",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/2f9b1b6b29361118f630783c19891ea0-Abstract.html",
        "author": "J.L. WYATT; Jr; D.L. STANDLEY",
        "abstract": "In  the  analog  VLSI  implementation  of  neural  systems,  it is",
        "bibtex": "@inproceedings{NIPS1987_2f9b1b6b,\n author = {WYATT, J.L. and STANDLEY, D.L.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A Method for the Design of Stable Lateral Inhibition Networks that is Robust in the Presence of Circuit Parasitics},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/2f9b1b6b29361118f630783c19891ea0-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/2f9b1b6b29361118f630783c19891ea0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/2f9b1b6b29361118f630783c19891ea0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1602203,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15196156826435051677&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e306223c51",
        "title": "A NEURAL NETWORK CLASSIFIER BASED ON CODING THEORY",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/3d8edd573b5b21fede5d98ecee0f6382-Abstract.html",
        "author": "Tzi-Dar Chiueh; Rodney Goodman",
        "abstract": "The new neural network classifier we propose transforms the  classification problem into the coding theory problem of decoding a noisy  codeword. An input vector in the feature space is transformed into an internal  representation which is a codeword in the code space, and then error correction  decoded in this space to classify the input feature vector to its class. Two classes  of codes which give high performance are the Hadamard matrix code and the  maximal length sequence code. We show that the number of classes stored in an  N-neuron system is linear in N and significantly more than that obtainable by  using the Hopfield type memory as a classifier.",
        "bibtex": "@inproceedings{NIPS1987_3d8edd57,\n author = {Chiueh, Tzi-Dar and Goodman, Rodney},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A NEURAL NETWORK CLASSIFIER BASED ON CODING THEORY},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/3d8edd573b5b21fede5d98ecee0f6382-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/3d8edd573b5b21fede5d98ecee0f6382-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/3d8edd573b5b21fede5d98ecee0f6382-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2187504,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8173541270802958450&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "eanrornla Instltute of Technology, Pasadena, eanromla 91125; eanrornla Instltute of Technology, Pasadena, eanromla 91125",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b307d48b2c",
        "title": "A Neural-Network Solution to the Concentrator Assignment Problem",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/77cdf4ffbd2afd02541e02533ec56820-Abstract.html",
        "author": "Gene A. Tagliarini; Edward W. Page",
        "abstract": "Networks  of simple analog  processors  having  neuron-like properties have  been  employed  to  compute  good  solutions  to  a  variety  of optimization  prob(cid:173) lems.  This  paper presents  a  neural-net solution to  a  resource allocation prob(cid:173) lem that arises  in  providing  local  access  to  the  backbone of a  wide-area  com(cid:173) munication  network.  The  problem is  described in  terms of an energy function  that can be  mapped onto an analog computational network.  Simulation results  characterizing  the  performance  of the  neural  computation  are  also  presented.",
        "bibtex": "@inproceedings{NIPS1987_77cdf4ff,\n author = {Tagliarini, Gene and Page, Edward},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A Neural-Network Solution to the Concentrator Assignment Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/77cdf4ffbd2afd02541e02533ec56820-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/77cdf4ffbd2afd02541e02533ec56820-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/77cdf4ffbd2afd02541e02533ec56820-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1487948,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9923284580004807168&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Clemson University, Clemson, SC 29634-1906; Department of Computer Science, Clemson University, Clemson, SC 29634-1906",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Clemson University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.clemson.edu",
        "aff_unique_abbr": "Clemson",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Clemson",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ce09ebf740",
        "title": "A Novel Net that Learns Sequential Decision Process",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/48152e80d1b9822ce18941eb437f1cba-Abstract.html",
        "author": "G.Z. SUN; Y.C. LEE; H.H. CHEN",
        "abstract": "We propose a  new  scheme  to construct  neural networks  to classify  pat(cid:173)",
        "bibtex": "@inproceedings{NIPS1987_48152e80,\n author = {SUN, G.Z. and LEE, Y.C. and CHEN, H.H.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A Novel Net that Learns Sequential Decision Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/48152e80d1b9822ce18941eb437f1cba-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/48152e80d1b9822ce18941eb437f1cba-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/48152e80d1b9822ce18941eb437f1cba-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1234154,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13232009852436531204&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6e0147b868",
        "title": "A Trellis-Structured Neural Network",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/0a5209deb79dc584d8ddb41a792d8549-Abstract.html",
        "author": "Thomas Petsche; Bradley W. Dickinson",
        "abstract": "We have developed a neural network which consists of cooperatively inter(cid:173)",
        "bibtex": "@inproceedings{NIPS1987_0a5209de,\n author = {Petsche, Thomas and Dickinson, Bradley},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {A Trellis-Structured Neural Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/0a5209deb79dc584d8ddb41a792d8549-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/0a5209deb79dc584d8ddb41a792d8549-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/0a5209deb79dc584d8ddb41a792d8549-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2314768,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18095678454496716472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Princeton University, Department of Electrical Engineering; Princeton University, Department of Electrical Engineering + Siemens Corporate Research and Support, Inc.",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Princeton University;Siemens Corporate Research and Support",
        "aff_unique_dep": "Department of Electrical Engineering;",
        "aff_unique_url": "https://www.princeton.edu;https://www.siemens.com/research",
        "aff_unique_abbr": "Princeton;SCR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8df85ffc57",
        "title": "An Adaptive and Heterodyne Filtering Procedure for the Imaging of Moving Objects",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/4878a0527a93d431d2637338f51cb4af-Abstract.html",
        "author": "F. H. Schuling; H. A. K. Mastebroek; W. H. Zaagman",
        "abstract": "Recent experimental work on the stimulus velocity dependent time resolving  power of the neural units, situated in the highest order optic ganglion of the  blowfly, revealed the at first sight amazing phenomenon that at this high level of  the fly visual system, the time constants of these units which are involved in the  processing of neural activity evoked by moving objects, are -roughly spoken(cid:173) inverse proportional to the velocity of those objects over an extremely wide range.  In this paper we will discuss the implementation of a two dimensional heterodyne  adaptive filter construction into a computer simulation model. The features of this  simulation model include the ability to account for the experimentally observed  stimulus-tuned adaptive temporal behaviour of time constants in the fly visual  system. The simulation results obtained, clearly show that the application of such  an adaptive processing procedure delivers an improved imaging technique of  moving patterns in the high velocity range.",
        "bibtex": "@inproceedings{NIPS1987_4878a052,\n author = {Schuling, F. and Mastebroek, H. and Zaagman, W.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {An Adaptive and Heterodyne Filtering Procedure for the Imaging of Moving Objects},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/4878a0527a93d431d2637338f51cb4af-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/4878a0527a93d431d2637338f51cb4af-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/4878a0527a93d431d2637338f51cb4af-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2369510,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LE_ahrayTooJ:scholar.google.com/&scioq=An+Adaptive+and+Heterodyne+Filtering+Procedure+for+the+Imaging+of+Moving+Objects&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dab6d3e27e",
        "title": "An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/853f7b3615411c82a2ae439ab8c4c96e-Abstract.html",
        "author": "Toshiteru Homma; Les E. Atlas; Robert J. Marks II",
        "abstract": "An  artificial  neural  network  is  developed  to  recognize  spatio-temporal  bipolar patterns  associatively.  The  function  of a formal  neuron is  generalized by  replacing  multiplication  with  convolution,  weights  with  transfer  functions,  and  thresholding  with  nonlinear  transform  following  adaptation.  The Hebbian  learn(cid:173) ing  rule  and  the  delta  learning  rule  are  generalized  accordingly,  resulting  in  the  learning  of weights  and  delays.  The  neural  network  which  was  first  developed  for  spatial  patterns  was  thus  generalized  for  spatio-temporal  patterns.  It  was  tested  using  a  set  of bipolar input patterns  derived from  speech  signals,  showing  robust classification of 30 model phonemes.",
        "bibtex": "@inproceedings{NIPS1987_853f7b36,\n author = {Homma, Toshiteru and Atlas, Les E. and Marks, Robert J.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/853f7b3615411c82a2ae439ab8c4c96e-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/853f7b3615411c82a2ae439ab8c4c96e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/853f7b3615411c82a2ae439ab8c4c96e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1934011,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4815741701337551945&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a64eb0398e",
        "title": "An Optimization Network for Matrix Inversion",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/9412531719be7ccf755c4ff98d0969dc-Abstract.html",
        "author": "Ju-Seog Jang; Soo-Young Lee; Sang-Yung Shin",
        "abstract": "Inverse  matrix  calculation  can  be  considered  as  an  optimization.  We  have  demonstrated  that  this  problem  can  be  rapidly  solved  by  highly  interconnected  simple  neuron-like  analog  processors.  A  network  for  matrix  inversion  based  on  the  concept  of  Hopfield's  neural  network  was  designed,  and  implemented  with  electronic  hardware.  With  slight  modifications,  the  network  is  readily  applicable  to  solving  a  linear  simultaneous  equation  efficiently.  Notable  features  of  this  circuit  are  potential  speed  due  to  parallel  processing,  and  robustness  against  variations  of  device  parameters.",
        "bibtex": "@inproceedings{NIPS1987_94125317,\n author = {Jang, Ju-Seog and Lee, Soo-Young and Shin, Sang-Yung},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {An Optimization Network for Matrix Inversion},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/9412531719be7ccf755c4ff98d0969dc-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/9412531719be7ccf755c4ff98d0969dc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/9412531719be7ccf755c4ff98d0969dc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 847937,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8619649439497207819&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9aac623fc7",
        "title": "Analysis and Comparison of Different Learning Algorithms for Pattern Association Problems",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/91fc23ceccb664ebb0cf4257e1ba9c51-Abstract.html",
        "author": "J. Bernasconi",
        "abstract": "We",
        "bibtex": "@inproceedings{NIPS1987_91fc23ce,\n author = {Bernasconi, J.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Analysis and Comparison of Different Learning Algorithms for Pattern Association Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/91fc23ceccb664ebb0cf4257e1ba9c51-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/91fc23ceccb664ebb0cf4257e1ba9c51-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/91fc23ceccb664ebb0cf4257e1ba9c51-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1775843,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3840715900606953428&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Brown Boveri Research Center",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Brown Boveri Research Center",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bbr.ch",
        "aff_unique_abbr": "BBRC",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "6d82f5f123",
        "title": "Analysis of Distributed Representation of Constituent Structure in Connectionist Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/68ba979a6eef19c1fa7771e6582185bc-Abstract.html",
        "author": "Paul Smolensky",
        "abstract": "A  general  method, the  tensor product representation, is described for  the distributed representation of  value/variable bindings.  The method allows the fully distributed representation of symbolic structures:  the roles  in  the structures, as well as the fillers  for  those roles, can be arbitrarily non-local.  Fully and  partially localized  special cases reduce to existing cases of connectionist representations of structured  data;  the  tensor  product  representation  generalizes  these  and  the  few  existing  examples  of  fuUy  distributed  representations  of structures.  The  representation  saturates  gracefully  as  larger  structures  are  represented;  it penn its  recursive  construction  of complex  representations  from  simpler  ones;  it  respects  the  independence  of the  capacities  to generate and  maintain  multiple bindings in  parallel;  it  extends naturally to continuous structures and continuous representational patterns; it pennits values to  also  serve  as  variables;  it  enables  analysis  of  the  interference  of  symbolic  structures  stored  in  associative  memories;  and  it  leads  to characterization  of optimal  distributed representations  of roles  and a recirculation algorithm for learning them.",
        "bibtex": "@inproceedings{NIPS1987_68ba979a,\n author = {Smolensky, Paul},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Analysis of Distributed Representation of Constituent Structure in Connectionist Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/68ba979a6eef19c1fa7771e6582185bc-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/68ba979a6eef19c1fa7771e6582185bc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/68ba979a6eef19c1fa7771e6582185bc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2495662,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16015386253110410768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Colorado, Boulder, CO 80309-0430",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Colorado Boulder",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU Boulder",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "870733ed96",
        "title": "Basins of Attraction for Electronic Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/fea47a8aa372e42f3c84327aec9506cf-Abstract.html",
        "author": "C. M. Marcus; R. M. Westervelt",
        "abstract": "We  have  studied  the  basins  of  attraction  for  fixed  point  and  oscillatory  attractors  in  an  electronic  analog  neural  network.  Basin  measurement  circuitry  periodically  opens  the  network  feedback  loop,  loads  raster-scanned  initial  conditions  and  examines  the  resulting  attractor.  Plotting  the  basins  for  fixed  points  (memories),  we  show  that  overloading  an  associative  memory  network  leads  to  irregular  basin  shapes.  The  network  also  includes  analog  time  delay  circuitry,  and  we  have  shown  that  delay  in  symmetric  networks  can  introduce  basins  for  oscillatory  attractors.  Conditions  leading  to  oscillation  are  related  to  the  presence  of  frustration;  reducing  frustration  by  diluting  the  connections  can  stabilize  a  delay  network.",
        "bibtex": "@inproceedings{NIPS1987_fea47a8a,\n author = {Marcus, C. M. and Westervelt, R. M.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Basins of Attraction for Electronic Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/fea47a8aa372e42f3c84327aec9506cf-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/fea47a8aa372e42f3c84327aec9506cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/fea47a8aa372e42f3c84327aec9506cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2199780,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8195970650483848124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Division of Applied Sciences and Department of Physics, Harvard University, Cambridge, MA 02138; Division of Applied Sciences and Department of Physics, Harvard University, Cambridge, MA 02138",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "Division of Applied Sciences and Department of Physics",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "748c43825a",
        "title": "Bit-Serial Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/523f87e9d08e6071a3bbd150e6da40fb-Abstract.html",
        "author": "Alan F. Murray; Anthony V. W. Smith; Zoe F. Butler",
        "abstract": "A  bit  - serial  VLSI  neural  network  is  described  from  an  initial  architecture  for  a  synapse array through to silicon layout and board design.  The issues surrounding bit  - serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel  development  of  a  hybrid  analog/digital  neural  network  is  outlined.  Learning  and  recall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected  specification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 MHz.  This tech(cid:173) nique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms,  using  a  \"paging\"  technique  to  time  - multiplex  calculations  through  the  synapse  array.",
        "bibtex": "@inproceedings{NIPS1987_523f87e9,\n author = {Murray, Alan and Smith, Anthony and Butler, Zoe},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Bit-Serial Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/523f87e9d08e6071a3bbd150e6da40fb-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/523f87e9d08e6071a3bbd150e6da40fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/523f87e9d08e6071a3bbd150e6da40fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2291566,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2326149593361954715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6ab8742a2b",
        "title": "Capacity for Patterns and Sequences in Kanerva's SDM as Compared to Other Associative Memory Models",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/3c5882a72c072a68dc50d4091eae11df-Abstract.html",
        "author": "James D. Keeler",
        "abstract": "The  information  capacity  of Kanerva's  Sparse,  Distributed Memory  (SDM)  and  Hopfield-type  neural networks  is  investigated.  Under  the  approximations  used here,  it  is shown  that  the  to(cid:173) tal  information  stored in  these  systems  is proportional  to  the  number  connections  in  the  net(cid:173) work.  The  proportionality  constant  is  the  same  for  the  SDM  and  HopJreld-type  models  in(cid:173) dependent  of  the  particular  model,  or  the  order  of  the  model.  The  approximations  are  checked  numerically.  This  same  analysis  can  be  used  to  show  that  the  SDM  can  store  se(cid:173) quences  of spatiotemporal patterns,  and  the  addition  of time-delayed  connections  allows  the  retrieval  of context  dependent  temporal  patterns.  A  minor  modification  of the  SDM  can  be  used to store correlated patterns.",
        "bibtex": "@inproceedings{NIPS1987_3c5882a7,\n author = {Keeler, James},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Capacity for Patterns and Sequences in Kanerva\\textquotesingle s SDM as Compared to Other Associative Memory Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/3c5882a72c072a68dc50d4091eae11df-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/3c5882a72c072a68dc50d4091eae11df-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/3c5882a72c072a68dc50d4091eae11df-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2443959,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16755604683636554383&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Chemistry Department, Stanford University, Stanford, CA 94305+RIACS, NASA-AMES 230-5 Moffett Field, CA 94035",
        "aff_domain": "hydra.riacs.edu",
        "email": "hydra.riacs.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Stanford University;Research Institute for Advanced Computer Science",
        "aff_unique_dep": "Chemistry Department;",
        "aff_unique_url": "https://www.stanford.edu;",
        "aff_unique_abbr": "Stanford;RIACS",
        "aff_campus_unique_index": "0+1",
        "aff_campus_unique": "Stanford;Moffett Field",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "20312be18a",
        "title": "Centric Models of the Orientation Map in Primary Visual Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/1eb34d662b67a14e3511d0dfd78669be-Abstract.html",
        "author": "William Baxter; Bruce Dow",
        "abstract": "In  the  visual  cortex  of  the  monkey  the  horizontal  organization  of  the  preferred  orientations of orientation-selective  cells  follows  two opposing  rules:  1) neighbors  tend  to  have similar orientation preferences, and  2) many different orientations are  observed  in a  local  region.  Several  orientation  models  which satisfy these  constraints are found  to  differ  in  the spacing  and  the  topological  index  of their singularities.  Using  the  rate  of orientation change as  a  measure,  the  models  are  compared to published experimental  results.",
        "bibtex": "@inproceedings{NIPS1987_1eb34d66,\n author = {Baxter, William and Dow, Bruce},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Centric Models of the Orientation Map in Primary Visual Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/1eb34d662b67a14e3511d0dfd78669be-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/1eb34d662b67a14e3511d0dfd78669be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/1eb34d662b67a14e3511d0dfd78669be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2413834,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7003325941699699521&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, S.U.N.Y. at Buffalo, NY 14620; Department of Physiology, S.U.N.Y. at Buffalo, NY 14620",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "State University of New York at Buffalo",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.buffalo.edu",
        "aff_unique_abbr": "SUNY Buffalo",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Buffalo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "52a7bf4bf3",
        "title": "Computing Motion Using Resistive Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/875578931a159790107a9184e39a67a4-Abstract.html",
        "author": "Christof Koch; Jin Luo; Carver Mead; James Hutchinson",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1987_87557893,\n author = {Koch, Christof and Luo, Jin and Mead, Carver and Hutchinson, James},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Computing Motion Using Resistive Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/875578931a159790107a9184e39a67a4-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/875578931a159790107a9184e39a67a4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/875578931a159790107a9184e39a67a4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1973441,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9194868833667218427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7bc02984e2",
        "title": "Connecting to the Past",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/0cafb7890f6a7d4de65507d5bb7e0187-Abstract.html",
        "author": "Bruce A. MacDonald",
        "abstract": "Recently  there  has  been  renewed  interest  in  neural-like  processing  systems,  evidenced  for  ex(cid:173) ample in  the two volumes  Parallel Distributed Processing edited by Rumelhart and McClelland,  and  discussed  as  parallel  distributed  systems,  connectionist  models,  neural  nets,  value  passing  systems  and  multiple  context  systems.  Dissatisfaction  with  symbolic  manipulation  paradigms  for  artificial  intelligence seems  partly  responsible  for  this  attention, encouraged by  the promise  of massively  parallel  systems  implemented  in  hardware.  This  paper  relates  simple  neural-like  systems  based  on  multiple  context  to  some  other  well-known  formalisms-namely  production  systems, k-Iength sequence prediction, finite-state  machines and Turing machines-and presents  earlier sequence  prediction  results  in  a  new  light.",
        "bibtex": "@inproceedings{NIPS1987_0cafb789,\n author = {MacDonald, Bruce},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Connecting to the Past},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/0cafb7890f6a7d4de65507d5bb7e0187-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/0cafb7890f6a7d4de65507d5bb7e0187-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/0cafb7890f6a7d4de65507d5bb7e0187-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2243031,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15692087219608013212&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Knowledge Sciences Laboratory, Computer Science Department, The University of Calgary",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Calgary",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.ucalgary.ca",
        "aff_unique_abbr": "U of Calgary",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "d44610897d",
        "title": "Connectivity Versus Entropy",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/ca8d99f92af40381763ab9fd7f926a57-Abstract.html",
        "author": "Yaser S. Abu-Mostafa",
        "abstract": "How  does  the  connectivity  of a  neural  network  (number  of synapses  per  neuron)  relate  to  the complexity  of the  problems  it  can  handle  (measured  by  the entropy)?  Switching theory would suggest no relation at all, since all Boolean  functions  can be  implemented  using  a  circuit  with very  low  connectivity  (e.g.,  using  two-input  NAND  gates).  However,  for  a  network  that  learns  a  problem  from  examples  using  a  local  learning  rule,  we  prove  that  the  entropy  of  the  problem becomes  a  lower  bound for  the connectivity of the network.",
        "bibtex": "@inproceedings{NIPS1987_ca8d99f9,\n author = {Abu-Mostafa, Yaser},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Connectivity Versus Entropy},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/ca8d99f92af40381763ab9fd7f926a57-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/ca8d99f92af40381763ab9fd7f926a57-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/ca8d99f92af40381763ab9fd7f926a57-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1322648,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17394015717475357167&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "California Institute of Technology",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ecddfd5175",
        "title": "Constrained Differential Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/a1126573153ad7e9f44ba80e99316482-Abstract.html",
        "author": "John C. Platt; Alan H. Barr",
        "abstract": "Many optimization models of neural  networks need constraints to restrict the space of outputs to  a subspace which satisfies external criteria.  Optimizations using energy methods yield \"forces\" which  act upon  the  state of the  neural  network.  The penalty method, in which quadratic  energy  constraints  are  added  to  an  existing  optimization  energy,  has  become  popular  recently,  but  is  not  guaranteed  to satisfy  the  constraint conditions  when  there  are  other forces  on  the  neural  model  or when  there  are  multiple constraints.  In this paper, we present the basic differential multiplier method (BDMM),  which  satisfies constraints exactly;  we  create forces  which gradually apply  the constraints over time,  using \"neurons\" that estimate Lagrange multipliers.",
        "bibtex": "@inproceedings{NIPS1987_a1126573,\n author = {Platt, John and Barr, Alan},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Constrained Differential Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/a1126573153ad7e9f44ba80e99316482-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/a1126573153ad7e9f44ba80e99316482-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/a1126573153ad7e9f44ba80e99316482-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2025058,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1270377086060124872&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "California Institute of Technology, Pasadena, CA 91125; California Institute of Technology, Pasadena, CA 91125",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8bbc67de33",
        "title": "Correlational Strength and Computational Algebra of Synaptic Connections Between Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/79c36fe64e04b80fc44845bb9fe73242-Abstract.html",
        "author": "Eberhard E. Fetz",
        "abstract": "Intracellular  recordings  in  spinal  cord  motoneurons  and  cerebral  cortex neurons have provided new evidence on the correlational strength of  monosynaptic  connections,  and  the  relation  between  the  shapes  of  postsynaptic  potentials  and  the  associated  increased  firing  probability.  In  these  cells,  excitatory  postsynaptic  potentials  (EPSPs)  produce  cross(cid:173) correlogram peaks  which resemble  in large part the derivative of  the EPSP.  Additional  synaptic  noise broadens  the peak,  but the  peak  area  -- i.e.,  the  number of above-chance firings triggered per EPSP -- remains proportional to  the EPSP  amplitude.  A typical EPSP of 100  ~v triggers about .01  firings per  EPSP.  The  consequences  of  these  data  for  information  processing  by  polysynaptic connections is discussed.  The effects of sequential polysynaptic  links  can  be  calculated  by  convolving  the  effects  of  the  underlying  monosynaptic connections.  The net effect of parallel pathways is the sum of  the individual contributions.",
        "bibtex": "@inproceedings{NIPS1987_79c36fe6,\n author = {Fetz, Eberhard},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Correlational Strength and Computational Algebra of Synaptic Connections Between Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/79c36fe64e04b80fc44845bb9fe73242-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/79c36fe64e04b80fc44845bb9fe73242-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/79c36fe64e04b80fc44845bb9fe73242-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1831855,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16354754756263528039&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Physiology & Biophysics, University of Washington, Seattle, WA 98195",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Physiology & Biophysics",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a7ffdbc9b4",
        "title": "Cycles: A Simulation Tool for Studying Cyclic Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/2c929ca7263d51b38312797c9a9a6358-Abstract.html",
        "author": "Michael T. Gately",
        "abstract": "A computer program has been designed and implemented to allow a researcher",
        "bibtex": "@inproceedings{NIPS1987_2c929ca7,\n author = {Gately, Michael},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Cycles: A Simulation Tool for Studying Cyclic Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/2c929ca7263d51b38312797c9a9a6358-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/2c929ca7263d51b38312797c9a9a6358-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/2c929ca7263d51b38312797c9a9a6358-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1295621,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15337711151466569129&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Texas Instruments Incorporated, Dallas, TX 75265",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Texas Instruments Incorporated",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ti.com",
        "aff_unique_abbr": "TI",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Dallas",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1a17928fe3",
        "title": "Discovering Structure from Motion in Monkey, Man and Machine",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/c0987e6b6da2428e8cd43efa74790ccb-Abstract.html",
        "author": "Ralph M. Siegel",
        "abstract": "The ability to obtain three-dimensional structure from visual motion is",
        "bibtex": "@inproceedings{NIPS1987_c0987e6b,\n author = {Siegel, Ralph},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Discovering Structure from Motion in Monkey, Man and Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/c0987e6b6da2428e8cd43efa74790ccb-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/c0987e6b6da2428e8cd43efa74790ccb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/c0987e6b6da2428e8cd43efa74790ccb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1729279,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11791167719849215229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Salk Institute of Biology, La Jolla, Ca. 92037 + Laboratory of Neurobiology, The Rockefeller University, 1230 York Avenue, New York, NY 10021",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Salk Institute of Biology;Rockefeller University",
        "aff_unique_dep": ";Laboratory of Neurobiology",
        "aff_unique_url": "https://www.salk.edu;https://www.rockefeller.edu",
        "aff_unique_abbr": "Salk Institute;Rockefeller University",
        "aff_campus_unique_index": "0+1",
        "aff_campus_unique": "La Jolla;New York",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7e53f24cc8",
        "title": "Distributed Neural Information Processing in the Vestibulo-Ocular System",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/b95b58ff6d46d4b7ef2b3e2fd0ddb24c-Abstract.html",
        "author": "Clifford Lau; Vicente Honrubia",
        "abstract": "A new distributed neural information-processing",
        "bibtex": "@inproceedings{NIPS1987_b95b58ff,\n author = {Lau, Clifford and Honrubia, Vicente},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Distributed Neural Information Processing in the Vestibulo-Ocular System},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/b95b58ff6d46d4b7ef2b3e2fd0ddb24c-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/b95b58ff6d46d4b7ef2b3e2fd0ddb24c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/b95b58ff6d46d4b7ef2b3e2fd0ddb24c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1616367,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16402649711560540534&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Office of Naval Research Detachment, Pasadena, CA 91106; UCLA Division of Head and Neck Surgery, Los Angeles, CA 90024",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Office of Naval Research;University of California, Los Angeles",
        "aff_unique_dep": ";Division of Head and Neck Surgery",
        "aff_unique_url": "https://www.onr.navy.mil;https://www.ucla.edu",
        "aff_unique_abbr": "ONR;UCLA",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pasadena;Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a7dcb25ca3",
        "title": "Encoding Geometric Invariances in Higher-Order Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/2ff175df3b37981e4ea5aab357043d82-Abstract.html",
        "author": "C.L. Giles; R.D. Griffin; T. Maxwell",
        "abstract": "We  describe  a  method  of  constructing  higher-order  neural",
        "bibtex": "@inproceedings{NIPS1987_2ff175df,\n author = {Giles, C.L. and Griffin, R.D. and Maxwell, T.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Encoding Geometric Invariances in Higher-Order Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/2ff175df3b37981e4ea5aab357043d82-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/2ff175df3b37981e4ea5aab357043d82-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/2ff175df3b37981e4ea5aab357043d82-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1736523,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11980350282919062860&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7ea0bf287b",
        "title": "Ensemble' Boltzmann Units have Collective Computational Properties like those of Hopfield and Tank Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/cdb1bbbe8f246aa0942da408d79f19ca-Abstract.html",
        "author": "Mark Derthick; Joe Tebelskis",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1987_cdb1bbbe,\n author = {Derthick, Mark and Tebelskis, Joe},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Ensemble\\textquotesingle  Boltzmann Units have Collective Computational Properties like those of Hopfield and Tank Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/cdb1bbbe8f246aa0942da408d79f19ca-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/cdb1bbbe8f246aa0942da408d79f19ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/cdb1bbbe8f246aa0942da408d79f19ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2018322,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11800996546293375715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Carnegie-Mellon University; Department of Computer Science, Carnegie-Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0e1d002f6d",
        "title": "Experimental Demonstrations of Optical Neural Computers",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/bc19061f88f16e9ed4a18f0bbd47048a-Abstract.html",
        "author": "Ken Hsu; David Brady; Demetri Psaltis",
        "abstract": "We  describe  two  expriments  in  optical  neural  computing.  In  the  first  a  closed  optical  feedback  loop  is  used  to  implement  auto-associative  image  recall.  In the second a perceptron-Iike learning algorithm is  implemented with  photorefractive holography.",
        "bibtex": "@inproceedings{NIPS1987_bc19061f,\n author = {Hsu, Ken and Brady, David and Psaltis, Demetri},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Experimental Demonstrations of Optical Neural Computers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/bc19061f88f16e9ed4a18f0bbd47048a-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/bc19061f88f16e9ed4a18f0bbd47048a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/bc19061f88f16e9ed4a18f0bbd47048a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2183260,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10303522547624106102&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "03740b60a3",
        "title": "Generalization of Back propagation to Recurrent and Higher Order Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/6ad10c3a760cfad3e5d1fa1ddaefdec2-Abstract.html",
        "author": "Fernando J. Pineda",
        "abstract": "A general method for deriving backpropagation algorithms for networks",
        "bibtex": "@inproceedings{NIPS1987_6ad10c3a,\n author = {Pineda, Fernando},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Generalization of Back propagation to Recurrent and Higher Order Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/6ad10c3a760cfad3e5d1fa1ddaefdec2-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/6ad10c3a760cfad3e5d1fa1ddaefdec2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/6ad10c3a760cfad3e5d1fa1ddaefdec2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2271775,
        "gs_citation": 1697,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5697451556799083030&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Applied Physics Laboratory, Johns Hopkins University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Applied Physics Laboratory",
        "aff_unique_url": "https://www.jhuapl.edu",
        "aff_unique_abbr": "JHU APL",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "31375710d5",
        "title": "HIGH DENSITY ASSOCIATIVE MEMORIES",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/6c44dc73014d66ba49b28d483a8f8b0d-Abstract.html",
        "author": "Amir Dembo; Ofer Zeitouni",
        "abstract": "from a description of desired properties",
        "bibtex": "@inproceedings{NIPS1987_6c44dc73,\n author = {Dembo, Amir and Zeitouni, Ofer},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {HIGH DENSITY ASSOCIATIVE MEMORIES},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/6c44dc73014d66ba49b28d483a8f8b0d-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/6c44dc73014d66ba49b28d483a8f8b0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/6c44dc73014d66ba49b28d483a8f8b0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1453423,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Information Systems Laboratory, Stanford University; Laboratory for Information and Decision Systems, MIT",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Information Systems Laboratory;Laboratory for Information and Decision Systems",
        "aff_unique_url": "https://www.stanford.edu;https://web.mit.edu",
        "aff_unique_abbr": "Stanford;MIT",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Stanford;Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e6b0a22b91",
        "title": "HOW THE CATFISH TRACKS ITS PREY: AN INTERACTIVE \"PIPELINED\" PROCESSING SYSTEM MAY DIRECT FORAGING VIA RETICULOSPINAL NEURONS",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/35ab33f5f9a61426560675e75c14cc0b-Abstract.html",
        "author": "Jagmeet S. Kanwal",
        "abstract": "of",
        "bibtex": "@inproceedings{NIPS1987_35ab33f5,\n author = {Kanwal, Jagmeet S.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {HOW THE CATFISH TRACKS ITS PREY: AN INTERACTIVE \"PIPELINED\" PROCESSING SYSTEM MAY DIRECT FORAGING VIA RETICULOSPINAL NEURONS},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/35ab33f5f9a61426560675e75c14cc0b-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/35ab33f5f9a61426560675e75c14cc0b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/35ab33f5f9a61426560675e75c14cc0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2076516,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10733527912939945302&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Dept. of Cellular & Structural Biology, Univ. of Colorado, Sch. of Medicine, 4200 East, Ninth Ave., Denver, CO 80262",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Colorado Denver",
        "aff_unique_dep": "Department of Cellular and Structural Biology",
        "aff_unique_url": "https://www.ucdenver.edu",
        "aff_unique_abbr": "UC Denver",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Denver",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "99d403bb66",
        "title": "Hierarchical Learning Control - An Approach with Neuron-Like Associative Memories",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/9fccde8a665c57cbde962b405ee6a44b-Abstract.html",
        "author": "E. Ers\u00fc; H. Tolle",
        "abstract": "Advances",
        "bibtex": "@inproceedings{NIPS1987_9fccde8a,\n author = {Ers\\\"{u}, E. and Tolle, H.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Hierarchical Learning Control - An Approach with Neuron-Like Associative Memories},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/9fccde8a665c57cbde962b405ee6a44b-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/9fccde8a665c57cbde962b405ee6a44b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/9fccde8a665c57cbde962b405ee6a44b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2550715,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8016218274841182597&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ISRA Systemtechnik GmbH, Schofferstr. 15, D-6100 Darmstadt, FRG; TH Darmstadt, Institut fur Regelungstechnik, Schlo~graben 1, D-6100 Darmstadt, FRG",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ISRA Systemtechnik GmbH;Technische Hochschule Darmstadt",
        "aff_unique_dep": ";Institut fur Regelungstechnik",
        "aff_unique_url": ";https://www.th darmstadt.de",
        "aff_unique_abbr": ";TH Darmstadt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "0fa7126a9f",
        "title": "High Order Neural Networks for Efficient Associative Memory Design",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/a0a81eed87dd44d6504fed5f81f6de5a-Abstract.html",
        "author": "I. GUYON; L. PERSONNAZ; J. P. NADAL; G. DREYFUS",
        "abstract": "We  propose  learning  rules  for  recurrent  neural  networks  with  high-order  interactions  between  some or all  neurons.  The designed  networks  exhibit the  desired associative  memory  function: perfect  storage  and  retrieval  of pieces  of information and/or sequences of information of any complexity.",
        "bibtex": "@inproceedings{NIPS1987_a0a81eed,\n author = {GUYON, I. and PERSONNAZ, L. and NADAL, J. P. and DREYFUS, G.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {High Order Neural Networks for Efficient Associative Memory Design},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/a0a81eed87dd44d6504fed5f81f6de5a-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/a0a81eed87dd44d6504fed5f81f6de5a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/a0a81eed87dd44d6504fed5f81f6de5a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1552923,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3969642423676059580&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b455f833f6",
        "title": "How Neural Nets Work",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/09c653c3ae9d116e5f288ff988283a06-Abstract.html",
        "author": "Alan Lapedes; Robert Farber",
        "abstract": "There is  presently great interest in the abilities of neural networks to mimic \n\"qualitative reasoning\"  by manipulating neural incodings of symbols.  Less work \nhas  been performed on using neural networks to process floating  point numbers \nand it is  sometimes stated that neural networks are somehow inherently inaccu(cid:173)\nrate  and  therefore  best  suited  for  \"fuzzy\"  qualitative reasoning.  Nevertheless, \nthe  potential  speed  of massively  parallel  operations  make  neural  net  \"number \ncrunching\"  an interesting topic  to explore.  In this paper we  discuss some of our \nwork in which we  demonstrate that for  certain applications neural networks can \nachieve  significantly  higher  numerical  accuracy  than  more  conventional  tech(cid:173)\nniques.  In  particular,  prediction  of future  values  of a  chaotic  time  series  can \nbe  performed  with  exceptionally  high  accuracy.  We  analyze  how  a  neural  net \nis  able  to do  this  ,  and in  the process  show  that  a  large class  of functions  from \nRn.  ~ Rffl  may  be  accurately  approximated  by  a  backpropagation  neural  net \nwith just two  \"hidden\"  layers.  The network  uses  this functional  approximation \nto perform either interpolation (signal processing applications)  or extrapolation \n(symbol processing applicationsJ.  Neural nets therefore use quite familiar meth(cid:173)\nods to perform. their tasks.  The geometrical viewpoint advocated here seems to \nbe a  useful  approach  to analyzing  neural  network  operation  and  relates  neural \nnetworks  to well  studied topics  in  functional  approximation.",
        "bibtex": "@inproceedings{NIPS1987_09c653c3,\n author = {Lapedes, Alan and Farber, Robert},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {How Neural Nets Work},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/09c653c3ae9d116e5f288ff988283a06-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/09c653c3ae9d116e5f288ff988283a06-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/09c653c3ae9d116e5f288ff988283a06-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3326359,
        "gs_citation": 772,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2059915737924806498&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Theoretical Division, Los Alamos National Laboratory; Theoretical Division, Los Alamos National Laboratory",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Los Alamos National Laboratory",
        "aff_unique_dep": "Theoretical Division",
        "aff_unique_url": "https://www.lanl.gov",
        "aff_unique_abbr": "LANL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bd7a11a489",
        "title": "Introduction to a System for Implementing Neural Net Connections on SIMD Architectures",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/069a002768bcb31509d4901961f23b3c-Abstract.html",
        "author": "Sherryl Tomboulian",
        "abstract": "Neural  networks  have  attracted  much  interest  recently,  and  using  parallel",
        "bibtex": "@inproceedings{NIPS1987_069a0027,\n author = {Tomboulian, Sherryl},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Introduction to a System for Implementing Neural Net Connections on SIMD Architectures},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/069a002768bcb31509d4901961f23b3c-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/069a002768bcb31509d4901961f23b3c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/069a002768bcb31509d4901961f23b3c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2668706,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13301223612003626725&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "aa61e694d4",
        "title": "Invariant Object Recognition Using a Distributed Associative Memory",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/cd3a9a55f7f3723133fa4a13628cdf03-Abstract.html",
        "author": "Harry Wechsler; George Lee Zimmerman",
        "abstract": "This  paper  describes  an  approach  to  2-dimensional  object  recognition.  Complex-log  con(cid:173) formal  mapping  is  combined  with  a  distributed  associative  memory  to  create  a  system  which  recognizes  objects  regardless  of  changes  in  rotation  or  scale.  Recalled  information  from  the  memorized  database  is  used  to  classify  an object,  reconstruct  the  memorized  ver(cid:173) sion  of the  object,  and estimate  the  magnitude of changes in  scale  or rotation.  The system  response  is  resistant  to  moderate  amounts of noise  and occlusion.  Several experiments,  us(cid:173) ing real,  gray scale images,  are  presented to show  the feasibility  of our approach.",
        "bibtex": "@inproceedings{NIPS1987_cd3a9a55,\n author = {Wechsler, Harry and Zimmerman, George},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Invariant Object Recognition Using a Distributed Associative Memory},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/cd3a9a55f7f3723133fa4a13628cdf03-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/cd3a9a55f7f3723133fa4a13628cdf03-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/cd3a9a55f7f3723133fa4a13628cdf03-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1894022,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9375510776559927492&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department or Electrical Engineering, University or Minnesota; Department or Electrical Engineering, University or Minnesota",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.umn.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c79f1ada52",
        "title": "LEARNING BY STATE RECURRENCE DETECTION",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/eca986d585a03890a412587a2f5ccb43-Abstract.html",
        "author": "Bruce E. Rosen; James M. Goodwin; Jacques J. Vidal",
        "abstract": "This research investigates a new technique for unsupervised learning of nonlinear  control problems. The approach is applied both to Michie and Chambers BOXES  algorithm and to Barto, Sutton and Anderson's extension, the ASE/ACE system, and  has significantly improved the convergence rate of stochastically based learning  automata.",
        "bibtex": "@inproceedings{NIPS1987_eca986d5,\n author = {Rosen, Bruce and Goodwin, James and Vidal, Jacques},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {LEARNING BY STATE RECURRENCE DETECTION},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/eca986d585a03890a412587a2f5ccb43-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/eca986d585a03890a412587a2f5ccb43-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/eca986d585a03890a412587a2f5ccb43-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2275478,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=737376012197096093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8b789857e6",
        "title": "Learning Representations by Recirculation",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/afacc5db3e0e85b446e6c7727cd7dca5-Abstract.html",
        "author": "Geoffrey E. Hinton; James L. McClelland",
        "abstract": "We describe a new learning procedure for networks that contain groups of non(cid:173)",
        "bibtex": "@inproceedings{NIPS1987_afacc5db,\n author = {Hinton, Geoffrey E and McClelland, James},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Learning Representations by Recirculation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/afacc5db3e0e85b446e6c7727cd7dca5-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/afacc5db3e0e85b446e6c7727cd7dca5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/afacc5db3e0e85b446e6c7727cd7dca5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1916060,
        "gs_citation": 288,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8920871170136261152&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science and Psychology Departments, University of Toronto, Toronto M5S lA4, Canada; Psychology and Computer Science Departments, Carnegie-Mellon University, Pittsburgh, PA 15213",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Toronto;Carnegie Mellon University",
        "aff_unique_dep": "Computer Science and Psychology Departments;Department of Psychology, Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca;https://www.cmu.edu",
        "aff_unique_abbr": "U of T;CMU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Toronto;Pittsburgh",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "18c50d46dd",
        "title": "Learning a Color Algorithm from Examples",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/21ac0a2e346b1e5b77b3b61ee63eaae3-Abstract.html",
        "author": "Anya C. Hurlbert; Tomaso A. Poggio",
        "abstract": "A lightness algorithm that separates surface reflectance from illumination in a  Mondrian world is synthesized automatically from a set of examples, pairs of input  (image irradiance) and desired output (surface reflectance). The algorithm, which re(cid:173) sembles a new lightness algorithm recently proposed by Land, is approximately equiva(cid:173) lent to filtering the image through a center-surround receptive field in individual chro(cid:173) matic channels. The synthesizing technique, optimal linear estimation, requires only  one assumption, that the operator that transforms input into output is linear. This  assumption is true for a certain class of early vision algorithms that may therefore be  synthesized in a similar way from examples. Other methods of synthesizing algorithms  from examples, or \"learning\", such as backpropagation, do not yield a significantly dif(cid:173) ferent or better lightness algorithm in the Mondrian world. The linear estimation and  backpropagation techniques both produce simultaneous brightness contrast effects.",
        "bibtex": "@inproceedings{NIPS1987_21ac0a2e,\n author = {Hurlbert, Anya C. and Poggio, Tomaso A.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Learning a Color Algorithm from Examples},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/21ac0a2e346b1e5b77b3b61ee63eaae3-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/21ac0a2e346b1e5b77b3b61ee63eaae3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/21ac0a2e346b1e5b77b3b61ee63eaae3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1817764,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15655621413641567280&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Artificial Intelligence Laboratory and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, USA; Artificial Intelligence Laboratory and Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, USA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://www.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bd4dc1a353",
        "title": "Learning in Networks of Nondeterministic Adaptive Logic Elements",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/676638b91bc90529e09b22e58abb01d6-Abstract.html",
        "author": "Richard C. Windecker",
        "abstract": "from",
        "bibtex": "@inproceedings{NIPS1987_676638b9,\n author = {Windecker, Richard},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Learning in Networks of Nondeterministic Adaptive Logic Elements},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/676638b91bc90529e09b22e58abb01d6-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/676638b91bc90529e09b22e58abb01d6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/676638b91bc90529e09b22e58abb01d6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2843148,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15090718433764209466&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "AT&T Bell Laboratories, Middletown, NJ 07748 + University of Chiang Mai, Thailand + University of Guelph, Ontario",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2",
        "aff_unique_norm": "AT&T Bell Laboratories;University of Chiang Mai;University of Guelph",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.att.com/labs;https://www.cmu.ac.th;https://www.uoguelph.ca",
        "aff_unique_abbr": "AT&T Labs;CMU;U of G",
        "aff_campus_unique_index": "0+2",
        "aff_campus_unique": "Middletown;;Guelph",
        "aff_country_unique_index": "0+1+2",
        "aff_country_unique": "United States;Thailand;Canada"
    },
    {
        "id": "bb9bcc659d",
        "title": "Learning on a General Network",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/678e209691cd37f145a5502695378bac-Abstract.html",
        "author": "Amir F. Atiya",
        "abstract": "This paper generalizes the backpropagation method to a  general network containing feed(cid:173)",
        "bibtex": "@inproceedings{NIPS1987_678e2096,\n author = {Atiya, Amir},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Learning on a General Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/678e209691cd37f145a5502695378bac-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/678e209691cd37f145a5502695378bac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/678e209691cd37f145a5502695378bac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1356375,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16009379689269590788&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical Engineering, California Institute of Technology",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4ff92d170f",
        "title": "MURPHY: A Robot that Learns by Doing",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/1ccc3bfa05cb37b917068778f3c4523a-Abstract.html",
        "author": "Bartlett W. Mel",
        "abstract": "MURPHY consists of a camera looking at a robot arm, with a connectionist network  architecture situated in between. By moving its arm through a small, representative  sample of the 1 billion possible joint configurations, MURPHY learns the relationships,  backwards and forwards, between the positions of its joints and the state of its visual field.  MURPHY can use its internal model in the forward direction to \"envision\" sequences  of actions for planning purposes, such as in grabbing a visually presented object, or in  the reverse direction to \"imitate\", with its arm, autonomous activity in its visual field.  Furthermore, by taking explicit advantage of continuity in the mappings between visual  space and joint space, MURPHY is able to learn non-linear mappings with only a single  layer of modifiable weights.",
        "bibtex": "@inproceedings{NIPS1987_1ccc3bfa,\n author = {Mel, Bartlett},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {MURPHY: A Robot that Learns by Doing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/1ccc3bfa05cb37b917068778f3c4523a-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/1ccc3bfa05cb37b917068778f3c4523a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/1ccc3bfa05cb37b917068778f3c4523a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2314127,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9302548661724183580&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "71302f987a",
        "title": "Mathematical Analysis of Learning Behavior of Neuronal Models",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/5190e987c46a346974e351f96997d640-Abstract.html",
        "author": "JOHN Y. CHEUNG; MASSOUD OMIDVAR",
        "abstract": "In  this  paper,  we  wish  to  analyze  the  convergence  behavior  of a  number  of neuronal plasticity models.  Recent neurophysiological research suggests that  the neuronal behavior is adaptive.  In particular, memory stored within a neuron  is  associated with the synaptic weights which are varied or adjusted to achieve  learning.  A  number  of adaptive  neuronal  models  have  been  proposed  in  the  literature.  Three specific models will be analyzed in this paper, specifically the  Hebb model, the Sutton-Barto model, and the most recent trace model.  In this  paper we  will  examine  the conditions  for  convergence,  the  position  of conver(cid:173) gence and the rate at convergence,  of these models  as they applied to classical  conditioning.  Simulation results  are also presented to verify the analysis.",
        "bibtex": "@inproceedings{NIPS1987_5190e987,\n author = {CHEUNG, JOHN Y. and OMIDVAR, MASSOUD},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Mathematical Analysis of Learning Behavior of Neuronal Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/5190e987c46a346974e351f96997d640-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/5190e987c46a346974e351f96997d640-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/5190e987c46a346974e351f96997d640-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1448683,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6234003043860460001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "SCHOOL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE, UNIVERSITY OF OKLAHOMA; SCHOOL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE, UNIVERSITY OF OKLAHOMA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oklahoma",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.ou.edu",
        "aff_unique_abbr": "OU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a13910a860",
        "title": "Microelectronic Implementations of Connectionist Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/c48e820389ae2420c1ad9d5856e1e41c-Abstract.html",
        "author": "Stuart Mackie; Hans P. Graf; Daniel B. Schwartz; John S. Denker",
        "abstract": "In  this  paper  we  discuss  why  special  purpose  chips  are  needed  for  useful  implementations  of connectionist  neural  networks  in  such  applications  as  pattern  recognition  and  classification.  Three  chip  designs  are  described:  a  hybrid  digital/analog programmable  connection  matrix,  an  analog  connection  matrix  with  adjustable connection strengths, and a digital pipe lined best-match chip.  The common  feature  of the designs  is the distribution of arithmetic processing power amongst the  data storage to minimize data movement.",
        "bibtex": "@inproceedings{NIPS1987_c48e8203,\n author = {Mackie, Stuart and Graf, Hans P. and Schwartz, Daniel B. and Denker, John S.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Microelectronic Implementations of Connectionist Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/c48e820389ae2420c1ad9d5856e1e41c-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/c48e820389ae2420c1ad9d5856e1e41c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/c48e820389ae2420c1ad9d5856e1e41c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1960316,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11586074796293155259&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "240c0c3c17",
        "title": "Minkowski-r Back-Propagation: Learning in Connectionist Models with Non-Euclidian Error Signals",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/bbf0dfe4064e04b4d24bce800ea5abba-Abstract.html",
        "author": "Stephen Jos\u00e9 Hanson; David J. Burr",
        "abstract": "Many connectionist learning models are implemented using a gradient descent  in a least squares error function of the output and teacher signal.  The present model  Fneralizes. in particular. back-propagation [1]  by using Minkowski-r power metrics.  For  small  r's  a  \"city-block\"  error  metric  is  approximated  and  for  large  r's  the  \"maximum\" or \"supremum\"  metric is  approached.  while  for r=2  the  standard  back(cid:173) propagation  model  results.  An  implementation  of Minkowski-r back-propagation  is  described.  and  several  experiments  are  done  which  show  that  different values  of r  may be desirable for various purposes. Different r values may be appropriate for the  reduction  of  the  effects  of outliers  (noise).  modeling  the  input  space  with  more  compact clusters. or modeling  the statistics of a particular domain more naturally or  in a way that may be more perceptually or psychologically meaningful (e.g. speech or  vision).",
        "bibtex": "@inproceedings{NIPS1987_bbf0dfe4,\n author = {Hanson, Stephen Jos\\'{e} and Burr, David J.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Minkowski-r Back-Propagation: Learning in Connectionist Models with Non-Euclidian Error Signals},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/bbf0dfe4064e04b4d24bce800ea5abba-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/bbf0dfe4064e04b4d24bce800ea5abba-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/bbf0dfe4064e04b4d24bce800ea5abba-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1567420,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1515381047311896763&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Bell Communications Research; Bell Communications Research",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Bell Communications Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bell-labs.com/",
        "aff_unique_abbr": "BCR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0742a7c3f2",
        "title": "Network Generality, Training Required, and Precision Required",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/b422680f3db0986ddd7f8f126baaf0fa-Abstract.html",
        "author": "John S. Denker; Ben S. Wittner; Leon Cooper",
        "abstract": "We  show  how  to estimate  (1)  the  number  of functions  that  can  be implemented  by  a  particular  network  architecture,  (2)  how  much  analog  precision  is  needed  in  the  con(cid:173) nections in the network, and (3) the number of training examples the network must see  before it can  be expected  to form  reliable  generalizations.",
        "bibtex": "@inproceedings{NIPS1987_b422680f,\n author = {Denker, John S. and Wittner, Ben S. and Cooper, Leon},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Network Generality, Training Required, and Precision Required},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/b422680f3db0986ddd7f8f126baaf0fa-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/b422680f3db0986ddd7f8f126baaf0fa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/b422680f3db0986ddd7f8f126baaf0fa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 775258,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13760071628167853740&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "53fb5cd301",
        "title": "Neural Net and Traditional Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/5b47430e24a5a1f9fe21f0e8eb814131-Abstract.html",
        "author": "William Y. Huang; Richard P. Lippmann",
        "abstract": "Abstract.  Previous  work on  nets  with  continuous-valued inputs  led  to generative \nprocedures  to construct convex decision  regions with two-layer perceptrons (one hidden \nlayer) and  arbitrary decision  regions  with  three-layer perceptrons  (two hidden layers). \nHere we demonstrate that two-layer perceptron classifiers trained with back propagation \ncan  form  both  convex  and  disjoint  decision  regions.  Such  classifiers  are  robust,  train \nrapidly,  and  provide  good  performance  with  simple  decision  regions.  When  complex \ndecision  regions  are  required,  however,  convergence  time  can  be  excessively  long  and \nperformance is  often no better than that of k-nearest  neighbor classifiers.  Three neural \nnet  classifiers  are  presented  that  provide  more  rapid  training  under  such  situations. \nTwo use  fixed  weights in  the  first  one  or  two layers and  are  similar  to classifiers  that \nestimate probability density functions using histograms.  A third \"feature map classifier\" \nuses  both  unsupervised  and  supervised  training.  It provides  good  performance  with \nlittle supervised training in situations such as speech recognition where much unlabeled \ntraining data is  available.  The architecture of this classifier can  be  used  to implement \na  neural net  k-nearest  neighbor classifier.",
        "bibtex": "@inproceedings{NIPS1987_5b47430e,\n author = {Huang, William Y. and Lippmann, Richard P.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Neural Net and Traditional Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/5b47430e24a5a1f9fe21f0e8eb814131-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/5b47430e24a5a1f9fe21f0e8eb814131-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/5b47430e24a5a1f9fe21f0e8eb814131-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2119387,
        "gs_citation": 300,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15017796644498900907&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8eac1a2921",
        "title": "Neural Network Implementation Approaches for the Connection Machine",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/de081105cd68393144944696d3fb6778-Abstract.html",
        "author": "Nathan H. Brown; Jr.",
        "abstract": "The SIMD parallelism of the Connection Machine (eM) allows the construction of \nneural network simulations by the use of simple data and control structures.  Two \napproaches are described which allow parallel computation of a model's nonlinear \nfunctions, parallel modification of a model's weights, and parallel propagation of a \nmodel's activation and error.  Each approach also allows a model's interconnect \nstructure to be physically dynamic.  A Hopfield model is implemented with each \napproach at six sizes over the same number of CM processors to provide a performance \ncomparison.",
        "bibtex": "@inproceedings{NIPS1987_de081105,\n author = {Brown, Nathan H.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Neural Network Implementation Approaches for the Connection Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/de081105cd68393144944696d3fb6778-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/de081105cd68393144944696d3fb6778-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/de081105cd68393144944696d3fb6778-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2382285,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15072778699494050098&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9e9dea5fe4",
        "title": "Neural Networks for Template Matching: Application to Real-Time Classification of the Action Potentials of Real Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/e8adc089e27840d852bf8d13951090b4-Abstract.html",
        "author": "Yiu-fai Wong; Jashojiban Banik; James M. Bower",
        "abstract": "Much  experimental study  of  real  neural  networks  relies  on  the  proper  classification  of",
        "bibtex": "@inproceedings{NIPS1987_e8adc089,\n author = {Wong, Yiu-fai and Banik, Jashojiban and Bower, James M.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Neural Networks for Template Matching: Application to Real-Time Classification of the Action Potentials of Real Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/e8adc089e27840d852bf8d13951090b4-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/e8adc089e27840d852bf8d13951090b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/e8adc089e27840d852bf8d13951090b4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1957916,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15098045654942740803&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Division of Engineering and Applied Science; Division of Biology; Division of Engineering and Applied Science + Division of Biology",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "California Institute of Technology;Division of Biology",
        "aff_unique_dep": "Division of Engineering and Applied Science;Division of Biology",
        "aff_unique_url": "https://www.caltech.edu;",
        "aff_unique_abbr": "Caltech;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "809e9f063e",
        "title": "Neuromorphic Networks Based on Sparse Optical Orthogonal Codes",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/4b613e438a6b3842f77056e5b4c9b42e-Abstract.html",
        "author": "Mario P. Vecchi; Jawad A. Salehi",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1987_4b613e43,\n author = {Vecchi, Mario and Salehi, Jawad},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Neuromorphic Networks Based on Sparse Optical Orthogonal Codes},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/4b613e438a6b3842f77056e5b4c9b42e-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/4b613e438a6b3842f77056e5b4c9b42e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/4b613e438a6b3842f77056e5b4c9b42e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2035960,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4080482915757783514&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Bell Communications Research; Bell Communications Research",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Bell Communications Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bell-labs.com/",
        "aff_unique_abbr": "BCR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "011b17bc64",
        "title": "New Hardware for Massive Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/4dd1a7279a8cfeea2660fbc34f02a2bc-Abstract.html",
        "author": "D. D. Coon; A. G. U. Perera",
        "abstract": "Transient phenomena associated with forward biased silicon p + - n - n + struc(cid:173) tures at 4.2K show remarkable similarities with biological neurons.  The devices  play  a  role  similar to the  two-terminal switching elements in  Hodgkin-Huxley  equivalent  circuit  diagrams.  The  devices  provide simpler  and  more  realistic  neuron  emulation  than  transistors  or op-amps.  They  have  such  low  power  and  current  requirements  that  they  could  be  used  in  massive  neural  networks.  Some  observed  properties  of  simple  circuits  containing  the  devices  include  action  potentials,  refractory  periods,  threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic  weights,  temporal  integration, memory,  network  connectivity modification  based  on  experience, pacemaker activity, firing  thresholds, coupling to sensors with graded sig(cid:173) nal  outputs  and  the  dependence  of firing  rate  on input  current.  Transfer functions  for simple artificial neurons with spiketrain inputs and spiketrain outputs have been  measured  and correlated with input coupling.",
        "bibtex": "@inproceedings{NIPS1987_4dd1a727,\n author = {Coon, D. D. and Perera, A. G. U.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {New Hardware for Massive Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/4dd1a7279a8cfeea2660fbc34f02a2bc-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/4dd1a7279a8cfeea2660fbc34f02a2bc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/4dd1a7279a8cfeea2660fbc34f02a2bc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1908821,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8192842717699266271&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Applied Technology Laboratory, University of Pittsburgh; Applied Technology Laboratory, University of Pittsburgh",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "Applied Technology Laboratory",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "854adadbb2",
        "title": "On Properties of Networks of Neuron-Like Elements",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/8af873095176650d4c5e738e35383498-Abstract.html",
        "author": "Pierre Baldi; Santosh S. Venkatesh",
        "abstract": "The  complexity  and  computational  capacity  of multi-layered,  feedforward  neural networks is examined.  Neural networks for special purpose (structured)  functions  are examined  from  the  perspective of circuit  complexity.  Known  re(cid:173) sults in  complexity theory are applied to the special instance of neural network  circuits,  and  in  particular,  classes  of functions  that  can  be  implemented  in  shallow circuits characterised.  Some conclusions are  also  drawn about learning  complexity,  and some  open problems raised.  The dual  problem of determining  the computational capacity of a  class of multi-layered  networks with  dynamics  regulated  by  an  algebraic  Hamiltonian  is  considered.  Formal  results  are  pre(cid:173) sented  on  the  storage  capacities  of programmed  higher-order  structures,  and  a  tradeoff between ease  of programming  and  capacity is  shown.  A  precise  de(cid:173) termination is  made of the static fixed  point structure of random higher-order  constructs,  and phase-transitions (0-1  laws)  are  shown.",
        "bibtex": "@inproceedings{NIPS1987_8af87309,\n author = {Baldi, Pierre and Venkatesh, Santosh},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {On Properties of Networks of Neuron-Like Elements},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/8af873095176650d4c5e738e35383498-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/8af873095176650d4c5e738e35383498-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/8af873095176650d4c5e738e35383498-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2485165,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4421724640092294161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Mathematics, University of California (San Diego), La Jolla, CA 92093; Moore School of Electrical Engineering, University of Pennsylvania, Philadelphia, PA 19104",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, San Diego;University of Pennsylvania",
        "aff_unique_dep": "Department of Mathematics;Moore School of Electrical Engineering",
        "aff_unique_url": "https://ucsd.edu;https://www.upenn.edu",
        "aff_unique_abbr": "UCSD;UPenn",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "La Jolla;Philadelphia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0fec45d6d4",
        "title": "On Tropistic Processing and Its Applications",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/baf570e47e7f4e314a9ffb72c4a5459c-Abstract.html",
        "author": "Manuel F. Fern\u00e1ndez",
        "abstract": "The  interaction  of  a  set  of  tropisms  is  sufficient  in  many",
        "bibtex": "@inproceedings{NIPS1987_baf570e4,\n author = {Fern\\'{a}ndez, Manuel},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {On Tropistic Processing and Its Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/baf570e47e7f4e314a9ffb72c4a5459c-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/baf570e47e7f4e314a9ffb72c4a5459c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/baf570e47e7f4e314a9ffb72c4a5459c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1612414,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0D9Rt_upoRQJ:scholar.google.com/&scioq=On+Tropistic+Processing+and+Its+Applications&hl=en&as_sdt=0,5",
        "gs_version_total": 6,
        "aff": "General Electric Advanced Technology Laboratories",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "General Electric",
        "aff_unique_dep": "Advanced Technology Laboratories",
        "aff_unique_url": "https://www.ge.com/research",
        "aff_unique_abbr": "GE",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c3ce3af75d",
        "title": "On the Power of Neural Networks for Solving Hard Problems",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/98ae8cb22462884b1b8c0b351779292b-Abstract.html",
        "author": "Jehoshua Bruck; Joseph W. Goodman",
        "abstract": "This  paper deals  with a  neural network model in  which each neuron  performs a  threshold logic function.  An important property of the model  is  that  it always  converges  to a  stable state  when  operating in  a  serial  mode [2,5].  This property is  the basis of the potential applications of the  model such as associative memory devices and combinatorial optimization  [3,6].  One of the motivations for use of the model for solving hard combinatorial  problems  is  the fact  that it can  be implemented  by optical devices  and  thus operate at a  higher speed  than conventional electronics.  The main theme in this work is  to investigate the power of the model for  solving NP-hard problems  [4,8],  and to understand  the relation  between  speed of operation and the size of a  neural network.  In particular, it will  be  shown  that  for  any  NP-hard  problem  the  existence  of a  polynomial  size  network  that  solves  it  implies  that  NP=co-NP.  Also,  for  Traveling  Salesman  Problem  (TSP), even  a  polynomial  size  network  that  gets  an  \u20ac-approximate  solution does  not exist unless  P=NP.",
        "bibtex": "@inproceedings{NIPS1987_98ae8cb2,\n author = {Bruck, Jehoshua and Goodman, Joseph},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {On the Power of Neural Networks for Solving Hard Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/98ae8cb22462884b1b8c0b351779292b-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/98ae8cb22462884b1b8c0b351779292b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/98ae8cb22462884b1b8c0b351779292b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1269361,
        "gs_citation": 125,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15373396395293764050&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Information Systems Laboratory, Department of Electrical Engineering, Stanford University; Information Systems Laboratory, Department of Electrical Engineering, Stanford University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "75b9d299b9",
        "title": "Optimal Neural Spike Classification",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/935ad074f32d1e8f085a143449894cdc-Abstract.html",
        "author": "Amir F. Atiya; James M. Bower",
        "abstract": "Being able  to record the electrical activities of a  number of neurons simultaneously is  likely  to be important in  the study of the functional organization of networks of real neurons.  Using  one  extracellular  microelectrode  to  record  from  several neurons  is  one  approach  to  studying  the  response  properties  of sets  of  adjacent  and  therefore  likely  related  neurons.  However,  to  do  this,  it  is  necessary  to  correctly  classify  the  signals  generated  by  these  different  neurons.  This paper considers  this problem of classifying the  signals  in  such an  extracellular recording,  based upon their shapes, and specifically considers the classification of signals in the case when  spikes overlap  temporally.",
        "bibtex": "@inproceedings{NIPS1987_935ad074,\n author = {Atiya, Amir F. and Bower, James M.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Optimal Neural Spike Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/935ad074f32d1e8f085a143449894cdc-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/935ad074f32d1e8f085a143449894cdc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/935ad074f32d1e8f085a143449894cdc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1562485,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15787106760971289444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Electrical Engineering; Division of Biology",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University Affiliation Not Specified;Division of Biology",
        "aff_unique_dep": "Department of Electrical Engineering;Division of Biology",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "edd6022b45",
        "title": "Optimization with Artificial Neural Network Systems: A Mapping Principle and a Comparison to Gradient Based Methods",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/d0e33d1c7888753180d9ccc28c983685-Abstract.html",
        "author": "Harrison MonFook Leong",
        "abstract": "General  formulae  for  mapping  optimization  problems  into  systems  of  ordinary  differential",
        "bibtex": "@inproceedings{NIPS1987_d0e33d1c,\n author = {Leong, Harrison},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Optimization with Artificial Neural Network Systems: A Mapping Principle and a Comparison to Gradient Based Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/d0e33d1c7888753180d9ccc28c983685-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/d0e33d1c7888753180d9ccc28c983685-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/d0e33d1c7888753180d9ccc28c983685-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2623378,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8918221824820620091&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0998254ed7",
        "title": "PARTITIONING OF SENSORY DATA BY A CORTICAL NETWORK",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/d9cd83bc91b8c36a0c7c0fcca59228f2-Abstract.html",
        "author": "Richard Granger; Jos\u00e9 Ambros-Ingerson; Howard Henry; Gary Lynch",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1987_d9cd83bc,\n author = {Granger, Richard and Ambros-Ingerson, Jos\\'{e} and Henry, Howard and Lynch, Gary},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {PARTITIONING OF SENSORY DATA BY A CORTICAL NETWORK},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/d9cd83bc91b8c36a0c7c0fcca59228f2-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/d9cd83bc91b8c36a0c7c0fcca59228f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/d9cd83bc91b8c36a0c7c0fcca59228f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4878687,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3449989900462106808&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c2126cd9b5",
        "title": "PATTERN CLASS DEGENERACY IN AN UNRESTRICTED STORAGE DENSITY MEMORY",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/03f8faa47a2e9614453e50aaf8d7f278-Abstract.html",
        "author": "Christopher L. Scofield; Douglas L. Reilly; Charles Elbaum; Leon N. Cooper",
        "abstract": "in",
        "bibtex": "@inproceedings{NIPS1987_03f8faa4,\n author = {Scofield, Christopher and Reilly, Douglas L. and Elbaum, Charles and Cooper, Leon},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {PATTERN CLASS DEGENERACY IN AN UNRESTRICTED STORAGE DENSITY MEMORY},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/03f8faa47a2e9614453e50aaf8d7f278-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/03f8faa47a2e9614453e50aaf8d7f278-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/03f8faa47a2e9614453e50aaf8d7f278-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1634817,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10897170609299563184&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "10c9473837",
        "title": "Performance Measures for Associative Memories that Learn and Forget",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/d0b9a3081f811b2a307c38ad457a487c-Abstract.html",
        "author": "Anthony Kuh",
        "abstract": "Recently,  many  modifications  to  the  McCulloch/Pitts  model  have  been  proposed  where  both  learning  and  forgetting  occur.  Given  that  the  network  never saturates  (ceases  to  function  effectively  due  to  an  overload  of  information),  the  learning  updates  can  con(cid:173) tinue indefinitely.  For these networks,  we  need  to introduce  performance  measmes in  addi(cid:173) tion  to  the  information  capacity  to  evaluate  the  different  networks.  We  mathematically  define  quantities such  as  the  plasticity  of  a  network,  the  efficacy  of an  information  vector,  and the  probability  of network  saturation.  From  these  quantities  we  analytically  compare  different networks.",
        "bibtex": "@inproceedings{NIPS1987_d0b9a308,\n author = {Kuh, Anthony},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Performance Measures for Associative Memories that Learn and Forget},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/d0b9a3081f811b2a307c38ad457a487c-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/d0b9a3081f811b2a307c38ad457a487c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/d0b9a3081f811b2a307c38ad457a487c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2101605,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5205762953287457539&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical Engineering, University of Hawaii at Manoa",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Hawaii at Manoa",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.hawaii.edu",
        "aff_unique_abbr": "UH Manoa",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Manoa",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c10439ccb7",
        "title": "Phase Transitions in Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/0e65cd5d77a5e8b2d1f9ab31ba50b49b-Abstract.html",
        "author": "Joshua Chover",
        "abstract": "Various  simulat.ions  of  cort.ical  subnetworks  have  evidenced  something  like  phase  transitions  with  respect  to  key  parameters.  We  demonstrate  that.  such  transi t.ions  must.  indeed  exist.  in  analogous  infinite  array  models.  For  related  finite  array  models  classical  phase  transi t.ions  (which  describe  steady-state  behavior)  may  not.  exist.,  but.  there  can  be  distinct. quali tative  changes  in  (\"metastable\")  transient  behavior  as  key  system  parameters  pass  through  crit.ical  values .",
        "bibtex": "@inproceedings{NIPS1987_0e65cd5d,\n author = {Chover, Joshua},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Phase Transitions in Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/0e65cd5d77a5e8b2d1f9ab31ba50b49b-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/0e65cd5d77a5e8b2d1f9ab31ba50b49b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/0e65cd5d77a5e8b2d1f9ab31ba50b49b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1588542,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9801715465882846230&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Wisconsin, Madison, WI 53706",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "61f1357bae",
        "title": "Phasor Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/25a3192c804d6b1c7d309c0155d3aa1a-Abstract.html",
        "author": "Andr\u00e9 J. Noest",
        "abstract": "A novel  network  type  is  introduced  which  uses  unit-length  2-vectors",
        "bibtex": "@inproceedings{NIPS1987_25a3192c,\n author = {Noest, Andr\\'{e}},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Phasor Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/25a3192c804d6b1c7d309c0155d3aa1a-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/25a3192c804d6b1c7d309c0155d3aa1a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/25a3192c804d6b1c7d309c0155d3aa1a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1411027,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2321727650276255707&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "N.I.B.R., NL-ll0S AZ Amsterdam, The Netherlands",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Netherlands Institute for Brain Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nibr.nl",
        "aff_unique_abbr": "N.I.B.R.",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "ebed501d6a",
        "title": "Presynaptic Neural Information Processing",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/f67b34cb0f0d24b6226178aa6a649cc4-Abstract.html",
        "author": "L. R. Carley",
        "abstract": "The  potential  for  presynaptic  information  processing  within  the  arbor  of a  single  axon  will  be  discussed  in  this  paper.  Current  knowledge  about  the  activity  dependence  of  the  firing  threshold,  the  conditions  required  for  conduction  failure,  and  the  similarity  of  nodes  along  a  single  axon  will  be  reviewed.  An  electronic  circuit  model  for  a  site  of low  conduction  safety  in  an  axon  will  be  presented.  In  response to  single  frequency  stimulation  the  electronic circuit acts  as  a  lowpass filter.",
        "bibtex": "@inproceedings{NIPS1987_f67b34cb,\n author = {Carley, L. R.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Presynaptic Neural Information Processing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/f67b34cb0f0d24b6226178aa6a649cc4-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/f67b34cb0f0d24b6226178aa6a649cc4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/f67b34cb0f0d24b6226178aa6a649cc4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1818427,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh PA 15213",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e1ac05674f",
        "title": "Probabilistic Characterization of Neural Model Computations",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/3ed923f9f88108cb066c6568d3df2666-Abstract.html",
        "author": "Richard M. Golden",
        "abstract": "Information  retrieval  in  a  neural  network  is  viewed  as  a  procedure  in",
        "bibtex": "@inproceedings{NIPS1987_3ed923f9,\n author = {Golden, Richard},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Probabilistic Characterization of Neural Model Computations},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/3ed923f9f88108cb066c6568d3df2666-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/3ed923f9f88108cb066c6568d3df2666-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/3ed923f9f88108cb066c6568d3df2666-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1181479,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1860600279497165664&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Pittsburgh, Pittsburgh, Pa. 15260 + Stanford University, Stanford, California, 94305",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "University of Pittsburgh;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.pitt.edu;https://www.stanford.edu",
        "aff_unique_abbr": "Pitt;Stanford",
        "aff_campus_unique_index": "0+1",
        "aff_campus_unique": "Pittsburgh;Stanford",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "091b33d42e",
        "title": "Programmable Synaptic Chip for Electronic Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/9db64c20dee0011899dfdf200e61ef35-Abstract.html",
        "author": "A. Moopenn; H. Langenbacher; A.P. Thakoor; S.K. Khanna",
        "abstract": "A binary  synaptic  matrix  chip  has  been  developed  for  electronic  neural  networks.  The  matrix  chip  contains  a  programmable  32X32  array  of  \"long  channel\"  NMOSFET  binary  connection  elements  imple(cid:173) mented  in  a  3-um  bulk  CMOS  process.  Since  the  neurons  are  kept  off(cid:173) chip,  the  synaptic  chip  serves  as  a  \"cascadable\"  building  block  for  a  multi-chip  synaptic  network  as  large  as  512X512  in  size.  As  an  alternative  the  programmable  NMOSFET  (long  channel)  connection  elements,  tailored  thin  film  resistors  are  deposited,  in  series  with  FET  switches,  on  some  CMOS  test  chips,  to  obtain  the  weak  synaptic  connections.  Although  deposition  and  patterning  of  the  resistors  require  additional  they  promise  substantial  savings  in  silcon  area.  The  performance  of  a  synaptic  chip  in  a  32- neuron  breadboard  system  in  an  associative  memory  test  application  is  discussed.",
        "bibtex": "@inproceedings{NIPS1987_9db64c20,\n author = {Moopenn, A. and Langenbacher, H. and Thakoor, A.P. and Khanna, S.K.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Programmable Synaptic Chip for Electronic Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/9db64c20dee0011899dfdf200e61ef35-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/9db64c20dee0011899dfdf200e61ef35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/9db64c20dee0011899dfdf200e61ef35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1862333,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8412573757706115012&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "004703250a",
        "title": "REFLEXIVE ASSOCIATIVE MEMORIES",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/ae5bb54885c12561e43ac87f6f89e934-Abstract.html",
        "author": "Hendricus G. Loos",
        "abstract": "In the synchronous discrete model, the average memory capacity of  bidirectional associative memories (BAMs) is compared with that of  Hopfield memories, by means of a calculat10n of the percentage of good  recall for 100 random BAMs of dimension 64x64, for different numbers  of stored vectors. The memory capac1ty Is found to be much smal1er than  the Kosko upper bound, which Is the lesser of the two dimensions of the  BAM. On the average, a 64x64 BAM has about 68 % of the capacity of the  corresponding Hopfield memory with the same number of neurons. Ortho(cid:173) normal coding of the BAM Increases the effective storage capaCity by  only 25 %. The memory capacity limitations are due to spurious stable  states, which arise In BAMs In much the same way as in Hopfleld  memories. Occurrence of spurious stable states can be avoided by  replacing the thresholding in the backlayer of the BAM by another  nonl1near process, here called \"Dominant Label Selection\" (DLS). The  simplest DLS is the wlnner-take-all net, which gives a fault-sensitive  memory. Fault tolerance can be improved by the use of an orthogonal or  unitary transformation. An optical application of the latter is a Fourier  transform, which is implemented simply by a lens.",
        "bibtex": "@inproceedings{NIPS1987_ae5bb548,\n author = {Loos, Hendricus G.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {REFLEXIVE ASSOCIATIVE MEMORIES},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/ae5bb54885c12561e43ac87f6f89e934-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/ae5bb54885c12561e43ac87f6f89e934-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/ae5bb54885c12561e43ac87f6f89e934-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2029122,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7997569563466845951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Laguna Research Laboratory, Fallbrook, CA 92028-9765",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Laguna Research Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f0fe2009fe",
        "title": "SPONTANEOUS AND  INFORMATION-TRIGGERED SEGMENTS OF SERIES OF HUMAN BRAIN ELECTRIC FIELD MAPS",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/a05a7b15b2006378156a777243d58818-Abstract.html",
        "author": "D. Lehmann; D. Brandeis; A. Horst; H. Ozaki; I. Pal",
        "abstract": "The brain works in a state-dependent manner: processin9",
        "bibtex": "@inproceedings{NIPS1987_a05a7b15,\n author = {Lehmann, D. and Brandeis, D. and Horst, A. and Ozaki, H. and Pal, I.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {SPONTANEOUS AND  INFORMATION-TRIGGERED SEGMENTS OF SERIES OF HUMAN BRAIN ELECTRIC FIELD MAPS},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/a05a7b15b2006378156a777243d58818-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/a05a7b15b2006378156a777243d58818-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/a05a7b15b2006378156a777243d58818-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1575737,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3074534843865807046&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Neurol09Y Department, University Hospital, 8091 Zurich, Switzerland; Neurol09Y Department, University Hospital, 8091 Zurich, Switzerland + Psychiat. Dept., V.A. Med. Center, San Francisco CA 94121; Neurol09Y Department, University Hospital, 8091 Zurich, Switzerland; Neurol09Y Department, University Hospital, 8091 Zurich, Switzerland + lab. Physiol. for the Developmentally Handicapped, Ibaraki Univ., Mito, Japan 310; Neurol09Y Department, University Hospital, 8091 Zurich, Switzerland + Biol09ic Systems Corp., Mundelein Il 60060",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0+2;0+3",
        "aff_unique_norm": "University Hospital Zurich;Veterans Affairs Medical Center;Ibaraki University;Biol09ic Systems Corp.",
        "aff_unique_dep": "Neurol09Y Department;Department of Psychiatry;Department of Physiology for the Developmentally Handicapped;",
        "aff_unique_url": "https://www.unizh.ch;https://www.va.gov/health;https://www.ibaraki.ac.jp;",
        "aff_unique_abbr": ";VAMC;Ibaraki Univ.;",
        "aff_campus_unique_index": "0;0+1;0;0+2;0",
        "aff_campus_unique": "Zurich;San Francisco;Mito;",
        "aff_country_unique_index": "0;0+1;0;0+2;0+1",
        "aff_country_unique": "Switzerland;United States;Japan"
    },
    {
        "id": "6225854d2a",
        "title": "Scaling Properties of Coarse-Coded Symbol Memories",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/317fd294bfd5c40816ce48bae30b1d4c-Abstract.html",
        "author": "Ronald Rosenfeld; David S. Touretzky",
        "abstract": "Abstract:  Coarse-coded symbol memories have appeared in several neural network \nsymbol  processing  models.  In  order  to  determine  how  these  models  would  scale,  one \nmust  first  have  some  understanding  of the  mathematics  of coarse-coded  representa(cid:173)\ntions.  We  define  the  general  structure  of coarse-coded  symbol  memories  and  derive \nmathematical relationships among  their essential parameters:  memory 8ize,  8ymbol-8et \nsize and capacity.  The computed capacity of one of the schemes agrees well with actual \nmeasurements oC  tbe coarse-coded working memory of DCPS, Touretzky and Hinton's \ndistributed connectionist production system.",
        "bibtex": "@inproceedings{NIPS1987_317fd294,\n author = {Rosenfeld, Ronald and Touretzky, David},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Scaling Properties of Coarse-Coded Symbol Memories},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/317fd294bfd5c40816ce48bae30b1d4c-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/317fd294bfd5c40816ce48bae30b1d4c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/317fd294bfd5c40816ce48bae30b1d4c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2083079,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=278418289160721306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Department, Carnegie Mellon University; Computer Science Department, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "95e6a6ff41",
        "title": "Schema for Motor Control Utilizing a Network Model of the Cerebellum",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/ee5cc652ce2fdd25562a9411809f6d8f-Abstract.html",
        "author": "James C. Houk",
        "abstract": "This  paper  outlines  a  schema  for  movement  control",
        "bibtex": "@inproceedings{NIPS1987_ee5cc652,\n author = {Houk, James},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Schema for Motor Control Utilizing a Network Model of the Cerebellum},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/ee5cc652ce2fdd25562a9411809f6d8f-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/ee5cc652ce2fdd25562a9411809f6d8f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/ee5cc652ce2fdd25562a9411809f6d8f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2275436,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3316590673866794341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Northwestern University Medical School, Chicago, Illinois",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Northwestern University",
        "aff_unique_dep": "Medical School",
        "aff_unique_url": "https://www.northwestern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3e905f5e93",
        "title": "Self-Organization of Associative Database and Its Applications",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/e8a21a93f244b29e4da50ccbe409c28f-Abstract.html",
        "author": "Hisashi Suzuki; Suguru Arimoto",
        "abstract": "An  efficient  method  of self-organizing  associative  databases  is  proposed  together  with  applications  to  robot  eyesight  systems.  The  proposed  databases  can  associate  any  input  with  some  output.  In  the  first  half part  of discussion,  an  algorithm of self-organization  is  proposed.  From  an  aspect  of  hardware,  it  produces  a  new  style  of  neural  network.  In  the  latter half part, an applicability to handwritten letter recognition and that to an autonomous  mobile  robot system are demonstrated.",
        "bibtex": "@inproceedings{NIPS1987_e8a21a93,\n author = {Suzuki, Hisashi and Arimoto, Suguru},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Self-Organization of Associative Database and Its Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/e8a21a93f244b29e4da50ccbe409c28f-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/e8a21a93f244b29e4da50ccbe409c28f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/e8a21a93f244b29e4da50ccbe409c28f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1702157,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16156761970987243227&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Osaka University, Toyonaka, Osaka 560, Japan; Osaka University, Toyonaka, Osaka 560, Japan",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Osaka University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "Osaka U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toyonaka",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "60c26433bd",
        "title": "Simulations Suggest Information Processing Roles for the Diverse Currents in Hippocampal Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/b328b0417abe3085884133503d2bc151-Abstract.html",
        "author": "Lyle J. Borg-Graham",
        "abstract": "A computer model of the hippocampal pyramidal cell (HPC) is  described",
        "bibtex": "@inproceedings{NIPS1987_b328b041,\n author = {Borg-Graham, Lyle},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Simulations Suggest Information Processing Roles for the Diverse Currents in Hippocampal Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/b328b0417abe3085884133503d2bc151-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/b328b0417abe3085884133503d2bc151-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/b328b0417abe3085884133503d2bc151-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2254009,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7790842175568063736&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4a6f24751b",
        "title": "Spatial Organization of Neural Networks: A Probabilistic Modeling Approach",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/e09d45e14e9ece7142217550ddd3c4d0-Abstract.html",
        "author": "A. Stafylopatis; M. Dikaiakos; D. Kontoravdis",
        "abstract": "The  aim  of  this  paper  is  to  explore  the  spatial  organization  of  neural  networks  under  Markovian  assumptions,  in  what  concerns  the be(cid:173) haviour  of  individual  cells  and  the  interconnection  mechanism.  Space(cid:173) organizational  properties  of  neural  nets  are  very  relevant  in  image  modeling  and  pattern  analysis,  where  spatial  computations  on  stocha(cid:173) stic  two-dimensional  image  fields  are  involved.  As  a  first  approach  we  develop  a  random  neural  network  model,  based  upon  simple  probabi(cid:173) listic  assumptions,  whose  organization  is  studied  by  means  of  dis(cid:173) crete-event  simulation.  We  then  investigate  the  possibility  of  ap(cid:173) proXimating  the  random  network's  behaviour  by  using  an  analytical  ap(cid:173) proach  originating  from  the  theory  of  general  product-form  queueing  networks.  The  neural  network  is  described  by  an  open  network  of  no(cid:173) des,  in  which  customers  moving  from  node  to  node  represent  stimula(cid:173) tions  and  connections  between  nodes  are  expressed  in  terms  of  sui(cid:173) tably  selected  routing  probabilities.  We  obtain  the  solution  of  the  model  under  different  disciplines  affecting  the  time  spent  by  a  sti(cid:173) mulation  at  each  node  visited.  Results  concerning  the  distribution  of  excitation  in  the  network  as  a  function  of  network  topology  and  external  stimulation  arrival  pattern  are  compared  with  measures  ob(cid:173) tained  from  the  simulation  and  validate  the  approach  followed.",
        "bibtex": "@inproceedings{NIPS1987_e09d45e1,\n author = {Stafylopatis, A. and Dikaiakos, M. and Kontoravdis, D.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Spatial Organization of Neural Networks: A Probabilistic Modeling Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/e09d45e14e9ece7142217550ddd3c4d0-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/e09d45e14e9ece7142217550ddd3c4d0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/e09d45e14e9ece7142217550ddd3c4d0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2326897,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2035662130020307234&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "583c13f257",
        "title": "Speech Recognition Experiments with Perceptrons",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/b3cd73d353d39e5cf6f6e9ff8d14c87f-Abstract.html",
        "author": "D. J. Burr",
        "abstract": "Artificial  neural  networks  (ANNs)  are  capable  of accurate  recognition  of  simple speech  vocabularies such  as  isolated  digits  [1].  This paper looks  at two  more  difficult  vocabularies,  the  alphabetic  E-set  and  a  set  of  polysyllabic  words.  The  E-set  is  difficult  because  it  contains  weak  discriminants  and  polysyllables  are  difficult  because  of  timing  variation.  Polysyllabic  word  recognition  is  aided  by a  time  pre-alignment technique  based on  dynamic pro(cid:173) gramming  and  E-set  recognition  is  improved  by  focusing  attention.  Recogni(cid:173) tion  accuracies  are  better  than  98%  for  both  vocabularies  when  implemented  with a single  layer perceptron.",
        "bibtex": "@inproceedings{NIPS1987_b3cd73d3,\n author = {Burr, D. J.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Speech Recognition Experiments with Perceptrons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/b3cd73d353d39e5cf6f6e9ff8d14c87f-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/b3cd73d353d39e5cf6f6e9ff8d14c87f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/b3cd73d353d39e5cf6f6e9ff8d14c87f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1737160,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2694660618408562293&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Bell Communications Research",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Bell Communications Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bell-labs.com/",
        "aff_unique_abbr": "BCR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5a403faa01",
        "title": "Stability Results for Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/b2801c6afc7057f3a3d331a71a028611-Abstract.html",
        "author": "A. N. Michel; J. A. Farrell; W. Porod",
        "abstract": "In the present paper we survey and utilize results from the qualitative theory of large  scale interconnected dynamical systems in order to develop  a  qualitative theory for  the  Hopfield model of neural networks.  In our approach we  view such networks as  an inter(cid:173) connection of many single neurons.  Our results  are  phrased in  terms of the  qualitative  properties of the individual neurons and in terms of the properties of the interconnecting  structure of the neural  networks.  Aspects of neural networks which  we  address include  asymptotic stability,  exponential stability,  and instability  of an  equilibrium;  estimates  of trajectory bounds; estimates of the domain of attraction of an asymptotically stable  equilibrium;  and stability of neural networks  under structural perturbations.",
        "bibtex": "@inproceedings{NIPS1987_b2801c6a,\n author = {Michel, A. N. and Farrell, J. A. and Porod, W.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Stability Results for Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/b2801c6afc7057f3a3d331a71a028611-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/b2801c6afc7057f3a3d331a71a028611-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/b2801c6afc7057f3a3d331a71a028611-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1957991,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11216706163312958154&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical and Computer Engineering, University of Notre Dame; Department of Electrical and Computer Engineering, University of Notre Dame; Department of Electrical and Computer Engineering, University of Notre Dame",
        "aff_domain": "; ;",
        "email": "; ;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Notre Dame",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.nd.edu",
        "aff_unique_abbr": "Notre Dame",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5021cb26cf",
        "title": "Static and Dynamic Error Propagation Networks with Application to Speech Coding",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/0316d8d63a0c252a3ec57921d7d2429b-Abstract.html",
        "author": "A J Robinson; F Fallside",
        "abstract": "Error propagation nets have been shown to be able to learn a variety of tasks in  which a static input pattern is mapped outo a static output pattern. This paper  presents a generalisation of these nets to deal with time varying, or dynamic  patterns, and three possible architectures are explored. As an example, dynamic  nets are applied to tbe problem of speech coding, in which a time sequence of  speech data are coded by one net and decoded by another. The use of dynamic  nets gives a better signal to noise ratio than that achieved using static nets.",
        "bibtex": "@inproceedings{NIPS1987_0316d8d6,\n author = {Robinson, A J and Fallside, F},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Static and Dynamic Error Propagation Networks with Application to Speech Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/0316d8d63a0c252a3ec57921d7d2429b-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/0316d8d63a0c252a3ec57921d7d2429b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/0316d8d63a0c252a3ec57921d7d2429b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1873472,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2106733195440156217&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "319d29b5b6",
        "title": "Stochastic Learning Networks and their Electronic Implementation",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/b9799a12d683d136cc817f94b73a8938-Abstract.html",
        "author": "Joshua Alspector; Robert B. Allen; Victor Hu; Srinagesh Satyanarayana",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1987_b9799a12,\n author = {Alspector, Joshua and Allen, Robert and Hu, Victor and Satyanarayana, Srinagesh},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Stochastic Learning Networks and their Electronic Implementation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/b9799a12d683d136cc817f94b73a8938-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/b9799a12d683d136cc817f94b73a8938-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/b9799a12d683d136cc817f94b73a8938-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2553441,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7616496193792616930&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Bell Communications Research; Bell Communications Research; Bell Communications Research; Bell Communications Research + University of California, Berkeley + Columbia University",
        "aff_domain": "bellcore.com; ; ; ",
        "email": "bellcore.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1+2",
        "aff_unique_norm": "Bell Communications Research;University of California, Berkeley;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.bell-labs.com/;https://www.berkeley.edu;https://www.columbia.edu",
        "aff_unique_abbr": "BCR;UC Berkeley;Columbia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0+0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "669f4f73cb",
        "title": "Strategies for Teaching Layered Networks Classification Tasks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/4a420924d20bc025ebb37849169e6ebd-Abstract.html",
        "author": "Ben S. Wittner; John S. Denker",
        "abstract": "There is  a widespread misconception  that the delta-rule is in some sense guaranteed to  work  on  networks  without hidden units.  As  previous authors have mentioned,  there is  no such guarantee for  classification tasks.  We  will begin by  presenting explicit counter(cid:173) examples illustrating two different interesting ways in which  the delta rule can fail.  We  go on  to provide conditions  which  do guarantee  that gradient  descent  will  successfully  train  networks  without  hidden  units  to  perform  two-category  classification  tasks.  We  discuss  the  generalization  of our  ideas  to  networks  with  hidden  units  and  to  multi(cid:173) category classification  tasks.",
        "bibtex": "@inproceedings{NIPS1987_4a420924,\n author = {Wittner, Ben and Denker, John},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Strategies for Teaching Layered Networks Classification Tasks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/4a420924d20bc025ebb37849169e6ebd-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/4a420924d20bc025ebb37849169e6ebd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/4a420924d20bc025ebb37849169e6ebd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1516332,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1182292911312356512&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "AT&T Bell Laboratories; AT&T Bell Laboratories",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "AT&T Bell Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.att.com/labs",
        "aff_unique_abbr": "AT&T Bell Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "47c8950c7c",
        "title": "Supervised Learning of Probability Distributions by Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/d8e3068a1b4ad91b2066d3e1780593ee-Abstract.html",
        "author": "Eric B. Baum; Frank Wilczek",
        "abstract": "We propose that the back propagation algorithm for super(cid:173)",
        "bibtex": "@inproceedings{NIPS1987_d8e3068a,\n author = {Baum, Eric and Wilczek, Frank},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Supervised Learning of Probability Distributions by Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/d8e3068a1b4ad91b2066d3e1780593ee-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/d8e3068a1b4ad91b2066d3e1780593ee-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/d8e3068a1b4ad91b2066d3e1780593ee-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1684280,
        "gs_citation": 374,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12233592416644330439&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Jet Propulsion Laboratory, Pasadena CA 91109; Department of Physics,Harvard University,Cambridge MA 02138 + Institute for Theoretical Physics, University of California, Santa Barbara CA 93106",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Jet Propulsion Laboratory;Harvard University;University of California, Santa Barbara",
        "aff_unique_dep": ";Department of Physics;Institute for Theoretical Physics",
        "aff_unique_url": "https://www.jpl.nasa.gov;https://www.harvard.edu;https://www.uci.edu",
        "aff_unique_abbr": "JPL;Harvard;UCSB",
        "aff_campus_unique_index": "0;1+2",
        "aff_campus_unique": "Pasadena;Cambridge;Santa Barbara",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ed253303b6",
        "title": "Synchronization in Neural Nets",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/03004620ea802b9118dd44d69f07af56-Abstract.html",
        "author": "Jacques J. Vidal; John Haggerty",
        "abstract": "The  paper  presents  an  artificial  neural  network  concept  (the  Synchronizable Oscillator Networks)  where the instants of individual  firings  in  the  form  of  point  processes  constitute  the  only  form  of  information  transmitted  between  joining  neurons.  This  type  of  communication contrasts with  that which  is  assumed  in most  other  models  which  typically  are  continuous  or  discrete  value-passing  networks.  Limiting the messages received  by each processing unit to  time  markers that signal  the firing  of other units  presents  significant  implemen tation advantages.",
        "bibtex": "@inproceedings{NIPS1987_03004620,\n author = {Vidal, Jacques and Haggerty, John},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Synchronization in Neural Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/03004620ea802b9118dd44d69f07af56-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/03004620ea802b9118dd44d69f07af56-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/03004620ea802b9118dd44d69f07af56-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1299052,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12101105183559564028&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of California Los Angeles, Los Angeles, Ca. 90024; Interactive Systems Los angeles 3030 W. 6th St. LA, Ca. 90020",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Los Angeles;Interactive Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;",
        "aff_unique_abbr": "UCLA;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8fa2a86b30",
        "title": "Teaching Artificial Neural Systems to Drive: Manual Training Techniques for Autonomous Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/b559156047e50cf316207249d0b5a6c5-Abstract.html",
        "author": "J. F. Shepanski; S. A. Macy",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1987_b5591560,\n author = {Shepanski, J. F. and Macy, S. A.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Teaching Artificial Neural Systems to Drive: Manual Training Techniques for Autonomous Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/b559156047e50cf316207249d0b5a6c5-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/b559156047e50cf316207249d0b5a6c5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/b559156047e50cf316207249d0b5a6c5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1644393,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2948626363814198180&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "635c46a73a",
        "title": "Temporal Patterns of Activity in Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/8081be7b20e6ddb1a26c2786da98e137-Abstract.html",
        "author": "Paolo Gaudiano",
        "abstract": "Patterns  of activity  over  real  neural  structures  are  known  to  exhibit  time(cid:173)",
        "bibtex": "@inproceedings{NIPS1987_8081be7b,\n author = {Gaudiano, Paolo},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Temporal Patterns of Activity in Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/8081be7b20e6ddb1a26c2786da98e137-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/8081be7b20e6ddb1a26c2786da98e137-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/8081be7b20e6ddb1a26c2786da98e137-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 970839,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15944660220028441457&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Aerospace Engineering Sciences, University of Colorado, Boulder CO 80309, USA",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Colorado Boulder",
        "aff_unique_dep": "Department of Aerospace Engineering Sciences",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU Boulder",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9d321201d8",
        "title": "The Capacity of the Kanerva Associative Memory is Exponential",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/099268c3121d49937a67a052c51f865d-Abstract.html",
        "author": "P. A. Chou",
        "abstract": "The  capacity  of  an  associative  memory  is  defined  as  the  maximum",
        "bibtex": "@inproceedings{NIPS1987_099268c3,\n author = {Chou, P. A.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {The Capacity of the Kanerva Associative Memory is Exponential},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/099268c3121d49937a67a052c51f865d-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/099268c3121d49937a67a052c51f865d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/099268c3121d49937a67a052c51f865d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1508033,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6049204615759082379&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University. Stanford. CA 94305",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "62ca1b1411",
        "title": "The Connectivity Analysis of Simple Association",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/7cc2b68825046ade9b4a2f3b1dbd675b-Abstract.html",
        "author": "Dan Hammerstrom",
        "abstract": "The efficient realization, using current silicon technology, of Very Large Connection  Networks (VLCN) with more than a billion connections requires that these networks exhibit  a high degree of communication locality. Real neural networks exhibit significant locality,  yet most connectionist/neural network models have little. In this paper, the connectivity  requirements of a simple associative network are analyzed using communication theory.  Several techniques based on communication theory are presented that improve the robust(cid:173) ness of the network in the face of sparse, local interconnect structures. Also discussed are  some potential problems when information is distributed too widely.",
        "bibtex": "@inproceedings{NIPS1987_7cc2b688,\n author = {Hammerstrom, Dan},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {The Connectivity Analysis of Simple Association},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/7cc2b68825046ade9b4a2f3b1dbd675b-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/7cc2b68825046ade9b4a2f3b1dbd675b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/7cc2b68825046ade9b4a2f3b1dbd675b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2093998,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18370582667515521591&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Oregon Graduate Center, Beaverton, OR 97006",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Oregon Graduate Center",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Beaverton",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "868baab729",
        "title": "The Hopfield Model with Multi-Level Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/f77e8ab11fcfa3f83e20d59b0a87f374-Abstract.html",
        "author": "Michael Fleisher",
        "abstract": "The  Hopfield  neural  network.  model  for  associative  memory  is  generalized.  The  generalization",
        "bibtex": "@inproceedings{NIPS1987_f77e8ab1,\n author = {Fleisher, Michael},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {The Hopfield Model with Multi-Level Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/f77e8ab11fcfa3f83e20d59b0a87f374-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/f77e8ab11fcfa3f83e20d59b0a87f374-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/f77e8ab11fcfa3f83e20d59b0a87f374-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1220711,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15963840316741934874&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical Engineering, Technion - Israel Institute of Technology",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "294c98b54c",
        "title": "The Performance of Convex Set Projection Based Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/c0d16b623be8b439d9c075eb5a97efd1-Abstract.html",
        "author": "Robert J. Marks II; Les E. Atlas; Seho Oh; James A. Ritcey",
        "abstract": "and",
        "bibtex": "@inproceedings{NIPS1987_c0d16b62,\n author = {Marks, Robert and Atlas, Les and Oh, Seho and Ritcey, James},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {The Performance of Convex Set Projection Based Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/c0d16b623be8b439d9c075eb5a97efd1-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/c0d16b623be8b439d9c075eb5a97efd1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/c0d16b623be8b439d9c075eb5a97efd1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2105727,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7112824337632115619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2114444e98",
        "title": "The Sigmoid Nonlinearity in Prepyriform Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/58b244eaff03de43ae7db44c4e787105-Abstract.html",
        "author": "Frank H. Eeckman",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1987_58b244ea,\n author = {Eeckman, Frank},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {The Sigmoid Nonlinearity in Prepyriform Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/58b244eaff03de43ae7db44c4e787105-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/58b244eaff03de43ae7db44c4e787105-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/58b244eaff03de43ae7db44c4e787105-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1554478,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5102050189744565444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Berkeley, CA 94720",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "41eec0ea5b",
        "title": "Time-Sequential Self-Organization of Hierarchical Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/6270a15843a2e06a95d3e3ad8b489e4b-Abstract.html",
        "author": "Ronald H. Silverman; Andrew S. Noetzel",
        "abstract": "Self-organization  of  multi-layered  networks  can  be  realized  time-sequential  organization  of  successive  neural  layers.  by  Lateral  inhibition  operating  in  the  surround  of  firing  cells  in  each  for  unsupervised  capture  of  excitation  patterns  presented  by  the  previous  layer.  By  presenting  patterns  of  organization,  higher  implicit  in  the  pattern  set.",
        "bibtex": "@inproceedings{NIPS1987_6270a158,\n author = {Silverman, Ronald and Noetzel, Andrew},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Time-Sequential Self-Organization of Hierarchical Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/6270a15843a2e06a95d3e3ad8b489e4b-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/6270a15843a2e06a95d3e3ad8b489e4b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/6270a15843a2e06a95d3e3ad8b489e4b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1169316,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11090881892072294948&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Cornell University Medical College, New York, NY 10021; polytechnic University, Brooklyn, NY 11201",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cornell University Medical College;Polytechnic University",
        "aff_unique_dep": "Medical College;",
        "aff_unique_url": "https://med.cornell.edu;",
        "aff_unique_abbr": "Weill Cornell Medicine;",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "New York;Brooklyn",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b8f6c8a944",
        "title": "Towards an Organizing Principle for a Layered Perceptual Network",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/d162c44a3b7215f415cfe881329ed408-Abstract.html",
        "author": "Ralph Linsker",
        "abstract": "An information-theoretic optimization principle is  proposed for  the development  of  each  processing  stage  of  a  multilayered  perceptual  network.  This  principle  of  \"maximum information preservation\"  states that the signal transformation that is to be  realized at each stage is one that maximizes the information that the output signal values  (from that stage) convey about the input signals values (to that stage), subject to certain  constraints and in  the presence of processing noise.  The quantity being maximized is  a  Shannon information rate.  I provide motivation for this principle and -- for some simple  model cases -- derive some of its consequences, discuss an algorithmic implementation,  and  show  how  the  principle  may  lead  to  biologically  relevant  neural  architectural  features  such  as  topographic  maps,  map  distortions,  orientation  selectivity,  and  extraction of spatial and temporal signal correlations.  A  possible  connection between  this  information-theoretic principle  and  a  principle  of minimum  entropy production in  nonequilibrium thermodynamics is suggested.",
        "bibtex": "@inproceedings{NIPS1987_d162c44a,\n author = {Linsker, Ralph},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Towards an Organizing Principle for a Layered Perceptual Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/d162c44a3b7215f415cfe881329ed408-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/d162c44a3b7215f415cfe881329ed408-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/d162c44a3b7215f415cfe881329ed408-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2599557,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1510073330145486021&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Thomas J. Watson Research Center",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM Watson",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Yorktown Heights",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "edfe6d6a2a",
        "title": "Using Neural Networks to Improve Cochlear Implant Speech Perception",
        "site": "https://papers.nips.cc/paper_files/paper/1987/hash/650e2245aa3513ed517f4cf1b3d58e06-Abstract.html",
        "author": "Manoel F. Tenorio",
        "abstract": "-",
        "bibtex": "@inproceedings{NIPS1987_650e2245,\n author = {Tenorio, Manoel F.},\n booktitle = {Neural Information Processing Systems},\n editor = {D. Anderson},\n pages = {},\n publisher = {American Institute of Physics},\n title = {Using Neural Networks to Improve Cochlear Implant Speech Perception},\n url = {https://proceedings.neurips.cc/paper_files/paper/1987/file/650e2245aa3513ed517f4cf1b3d58e06-Paper.pdf},\n volume = {0},\n year = {1987}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1987/file/650e2245aa3513ed517f4cf1b3d58e06-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1987/file/650e2245aa3513ed517f4cf1b3d58e06-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2048313,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fxKAbrapWN0J:scholar.google.com/&scioq=Using+Neural+Networks+to+Improve+Cochlear+Implant+Speech+Perception&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "School of Electrical Engineering, Purdue University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "School of Electrical Engineering",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "West Lafayette",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    }
]