[
    {
        "title": "(Almost) No Label No Cry",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4352",
        "id": "4352",
        "author_site": "Giorgio Patrini, Richard Nock, Tiberio Caetano, Paul Rivera",
        "author": "Giorgio Patrini; Richard Nock; Paul Rivera; Tiberio Caetano",
        "abstract": "In Learning with Label Proportions (LLP), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known. This setting has broad practical relevance, in particular for privacy preserving data processing. We first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then, we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting, introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains, whose size ranges up to 300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover, in many cases, our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains, half a dozen proportions can suffice, i.e. roughly 40K times less than the total number of labels.",
        "bibtex": "@inproceedings{NIPS2014_d3313de3,\n author = {Patrini, Giorgio and Nock, Richard and Rivera, Paul and Caetano, Tiberio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {(Almost) No Label No Cry},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d3313de3f431fd64513431c4326d237c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d3313de3f431fd64513431c4326d237c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d3313de3f431fd64513431c4326d237c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d3313de3f431fd64513431c4326d237c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d3313de3f431fd64513431c4326d237c-Reviews.html",
        "metareview": "",
        "pdf_size": 430756,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17029907727312745941&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Australian National University1 + NICTA2; Australian National University1 + NICTA2; Australian National University1 + NICTA2; Australian National University1 + University of New South Wales3 + Ambiata4",
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d3313de3f431fd64513431c4326d237c-Abstract.html",
        "aff_unique_index": "0+1;0+1;0+1;0+2+3",
        "aff_unique_norm": "Australian National University;NICTA;University of New South Wales;Ambiata",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au;https://www.unsw.edu.au;https://www.ambiata.com",
        "aff_unique_abbr": "ANU;NICTA;UNSW;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "A Bayesian model for identifying hierarchically organised states in neural population activity",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4354",
        "id": "4354",
        "author_site": "Patrick Putzky, Florian Franzen, Giacomo Bassetto, Jakob H Macke",
        "author": "Patrick Putzky; Florian Franzen; Giacomo Bassetto; Jakob H. Macke",
        "abstract": "Neural population activity in cortical circuits is not solely driven by external inputs, but is also modulated by endogenous states which vary on multiple time-scales. To understand information processing in cortical circuits, we need to understand the statistical structure of internal states and their interaction with sensory inputs. Here, we present a statistical model for extracting hierarchically organised neural population states from multi-channel recordings of neural spiking activity. Population states are modelled using a hidden Markov decision tree with state-dependent tuning parameters and a generalised linear observation model. We present a variational Bayesian inference algorithm for estimating the posterior distribution over parameters from neural population recordings. On simulated data, we show that we can identify the underlying sequence of population states and reconstruct the ground truth parameters. Using population recordings from visual cortex, we find that a model with two levels of population states outperforms both a one-state and a two-state generalised linear model. Finally, we find that modelling of state-dependence also improves the accuracy with which sensory stimuli can be decoded from the population response.",
        "bibtex": "@inproceedings{NIPS2014_626b31cb,\n author = {Putzky, Patrick and Franzen, Florian and Bassetto, Giacomo and Macke, Jakob H},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian model for identifying hierarchically organised states in neural population activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/626b31cbdef068dd17611d021e7c5d0d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/626b31cbdef068dd17611d021e7c5d0d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/626b31cbdef068dd17611d021e7c5d0d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/626b31cbdef068dd17611d021e7c5d0d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/626b31cbdef068dd17611d021e7c5d0d-Reviews.html",
        "metareview": "",
        "pdf_size": 1305562,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5553316523454534054&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Max Planck Institute for Biological Cybernetics, T\u00fcbingen+Graduate Training Centre of Neuroscience, University of T\u00fcbingen+Bernstein Center for Computational Neuroscience, T\u00fcbingen; Max Planck Institute for Biological Cybernetics, T\u00fcbingen+Graduate Training Centre of Neuroscience, University of T\u00fcbingen+Bernstein Center for Computational Neuroscience, T\u00fcbingen; Max Planck Institute for Biological Cybernetics, T\u00fcbingen+Bernstein Center for Computational Neuroscience, T\u00fcbingen; Max Planck Institute for Biological Cybernetics, T\u00fcbingen+Bernstein Center for Computational Neuroscience, T\u00fcbingen",
        "aff_domain": "gmail.com;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "gmail.com;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/626b31cbdef068dd17611d021e7c5d0d-Abstract.html",
        "aff_unique_index": "0+1+2;0+1+2;0+2;0+2",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;University of T\u00fcbingen;Bernstein Center for Computational Neuroscience",
        "aff_unique_dep": ";Graduate Training Centre of Neuroscience;Computational Neuroscience",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;https://www.uni-tuebingen.de;",
        "aff_unique_abbr": "MPIBC;;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "A Block-Coordinate Descent Approach for Large-scale Sparse Inverse Covariance Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4477",
        "id": "4477",
        "author_site": "Eran Treister, Javier S Turek",
        "author": "Eran Treister; Javier Turek",
        "abstract": "The sparse inverse covariance estimation problem arises in many statistical applications in machine learning and signal processing. In this problem, the inverse of a covariance matrix of a multivariate normal distribution is estimated, assuming that it is sparse. An $\\ell_1$ regularized log-determinant optimization problem is typically solved to approximate such matrices. Because of memory limitations, most existing algorithms are unable to handle large scale instances of this problem. In this paper we present a new block-coordinate descent approach for solving the problem for large-scale data sets. Our method treats the sought matrix block-by-block using quadratic approximations, and we show that this approach has advantages over existing methods in several aspects. Numerical experiments on both synthetic and real gene expression data demonstrate that our approach outperforms the existing state of the art methods, especially for large-scale problems.",
        "bibtex": "@inproceedings{NIPS2014_cd26d8e4,\n author = {Treister, Eran and Turek, Javier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Block-Coordinate Descent Approach for Large-scale Sparse Inverse Covariance Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cd26d8e4103326d1e10b40e69c6ece01-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cd26d8e4103326d1e10b40e69c6ece01-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/cd26d8e4103326d1e10b40e69c6ece01-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cd26d8e4103326d1e10b40e69c6ece01-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cd26d8e4103326d1e10b40e69c6ece01-Reviews.html",
        "metareview": "",
        "pdf_size": 301263,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11097053948000156605&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science, Technion, Israel+Earth and Ocean Sciences, UBC, Vancouver, BC, V6T 1Z2, Canada; Department of Computer Science, Technion, Israel Institute of Technology, Technion City, Haifa 32000, Israel",
        "aff_domain": "cs.technion.ac.il;cs.technion.ac.il",
        "email": "cs.technion.ac.il;cs.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cd26d8e4103326d1e10b40e69c6ece01-Abstract.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Technion - Israel Institute of Technology;University of British Columbia;Technion, Israel Institute of Technology",
        "aff_unique_dep": "Computer Science;Department of Earth and Ocean Sciences;Department of Computer Science",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.ubc.ca;https://www.technion.ac.il",
        "aff_unique_abbr": "Technion;UBC;Technion",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Vancouver;Haifa",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Israel;Canada"
    },
    {
        "title": "A Boosting Framework on Grounds of Online Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4478",
        "id": "4478",
        "author_site": "Tofigh Naghibi Mohamadpoor, Beat Pfister",
        "author": "Tofigh Naghibi; Beat Pfister",
        "abstract": "By exploiting the duality between boosting and online learning, we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. Using this framework, we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting, smooth-distribution boosting, agnostic learning and, as a by-product, some generalization to double-projection online learning algorithms.",
        "bibtex": "@inproceedings{NIPS2014_949726c8,\n author = {Naghibi, Tofigh and Pfister, Beat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Boosting Framework on Grounds of Online Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/949726c8c457f0001eba5a1c5b2bbfb1-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/949726c8c457f0001eba5a1c5b2bbfb1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/949726c8c457f0001eba5a1c5b2bbfb1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/949726c8c457f0001eba5a1c5b2bbfb1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/949726c8c457f0001eba5a1c5b2bbfb1-Reviews.html",
        "metareview": "",
        "pdf_size": 152400,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12873758202006458028&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Engineering and Networks Laboratory, ETH Zurich, Switzerland; Computer Engineering and Networks Laboratory, ETH Zurich, Switzerland",
        "aff_domain": "tik.ee.ethz.ch;tik.ee.ethz.ch",
        "email": "tik.ee.ethz.ch;tik.ee.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/949726c8c457f0001eba5a1c5b2bbfb1-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Computer Engineering and Networks Laboratory",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "A Complete Variational Tracker",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4479",
        "id": "4479",
        "author_site": "Ryan D Turner, Steven Bottone, Bhargav Avasarala",
        "author": "Ryan Turner; Steven Bottone; Bhargav Avasarala",
        "abstract": "We introduce a novel probabilistic tracking algorithm that incorporates combinatorial data association constraints and model-based track management using variational Bayes. We use a Bethe entropy approximation to incorporate data association constraints that are often ignored in previous probabilistic tracking algorithms. Noteworthy aspects of our method include a model-based mechanism to replace heuristic logic typically used to initiate and destroy tracks, and an assignment posterior with linear computation cost in window length as opposed to the exponential scaling of previous MAP-based approaches. We demonstrate the applicability of our method on radar tracking and computer vision problems.",
        "bibtex": "@inproceedings{NIPS2014_0a8cd36e,\n author = {Turner, Ryan and Bottone, Steven and Avasarala, Bhargav},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Complete Variational Tracker},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0a8cd36e8193ba3773f8bcb9ed416ebb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0a8cd36e8193ba3773f8bcb9ed416ebb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0a8cd36e8193ba3773f8bcb9ed416ebb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0a8cd36e8193ba3773f8bcb9ed416ebb-Reviews.html",
        "metareview": "",
        "pdf_size": 663844,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5675616425844751456&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Northrop Grumman Corp.; Northrop Grumman Corp.; Northrop Grumman Corp.",
        "aff_domain": "ngc.com;ngc.com;ngc.com",
        "email": "ngc.com;ngc.com;ngc.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0a8cd36e8193ba3773f8bcb9ed416ebb-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northrop Grumman Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northropgrumman.com",
        "aff_unique_abbr": "NGC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Differential Equation for Modeling Nesterov\u2019s Accelerated Gradient Method: Theory and Insights",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4356",
        "id": "4356",
        "author_site": "Weijie Su, Stephen Boyd, Emmanuel Candes",
        "author": "Weijie Su; Stephen Boyd; Emmanuel J. Cand\u00e8s",
        "abstract": "We derive a second-order ordinary differential equation (ODE), which is the limit of Nesterov\u2019s accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov\u2019s scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov\u2019s scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov\u2019s scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.",
        "bibtex": "@inproceedings{NIPS2014_98bd09b1,\n author = {Su, Weijie and Boyd, Stephen and Cand\\`{e}s, Emmanuel J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Differential Equation for Modeling Nesterov\u2019s Accelerated Gradient Method: Theory and Insights},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/98bd09b1b9ff1342aac39b3067afcdb6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/98bd09b1b9ff1342aac39b3067afcdb6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/98bd09b1b9ff1342aac39b3067afcdb6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/98bd09b1b9ff1342aac39b3067afcdb6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/98bd09b1b9ff1342aac39b3067afcdb6-Reviews.html",
        "metareview": "",
        "pdf_size": 246998,
        "gs_citation": 1441,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10714185272774410323&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Statistics, Stanford University; Department of Electrical Engineering, Stanford University; Department of Statistics, Stanford University + Department of Mathematics, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/98bd09b1b9ff1342aac39b3067afcdb6-Abstract.html",
        "aff_unique_index": "0;0;0+0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0+0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4480",
        "id": "4480",
        "author_site": "Haipeng Luo, Robert E Schapire",
        "author": "Haipeng Luo; Robert E. Schapire",
        "abstract": "We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.",
        "bibtex": "@inproceedings{NIPS2014_24402144,\n author = {Luo, Haipeng and Schapire, Robert E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Drifting-Games Analysis for Online Learning and Applications to Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/24402144990624b417229a96ad7fa7bc-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/24402144990624b417229a96ad7fa7bc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/24402144990624b417229a96ad7fa7bc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/24402144990624b417229a96ad7fa7bc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/24402144990624b417229a96ad7fa7bc-Reviews.html",
        "metareview": "",
        "pdf_size": 416055,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16862937468782488010&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Princeton University + Microsoft Research",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/24402144990624b417229a96ad7fa7bc-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Princeton University;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Microsoft Research",
        "aff_unique_url": "https://www.princeton.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Princeton;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Dual Algorithm for Olfactory Computation in the Locust Brain",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4481",
        "id": "4481",
        "author_site": "Sina Tootoonian, Mate Lengyel",
        "author": "Sina Tootoonian; M\u00e1t\u00e9 Lengyel",
        "abstract": "We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses independent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.",
        "bibtex": "@inproceedings{NIPS2014_e0f8bcb7,\n author = {Tootoonian, Sina and Lengyel, M\\'{a}t\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Dual Algorithm for Olfactory Computation in the Locust Brain},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e0f8bcb7acfc792c3cbf5739dfc9abb0-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e0f8bcb7acfc792c3cbf5739dfc9abb0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e0f8bcb7acfc792c3cbf5739dfc9abb0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e0f8bcb7acfc792c3cbf5739dfc9abb0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e0f8bcb7acfc792c3cbf5739dfc9abb0-Reviews.html",
        "metareview": "",
        "pdf_size": 1203666,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8143904157304357413&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Computational & Biological Learning Laboratory, Department of Engineering, University of Cambridge; Computational & Biological Learning Laboratory, Department of Engineering, University of Cambridge",
        "aff_domain": "eng.cam.ac.uk;eng.cam.ac.uk",
        "email": "eng.cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e0f8bcb7acfc792c3cbf5739dfc9abb0-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "A Filtering Approach to Stochastic Variational Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4482",
        "id": "4482",
        "author_site": "Neil Houlsby, David Blei",
        "author": "Neil M.T. Houlsby; David M. Blei",
        "abstract": "Stochastic variational inference (SVI) uses stochastic optimization to scale up Bayesian computation to massive data. We present an alternative perspective on SVI as approximate parallel coordinate ascent. SVI trades-off bias and variance to step close to the unknown true coordinate optimum given by batch variational Bayes (VB). We define a model to automate this process. The model infers the location of the next VB optimum from a sequence of noisy realizations. As a consequence of this construction, we update the variational parameters using Bayes rule, rather than a hand-crafted optimization schedule. When our model is a Kalman filter this procedure can recover the original SVI algorithm and SVI with adaptive steps. We may also encode additional assumptions in the model, such as heavy-tailed noise. By doing so, our algorithm outperforms the original SVI schedule and a state-of-the-art adaptive SVI algorithm in two diverse domains.",
        "bibtex": "@inproceedings{NIPS2014_c5f8b7ee,\n author = {Houlsby, Neil M.T. and Blei, David M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Filtering Approach to Stochastic Variational Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c5f8b7ee19bb50cd0ea779f05c3f1b3e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c5f8b7ee19bb50cd0ea779f05c3f1b3e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c5f8b7ee19bb50cd0ea779f05c3f1b3e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c5f8b7ee19bb50cd0ea779f05c3f1b3e-Reviews.html",
        "metareview": "",
        "pdf_size": 473277,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4263637356257203381&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Google Research, Zurich, Switzerland + University of Cambridge, visiting Princeton University; Department of Statistics, Colombia University + Department of Computer Science, Colombia University",
        "aff_domain": "google.com;colombia.edu",
        "email": "google.com;colombia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c5f8b7ee19bb50cd0ea779f05c3f1b3e-Abstract.html",
        "aff_unique_index": "0+1;2+2",
        "aff_unique_norm": "Google;University of Cambridge;Colombia University",
        "aff_unique_dep": "Google Research;;Department of Statistics",
        "aff_unique_url": "https://research.google;https://www.cam.ac.uk;https://www.columbia.edu",
        "aff_unique_abbr": "Google;Cambridge;Columbia",
        "aff_campus_unique_index": "0+1;",
        "aff_campus_unique": "Zurich;Cambridge;",
        "aff_country_unique_index": "0+1;2+3",
        "aff_country_unique": "Switzerland;United Kingdom;United States;Colombia"
    },
    {
        "title": "A Framework for Testing Identifiability of Bayesian Models of Perception",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4483",
        "id": "4483",
        "author_site": "Luigi Acerbi, Wei Ji Ma, Sethu Vijayakumar",
        "author": "Luigi Acerbi; Wei Ji Ma; Sethu Vijayakumar",
        "abstract": "Bayesian observer models are very effective in describing human performance in perceptual tasks, so much so that they are trusted to faithfully recover hidden mental representations of priors, likelihoods, or loss functions from the data. However, the intrinsic degeneracy of the Bayesian framework, as multiple combinations of elements can yield empirically indistinguishable results, prompts the question of model identifiability. We propose a novel framework for a systematic testing of the identifiability of a significant class of Bayesian observer models, with practical applications for improving experimental design. We examine the theoretical identifiability of the inferred internal representations in two case studies. First, we show which experimental designs work better to remove the underlying degeneracy in a time interval estimation task. Second, we find that the reconstructed representations in a speed perception task under a slow-speed prior are fairly robust.",
        "bibtex": "@inproceedings{NIPS2014_1a744d70,\n author = {Acerbi, Luigi and Ma, Wei Ji and Vijayakumar, Sethu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Framework for Testing Identifiability of Bayesian Models of Perception},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1a744d7059a715367fd9e10da6981385-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1a744d7059a715367fd9e10da6981385-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/1a744d7059a715367fd9e10da6981385-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1a744d7059a715367fd9e10da6981385-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1a744d7059a715367fd9e10da6981385-Reviews.html",
        "metareview": "",
        "pdf_size": 674325,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4010573737868903641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Informatics, University of Edinburgh, UK + Center for Neural Science & Department of Psychology, New York University, USA; Center for Neural Science & Department of Psychology, New York University, USA; School of Informatics, University of Edinburgh, UK",
        "aff_domain": "nyu.edu;nyu.edu;ed.ac.uk",
        "email": "nyu.edu;nyu.edu;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1a744d7059a715367fd9e10da6981385-Abstract.html",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "University of Edinburgh;New York University",
        "aff_unique_dep": "School of Informatics;Center for Neural Science & Department of Psychology",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.nyu.edu",
        "aff_unique_abbr": "Edinburgh;NYU",
        "aff_campus_unique_index": "0+1;1;0",
        "aff_campus_unique": "Edinburgh;New York",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "A Latent Source Model for Online Collaborative Filtering",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4358",
        "id": "4358",
        "author_site": "Guy Bresler, George H Chen, Devavrat Shah",
        "author": "Guy Bresler; George H. Chen; Devavrat Shah",
        "abstract": "Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the ``online'' setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of $n$ users either likes or dislikes each of $m$ items. We assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly $\\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).",
        "bibtex": "@inproceedings{NIPS2014_ec958168,\n author = {Bresler, Guy and Chen, George H. and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Latent Source Model for Online Collaborative Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ec95816856845b0d9705bb94bef0e23b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ec95816856845b0d9705bb94bef0e23b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ec95816856845b0d9705bb94bef0e23b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ec95816856845b0d9705bb94bef0e23b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ec95816856845b0d9705bb94bef0e23b-Reviews.html",
        "metareview": "",
        "pdf_size": 482455,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6835661406319930946&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ec95816856845b0d9705bb94bef0e23b-Abstract.html"
    },
    {
        "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4484",
        "id": "4484",
        "author_site": "Mateusz Malinowski, Mario Fritz",
        "author": "Mateusz Malinowski; Mario Fritz",
        "abstract": "We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.",
        "bibtex": "@inproceedings{NIPS2014_b63e91b9,\n author = {Malinowski, Mateusz and Fritz, Mario},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b63e91b999a207f4ee6f4c15962dd961-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b63e91b999a207f4ee6f4c15962dd961-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b63e91b999a207f4ee6f4c15962dd961-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b63e91b999a207f4ee6f4c15962dd961-Reviews.html",
        "metareview": "",
        "pdf_size": 2087853,
        "gs_citation": 937,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18157988987497683849&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Max Planck Institute for Informatics; Max Planck Institute for Informatics",
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b63e91b999a207f4ee6f4c15962dd961-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4485",
        "id": "4485",
        "author_site": "Jamie Kiros, Richard Zemel, Russ Salakhutdinov",
        "author": "Ryan Kiros; Richard S. Zemel; Ruslan Salakhutdinov",
        "abstract": "In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to a wide variety of concepts, such as document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.",
        "bibtex": "@inproceedings{NIPS2014_ef46340b,\n author = {Kiros, Ryan and Zemel, Richard S. and Salakhutdinov, Ruslan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Multiplicative Model for Learning Distributed Text-Based Attribute Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ef46340b8faf3b2ab30fa9bf08f496a9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ef46340b8faf3b2ab30fa9bf08f496a9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ef46340b8faf3b2ab30fa9bf08f496a9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ef46340b8faf3b2ab30fa9bf08f496a9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ef46340b8faf3b2ab30fa9bf08f496a9-Reviews.html",
        "metareview": "",
        "pdf_size": 398001,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1432525796957732896&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "University of Toronto; University of Toronto; University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ef46340b8faf3b2ab30fa9bf08f496a9-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "A Probabilistic Framework for Multimodal Retrieval using Integrative Indian Buffet Process",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4486",
        "id": "4486",
        "author_site": "Bahadir Ozdemir, Larry Davis",
        "author": "Bahadir Ozdemir; Larry S. Davis",
        "abstract": "We propose a multimodal retrieval procedure based on latent feature models. The procedure consists of a nonparametric Bayesian framework for learning underlying semantically meaningful abstract features in a multimodal dataset, a probabilistic retrieval model that allows cross-modal queries and an extension model for relevance feedback. Experiments on two multimodal datasets, PASCAL-Sentence and SUN-Attribute, demonstrate the effectiveness of the proposed retrieval procedure in comparison to the state-of-the-art algorithms for learning binary codes.",
        "bibtex": "@inproceedings{NIPS2014_c61caed9,\n author = {Ozdemir, Bahadir and Davis, Larry S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Probabilistic Framework for Multimodal Retrieval using Integrative Indian Buffet Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c61caed993565cc3c90d7f6d129928a3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c61caed993565cc3c90d7f6d129928a3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c61caed993565cc3c90d7f6d129928a3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c61caed993565cc3c90d7f6d129928a3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c61caed993565cc3c90d7f6d129928a3-Reviews.html",
        "metareview": "",
        "pdf_size": 2148934,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12528740133568144395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Maryland, College Park, MD 20742 USA; Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742 USA",
        "aff_domain": "cs.umd.edu;umiacs.umd.edu",
        "email": "cs.umd.edu;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c61caed993565cc3c90d7f6d129928a3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Representation Theory for Ranking Functions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4487",
        "id": "4487",
        "author_site": "Harsh H Pareek, Pradeep Ravikumar",
        "author": "Harsh Pareek; Pradeep Ravikumar",
        "abstract": "This paper presents a representation theory for permutation-valued functions, which in their general form can also be called listwise ranking functions. Pointwise ranking functions assign a score to each object independently, without taking into account the other objects under consideration; whereas listwise loss functions evaluate the set of scores assigned to all objects as a whole. In many supervised learning to rank tasks, it might be of interest to use listwise ranking functions instead; in particular, the Bayes Optimal ranking functions might themselves be listwise, especially if the loss function is listwise. A key caveat to using listwise ranking functions has been the lack of an appropriate representation theory for such functions. We show that a natural symmetricity assumption that we call exchangeability allows us to explicitly characterize the set of such exchangeable listwise ranking functions. Our analysis draws from the theories of tensor analysis, functional analysis and De Finetti theorems. We also present experiments using a novel reranking method motivated by our representation theory.",
        "bibtex": "@inproceedings{NIPS2014_b99e6907,\n author = {Pareek, Harsh and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Representation Theory for Ranking Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b99e69074b2fa1d8c8fe0d5b60e19397-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b99e69074b2fa1d8c8fe0d5b60e19397-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b99e69074b2fa1d8c8fe0d5b60e19397-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b99e69074b2fa1d8c8fe0d5b60e19397-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b99e69074b2fa1d8c8fe0d5b60e19397-Reviews.html",
        "metareview": "",
        "pdf_size": 249669,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16416290727477723443&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b99e69074b2fa1d8c8fe0d5b60e19397-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4458",
        "id": "4458",
        "author_site": "Miles Lopes",
        "author": "Miles E. Lopes",
        "abstract": "We study the residual bootstrap (RB) method in the context of high-dimensional linear regression. Specifically, we analyze the distributional approximation of linear contrasts $c^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, where $\\hat{\\beta}_{\\rho}$ is a ridge-regression estimator. When regression coefficients are estimated via least squares, classical results show that RB consistently approximates the laws of contrasts, provided that $p\\ll n$, where the design matrix is of size $n\\times p$. Up to now, relatively little work has considered how additional structure in the linear model may extend the validity of RB to the setting where $p/n\\asymp 1$. In this setting, we propose a version of RB that resamples residuals obtained from ridge regression. Our main structural assumption on the design matrix is that it is nearly low rank --- in the sense that its singular values decay according to a power-law profile. Under a few extra technical assumptions, we derive a simple criterion for ensuring that RB consistently approximates the law of a given contrast. We then specialize this result to study confidence intervals for mean response values $X_i^{\\top} \\beta$, where $X_i^{\\top}$ is the $i$th row of the design. More precisely, we show that conditionally on a Gaussian design with near low-rank structure, RB \\emph{simultaneously} approximates all of the laws $X_i^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, $i=1,\\dots,n$. This result is also notable as it imposes no sparsity assumptions on $\\beta$. Furthermore, since our consistency results are formulated in terms of the Mallows (Kantorovich) metric, the existence of a limiting distribution is not required.",
        "bibtex": "@inproceedings{NIPS2014_ba9257d4,\n author = {Lopes, Miles E.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ba9257d4777bf2e7cb2c09c15628dfd3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ba9257d4777bf2e7cb2c09c15628dfd3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ba9257d4777bf2e7cb2c09c15628dfd3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ba9257d4777bf2e7cb2c09c15628dfd3-Reviews.html",
        "metareview": "",
        "pdf_size": 262850,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2101629445785932339&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, University of California, Berkeley",
        "aff_domain": "stat.berkeley.edu",
        "email": "stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ba9257d4777bf2e7cb2c09c15628dfd3-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Safe Screening Rule for Sparse Logistic Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4488",
        "id": "4488",
        "author_site": "Jie Wang, Jiayu Zhou, Jun Liu, Peter Wonka, Jieping Ye",
        "author": "Jie Wang; Jiayu Zhou; Jun Liu; Peter Wonka; Jieping Ye",
        "abstract": "The l1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges. In this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify the zero components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Extensive experimental results demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression is improved by one magnitude in general.",
        "bibtex": "@inproceedings{NIPS2014_65b98077,\n author = {Wang, Jie and Zhou, Jiayu and Liu, Jun and Wonka, Peter and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Safe Screening Rule for Sparse Logistic Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/65b9807722a2a0b28aaebd00b94d6521-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/65b9807722a2a0b28aaebd00b94d6521-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/65b9807722a2a0b28aaebd00b94d6521-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/65b9807722a2a0b28aaebd00b94d6521-Reviews.html",
        "metareview": "",
        "pdf_size": 557373,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3086761218939274116&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff": "Arizona State University; Arizona State University; SAS Institute Inc.; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;sas.com;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;sas.com;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/65b9807722a2a0b28aaebd00b94d6521-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Arizona State University;SAS Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.asu.edu;https://www.sas.com",
        "aff_unique_abbr": "ASU;SAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A State-Space Model for Decoding Auditory Attentional Modulation from MEG in a Competing-Speaker Environment",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4489",
        "id": "4489",
        "author_site": "Sahar Akram, Jonathan Z Simon, Shihab A Shamma, Behtash Babadi",
        "author": "Sahar Akram; Jonathan Z. Simon; Shihab Shamma; Behtash Babadi",
        "abstract": "Humans are able to segregate auditory objects in a complex acoustic scene, through an interplay of bottom-up feature extraction and top-down selective attention in the brain. The detailed mechanism underlying this process is largely unknown and the ability to mimic this procedure is an important problem in artificial intelligence and computational neuroscience. We consider the problem of decoding the attentional state of a listener in a competing-speaker environment from magnetoencephalographic (MEG) recordings from the human brain. We develop a behaviorally inspired state-space model to account for the modulation of the MEG with respect to attentional state of the listener. We construct a decoder based on the maximum a posteriori (MAP) estimate of the state parameters via the Expectation-Maximization (EM) algorithm. The resulting decoder is able to track the attentional modulation of the listener with multi-second resolution using only the envelopes of the two speech streams as covariates. We present simulation studies as well as application to real MEG data from two human subjects. Our results reveal that the proposed decoder provides substantial gains in terms of temporal resolution, complexity, and decoding accuracy.",
        "bibtex": "@inproceedings{NIPS2014_b0d29a25,\n author = {Akram, Sahar and Simon, Jonathan Z. and Shamma, Shihab and Babadi, Behtash},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A State-Space Model for Decoding Auditory Attentional Modulation from MEG in a Competing-Speaker Environment},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b0d29a25b33412c3f3d000b2d819136e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b0d29a25b33412c3f3d000b2d819136e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b0d29a25b33412c3f3d000b2d819136e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b0d29a25b33412c3f3d000b2d819136e-Reviews.html",
        "metareview": "",
        "pdf_size": 3246020,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9099554175293048413&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical and Computer Engineering + Institute for Systems Research; Department of Electrical and Computer Engineering + Institute for Systems Research + Department of Biology; Department of Electrical and Computer Engineering + Institute for Systems Research; Department of Electrical and Computer Engineering + Institute for Systems Research",
        "aff_domain": "umd.edu;umd.edu;umd.edu;umd.edu",
        "email": "umd.edu;umd.edu;umd.edu;umd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b0d29a25b33412c3f3d000b2d819136e-Abstract.html",
        "aff_unique_index": "0+1;0+1+2;0+1;0+1",
        "aff_unique_norm": "Unknown Institution;University of Maryland, College Park;Institution not specified",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Institute for Systems Research;Department of Biology",
        "aff_unique_url": ";https://www.isr.umd.edu;",
        "aff_unique_abbr": ";ISR;",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";College Park",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "A Statistical Decision-Theoretic Framework for Social Choice",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4314",
        "id": "4314",
        "author_site": "Hossein Azari Soufiani, David Parkes, Lirong Xia",
        "author": "Hossein Azari Soufiani; David C. Parkes; Lirong Xia",
        "abstract": "In this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents. In our framework, we are given a statistical ranking model, a decision space, and a loss function defined on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize expected loss. This suggests a general framework for the design and analysis of new social choice mechanisms. We compare Bayesian estimators, which minimize Bayesian expected loss, for the Mallows model and the Condorcet model respectively, and the Kemeny rule. We consider various normative properties, in addition to computational complexity and asymptotic behavior. In particular, we show that the Bayesian estimator for the Condorcet model satisfies some desired properties such as anonymity, neutrality, and monotonicity, can be computed in polynomial time, and is asymptotically different from the other two rules when the data are generated from the Condorcet model for some ground truth parameter.",
        "bibtex": "@inproceedings{NIPS2014_405bfe28,\n author = {Azari Soufiani, Hossein and Parkes, David C and Xia, Lirong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Statistical Decision-Theoretic Framework for Social Choice},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/405bfe28e212b7e1d100f013aae9b2d1-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/405bfe28e212b7e1d100f013aae9b2d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/405bfe28e212b7e1d100f013aae9b2d1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/405bfe28e212b7e1d100f013aae9b2d1-Reviews.html",
        "metareview": "",
        "pdf_size": 481949,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2536776354242936242&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Google Research, New York, NY 10011, USA + Harvard University; Harvard University, Cambridge, MA 02138, USA; Rensselaer Polytechnic Institute, Troy, NY 12180, USA",
        "aff_domain": "google.com;eecs.harvard.edu;cs.rpi.edu",
        "email": "google.com;eecs.harvard.edu;cs.rpi.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/405bfe28e212b7e1d100f013aae9b2d1-Abstract.html",
        "aff_unique_index": "0+1;1;2",
        "aff_unique_norm": "Google;Harvard University;Rensselaer Polytechnic Institute",
        "aff_unique_dep": "Google Research;;",
        "aff_unique_url": "https://research.google;https://www.harvard.edu;https://www.rpi.edu",
        "aff_unique_abbr": "Google Research;Harvard;RPI",
        "aff_campus_unique_index": "0;2;3",
        "aff_campus_unique": "New York;;Cambridge;Troy",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Synaptical Story of Persistent Activity with Graded Lifetime in a Neural System",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4490",
        "id": "4490",
        "author_site": "Yuanyuan Mi, Luozheng Li, Dahui Wang, Si Wu",
        "author": "Yuanyuan Mi; Luozheng Li; Dahui Wang; Si Wu",
        "abstract": "Persistent activity refers to the phenomenon that cortical neurons keep firing even after the stimulus triggering the initial neuronal responses is moved. Persistent activity is widely believed to be the substrate for a neural system retaining a memory trace of the stimulus information. In a conventional view, persistent activity is regarded as an attractor of the network dynamics, but it faces a challenge of how to be closed properly. Here, in contrast to the view of attractor, we consider that the stimulus information is encoded in a marginally unstable state of the network which decays very slowly and exhibits persistent firing for a prolonged duration. We propose a simple yet effective mechanism to achieve this goal, which utilizes the property of short-term plasticity (STP) of neuronal synapses. STP has two forms, short-term depression (STD) and short-term facilitation (STF), which have opposite effects on retaining neuronal responses. We find that by properly combining STF and STD, a neural system can hold persistent activity of graded lifetime, and that persistent activity fades away naturally without relying on an external drive. The implications of these results on neural information representation are discussed.",
        "bibtex": "@inproceedings{NIPS2014_c14a5587,\n author = {Mi, Yuanyuan and Li, Luozheng and Wang, Dahui and Wu, Si},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Synaptical Story of Persistent Activity with Graded Lifetime in a Neural System},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c14a5587798214dc951aae42fa3b6347-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c14a5587798214dc951aae42fa3b6347-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c14a5587798214dc951aae42fa3b6347-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c14a5587798214dc951aae42fa3b6347-Reviews.html",
        "metareview": "",
        "pdf_size": 549287,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5878463939692479557&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "State Key Laboratory of Cognitive Neuroscience & Learning, Beijing Normal University; State Key Laboratory of Cognitive Neuroscience & Learning, Beijing Normal University; State Key Laboratory of Cognitive Neuroscience & Learning, School of System Science, Beijing Normal University; State Key Laboratory of Cognitive Neuroscience & Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University",
        "aff_domain": "163.com;mail.bnu.edu.cn;bnu.edu.cn;bnu.edu.cn",
        "email": "163.com;mail.bnu.edu.cn;bnu.edu.cn;bnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c14a5587798214dc951aae42fa3b6347-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing Normal University",
        "aff_unique_dep": "State Key Laboratory of Cognitive Neuroscience & Learning",
        "aff_unique_url": "http://www.bnu.edu.cn",
        "aff_unique_abbr": "BNU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Unified Semantic Embedding: Relating Taxonomies and Attributes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4491",
        "id": "4491",
        "author_site": "Sung Ju Hwang, Leonid Sigal",
        "author": "Sung Ju Hwang; Leonid Sigal",
        "abstract": "We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be generated as a sparse combination of a supercategory + attributes, with an additional exclusive regularization to learn discriminative composition. The proposed reconstructive regularization guides the discriminative learning process to learn a better generalizing model, as well as generates compact semantic description of each category, which enables humans to analyze what has been learned.",
        "bibtex": "@inproceedings{NIPS2014_5c6614ea,\n author = {Hwang, Sung Ju and Sigal, Leonid},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Unified Semantic Embedding: Relating Taxonomies and Attributes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5c6614ea3b58bfdc092981678c2c2a88-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5c6614ea3b58bfdc092981678c2c2a88-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5c6614ea3b58bfdc092981678c2c2a88-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5c6614ea3b58bfdc092981678c2c2a88-Reviews.html",
        "metareview": "",
        "pdf_size": 1118772,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12170707102037986937&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Disney Research Pittsburgh, PA + Ulsan National Institute of Science and Technology in Ulsan, South Korea; Disney Research Pittsburgh, PA",
        "aff_domain": "disneyresearch.com;disneyresearch.com",
        "email": "disneyresearch.com;disneyresearch.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5c6614ea3b58bfdc092981678c2c2a88-Abstract.html",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Disney Research;Ulsan National Institute of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.disneyresearch.com;https://www.unist.ac.kr",
        "aff_unique_abbr": "Disney Research;UNIST",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Pittsburgh;Ulsan",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "title": "A Wild Bootstrap for Degenerate Kernel Tests",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4316",
        "id": "4316",
        "author_site": "Kacper P Chwialkowski, Dino Sejdinovic, Arthur Gretton",
        "author": "Kacper Chwialkowski; Dino Sejdinovic; Arthur Gretton",
        "abstract": "A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. The code is available at https://github.com/kacperChwialkowski/wildBootstrap.",
        "bibtex": "@inproceedings{NIPS2014_4e382cb4,\n author = {Chwialkowski, Kacper and Sejdinovic, Dino and Gretton, Arthur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Wild Bootstrap for Degenerate Kernel Tests},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4e382cb49370f64415df2672b19fb1f2-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4e382cb49370f64415df2672b19fb1f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4e382cb49370f64415df2672b19fb1f2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4e382cb49370f64415df2672b19fb1f2-Reviews.html",
        "metareview": "",
        "pdf_size": 367514,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15611401321673567766&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University College London; Gatsby Computational Neuroscience Unit, UCL; Gatsby Computational Neuroscience Unit, UCL",
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "github": "https://github.com/kacperChwialkowski/wildBootstrap",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4e382cb49370f64415df2672b19fb1f2-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "A framework for studying synaptic plasticity with neural spike train data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4492",
        "id": "4492",
        "author_site": "Scott Linderman, Christopher H Stock, Ryan Adams",
        "author": "Scott W. Linderman; Christopher H. Stock; Ryan P. Adams",
        "abstract": "Learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. The computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. Until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. However, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. Here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. We treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM). In addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules. Using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a substantial role. On synthetic data generated from the biophysical simulator NEURON, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.",
        "bibtex": "@inproceedings{NIPS2014_b909dedb,\n author = {Linderman, Scott W. and Stock, Christopher H. and Adams, Ryan P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A framework for studying synaptic plasticity with neural spike train data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b909dedbbccc4dd2260fdf854d7879eb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b909dedbbccc4dd2260fdf854d7879eb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b909dedbbccc4dd2260fdf854d7879eb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b909dedbbccc4dd2260fdf854d7879eb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b909dedbbccc4dd2260fdf854d7879eb-Reviews.html",
        "metareview": "",
        "pdf_size": 4819172,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15785628727047634510&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Harvard University; Harvard College; Harvard University",
        "aff_domain": "seas.harvard.edu;post.harvard.edu;seas.harvard.edu",
        "email": "seas.harvard.edu;post.harvard.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b909dedbbccc4dd2260fdf854d7879eb-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A provable SVD-based algorithm for learning topics in dominant admixture corpus",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4493",
        "id": "4493",
        "author_site": "Trapit Bansal, Chiranjib Bhattacharyya, Ravindran Kannan",
        "author": "Trapit Bansal; C. Bhattacharyya; Ravindran Kannan",
        "abstract": "Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents are drawn from admixtures of distributions over words, known as topics. The inference problem of recovering topics from such a collection of documents drawn from admixtures, is NP-hard. Making a strong assumption called separability, [4] gave the first provable algorithm for inference. For the widely used LDA model, [6] gave a provable algorithm using clever tensor-methods. But [4, 6] do not learn topic vectors with bounded $l_1$ error (a natural measure for probability vectors). Our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural, simple components such as SVD, which provably solves the inference problem for the model with bounded $l_1$ error. A topic in LDA and other models is essentially characterized by a group of co-occurring words. Motivated by this, we introduce topic specific Catchwords, a group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually. A major contribution of the paper is to show that under this more realistic assumption, which is empirically verified on real corpora, a singular value decomposition (SVD) based algorithm with a crucial pre-processing step of thresholding, can provably recover the topics from a collection of documents drawn from Dominant admixtures. Dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than the others. Apart from the simplicity of the algorithm, the sample complexity has near optimal dependence on $w_0$, the lowest probability that a topic is dominant, and is better than [4]. Empirical evidence shows that on several real world corpora, both Catchwords and Dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [5].",
        "bibtex": "@inproceedings{NIPS2014_456f7c0f,\n author = {Bansal, Trapit and Bhattacharyya, C. and Kannan, Ravindran},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A provable SVD-based algorithm for learning topics in dominant admixture corpus},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/456f7c0f2aa088d6ddcba4af487b63ca-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/456f7c0f2aa088d6ddcba4af487b63ca-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/456f7c0f2aa088d6ddcba4af487b63ca-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/456f7c0f2aa088d6ddcba4af487b63ca-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/456f7c0f2aa088d6ddcba4af487b63ca-Reviews.html",
        "metareview": "",
        "pdf_size": 312032,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3762430673894418336&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science; Department of Computer Science and Automation, Indian Institute of Science; Microsoft Research",
        "aff_domain": "gmail.com;csa.iisc.ernet.in;microsoft.com",
        "email": "gmail.com;csa.iisc.ernet.in;microsoft.com",
        "github": "",
        "project": "http://mllab.csa.iisc.ernet.in/tsvd",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/456f7c0f2aa088d6ddcba4af487b63ca-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Indian Institute of Science;Microsoft",
        "aff_unique_dep": "Department of Computer Science and Automation;Microsoft Research",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "IISc;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "A statistical model for tensor PCA",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4494",
        "id": "4494",
        "author_site": "Emile Richard, Andrea Montanari",
        "author": "Andrea Montanari; Emile Richard",
        "abstract": "We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio beta becomes larger than C\\sqrt{k log k} (and in particular beta can remain bounded has the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. For moderate dimensions, we propose an hybrid approach that uses unfolding together with power iteration, and show that it outperforms significantly baseline methods. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allow the iterative algorithms to converge to a good estimate.",
        "bibtex": "@inproceedings{NIPS2014_cdb011b4,\n author = {Montanari, Andrea and Richard, Emile},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A statistical model for tensor PCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cdb011b40de6e23c09618d0651fb6555-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cdb011b40de6e23c09618d0651fb6555-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/cdb011b40de6e23c09618d0651fb6555-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cdb011b40de6e23c09618d0651fb6555-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cdb011b40de6e23c09618d0651fb6555-Reviews.html",
        "metareview": "",
        "pdf_size": 347316,
        "gs_citation": 336,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5271970676931943463&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Statistics & Electrical Engineering, Stanford University; Electrical Engineering, Stanford University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cdb011b40de6e23c09618d0651fb6555-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Statistics and Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A* Sampling",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4318",
        "id": "4318",
        "author_site": "Chris Maddison, Danny Tarlow, Tom Minka",
        "author": "Chris J. Maddison; Daniel Tarlow; Tom Minka",
        "abstract": "The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A* sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A* search. We analyze the correctness and convergence time of A* sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.",
        "bibtex": "@inproceedings{NIPS2014_937debc7,\n author = {Maddison, Chris J. and Tarlow, Daniel and Minka, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A\\ast  Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/937debc749f041eb5700df7211ac795c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/937debc749f041eb5700df7211ac795c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/937debc749f041eb5700df7211ac795c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/937debc749f041eb5700df7211ac795c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/937debc749f041eb5700df7211ac795c-Reviews.html",
        "metareview": "",
        "pdf_size": 1914271,
        "gs_citation": 452,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1757064827743045&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Dept. of Computer Science, University of Toronto; Microsoft Research; Microsoft Research",
        "aff_domain": "cs.toronto.edu;microsoft.com;microsoft.com",
        "email": "cs.toronto.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/937debc749f041eb5700df7211ac795c-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Toronto;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Microsoft Research",
        "aff_unique_url": "https://www.utoronto.ca;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "U of T;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Toronto;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Accelerated Mini-batch Randomized Block Coordinate Descent Method",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4495",
        "id": "4495",
        "author_site": "Tuo Zhao, Mo Yu, Yiming Wang, Raman Arora, Han Liu",
        "author": "Tuo Zhao; Mo Yu; Yiming Wang; Raman Arora; Han Liu",
        "abstract": "We consider regularized empirical risk minimization problems. In particular, we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function. When the regularization function is block separable, we can solve the minimization problems in a randomized block coordinate descent (RBCD) manner. Existing RBCD methods usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinates in each iteration. Thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained. However, such a ``batch setting may be computationally expensive in practice. In this paper, we propose a mini-batch randomized block coordinate descent (MRBCD) method, which estimates the partial gradient of the selected block based on a mini-batch of randomly sampled data in each iteration. We further accelerate the MRBCD method by exploiting the semi-stochastic optimization scheme, which effectively reduces the variance of the partial gradient estimators. Theoretically, we show that for strongly convex functions, the MRBCD method attains lower overall iteration complexity than existing RBCD methods. As an application, we further trim the MRBCD method to solve the regularized sparse learning problems. Our numerical experiments shows that the MRBCD method naturally exploits the sparsity structure and achieves better computational performance than existing methods.\"",
        "bibtex": "@inproceedings{NIPS2014_a6326189,\n author = {Zhao, Tuo and Yu, Mo and Wang, Yiming and Arora, Raman and Liu, Han},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accelerated Mini-batch Randomized Block Coordinate Descent Method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a63261899803634b2faaade99bb1e8f6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a63261899803634b2faaade99bb1e8f6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a63261899803634b2faaade99bb1e8f6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a63261899803634b2faaade99bb1e8f6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a63261899803634b2faaade99bb1e8f6-Reviews.html",
        "metareview": "",
        "pdf_size": 1906244,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9223689177026706178&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Johns Hopkins University; Harbin Institute of Technology; Johns Hopkins University; Johns Hopkins University; Princeton University",
        "aff_domain": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;princeton.edu",
        "email": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a63261899803634b2faaade99bb1e8f6-Abstract.html",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "Johns Hopkins University;Harbin Institute of Technology;Princeton University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.jhu.edu;http://www.hit.edu.cn/;https://www.princeton.edu",
        "aff_unique_abbr": "JHU;HIT;Princeton",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Harbin",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Active Learning and Best-Response Dynamics",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4496",
        "id": "4496",
        "author_site": "Maria-Florina F Balcan, Christopher Berlind, Avrim Blum, Emma Cohen, Kaushik Patnaik, Le Song",
        "author": "Maria-Florina Balcan; Christopher Berlind; Avrim Blum; Emma Cohen; Kaushik Patnaik; Le Song",
        "abstract": "We consider a setting in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-de\ufb01ned game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. We prove positive (and negative) results on the denoising power of several natural dynamics, and also show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.",
        "bibtex": "@inproceedings{NIPS2014_ecd08843,\n author = {Balcan, Maria-Florina and Berlind, Christopher and Blum, Avrim and Cohen, Emma and Patnaik, Kaushik and Song, Le},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Learning and Best-Response Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ecd08843b08566c23903b80c40200c89-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ecd08843b08566c23903b80c40200c89-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ecd08843b08566c23903b80c40200c89-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ecd08843b08566c23903b80c40200c89-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ecd08843b08566c23903b80c40200c89-Reviews.html",
        "metareview": "",
        "pdf_size": 417565,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10256087617531491651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Carnegie Mellon; Georgia Tech; Carnegie Mellon; Georgia Tech; Georgia Tech; Georgia Tech",
        "aff_domain": "cs.cmu.edu;gatech.edu;cs.cmu.edu;gatech.edu;gatech.edu;cc.gatech.edu",
        "email": "cs.cmu.edu;gatech.edu;cs.cmu.edu;gatech.edu;gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ecd08843b08566c23903b80c40200c89-Abstract.html",
        "aff_unique_index": "0;1;0;1;1;1",
        "aff_unique_norm": "Carnegie Mellon University;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.gatech.edu",
        "aff_unique_abbr": "CMU;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Active Regression by Stratification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4497",
        "id": "4497",
        "author_site": "Sivan Sabato, Remi Munos",
        "author": "Sivan Sabato; Remi Munos",
        "abstract": "We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of O(1/epsilon) cannot in general be improved upon. Nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches a the optimal risk using piecewise constant approximations.",
        "bibtex": "@inproceedings{NIPS2014_014b0027,\n author = {Sabato, Sivan and Munos, Remi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Regression by Stratification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/014b0027decf8737e4c1242be3054307-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/014b0027decf8737e4c1242be3054307-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/014b0027decf8737e4c1242be3054307-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/014b0027decf8737e4c1242be3054307-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/014b0027decf8737e4c1242be3054307-Reviews.html",
        "metareview": "",
        "pdf_size": 343278,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15668596669425851664&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Ben Gurion University, Beer Sheva, Israel; INRIA, Lille, France + Google DeepMind",
        "aff_domain": "cs.bgu.ac.il;inria.fr",
        "email": "cs.bgu.ac.il;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/014b0027decf8737e4c1242be3054307-Abstract.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Ben Gurion University;INRIA;Google",
        "aff_unique_dep": "Department of Computer Science;;Google DeepMind",
        "aff_unique_url": "https://www.bgu.ac.il;https://www.inria.fr;https://deepmind.com",
        "aff_unique_abbr": "BGU;INRIA;DeepMind",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Beer Sheva;Lille;",
        "aff_country_unique_index": "0;1+2",
        "aff_country_unique": "Israel;France;United Kingdom"
    },
    {
        "title": "Advances in Learning Bayesian Networks of Bounded Treewidth",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4360",
        "id": "4360",
        "author_site": "Siqi Nie, Denis Maua, Cassio P de Campos, Qiang Ji",
        "author": "Siqi Nie; Denis D. Mau\u00e1; Cassio P. de Campos; Qiang Ji",
        "abstract": "This work presents novel algorithms for learning Bayesian networks of bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in sampling k-trees (maximal graphs of treewidth k), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that k-tree. The approaches are empirically compared to each other and to state-of-the-art methods on a collection of public data sets with up to 100 variables.",
        "bibtex": "@inproceedings{NIPS2014_aa328e0f,\n author = {Nie, Siqi and Mau\\'{a}, Denis D. and de Campos, Cassio P. and Ji, Qiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Advances in Learning Bayesian Networks of Bounded Treewidth},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/aa328e0f345d2d4909542b1d9fd4fbfc-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/aa328e0f345d2d4909542b1d9fd4fbfc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/aa328e0f345d2d4909542b1d9fd4fbfc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/aa328e0f345d2d4909542b1d9fd4fbfc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/aa328e0f345d2d4909542b1d9fd4fbfc-Reviews.html",
        "metareview": "",
        "pdf_size": 240510,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2762950505922471432&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Rensselaer Polytechnic Institute, Troy, NY, USA; University of S \u02dcao Paulo, S\u02dcao Paulo, Brazil; Queen\u2019s University Belfast, Belfast, UK; Rensselaer Polytechnic Institute, Troy, NY, USA",
        "aff_domain": "rpi.edu;usp.br;qub.ac.uk;ecse.rpi.edu",
        "email": "rpi.edu;usp.br;qub.ac.uk;ecse.rpi.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/aa328e0f345d2d4909542b1d9fd4fbfc-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;University of Sao Paulo;Queen's University Belfast",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.rpi.edu;https://www.usp.br;https://www.qub.ac.uk",
        "aff_unique_abbr": "RPI;USP;QUB",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Troy;Sao Paulo;Belfast",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "United States;Brazil;United Kingdom"
    },
    {
        "title": "Algorithm selection by rational metareasoning as a model of human strategy selection",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4498",
        "id": "4498",
        "author_site": "Falk Lieder, Dillon Plunkett, Jessica B Hamrick, Stuart J Russell, Nicholas Hay, Tom Griffiths",
        "author": "Falk Lieder; Dillon Plunkett; Jessica B. Hamrick; Stuart J. Russell; Nicholas J. Hay; Thomas L. Griffiths",
        "abstract": "Selecting the right algorithm is an important problem in computer science, because the algorithm often has to exploit the structure of the input to be efficient. The human mind faces the same challenge. Therefore, solutions to the algorithm selection problem can inspire models of human strategy selection and vice versa. Here, we view the algorithm selection problem as a special case of metareasoning and derive a solution that outperforms existing methods in sorting algorithm selection. We apply our theory to model how people choose between cognitive strategies and test its prediction in a behavioral experiment. We find that people quickly learn to adaptively choose between cognitive strategies. People's choices in our experiment are consistent with our model but inconsistent with previous theories of human strategy selection. Rational metareasoning appears to be a promising framework for reverse-engineering how people choose among cognitive strategies and translating the results into better solutions to the algorithm selection problem.",
        "bibtex": "@inproceedings{NIPS2014_87b5e7e5,\n author = {Lieder, Falk and Plunkett, Dillon and Hamrick, Jessica B. and Russell, Stuart J. and Hay, Nicholas J. and Griffiths, Thomas L.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Algorithm selection by rational metareasoning as a model of human strategy selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/87b5e7e570f757a0b99f0a370ce2438c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/87b5e7e570f757a0b99f0a370ce2438c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/87b5e7e570f757a0b99f0a370ce2438c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/87b5e7e570f757a0b99f0a370ce2438c-Reviews.html",
        "metareview": "",
        "pdf_size": 599665,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7905017643268725423&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Helen Wills Neuroscience Institute, UC Berkeley; Department of Psychology, UC Berkeley; Department of Psychology, UC Berkeley; EECS Department, UC Berkeley; EECS Department, UC Berkeley; Department of Psychology, UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;cs.berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;cs.berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/87b5e7e570f757a0b99f0a370ce2438c-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Helen Wills Neuroscience Institute",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Algorithms for CVaR Optimization in MDPs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4499",
        "id": "4499",
        "author_site": "Yinlam Chow, Mohammad Ghavamzadeh",
        "author": "Yinlam Chow; Mohammad Ghavamzadeh",
        "abstract": "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.",
        "bibtex": "@inproceedings{NIPS2014_35f1050a,\n author = {Chow, Yinlam and Ghavamzadeh, Mohammad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Algorithms for CVaR Optimization in MDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/35f1050a4381d2d216bf56ad46b0277d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/35f1050a4381d2d216bf56ad46b0277d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/35f1050a4381d2d216bf56ad46b0277d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/35f1050a4381d2d216bf56ad46b0277d-Reviews.html",
        "metareview": "",
        "pdf_size": 443520,
        "gs_citation": 423,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15978718197830907760&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Institute of Computational & Mathematical Engineering, Stanford University; Adobe Research + INRIA Lille - Team SequeL",
        "aff_domain": "stanford.edu;adobe.com",
        "email": "stanford.edu;adobe.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/35f1050a4381d2d216bf56ad46b0277d-Abstract.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Stanford University;Adobe;INRIA Lille",
        "aff_unique_dep": "Institute of Computational & Mathematical Engineering;Adobe Research;Team SequeL",
        "aff_unique_url": "https://www.stanford.edu;https://research.adobe.com;https://www.inria.fr/en teams/lille",
        "aff_unique_abbr": "Stanford;Adobe;INRIA",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Stanford;;Lille",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Altitude Training: Strong Bounds for Single-Layer Dropout",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4500",
        "id": "4500",
        "author_site": "Stefan Wager, William S Fithian, Sida Wang, Percy Liang",
        "author": "Stefan Wager; William Fithian; Sida Wang; Percy Liang",
        "abstract": "Dropout training, originally designed for deep neural networks, has been successful on high-dimensional single-layer natural language tasks. This paper proposes a theoretical explanation for this phenomenon: we show that, under a generative Poisson topic model with long documents, dropout training improves the exponent in the generalization bound for empirical risk minimization. Dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted test set. We also show that, under similar conditions, dropout preserves the Bayes decision boundary and should therefore induce minimal bias in high dimensions.",
        "bibtex": "@inproceedings{NIPS2014_a06f20b3,\n author = {Wager, Stefan and Fithian, William and Wang, Sida and Liang, Percy S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Altitude Training: Strong Bounds for Single-Layer Dropout},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a06f20b349c6cf09a6b171c71b88bbfc-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a06f20b349c6cf09a6b171c71b88bbfc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a06f20b349c6cf09a6b171c71b88bbfc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a06f20b349c6cf09a6b171c71b88bbfc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a06f20b349c6cf09a6b171c71b88bbfc-Reviews.html",
        "metareview": "",
        "pdf_size": 560177,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10188722837751369921&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Departments of Statistics; Departments of Statistics; Computer Science; Departments of Statistics + Computer Science",
        "aff_domain": "stanford.edu;stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "stanford.edu;stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a06f20b349c6cf09a6b171c71b88bbfc-Abstract.html",
        "aff_unique_index": "0;0;1;0+1",
        "aff_unique_norm": "University Affiliation Not Specified;Computer Science",
        "aff_unique_dep": "Departments of Statistics;Computer Science Department",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "An Accelerated Proximal Coordinate Gradient Method",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4501",
        "id": "4501",
        "author_site": "Qihang Lin, Zhaosong Lu, Lin Xiao",
        "author": "Qihang Lin; Zhaosong Lu; Lin Xiao",
        "abstract": "We develop an accelerated randomized proximal coordinate gradient (APCG) method, for solving a broad class of composite convex optimization problems. In particular, our method achieves faster linear convergence rates for minimizing strongly convex functions than existing randomized proximal coordinate gradient methods. We show how to apply the APCG method to solve the dual of the regularized empirical risk minimization (ERM) problem, and devise efficient implementations that can avoid full-dimensional vector operations. For ill-conditioned ERM problems, our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent (SDCA) method.",
        "bibtex": "@inproceedings{NIPS2014_7603c1fc,\n author = {Lin, Qihang and Lu, Zhaosong and Xiao, Lin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Accelerated Proximal Coordinate Gradient Method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7603c1fcfb1ac7b7a311456f72f99584-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7603c1fcfb1ac7b7a311456f72f99584-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7603c1fcfb1ac7b7a311456f72f99584-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7603c1fcfb1ac7b7a311456f72f99584-Reviews.html",
        "metareview": "",
        "pdf_size": 202977,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1179671664632647461&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Iowa; Simon Fraser University; Microsoft Research",
        "aff_domain": "uiowa.edu;sfu.ca;microsoft.com",
        "email": "uiowa.edu;sfu.ca;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7603c1fcfb1ac7b7a311456f72f99584-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Iowa;Simon Fraser University;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www.uiowa.edu;https://www.sfu.ca;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UIowa;SFU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "An Autoencoder Approach to Learning Bilingual Word Representations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4502",
        "id": "4502",
        "author_site": "Sarath Chandar, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, Amrita Saha",
        "author": "Sarath Chandar A P; Stanislas Lauly; Hugo Larochelle; Mitesh M Khapra; Balaraman Ravindran; Vikas Raykar; Amrita Saha",
        "abstract": "Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. We empirically investigate the success of our approach on the problem of cross-language text classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). In experiments on 3 language pairs, we show that our approach achieves state-of-the-art performance, outperforming a method exploiting word alignments and a strong machine translation baseline.",
        "bibtex": "@inproceedings{NIPS2014_2e2f5540,\n author = {P, Sarath Chandar A and Lauly, Stanislas and Larochelle, Hugo and Khapra, Mitesh M and Ravindran, Balaraman and Raykar, Vikas and Saha, Amrita},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Autoencoder Approach to Learning Bilingual Word Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2e2f5540941a46e2f642b33f3276928d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2e2f5540941a46e2f642b33f3276928d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/2e2f5540941a46e2f642b33f3276928d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2e2f5540941a46e2f642b33f3276928d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2e2f5540941a46e2f642b33f3276928d-Reviews.html",
        "metareview": "",
        "pdf_size": 435929,
        "gs_citation": 401,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14822498394813295410&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Indian Institute of Technology Madras; Universit\u00e9 de Sherbrooke; Universit\u00e9 de Sherbrooke; IBM Research India; Indian Institute of Technology Madras; IBM Research India; IBM Research India",
        "aff_domain": "gmail.com;usherbrooke.ca;usherbrooke.ca;in.ibm.com;cse.iitm.ac.in;in.ibm.com;in.ibm.com",
        "email": "gmail.com;usherbrooke.ca;usherbrooke.ca;in.ibm.com;cse.iitm.ac.in;in.ibm.com;in.ibm.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2e2f5540941a46e2f642b33f3276928d-Abstract.html",
        "aff_unique_index": "0;1;1;2;0;2;2",
        "aff_unique_norm": "Indian Institute of Technology Madras;Universit\u00e9 de Sherbrooke;IBM",
        "aff_unique_dep": ";;Research",
        "aff_unique_url": "https://www.iitm.ac.in;https://www.usherbrooke.ca;https://www.ibm.com/research/in",
        "aff_unique_abbr": "IIT Madras;UdeS;IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madras;",
        "aff_country_unique_index": "0;1;1;0;0;0;0",
        "aff_country_unique": "India;Canada"
    },
    {
        "title": "An Integer Polynomial Programming Based Framework for Lifted MAP Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4503",
        "id": "4503",
        "author_site": "Somdeb Sarkhel, Deepak Venugopal, Parag Singla, Vibhav Gogate",
        "author": "Somdeb Sarkhel; Deepak Venugopal; Parag Singla; Vibhav Gogate",
        "abstract": "In this paper, we present a new approach for lifted MAP inference in Markov logic networks (MLNs). The key idea in our approach is to compactly encode the MAP inference problem as an Integer Polynomial Program (IPP) by schematically applying three lifted inference steps to the MLN: lifted decomposition, lifted conditioning, and partial grounding. Our IPP encoding is lifted in the sense that an integer assignment to a variable in the IPP may represent a truth-assignment to multiple indistinguishable ground atoms in the MLN. We show how to solve the IPP by first converting it to an Integer Linear Program (ILP) and then solving the latter using state-of-the-art ILP techniques. Experiments on several benchmark MLNs show that our new algorithm is substantially superior to ground inference and existing methods in terms of computational efficiency and solution quality.",
        "bibtex": "@inproceedings{NIPS2014_73ff9cef,\n author = {Sarkhel, Somdeb and Venugopal, Deepak and Singla, Parag and Gogate, Vibhav},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Integer Polynomial Programming Based Framework for Lifted MAP Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/73ff9cef00c79b1e69b184825bed619a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/73ff9cef00c79b1e69b184825bed619a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/73ff9cef00c79b1e69b184825bed619a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/73ff9cef00c79b1e69b184825bed619a-Reviews.html",
        "metareview": "",
        "pdf_size": 386731,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2276833585260164429&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Department, The University of Texas at Dallas; Computer Science Department, The University of Texas at Dallas; Department of CSE, I.I.T. Delhi; Computer Science Department, The University of Texas at Dallas",
        "aff_domain": "utdallas.edu;utdallas.edu;cse.iitd.ac.in;hlt.utdallas.edu",
        "email": "utdallas.edu;utdallas.edu;cse.iitd.ac.in;hlt.utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/73ff9cef00c79b1e69b184825bed619a-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Texas at Dallas;Indian Institute of Technology Delhi",
        "aff_unique_dep": "Computer Science Department;Department of CSE",
        "aff_unique_url": "https://www.utdallas.edu;https://www.iitd.ac.in",
        "aff_unique_abbr": "UT Dallas;IIT Delhi",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Dallas;Delhi",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "Analog Memories in a Balanced Rate-Based Network of E-I Neurons",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4320",
        "id": "4320",
        "author_site": "Dylan Festa, Guillaume Hennequin, Mate Lengyel",
        "author": "Dylan Festa; Guillaume Hennequin; M\u00e1t\u00e9 Lengyel",
        "abstract": "The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.",
        "bibtex": "@inproceedings{NIPS2014_88a24882,\n author = {Festa, Dylan and Hennequin, Guillaume and Lengyel, M\\'{a}t\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analog Memories in a Balanced Rate-Based Network of E-I Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/88a2488241e41ddeed33578cb15ae0a8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/88a2488241e41ddeed33578cb15ae0a8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/88a2488241e41ddeed33578cb15ae0a8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/88a2488241e41ddeed33578cb15ae0a8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/88a2488241e41ddeed33578cb15ae0a8-Reviews.html",
        "metareview": "",
        "pdf_size": 570914,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6693629056073166242&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computational & Biological Learning Lab, Department of Engineering, University of Cambridge, UK; Computational & Biological Learning Lab, Department of Engineering, University of Cambridge, UK; Computational & Biological Learning Lab, Department of Engineering, University of Cambridge, UK",
        "aff_domain": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/88a2488241e41ddeed33578cb15ae0a8-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Analysis of Brain States from Multi-Region LFP Time-Series",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4504",
        "id": "4504",
        "author_site": "Kyle R Ulrich, David Carlson, Wenzhao Lian, Jana S Borg, Kafui Dzirasa, Lawrence Carin",
        "author": "Kyle Ulrich; David E. Carlson; Wenzhao Lian; Jana Schaich Borg; Kafui Dzirasa; Lawrence Carin",
        "abstract": "The local field potential (LFP) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a ``brain state,'' relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states, based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs, with state- and region-dependent mixture weights, and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets, using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.",
        "bibtex": "@inproceedings{NIPS2014_52bcf2af,\n author = {Ulrich, Kyle and Carlson, David E. and Lian, Wenzhao and Borg, Jana Schaich and Dzirasa, Kafui and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analysis of Brain States from Multi-Region LFP Time-Series},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/52bcf2af8bea4119d710d252005dfddf-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/52bcf2af8bea4119d710d252005dfddf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/52bcf2af8bea4119d710d252005dfddf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/52bcf2af8bea4119d710d252005dfddf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/52bcf2af8bea4119d710d252005dfddf-Reviews.html",
        "metareview": "",
        "pdf_size": 824724,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12057247212453539104&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Psychiatry and Behavioral Sciences; Department of Psychiatry and Behavioral Sciences; Department of Electrical and Computer Engineering",
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/52bcf2af8bea4119d710d252005dfddf-Abstract.html",
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "Unknown Institution;Department of Psychiatry and Behavioral Sciences",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Psychiatry and Behavioral Sciences",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Analysis of Learning from Positive and Unlabeled Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4505",
        "id": "4505",
        "author_site": "Marthinus C du Plessis, Gang Niu, Masashi Sugiyama",
        "author": "Marthinus C. du Plessis; Gang Niu; Masashi Sugiyama",
        "abstract": "Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than $2\\sqrt{2}$ times the fully supervised case. These theoretical findings are also validated through experiments.",
        "bibtex": "@inproceedings{NIPS2014_f032bc3f,\n author = {du Plessis, Marthinus C. and Niu, Gang and Sugiyama, Masashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analysis of Learning from Positive and Unlabeled Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f032bc3f1eb547f716df87edb523b8f0-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f032bc3f1eb547f716df87edb523b8f0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f032bc3f1eb547f716df87edb523b8f0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f032bc3f1eb547f716df87edb523b8f0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f032bc3f1eb547f716df87edb523b8f0-Reviews.html",
        "metareview": "",
        "pdf_size": 221553,
        "gs_citation": 491,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10368209319771179690&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f032bc3f1eb547f716df87edb523b8f0-Abstract.html"
    },
    {
        "title": "Analysis of Variational Bayesian Latent Dirichlet Allocation: Weaker Sparsity Than MAP",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4506",
        "id": "4506",
        "author_site": "Shinichi Nakajima, Issei Sato, Masashi Sugiyama, Kazuho Watanabe, Hiroko Kobayashi",
        "author": "Shinichi Nakajima; Issei Sato; Masashi Sugiyama; Kazuho Watanabe; Hiroko Kobayashi",
        "abstract": "Latent Dirichlet allocation (LDA) is a popular generative model of various objects such as texts and images, where an object is expressed as a mixture of latent topics. In this paper, we theoretically investigate variational Bayesian (VB) learning in LDA. More specifically, we analytically derive the leading term of the VB free energy under an asymptotic setup, and show that there exist transition thresholds in Dirichlet hyperparameters around which the sparsity-inducing behavior drastically changes. Then we further theoretically reveal the notable phenomenon that VB tends to induce weaker sparsity than MAP in the LDA model, which is opposed to other models. We experimentally demonstrate the practical validity of our asymptotic theory on real-world Last.FM music data.",
        "bibtex": "@inproceedings{NIPS2014_4aff5873,\n author = {Nakajima, Shinichi and Sato, Issei and Sugiyama, Masashi and Watanabe, Kazuho and Kobayashi, Hiroko},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analysis of Variational Bayesian Latent Dirichlet Allocation: Weaker Sparsity Than MAP},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4aff5873f7e950764e21283fc0638818-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4aff5873f7e950764e21283fc0638818-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/4aff5873f7e950764e21283fc0638818-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4aff5873f7e950764e21283fc0638818-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4aff5873f7e950764e21283fc0638818-Reviews.html",
        "metareview": "",
        "pdf_size": 338765,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13449646713916656055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Berlin Big Data Center, TU Berlin; University of Tokyo; University of Tokyo; Toyohashi University of Technology; Nikon Corporation",
        "aff_domain": "tu-berlin.de;r.dl.itc.u-tokyo.ac.jp;k.u-tokyo.ac.jp;cs.tut.ac.jp;nikon.com",
        "email": "tu-berlin.de;r.dl.itc.u-tokyo.ac.jp;k.u-tokyo.ac.jp;cs.tut.ac.jp;nikon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4aff5873f7e950764e21283fc0638818-Abstract.html",
        "aff_unique_index": "0;1;1;2;3",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;University of Tokyo;Toyohashi University of Technology;Nikon Corporation",
        "aff_unique_dep": "Berlin Big Data Center;;;",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.u-tokyo.ac.jp;https://www.tut.ac.jp;https://www.nikon.com",
        "aff_unique_abbr": "TU Berlin;UTokyo;TUT;Nikon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berlin;",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "Germany;Japan"
    },
    {
        "title": "Approximating Hierarchical MV-sets for Hierarchical Clustering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4507",
        "id": "4507",
        "author_site": "Assaf Glazer, Omer Weissbrod, Michael Lindenbaum, Shaul Markovitch",
        "author": "Assaf Glazer; Omer Weissbrod; Michael Lindenbaum; Shaul Markovitch",
        "abstract": "The goal of hierarchical clustering is to construct a cluster tree, which can be viewed as the modal structure of a density. For this purpose, we use a convex optimization program that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions. We further extend existing graph-based methods to approximate the cluster tree of a distribution. By avoiding direct density estimation, our method is able to handle high-dimensional data more efficiently than existing density-based approaches. We present empirical results that demonstrate the superiority of our method over existing ones.",
        "bibtex": "@inproceedings{NIPS2014_d95d5cf0,\n author = {Glazer, Assaf and Weissbrod, Omer and Lindenbaum, Michael and Markovitch, Shaul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximating Hierarchical MV-sets for Hierarchical Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d95d5cf0eb5d33a5658fbd7b6ad3f306-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d95d5cf0eb5d33a5658fbd7b6ad3f306-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d95d5cf0eb5d33a5658fbd7b6ad3f306-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d95d5cf0eb5d33a5658fbd7b6ad3f306-Reviews.html",
        "metareview": "",
        "pdf_size": 2307545,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8838202285146620780&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Technion - Israel Institute of Technology; Department of Computer Science, Technion - Israel Institute of Technology; Department of Computer Science, Technion - Israel Institute of Technology; Department of Computer Science, Technion - Israel Institute of Technology",
        "aff_domain": "cs.technion.ac.il;cs.technion.ac.il;cs.technion.ac.il;cs.technion.ac.il",
        "email": "cs.technion.ac.il;cs.technion.ac.il;cs.technion.ac.il;cs.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d95d5cf0eb5d33a5658fbd7b6ad3f306-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4508",
        "id": "4508",
        "author_site": "Xianjie Chen, Alan Yuille",
        "author": "Xianjie Chen; Alan Yuille",
        "abstract": "We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.",
        "bibtex": "@inproceedings{NIPS2014_6ac19afb,\n author = {Chen, Xianjie and Yuille, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6ac19afba0591c506d495d9d6bed5802-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/6ac19afba0591c506d495d9d6bed5802-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/6ac19afba0591c506d495d9d6bed5802-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/6ac19afba0591c506d495d9d6bed5802-Reviews.html",
        "metareview": "",
        "pdf_size": 6944350,
        "gs_citation": 717,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17124544268300703083&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "ucla.edu;stat.ucla.edu",
        "email": "ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/6ac19afba0591c506d495d9d6bed5802-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4322",
        "id": "4322",
        "author_site": "Anshumali Shrivastava, Ping Li",
        "author": "Anshumali Shrivastava; Ping Li",
        "abstract": "We present the first provably sublinear time hashing algorithm for approximate \\emph{Maximum Inner Product Search} (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on $p$-stable distributions for $L_2$ norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.",
        "bibtex": "@inproceedings{NIPS2014_c98e7c4b,\n author = {Shrivastava, Anshumali and Li, Ping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c98e7c4b8f20d384e3ad857d0ee226cc-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c98e7c4b8f20d384e3ad857d0ee226cc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c98e7c4b8f20d384e3ad857d0ee226cc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c98e7c4b8f20d384e3ad857d0ee226cc-Reviews.html",
        "metareview": "",
        "pdf_size": 429326,
        "gs_citation": 608,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4104228853766090158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science, Computing and Information Science, Cornell University; Department of Statistics and Biostatistics, Department of Computer Science, Rutgers University",
        "aff_domain": "cs.cornell.edu;stat.rutgers.edu",
        "email": "cs.cornell.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c98e7c4b8f20d384e3ad857d0ee226cc-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cornell University;Rutgers University",
        "aff_unique_dep": "Department of Computer Science, Computing and Information Science;Department of Statistics and Biostatistics",
        "aff_unique_url": "https://www.cornell.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "Cornell;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Asynchronous Anytime Sequential Monte Carlo",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4324",
        "id": "4324",
        "author_site": "Brooks Paige, Frank Wood, Arnaud Doucet, Yee Whye Teh",
        "author": "Brooks Paige; Frank Wood; Arnaud Doucet; Yee Whye Teh",
        "abstract": "We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional sequential Monte Carlo algorithms that is amenable to parallel and distributed implementations. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade provides an unbiased marginal likelihood estimator which can be straightforwardly plugged into existing pseudo-marginal methods.",
        "bibtex": "@inproceedings{NIPS2014_83e8c422,\n author = {Paige, Brooks and Wood, Frank and Doucet, Arnaud and Teh, Yee Whye},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Asynchronous Anytime Sequential Monte Carlo},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/83e8c422c21a38e994a835120f898723-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/83e8c422c21a38e994a835120f898723-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/83e8c422c21a38e994a835120f898723-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/83e8c422c21a38e994a835120f898723-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/83e8c422c21a38e994a835120f898723-Reviews.html",
        "metareview": "",
        "pdf_size": 619434,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7541593620256878951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Engineering Science, University of Oxford, Oxford, UK; Department of Engineering Science, University of Oxford, Oxford, UK; Department of Statistics, University of Oxford, Oxford, UK; Department of Statistics, University of Oxford, Oxford, UK",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;stats.ox.ac.uk;stats.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;stats.ox.ac.uk;stats.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/83e8c422c21a38e994a835120f898723-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Engineering Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Attentional Neural Network: Feature Selection Using Cognitive Feedback",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4509",
        "id": "4509",
        "author_site": "Qian Wang, Jiaxing Zhang, Sen Song, Zheng Zhang",
        "author": "Qian Wang; Jiaxing Zhang; Sen Song; Zheng Zhang",
        "abstract": "Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.",
        "bibtex": "@inproceedings{NIPS2014_e3fa9a39,\n author = {Wang, Qian and Zhang, Jiaxing and Song, Sen and Zhang, Zheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Attentional Neural Network: Feature Selection Using Cognitive Feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e3fa9a393d67041081e8916c1db47f97-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e3fa9a393d67041081e8916c1db47f97-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e3fa9a393d67041081e8916c1db47f97-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e3fa9a393d67041081e8916c1db47f97-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e3fa9a393d67041081e8916c1db47f97-Reviews.html",
        "metareview": "",
        "pdf_size": 656724,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13131924308650087933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Biomedical Engineering, Tsinghua University; Microsoft Research Asia; Department of Biomedical Engineering, Tsinghua University + Department of Computer Science, NYU Shanghai; Department of Computer Science, NYU Shanghai",
        "aff_domain": "gmail.com;microsoft.com;gmail.com;nyu.edu",
        "email": "gmail.com;microsoft.com;gmail.com;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e3fa9a393d67041081e8916c1db47f97-Abstract.html",
        "aff_unique_index": "0;1;0+2;2",
        "aff_unique_norm": "Tsinghua University;Microsoft;New York University Shanghai",
        "aff_unique_dep": "Department of Biomedical Engineering;Research;Department of Computer Science",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research/group/asia;https://shanghai.nyu.edu",
        "aff_unique_abbr": "THU;MSR Asia;NYU Shanghai",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";Asia;Shanghai",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Augmentative Message Passing for Traveling Salesman Problem and Graph Partitioning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4510",
        "id": "4510",
        "author_site": "Mohsen Ravanbakhsh, Reihaneh Rabbany, Russell Greiner",
        "author": "Siamak Ravanbakhsh; Reihaneh Rabbany; Russell Greiner",
        "abstract": "The cutting plane method is an augmentative constrained optimization procedure that is often used with continuous-domain optimization techniques such as linear and convex programs. We investigate the viability of a similar idea within message passing -- for integral solutions -- in the context of two combinatorial problems: 1) For Traveling Salesman Problem (TSP), we propose a factor-graph based on Held-Karp formulation, with an exponential number of constraint factors, each of which has an exponential but sparse tabular form. 2) For graph-partitioning (a.k.a. community mining) using modularity optimization, we introduce a binary variable model with a large number of constraints that enforce formation of cliques. In both cases we are able to derive surprisingly simple message updates that lead to competitive solutions on benchmark instances. In particular for TSP we are able to find near-optimal solutions in the time that empirically grows with $N^3$, demonstrating that augmentation is practical and efficient.",
        "bibtex": "@inproceedings{NIPS2014_438e7be5,\n author = {Ravanbakhsh, Siamak and Rabbany, Reihaneh and Greiner, Russell},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Augmentative Message Passing for Traveling Salesman Problem and Graph Partitioning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/438e7be59090dde06455320f800b1a75-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/438e7be59090dde06455320f800b1a75-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/438e7be59090dde06455320f800b1a75-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/438e7be59090dde06455320f800b1a75-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/438e7be59090dde06455320f800b1a75-Reviews.html",
        "metareview": "",
        "pdf_size": 3553924,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7893291253641260508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/438e7be59090dde06455320f800b1a75-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Augur: Data-Parallel Probabilistic Modeling",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4362",
        "id": "4362",
        "author_site": "Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam Pocock, Stephen Green, Guy L Steele",
        "author": "Jean-Baptiste Tristan; Daniel Huang; Joseph Tassarotti; Adam Pocock; Stephen J. Green; Guy L. Steele; Jr",
        "abstract": "Implementing inference procedures for each new probabilistic model is time-consuming and error-prone. Probabilistic programming addresses this problem by allowing a user to specify the model and then automatically generating the inference procedure. To make this practical it is important to generate high performance inference code. In turn, on modern architectures, high performance requires parallel execution. In this paper we present Augur, a probabilistic modeling language and compiler for Bayesian networks designed to make effective use of data-parallel architectures such as GPUs. We show that the compiler can generate data-parallel inference code scalable to thousands of GPU cores by making use of the conditional independence relationships in the Bayesian network.",
        "bibtex": "@inproceedings{NIPS2014_2d6e6b96,\n author = {Tristan, Jean-Baptiste and Huang, Daniel and Tassarotti, Joseph and Pocock, Adam and Green, Stephen J. and Steele, Guy L.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Augur: Data-Parallel Probabilistic Modeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2d6e6b9675fb31f6c5250b7ea73fc37d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2d6e6b9675fb31f6c5250b7ea73fc37d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/2d6e6b9675fb31f6c5250b7ea73fc37d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2d6e6b9675fb31f6c5250b7ea73fc37d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2d6e6b9675fb31f6c5250b7ea73fc37d-Reviews.html",
        "metareview": "",
        "pdf_size": 312079,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9840716222752418838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2d6e6b9675fb31f6c5250b7ea73fc37d-Abstract.html"
    },
    {
        "title": "Automated Variational Inference for Gaussian Process Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4511",
        "id": "4511",
        "author_site": "Trung V Nguyen, Edwin Bonilla",
        "author": "Trung V. Nguyen; Edwin V. Bonilla",
        "abstract": "We develop an automated variational method for approximate inference in Gaussian process (GP) models whose posteriors are often intractable. Using a mixture of Gaussians as the variational distribution, we show that (i) the variational objective and its gradients can be approximated efficiently via sampling from univariate Gaussian distributions and (ii) the gradients of the GP hyperparameters can be obtained analytically regardless of the model likelihood. We further propose two instances of the variational distribution whose covariance matrices can be parametrized linearly in the number of observations. These results allow gradient-based optimization to be done efficiently in a black-box manner. Our approach is thoroughly verified on 5 models using 6 benchmark datasets, performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative MCMC sampling approaches. Our method can be a valuable tool for practitioners and researchers to investigate new models with minimal effort in deriving model-specific inference algorithms.",
        "bibtex": "@inproceedings{NIPS2014_78b6d489,\n author = {Nguyen, Trung V. and Bonilla, Edwin V.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Automated Variational Inference for Gaussian Process Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/78b6d4893436fad2d3e8c13be6947854-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/78b6d4893436fad2d3e8c13be6947854-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/78b6d4893436fad2d3e8c13be6947854-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/78b6d4893436fad2d3e8c13be6947854-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/78b6d4893436fad2d3e8c13be6947854-Reviews.html",
        "metareview": "",
        "pdf_size": 836914,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8068973524092784053&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ANU & NICTA; The University of New South Wales",
        "aff_domain": "nicta.com.au;unsw.edu.au",
        "email": "nicta.com.au;unsw.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/78b6d4893436fad2d3e8c13be6947854-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Australian National University;University of New South Wales",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.unsw.edu.au",
        "aff_unique_abbr": "ANU;UNSW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Automatic Discovery of Cognitive Skills to Improve the Prediction of Student Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4512",
        "id": "4512",
        "author_site": "Robert Lindsey, Mohammad Khajah, Michael Mozer",
        "author": "Robert V. Lindsey; Mohammad Khajah; Michael C. Mozer",
        "abstract": "To master a discipline such as algebra or physics, students must acquire a set of cognitive skills. Traditionally, educators and domain experts manually determine what these skills are and then select practice exercises to hone a particular skill. We propose a technique that uses student performance data to automatically discover the skills needed in a discipline. The technique assigns a latent skill to each exercise such that a student's expected accuracy on a sequence of same-skill exercises improves monotonically with practice. Rather than discarding the skills identified by experts, our technique incorporates a nonparametric prior over the exercise-skill assignments that is based on the expert-provided skills and a weighted Chinese restaurant process. We test our technique on datasets from five different intelligent tutoring systems designed for students ranging in age from middle school through college. We obtain two surprising results. First, in three of the five datasets, the skills inferred by our technique support significantly improved predictions of student performance over the expert-provided skills. Second, the expert-provided skills have little value: our technique predicts student performance nearly as well when it ignores the domain expertise as when it attempts to leverage it. We discuss explanations for these surprising results and also the relationship of our skill-discovery technique to alternative approaches.",
        "bibtex": "@inproceedings{NIPS2014_a4bfca3e,\n author = {Lindsey, Robert V. and Khajah, Mohammad and Mozer, Michael C.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Automatic Discovery of Cognitive Skills to Improve the Prediction of Student Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a4bfca3e09377c477f2d70e104363061-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a4bfca3e09377c477f2d70e104363061-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a4bfca3e09377c477f2d70e104363061-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a4bfca3e09377c477f2d70e104363061-Reviews.html",
        "metareview": "",
        "pdf_size": 346690,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15527333194309777571&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Institute of Cognitive Science, University of Colorado, Boulder; Department of Computer Science and Institute of Cognitive Science, University of Colorado, Boulder; Department of Computer Science and Institute of Cognitive Science, University of Colorado, Boulder",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a4bfca3e09377c477f2d70e104363061-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bandit Convex Optimization: Towards Tight Bounds",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4513",
        "id": "4513",
        "author_site": "Elad Hazan, Kfir Y. Levy",
        "author": "Elad Hazan; Kfir Y. Levy",
        "abstract": "Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.",
        "bibtex": "@inproceedings{NIPS2014_3ce3c4f8,\n author = {Hazan, Elad and Levy, Kfir Y.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bandit Convex Optimization: Towards Tight Bounds},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3ce3c4f8183442957d051dc8ebed2668-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3ce3c4f8183442957d051dc8ebed2668-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3ce3c4f8183442957d051dc8ebed2668-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3ce3c4f8183442957d051dc8ebed2668-Reviews.html",
        "metareview": "",
        "pdf_size": 702371,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16968832611121836002&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Technion\u2014Israel Institute of Technology; Technion\u2014Israel Institute of Technology",
        "aff_domain": "ie.technion.ac.il;tx.technion.ac.il",
        "email": "ie.technion.ac.il;tx.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3ce3c4f8183442957d051dc8ebed2668-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion\u2014Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Bayes-Adaptive Simulation-based Search with Value Function Approximation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4514",
        "id": "4514",
        "author_site": "Arthur Guez, Nicolas Heess, David Silver, Peter Dayan",
        "author": "Arthur Guez; Nicolas Heess; David Silver; Peter Dayan",
        "abstract": "Bayes-adaptive planning offers a principled solution to the exploration-exploitation trade-off under model uncertainty. It finds the optimal policy in belief space, which explicitly accounts for the expected effect on future rewards of reductions in uncertainty. However, the Bayes-adaptive solution is typically intractable in domains with large or continuous state spaces. We present a tractable method for approximating the Bayes-adaptive solution by combining simulation-based search with a novel value function approximation technique that generalises over belief space. Our method outperforms prior approaches in both discrete bandit tasks and simple continuous navigation and control tasks.",
        "bibtex": "@inproceedings{NIPS2014_74d863ca,\n author = {Guez, Arthur and Heess, Nicolas and Silver, David and Dayan, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayes-Adaptive Simulation-based Search with Value Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/74d863ca4a12ccca50a754a3b277dbf7-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/74d863ca4a12ccca50a754a3b277dbf7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/74d863ca4a12ccca50a754a3b277dbf7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/74d863ca4a12ccca50a754a3b277dbf7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/74d863ca4a12ccca50a754a3b277dbf7-Reviews.html",
        "metareview": "",
        "pdf_size": 407349,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8897476105824309347&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Gatsby Unit, UCL + Google DeepMind; Google DeepMind; Google DeepMind; Gatsby Unit, UCL",
        "aff_domain": "google.com; ; ; ",
        "email": "google.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/74d863ca4a12ccca50a754a3b277dbf7-Abstract.html",
        "aff_unique_index": "0+1;1;1;0",
        "aff_unique_norm": "University College London;Google",
        "aff_unique_dep": "Gatsby Unit;Google DeepMind",
        "aff_unique_url": "https://www.ucl.ac.uk;https://deepmind.com",
        "aff_unique_abbr": "UCL;DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Bayesian Inference for Structured Spike and Slab Priors",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4515",
        "id": "4515",
        "author_site": "Michael Riis Andersen, Ole Winther, Lars K Hansen",
        "author": "Michael Riis Andersen; Ole Winther; Lars Kai Hansen",
        "abstract": "Sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint. We propose a novel prior formulation, the structured spike and slab prior, which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial Gaussian process on the spike and slab probabilities. Thus, prior information on the structure of the sparsity pattern can be encoded using generic covariance functions. Furthermore, we provide a Bayesian inference scheme for the proposed model based on the expectation propagation framework. Using numerical experiments on synthetic data, we demonstrate the benefits of the model.",
        "bibtex": "@inproceedings{NIPS2014_8927f9b4,\n author = {Andersen, Michael Riis and Winther, Ole and Hansen, Lars Kai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Inference for Structured Spike and Slab Priors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8927f9b43dbc13ddfa1edf2103f2191f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8927f9b43dbc13ddfa1edf2103f2191f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8927f9b43dbc13ddfa1edf2103f2191f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8927f9b43dbc13ddfa1edf2103f2191f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8927f9b43dbc13ddfa1edf2103f2191f-Reviews.html",
        "metareview": "",
        "pdf_size": 1449112,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3532945647061133737&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8927f9b43dbc13ddfa1edf2103f2191f-Abstract.html"
    },
    {
        "title": "Bayesian Nonlinear Support Vector Machines and Discriminative Factor Modeling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4516",
        "id": "4516",
        "author_site": "Ricardo Henao, Xin Yuan, Lawrence Carin",
        "author": "Ricardo Henao; Xin Yuan; Lawrence Carin",
        "abstract": "A new Bayesian formulation is developed for nonlinear support vector machines (SVMs), based on a Gaussian process and with the SVM hinge loss expressed as a scaled mixture of normals. We then integrate the Bayesian SVM into a factor model, in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier. Inference is performed with expectation conditional maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive set of experiments demonstrate the utility of using a nonlinear Bayesian SVM within discriminative feature learning and factor modeling, from the standpoints of accuracy and interpretability",
        "bibtex": "@inproceedings{NIPS2014_a05a0bed,\n author = {Henao, Ricardo and Yuan, Xin and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Nonlinear Support Vector Machines and Discriminative Factor Modeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a05a0bed08d0c0f10f00a8067a668efc-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a05a0bed08d0c0f10f00a8067a668efc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a05a0bed08d0c0f10f00a8067a668efc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a05a0bed08d0c0f10f00a8067a668efc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a05a0bed08d0c0f10f00a8067a668efc-Reviews.html",
        "metareview": "",
        "pdf_size": 248228,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13594727406315389864&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708",
        "aff_domain": "duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a05a0bed08d0c0f10f00a8067a668efc-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Durham",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bayesian Sampling Using Stochastic Gradient Thermostats",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4517",
        "id": "4517",
        "author_site": "Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert D Skeel, Hartmut Neven",
        "author": "Nan Ding; Youhan Fang; Ryan Babbush; Changyou Chen; Robert D. Skeel; Hartmut Neven",
        "abstract": "Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem, we show that one can leverage a small number of additional variables in order to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.",
        "bibtex": "@inproceedings{NIPS2014_b610047c,\n author = {Ding, Nan and Fang, Youhan and Babbush, Ryan and Chen, Changyou and Skeel, Robert D. and Neven, Hartmut},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Sampling Using Stochastic Gradient Thermostats},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b610047c85e73cb7ec04fd36ec503f93-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b610047c85e73cb7ec04fd36ec503f93-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b610047c85e73cb7ec04fd36ec503f93-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b610047c85e73cb7ec04fd36ec503f93-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b610047c85e73cb7ec04fd36ec503f93-Reviews.html",
        "metareview": "",
        "pdf_size": 465999,
        "gs_citation": 279,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3698848005403648844&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Google Inc.; Purdue University; Google Inc.; Duke University; Purdue University; Google Inc.",
        "aff_domain": "google.com;cs.purdue.edu;google.com;gmail.com;cs.purdue.edu;google.com",
        "email": "google.com;cs.purdue.edu;google.com;gmail.com;cs.purdue.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b610047c85e73cb7ec04fd36ec503f93-Abstract.html",
        "aff_unique_index": "0;1;0;2;1;0",
        "aff_unique_norm": "Google;Purdue University;Duke University",
        "aff_unique_dep": "Google;;",
        "aff_unique_url": "https://www.google.com;https://www.purdue.edu;https://www.duke.edu",
        "aff_unique_abbr": "Google;Purdue;Duke",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Best-Arm Identification in Linear Bandits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4519",
        "id": "4519",
        "author_site": "Marta Soare, Alessandro Lazaric, Remi Munos",
        "author": "Marta Soare; Alessandro Lazaric; R\u00e9mi Munos",
        "abstract": "We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\\theta^*$ and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the $G$-optimality criterion used in optimal experimental design.",
        "bibtex": "@inproceedings{NIPS2014_f8d84caa,\n author = {Soare, Marta and Lazaric, Alessandro and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Best-Arm Identification in Linear Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f8d84caae48546d0934d637ab54f7086-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f8d84caae48546d0934d637ab54f7086-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f8d84caae48546d0934d637ab54f7086-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f8d84caae48546d0934d637ab54f7086-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f8d84caae48546d0934d637ab54f7086-Reviews.html",
        "metareview": "",
        "pdf_size": 395824,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9058729758777559994&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "INRIALille\u2013NordEurope,SequeLTeam; INRIALille\u2013NordEurope,SequeLTeam; INRIALille\u2013NordEurope,SequeLTeam + MicrosoftResearchNew-England + GoogleDeepMind",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f8d84caae48546d0934d637ab54f7086-Abstract.html",
        "aff_unique_index": "0;0;0+1+2",
        "aff_unique_norm": "INRIA Lille - Nord Europe;Microsoft;DeepMind",
        "aff_unique_dep": "SequeL Team;Microsoft Research;DeepMind",
        "aff_unique_url": "https://www.inria.fr/lille;https://www.microsoft.com/en-us/research/group/new-england;https://deepmind.com",
        "aff_unique_abbr": "INRIA;MSR NE;DeepMind",
        "aff_campus_unique_index": "0;0;0+1",
        "aff_campus_unique": "Lille;New England;",
        "aff_country_unique_index": "0;0;0+1+2",
        "aff_country_unique": "France;United States;United Kingdom"
    },
    {
        "title": "Beta-Negative Binomial Process and Exchangeable \ufffcRandom Partitions for Mixed-Membership Modeling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4520",
        "id": "4520",
        "author": "Mingyuan Zhou",
        "abstract": "The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.",
        "bibtex": "@inproceedings{NIPS2014_050a4029,\n author = {Zhou, Mingyuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beta-Negative Binomial Process and Exchangeable \ufffcRandom Partitions for Mixed-Membership Modeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/050a402944ba50e4ffc727ce02cfb403-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/050a402944ba50e4ffc727ce02cfb403-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/050a402944ba50e4ffc727ce02cfb403-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/050a402944ba50e4ffc727ce02cfb403-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/050a402944ba50e4ffc727ce02cfb403-Reviews.html",
        "metareview": "",
        "pdf_size": 646312,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1785803968882094773&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "IROM Department, McCombs School of Business, The University of Texas at Austin, Austin, TX 78712, USA",
        "aff_domain": "mccombs.utexas.edu",
        "email": "mccombs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/050a402944ba50e4ffc727ce02cfb403-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "IROM Department",
        "aff_unique_url": "https://www.mccombs.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Beyond Disagreement-Based Agnostic Active Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4364",
        "id": "4364",
        "author_site": "Chicheng Zhang, Kamalika Chaudhuri",
        "author": "Chicheng Zhang; Kamalika Chaudhuri",
        "abstract": "We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are {\\em{disagreement-based active learning}}, which has a high label requirement, and {\\em{margin-based active learning}}, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions -- a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor.",
        "bibtex": "@inproceedings{NIPS2014_09939c83,\n author = {Zhang, Chicheng and Chaudhuri, Kamalika},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond Disagreement-Based Agnostic Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/09939c83d244f420d893535340da3ae4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/09939c83d244f420d893535340da3ae4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/09939c83d244f420d893535340da3ae4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/09939c83d244f420d893535340da3ae4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/09939c83d244f420d893535340da3ae4-Reviews.html",
        "metareview": "",
        "pdf_size": 329394,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10897937291091701392&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, San Diego; University of California, San Diego",
        "aff_domain": "ucsd.edu;cs.ucsd.edu",
        "email": "ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/09939c83d244f420d893535340da3ae4-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Beyond the Birkhoff Polytope: Convex Relaxations for Vector Permutation Problems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4521",
        "id": "4521",
        "author_site": "Cong Han Lim, Stephen Wright",
        "author": "Cong Han Lim; Stephen J. Wright",
        "abstract": "The Birkhoff polytope (the convex hull of the set of permutation matrices), which is represented using $\\Theta(n^2)$ variables and constraints, is frequently invoked in formulating relaxations of optimization problems over permutations. Using a recent construction of Goemans (2010), we show that when optimizing over the convex hull of the permutation vectors (the permutahedron), we can reduce the number of variables and constraints to $\\Theta(n \\log n)$ in theory and $\\Theta(n \\log^2 n)$ in practice. We modify the recent convex formulation of the 2-SUM problem introduced by Fogel et al. (2013) to use this polytope, and demonstrate how we can attain results of similar quality in significantly less computational time for large $n$. To our knowledge, this is the first usage of Goemans' compact formulation of the permutahedron in a convex optimization problem. We also introduce a simpler regularization scheme for this convex formulation of the 2-SUM problem that yields good empirical results.",
        "bibtex": "@inproceedings{NIPS2014_675731a0,\n author = {Lim, Cong Han and Wright, Stephen J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond the Birkhoff Polytope: Convex Relaxations for Vector Permutation Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/675731a0bb3936b988f8203106382076-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/675731a0bb3936b988f8203106382076-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/675731a0bb3936b988f8203106382076-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/675731a0bb3936b988f8203106382076-Reviews.html",
        "metareview": "",
        "pdf_size": 463642,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11899615367670221258&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Sciences, University of Wisconsin - Madison; Department of Computer Sciences, University of Wisconsin - Madison",
        "aff_domain": "cs.wisc.edu;cs.wisc.edu",
        "email": "cs.wisc.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/675731a0bb3936b988f8203106382076-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "Department of Computer Sciences",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Biclustering Using Message Passing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4522",
        "id": "4522",
        "author_site": "Luke O'Connor, Soheil Feizi",
        "author": "Luke O'Connor; Soheil Feizi",
        "abstract": "Biclustering is the analog of clustering on a bipartite graph. Existent methods infer biclusters through local search strategies that find one cluster at a time; a common technique is to update the row memberships based on the current column memberships, and vice versa. We propose a biclustering algorithm that maximizes a global objective function using message passing. Our objective function closely approximates a general likelihood function, separating a cluster size penalty term into row- and column-count penalties. Because we use a global optimization framework, our approach excels at resolving the overlaps between biclusters, which are important features of biclusters in practice. Moreover, Expectation-Maximization can be used to learn the model parameters if they are unknown. In simulations, we find that our method outperforms two of the best existing biclustering algorithms, ISA and LAS, when the planted clusters overlap. Applied to three gene expression datasets, our method finds coregulated gene clusters that have high quality in terms of cluster size and density.",
        "bibtex": "@inproceedings{NIPS2014_03bc9977,\n author = {O\\textquotesingle Connor, Luke and Feizi, Soheil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Biclustering Using Message Passing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/03bc99773b4d3aa3cac5b59ce24d8afd-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/03bc99773b4d3aa3cac5b59ce24d8afd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/03bc99773b4d3aa3cac5b59ce24d8afd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/03bc99773b4d3aa3cac5b59ce24d8afd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/03bc99773b4d3aa3cac5b59ce24d8afd-Reviews.html",
        "metareview": "",
        "pdf_size": 1250211,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7163072980133000338&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Bioinformatics and Integrative Genomics, Harvard University; Electrical Engineering and Computer Science, Massachusetts Institute of Technology",
        "aff_domain": "g.harvard.edu;mit.edu",
        "email": "g.harvard.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/03bc99773b4d3aa3cac5b59ce24d8afd-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Harvard University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Bioinformatics and Integrative Genomics;Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.harvard.edu;https://web.mit.edu",
        "aff_unique_abbr": "Harvard;MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Blossom Tree Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4523",
        "id": "4523",
        "author_site": "Zhe Liu, John Lafferty",
        "author": "Zhe Liu; John Lafferty",
        "abstract": "We combine the ideas behind trees and Gaussian graphical models to form a new nonparametric family of graphical models. Our approach is to attach nonparanormal blossoms\", with arbitrary graphs, to a collection of nonparametric trees. The tree edges are chosen to connect variables that most violate joint Gaussianity. The non-tree edges are partitioned into disjoint groups, and assigned to tree nodes using a nonparametric partial correlation statistic. A nonparanormal blossom is then \"grown\" for each group using established methods based on the graphical lasso. The result is a factorization with respect to the union of the tree branches and blossoms, defining a high-dimensional joint density that can be efficiently estimated and evaluated on test points. Theoretical properties and experiments with simulated and real data demonstrate the effectiveness of blossom trees.\"",
        "bibtex": "@inproceedings{NIPS2014_4ac8790a,\n author = {Liu, Zhe and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Blossom Tree Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4ac8790a1ebfd6ccc6b3a46c096cf42c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4ac8790a1ebfd6ccc6b3a46c096cf42c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/4ac8790a1ebfd6ccc6b3a46c096cf42c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4ac8790a1ebfd6ccc6b3a46c096cf42c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4ac8790a1ebfd6ccc6b3a46c096cf42c-Reviews.html",
        "metareview": "",
        "pdf_size": 992223,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3102817830595895101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Statistics, University of Chicago; Department of Statistics, University of Chicago + Department of Computer Science, University of Chicago",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4ac8790a1ebfd6ccc6b3a46c096cf42c-Abstract.html",
        "aff_unique_index": "0;0+0",
        "aff_unique_norm": "University of Chicago",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.uchicago.edu",
        "aff_unique_abbr": "UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bounded Regret for Finite-Armed Structured Bandits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4524",
        "id": "4524",
        "author_site": "Tor Lattimore, Remi Munos",
        "author": "Tor Lattimore; R\u00e9mi Munos",
        "abstract": "We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.",
        "bibtex": "@inproceedings{NIPS2014_f15f1891,\n author = {Lattimore, Tor and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bounded Regret for Finite-Armed Structured Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f15f1891ab6f5f5084f9f7ecace711fd-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f15f1891ab6f5f5084f9f7ecace711fd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f15f1891ab6f5f5084f9f7ecace711fd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f15f1891ab6f5f5084f9f7ecace711fd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f15f1891ab6f5f5084f9f7ecace711fd-Reviews.html",
        "metareview": "",
        "pdf_size": 275324,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10838623179333854128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computing Science, University of Alberta, Canada; INRIA, Lille, France + Google DeepMind",
        "aff_domain": "ualberta.ca;inria.fr",
        "email": "ualberta.ca;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f15f1891ab6f5f5084f9f7ecace711fd-Abstract.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "University of Alberta;INRIA;Google",
        "aff_unique_dep": "Department of Computing Science;;Google DeepMind",
        "aff_unique_url": "https://www.ualberta.ca;https://www.inria.fr;https://deepmind.com",
        "aff_unique_abbr": "UAlberta;INRIA;DeepMind",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;1+2",
        "aff_country_unique": "Canada;France;United Kingdom"
    },
    {
        "title": "Bregman Alternating Direction Method of Multipliers",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4525",
        "id": "4525",
        "author_site": "Huahua Wang, Arindam Banerjee",
        "author": "Huahua Wang; Arindam Banerjee",
        "abstract": "The mirror descent algorithm (MDA) generalizes gradient descent by using a Bregman divergence to replace squared Euclidean distance. In this paper, we similarly generalize the alternating direction method of multipliers (ADMM) to Bregman ADMM (BADMM), which allows the choice of different Bregman divergences to exploit the structure of problems. BADMM provides a unified framework for ADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM. We establish the global convergence and the $O(1/T)$ iteration complexity for BADMM. In some cases, BADMM can be faster than ADMM by a factor of $O(n/\\ln n)$ where $n$ is the dimensionality. In solving the linear program of mass transportation problem, BADMM leads to massive parallelism and can easily run on GPU. BADMM is several times faster than highly optimized commercial software Gurobi.",
        "bibtex": "@inproceedings{NIPS2014_65ccb47f,\n author = {Wang, Huahua and Banerjee, Arindam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bregman Alternating Direction Method of Multipliers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/65ccb47f8f0e0f6de0748ea2ca32b108-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/65ccb47f8f0e0f6de0748ea2ca32b108-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/65ccb47f8f0e0f6de0748ea2ca32b108-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/65ccb47f8f0e0f6de0748ea2ca32b108-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/65ccb47f8f0e0f6de0748ea2ca32b108-Reviews.html",
        "metareview": "",
        "pdf_size": 377011,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10845559271528731281&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dept of Computer Science & Engg, University of Minnesota, Twin Cities; Dept of Computer Science & Engg, University of Minnesota, Twin Cities",
        "aff_domain": "cs.umn.edu;cs.umn.edu",
        "email": "cs.umn.edu;cs.umn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/65ccb47f8f0e0f6de0748ea2ca32b108-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "Department of Computer Science & Engineering",
        "aff_unique_url": "https://www.umn.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Twin Cities",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Capturing Semantically Meaningful Word Dependencies with an Admixture of Poisson MRFs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4526",
        "id": "4526",
        "author_site": "David I Inouye, Pradeep Ravikumar, Inderjit Dhillon",
        "author": "David I. Inouye; Pradeep Ravikumar; Inderjit S. Dhillon",
        "abstract": "We develop a fast algorithm for the Admixture of Poisson MRFs (APM) topic model and propose a novel metric to directly evaluate this model. The APM topic model recently introduced by Inouye et al. (2014) is the first topic model that allows for word dependencies within each topic unlike in previous topic models like LDA that assume independence between words within a topic. Research in both the semantic coherence of a topic models (Mimno et al. 2011, Newman et al. 2010) and measures of model fitness (Mimno & Blei 2011) provide strong support that explicitly modeling word dependencies---as in APM---could be both semantically meaningful and essential for appropriately modeling real text data. Though APM shows significant promise for providing a better topic model, APM has a high computational complexity because $O(p^2)$ parameters must be estimated where $p$ is the number of words (Inouye et al. could only provide results for datasets with $p = 200$). In light of this, we develop a parallel alternating Newton-like algorithm for training the APM model that can handle $p = 10^4$ as an important step towards scaling to large datasets. In addition, Inouye et al. only provided tentative and inconclusive results on the utility of APM. Thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e. how much one word brings to mind\" another word (Boyd-Graber et al. 2006)). We provide compelling quantitative and qualitative results on the BNC corpus that demonstrate the superiority of APM over previous topic models for identifying semantically meaningful word dependencies. (MATLAB code available at: http://bigdata.ices.utexas.edu/software/apm/)\"",
        "bibtex": "@inproceedings{NIPS2014_11f57302,\n author = {Inouye, David I. and Ravikumar, Pradeep and Dhillon, Inderjit S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Capturing Semantically Meaningful Word Dependencies with an Admixture of Poisson MRFs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/11f57302e794a5097ee729d99e6c69fb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/11f57302e794a5097ee729d99e6c69fb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/11f57302e794a5097ee729d99e6c69fb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/11f57302e794a5097ee729d99e6c69fb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/11f57302e794a5097ee729d99e6c69fb-Reviews.html",
        "metareview": "",
        "pdf_size": 1166102,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8442393500224112471&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "http://bigdata.ices.utexas.edu/software/apm/",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/11f57302e794a5097ee729d99e6c69fb-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Causal Inference through a Witness Protection Program",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4527",
        "id": "4527",
        "author_site": "Ricardo Silva, Robin Evans",
        "author": "Ricardo Silva; Robin Evans",
        "abstract": "One of the most fundamental problems in causal inference is the estimation of a causal effect when variables are confounded. This is difficult in an observational study because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest",
        "bibtex": "@inproceedings{NIPS2014_24b97695,\n author = {Silva, Ricardo and Evans, Robin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Causal Inference through a Witness Protection Program},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/24b9769502b00c79bfd0d5ef3a616ca6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/24b9769502b00c79bfd0d5ef3a616ca6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/24b9769502b00c79bfd0d5ef3a616ca6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/24b9769502b00c79bfd0d5ef3a616ca6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/24b9769502b00c79bfd0d5ef3a616ca6-Reviews.html",
        "metareview": "",
        "pdf_size": 290666,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12078289557801549671&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistical Science and CSML, University College London; Department of Statistics, University of Oxford",
        "aff_domain": "stats.ucl.ac.uk;stats.ox.ac.uk",
        "email": "stats.ucl.ac.uk;stats.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/24b9769502b00c79bfd0d5ef3a616ca6-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;University of Oxford",
        "aff_unique_dep": "Department of Statistical Science and CSML;Department of Statistics",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.ox.ac.uk",
        "aff_unique_abbr": "UCL;Oxford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "London;Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Causal Strategic Inference in Networked Microfinance Economies",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4366",
        "id": "4366",
        "author_site": "Mohammad T Irfan, Luis E Ortiz",
        "author": "Mohammad T. Irfan; Luis E. Ortiz",
        "abstract": "Performing interventions is a major challenge in economic policy-making. We propose \\emph{causal strategic inference} as a framework for conducting interventions and apply it to large, networked microfinance economies. The basic solution platform consists of modeling a microfinance market as a networked economy, learning the parameters of the model from the real-world microfinance data, and designing algorithms for various computational problems in question. We adopt Nash equilibrium as the solution concept for our model. For a special case of our model, we show that an equilibrium point always exists and that the equilibrium interest rates are unique. For the general case, we give a constructive proof of the existence of an equilibrium point. Our empirical study is based on the microfinance data from Bangladesh and Bolivia, which we use to first learn our models. We show that causal strategic inference can assist policy-makers by evaluating the outcomes of various types of interventions, such as removing a loss-making bank from the market, imposing an interest rate cap, and subsidizing banks.",
        "bibtex": "@inproceedings{NIPS2014_26b6534e,\n author = {Irfan, Mohammad T. and Ortiz, Luis E.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Causal Strategic Inference in Networked Microfinance Economies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/26b6534eeac6dfc4a53a5acf158b9579-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/26b6534eeac6dfc4a53a5acf158b9579-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/26b6534eeac6dfc4a53a5acf158b9579-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/26b6534eeac6dfc4a53a5acf158b9579-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/26b6534eeac6dfc4a53a5acf158b9579-Reviews.html",
        "metareview": "",
        "pdf_size": 273532,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2479769306868321090&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Bowdoin College; Department of Computer Science, Stony Brook University",
        "aff_domain": "bowdoin.edu;cs.stonybrook.edu",
        "email": "bowdoin.edu;cs.stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/26b6534eeac6dfc4a53a5acf158b9579-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Bowdoin College;Stony Brook University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.bowdoin.edu;https://www.stonybrook.edu",
        "aff_unique_abbr": "Bowdoin;SBU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stony Brook",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Clamping Variables and Approximate Inference",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4326",
        "id": "4326",
        "author_site": "Adrian Weller, Tony Jebara",
        "author": "Adrian Weller; Tony Jebara",
        "abstract": "It was recently proved using graph covers (Ruozzi, 2012) that the Bethe partition function is upper bounded by the true partition function for a binary pairwise model that is attractive. Here we provide a new, arguably simpler proof from first principles. We make use of the idea of clamping a variable to a particular value. For an attractive model, we show that summing over the Bethe partition functions for each sub-model obtained after clamping any variable can only raise (and hence improve) the approximation. In fact, we derive a stronger result that may have other useful implications. Repeatedly clamping until we obtain a model with no cycles, where the Bethe approximation is exact, yields the result. We also provide a related lower bound on a broad class of approximate partition functions of general pairwise multi-label models that depends only on the topology. We demonstrate that clamping a few wisely chosen variables can be of practical value by dramatically reducing approximation error.",
        "bibtex": "@inproceedings{NIPS2014_3da20e0a,\n author = {Weller, Adrian and Jebara, Tony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clamping Variables and Approximate Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3da20e0a3993e68d9a1e7292135b445d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3da20e0a3993e68d9a1e7292135b445d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3da20e0a3993e68d9a1e7292135b445d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3da20e0a3993e68d9a1e7292135b445d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3da20e0a3993e68d9a1e7292135b445d-Reviews.html",
        "metareview": "",
        "pdf_size": 1238072,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7724837977129328706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Columbia University, New York, NY 10027; Columbia University, New York, NY 10027",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3da20e0a3993e68d9a1e7292135b445d-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Clustered factor analysis of multineuronal spike data",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4368",
        "id": "4368",
        "author_site": "Lars Buesing, Timothy A Machado, John P Cunningham, Liam Paninski",
        "author": "Lars Buesing; Timothy A. Machado; John P. Cunningham; Liam Paninski",
        "abstract": "High-dimensional, simultaneous recordings of neural spiking activity are often explored, analyzed and visualized with the help of latent variable or factor models. Such models are however ill-equipped to extract structure beyond shared, distributed aspects of firing activity across multiple cells. Here, we extend unstructured factor models by proposing a model that discovers subpopulations or groups of cells from the pool of recorded neurons. The model combines aspects of mixture of factor analyzer models for capturing clustering structure, and aspects of latent dynamical system models for capturing temporal dependencies. In the resulting model, we infer the subpopulations and the latent factors from data using variational inference and model parameters are estimated by Expectation Maximization (EM). We also address the crucial problem of initializing parameters for EM by extending a sparse subspace clustering algorithm to integer-valued spike count observations. We illustrate the merits of the proposed model by applying it to calcium-imaging data from spinal cord neurons, and we show that it uncovers meaningful clustering structure in the data.",
        "bibtex": "@inproceedings{NIPS2014_047f66ae,\n author = {Buesing, Lars and Machado, Timothy A. and Cunningham, John P. and Paninski, Liam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustered factor analysis of multineuronal spike data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/047f66ae639d534aad092409f428e130-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/047f66ae639d534aad092409f428e130-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/047f66ae639d534aad092409f428e130-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/047f66ae639d534aad092409f428e130-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/047f66ae639d534aad092409f428e130-Reviews.html",
        "metareview": "",
        "pdf_size": 450600,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9084070362823390152&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistics, Center for Theoretical Neuroscience & Grossman Center for the Statistics of Mind; Department of Statistics, Center for Theoretical Neuroscience & Grossman Center for the Statistics of Mind + Howard Hughes Medical Institute & Department of Neuroscience; Department of Statistics, Center for Theoretical Neuroscience & Grossman Center for the Statistics of Mind; Department of Statistics, Center for Theoretical Neuroscience & Grossman Center for the Statistics of Mind",
        "aff_domain": "stat.columbia.edu;stat.columbia.edu;stat.columbia.edu;stat.columbia.edu",
        "email": "stat.columbia.edu;stat.columbia.edu;stat.columbia.edu;stat.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/047f66ae639d534aad092409f428e130-Abstract.html",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Center for Theoretical Neuroscience & Grossman Center for the Statistics of Mind;Howard Hughes Medical Institute",
        "aff_unique_dep": "Department of Statistics;Department of Neuroscience",
        "aff_unique_url": ";https://www.hhmi.org",
        "aff_unique_abbr": ";HHMI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Clustering from Labels and Time-Varying Graphs",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4370",
        "id": "4370",
        "author_site": "Shiau Hong Lim, Yudong Chen, Huan Xu",
        "author": "Shiau Hong Lim; Yudong Chen; Huan Xu",
        "abstract": "We present a general framework for graph clustering where a label is observed to each pair of nodes. This allows a very rich encoding of various types of pairwise interactions between nodes. We propose a new tractable approach to this problem based on maximum likelihood estimator and convex optimization. We analyze our algorithm under a general generative model, and provide both necessary and sufficient conditions for successful recovery of the underlying clusters. Our theoretical results cover and subsume a wide range of existing graph clustering results including planted partition, weighted clustering and partially observed graphs. Furthermore, the result is applicable to novel settings including time-varying graphs such that new insights can be gained on solving these problems. Our theoretical findings are further supported by empirical results on both synthetic and real data.",
        "bibtex": "@inproceedings{NIPS2014_9f3ad4f0,\n author = {Lim, Shiau Hong and Chen, Yudong and Xu, Huan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering from Labels and Time-Varying Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9f3ad4f0086e39d6e0669292affc30f8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9f3ad4f0086e39d6e0669292affc30f8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/9f3ad4f0086e39d6e0669292affc30f8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9f3ad4f0086e39d6e0669292affc30f8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9f3ad4f0086e39d6e0669292affc30f8-Reviews.html",
        "metareview": "",
        "pdf_size": 355675,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17208747704368529503&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "National University of Singapore; EECS, University of California, Berkeley; National University of Singapore",
        "aff_domain": "nus.edu.sg;eecs.berkeley.edu;nus.edu.sg",
        "email": "nus.edu.sg;eecs.berkeley.edu;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9f3ad4f0086e39d6e0669292affc30f8-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "National University of Singapore;University of California, Berkeley",
        "aff_unique_dep": ";EECS",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.berkeley.edu",
        "aff_unique_abbr": "NUS;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "title": "Combinatorial Pure Exploration of Multi-Armed Bandits",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4328",
        "id": "4328",
        "author_site": "Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, Wei Chen",
        "author": "Shouyuan Chen; Tian Lin; Irwin King; Michael R. Lyu; Wei Chen",
        "abstract": "We study the {\\em combinatorial pure exploration (CPE)} problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a \\emph{decision class}, which is a collection of subsets of arms with certain combinatorial structures such as size-$K$ subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a non-trivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-$K$ arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.",
        "bibtex": "@inproceedings{NIPS2014_d3ea0f33,\n author = {Chen, Shouyuan and Lin, Tian and King, Irwin and Lyu, Michael R. and Chen, Wei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Combinatorial Pure Exploration of Multi-Armed Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d3ea0f3316d2da934d79b8b344eafee4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d3ea0f3316d2da934d79b8b344eafee4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d3ea0f3316d2da934d79b8b344eafee4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d3ea0f3316d2da934d79b8b344eafee4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d3ea0f3316d2da934d79b8b344eafee4-Reviews.html",
        "metareview": "",
        "pdf_size": 568809,
        "gs_citation": 265,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10292626394149260341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "The Chinese University of Hong Kong; Tsinghua University; The Chinese University of Hong Kong; The Chinese University of Hong Kong; Microsoft Research Asia",
        "aff_domain": "cse.cuhk.edu.hk;mails.tsinghua.edu.cn;cse.cuhk.edu.hk;cse.cuhk.edu.hk;microsoft.com",
        "email": "cse.cuhk.edu.hk;mails.tsinghua.edu.cn;cse.cuhk.edu.hk;cse.cuhk.edu.hk;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d3ea0f3316d2da934d79b8b344eafee4-Abstract.html",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "Chinese University of Hong Kong;Tsinghua University;Microsoft",
        "aff_unique_dep": ";;Research",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "CUHK;THU;MSR Asia",
        "aff_campus_unique_index": "0;0;0;2",
        "aff_campus_unique": "Hong Kong SAR;;Asia",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Communication Efficient Distributed Machine Learning with the Parameter Server",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4528",
        "id": "4528",
        "author_site": "Mu Li, David G Andersen, Alexander Smola, Kai Yu",
        "author": "Mu Li; David G. Andersen; Alexander Smola; Kai Yu",
        "abstract": "This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from $\\ell_1$-regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.",
        "bibtex": "@inproceedings{NIPS2014_935ad074,\n author = {Li, Mu and Andersen, David G. and Smola, Alexander and Yu, Kai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Communication Efficient Distributed Machine Learning with the Parameter Server},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/935ad074f32d1e8f085a143449894cdc-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/935ad074f32d1e8f085a143449894cdc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/935ad074f32d1e8f085a143449894cdc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/935ad074f32d1e8f085a143449894cdc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/935ad074f32d1e8f085a143449894cdc-Reviews.html",
        "metareview": "",
        "pdf_size": 415496,
        "gs_citation": 2893,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8613900718926993656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 37,
        "aff": "Carnegie Mellon University\u2020Baidu; Carnegie Mellon University; Carnegie Mellon University\u2021Google; Baidu",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;smola.org;baidu.com",
        "email": "cs.cmu.edu;cs.cmu.edu;smola.org;baidu.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/935ad074f32d1e8f085a143449894cdc-Abstract.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Baidu",
        "aff_unique_dep": ";Baidu, Inc.",
        "aff_unique_url": "https://www.cmu.edu;https://www.baidu.com",
        "aff_unique_abbr": "CMU;Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Communication-Efficient Distributed Dual Coordinate Ascent",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4529",
        "id": "4529",
        "author_site": "Martin Jaggi, Virginia Smith, Martin Takac, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, Michael Jordan",
        "author": "Martin Jaggi; Virginia Smith; Martin Tak\u00e1\u010d; Jonathan Terhorst; Sanjay Krishnan; Thomas Hofmann; Michael I. Jordan",
        "abstract": "Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, COCOA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, COCOA converges to the same .001-accurate solution quality on average 25\u00d7 as quickly.",
        "bibtex": "@inproceedings{NIPS2014_44233fe7,\n author = {Jaggi, Martin and Smith, Virginia and Tak\\'{a}\\v{c}, Martin and Terhorst, Jonathan and Krishnan, Sanjay and Hofmann, Thomas and Jordan, Michael I.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Communication-Efficient Distributed Dual Coordinate Ascent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/44233fe752d33bb5c9adfc8b288e2ba3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/44233fe752d33bb5c9adfc8b288e2ba3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/44233fe752d33bb5c9adfc8b288e2ba3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/44233fe752d33bb5c9adfc8b288e2ba3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/44233fe752d33bb5c9adfc8b288e2ba3-Reviews.html",
        "metareview": "",
        "pdf_size": 374244,
        "gs_citation": 435,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3477259198051778921&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "ETH Zurich+UC Berkeley; UC Berkeley; Lehigh University; UC Berkeley; UC Berkeley; ETH Zurich; UC Berkeley",
        "aff_domain": "ethz.ch;berkeley.edu;lehigh.edu;berkeley.edu;berkeley.edu;ethz.ch;berkeley.edu",
        "email": "ethz.ch;berkeley.edu;lehigh.edu;berkeley.edu;berkeley.edu;ethz.ch;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/44233fe752d33bb5c9adfc8b288e2ba3-Abstract.html",
        "aff_unique_index": "0+1;1;2;1;1;0;1",
        "aff_unique_norm": "ETH Zurich;University of California, Berkeley;Lehigh University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ethz.ch;https://www.berkeley.edu;https://www.lehigh.edu",
        "aff_unique_abbr": "ETHZ;UC Berkeley;Lehigh",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+1;1;1;1;1;0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Compressive Sensing of Signals from a GMM with Sparse Precision Matrices",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4530",
        "id": "4530",
        "author_site": "Jianbo Yang, Xuejun Liao, Minhua Chen, Lawrence Carin",
        "author": "Jianbo Yang; Xuejun Liao; Minhua Chen; Lawrence Carin",
        "abstract": "This paper is concerned with compressive sensing of signals drawn from a Gaussian mixture model (GMM) with sparse precision matrices. Previous work has shown: (i) a signal drawn from a given GMM can be perfectly reconstructed from r noise-free measurements if the (dominant) rank of each covariance matrix is less than r; (ii) a sparse Gaussian graphical model can be efficiently estimated from fully-observed training signals using graphical lasso. This paper addresses a problem more challenging than both (i) and (ii), by assuming that the GMM is unknown and each signal is only partially observed through incomplete linear measurements. Under these challenging assumptions, we develop a hierarchical Bayesian method to simultaneously estimate the GMM and recover the signals using solely the incomplete measurements and a Bayesian shrinkage prior that promotes sparsity of the Gaussian precision matrices. In addition, we provide theoretical performance bounds to relate the reconstruction error to the number of signals for which measurements are available, the sparsity level of precision matrices, and the \u201cincompleteness\u201d of measurements. The proposed method is demonstrated extensively on compressive sensing of imagery and video, and the results with simulated and hardware-acquired real measurements show significant performance improvement over state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2014_15c8caab,\n author = {Yang, Jianbo and Liao, Xuejun and Chen, Minhua and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Compressive Sensing of Signals from a GMM with Sparse Precision Matrices},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/15c8caab99e6e6bed7418464beaf41a5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/15c8caab99e6e6bed7418464beaf41a5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/15c8caab99e6e6bed7418464beaf41a5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/15c8caab99e6e6bed7418464beaf41a5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/15c8caab99e6e6bed7418464beaf41a5-Reviews.html",
        "metareview": "",
        "pdf_size": 1347625,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6343651170391185273&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical and Computer Engineering, Duke University; Department of Statistics & Department of Computer Science, University of Chicago; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University",
        "aff_domain": "duke.edu;duke.edu;duke.edu;gmail.com",
        "email": "duke.edu;duke.edu;duke.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/15c8caab99e6e6bed7418464beaf41a5-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Duke University;University of Chicago",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Statistics",
        "aff_unique_url": "https://www.duke.edu;https://www.uchicago.edu",
        "aff_unique_abbr": "Duke;UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Computing Nash Equilibria in Generalized Interdependent Security Games",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4531",
        "id": "4531",
        "author_site": "Hau Chan, Luis E Ortiz",
        "author": "Hau Chan; Luis E. Ortiz",
        "abstract": "We study the computational complexity of computing Nash equilibria in generalized interdependent-security (IDS) games. Like traditional IDS games, originally introduced by economists and risk-assessment experts Heal and Kunreuther about a decade ago, generalized IDS games model agents\u2019 voluntary investment decisions when facing potential direct risk and transfer risk exposure from other agents. A distinct feature of generalized IDS games, however, is that full investment can reduce transfer risk. As a result, depending on the transfer-risk reduction level, generalized IDS games may exhibit strategic complementarity (SC) or strategic substitutability (SS). We consider three variants of generalized IDS games in which players exhibit only SC, only SS, and both SC+SS. We show that determining whether there is a pure-strategy Nash equilibrium (PSNE) in SC+SS-type games is NP-complete, while computing a single PSNE in SC-type games takes worst-case polynomial time. As for the problem of computing all mixed-strategy Nash equilibria (MSNE) efficiently, we produce a partial characterization. Whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that Kearns and Ortiz originally studied in the context of traditional IDS games in their NIPS 2003 paper, we can compute all MSNE that satisfy some ordering constraints in polynomial time in all three game variants. Yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the Pure-Nash-Extension problem, also originally introduced by Kearns and Ortiz, and that it is NP complete for all three variants. Finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized IDS games has on MSNE by solving several randomly-generated instances of SC+SS-type games with graph structures taken from several real-world datasets.",
        "bibtex": "@inproceedings{NIPS2014_8b27f75d,\n author = {Chan, Hau and Ortiz, Luis E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Computing Nash Equilibria in Generalized Interdependent Security Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8b27f75da1e993a693aeb50d1bd667a3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8b27f75da1e993a693aeb50d1bd667a3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8b27f75da1e993a693aeb50d1bd667a3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8b27f75da1e993a693aeb50d1bd667a3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8b27f75da1e993a693aeb50d1bd667a3-Reviews.html",
        "metareview": "",
        "pdf_size": 378504,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4547589330233980612&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University",
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8b27f75da1e993a693aeb50d1bd667a3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stony Brook",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Concavity of reweighted Kikuchi approximation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4532",
        "id": "4532",
        "author_site": "Po-Ling Loh, Andre Wibisono",
        "author": "Po-Ling Loh; Andre Wibisono",
        "abstract": "We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph. We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion, and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges. When the region graph has two layers, corresponding to a Bethe approximation, we show that our sufficient conditions for concavity are also necessary. Finally, we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph. We conclude with simulations that demonstrate the advantages of the reweighted Kikuchi approach.",
        "bibtex": "@inproceedings{NIPS2014_629e36c4,\n author = {Loh, Po-Ling and Wibisono, Andre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Concavity of reweighted Kikuchi approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/629e36c41dbcee2a804451dffb62882b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/629e36c41dbcee2a804451dffb62882b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/629e36c41dbcee2a804451dffb62882b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/629e36c41dbcee2a804451dffb62882b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/629e36c41dbcee2a804451dffb62882b-Reviews.html",
        "metareview": "",
        "pdf_size": 6071550,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2793919678434909184&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, The Wharton School, University of Pennsylvania; Computer Science Division, University of California, Berkeley",
        "aff_domain": "wharton.upenn.edu;berkeley.edu",
        "email": "wharton.upenn.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/629e36c41dbcee2a804451dffb62882b-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Pennsylvania;University of California, Berkeley",
        "aff_unique_dep": "Department of Statistics;Computer Science Division",
        "aff_unique_url": "https://www.upenn.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "UPenn;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4330",
        "id": "4330",
        "author_site": "Waleed Ammar, Chris Dyer, Noah A Smith",
        "author": "Waleed Ammar; Chris Dyer; Noah A. Smith",
        "abstract": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. Finally, we show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.",
        "bibtex": "@inproceedings{NIPS2014_b1d9874a,\n author = {Ammar, Waleed and Dyer, Chris and Smith, Noah A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Conditional Random Field Autoencoders for Unsupervised Structured Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b1d9874a479b1c6680db885011f95fd2-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b1d9874a479b1c6680db885011f95fd2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b1d9874a479b1c6680db885011f95fd2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b1d9874a479b1c6680db885011f95fd2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b1d9874a479b1c6680db885011f95fd2-Reviews.html",
        "metareview": "",
        "pdf_size": 569562,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16793416702785092828&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b1d9874a479b1c6680db885011f95fd2-Abstract.html"
    },
    {
        "title": "Conditional Swap Regret and Conditional Correlated Equilibrium",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4533",
        "id": "4533",
        "author_site": "Mehryar Mohri, Scott Yang",
        "author": "Mehryar Mohri; Scott Yang",
        "abstract": "We introduce a natural extension of the notion of swap regret, conditional swap regret, that allows for action modifications conditioned on the player\u2019s action history. We prove a series of new results for conditional swap regret minimization. We present algorithms for minimizing conditional swap regret with bounded conditioning history. We further extend these results to the case where conditional swaps are considered only for a subset of actions. We also define a new notion of equilibrium, conditional correlated equilibrium, that is tightly connected to the notion of conditional swap regret: when all players follow conditional swap regret minimization strategies, then the empirical distribution approaches this equilibrium. Finally, we extend our results to the multi-armed bandit scenario.",
        "bibtex": "@inproceedings{NIPS2014_0e0a0236,\n author = {Mohri, Mehryar and Yang, Scott},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Conditional Swap Regret and Conditional Correlated Equilibrium},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0e0a0236834aed19e133e651331210db-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0e0a0236834aed19e133e651331210db-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0e0a0236834aed19e133e651331210db-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0e0a0236834aed19e133e651331210db-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0e0a0236834aed19e133e651331210db-Reviews.html",
        "metareview": "",
        "pdf_size": 177872,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1432581474859648044&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Courant Institute and Google; Courant Institute",
        "aff_domain": "cims.nyu.edu;cims.nyu.edu",
        "email": "cims.nyu.edu;cims.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0e0a0236834aed19e133e651331210db-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Mathematical Sciences",
        "aff_unique_url": "https://courant.nyu.edu",
        "aff_unique_abbr": "Courant",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Cone-Constrained Principal Component Analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4534",
        "id": "4534",
        "author_site": "Yash Deshpande, Andrea Montanari, Emile Richard",
        "author": "Yash Deshpande; Andrea Montanari; Emile Richard",
        "abstract": "Estimating a vector from noisy quadratic observations is a task that arises naturally in many contexts, from dimensionality reduction, to synchronization and phase retrieval problems. It is often the case that additional information is available about the unknown vector (for instance, sparsity, sign or magnitude of its entries). Many authors propose non-convex quadratic optimization problems that aim at exploiting optimally this information. However, solving these problems is typically NP-hard. We consider a simple model for noisy quadratic observation of an unknown vector $\\bvz$. The unknown vector is constrained to belong to a cone $\\Cone \\ni \\bvz$. While optimal estimation appears to be intractable for the general problems in this class, we provide evidence that it is tractable when $\\Cone$ is a convex cone with an efficient projection. This is surprising, since the corresponding optimization problem is non-convex and --from a worst case perspective-- often NP hard. We characterize the resulting minimax risk in terms of the statistical dimension of the cone $\\delta(\\Cone)$. This quantity is already known to control the risk of estimation from gaussian observations and random linear measurements. It is rather surprising that the same quantity plays a role in the estimation risk from quadratic measurements.",
        "bibtex": "@inproceedings{NIPS2014_05a3e71d,\n author = {Deshpande, Yash and Montanari, Andrea and Richard, Emile},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cone-Constrained Principal Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/05a3e71d36f5c05318c0f70a6b7c485f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/05a3e71d36f5c05318c0f70a6b7c485f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/05a3e71d36f5c05318c0f70a6b7c485f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/05a3e71d36f5c05318c0f70a6b7c485f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/05a3e71d36f5c05318c0f70a6b7c485f-Reviews.html",
        "metareview": "",
        "pdf_size": 360405,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12892953387384424069&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/05a3e71d36f5c05318c0f70a6b7c485f-Abstract.html"
    },
    {
        "title": "Consistency of Spectral Partitioning of Uniform Hypergraphs under Planted Partition Model",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4535",
        "id": "4535",
        "author_site": "Debarghya Ghoshdastidar, Ambedkar Dukkipati",
        "author": "Debarghya Ghoshdastidar; Ambedkar Dukkipati",
        "abstract": "Spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science. Some notable studies have been carried out regarding the behavior of these methods for infinitely large sample size (von Luxburg et al., 2008; Rohe et al., 2011), which provide sufficient confidence to practitioners about the effectiveness of these methods. On the other hand, recent developments in computer vision have led to a plethora of applications, where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs. In this paper, we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting. We develop a planted partition model or stochastic blockmodel for such problems using higher order tensors, present a spectral technique suited for the purpose and study its large sample behavior. The analysis reveals that the algorithm is consistent for m-uniform hypergraphs for larger values of m, and also the rate of convergence improves for increasing m. Our result provides the first theoretical evidence that establishes the importance of m-way affinities.",
        "bibtex": "@inproceedings{NIPS2014_ca5fcdda,\n author = {Ghoshdastidar, Debarghya and Dukkipati, Ambedkar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Consistency of Spectral Partitioning of Uniform Hypergraphs under Planted Partition Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ca5fcddad0d403f10ee1c6dc9557ff8e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ca5fcddad0d403f10ee1c6dc9557ff8e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ca5fcddad0d403f10ee1c6dc9557ff8e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ca5fcddad0d403f10ee1c6dc9557ff8e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ca5fcddad0d403f10ee1c6dc9557ff8e-Reviews.html",
        "metareview": "",
        "pdf_size": 741463,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1098445197730943386&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science & Automation, Indian Institute of Science; Department of Computer Science & Automation, Indian Institute of Science",
        "aff_domain": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "email": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ca5fcddad0d403f10ee1c6dc9557ff8e-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science & Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Consistency of weighted majority votes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4536",
        "id": "4536",
        "author_site": "Daniel Berend, Aryeh Kontorovich",
        "author": "Daniel Berend; Aryeh Kontorovich",
        "abstract": "We revisit from a statistical learning perspective the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed. Experimental results are provided to illustrate the theory.",
        "bibtex": "@inproceedings{NIPS2014_3cf61323,\n author = {Berend, Daniel and Kontorovich, Aryeh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Consistency of weighted majority votes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3cf61323ed04cc4581f435dab1256ab8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3cf61323ed04cc4581f435dab1256ab8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3cf61323ed04cc4581f435dab1256ab8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3cf61323ed04cc4581f435dab1256ab8-Reviews.html",
        "metareview": "",
        "pdf_size": 231645,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5863446259515337257&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Department and Mathematics Department, Ben Gurion University, Beer Sheva, Israel; Computer Science Department, Ben Gurion University, Beer Sheva, Israel",
        "aff_domain": "cs.bgu.ac.il;cs.bgu.ac.il",
        "email": "cs.bgu.ac.il;cs.bgu.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3cf61323ed04cc4581f435dab1256ab8-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ben Gurion University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.bgu.ac.il",
        "aff_unique_abbr": "BGU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beer Sheva",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Consistent Binary Classification with Generalized Performance Metrics",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4372",
        "id": "4372",
        "author_site": "Sanmi Koyejo, Nagarajan Natarajan, Pradeep Ravikumar, Inderjit Dhillon",
        "author": "Oluwasanmi Koyejo; Nagarajan Natarajan; Pradeep Ravikumar; Inderjit S. Dhillon",
        "abstract": "Performance metrics for binary classification are designed to capture tradeoffs between four fundamental population quantities: true positives, false positives, true negatives and false negatives. Despite significant interest from theoretical and applied communities, little is known about either optimal classifiers or consistent algorithms for optimizing binary classification performance metrics beyond a few special cases. We consider a fairly large family of performance metrics given by ratios of linear combinations of the four fundamental population quantities. This family includes many well known binary classification metrics such as classification accuracy, AM measure, F-measure and the Jaccard similarity coefficient as special cases. Our analysis identifies the optimal classifiers as the sign of the thresholded conditional probability of the positive class, with a performance metric-dependent threshold. The optimal threshold can be constructed using simple plug-in estimators when the performance metric is a linear combination of the population quantities, but alternative techniques are required for the general case. We propose two algorithms for estimating the optimal classifiers, and prove their statistical consistency. Both algorithms are straightforward modifications of standard approaches to address the key challenge of optimal threshold selection, thus are simple to implement in practice. The first algorithm combines a plug-in estimate of the conditional probability of the positive class with optimal threshold selection. The second algorithm leverages recent work on calibrated asymmetric surrogate losses to construct candidate classifiers. We present empirical comparisons between these algorithms on benchmark datasets.",
        "bibtex": "@inproceedings{NIPS2014_98053046,\n author = {Koyejo, Oluwasanmi and Natarajan, Nagarajan and Ravikumar, Pradeep and Dhillon, Inderjit S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Consistent Binary Classification with Generalized Performance Metrics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/98053046e0dce5c7d946c67b96a85e18-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/98053046e0dce5c7d946c67b96a85e18-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/98053046e0dce5c7d946c67b96a85e18-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/98053046e0dce5c7d946c67b96a85e18-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/98053046e0dce5c7d946c67b96a85e18-Reviews.html",
        "metareview": "",
        "pdf_size": 529230,
        "gs_citation": 234,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2462642331858963676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Psychology, Stanford University; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "stanford.edu;cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "stanford.edu;cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/98053046e0dce5c7d946c67b96a85e18-Abstract.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Stanford University;University of Texas at Austin",
        "aff_unique_dep": "Department of Psychology;Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu;https://www.utexas.edu",
        "aff_unique_abbr": "Stanford;UT Austin",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Stanford;Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4537",
        "id": "4537",
        "author_site": "Ian En-Hsu Yen, Cho-Jui Hsieh, Pradeep Ravikumar, Inderjit Dhillon",
        "author": "Ian E.H. Yen; Cho-Jui Hsieh; Pradeep Ravikumar; Inderjit Dhillon",
        "abstract": "State of the art statistical estimators for high-dimensional problems take the form of regularized, and hence non-smooth, convex programs. A key facet of thesestatistical estimation problems is that these are typically not strongly convex under a high-dimensional sampling regime when the Hessian matrix becomes rank-deficient. Under vanilla convexity however, proximal optimization methods attain only a sublinear rate. In this paper, we investigate a novel variant of strong convexity, which we call Constant Nullspace Strong Convexity (CNSC), where we require that the objective function be strongly convex only over a constant subspace. As we show, the CNSC condition is naturally satisfied by high-dimensional statistical estimators. We then analyze the behavior of proximal methods under this CNSC condition: we show global linear convergence of Proximal Gradient and local quadratic convergence of Proximal Newton Method, when the regularization function comprising the statistical estimator is decomposable. We corroborate our theory via numerical experiments, and show a qualitative difference in the convergence rates of the proximal algorithms when the loss function does satisfy the CNSC condition.",
        "bibtex": "@inproceedings{NIPS2014_dd770720,\n author = {Yen, Ian E.H. and Hsieh, Cho-Jui and Ravikumar, Pradeep and Dhillon, Inderjit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/dd770720cb74c89878890c18c202a1e2-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/dd770720cb74c89878890c18c202a1e2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/dd770720cb74c89878890c18c202a1e2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/dd770720cb74c89878890c18c202a1e2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/dd770720cb74c89878890c18c202a1e2-Reviews.html",
        "metareview": "",
        "pdf_size": 140226,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15691709562342230475&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/dd770720cb74c89878890c18c202a1e2-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Constrained convex minimization via model-based excessive gap",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4538",
        "id": "4538",
        "author_site": "Quoc Tran-Dinh, Volkan Cevher",
        "author": "Quoc Tran-Dinh; Volkan Cevher",
        "abstract": "We introduce a model-based excessive gap technique to analyze first-order primal- dual methods for constrained convex minimization. As a result, we construct first- order primal-dual methods with optimal convergence rates on the primal objec- tive residual and the primal feasibility gap of their iterates separately. Through a dual smoothing and prox-center selection strategy, our framework subsumes the augmented Lagrangian, alternating direction, and dual fast-gradient methods as special cases, where our rates apply.",
        "bibtex": "@inproceedings{NIPS2014_757cb4f4,\n author = {Tran-Dinh, Quoc and Cevher, Volkan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Constrained convex minimization via model-based excessive gap},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/757cb4f450d81d06ffd14305aa20df13-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/757cb4f450d81d06ffd14305aa20df13-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/757cb4f450d81d06ffd14305aa20df13-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/757cb4f450d81d06ffd14305aa20df13-Reviews.html",
        "metareview": "",
        "pdf_size": 519609,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6997817328931774347&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Laboratory for Information and Inference Systems (LIONS) + \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL), CH1015-Lausanne, Switzerland; Laboratory for Information and Inference Systems (LIONS) + \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL), CH1015-Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/757cb4f450d81d06ffd14305aa20df13-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Laboratory for Information and Inference Systems;EPFL",
        "aff_unique_dep": "Information and Inference Systems;",
        "aff_unique_url": ";https://www.epfl.ch",
        "aff_unique_abbr": "LIONS;EPFL",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Lausanne",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";Switzerland"
    },
    {
        "title": "Content-based recommendations with Poisson factorization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4539",
        "id": "4539",
        "author_site": "Prem Gopalan, Laurent Charlin, David Blei",
        "author": "Prem Gopalan; Laurent Charlin; David M. Blei",
        "abstract": "We develop collaborative topic Poisson factorization (CTPF), a generative model of articles and reader preferences. CTPF can be used to build recommender systems by learning from reader histories and content to recommend personalized articles of interest. In detail, CTPF models both reader behavior and article texts with Poisson distributions, connecting the latent topics that represent the texts with the latent preferences that represent the readers. This provides better recommendations than competing methods and gives an interpretable latent space for understanding patterns of readership. Further, we exploit stochastic variational inference to model massive real-world datasets. For example, we can fit CPTF to the full arXiv usage dataset, which contains over 43 million ratings and 42 million word counts, within a day. We demonstrate empirically that our model outperforms several baselines, including the previous state-of-the-art approach.",
        "bibtex": "@inproceedings{NIPS2014_725a9842,\n author = {Gopalan, Prem and Charlin, Laurent and Blei, David M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Content-based recommendations with Poisson factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/725a9842a78cde3a681babf8e704f191-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/725a9842a78cde3a681babf8e704f191-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/725a9842a78cde3a681babf8e704f191-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/725a9842a78cde3a681babf8e704f191-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/725a9842a78cde3a681babf8e704f191-Reviews.html",
        "metareview": "",
        "pdf_size": 424811,
        "gs_citation": 238,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10556100607549153879&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Columbia University; Departments of Statistics & Computer Science, Columbia University",
        "aff_domain": "cs.princeton.edu;cs.columbia.edu;columbia.edu",
        "email": "cs.princeton.edu;cs.columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/725a9842a78cde3a681babf8e704f191-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Princeton University;Columbia University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu;https://www.columbia.edu",
        "aff_unique_abbr": "Princeton;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Controlling privacy in recommender systems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4540",
        "id": "4540",
        "author_site": "Yu Xin, Tommi Jaakkola",
        "author": "Yu Xin; Tommi Jaakkola",
        "abstract": "Recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences. In this paper, we explore a two-tiered notion of privacy where there is a small set of",
        "bibtex": "@inproceedings{NIPS2014_215a61e4,\n author = {Xin, Yu and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Controlling privacy in recommender systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/215a61e48cfa5a74fe875610b42e9991-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/215a61e48cfa5a74fe875610b42e9991-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/215a61e48cfa5a74fe875610b42e9991-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/215a61e48cfa5a74fe875610b42e9991-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/215a61e48cfa5a74fe875610b42e9991-Reviews.html",
        "metareview": "",
        "pdf_size": 252666,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=320871014436869123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "CSAIL, MIT; CSAIL, MIT",
        "aff_domain": "mit.edu;csail.mit.edu",
        "email": "mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/215a61e48cfa5a74fe875610b42e9991-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Convex Deep Learning via Normalized Kernels",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4541",
        "id": "4541",
        "author_site": "\u00d6zlem Aslan, Xinhua Zhang, Dale Schuurmans",
        "author": "\u00d6zlem Aslan; Xinhua Zhang; Dale Schuurmans",
        "abstract": "Deep learning has been a long standing pursuit in machine learning, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training. A complementary research strategy is to develop alternative modeling architectures that admit efficient training methods while expanding the range of representable structures toward deep models. In this paper, we develop a new architecture for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality. The approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer. The outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.",
        "bibtex": "@inproceedings{NIPS2014_2b434b7c,\n author = {Aslan, \\\"{O}zlem and Zhang, Xinhua and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convex Deep Learning via Normalized Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2b434b7c27c372d232dc6ba4c5402a09-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2b434b7c27c372d232dc6ba4c5402a09-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/2b434b7c27c372d232dc6ba4c5402a09-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2b434b7c27c372d232dc6ba4c5402a09-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2b434b7c27c372d232dc6ba4c5402a09-Reviews.html",
        "metareview": "",
        "pdf_size": 416684,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9954267721641559067&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept of Computing Science, University of Alberta, Canada; Machine Learning Group, NICTA and ANU; Dept of Computing Science, University of Alberta, Canada",
        "aff_domain": "cs.ualberta.ca;nicta.com.au;cs.ualberta.ca",
        "email": "cs.ualberta.ca;nicta.com.au;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2b434b7c27c372d232dc6ba4c5402a09-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Alberta;National Information and Communications Technology Australia",
        "aff_unique_dep": "Dept of Computing Science;Machine Learning Group",
        "aff_unique_url": "https://www.ualberta.ca;https://www.nicta.com.au",
        "aff_unique_abbr": "UAlberta;NICTA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;Australia"
    },
    {
        "title": "Convex Optimization Procedure for Clustering: Theoretical Revisit",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4542",
        "id": "4542",
        "author_site": "Changbo Zhu, Huan Xu, Chenlei Leng, Shuicheng Yan",
        "author": "Changbo Zhu; Huan Xu; Chenlei Leng; Shuicheng Yan",
        "abstract": "In this paper, we present theoretical analysis of SON~--~a convex optimization procedure for clustering using a sum-of-norms (SON) regularization recently proposed in \\cite{ICML2011Hocking_419,SON, Lindsten650707, pelckmans2005convex}. In particular, we show if the samples are drawn from two cubes, each being one cluster, then SON can provably identify the cluster membership provided that the distance between the two cubes is larger than a threshold which (linearly) depends on the size of the cube and the ratio of numbers of samples in each cluster. To the best of our knowledge, this paper is the first to provide a rigorous analysis to understand why and when SON works. We believe this may provide important insights to develop novel convex optimization based algorithms for clustering.",
        "bibtex": "@inproceedings{NIPS2014_3c9d14ca,\n author = {Zhu, Changbo and Xu, Huan and Leng, Chenlei and Yan, Shuicheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convex Optimization Procedure for Clustering: Theoretical Revisit},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3c9d14ca7be84f921b2dd647c09aa1bf-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3c9d14ca7be84f921b2dd647c09aa1bf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3c9d14ca7be84f921b2dd647c09aa1bf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3c9d14ca7be84f921b2dd647c09aa1bf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3c9d14ca7be84f921b2dd647c09aa1bf-Reviews.html",
        "metareview": "",
        "pdf_size": 309171,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16411482632049606496&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical and Computer Engineering+Department of Mathematics, National University of Singapore; Department of Mechanical Engineering, National University of Singapore; Department of Statistics, University of Warwick; Department of Electrical and Computer Engineering, National University of Singapore",
        "aff_domain": "nus.edu.sg;nus.edu.sg;warwick.ac.uk;nus.edu.sg",
        "email": "nus.edu.sg;nus.edu.sg;warwick.ac.uk;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3c9d14ca7be84f921b2dd647c09aa1bf-Abstract.html",
        "aff_unique_index": "0+1;1;2;1",
        "aff_unique_norm": "Unknown Institution;National University of Singapore;University of Warwick",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Mathematics;Department of Statistics",
        "aff_unique_url": ";https://www.nus.edu.sg;https://warwick.ac.uk",
        "aff_unique_abbr": ";NUS;Warwick",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;2;1",
        "aff_country_unique": ";Singapore;United Kingdom"
    },
    {
        "title": "Convolutional Kernel Networks",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4374",
        "id": "4374",
        "author_site": "Julien Mairal, Piotr Koniusz, Zaid Harchaoui, Cordelia Schmid",
        "author": "Julien Mairal; Piotr Koniusz; Zaid Harchaoui; Cordelia Schmid",
        "abstract": "An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.",
        "bibtex": "@inproceedings{NIPS2014_768fa80c,\n author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convolutional Kernel Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/768fa80ce4990fe601f5b2e094950511-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/768fa80ce4990fe601f5b2e094950511-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/768fa80ce4990fe601f5b2e094950511-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/768fa80ce4990fe601f5b2e094950511-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/768fa80ce4990fe601f5b2e094950511-Reviews.html",
        "metareview": "",
        "pdf_size": 224326,
        "gs_citation": 496,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11965513054905197591&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "Inria\u2217; Inria\u2217; Inria\u2217; Inria\u2217",
        "aff_domain": "inria.fr;inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/768fa80ce4990fe601f5b2e094950511-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4543",
        "id": "4543",
        "author_site": "Baotian Hu, Zhengdong Lu, Hang Li, Qingcai Chen",
        "author": "Baotian Hu; Zhengdong Lu; Hang Li; Qingcai Chen",
        "abstract": "Semantic matching is of central importance to many natural language tasks \\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.",
        "bibtex": "@inproceedings{NIPS2014_ab1010aa,\n author = {Hu, Baotian and Lu, Zhengdong and Li, Hang and Chen, Qingcai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convolutional Neural Network Architectures for Matching Natural Language Sentences},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ab1010aae16a7f29f02977d84c61b6cf-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ab1010aae16a7f29f02977d84c61b6cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ab1010aae16a7f29f02977d84c61b6cf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ab1010aae16a7f29f02977d84c61b6cf-Reviews.html",
        "metareview": "",
        "pdf_size": 464581,
        "gs_citation": 1735,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=212244750819910358&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science & Technology, Harbin Institute of Technology Shenzhen Graduate School, Xili, China + Noah\u2019s Ark Lab, Huawei Technologies Co. Ltd., Sha Tin, Hong Kong; Noah\u2019s Ark Lab, Huawei Technologies Co. Ltd., Sha Tin, Hong Kong; Noah\u2019s Ark Lab, Huawei Technologies Co. Ltd., Sha Tin, Hong Kong; Department of Computer Science & Technology, Harbin Institute of Technology Shenzhen Graduate School, Xili, China",
        "aff_domain": "gmail.com;huawei.com;huawei.com;hitsz.edu.cn",
        "email": "gmail.com;huawei.com;huawei.com;hitsz.edu.cn",
        "github": "",
        "project": "http://www.noahlab.com.hk/technology/Learning2Match.html",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ab1010aae16a7f29f02977d84c61b6cf-Abstract.html",
        "aff_unique_index": "0+1;1;1;0",
        "aff_unique_norm": "Harbin Institute of Technology;Huawei",
        "aff_unique_dep": "Department of Computer Science & Technology;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://en.hhit.edu.cn/;https://www.huawei.com",
        "aff_unique_abbr": "HIT;Huawei",
        "aff_campus_unique_index": "0+1;1;1;0",
        "aff_campus_unique": "Shenzhen;Hong Kong SAR",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Coresets for k-Segmentation of Streaming Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4544",
        "id": "4544",
        "author_site": "Guy Rosman, Mikhail Volkov, Dan Feldman, John Fisher III, Daniela Rus",
        "author": "Guy Rosman; Mikhail Volkov; Danny Feldman; John W. Fisher III; Daniela Rus",
        "abstract": "Life-logging video streams, financial time series, and Twitter tweets are a few examples of high-dimensional signals over practically unbounded time. We consider the problem of computing optimal segmentation of such signals by k-piecewise linear function, using only one pass over the data by maintaining a coreset for the signal. The coreset enables fast further analysis such as automatic summarization and analysis of such signals. A coreset (core-set) is a compact representation of the data seen so far, which approximates the data well for a specific task -- in our case, segmentation of the stream. We show that, perhaps surprisingly, the segmentation problem admits coresets of cardinality only linear in the number of segments k, independently of both the dimension d of the signal, and its number n of points. More precisely, we construct a representation of size O(klog n /eps^2) that provides a (1+eps)-approximation for the sum of squared distances to any given k-piecewise linear function. Moreover, such coresets can be constructed in a parallel streaming approach. Our results rely on a novel eduction of statistical estimations to problems in computational geometry. We empirically evaluate our algorithms on very large synthetic and real data sets from GPS, video and financial domains, using 255 machines in Amazon cloud.",
        "bibtex": "@inproceedings{NIPS2014_769da5fb,\n author = {Rosman, Guy and Volkov, Mikhail and Feldman, Danny and Fisher, John W. and Rus, Daniela},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Coresets for k-Segmentation of Streaming Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/769da5fb4e243639ece7b11c3abd0bef-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/769da5fb4e243639ece7b11c3abd0bef-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/769da5fb4e243639ece7b11c3abd0bef-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/769da5fb4e243639ece7b11c3abd0bef-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/769da5fb4e243639ece7b11c3abd0bef-Reviews.html",
        "metareview": "",
        "pdf_size": 506527,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5856273843095376850&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "CSAIL, MIT; CSAIL, MIT; CSAIL, MIT; CSAIL, MIT; CSAIL, MIT",
        "aff_domain": "csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/769da5fb4e243639ece7b11c3abd0bef-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Covariance shrinkage for autocorrelated data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4545",
        "id": "4545",
        "author_site": "Daniel Bartz, Klaus-Robert M\u00fcller",
        "author": "Daniel Bartz; Klaus-Robert M\u00fcller",
        "abstract": "The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.",
        "bibtex": "@inproceedings{NIPS2014_11459f04,\n author = {Bartz, Daniel and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Covariance shrinkage for autocorrelated data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/11459f04a46a9e348cdeee6986fcf5f2-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/11459f04a46a9e348cdeee6986fcf5f2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/11459f04a46a9e348cdeee6986fcf5f2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/11459f04a46a9e348cdeee6986fcf5f2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/11459f04a46a9e348cdeee6986fcf5f2-Reviews.html",
        "metareview": "",
        "pdf_size": 351402,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17819663216824136654&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "TU Berlin, Berlin, Germany; TU Berlin, Berlin, Germany + Korea University, Korea, Seoul",
        "aff_domain": "tu-berlin.de;tu-berlin.de",
        "email": "tu-berlin.de;tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/11459f04a46a9e348cdeee6986fcf5f2-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Technical University of Berlin;Korea University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.korea.ac.kr",
        "aff_unique_abbr": "TU Berlin;KU",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Berlin;Seoul",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "Germany;South Korea"
    },
    {
        "title": "DFacTo: Distributed Factorization of Tensors",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4546",
        "id": "4546",
        "author_site": "Joon Hee Choi, S. Vishwanathan",
        "author": "Joon Hee Choi; S. V. N. Vishwanathan",
        "abstract": "We present a technique for significantly speeding up Alternating Least Squares (ALS) and Gradient Descent (GD), two widely used algorithms for tensor factorization. By exploiting properties of the Khatri-Rao product, we show how to efficiently address a computationally challenging sub-step of both algorithms. Our algorithm, DFacTo, only requires two matrix-vector products and is easy to parallelize. DFacTo is not only scalable but also on average 4 to 10 times faster than competing algorithms on a variety of datasets. For instance, DFacTo only takes 480 seconds on 4 machines to perform one iteration of the ALS algorithm and 1,143 seconds to perform one iteration of the GD algorithm on a 6.5 million x 2.5 million x 1.5 million dimensional tensor with 1.2 billion non-zero entries.",
        "bibtex": "@inproceedings{NIPS2014_bd1fcb88,\n author = {Choi, Joon Hee and Vishwanathan, S. V. N.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {DFacTo: Distributed Factorization of Tensors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/bd1fcb886d5519677d73717abc269b90-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/bd1fcb886d5519677d73717abc269b90-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/bd1fcb886d5519677d73717abc269b90-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/bd1fcb886d5519677d73717abc269b90-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/bd1fcb886d5519677d73717abc269b90-Reviews.html",
        "metareview": "",
        "pdf_size": 276687,
        "gs_citation": 204,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=298895191845859389&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Electrical and Computer Engineering, Purdue University; Statistics and Computer Science, Purdue University",
        "aff_domain": "purdue.edu;stat.purdue.edu",
        "email": "purdue.edu;stat.purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/bd1fcb886d5519677d73717abc269b90-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Electrical and Computer Engineering",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Decomposing Parameter Estimation Problems",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4547",
        "id": "4547",
        "author_site": "Khaled Refaat, Arthur Choi, Adnan Darwiche",
        "author": "Khaled S. Refaat; Arthur Choi; Adnan Darwiche",
        "abstract": "We propose a technique for decomposing the parameter learning problem in Bayesian networks into independent learning problems. Our technique applies to incomplete datasets and exploits variables that are either hidden or observed in the given dataset. We show empirically that the proposed technique can lead to orders-of-magnitude savings in learning time. We explain, analytically and empirically, the reasons behind our reported savings, and compare the proposed technique to related ones that are sometimes used by inference algorithms.",
        "bibtex": "@inproceedings{NIPS2014_e34920af,\n author = {Refaat, Khaled S. and Choi, Arthur and Darwiche, Adnan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decomposing Parameter Estimation Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e34920aff3b2c15c646568c5f7a9a761-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e34920aff3b2c15c646568c5f7a9a761-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e34920aff3b2c15c646568c5f7a9a761-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e34920aff3b2c15c646568c5f7a9a761-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e34920aff3b2c15c646568c5f7a9a761-Reviews.html",
        "metareview": "",
        "pdf_size": 436753,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1428993929716216055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e34920aff3b2c15c646568c5f7a9a761-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deconvolution of High Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4548",
        "id": "4548",
        "author_site": "Charles Y Zheng, Franco Pestilli, Ariel Rokem",
        "author": "Charles Y. Zheng; Franco Pestilli; Ariel Rokem",
        "abstract": "Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost, together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI). We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.",
        "bibtex": "@inproceedings{NIPS2014_2b0524a3,\n author = {Zheng, Charles Y. and Pestilli, Franco and Rokem, Ariel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deconvolution of High Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2b0524a3000678a1f66bf38d546c8fd8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2b0524a3000678a1f66bf38d546c8fd8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/2b0524a3000678a1f66bf38d546c8fd8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2b0524a3000678a1f66bf38d546c8fd8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2b0524a3000678a1f66bf38d546c8fd8-Reviews.html",
        "metareview": "",
        "pdf_size": 8927646,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10944484208693431350&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Statistics, Stanford University; Department of Psychological and Brain Sciences, Indiana University; Department of Psychology, Stanford University",
        "aff_domain": "stanford.edu;indiana.edu;stanford.edu",
        "email": "stanford.edu;indiana.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2b0524a3000678a1f66bf38d546c8fd8-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;Indiana University",
        "aff_unique_dep": "Department of Statistics;Department of Psychological and Brain Sciences",
        "aff_unique_url": "https://www.stanford.edu;https://www.indiana.edu",
        "aff_unique_abbr": "Stanford;IU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Decoupled Variational Gaussian Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4549",
        "id": "4549",
        "author": "Mohammad Emtiyaz Khan",
        "abstract": "Variational Gaussian (VG) inference methods that optimize a lower bound to the marginal likelihood are a popular approach for Bayesian inference. These methods are fast and easy to use, while being reasonably accurate. A difficulty remains in computation of the lower bound when the latent dimensionality $L$ is large. Even though the lower bound is concave for many models, its computation requires optimization over $O(L^2)$ variational parameters. Efficient reparameterization schemes can reduce the number of parameters, but give inaccurate solutions or destroy concavity leading to slow convergence. We propose decoupled variational inference that brings the best of both worlds together. First, it maximizes a Lagrangian of the lower bound reducing the number of parameters to $O(N)$, where $N$ is the number of data examples. The reparameterization obtained is unique and recovers maxima of the lower-bound even when the bound is not concave. Second, our method maximizes the lower bound using a sequence of convex problems, each of which is parallellizable over data examples and computes gradient efficiently. Overall, our approach avoids all direct computations of the covariance, only requiring its linear projections. Theoretically, our method converges at the same rate as existing methods in the case of concave lower bounds, while remaining convergent at a reasonable rate for the non-concave case.",
        "bibtex": "@inproceedings{NIPS2014_34d5bca0,\n author = {Khan, Mohammad Emtiyaz},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decoupled Variational Gaussian Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/34d5bca0f6c6d2e9962c84f5bddc3468-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/34d5bca0f6c6d2e9962c84f5bddc3468-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/34d5bca0f6c6d2e9962c84f5bddc3468-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/34d5bca0f6c6d2e9962c84f5bddc3468-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/34d5bca0f6c6d2e9962c84f5bddc3468-Reviews.html",
        "metareview": "",
        "pdf_size": 277874,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17811130353778029713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Ecole Polytechnique F\u00e9drale de Lausanne (EPFL), Switzerland",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/34d5bca0f6c6d2e9962c84f5bddc3468-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ecole Polytechnique F\u00e9drale de Lausanne",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Deep Convolutional Neural Network for Image Deconvolution",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4758",
        "id": "4758",
        "author_site": "Li Xu, Jimmy S. Ren, Ce Liu, Jiaya Jia",
        "author": "Li Xu; Jimmy SJ. Ren; Ce Liu; Jiaya Jia",
        "abstract": "Many fundamental image-related problems involve deconvolution operators. Real blur degradation seldom complies with an deal linear convolution model due to camera noise, saturation, image compression, to name a few. Instead of perfectly modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics of degradation. We note directly applying existing deep neural networks does not produce reasonable results. Our solution is to establish the connection between traditional optimization-based schemes and a neural network architecture where a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts. Our network contains two submodules, both trained in a supervised manner with proper initialization. They yield decent performance on non-blind image deconvolution compared to previous generative-model based methods.",
        "bibtex": "@inproceedings{NIPS2014_37f8ddca,\n author = {Xu, Li and Ren, Jimmy SJ. and Liu, Ce and Jia, Jiaya},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Convolutional Neural Network for Image Deconvolution},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/37f8ddca0e675015440e5ff536c8fa83-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/37f8ddca0e675015440e5ff536c8fa83-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/37f8ddca0e675015440e5ff536c8fa83-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/37f8ddca0e675015440e5ff536c8fa83-Reviews.html",
        "metareview": "",
        "pdf_size": 5597648,
        "gs_citation": 1319,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2895078575192815499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Lenovo Research & Technology; Lenovo Research & Technology; Microsoft Research; The Chinese University of Hong Kong",
        "aff_domain": "lenovo.com;gmail.com;microsoft.com;cse.cuhk.edu.hk",
        "email": "lenovo.com;gmail.com;microsoft.com;cse.cuhk.edu.hk",
        "github": "",
        "project": "http://www.lxu.me/projects/dcnn/",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/37f8ddca0e675015440e5ff536c8fa83-Abstract.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Lenovo;Microsoft;Chinese University of Hong Kong",
        "aff_unique_dep": "Research & Technology;Microsoft Research;",
        "aff_unique_url": "https://www.lenovo.com;https://www.microsoft.com/en-us/research;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "Lenovo;MSR;CUHK",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4550",
        "id": "4550",
        "author_site": "Andrej Karpathy, Armand Joulin, Li Fei-Fei",
        "author": "Andrej Karpathy; Armand Joulin; Li Fei-Fei",
        "abstract": "We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.",
        "bibtex": "@inproceedings{NIPS2014_fe316643,\n author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Fragment Embeddings for Bidirectional Image Sentence Mapping},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/fe31664311a4a44314c89321386a20f6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/fe31664311a4a44314c89321386a20f6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/fe31664311a4a44314c89321386a20f6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/fe31664311a4a44314c89321386a20f6-Reviews.html",
        "metareview": "",
        "pdf_size": 5542666,
        "gs_citation": 1146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17268489282982414918&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Stanford University, Stanford, CA 94305, USA; Department of Computer Science, Stanford University, Stanford, CA 94305, USA; Department of Computer Science, Stanford University, Stanford, CA 94305, USA",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/fe31664311a4a44314c89321386a20f6-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Joint Task Learning for Generic Object Extraction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4635",
        "id": "4635",
        "author_site": "Xiaolong Wang, Liliang Zhang, Liang Lin, Zhujin Liang, Wangmeng Zuo",
        "author": "Xiaolong Wang; Liliang Zhang; Liang Lin; Zhujin Liang; Wangmeng Zuo",
        "abstract": "This paper investigates how to extract objects-of-interest without relying on hand-craft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations. We present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance. In particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner. The first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks. An EM-type method is then studied for the joint optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an MCMC-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables. Extensive experiments demonstrate that our joint learning framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g., 1000 times faster than competing approaches).",
        "bibtex": "@inproceedings{NIPS2014_3a71f537,\n author = {Wang, Xiaolong and Zhang, Liliang and Lin, Liang and Liang, Zhujin and Zuo, Wangmeng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Joint Task Learning for Generic Object Extraction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3a71f5372dbc341c48a65df7e1efb831-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3a71f5372dbc341c48a65df7e1efb831-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3a71f5372dbc341c48a65df7e1efb831-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3a71f5372dbc341c48a65df7e1efb831-Reviews.html",
        "metareview": "",
        "pdf_size": 650635,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6260299058855383181&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Sun Yat-sen University; Sun Yat-sen University; Sun Yat-sen University + SYSU-CMU Shunde International Joint Research Institute; Sun Yat-sen University; School of Computer Science and Technology, Harbin Institute of Technology",
        "aff_domain": "cmu.edu; ;ieee.org; ; ",
        "email": "cmu.edu; ;ieee.org; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3a71f5372dbc341c48a65df7e1efb831-Abstract.html",
        "aff_unique_index": "0;0;0+1;0;2",
        "aff_unique_norm": "Sun Yat-sen University;SYSU-CMU Shunde International Joint Research Institute;Harbin Institute of Technology",
        "aff_unique_dep": ";;School of Computer Science and Technology",
        "aff_unique_url": "http://www.sysu.edu.cn/;;http://www.hit.edu.cn/",
        "aff_unique_abbr": "SYSU;;HIT",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Shunde;Harbin",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Learning Face Representation by Joint Identification-Verification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4577",
        "id": "4577",
        "author_site": "Yi Sun, Yuheng Chen, Xiaogang Wang, Xiaoou Tang",
        "author": "Yi Sun; Yuheng Chen; Xiaogang Wang; Xiaoou Tang",
        "abstract": "The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result on LFW, the error rate has been significantly reduced by 67%.",
        "bibtex": "@inproceedings{NIPS2014_2f9d6452,\n author = {Sun, Yi and Chen, Yuheng and Wang, Xiaogang and Tang, Xiaoou},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Learning Face Representation by Joint Identification-Verification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2f9d64528ced0ea456b16aa7268f3463-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2f9d64528ced0ea456b16aa7268f3463-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2f9d64528ced0ea456b16aa7268f3463-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2f9d64528ced0ea456b16aa7268f3463-Reviews.html",
        "metareview": "",
        "pdf_size": 2201128,
        "gs_citation": 2970,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2652536617023372962&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong; SenseTime Group; Department of Electronic Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "aff_domain": "ie.cuhk.edu.hk;gmail.com;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;gmail.com;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2f9d64528ced0ea456b16aa7268f3463-Abstract.html",
        "aff_unique_index": "0;1;0+2;0+2",
        "aff_unique_norm": "Chinese University of Hong Kong;SenseTime Group;Chinese Academy of Sciences",
        "aff_unique_dep": "Department of Information Engineering;;Shenzhen Institutes of Advanced Technology",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.sensetime.com;http://www.siat.cas.cn",
        "aff_unique_abbr": "CUHK;SenseTime;SIAT",
        "aff_campus_unique_index": "0;0+2;0+2",
        "aff_campus_unique": "Hong Kong SAR;;Shenzhen",
        "aff_country_unique_index": "0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4552",
        "id": "4552",
        "author_site": "Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, Xiaoshi Wang",
        "author": "Xiaoxiao Guo; Satinder Singh; Honglak Lee; Richard Lewis; Xiaoshi Wang",
        "abstract": "The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.",
        "bibtex": "@inproceedings{NIPS2014_88bf0c64,\n author = {Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard and Wang, Xiaoshi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/88bf0c64edabeeb913c378227beef8f9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/88bf0c64edabeeb913c378227beef8f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/88bf0c64edabeeb913c378227beef8f9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/88bf0c64edabeeb913c378227beef8f9-Reviews.html",
        "metareview": "",
        "pdf_size": 494615,
        "gs_citation": 474,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=287674005991514919&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computer Science and Eng. University of Michigan; Computer Science and Eng. University of Michigan; Computer Science and Eng. University of Michigan; Department of Psychology University of Michigan; Computer Science and Eng. University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/88bf0c64edabeeb913c378227beef8f9-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Networks with Internal Selective Attention through Feedback Connections",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4553",
        "id": "4553",
        "author_site": "Marijn F Stollenga, Jonathan Masci, Faustino Gomez, J\u00fcrgen Schmidhuber",
        "author": "Marijn F. Stollenga; Jonathan Masci; Faustino Gomez; Juergen Schmidhuber",
        "abstract": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet's feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.",
        "bibtex": "@inproceedings{NIPS2014_2161abe7,\n author = {Stollenga, Marijn F. and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Juergen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Networks with Internal Selective Attention through Feedback Connections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2161abe764d3d61f4d3da5fdbed84297-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2161abe764d3d61f4d3da5fdbed84297-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/2161abe764d3d61f4d3da5fdbed84297-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2161abe764d3d61f4d3da5fdbed84297-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2161abe764d3d61f4d3da5fdbed84297-Reviews.html",
        "metareview": "",
        "pdf_size": 943167,
        "gs_citation": 341,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6502206061321814323&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "IDSIA, USI-SUPSI; IDSIA, USI-SUPSI; IDSIA, USI-SUPSI; IDSIA, USI-SUPSI",
        "aff_domain": "idsia.ch;idsia.ch;idsia.ch;idsia.ch",
        "email": "idsia.ch;idsia.ch;idsia.ch;idsia.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2161abe764d3d61f4d3da5fdbed84297-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IDSIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idsia.ch",
        "aff_unique_abbr": "IDSIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Deep Recursive Neural Networks for Compositionality in Language",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4554",
        "id": "4554",
        "author_site": "Ozan Irsoy, Claire Cardie",
        "author": "Ozan \u0130rsoy; Claire Cardie",
        "abstract": "Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture --- a deep recursive neural network (deep RNN) --- constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.",
        "bibtex": "@inproceedings{NIPS2014_cb954cd5,\n author = {\\.{I}rsoy, Ozan and Cardie, Claire},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Recursive Neural Networks for Compositionality in Language},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cb954cd53b6527a80248cc0732366871-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cb954cd53b6527a80248cc0732366871-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cb954cd53b6527a80248cc0732366871-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cb954cd53b6527a80248cc0732366871-Reviews.html",
        "metareview": "",
        "pdf_size": 232593,
        "gs_citation": 410,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14795161263853850797&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Cornell University; Department of Computer Science, Cornell University",
        "aff_domain": "cs.cornell.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cb954cd53b6527a80248cc0732366871-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Symmetry Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4555",
        "id": "4555",
        "author_site": "Robert Gens, Pedro Domingos",
        "author": "Robert Gens; Pedro Domingos",
        "abstract": "The chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation. These sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups. As a result, these groups' effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity. In this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets, they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on NORB and MNIST-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.",
        "bibtex": "@inproceedings{NIPS2014_4168b833,\n author = {Gens, Robert and Domingos, Pedro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Symmetry Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4168b8336761775af2637fe15605a372-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4168b8336761775af2637fe15605a372-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4168b8336761775af2637fe15605a372-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4168b8336761775af2637fe15605a372-Reviews.html",
        "metareview": "",
        "pdf_size": 1336218,
        "gs_citation": 333,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4820202775829919990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4168b8336761775af2637fe15605a372-Abstract.html"
    },
    {
        "title": "Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4556",
        "id": "4556",
        "author_site": "Brendan McMahan, Matthew Streeter",
        "author": "H. Brendan McMahan; Matthew Streeter",
        "abstract": "We analyze new online gradient descent algorithms for distributed systems with large delays between gradient computations and the corresponding updates. Using insights from adaptive gradient methods, we develop algorithms that adapt not only to the sequence of gradients, but also to the precise update delays that occur. We first give an impractical algorithm that achieves a regret bound that precisely quantifies the impact of the delays. We then analyze AdaptiveRevision, an algorithm that is efficiently implementable and achieves comparable guarantees. The key algorithmic technique is appropriately and efficiently revising the learning rate used for previous gradient steps. Experimental results show when the delays grow large (1000 updates or more), our new algorithms perform significantly better than standard adaptive gradient methods.",
        "bibtex": "@inproceedings{NIPS2014_0f41d814,\n author = {McMahan, H. Brendan and Streeter, Matthew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0f41d814a243c98c672bdbfabaa40f5e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0f41d814a243c98c672bdbfabaa40f5e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0f41d814a243c98c672bdbfabaa40f5e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0f41d814a243c98c672bdbfabaa40f5e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0f41d814a243c98c672bdbfabaa40f5e-Reviews.html",
        "metareview": "",
        "pdf_size": 782003,
        "gs_citation": 168,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17545397603546539677&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Google, Inc. Seattle, WA; Duolingo, Inc. Pittsburgh, PA + Google, Inc.",
        "aff_domain": "google.com;duolingo.com",
        "email": "google.com;duolingo.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0f41d814a243c98c672bdbfabaa40f5e-Abstract.html",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "Google;Duolingo, Inc.",
        "aff_unique_dep": "Google, Inc.;",
        "aff_unique_url": "https://www.google.com;https://www.duolingo.com",
        "aff_unique_abbr": "Google;Duolingo",
        "aff_campus_unique_index": "0;1+2",
        "aff_campus_unique": "Seattle;Pittsburgh;Mountain View",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Dependent nonparametric trees for dynamic hierarchical clustering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4557",
        "id": "4557",
        "author_site": "Kumar Avinava Dubey, Qirong Ho, Sinead Williamson, Eric Xing",
        "author": "Avinava Dubey; Qirong Ho; Sinead Williamson; Eric P. Xing",
        "abstract": "Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets. However, the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time: We expect both the structure of our hierarchy, and the parameters of the clusters, to evolve with time. In this paper, we present a distribution over collections of time-dependent, infinite-dimensional trees that can be used to model evolving hierarchies, and present an efficient and scalable algorithm for performing approximate inference in such a model. We demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora.",
        "bibtex": "@inproceedings{NIPS2014_096e2c25,\n author = {Dubey, Avinava and Ho, Qirong and Williamson, Sinead and Xing, Eric P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dependent nonparametric trees for dynamic hierarchical clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/096e2c25cfb42668e439dfc0162b2520-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/096e2c25cfb42668e439dfc0162b2520-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/096e2c25cfb42668e439dfc0162b2520-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/096e2c25cfb42668e439dfc0162b2520-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/096e2c25cfb42668e439dfc0162b2520-Reviews.html",
        "metareview": "",
        "pdf_size": 2062262,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8510804633733311776&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Machine Learning Department, Carnegie Mellon University\u2020; Institute for Infocomm Research, A*STAR\u2021; McCombs School of Business, University of Texas at Austin\u00a3; Machine Learning Department, Carnegie Mellon University\u2020",
        "aff_domain": "cs.cmu.edu;gmail.com;mccombs.utexas.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;gmail.com;mccombs.utexas.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/096e2c25cfb42668e439dfc0162b2520-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Carnegie Mellon University;Institute for Infocomm Research;University of Texas at Austin",
        "aff_unique_dep": "Machine Learning Department;;McCombs School of Business",
        "aff_unique_url": "https://www.cmu.edu;https://www.i2r.a-star.edu.sg;https://www.mccombs.utexas.edu",
        "aff_unique_abbr": "CMU;I2R;UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4376",
        "id": "4376",
        "author_site": "David Eigen, Christian Puhrsch, Rob Fergus",
        "author": "David Eigen; Christian Puhrsch; Rob Fergus",
        "abstract": "Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.",
        "bibtex": "@inproceedings{NIPS2014_91c56ce4,\n author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Depth Map Prediction from a Single Image using a Multi-Scale Deep Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/91c56ce4a249fae5419b90cba831e303-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/91c56ce4a249fae5419b90cba831e303-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/91c56ce4a249fae5419b90cba831e303-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/91c56ce4a249fae5419b90cba831e303-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/91c56ce4a249fae5419b90cba831e303-Reviews.html",
        "metareview": "",
        "pdf_size": 4610177,
        "gs_citation": 5248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2789414965913271365&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science Courant Institute New York University; Department of Computer Science Courant Institute New York University; Department of Computer Science Courant Institute New York University",
        "aff_domain": "cs.nyu.edu;nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/91c56ce4a249fae5419b90cba831e303-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Design Principles of the Hippocampal Cognitive Map",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4378",
        "id": "4378",
        "author_site": "Kimberly Stachenfeld, Matthew Botvinick, Samuel J Gershman",
        "author": "Kimberly L. Stachenfeld; Matthew M. Botvinick; Samuel J. Gershman",
        "abstract": "Hippocampal place fields have been shown to reflect behaviorally relevant aspects of space. For instance, place fields tend to be skewed along commonly traveled directions, they cluster around rewarded locations, and they are constrained by the geometric structure of the environment. We hypothesize a set of design principles for the hippocampal cognitive map that explain how place fields represent space in a way that facilitates navigation and reinforcement learning. In particular, we suggest that place fields encode not just information about the current location, but also predictions about future locations under the current transition distribution. Under this model, a variety of place field phenomena arise naturally from the structure of rewards, barriers, and directional biases as reflected in the transition policy. Furthermore, we demonstrate that this representation of space can support efficient reinforcement learning. We also propose that grid cells compute the eigendecomposition of place fields in part because is useful for segmenting an enclosure along natural boundaries. When applied recursively, this segmentation can be used to discover a hierarchical decomposition of space. Thus, grid cells might be involved in computing subgoals for hierarchical reinforcement learning.",
        "bibtex": "@inproceedings{NIPS2014_6083b607,\n author = {Stachenfeld, Kimberly L. and Botvinick, Matthew M. and Gershman, Samuel J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Design Principles of the Hippocampal Cognitive Map},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6083b607d0b81940c0280e465c79f5d5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/6083b607d0b81940c0280e465c79f5d5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/6083b607d0b81940c0280e465c79f5d5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/6083b607d0b81940c0280e465c79f5d5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/6083b607d0b81940c0280e465c79f5d5-Reviews.html",
        "metareview": "",
        "pdf_size": 1761891,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8389714100529498040&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Princeton Neuroscience Institute and Department of Psychology, Princeton University; Princeton Neuroscience Institute and Department of Psychology, Princeton University; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_domain": "princeton.edu;princeton.edu;mit.edu",
        "email": "princeton.edu;princeton.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/6083b607d0b81940c0280e465c79f5d5-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Princeton University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Princeton Neuroscience Institute and Department of Psychology;Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://www.princeton.edu;https://web.mit.edu",
        "aff_unique_abbr": "Princeton;MIT",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Princeton;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deterministic Symmetric Positive Semidefinite Matrix Completion",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4558",
        "id": "4558",
        "author_site": "William E Bishop, Byron M Yu",
        "author": "William E. Bishop; Byron M. Yu",
        "abstract": "We consider the problem of recovering a symmetric, positive semidefinite (SPSD) matrix from a subset of its entries, possibly corrupted by noise. In contrast to previous matrix recovery work, we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix. We develop a set of sufficient conditions for the recovery of a SPSD matrix from a set of its principal submatrices, present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met. The proposed algorithm is naturally generalized to the problem of noisy matrix recovery, and we provide a worst-case bound on reconstruction error for this scenario. Finally, we demonstrate the algorithm's utility on noiseless and noisy simulated datasets.",
        "bibtex": "@inproceedings{NIPS2014_a1abaccc,\n author = {Bishop, William E. and Yu, Byron M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deterministic Symmetric Positive Semidefinite Matrix Completion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a1abaccc942e69bea2b86f9a0a3a1026-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a1abaccc942e69bea2b86f9a0a3a1026-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a1abaccc942e69bea2b86f9a0a3a1026-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a1abaccc942e69bea2b86f9a0a3a1026-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a1abaccc942e69bea2b86f9a0a3a1026-Reviews.html",
        "metareview": "",
        "pdf_size": 743316,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10787567561218688992&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning+Center for the Neural Basis of Cognition; Center for the Neural Basis of Cognition+Biomedical Engineering+Electrical and Computer Engineering",
        "aff_domain": "cmu.edu;cmu.edu",
        "email": "cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a1abaccc942e69bea2b86f9a0a3a1026-Abstract.html",
        "aff_unique_index": "0+1;1+2",
        "aff_unique_norm": "Machine Learning;Center for the Neural Basis of Cognition;Biomedical Engineering;",
        "aff_unique_dep": ";;Biomedical Engineering;Electrical and Computer Engineering",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Difference of Convex Functions Programming for Reinforcement Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4380",
        "id": "4380",
        "author_site": "Bilal Piot, Matthieu Geist, Olivier Pietquin",
        "author": "Bilal Piot; Matthieu Geist; Olivier Pietquin",
        "abstract": "Large Markov Decision Processes (MDPs) are usually solved using Approximate Dynamic Programming (ADP) methods such as Approximate Value Iteration (AVI) or Approximate Policy Iteration (API). The main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so, we study the minimization of a norm of the Optimal Bellman Residual (OBR) $T^*Q-Q$, where $T^*$ is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally, we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning (RL) problem.",
        "bibtex": "@inproceedings{NIPS2014_0fa42ea2,\n author = {Piot, Bilal and Geist, Matthieu and Pietquin, Olivier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Difference of Convex Functions Programming for Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0fa42ea281a5043992988e446f91417f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0fa42ea281a5043992988e446f91417f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0fa42ea281a5043992988e446f91417f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0fa42ea281a5043992988e446f91417f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0fa42ea281a5043992988e446f91417f-Reviews.html",
        "metareview": "",
        "pdf_size": 394182,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6843345785227345714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "MaLIS research group (SUPELEC) - UMI 2958 (GeorgiaTech-CNRS), France + LIFL (UMR 8022 CNRS/Lille 1) - SequeL team, Lille, France; MaLIS research group (SUPELEC) - UMI 2958 (GeorgiaTech-CNRS), France; University Lille 1 - IUF (Institut Universitaire de France), France + LIFL (UMR 8022 CNRS/Lille 1) - SequeL team, Lille, France",
        "aff_domain": "lifl.fr;supelec.fr;univ-lille1.fr",
        "email": "lifl.fr;supelec.fr;univ-lille1.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0fa42ea281a5043992988e446f91417f-Abstract.html",
        "aff_unique_index": "0+1;0;2+1",
        "aff_unique_norm": "SUPELEC;Lille 1 University of Science and Technology;University Lille 1",
        "aff_unique_dep": "MaLIS research group;SequeL team;IUF (Institut Universitaire de France)",
        "aff_unique_url": "https://www.supelec.fr;https://www.univ-lille.fr;",
        "aff_unique_abbr": "SUPELEC;Lille 1;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "France"
    },
    {
        "title": "Dimensionality Reduction with Subspace Structure Preservation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4559",
        "id": "4559",
        "author_site": "Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju",
        "author": "Devansh Arpit; Ifeoma Nwogu; Venu Govindaraju",
        "abstract": "Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that $2K$ projection vectors are sufficient for the independence preservation of any $K$ class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving \\textit{state-of-the-art} results compared to popular dimensionality reduction techniques.",
        "bibtex": "@inproceedings{NIPS2014_14e9ba15,\n author = {Arpit, Devansh and Nwogu, Ifeoma and Govindaraju, Venu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dimensionality Reduction with Subspace Structure Preservation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/14e9ba1581e99c7b546f18c9ba313a97-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/14e9ba1581e99c7b546f18c9ba313a97-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/14e9ba1581e99c7b546f18c9ba313a97-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/14e9ba1581e99c7b546f18c9ba313a97-Reviews.html",
        "metareview": "",
        "pdf_size": 359176,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4022354462456933616&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, SUNY Buffalo; Department of Computer Science, SUNY Buffalo; Department of Computer Science, SUNY Buffalo",
        "aff_domain": "buffalo.edu;buffalo.edu;buffalo.edu",
        "email": "buffalo.edu;buffalo.edu;buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/14e9ba1581e99c7b546f18c9ba313a97-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "State University of New York at Buffalo",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.buffalo.edu",
        "aff_unique_abbr": "SUNY Buffalo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Buffalo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Discovering Structure in High-Dimensional Data Through Correlation Explanation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4560",
        "id": "4560",
        "author_site": "Greg Ver Steeg, Aram Galstyan",
        "author": "Greg Ver Steeg; Aram Galstyan",
        "abstract": "We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language.",
        "bibtex": "@inproceedings{NIPS2014_ef654f4d,\n author = {Ver Steeg, Greg and Galstyan, Aram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discovering Structure in High-Dimensional Data Through Correlation Explanation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ef654f4d2a51041519181e2e74921d04-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ef654f4d2a51041519181e2e74921d04-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ef654f4d2a51041519181e2e74921d04-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ef654f4d2a51041519181e2e74921d04-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ef654f4d2a51041519181e2e74921d04-Reviews.html",
        "metareview": "",
        "pdf_size": 1407842,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18041396857631893751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Information Sciences Institute, University of Southern California, Marina del Rey, CA 90292; Information Sciences Institute, University of Southern California, Marina del Rey, CA 90292",
        "aff_domain": "isi.edu;isi.edu",
        "email": "isi.edu;isi.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ef654f4d2a51041519181e2e74921d04-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Information Sciences Institute",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Marina del Rey",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Discovering, Learning and Exploiting Relevance",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4561",
        "id": "4561",
        "author_site": "Cem Tekin, Mihaela van der Schaar",
        "author": "Cem Tekin; Mihaela van der Schaar",
        "abstract": "In this paper we consider the problem of learning online what is the information to consider when making sequential decisions. We formalize this as a contextual multi-armed bandit problem where a high dimensional ($D$-dimensional) context vector arrives to a learner which needs to select an action to maximize its expected reward at each time step. Each dimension of the context vector is called a type. We assume that there exists an unknown relation between actions and types, called the relevance relation, such that the reward of an action only depends on the contexts of the relevant types. When the relation is a function, i.e., the reward of an action only depends on the context of a single type, and the expected reward of an action is Lipschitz continuous in the context of its relevant type, we propose an algorithm that achieves $\\tilde{O}(T^{\\gamma})$ regret with a high probability, where $\\gamma=2/(1+\\sqrt{2})$. Our algorithm achieves this by learning the unknown relevance relation, whereas prior contextual bandit algorithms that do not exploit the existence of a relevance relation will have $\\tilde{O}(T^{(D+1)/(D+2)})$ regret. Our algorithm alternates between exploring and exploiting, it does not require reward observations in exploitations, and it guarantees with a high probability that actions with suboptimality greater than $\\epsilon$ are never selected in exploitations. Our proposed method can be applied to a variety of learning applications including medical diagnosis, recommender systems, popularity prediction from social networks, network security etc., where at each instance of time vast amounts of different types of information are available to the decision maker, but the effect of an action depends only on a single type.",
        "bibtex": "@inproceedings{NIPS2014_e1862b12,\n author = {Tekin, Cem and van der Schaar, Mihaela},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discovering, Learning and Exploiting Relevance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e1862b129d720a03b06f94ecb83d824d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e1862b129d720a03b06f94ecb83d824d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e1862b129d720a03b06f94ecb83d824d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e1862b129d720a03b06f94ecb83d824d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e1862b129d720a03b06f94ecb83d824d-Reviews.html",
        "metareview": "",
        "pdf_size": 441230,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13067643077847945203&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Electrical Engineering Department, University of California Los Angeles; Electrical Engineering Department, University of California Los Angeles",
        "aff_domain": "ucla.edu;ee.ucla.edu",
        "email": "ucla.edu;ee.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e1862b129d720a03b06f94ecb83d824d-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Electrical Engineering Department",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Discrete Graph Hashing",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4382",
        "id": "4382",
        "author_site": "Wei Liu, Cun Mu, Sanjiv Kumar, Shih-Fu Chang",
        "author": "Wei Liu; Cun Mu; Sanjiv Kumar; Shih-Fu Chang",
        "abstract": "Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods, especially for longer codes.",
        "bibtex": "@inproceedings{NIPS2014_e39e3e21,\n author = {Liu, Wei and Mu, Cun and Kumar, Sanjiv and Chang, Shih-Fu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discrete Graph Hashing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e39e3e2197e12c6837ccd856975b7971-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e39e3e2197e12c6837ccd856975b7971-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e39e3e2197e12c6837ccd856975b7971-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e39e3e2197e12c6837ccd856975b7971-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e39e3e2197e12c6837ccd856975b7971-Reviews.html",
        "metareview": "",
        "pdf_size": 202837,
        "gs_citation": 658,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16583591227025747672&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "IBM T. J. Watson Research Center; Columbia University; Google Research; Columbia University",
        "aff_domain": "us.ibm.com;columbia.edu;google.com;ee.columbia.edu",
        "email": "us.ibm.com;columbia.edu;google.com;ee.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e39e3e2197e12c6837ccd856975b7971-Abstract.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "IBM;Columbia University;Google",
        "aff_unique_dep": "IBM;;Google Research",
        "aff_unique_url": "https://www.ibm.com/research/watson;https://www.columbia.edu;https://research.google",
        "aff_unique_abbr": "IBM;Columbia;Google Research",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "T. J. Watson;;Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Discriminative Metric Learning by Neighborhood Gerrymandering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4562",
        "id": "4562",
        "author_site": "Shubhendu Trivedi, David Mcallester, Greg Shakhnarovich",
        "author": "Shubhendu Trivedi; David McAllester; Gregory Shakhnarovich",
        "abstract": "We formulate the problem of metric learning for k nearest neighbor classification as a large margin structured prediction problem, with a latent variable representing the choice of neighbors and the task loss directly corresponding to classification error. We describe an efficient algorithm for exact loss augmented inference,and a fast gradient descent algorithm for learning in this model. The objective drives the metric to establish neighborhood boundaries that benefit the true class labels for the training points. Our approach, reminiscent of gerrymandering (redrawing of political boundaries to provide advantage to certain parties), is more direct in its handling of optimizing classification accuracy than those previously proposed. In experiments on a variety of data sets our method is shown to achieve excellent results compared to current state of the art in metric learning.",
        "bibtex": "@inproceedings{NIPS2014_f9f2c04c,\n author = {Trivedi, Shubhendu and McAllester, David and Shakhnarovich, Gregory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative Metric Learning by Neighborhood Gerrymandering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f9f2c04c803dac9b31f9e27256f23335-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f9f2c04c803dac9b31f9e27256f23335-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f9f2c04c803dac9b31f9e27256f23335-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f9f2c04c803dac9b31f9e27256f23335-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f9f2c04c803dac9b31f9e27256f23335-Reviews.html",
        "metareview": "",
        "pdf_size": 310938,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5256048334100577583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f9f2c04c803dac9b31f9e27256f23335-Abstract.html"
    },
    {
        "title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4563",
        "id": "4563",
        "author_site": "Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox",
        "author": "Alexey Dosovitskiy; Jost Tobias Springenberg; Martin Riedmiller; Thomas Brox",
        "abstract": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).",
        "bibtex": "@inproceedings{NIPS2014_fd1b9ae9,\n author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative Unsupervised Feature Learning with Convolutional Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/fd1b9ae90284fca85ba3fd719f3ba756-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/fd1b9ae90284fca85ba3fd719f3ba756-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/fd1b9ae90284fca85ba3fd719f3ba756-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/fd1b9ae90284fca85ba3fd719f3ba756-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/fd1b9ae90284fca85ba3fd719f3ba756-Reviews.html",
        "metareview": "",
        "pdf_size": 402121,
        "gs_citation": 2095,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8846448899055428439&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computer Science, University of Freiburg; Department of Computer Science, University of Freiburg; Department of Computer Science, University of Freiburg; Department of Computer Science, University of Freiburg",
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/fd1b9ae90284fca85ba3fd719f3ba756-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Freiburg",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uni-freiburg.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Distance-Based Network Recovery under Feature Correlation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4564",
        "id": "4564",
        "author_site": "David Adametz, Volker Roth",
        "author": "David Adametz; Volker Roth",
        "abstract": "We present an inference method for Gaussian graphical models when only pairwise distances of n objects are observed. Formally, this is a problem of estimating an n x n covariance matrix from the Mahalanobis distances dMH(xi, xj), where object xi lives in a latent feature space. We solve the problem in fully Bayesian fashion by integrating over the Matrix-Normal likelihood and a Matrix-Gamma prior; the resulting Matrix-T posterior enables network recovery even under strongly correlated features. Hereby, we generalize TiWnet, which assumes Euclidean distances with strict feature independence. In spite of the greatly increased flexibility, our model neither loses statistical power nor entails more computational cost. We argue that the extension is highly relevant as it yields significantly better results in both synthetic and real-world experiments, which is successfully demonstrated for a network of biological pathways in cancer patients.",
        "bibtex": "@inproceedings{NIPS2014_0a158fff,\n author = {Adametz, David and Roth, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distance-Based Network Recovery under Feature Correlation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0a158fff343cd8aa7f09f90d014cf7dd-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0a158fff343cd8aa7f09f90d014cf7dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0a158fff343cd8aa7f09f90d014cf7dd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0a158fff343cd8aa7f09f90d014cf7dd-Reviews.html",
        "metareview": "",
        "pdf_size": 369577,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1962393887820606081&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Mathematics and Computer Science, University of Basel, Switzerland; Department of Mathematics and Computer Science, University of Basel, Switzerland",
        "aff_domain": "unibas.ch;unibas.ch",
        "email": "unibas.ch;unibas.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0a158fff343cd8aa7f09f90d014cf7dd-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Basel",
        "aff_unique_dep": "Department of Mathematics and Computer Science",
        "aff_unique_url": "https://www.unibas.ch",
        "aff_unique_abbr": "UniBas",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Distributed Balanced Clustering via Mapping Coresets",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4565",
        "id": "4565",
        "author_site": "Mohammadhossein Bateni, Aditya Bhaskara, Silvio Lattanzi, Vahab Mirrokni",
        "author": "MohammadHossein Bateni; Aditya Bhaskara; Silvio Lattanzi; Vahab Mirrokni",
        "abstract": "Large-scale clustering of data points in metric spaces is an important problem in mining big data sets. For many applications, we face explicit or implicit size constraints for each cluster which leads to the problem of clustering under capacity constraints or the",
        "bibtex": "@inproceedings{NIPS2014_788872e0,\n author = {Bateni, MohammadHossein and Bhaskara, Aditya and Lattanzi, Silvio and Mirrokni, Vahab},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Balanced Clustering via Mapping Coresets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/788872e0da448a4707e9a70a1b1e74d8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/788872e0da448a4707e9a70a1b1e74d8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/788872e0da448a4707e9a70a1b1e74d8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/788872e0da448a4707e9a70a1b1e74d8-Reviews.html",
        "metareview": "",
        "pdf_size": 312526,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12910678393739401997&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Google NYC; Google NYC; Google NYC; Google NYC",
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/788872e0da448a4707e9a70a1b1e74d8-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "New York City",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Distributed Bayesian Posterior Sampling via Moment Sharing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4566",
        "id": "4566",
        "author_site": "Minjie Xu, Balaji Lakshminarayanan, Yee Whye Teh, Jun Zhu, Bo Zhang",
        "author": "Minjie Xu; Balaji Lakshminarayanan; Yee Whye Teh; Jun Zhu; Bo Zhang",
        "abstract": "We propose a distributed Markov chain Monte Carlo (MCMC) inference algorithm for large scale Bayesian posterior simulation. We assume that the dataset is partitioned and stored across nodes of a cluster. Our procedure involves an independent MCMC posterior sampler at each node based on its local partition of the data. Moment statistics of the local posteriors are collected from each sampler and propagated across the cluster using expectation propagation message passing with low communication costs. The moment sharing scheme improves posterior estimation quality by enforcing agreement among the samplers. We demonstrate the speed and inference quality of our method with empirical studies on Bayesian logistic regression and sparse linear regression with a spike-and-slab prior.",
        "bibtex": "@inproceedings{NIPS2014_90a32a4f,\n author = {Xu, Minjie and Lakshminarayanan, Balaji and Teh, Yee Whye and Zhu, Jun and Zhang, Bo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Bayesian Posterior Sampling via Moment Sharing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/90a32a4fcbae4783bd3484b255422681-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/90a32a4fcbae4783bd3484b255422681-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/90a32a4fcbae4783bd3484b255422681-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/90a32a4fcbae4783bd3484b255422681-Reviews.html",
        "metareview": "",
        "pdf_size": 689686,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=608399121365554552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "State Key Lab of Intelligent Technology and Systems + Tsinghua National TNList Lab + Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Gatsby Unit, University College London, 17 Queen Square, London WC1N 3AR, UK; Department of Statistics, University of Oxford, 1 South Parks Road, Oxford OX1 3TG, UK; State Key Lab of Intelligent Technology and Systems + Tsinghua National TNList Lab + Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; State Key Lab of Intelligent Technology and Systems + Tsinghua National TNList Lab + Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/90a32a4fcbae4783bd3484b255422681-Abstract.html",
        "aff_unique_index": "0+1+1;2;3;0+1+1;0+1+1",
        "aff_unique_norm": "State Key Lab of Intelligent Technology and Systems;Tsinghua University;University College London;University of Oxford",
        "aff_unique_dep": ";National TNList Lab;Gatsby Unit;Department of Statistics",
        "aff_unique_url": ";http://www.tsinghua.edu.cn;https://www.ucl.ac.uk;https://www.ox.ac.uk",
        "aff_unique_abbr": ";Tsinghua;UCL;Oxford",
        "aff_campus_unique_index": "1;2;3;1;1",
        "aff_campus_unique": ";Beijing;London;Oxford",
        "aff_country_unique_index": "0+0+0;1;1;0+0+0;0+0+0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "title": "Distributed Estimation, Information Loss and Exponential Families",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4567",
        "id": "4567",
        "author_site": "Qiang Liu, Alexander Ihler",
        "author": "Qiang Liu; Alexander Ihler",
        "abstract": "Distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important. We study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (MLE) based on the data subsets, and then combines the local MLEs to achieve the best possible approximation to the global MLE, based on the whole dataset jointly. We study the statistical properties of this framework, showing that the loss of efficiency compared to the global setting relates to how much the underlying distribution families deviate from full exponential families, drawing connection to the theory of information loss by Fisher, Rao and Efron. We show that the full-exponential-family-ness\" represents the lower bound of the error rate of arbitrary combinations of local MLEs, and is achieved by a KL-divergence-based combination method but not by a more common linear combination method. We also study the empirical properties of the KL and linear combination methods, showing that the KL method significantly outperforms linear combination in practical settings with issues such as model misspecification, non-convexity, and heterogeneous data partitions.\"",
        "bibtex": "@inproceedings{NIPS2014_056d7ac1,\n author = {Liu, Qiang and Ihler, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Estimation, Information Loss and Exponential Families},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/056d7ac16aa3fc9dc241a20cfb56539c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/056d7ac16aa3fc9dc241a20cfb56539c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/056d7ac16aa3fc9dc241a20cfb56539c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/056d7ac16aa3fc9dc241a20cfb56539c-Reviews.html",
        "metareview": "",
        "pdf_size": 4017460,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=519725334060106143&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine",
        "aff_domain": "uci.edu;ics.uci.edu",
        "email": "uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/056d7ac16aa3fc9dc241a20cfb56539c-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Distributed Parameter Estimation in Probabilistic Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4568",
        "id": "4568",
        "author_site": "Yariv D Mizrahi, Misha Denil, Nando de Freitas",
        "author": "Yariv D. Mizrahi; Misha Denil; Nando de Freitas",
        "abstract": "This paper presents foundational theoretical results on distributed parameter estimation for undirected probabilistic graphical models. It introduces a general condition on composite likelihood decompositions of these models which guarantees the global consistency of distributed estimators, provided the local estimators are consistent.",
        "bibtex": "@inproceedings{NIPS2014_ef8ac102,\n author = {Mizrahi, Yariv D. and Denil, Misha and de Freitas, Nando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Parameter Estimation in Probabilistic Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ef8ac102d24db30bddd35492a8a28f46-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ef8ac102d24db30bddd35492a8a28f46-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ef8ac102d24db30bddd35492a8a28f46-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ef8ac102d24db30bddd35492a8a28f46-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ef8ac102d24db30bddd35492a8a28f46-Reviews.html",
        "metareview": "",
        "pdf_size": 344341,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ef8ac102d24db30bddd35492a8a28f46-Abstract.html"
    },
    {
        "title": "Distributed Power-law Graph Computing: Theoretical and Empirical Analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4569",
        "id": "4569",
        "author_site": "Cong Xie, Ling Yan, Wu-Jun Li, Zhihua Zhang",
        "author": "Cong Xie; Ling Yan; Wu-Jun Li; Zhihua Zhang",
        "abstract": "With the emergence of big graphs in a variety of real applications like social networks, machine learning based on distributed graph-computing~(DGC) frameworks has attracted much attention from big data machine learning community. In DGC frameworks, the graph partitioning~(GP) strategy plays a key role to affect the performance, including the workload balance and communication cost. Typically, the degree distributions of natural graphs from real applications follow skewed power laws, which makes GP a challenging task. Recently, many methods have been proposed to solve the GP problem. However, the existing GP methods cannot achieve satisfactory performance for applications with power-law graphs. In this paper, we propose a novel vertex-cut method, called \\emph{degree-based hashing}~(DBH), for GP. DBH makes effective use of the skewed degree distributions for GP. We theoretically prove that DBH can achieve lower communication cost than existing methods and can simultaneously guarantee good workload balance. Furthermore, empirical results on several large power-law graphs also show that DBH can outperform the state of the art.",
        "bibtex": "@inproceedings{NIPS2014_3a794b71,\n author = {Xie, Cong and Yan, Ling and Li, Wu-Jun and Zhang, Zhihua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Power-law Graph Computing: Theoretical and Empirical Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3a794b71830091b1e8048312eb649c88-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3a794b71830091b1e8048312eb649c88-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3a794b71830091b1e8048312eb649c88-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3a794b71830091b1e8048312eb649c88-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3a794b71830091b1e8048312eb649c88-Reviews.html",
        "metareview": "",
        "pdf_size": 427322,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4786376872348172163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Comp. Sci. and Eng., Shanghai Jiao Tong University; Dept. of Comp. Sci. and Eng., Shanghai Jiao Tong University; National Key Lab. for Novel Software Tech., Dept. of Comp. Sci. and Tech., Nanjing University; Dept. of Comp. Sci. and Eng., Shanghai Jiao Tong University",
        "aff_domain": "gmail.com;sjtu.edu.cn;nju.edu.cn;cs.sjtu.edu.cn",
        "email": "gmail.com;sjtu.edu.cn;nju.edu.cn;cs.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3a794b71830091b1e8048312eb649c88-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;Nanjing University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science and Technology",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.nju.edu.cn",
        "aff_unique_abbr": "SJTU;Nanjing University",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4570",
        "id": "4570",
        "author_site": "Yarin Gal, Mark van der Wilk, Carl Edward Rasmussen",
        "author": "Yarin Gal; Mark van der Wilk; Carl E. Rasmussen",
        "abstract": "Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.",
        "bibtex": "@inproceedings{NIPS2014_cee69087,\n author = {Gal, Yarin and van der Wilk, Mark and Rasmussen, Carl E.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cee69087de9e04417994cb48036b232e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cee69087de9e04417994cb48036b232e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/cee69087de9e04417994cb48036b232e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cee69087de9e04417994cb48036b232e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cee69087de9e04417994cb48036b232e-Reviews.html",
        "metareview": "",
        "pdf_size": 302730,
        "gs_citation": 207,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6309878675442298306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Cambridge; University of Cambridge; University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cee69087de9e04417994cb48036b232e-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Diverse Randomized Agents Vote to Win",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4571",
        "id": "4571",
        "author_site": "Albert Jiang, Leandro Soriano Marcolino, Ariel Procaccia, Tuomas Sandholm, Nisarg Shah, Milind Tambe",
        "author": "Albert Xin Jiang; Leandro Soriano Marcolino; Ariel D. Procaccia; Tuomas Sandholm; Nisarg Shah; Milind Tambe",
        "abstract": "We investigate the power of voting among diverse, randomized software agents. With teams of computer Go agents in mind, we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning. This model allows us to reason about a collection of agents with different biases (determined by the first-stage noise models), which, furthermore, apply randomized algorithms to evaluate alternatives and produce votes (captured by the second-stage noise models). We analytically demonstrate that a uniform team, consisting of multiple instances of any single agent, must make a significant number of mistakes, whereas a diverse team converges to perfection as the number of agents grows. Our experiments, which pit teams of computer Go agents against strong agents, provide evidence for the effectiveness of voting when agents are diverse.",
        "bibtex": "@inproceedings{NIPS2014_dd3ca198,\n author = {Jiang, Albert Xin and Marcolino, Leandro Soriano and Procaccia, Ariel D. and Sandholm, Tuomas and Shah, Nisarg and Tambe, Milind},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Diverse Randomized Agents Vote to Win},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/dd3ca198a5dc590a040db5ade5b8503f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/dd3ca198a5dc590a040db5ade5b8503f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/dd3ca198a5dc590a040db5ade5b8503f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/dd3ca198a5dc590a040db5ade5b8503f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/dd3ca198a5dc590a040db5ade5b8503f-Reviews.html",
        "metareview": "",
        "pdf_size": 360624,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=973333114839574410&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Trinity University; USC; CMU; CMU; CMU; USC",
        "aff_domain": "trinity.edu;usc.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;usc.edu",
        "email": "trinity.edu;usc.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/dd3ca198a5dc590a040db5ade5b8503f-Abstract.html",
        "aff_unique_index": "0;1;2;2;2;1",
        "aff_unique_norm": "Trinity University;University of Southern California;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.trinity.edu;https://www.usc.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Trinity;USC;CMU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Diverse Sequential Subset Selection for Supervised Video Summarization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4572",
        "id": "4572",
        "author_site": "Boqing Gong, Wei-Lun Chao, Kristen Grauman, Fei Sha",
        "author": "Boqing Gong; Wei-Lun Chao; Kristen Grauman; Fei Sha",
        "abstract": "Video summarization is a challenging problem with great application potential. Whereas prior approaches, largely unsupervised in nature, focus on sampling useful frames and assembling them as summaries, we consider video summarization as a supervised subset selection problem. Our idea is to teach the system to learn from human-created summaries how to select informative and diverse subsets, so as to best meet evaluation metrics derived from human-perceived quality. To this end, we propose the sequential determinantal point process (seqDPP), a probabilistic model for diverse sequential subset selection. Our novel seqDPP heeds the inherent sequential structures in video data, thus overcoming the deficiency of the standard DPP, which treats video frames as randomly permutable items. Meanwhile, seqDPP retains the power of modeling diverse subsets, essential for summarization. Our extensive results of summarizing videos from 3 datasets demonstrate the superior performance of our method, compared to not only existing unsupervised methods but also naive applications of the standard DPP model.",
        "bibtex": "@inproceedings{NIPS2014_5d3b9e06,\n author = {Gong, Boqing and Chao, Wei-Lun and Grauman, Kristen and Sha, Fei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Diverse Sequential Subset Selection for Supervised Video Summarization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5d3b9e06117de70a7e5076cc3ed89e18-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5d3b9e06117de70a7e5076cc3ed89e18-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5d3b9e06117de70a7e5076cc3ed89e18-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5d3b9e06117de70a7e5076cc3ed89e18-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5d3b9e06117de70a7e5076cc3ed89e18-Reviews.html",
        "metareview": "",
        "pdf_size": 822272,
        "gs_citation": 570,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10875185097101963195&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Southern California",
        "aff_domain": "usc.edu;usc.edu;cs.utexas.edu;usc.edu",
        "email": "usc.edu;usc.edu;cs.utexas.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5d3b9e06117de70a7e5076cc3ed89e18-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Southern California;University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.usc.edu;https://www.utexas.edu",
        "aff_unique_abbr": "USC;UT Austin",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Los Angeles;Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Divide-and-Conquer Learning by Anchoring a Conical Hull",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4573",
        "id": "4573",
        "author_site": "Tianyi Zhou, Jeffrey A Bilmes, Carlos Guestrin",
        "author": "Tianyi Zhou; Jeff Bilmes; Carlos Guestrin",
        "abstract": "We reduce a broad class of machine learning problems, usually addressed by EM or sampling, to the problem of finding the $k$ extremal rays spanning the conical hull of a data point set. These $k$ ``anchors'' lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the $k$ anchors, we propose a novel divide-and-conquer learning scheme ``DCA'' that distributes the problem to $\\mathcal O(k\\log k)$ same-type sub-problems on different low-D random hyperplanes, each can be solved by any solver. For the 2D sub-problem, we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine for other methods to check whether a point is covered in a conical hull, which improves algorithm design in multiple dimensions and brings significant speedup to learning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on rich datasets.",
        "bibtex": "@inproceedings{NIPS2014_eab6ff00,\n author = {Zhou, Tianyi and Bilmes, Jeff and Guestrin, Carlos},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Divide-and-Conquer Learning by Anchoring a Conical Hull},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/eab6ff004e7abb2c91411d2843cfe9ac-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/eab6ff004e7abb2c91411d2843cfe9ac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/eab6ff004e7abb2c91411d2843cfe9ac-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/eab6ff004e7abb2c91411d2843cfe9ac-Reviews.html",
        "metareview": "",
        "pdf_size": 1164141,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10647373538467116472&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science & Engineering; Electrical Engineering; Computer Science & Engineering",
        "aff_domain": "u.washington.edu;u.washington.edu;u.washington.edu",
        "email": "u.washington.edu;u.washington.edu;u.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/eab6ff004e7abb2c91411d2843cfe9ac-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Computer Science & Engineering;Electrical Engineering Department",
        "aff_unique_dep": "Computer Science & Engineering;Electrical Engineering",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Do Convnets Learn Correspondence?",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4574",
        "id": "4574",
        "author_site": "Jonathan L Long, Ning Zhang, Trevor Darrell",
        "author": "Jonathan Long; Ning Zhang; Trevor Darrell",
        "abstract": "Convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011.",
        "bibtex": "@inproceedings{NIPS2014_50f6d53b,\n author = {Long, Jonathan and Zhang, Ning and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Do Convnets Learn Correspondence?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/50f6d53bcaae4f4d70d1ecf5341f6eb4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/50f6d53bcaae4f4d70d1ecf5341f6eb4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/50f6d53bcaae4f4d70d1ecf5341f6eb4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/50f6d53bcaae4f4d70d1ecf5341f6eb4-Reviews.html",
        "metareview": "",
        "pdf_size": 4303882,
        "gs_citation": 399,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=163870541144798819&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of California \u2013 Berkeley; University of California \u2013 Berkeley; University of California \u2013 Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/50f6d53bcaae4f4d70d1ecf5341f6eb4-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Do Deep Nets Really Need to be Deep?",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4575",
        "id": "4575",
        "author_site": "Jimmy Ba, Rich Caruana",
        "author": "Lei Jimmy Ba; Rich Caruana",
        "abstract": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.",
        "bibtex": "@inproceedings{NIPS2014_b0c355a9,\n author = {Ba, Lei Jimmy and Caruana, Rich},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Do Deep Nets Really Need to be Deep?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b0c355a9dedccb50e5537e8f2e3f0810-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b0c355a9dedccb50e5537e8f2e3f0810-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b0c355a9dedccb50e5537e8f2e3f0810-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b0c355a9dedccb50e5537e8f2e3f0810-Reviews.html",
        "metareview": "",
        "pdf_size": 229382,
        "gs_citation": 2798,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=207936000892324125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "University of Toronto; Microsoft Research",
        "aff_domain": "psi.utoronto.ca;microsoft.com",
        "email": "psi.utoronto.ca;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b0c355a9dedccb50e5537e8f2e3f0810-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Toronto;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.utoronto.ca;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "U of T;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Dynamic Rank Factor Model for Text Streams",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4576",
        "id": "4576",
        "author_site": "Shaobo Han, Lin Du, Esther Salazar, Lawrence Carin",
        "author": "Shaobo Han; Lin Du; Esther Salazar; Lawrence Carin",
        "abstract": "We propose a semi-parametric and dynamic rank factor model for topic modeling, capable of (1) discovering topic prevalence over time, and (2) learning contemporary multi-scale dependence structures, providing topic and word correlations as a byproduct. The high-dimensional and time-evolving ordinal/rank observations (such as word counts), after an arbitrary monotone transformation, are well accommodated through an underlying dynamic sparse factor model. The framework naturally admits heavy-tailed innovations, capable of inferring abrupt temporal jumps in the importance of topics. Posterior inference is performed through straightforward Gibbs sampling, based on the forward-filtering backward-sampling algorithm. Moreover, an efficient data subsampling scheme is leveraged to speed up inference on massive datasets. The modeling framework is illustrated on two real datasets: the US State of the Union Address and the JSTOR collection from Science.",
        "bibtex": "@inproceedings{NIPS2014_0673011f,\n author = {Han, Shaobo and Du, Lin and Salazar, Esther and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic Rank Factor Model for Text Streams},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0673011fbdc464f51b05897b7db2d151-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0673011fbdc464f51b05897b7db2d151-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0673011fbdc464f51b05897b7db2d151-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0673011fbdc464f51b05897b7db2d151-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0673011fbdc464f51b05897b7db2d151-Reviews.html",
        "metareview": "",
        "pdf_size": 681730,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1928893257915065396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Duke University; Duke University; Duke University; Duke University",
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0673011fbdc464f51b05897b7db2d151-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Efficient Inference of Continuous Markov Random Fields with Polynomial Potentials",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4578",
        "id": "4578",
        "author_site": "Shenlong Wang, Alex Schwing, Raquel Urtasun",
        "author": "Shenlong Wang; Alexander G. Schwing; Raquel Urtasun",
        "abstract": "In this paper, we prove that every multivariate polynomial with even degree can be decomposed into a sum of convex and concave polynomials. Motivated by this property, we exploit the concave-convex procedure to perform inference on continuous Markov random fields with polynomial potentials. In particular, we show that the concave-convex decomposition of polynomials can be expressed as a sum-of-squares optimization, which can be efficiently solved via semidefinite programming. We demonstrate the effectiveness of our approach in the context of 3D reconstruction, shape from shading and image denoising, and show that our approach significantly outperforms existing approaches in terms of efficiency as well as the quality of the retrieved solution.",
        "bibtex": "@inproceedings{NIPS2014_a1fb5e16,\n author = {Wang, Shenlong and Schwing, Alexander G. and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Inference of Continuous Markov Random Fields with Polynomial Potentials},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a1fb5e16d4aa756c25e2f035d2f402c6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a1fb5e16d4aa756c25e2f035d2f402c6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a1fb5e16d4aa756c25e2f035d2f402c6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a1fb5e16d4aa756c25e2f035d2f402c6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a1fb5e16d4aa756c25e2f035d2f402c6-Reviews.html",
        "metareview": "",
        "pdf_size": 753709,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6519406817056862676&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "University of Toronto; University of Toronto; University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a1fb5e16d4aa756c25e2f035d2f402c6-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Efficient Minimax Signal Detection on Graphs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4579",
        "id": "4579",
        "author_site": "Jing Qian, Venkatesh Saligrama",
        "author": "Jing Qian; Venkatesh Saligrama",
        "abstract": "Several problems such as network intrusion, community detection, and disease outbreak can be described by observations attributed to nodes or edges of a graph. In these applications presence of intrusion, community or disease outbreak is characterized by novel observations on some unknown connected subgraph. These problems can be formulated in terms of optimization of suitable objectives on connected subgraphs, a problem which is generally computationally difficult. We overcome the combinatorics of connectivity by embedding connected subgraphs into linear matrix inequalities (LMI). Computationally efficient tests are then realized by optimizing convex objective functions subject to these LMI constraints. We prove, by means of a novel Euclidean embedding argument, that our tests are minimax optimal for exponential family of distributions on 1-D and 2-D lattices. We show that internal conductance of the connected subgraph family plays a fundamental role in characterizing detectability.",
        "bibtex": "@inproceedings{NIPS2014_ea987c6d,\n author = {Qian, Jing and Saligrama, Venkatesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Minimax Signal Detection on Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ea987c6d45415e2fcac36c76b1b61810-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ea987c6d45415e2fcac36c76b1b61810-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ea987c6d45415e2fcac36c76b1b61810-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ea987c6d45415e2fcac36c76b1b61810-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ea987c6d45415e2fcac36c76b1b61810-Reviews.html",
        "metareview": "",
        "pdf_size": 169749,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17699628650825073281&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Division of Systems Engineering, Boston University; Department of Electrical and Computer Engineering, Boston University",
        "aff_domain": "bu.edu;bu.edu",
        "email": "bu.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ea987c6d45415e2fcac36c76b1b61810-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Division of Systems Engineering",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Efficient Minimax Strategies for Square Loss Games",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4580",
        "id": "4580",
        "author_site": "Wouter M Koolen, Alan Malek, Peter Bartlett",
        "author": "Wouter M. Koolen; Alan Malek; Peter L. Bartlett",
        "abstract": "We consider online prediction problems where the loss between the prediction and the outcome is measured by the squared Euclidean distance and its generalization, the squared Mahalanobis distance. We derive the minimax solutions for the case where the prediction and action spaces are the simplex (this setup is sometimes called the Brier game) and the $\\ell_2$ ball (this setup is related to Gaussian density estimation). We show that in both cases the value of each sub-game is a quadratic function of a simple statistic of the state, with coefficients that can be efficiently computed using an explicit recurrence relation. The resulting deterministic minimax strategy and randomized maximin strategy are linear functions of the statistic.",
        "bibtex": "@inproceedings{NIPS2014_178eb467,\n author = {Koolen, Wouter M. and Malek, Alan and Bartlett, Peter L.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Minimax Strategies for Square Loss Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/178eb467f26013c4a2db409f2255f893-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/178eb467f26013c4a2db409f2255f893-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/178eb467f26013c4a2db409f2255f893-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/178eb467f26013c4a2db409f2255f893-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/178eb467f26013c4a2db409f2255f893-Reviews.html",
        "metareview": "",
        "pdf_size": 284367,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7767944438151701502&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Queensland University of Technology + UC Berkeley; University of California, Berkeley; University of California, Berkeley + Queensland University of Technology",
        "aff_domain": "qut.edu.au;eecs.berkeley.edu;berkeley.edu",
        "email": "qut.edu.au;eecs.berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/178eb467f26013c4a2db409f2255f893-Abstract.html",
        "aff_unique_index": "0+1;1;1+0",
        "aff_unique_norm": "Queensland University of Technology;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.qut.edu.au;https://www.berkeley.edu",
        "aff_unique_abbr": "QUT;UC Berkeley",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+1;1;1+0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "title": "Efficient Optimization for Average Precision SVM",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4581",
        "id": "4581",
        "author_site": "Pritish Mohapatra, C.V. Jawahar, M. Pawan Kumar",
        "author": "Pritish Mohapatra; C.V. Jawahar; M. Pawan Kumar",
        "abstract": "The accuracy of information retrieval systems is often measured using average precision (AP). Given a set of positive (relevant) and negative (non-relevant) samples, the parameters of a retrieval system can be estimated using the AP-SVM framework, which minimizes a regularized convex upper bound on the empirical AP loss. However, the high computational complexity of loss-augmented inference, which is required for learning an AP-SVM, prohibits its use with large training datasets. To alleviate this deficiency, we propose three complementary approaches. The first approach guarantees an asymptotic decrease in the computational complexity of loss-augmented inference by exploiting the problem structure. The second approach takes advantage of the fact that we do not require a full ranking during loss-augmented inference. This helps us to avoid the expensive step of sorting the negative samples according to their individual scores. The third approach approximates the AP loss over all samples by the AP loss over difficult samples (for example, those that are incorrectly classified by a binary SVM), while ensuring the correct classification of the remaining samples. Using the PASCAL VOC action classification and object detection datasets, we show that our approaches provide significant speed-ups during training without degrading the test accuracy of AP-SVM.",
        "bibtex": "@inproceedings{NIPS2014_efd467cb,\n author = {Mohapatra, Pritish and Jawahar, C.V. and Kumar, M. Pawan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Optimization for Average Precision SVM},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/efd467cb94fec6988490a8833ef68ff8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/efd467cb94fec6988490a8833ef68ff8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/efd467cb94fec6988490a8833ef68ff8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/efd467cb94fec6988490a8833ef68ff8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/efd467cb94fec6988490a8833ef68ff8-Reviews.html",
        "metareview": "",
        "pdf_size": 260483,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16791161766672782915&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "IIIT Hyderabad; IIIT Hyderabad; Ecole Centrale Paris + INRIA Saclay",
        "aff_domain": "research.iiit.ac.in;iiit.ac.in;ecp.fr",
        "email": "research.iiit.ac.in;iiit.ac.in;ecp.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/efd467cb94fec6988490a8833ef68ff8-Abstract.html",
        "aff_unique_index": "0;0;1+2",
        "aff_unique_norm": "International Institute of Information Technology, Hyderabad;Ecole Centrale Paris;INRIA",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://iiit Hyderabad.ac.in;https://www.ecp.fr;https://www.inria.fr",
        "aff_unique_abbr": "IIIT-H;ECP;INRIA",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Hyderabad;;Saclay",
        "aff_country_unique_index": "0;0;1+1",
        "aff_country_unique": "India;France"
    },
    {
        "title": "Efficient Partial Monitoring with Prior Information",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4582",
        "id": "4582",
        "author_site": "Hastagiri P Vanchinathan, G\u00e1bor Bart\u00f3k, Andreas Krause",
        "author": "Hastagiri P Vanchinathan; G\u00e1bor Bart\u00f3k; Andreas Krause",
        "abstract": "Partial monitoring is a general model for online learning with limited feedback: a learner chooses actions in a sequential manner while an opponent chooses outcomes. In every round, the learner suffers some loss and receives some feedback based on the action and the outcome. The goal of the learner is to minimize her cumulative loss. Applications range from dynamic pricing to label-efficient prediction to dueling bandits. In this paper, we assume that we are given some prior information about the distribution based on which the opponent generates the outcomes. We propose BPM, a family of new efficient algorithms whose core is to track the outcome distribution with an ellipsoid centered around the estimated distribution. We show that our algorithm provably enjoys near-optimal regret rate for locally observable partial-monitoring problems against stochastic opponents. As demonstrated with experiments on synthetic as well as real-world data, the algorithm outperforms previous approaches, even for very uninformed priors, with an order of magnitude smaller regret and lower running time.",
        "bibtex": "@inproceedings{NIPS2014_5c07b60e,\n author = {Vanchinathan, Hastagiri P and Bart\\'{o}k, G\\'{a}bor and Krause, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Partial Monitoring with Prior Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5c07b60e74014b3ee8157a25e9a65dc8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5c07b60e74014b3ee8157a25e9a65dc8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5c07b60e74014b3ee8157a25e9a65dc8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5c07b60e74014b3ee8157a25e9a65dc8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5c07b60e74014b3ee8157a25e9a65dc8-Reviews.html",
        "metareview": "",
        "pdf_size": 530902,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2615449474832016717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Science, ETH Z \u00a8urich, Switzerland; Dept. of Computer Science, ETH Z \u00a8urich, Switzerland; Dept. of Computer Science, ETH Z \u00a8urich, Switzerland",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5c07b60e74014b3ee8157a25e9a65dc8-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Efficient Sampling for Learning Sparse Additive Models in High Dimensions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4583",
        "id": "4583",
        "author_site": "Hemant Tyagi, Bernd G\u00e4rtner, Andreas Krause",
        "author": "Hemant Tyagi; Andreas Krause; Bernd G\u00e4rtner",
        "abstract": "We consider the problem of learning sparse additive models, i.e., functions of the form: $f(\\vecx) = \\sum_{l \\in S} \\phi_{l}(x_l)$, $\\vecx \\in \\matR^d$ from point queries of $f$. Here $S$ is an unknown subset of coordinate variables with $\\abs{S} = k \\ll d$. Assuming $\\phi_l$'s to be smooth, we propose a set of points at which to sample $f$ and an efficient randomized algorithm that recovers a \\textit{uniform approximation} to each unknown $\\phi_l$. We provide a rigorous theoretical analysis of our scheme along with sample complexity bounds. Our algorithm utilizes recent results from compressive sensing theory along with a novel convex quadratic program for recovering robust uniform approximations to univariate functions, from point queries corrupted with arbitrary bounded noise. Lastly we theoretically analyze the impact of noise -- either arbitrary but bounded, or stochastic -- on the performance of our algorithm.",
        "bibtex": "@inproceedings{NIPS2014_6db014d2,\n author = {Tyagi, Hemant and Krause, Andreas and G\\\"{a}rtner, Bernd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Sampling for Learning Sparse Additive Models in High Dimensions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6db014d26321351d6c2f991781f02c7d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/6db014d26321351d6c2f991781f02c7d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/6db014d26321351d6c2f991781f02c7d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/6db014d26321351d6c2f991781f02c7d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/6db014d26321351d6c2f991781f02c7d-Reviews.html",
        "metareview": "",
        "pdf_size": 309508,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17450019358503399985&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "ETH Z \u00a8urich; ETH Z \u00a8urich; ETH Z \u00a8urich",
        "aff_domain": "inf.ethz.ch;ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/6db014d26321351d6c2f991781f02c7d-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Z\u00fcrich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Efficient Structured Matrix Rank Minimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4584",
        "id": "4584",
        "author_site": "Adams Wei Yu, Wanli Ma, Yaoliang Yu, Jaime Carbonell, Suvrit Sra",
        "author": "Adams Wei Yu; Wanli Ma; Yaoliang Yu; Jaime G. Carbonell; Suvrit Sra",
        "abstract": "We study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map. In contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full SVD; nor (b) resort to augmented Lagrangian techniques; nor (c) solve linear systems per iteration. Instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.",
        "bibtex": "@inproceedings{NIPS2014_1c8490c5,\n author = {Yu, Adams Wei and Ma, Wanli and Yu, Yaoliang and Carbonell, Jaime G. and Sra, Suvrit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Structured Matrix Rank Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1c8490c54331f54ba59e2f0036498668-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1c8490c54331f54ba59e2f0036498668-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/1c8490c54331f54ba59e2f0036498668-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1c8490c54331f54ba59e2f0036498668-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1c8490c54331f54ba59e2f0036498668-Reviews.html",
        "metareview": "",
        "pdf_size": 644608,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7705852912415845737&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, Carnegie Mellon University\u2020; School of Computer Science, Carnegie Mellon University\u2020; School of Computer Science, Carnegie Mellon University\u2020; School of Computer Science, Carnegie Mellon University\u2020; Max Planck Institute for Intelligent Systems\u2021",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;tuebingen.mpg.de",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1c8490c54331f54ba59e2f0036498668-Abstract.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.cmu.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "CMU;MPI-IS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Efficient learning by implicit exploration in bandit problems with side observations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4585",
        "id": "4585",
        "author_site": "Tom\u00e1\u0161 Koc\u00e1k, Gergely Neu, Michal Valko, Remi Munos",
        "author": "Tom\u00e1\u0161 Koc\u00e1k; Gergely Neu; Michal Valko; R\u00e9mi Munos",
        "abstract": "We consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback. In the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions. The revealed losses depend on the learner's action and a directed observation system chosen by the environment. For this setting, we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines, we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback. As the predictions of our first algorithm cannot be always computed efficiently in this setting, we propose another algorithm with similar properties and with the benefit of always being computationally efficient, at the price of a slightly more complicated tuning mechanism. Both algorithms rely on a novel exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.",
        "bibtex": "@inproceedings{NIPS2014_8169cf3d,\n author = {Koc\\'{a}k, Tom\\'{a}\\v{s} and Neu, Gergely and Valko, Michal and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient learning by implicit exploration in bandit problems with side observations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8169cf3dc05090c7774c8dc38317c43d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8169cf3dc05090c7774c8dc38317c43d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8169cf3dc05090c7774c8dc38317c43d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8169cf3dc05090c7774c8dc38317c43d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8169cf3dc05090c7774c8dc38317c43d-Reviews.html",
        "metareview": "",
        "pdf_size": 380035,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17635527035548913702&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "SequeL team, INRIA Lille \u2013 Nord Europe, France; SequeL team, INRIA Lille \u2013 Nord Europe, France; SequeL team, INRIA Lille \u2013 Nord Europe, France; SequeL team, INRIA Lille \u2013 Nord Europe, France + Google DeepMind",
        "aff_domain": "inria.fr;inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8169cf3dc05090c7774c8dc38317c43d-Abstract.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "INRIA Lille \u2013 Nord Europe;Google",
        "aff_unique_dep": "SequeL team;Google DeepMind",
        "aff_unique_url": "https://www.inria.fr/en/centre/lille-nord-europe;https://deepmind.com",
        "aff_unique_abbr": "INRIA;DeepMind",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lille;",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "France;United Kingdom"
    },
    {
        "title": "Elementary Estimators for Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4586",
        "id": "4586",
        "author_site": "Eunho Yang, Aurelie Lozano, Pradeep Ravikumar",
        "author": "Eunho Yang; Aur\u00e9lie C. Lozano; Pradeep Ravikumar",
        "abstract": "We propose a class of closed-form estimators for sparsity-structured graphical models, expressed as exponential family distributions, under high-dimensional settings. Our approach builds on observing the precise manner in which the classical graphical model MLE ``breaks down'' under high-dimensional settings. Our estimator uses a carefully constructed, well-defined and closed-form backward map, and then performs thresholding operations to ensure the desired sparsity structure. We provide a rigorous statistical analysis that shows that surprisingly our simple class of estimators recovers the same asymptotic convergence rates as those of the $\\ell_1$-regularized MLEs that are much more difficult to compute. We corroborate this statistical performance, as well as significant computational advantages via simulations of both discrete and Gaussian graphical models.",
        "bibtex": "@inproceedings{NIPS2014_3f55669d,\n author = {Yang, Eunho and Lozano, Aur\\'{e}lie C. and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Elementary Estimators for Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3f55669d04911bca634c4c3df742b37b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3f55669d04911bca634c4c3df742b37b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3f55669d04911bca634c4c3df742b37b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3f55669d04911bca634c4c3df742b37b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3f55669d04911bca634c4c3df742b37b-Reviews.html",
        "metareview": "",
        "pdf_size": 456121,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9970360881830864295&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "IBM T.J. Watson Research Center; IBM T.J. Watson Research Center; University of Texas at Austin",
        "aff_domain": "us.ibm.com;us.ibm.com;cs.utexas.edu",
        "email": "us.ibm.com;us.ibm.com;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3f55669d04911bca634c4c3df742b37b-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "IBM;University of Texas at Austin",
        "aff_unique_dep": "Research Center;",
        "aff_unique_url": "https://www.ibm.com/research/watson;https://www.utexas.edu",
        "aff_unique_abbr": "IBM;UT Austin",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "T.J. Watson;Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4587",
        "id": "4587",
        "author_site": "Lingqiao Liu, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang",
        "author": "Lingqiao Liu; Chunhua Shen; Lei Wang; Anton van den Hengel; Chao Wang",
        "abstract": "Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians. In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems.",
        "bibtex": "@inproceedings{NIPS2014_cf0bbde5,\n author = {Liu, Lingqiao and Shen, Chunhua and Wang, Lei and van den Hengel, Anton and Wang, Chao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cf0bbde5d75ae026834db8fda900b504-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cf0bbde5d75ae026834db8fda900b504-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cf0bbde5d75ae026834db8fda900b504-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cf0bbde5d75ae026834db8fda900b504-Reviews.html",
        "metareview": "",
        "pdf_size": 275853,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5013754988032370670&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science, University of Adelaide, Australia + ARC Centre of Excellence for Robotic Vision; School of Computer Science, University of Adelaide, Australia + ARC Centre of Excellence for Robotic Vision; School of Computer Science and Software Engineering, University of Wollongong, Australia; School of Computer Science, University of Adelaide, Australia + ARC Centre of Excellence for Robotic Vision; School of Computer Science and Software Engineering, University of Wollongong, Australia",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cf0bbde5d75ae026834db8fda900b504-Abstract.html",
        "aff_unique_index": "0+1;0+1;2;0+1;2",
        "aff_unique_norm": "University of Adelaide;ARC Centre of Excellence for Robotic Vision;University of Wollongong",
        "aff_unique_dep": "School of Computer Science;;School of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.adelaide.edu.au;https://roboticvision.org/;https://www.uow.edu.au",
        "aff_unique_abbr": "Adelaide;ARC CoE for Robotic Vision;UOW",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Estimation with Norm Regularization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4588",
        "id": "4588",
        "author_site": "Arindam Banerjee, Sheng Chen, Farideh Fazayeli, Vidyashankar Sivakumar",
        "author": "Arindam Banerjee; Sheng Chen; Farideh Fazayeli; Vidyashankar Sivakumar",
        "abstract": "Analysis of estimation error and associated structured statistical recovery based on norm regularized regression, e.g., Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise vector. This paper presents generalizations of such estimation error analysis on all four aspects, compared to the existing literature. We characterize the restricted error set, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to {\\em any} norm. Precise characterizations of the bound are presented for a variety of noise vectors, design matrices, including sub-Gaussian, anisotropic, and dependent samples, and loss functions, including least squares and generalized linear models. Gaussian widths, as a measure of size of suitable sets, and associated tools play a key role in our generalized analysis.",
        "bibtex": "@inproceedings{NIPS2014_ed06bf57,\n author = {Banerjee, Arindam and Chen, Sheng and Fazayeli, Farideh and Sivakumar, Vidyashankar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimation with Norm Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ed06bf57b6e6a0ee8dbbaf3dade4f51f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ed06bf57b6e6a0ee8dbbaf3dade4f51f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ed06bf57b6e6a0ee8dbbaf3dade4f51f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ed06bf57b6e6a0ee8dbbaf3dade4f51f-Reviews.html",
        "metareview": "",
        "pdf_size": 283781,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5217177018992236381&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science & Engineering, University of Minnesota, Twin Cities; Department of Computer Science & Engineering, University of Minnesota, Twin Cities; Department of Computer Science & Engineering, University of Minnesota, Twin Cities; Department of Computer Science & Engineering, University of Minnesota, Twin Cities",
        "aff_domain": "cs.umn.edu;cs.umn.edu;cs.umn.edu;cs.umn.edu",
        "email": "cs.umn.edu;cs.umn.edu;cs.umn.edu;cs.umn.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ed06bf57b6e6a0ee8dbbaf3dade4f51f-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "Department of Computer Science & Engineering",
        "aff_unique_url": "https://www.minnesota.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Twin Cities",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Exact Post Model Selection Inference for Marginal Screening",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4589",
        "id": "4589",
        "author_site": "Jason D Lee, Jonathan E Taylor",
        "author": "Jason D. Lee; Jonathan E. Taylor",
        "abstract": "We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response $y$, conditional on the model being selected (``condition on selection framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix $X$. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit and marginal screening+Lasso.\"",
        "bibtex": "@inproceedings{NIPS2014_11e9c512,\n author = {Lee, Jason D. and Taylor, Jonathan E.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exact Post Model Selection Inference for Marginal Screening},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/11e9c51241de4f0cae8dc1b7ef3dfe3a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/11e9c51241de4f0cae8dc1b7ef3dfe3a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/11e9c51241de4f0cae8dc1b7ef3dfe3a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/11e9c51241de4f0cae8dc1b7ef3dfe3a-Reviews.html",
        "metareview": "",
        "pdf_size": 297008,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12049324749097940851&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computational and Mathematical Engineering, Stanford University; Department of Statistics, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/11e9c51241de4f0cae8dc1b7ef3dfe3a-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computational and Mathematical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Exclusive Feature Learning on Arbitrary Structures via $\\ell_{1,2}$-norm",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4590",
        "id": "4590",
        "author_site": "Deguang Kong, Ryohei Fujimaki, Ji Liu, Feiping Nie, Chris Ding",
        "author": "Deguang Kong; Ryohei Fujimaki; Ji Liu; Feiping Nie; Chris Ding",
        "abstract": "Group lasso is widely used to enforce the structural sparsity, which achieves the sparsity at inter-group level. In this paper, we propose a new formulation called ``exclusive group lasso'', which brings out sparsity at intra-group level in the context of feature selection. The proposed exclusive group lasso is applicable on any feature structures, regardless of their overlapping or non-overlapping structures. We give analysis on the properties of exclusive group lasso, and propose an effective iteratively re-weighted algorithm to solve the corresponding optimization problem with rigorous convergence analysis. We show applications of exclusive group lasso for uncorrelated feature selection. Extensive experiments on both synthetic and real-world datasets indicate the good performance of proposed methods.",
        "bibtex": "@inproceedings{NIPS2014_4faac7f2,\n author = {Kong, Deguang and Fujimaki, Ryohei and Liu, Ji and Nie, Feiping and Ding, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exclusive Feature Learning on Arbitrary Structures via \\textbackslash ell\\_\\lbrace 1,2\\rbrace -norm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4faac7f26cc36b77a2b8f1b4d7f5ecf5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4faac7f26cc36b77a2b8f1b4d7f5ecf5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4faac7f26cc36b77a2b8f1b4d7f5ecf5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4faac7f26cc36b77a2b8f1b4d7f5ecf5-Reviews.html",
        "metareview": "",
        "pdf_size": 539696,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15924119326493191934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Science, University of Texas Arlington, TX, 76019; NEC Laboratories America, Cupertino, CA, 95014; Dept. of Computer Science, University of Rochester, Rochester, NY, 14627; Dept. of Computer Science, University of Texas Arlington, TX, 76019; Dept. of Computer Science, University of Texas Arlington, TX, 76019",
        "aff_domain": "gmail.com;nec-labs.com;cs.rochester.edu;gmail.com;uta.edu",
        "email": "gmail.com;nec-labs.com;cs.rochester.edu;gmail.com;uta.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4faac7f26cc36b77a2b8f1b4d7f5ecf5-Abstract.html",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "University of Texas at Arlington;NEC Laboratories America;University of Rochester",
        "aff_unique_dep": "Department of Computer Science;;Department of Computer Science",
        "aff_unique_url": "https://www.uta.edu;https://www.nec-labs.com;https://www.rochester.edu",
        "aff_unique_abbr": "UTA;NEC Labs;U of R",
        "aff_campus_unique_index": "0;1;2;0;0",
        "aff_campus_unique": "Arlington;Cupertino;Rochester",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4591",
        "id": "4591",
        "author_site": "Daniel Soudry, Itay Hubara, Ron Meir",
        "author": "Daniel Soudry; Itay Hubara; Ron Meir",
        "abstract": "Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a \u201cmean-field\u201d factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs. Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude. We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior.",
        "bibtex": "@inproceedings{NIPS2014_c7a8ce29,\n author = {Soudry, Daniel and Hubara, Itay and Meir, Ron},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c7a8ce290377aa76aef3b8eb2c629574-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c7a8ce290377aa76aef3b8eb2c629574-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c7a8ce290377aa76aef3b8eb2c629574-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c7a8ce290377aa76aef3b8eb2c629574-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c7a8ce290377aa76aef3b8eb2c629574-Reviews.html",
        "metareview": "",
        "pdf_size": 360853,
        "gs_citation": 312,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4406652975587130679&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, Columbia University; Department of Electrical Engineering, Technion, Israel Institute of Technology; Department of Electrical Engineering, Technion, Israel Institute of Technology",
        "aff_domain": "gmail.com;gmail.com;ee.technion.ac.il",
        "email": "gmail.com;gmail.com;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c7a8ce290377aa76aef3b8eb2c629574-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Columbia University;Technion, Israel Institute of Technology",
        "aff_unique_dep": "Department of Statistics;Department of Electrical Engineering",
        "aff_unique_url": "https://www.columbia.edu;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Columbia;Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "Expectation-Maximization for Learning Determinantal Point Processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4592",
        "id": "4592",
        "author_site": "Jennifer A Gillenwater, Alex Kulesza, Emily Fox, Ben Taskar",
        "author": "Jennifer Gillenwater; Alex Kulesza; Emily Fox; Ben Taskar",
        "abstract": "A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix, or learning weights for a linear combination of DPPs with fixed kernel matrices. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.",
        "bibtex": "@inproceedings{NIPS2014_89c5a9c6,\n author = {Gillenwater, Jennifer and Kulesza, Alex and Fox, Emily and Taskar, Ben},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Expectation-Maximization for Learning Determinantal Point Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/89c5a9c69a6506f75b67712438f87996-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/89c5a9c69a6506f75b67712438f87996-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/89c5a9c69a6506f75b67712438f87996-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/89c5a9c69a6506f75b67712438f87996-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/89c5a9c69a6506f75b67712438f87996-Reviews.html",
        "metareview": "",
        "pdf_size": 1830199,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3033197147216291261&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Computer and Information Science, University of Pennsylvania; Computer Science and Engineering, University of Michigan; Statistics, University of Washington; Computer Science and Engineering, University of Washington",
        "aff_domain": "cis.upenn.edu;umich.edu;stat.washington.edu;cs.washington.edu",
        "email": "cis.upenn.edu;umich.edu;stat.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/89c5a9c69a6506f75b67712438f87996-Abstract.html",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "University of Pennsylvania;University of Michigan;University of Washington",
        "aff_unique_dep": "Computer and Information Science;Computer Science and Engineering;Department of Statistics",
        "aff_unique_url": "https://www.upenn.edu;https://www.umich.edu;https://www.washington.edu",
        "aff_unique_abbr": "UPenn;UM;UW",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";Ann Arbor;Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4593",
        "id": "4593",
        "author_site": "Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, Rob Fergus",
        "author": "Emily Denton; Wojciech Zaremba; Joan Bruna; Yann LeCun; Rob Fergus",
        "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2\u00d7, while keeping the accuracy within 1% of the original model.",
        "bibtex": "@inproceedings{NIPS2014_1adaeb99,\n author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1adaeb993eba95859121a43ea61bd858-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1adaeb993eba95859121a43ea61bd858-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/1adaeb993eba95859121a43ea61bd858-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1adaeb993eba95859121a43ea61bd858-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1adaeb993eba95859121a43ea61bd858-Reviews.html",
        "metareview": "",
        "pdf_size": 1506159,
        "gs_citation": 2191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11064922358338176957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Science, Courant Institute, New York University; Dept. of Computer Science, Courant Institute, New York University; Dept. of Computer Science, Courant Institute, New York University; Dept. of Computer Science, Courant Institute, New York University; Dept. of Computer Science, Courant Institute, New York University",
        "aff_domain": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1adaeb993eba95859121a43ea61bd858-Abstract.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Exploiting easy data in online optimization",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4384",
        "id": "4384",
        "author_site": "Amir Sani, Gergely Neu, Alessandro Lazaric",
        "author": "Amir Sani; Gergely Neu; Alessandro Lazaric",
        "abstract": "We consider the problem of online optimization, where a learner chooses a decision from a given decision set and suffers some loss associated with the decision and the state of the environment. The learner's objective is to minimize its cumulative regret against the best fixed decision in hindsight. Over the past few decades numerous variants have been considered, with many algorithms designed to achieve sub-linear regret in the worst case. However, this level of robustness comes at a cost. Proposed algorithms are often over-conservative, failing to adapt to the actual complexity of the loss sequence which is often far from the worst case. In this paper we introduce a general algorithm that, provided with a safe learning algorithm and an opportunistic benchmark, can effectively combine good worst-case guarantees with much improved performance on easy data. We derive general theoretical bounds on the regret of the proposed algorithm and discuss its implementation in a wide range of applications, notably in the problem of learning with shifting experts (a recent COLT open problem). Finally, we provide numerical simulations in the setting of prediction with expert advice with comparisons to the state of the art.",
        "bibtex": "@inproceedings{NIPS2014_8cfb39d9,\n author = {Sani, Amir and Neu, Gergely and Lazaric, Alessandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exploiting easy data in online optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8cfb39d9174128beb141866808bd154e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8cfb39d9174128beb141866808bd154e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8cfb39d9174128beb141866808bd154e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8cfb39d9174128beb141866808bd154e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8cfb39d9174128beb141866808bd154e-Reviews.html",
        "metareview": "",
        "pdf_size": 451181,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12053291885492305754&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "SequeL team, INRIA Lille \u2013 Nord Europe, France; SequeL team, INRIA Lille \u2013 Nord Europe, France; SequeL team, INRIA Lille \u2013 Nord Europe, France",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8cfb39d9174128beb141866808bd154e-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA Lille \u2013 Nord Europe",
        "aff_unique_dep": "SequeL team",
        "aff_unique_url": "https://www.inria.fr/en/centre/lille-nord-europe",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Exponential Concentration of a Density Functional Estimator",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4594",
        "id": "4594",
        "author_site": "Shashank Singh, Barnabas Poczos",
        "author": "Shashank Singh; Barnab\u00e1s P\u00f3czos",
        "abstract": "We analyse a plug-in estimator for a large class of integral functionals of one or more continuous probability densities. This class includes important families of entropy, divergence, mutual information, and their conditional versions. For densities on the d-dimensional unit cube [0,1]^d that lie in a beta-Holder smoothness class, we prove our estimator converges at the rate O(n^(1/(beta+d))). Furthermore, we prove that the estimator obeys an exponential concentration inequality about its mean, whereas most previous related results have bounded only expected error of estimators. Finally, we demonstrate our bounds to the case of conditional Renyi mutual information.",
        "bibtex": "@inproceedings{NIPS2014_1b4d1297,\n author = {Singh, Shashank and P\\'{o}czos, Barnab\\'{a}s},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exponential Concentration of a Density Functional Estimator},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1b4d1297f046956c58ea594238948e16-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1b4d1297f046956c58ea594238948e16-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1b4d1297f046956c58ea594238948e16-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1b4d1297f046956c58ea594238948e16-Reviews.html",
        "metareview": "",
        "pdf_size": 295505,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14269509426216539244&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Statistics & Machine Learning Departments, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cs.cmu.edu",
        "email": "andrew.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1b4d1297f046956c58ea594238948e16-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Statistics & Machine Learning Departments",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Extended and Unscented Gaussian Processes",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4386",
        "id": "4386",
        "author_site": "Daniel M Steinberg, Edwin Bonilla",
        "author": "Daniel M. Steinberg; Edwin V. Bonilla",
        "abstract": "We present two new methods for inference in Gaussian process (GP) models with general nonlinear likelihoods. Inference is based on a variational framework where a Gaussian posterior is assumed and the likelihood is linearized about the variational posterior mean using either a Taylor series expansion or statistical linearization. We show that the parameter updates obtained by these algorithms are equivalent to the state update equations in the iterative extended and unscented Kalman filters respectively, hence we refer to our algorithms as extended and unscented GPs. The unscented GP treats the likelihood as a 'black-box' by not requiring its derivative for inference, so it also applies to non-differentiable likelihood models. We evaluate the performance of our algorithms on a number of synthetic inversion problems and a binary classification dataset.",
        "bibtex": "@inproceedings{NIPS2014_8d3a49c7,\n author = {Steinberg, Daniel M. and Bonilla, Edwin V.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extended and Unscented Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8d3a49c76d058fc2423a1e6e2f1879d3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8d3a49c76d058fc2423a1e6e2f1879d3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8d3a49c76d058fc2423a1e6e2f1879d3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8d3a49c76d058fc2423a1e6e2f1879d3-Reviews.html",
        "metareview": "",
        "pdf_size": 460832,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13972809691760979405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "NICTA; The University of New South Wales",
        "aff_domain": "nicta.com.au;unsw.edu.au",
        "email": "nicta.com.au;unsw.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8d3a49c76d058fc2423a1e6e2f1879d3-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "National Information and Communications Technology Australia;University of New South Wales",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nicta.com.au;https://www.unsw.edu.au",
        "aff_unique_abbr": "NICTA;UNSW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Extracting Certainty from Uncertainty: Transductive Pairwise Classification from Pairwise Similarities",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4595",
        "id": "4595",
        "author_site": "Tianbao Yang, Rong Jin",
        "author": "Tianbao Yang; Rong Jin",
        "abstract": "In this work, we study the problem of transductive pairwise classification from pairwise similarities~\\footnote{The pairwise similarities are usually derived from some side information instead of the underlying class labels.}. The goal of transductive pairwise classification from pairwise similarities is to infer the pairwise class relationships, to which we refer as pairwise labels, between all examples given a subset of class relationships for a small set of examples, to which we refer as labeled examples. We propose a very simple yet effective algorithm that consists of two simple steps: the first step is to complete the sub-matrix corresponding to the labeled examples and the second step is to reconstruct the label matrix from the completed sub-matrix and the provided similarity matrix. Our analysis exhibits that under several mild preconditions we can recover the label matrix with a small error, if the top eigen-space that corresponds to the largest eigenvalues of the similarity matrix covers well the column space of label matrix and is subject to a low coherence, and the number of observed pairwise labels is sufficiently enough. We demonstrate the effectiveness of the proposed algorithm by several experiments.",
        "bibtex": "@inproceedings{NIPS2014_8e825b56,\n author = {Yang, Tianbao and Jin, Rong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extracting Certainty from Uncertainty: Transductive Pairwise Classification from Pairwise Similarities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8e825b56a1b466473a06bbb6a0aaddc9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8e825b56a1b466473a06bbb6a0aaddc9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8e825b56a1b466473a06bbb6a0aaddc9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8e825b56a1b466473a06bbb6a0aaddc9-Reviews.html",
        "metareview": "",
        "pdf_size": 1580129,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8221707472263601941&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "The University of Iowa; Michigan State University + Alibaba Group",
        "aff_domain": "uiowa.edu;msu.edu",
        "email": "uiowa.edu;msu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8e825b56a1b466473a06bbb6a0aaddc9-Abstract.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "University of Iowa;Michigan State University;Alibaba Group",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uiowa.edu;https://www.msu.edu;https://www.alibaba.com",
        "aff_unique_abbr": "UIowa;MSU;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Extracting Latent Structure From Multiple Interacting Neural Populations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4596",
        "id": "4596",
        "author_site": "Joao Semedo, Amin Zandvakili, Adam Kohn, Christian Machens, Byron M Yu",
        "author": "Jo\u00e3o D. Semedo; Amin Zandvakili; Adam Kohn; Christian K. Machens; Byron M. Yu",
        "abstract": "Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded (e.g., excitatory vs. inhibitory). There is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons. Rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared), we propose to extract a smaller number of latent variables from each population and study how the latent variables interact. Specifically, we propose extensions to probabilistic canonical correlation analysis (pCCA) to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions (termed Group Latent Auto-Regressive Analysis, gLARA). We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2, and found that gLARA provides a better description of the recordings than pCCA. This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.",
        "bibtex": "@inproceedings{NIPS2014_0e7f2179,\n author = {Semedo, Jo\\~{a}o D. and Zandvakili, Amin and Kohn, Adam and Machens, Christian K. and Yu, Byron M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extracting Latent Structure From Multiple Interacting Neural Populations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0e7f2179300fe21031b938a265a39409-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0e7f2179300fe21031b938a265a39409-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0e7f2179300fe21031b938a265a39409-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0e7f2179300fe21031b938a265a39409-Reviews.html",
        "metareview": "",
        "pdf_size": 448808,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16579819817429139208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical and Computer Engineering, Carnegie Mellon University + Department of Electrical and Computer Engineering, Instituto Superior T\u00e9cnico + Champalimaud Neuroscience Programme, Champalimaud Center for the Unknown; Dominick Purpura Department of Neuroscience, Albert Einstein College of Medicine; Dominick Purpura Department of Neuroscience, Albert Einstein College of Medicine; Champalimaud Neuroscience Programme, Champalimaud Center for the Unknown; Department of Electrical and Computer Engineering, Carnegie Mellon University + Department of Biomedical Engineering, Carnegie Mellon University",
        "aff_domain": "cmu.edu;einstein.yu.edu;einstein.yu.edu;neuro.fchampalimaud.org;cmu.edu",
        "email": "cmu.edu;einstein.yu.edu;einstein.yu.edu;neuro.fchampalimaud.org;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0e7f2179300fe21031b938a265a39409-Abstract.html",
        "aff_unique_index": "0+1+2;3;3;2;0+0",
        "aff_unique_norm": "Carnegie Mellon University;Instituto Superior T\u00e9cnico;Champalimaud Center for the Unknown;Albert Einstein College of Medicine",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Electrical and Computer Engineering;Champalimaud Neuroscience Programme;Dominick Purpura Department of Neuroscience",
        "aff_unique_url": "https://www.cmu.edu;https://www.ist.utl.pt;https://www.champalimaud.org;https://www.einsteinmed.org",
        "aff_unique_abbr": "CMU;IST;CCU;AECOM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0+1+1;0;0;1;0+0",
        "aff_country_unique": "United States;Portugal"
    },
    {
        "title": "Extremal Mechanisms for Local Differential Privacy",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4597",
        "id": "4597",
        "author_site": "Peter Kairouz, Sewoong Oh, Pramod Viswanath",
        "author": "Peter Kairouz; Sewoong Oh; Pramod Viswanath",
        "abstract": "Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where the data providers and data analysts want to maximize the utility of statistical inferences performed on the released data, we study the fundamental tradeoff between local differential privacy and information theoretic utility functions. We introduce a family of extremal privatization mechanisms, which we call staircase mechanisms, and prove that it contains the optimal privatization mechanism that maximizes utility. We further show that for all information theoretic utility functions studied in this paper, maximizing utility is equivalent to solving a linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the data size. To account for this, we show that two simple staircase mechanisms, the binary and randomized response mechanisms, are universally optimal in the high and low privacy regimes, respectively, and well approximate the intermediate regime.",
        "bibtex": "@inproceedings{NIPS2014_c16cf23d,\n author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extremal Mechanisms for Local Differential Privacy},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c16cf23dd72c445d3050d0fcd3f28728-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c16cf23dd72c445d3050d0fcd3f28728-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c16cf23dd72c445d3050d0fcd3f28728-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c16cf23dd72c445d3050d0fcd3f28728-Reviews.html",
        "metareview": "",
        "pdf_size": 293353,
        "gs_citation": 516,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8076411212965878170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Electrical & Computer Engineering; Department of Industrial & Enterprise Systems Engineering; Department of Electrical & Computer Engineering",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c16cf23dd72c445d3050d0fcd3f28728-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Institution not specified;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Electrical & Computer Engineering;Department of Industrial & Enterprise Systems Engineering",
        "aff_unique_url": ";https://ie.sysengr.illinois.edu/",
        "aff_unique_abbr": ";UIUC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Extreme bandits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4598",
        "id": "4598",
        "author_site": "Alexandra Carpentier, Michal Valko",
        "author": "Alexandra Carpentier; Michal Valko",
        "abstract": "In many areas of medicine, security, and life sciences, we want to allocate limited resources to different sources in order to detect extreme values. In this paper, we study an efficient way to allocate these resources sequentially under limited feedback. While sequential design of experiments is well studied in bandit theory, the most commonly optimized property is the regret with respect to the maximum mean reward. However, in other problems such as network intrusion detection, we are interested in detecting the most extreme value output by the sources. Therefore, in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail. We propose the ExtremeHunter algorithm, provide its analysis, and evaluate it empirically on synthetic and real-world experiments.",
        "bibtex": "@inproceedings{NIPS2014_16577b42,\n author = {Carpentier, Alexandra and Valko, Michal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extreme bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/16577b42c2a7b2820435b84f2f5389ff-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/16577b42c2a7b2820435b84f2f5389ff-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/16577b42c2a7b2820435b84f2f5389ff-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/16577b42c2a7b2820435b84f2f5389ff-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/16577b42c2a7b2820435b84f2f5389ff-Reviews.html",
        "metareview": "",
        "pdf_size": 334349,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2363807495166990202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Statistical Laboratory, CMS, University of Cambridge, UK; SequeL team, INRIA Lille - Nord Europe, France",
        "aff_domain": "statslab.cam.ac.uk;inria.fr",
        "email": "statslab.cam.ac.uk;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/16577b42c2a7b2820435b84f2f5389ff-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;INRIA Lille - Nord Europe",
        "aff_unique_dep": "Statistical Laboratory;SequeL team",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.inria.fr/en/centre/lille-nord-europe",
        "aff_unique_abbr": "Cambridge;INRIA",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Lille",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "title": "Factoring Variations in Natural Images with Deep Gaussian Mixture Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4599",
        "id": "4599",
        "author_site": "Aaron van den Oord, Benjamin Schrauwen",
        "author": "A\u00e4ron van den Oord; Benjamin Schrauwen",
        "abstract": "Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their success in supervised learning. However, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale. As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model and the Student-t Mixture Model, that remain surprisingly competitive. In this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model, that is a straightforward but powerful generalization of GMMs to multiple layers. The parametrization of a Deep GMM allows it to efficiently capture products of variations in natural images. We propose a new EM-based algorithm that scales well to large datasets, and we show that both the Expectation and the Maximization steps can easily be distributed over multiple machines. In our density estimation experiments we show that deeper GMM architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.",
        "bibtex": "@inproceedings{NIPS2014_f509576e,\n author = {van den Oord, A\\\"{a}ron and Schrauwen, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Factoring Variations in Natural Images with Deep Gaussian Mixture Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f509576ef1e4ebe32746b1203e2d0ee3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f509576ef1e4ebe32746b1203e2d0ee3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f509576ef1e4ebe32746b1203e2d0ee3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f509576ef1e4ebe32746b1203e2d0ee3-Reviews.html",
        "metareview": "",
        "pdf_size": 676102,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7909162201967103517&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Electronics and Information Systems department (ELIS), Ghent University; Electronics and Information Systems department (ELIS), Ghent University",
        "aff_domain": "ugent.be;ugent.be",
        "email": "ugent.be;ugent.be",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f509576ef1e4ebe32746b1203e2d0ee3-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ghent University",
        "aff_unique_dep": "Electronics and Information Systems department (ELIS)",
        "aff_unique_url": "https://www.ugent.be",
        "aff_unique_abbr": "UGent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "title": "Fairness in Multi-Agent Sequential Decision-Making",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4600",
        "id": "4600",
        "author_site": "Chongjie Zhang, Julie A Shah",
        "author": "Chongjie Zhang; Julie A. Shah",
        "abstract": "We define a fairness solution criterion for multi-agent decision-making problems, where agents have local interests. This new criterion aims to maximize the worst performance of agents with consideration on the overall performance. We develop a simple linear programming approach and a more scalable game-theoretic approach for computing an optimal fairness policy. This game-theoretic approach formulates this fairness optimization as a two-player, zero-sum game and employs an iterative algorithm for finding a Nash equilibrium, corresponding to an optimal fairness policy. We scale up this approach by exploiting problem structure and value function approximation. Our experiments on resource allocation problems show that this fairness criterion provides a more favorable solution than the utilitarian criterion, and that our game-theoretic approach is significantly faster than linear programming.",
        "bibtex": "@inproceedings{NIPS2014_5556d1d6,\n author = {Zhang, Chongjie and Shah, Julie A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fairness in Multi-Agent Sequential Decision-Making},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5556d1d6ca0d004accf36cc2db73e736-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5556d1d6ca0d004accf36cc2db73e736-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5556d1d6ca0d004accf36cc2db73e736-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5556d1d6ca0d004accf36cc2db73e736-Reviews.html",
        "metareview": "",
        "pdf_size": 237302,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14477751850638893523&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5556d1d6ca0d004accf36cc2db73e736-Abstract.html"
    },
    {
        "title": "Fast Kernel Learning for Multidimensional Pattern Extrapolation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4601",
        "id": "4601",
        "author_site": "Andrew Wilson, Elad Gilboa, John P Cunningham, Arye Nehorai",
        "author": "Andrew Gordon Wilson; Elad Gilboa; Arye Nehorai; John P. Cunningham",
        "abstract": "The ability to automatically discover patterns and perform extrapolation is an essential quality of intelligent systems. Kernel methods, such as Gaussian processes, have great potential for pattern extrapolation, since the kernel flexibly and interpretably controls the generalisation properties of these methods. However, automatically extrapolating large scale multidimensional patterns is in general difficult, and developing Gaussian process models for this purpose involves several challenges. A vast majority of kernels, and kernel learning methods, currently only succeed in smoothing and interpolation. This difficulty is compounded by the fact that Gaussian processes are typically only tractable for small datasets, and scaling an expressive kernel learning approach poses different challenges than scaling a standard Gaussian process model. One faces additional computational constraints, and the need to retain significant model structure for expressing the rich information available in a large dataset. In this paper, we propose a Gaussian process approach for large scale multidimensional pattern extrapolation. We recover sophisticated out of class kernels, perform texture extrapolation, inpainting, and video extrapolation, and long range forecasting of land surface temperatures, all on large multidimensional datasets, including a problem with 383,400 training points. The proposed method significantly outperforms alternative scalable and flexible Gaussian process methods, in speed and accuracy. Moreover, we show that a distinct combination of expressive kernels, a fully non-parametric representation, and scalable inference which exploits existing model structure, are critical for large scale multidimensional pattern extrapolation.",
        "bibtex": "@inproceedings{NIPS2014_f1b9324d,\n author = {Wilson, Andrew Gordon and Gilboa, Elad and Nehorai, Arye and Cunningham, John P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Kernel Learning for Multidimensional Pattern Extrapolation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f1b9324dd8d5843502953afb5efa289e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f1b9324dd8d5843502953afb5efa289e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f1b9324dd8d5843502953afb5efa289e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f1b9324dd8d5843502953afb5efa289e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f1b9324dd8d5843502953afb5efa289e-Reviews.html",
        "metareview": "",
        "pdf_size": 1177834,
        "gs_citation": 196,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17247917210779543051&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "CMU; WUSTL; WUSTL; Columbia",
        "aff_domain": "cmu.edu;wustl.edu;wustl.edu;stat.columbia.edu",
        "email": "cmu.edu;wustl.edu;wustl.edu;stat.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f1b9324dd8d5843502953afb5efa289e-Abstract.html",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Carnegie Mellon University;Washington University in St. Louis;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.wustl.edu;https://www.columbia.edu",
        "aff_unique_abbr": "CMU;WUSTL;Columbia",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";St. Louis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4388",
        "id": "4388",
        "author_site": "Mohammad Taha Bahadori, Qi (Rose) Yu, Yan Liu",
        "author": "Mohammad Taha Bahadori; Qi (Rose) Yu; Yan Liu",
        "abstract": "Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.",
        "bibtex": "@inproceedings{NIPS2014_589906c7,\n author = {Bahadori, Mohammad Taha and Yu, Qi (Rose) and Liu, Yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/589906c76dcc1ac3bf3a7dc3d7a56ec8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/589906c76dcc1ac3bf3a7dc3d7a56ec8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/589906c76dcc1ac3bf3a7dc3d7a56ec8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/589906c76dcc1ac3bf3a7dc3d7a56ec8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/589906c76dcc1ac3bf3a7dc3d7a56ec8-Reviews.html",
        "metareview": "",
        "pdf_size": 332311,
        "gs_citation": 234,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2597464758702573712&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Dept. of Electrical Engineering, Univ. of Southern California; Dept. of Computer Science, Univ. of Southern California; Dept. of Computer Science, Univ. of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/589906c76dcc1ac3bf3a7dc3d7a56ec8-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Dept. of Electrical Engineering",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast Prediction for Large-Scale Kernel Machines",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4602",
        "id": "4602",
        "author_site": "Cho-Jui Hsieh, Si Si, Inderjit Dhillon",
        "author": "Cho-Jui Hsieh; Si Si; Inderjit S. Dhillon",
        "abstract": "Kernel machines such as kernel SVM and kernel ridge regression usually construct high quality models; however, their use in real-world applications remains limited due to the high prediction cost. In this paper, we present two novel insights for improving the prediction efficiency of kernel machines. First, we show that by adding \u201cpseudo landmark points\u201d to the classical Nystr\u00a8om kernel approximation in an elegant way, we can significantly reduce the prediction error without much additional prediction cost. Second, we provide a new theoretical analysis on bounding the error of the solution computed by using Nystr\u00a8om kernel approximation method, and show that the error is related to the weighted kmeans objective function where the weights are given by the model computed from the original kernel. This theoretical insight suggests a new landmark point selection technique for the situation where we have knowledge of the original model. Based on these two insights, we provide a divide-and-conquer framework for improving the prediction speed. First, we divide the whole problem into smaller local subproblems to reduce the problem size. In the second phase, we develop a kernel approximation based fast prediction approach within each subproblem. We apply our algorithm to real world large-scale classification and regression datasets, and show that the proposed algorithm is consistently and significantly better than other competitors. For example, on the Covertype classification problem, in terms of prediction time, our algorithm achieves more than 10000 times speedup over the full kernel SVM, and a two-fold speedup over the state-of-the-art LDKL approach, while obtaining much higher prediction accuracy than LDKL (95.2% vs. 89.53%).",
        "bibtex": "@inproceedings{NIPS2014_cfe83d5a,\n author = {Hsieh, Cho-Jui and Si, Si and Dhillon, Inderjit S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Prediction for Large-Scale Kernel Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cfe83d5a2349cca33a795bb5e1dc67cb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cfe83d5a2349cca33a795bb5e1dc67cb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/cfe83d5a2349cca33a795bb5e1dc67cb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cfe83d5a2349cca33a795bb5e1dc67cb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cfe83d5a2349cca33a795bb5e1dc67cb-Reviews.html",
        "metareview": "",
        "pdf_size": 244718,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16912382149632675803&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cfe83d5a2349cca33a795bb5e1dc67cb-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast Sampling-Based Inference in Balanced Neuronal Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4603",
        "id": "4603",
        "author_site": "Guillaume Hennequin, Laurence Aitchison, Mate Lengyel",
        "author": "Guillaume Hennequin; Laurence Aitchison; M\u00e1t\u00e9 Lengyel",
        "abstract": "Multiple lines of evidence support the notion that the brain performs probabilistic inference in multiple cognitive domains, including perception and decision making. There is also evidence that probabilistic inference may be implemented in the brain through the (quasi-)stochastic activity of neural circuits, producing samples from the appropriate posterior distributions, effectively implementing a Markov chain Monte Carlo algorithm. However, time becomes a fundamental bottleneck in such sampling-based probabilistic representations: the quality of inferences depends on how fast the neural circuit generates new, uncorrelated samples from its stationary distribution (the posterior). We explore this bottleneck in a simple, linear-Gaussian latent variable model, in which posterior sampling can be achieved by stochastic neural networks with linear dynamics. The well-known Langevin sampling (LS) recipe, so far the only sampling algorithm for continuous variables of which a neural implementation has been suggested, naturally fits into this dynamical framework. However, we first show analytically and through simulations that the symmetry of the synaptic weight matrix implied by LS yields critically slow mixing when the posterior is high-dimensional. Next, using methods from control theory, we construct and inspect networks that are optimally fast, and hence orders of magnitude faster than LS, while being far more biologically plausible. In these networks, strong -- but transient -- selective amplification of external noise generates the spatially correlated activity fluctuations prescribed by the posterior. Intriguingly, although a detailed balance of excitation and inhibition is dynamically maintained, detailed balance of Markov chain steps in the resulting sampler is violated, consistent with recent findings on how statistical irreversibility can overcome the speed limitation of random walks in other domains.",
        "bibtex": "@inproceedings{NIPS2014_604ca44b,\n author = {Hennequin, Guillaume and Aitchison, Laurence and Lengyel, M\\'{a}t\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Sampling-Based Inference in Balanced Neuronal Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/604ca44b3dd6eaa7f3f74a71aaf0c596-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/604ca44b3dd6eaa7f3f74a71aaf0c596-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/604ca44b3dd6eaa7f3f74a71aaf0c596-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/604ca44b3dd6eaa7f3f74a71aaf0c596-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/604ca44b3dd6eaa7f3f74a71aaf0c596-Reviews.html",
        "metareview": "",
        "pdf_size": 849118,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16682802236791828116&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK; Gatsby Computational Neuroscience Unit, University College London, UK; Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK",
        "aff_domain": "cam.ac.uk;gatsby.ucl.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;gatsby.ucl.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/604ca44b3dd6eaa7f3f74a71aaf0c596-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Cambridge;University College London",
        "aff_unique_dep": "Dept. of Engineering;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Cambridge;UCL",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Fast Training of Pose Detectors in the Fourier Domain",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4604",
        "id": "4604",
        "author_site": "Jo\u00e3o Henriques, Pedro Martins, Rui F Caseiro, Jorge Batista",
        "author": "Jo\u00e3o F. Henriques; Pedro Martins; Rui Caseiro; Jorge Batista",
        "abstract": "In many datasets, the samples are related by a known image transformation, such as rotation, or a repeatable non-rigid deformation. This applies to both datasets with the same objects under different viewpoints, and datasets augmented with virtual samples. Such datasets possess a high degree of redundancy, because geometrically-induced transformations should preserve intrinsic properties of the objects. Likewise, ensembles of classifiers used for pose estimation should also share many characteristics, since they are related by a geometric transformation. By assuming that this transformation is norm-preserving and cyclic, we propose a closed-form solution in the Fourier domain that can eliminate most redundancies. It can leverage off-the-shelf solvers with no modification (e.g. libsvm), and train several pose classifiers simultaneously at no extra cost. Our experiments show that training a sliding-window object detector and pose estimator can be sped up by orders of magnitude, for transformations as diverse as planar rotation, the walking motion of pedestrians, and out-of-plane rotations of cars.",
        "bibtex": "@inproceedings{NIPS2014_abbe3453,\n author = {Henriques, Jo\\~{a}o F. and Martins, Pedro and Caseiro, Rui and Batista, Jorge},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Training of Pose Detectors in the Fourier Domain},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/abbe345389dbe642adfd5aa15f095749-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/abbe345389dbe642adfd5aa15f095749-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/abbe345389dbe642adfd5aa15f095749-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/abbe345389dbe642adfd5aa15f095749-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/abbe345389dbe642adfd5aa15f095749-Reviews.html",
        "metareview": "",
        "pdf_size": 1227637,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11206111717412482400&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Institute of Systems and Robotics; Institute of Systems and Robotics; Institute of Systems and Robotics; Institute of Systems and Robotics",
        "aff_domain": "isr.uc.pt;isr.uc.pt;isr.uc.pt;isr.uc.pt",
        "email": "isr.uc.pt;isr.uc.pt;isr.uc.pt;isr.uc.pt",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/abbe345389dbe642adfd5aa15f095749-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Institute of Systems and Robotics",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Fast and Robust Least Squares Estimation in Corrupted Linear Models",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4390",
        "id": "4390",
        "author_site": "Brian McWilliams, Gabriel Krummenacher, Mario Lucic, Joachim M Buhmann",
        "author": "Brian McWilliams; Gabriel Krummenacher; Mario Lucic; Joachim M. Buhmann",
        "abstract": "Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates. The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence -- for which we also develop a randomized approximation -- motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.",
        "bibtex": "@inproceedings{NIPS2014_a7928e79,\n author = {McWilliams, Brian and Krummenacher, Gabriel and Lucic, Mario and Buhmann, Joachim M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast and Robust Least Squares Estimation in Corrupted Linear Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a7928e79281cd54ad83c93b3bcb986be-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a7928e79281cd54ad83c93b3bcb986be-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a7928e79281cd54ad83c93b3bcb986be-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a7928e79281cd54ad83c93b3bcb986be-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a7928e79281cd54ad83c93b3bcb986be-Reviews.html",
        "metareview": "",
        "pdf_size": 782424,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13752619544734841307&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, ETH Z\u00fcrich, Switzerland; Department of Computer Science, ETH Z\u00fcrich, Switzerland; Department of Computer Science, ETH Z\u00fcrich, Switzerland; Department of Computer Science, ETH Z\u00fcrich, Switzerland",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a7928e79281cd54ad83c93b3bcb986be-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Feature Cross-Substitution in Adversarial Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4605",
        "id": "4605",
        "author_site": "Bo Li, Yevgeniy Vorobeychik",
        "author": "Bo Li; Yevgeniy Vorobeychik",
        "abstract": "The success of machine learning, particularly in supervised settings, has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static data generators, but make a deliberate effort to evade the classifiers deployed to detect them. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. In particular, we demonstrate severe shortcomings of feature reduction in adversarial settings using several natural adversarial objective functions, an observation that is particularly pronounced when the adversary is able to substitute across similar features (for example, replace words with synonyms or replace letters in words). We offer a simple heuristic method for making learning more robust to feature cross-substitution attacks. We then present a more general approach based on mixed-integer linear programming with constraint generation, which implicitly trades off overfitting and feature selection in an adversarial setting using a sparse regularizer along with an evasion model. Our approach is the first method for combining an adversarial classification algorithm with a very general class of models of adversarial classifier evasion. We show that our algorithmic approach significantly outperforms state-of-the-art alternatives.",
        "bibtex": "@inproceedings{NIPS2014_234037af,\n author = {Li, Bo and Vorobeychik, Yevgeniy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feature Cross-Substitution in Adversarial Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/234037af73bfcdefaf7b65426bd5a295-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/234037af73bfcdefaf7b65426bd5a295-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/234037af73bfcdefaf7b65426bd5a295-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/234037af73bfcdefaf7b65426bd5a295-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/234037af73bfcdefaf7b65426bd5a295-Reviews.html",
        "metareview": "",
        "pdf_size": 2803378,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5059399099962435095&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Electrical Engineering and Computer Science, Vanderbilt University; Electrical Engineering and Computer Science, Vanderbilt University",
        "aff_domain": "vanderbilt.edu;vanderbilt.edu",
        "email": "vanderbilt.edu;vanderbilt.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/234037af73bfcdefaf7b65426bd5a295-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Vanderbilt University",
        "aff_unique_dep": "Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.vanderbilt.edu",
        "aff_unique_abbr": "Vanderbilt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Feedback Detection for Live Predictors",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4606",
        "id": "4606",
        "author_site": "Stefan Wager, Nick Chamandy, Omkar Muralidharan, Amir Najmi",
        "author": "Stefan Wager; Nick Chamandy; Omkar Muralidharan; Amir Najmi",
        "abstract": "A predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. In this paper we analyze predictor feedback detection as a causal inference problem, and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems. We conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine.",
        "bibtex": "@inproceedings{NIPS2014_7862f08c,\n author = {Wager, Stefan and Chamandy, Nick and Muralidharan, Omkar and Najmi, Amir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feedback Detection for Live Predictors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7862f08c6239a30915a1c9c06fdd4c9b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7862f08c6239a30915a1c9c06fdd4c9b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7862f08c6239a30915a1c9c06fdd4c9b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7862f08c6239a30915a1c9c06fdd4c9b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7862f08c6239a30915a1c9c06fdd4c9b-Reviews.html",
        "metareview": "",
        "pdf_size": 324290,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10430758899050511563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Stanford University + Google, Inc.; Google, Inc.; Google, Inc.; Google, Inc.",
        "aff_domain": "stanford.edu;google.com;google.com;google.com",
        "email": "stanford.edu;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7862f08c6239a30915a1c9c06fdd4c9b-Abstract.html",
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "Stanford University;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.stanford.edu;https://www.google.com",
        "aff_unique_abbr": "Stanford;Google",
        "aff_campus_unique_index": "0+1;1;1;1",
        "aff_campus_unique": "Stanford;Mountain View",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Feedforward Learning of Mixture Models",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4332",
        "id": "4332",
        "author_site": "Matthew Lawlor, Steven W Zucker",
        "author": "Matthew Lawlor; Steven W. Zucker",
        "abstract": "We develop a biologically-plausible learning rule that provably converges to the class means of general mixture models. This rule generalizes the classical BCM neural rule within a tensor framework, substantially increasing the generality of the learning problem it solves. It achieves this by incorporating triplets of samples from the mixtures, which provides a novel information processing interpretation to spike-timing-dependent plasticity. We provide both proofs of convergence, and a close fit to experimental data on STDP.",
        "bibtex": "@inproceedings{NIPS2014_253f0c4f,\n author = {Lawlor, Matthew and Zucker, Steven W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feedforward Learning of Mixture Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/253f0c4f7b19222b9059d1ae115e05b8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/253f0c4f7b19222b9059d1ae115e05b8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/253f0c4f7b19222b9059d1ae115e05b8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/253f0c4f7b19222b9059d1ae115e05b8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/253f0c4f7b19222b9059d1ae115e05b8-Reviews.html",
        "metareview": "",
        "pdf_size": 566368,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7658398449075427772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Applied Math, Yale University; Computer Science, Yale University",
        "aff_domain": "gmail.com;cs.yale.edu",
        "email": "gmail.com;cs.yale.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/253f0c4f7b19222b9059d1ae115e05b8-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New Haven",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4607",
        "id": "4607",
        "author_site": "Qing Qu, Ju Sun, John Wright",
        "author": "Qing Qu; Ju Sun; John Wright",
        "abstract": "We consider the problem of recovering the sparsest vector in a subspace $ \\mathcal{S} \\in \\mathbb{R}^p $ with $ \\text{dim}(\\mathcal{S})=n$. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $1/ \\sqrt{n}$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\\Omega(1)$. To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.",
        "bibtex": "@inproceedings{NIPS2014_419e33cf,\n author = {Qu, Qing and Sun, Ju and Wright, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Finding a sparse vector in a subspace: Linear sparsity using alternating directions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/419e33cfb3aebab7d7d4ed588f3e61c1-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/419e33cfb3aebab7d7d4ed588f3e61c1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/419e33cfb3aebab7d7d4ed588f3e61c1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/419e33cfb3aebab7d7d4ed588f3e61c1-Reviews.html",
        "metareview": "",
        "pdf_size": 1162974,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5280990097889392278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Dept. of Electrical Engineering, Columbia University, New York City, NY, USA, 10027; Dept. of Electrical Engineering, Columbia University, New York City, NY, USA, 10027; Dept. of Electrical Engineering, Columbia University, New York City, NY, USA, 10027",
        "aff_domain": "columbia.edu;columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/419e33cfb3aebab7d7d4ed588f3e61c1-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Dept. of Electrical Engineering",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York City",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Flexible Transfer Learning under Support and Model Shift",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4608",
        "id": "4608",
        "author_site": "Xuezhi Wang, Jeff Schneider",
        "author": "Xuezhi Wang; Jeff Schneider",
        "abstract": "Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source/training domain) but only very limited training data for a second task (the target/test domain) that is similar but not identical to the first. Previous work on transfer learning has focused on relatively restricted settings, where specific parts of the model are considered to be carried over between tasks. Recent work on covariate shift focuses on matching the marginal distributions on observations $X$ across domains. Similarly, work on target/conditional shift focuses on matching marginal distributions on labels $Y$ and adjusting conditional distributions $P(X|Y)$, such that $P(X)$ can be matched across domains. However, covariate shift assumes that the support of test $P(X)$ is contained in the support of training $P(X)$, i.e., the training set is richer than the test set. Target/conditional shift makes a similar assumption for $P(Y)$. Moreover, not much work on transfer learning has considered the case when a few labels in the test domain are available. Also little work has been done when all marginal and conditional distributions are allowed to change while the changes are smooth. In this paper, we consider a general case where both the support and the model change across domains. We transform both $X$ and $Y$ by a location-scale shift to achieve transfer between tasks. Since we allow more flexible transformations, the proposed method yields better results on both synthetic data and real-world data.",
        "bibtex": "@inproceedings{NIPS2014_21085aa9,\n author = {Wang, Xuezhi and Schneider, Jeff},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Flexible Transfer Learning under Support and Model Shift},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/21085aa904b9fe66bf35f67c34d176d0-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/21085aa904b9fe66bf35f67c34d176d0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/21085aa904b9fe66bf35f67c34d176d0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/21085aa904b9fe66bf35f67c34d176d0-Reviews.html",
        "metareview": "",
        "pdf_size": 1058261,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11977599093119185908&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Department, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/21085aa904b9fe66bf35f67c34d176d0-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "From MAP to Marginals: Variational Inference in Bayesian Submodular Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4610",
        "id": "4610",
        "author_site": "Josip Djolonga, Andreas Krause",
        "author": "Josip Djolonga; Andreas Krause",
        "abstract": "Submodular optimization has found many applications in machine learning and beyond. We carry out the first systematic investigation of inference in probabilistic models defined through submodular functions, generalizing regular pairwise MRFs and Determinantal Point Processes. In particular, we present L-Field, a variational approach to general log-submodular and log-supermodular distributions based on sub- and supergradients. We obtain both lower and upper bounds on the log-partition function, which enables us to compute probability intervals for marginals, conditionals and marginal likelihoods. We also obtain fully factorized approximate posteriors, at the same computational cost as ordinary submodular optimization. Our framework results in convex problems for optimizing over differentials of submodular functions, which we show how to optimally solve. We provide theoretical guarantees of the approximation quality with respect to the curvature of the function. We further establish natural relations between our variational approach and the classical mean-field method. Lastly, we empirically demonstrate the accuracy of our inference scheme on several submodular models.",
        "bibtex": "@inproceedings{NIPS2014_efe0df3e,\n author = {Djolonga, Josip and Krause, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From MAP to Marginals: Variational Inference in Bayesian Submodular Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/efe0df3ea4a53a04614ad79e7a8a57de-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/efe0df3ea4a53a04614ad79e7a8a57de-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/efe0df3ea4a53a04614ad79e7a8a57de-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/efe0df3ea4a53a04614ad79e7a8a57de-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/efe0df3ea4a53a04614ad79e7a8a57de-Reviews.html",
        "metareview": "",
        "pdf_size": 441908,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12768326147901359102&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich",
        "aff_domain": "inf.ethz.ch;ethz.ch",
        "email": "inf.ethz.ch;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/efe0df3ea4a53a04614ad79e7a8a57de-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "From Stochastic Mixability to Fast Rates",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4334",
        "id": "4334",
        "author_site": "Nishant Mehta, Robert Williamson",
        "author": "Nishant A. Mehta; Robert C. Williamson",
        "abstract": "Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution $\\mathsf{P}$ and returns a hypothesis $f$ chosen from a fixed class $\\mathcal{F}$ with small loss $\\ell$. In the parametric setting, depending upon $(\\ell, \\mathcal{F},\\mathsf{P})$ ERM can have slow $(1/\\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a function of the sample size $n$. There exist several results that give sufficient conditions for fast rates in terms of joint properties of $\\ell$, $\\mathcal{F}$, and $\\mathsf{P}$, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss $\\ell$ (there being no role there for $\\mathcal{F}$ or $\\mathsf{P}$). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of $(\\ell,\\mathcal{F}, \\mathsf{P})$, and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.",
        "bibtex": "@inproceedings{NIPS2014_002302d5,\n author = {Mehta, Nishant A. and Williamson, Robert C.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From Stochastic Mixability to Fast Rates},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/002302d5a1c66195b6981e33e38df11d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/002302d5a1c66195b6981e33e38df11d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/002302d5a1c66195b6981e33e38df11d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/002302d5a1c66195b6981e33e38df11d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/002302d5a1c66195b6981e33e38df11d-Reviews.html",
        "metareview": "",
        "pdf_size": 401095,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14122890671564535486&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Research School of Computer Science, Australian National University; Research School of Computer Science, Australian National University + NICTA",
        "aff_domain": "anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/002302d5a1c66195b6981e33e38df11d-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Australian National University;National Information and Communications Technology Australia",
        "aff_unique_dep": "Research School of Computer Science;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au",
        "aff_unique_abbr": "ANU;NICTA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4611",
        "id": "4611",
        "author": "Ohad Shamir",
        "abstract": "Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); communication constraints (e.g. distributed learning); partial access to the underlying data (e.g. missing features and multi-armed bandits) and more. However, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics. For example, are there learning problems where any algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints? In this paper, we describe how a single set of results implies positive answers to the above, for several different settings.",
        "bibtex": "@inproceedings{NIPS2014_cc427d93,\n author = {Shamir, Ohad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cc427d934a7f6c0663e5923f49eba531-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cc427d934a7f6c0663e5923f49eba531-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/cc427d934a7f6c0663e5923f49eba531-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cc427d934a7f6c0663e5923f49eba531-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cc427d934a7f6c0663e5923f49eba531-Reviews.html",
        "metareview": "",
        "pdf_size": 402212,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5998207736559703532&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Weizmann Institute of Science",
        "aff_domain": "weizmann.ac.il",
        "email": "weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cc427d934a7f6c0663e5923f49eba531-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Weizmann Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.weizmann.org.il",
        "aff_unique_abbr": "Weizmann",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Gaussian Process Volatility Model",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4612",
        "id": "4612",
        "author_site": "Yue Wu, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Zoubin Ghahramani",
        "author": "Yue Wu; Jos\u00e9 Miguel Hern\u00e1ndez Lobato; Zoubin Ghahramani",
        "abstract": "The prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance. Moreover, functional parameters are usually learned by maximum likelihood, which can lead to overfitting. To address these problems we introduce GP-Vol, a novel non-parametric model for time-changing variances based on Gaussian Processes. This new model can capture highly flexible functional relationships for the variances. Furthermore, we introduce a new online algorithm for fast inference in GP-Vol. This method is much faster than current offline inference procedures and it avoids overfitting problems by following a fully Bayesian approach. Experiments with financial data show that GP-Vol performs significantly better than current standard alternatives.",
        "bibtex": "@inproceedings{NIPS2014_0525ce70,\n author = {Wu, Yue and Lobato, Jos\\'{e} Miguel Hern\\'{a}ndez and Ghahramani, Zoubin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian Process Volatility Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0525ce70d439c1ddeadc8277ca151195-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0525ce70d439c1ddeadc8277ca151195-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0525ce70d439c1ddeadc8277ca151195-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0525ce70d439c1ddeadc8277ca151195-Reviews.html",
        "metareview": "",
        "pdf_size": 531885,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8324097354304781229&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Cambridge University; Cambridge University; Cambridge University",
        "aff_domain": "post.harvard.edu;cam.ac.uk;eng.cam.ac.uk",
        "email": "post.harvard.edu;cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0525ce70d439c1ddeadc8277ca151195-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "General Stochastic Networks for Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4613",
        "id": "4613",
        "author_site": "Matthias Z\u00f6hrer, Franz Pernkopf",
        "author": "Matthias Z\u00f6hrer; Franz Pernkopf",
        "abstract": "We extend generative stochastic networks to supervised learning of representations. In particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter lambda. We use a new variant of network training involving noise injection, i.e. walkback training, to jointly optimize multiple network layers. Neither additional regularization constraints, such as l1, l2 norms or dropout variants, nor pooling- or convolutional layers were added. Nevertheless, we are able to obtain state-of-the-art performance on the MNIST dataset, without using permutation invariant digits and outperform baseline models on sub-variants of the MNIST and rectangles dataset significantly.",
        "bibtex": "@inproceedings{NIPS2014_8803b45e,\n author = {Z\\\"{o}hrer, Matthias and Pernkopf, Franz},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {General Stochastic Networks for Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8803b45e1f38aff1ea1b90886b7c9c55-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8803b45e1f38aff1ea1b90886b7c9c55-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8803b45e1f38aff1ea1b90886b7c9c55-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8803b45e1f38aff1ea1b90886b7c9c55-Reviews.html",
        "metareview": "",
        "pdf_size": 698984,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1654461379440096260&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Signal Processing and Speech Communication Laboratory, Graz University of Technology; Signal Processing and Speech Communication Laboratory, Graz University of Technology",
        "aff_domain": "tugraz.at;tugraz.at",
        "email": "tugraz.at;tugraz.at",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8803b45e1f38aff1ea1b90886b7c9c55-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Signal Processing and Speech Communication Laboratory",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TUGraz",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "General Table Completion using a Bayesian Nonparametric Model",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4614",
        "id": "4614",
        "author_site": "Isabel Valera, Zoubin Ghahramani",
        "author": "Isabel Valera; Zoubin Ghahramani",
        "abstract": "Even though heterogeneous databases can be found in a broad variety of applications, there exists a lack of tools for estimating missing data in such databases. In this paper, we provide an efficient and robust table completion tool, based on a Bayesian nonparametric latent feature model. In particular, we propose a general observation model for the Indian buffet process (IBP) adapted to mixed continuous (real-valued and positive real-valued) and discrete (categorical, ordinal and count) observations. Then, we propose an inference algorithm that scales linearly with the number of observations. Finally, our experiments over five real databases show that the proposed approach provides more robust and accurate estimates than the standard IBP and the Bayesian probabilistic matrix factorization with Gaussian observations.",
        "bibtex": "@inproceedings{NIPS2014_da8ea3dd,\n author = {Valera, Isabel and Ghahramani, Zoubin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {General Table Completion using a Bayesian Nonparametric Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/da8ea3ddebde9f9926c88c9b48c19c97-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/da8ea3ddebde9f9926c88c9b48c19c97-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/da8ea3ddebde9f9926c88c9b48c19c97-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/da8ea3ddebde9f9926c88c9b48c19c97-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/da8ea3ddebde9f9926c88c9b48c19c97-Reviews.html",
        "metareview": "",
        "pdf_size": 367415,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2358950866927342266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Signal Processing and Communications, University Carlos III in Madrid; Department of Engineering, University of Cambridge",
        "aff_domain": "tsc.uc3m.es;eng.cam.ac.uk",
        "email": "tsc.uc3m.es;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/da8ea3ddebde9f9926c88c9b48c19c97-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University Carlos III in Madrid;University of Cambridge",
        "aff_unique_dep": "Department of Signal Processing and Communications;Department of Engineering",
        "aff_unique_url": "https://www.uc3m.es;https://www.cam.ac.uk",
        "aff_unique_abbr": "UC3M;Cambridge",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Madrid;Cambridge",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Spain;United Kingdom"
    },
    {
        "title": "Generalized Dantzig Selector: Application to the k-support norm",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4615",
        "id": "4615",
        "author_site": "Soumyadeep Chatterjee, Sheng Chen, Arindam Banerjee",
        "author": "Soumyadeep Chatterjee; Sheng Chen; Arindam Banerjee",
        "abstract": "We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS. Thereafter, non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian widths of the unit norm ball and the error set. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.",
        "bibtex": "@inproceedings{NIPS2014_f5fba411,\n author = {Chatterjee, Soumyadeep and Chen, Sheng and Banerjee, Arindam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalized Dantzig Selector: Application to the k-support norm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f5fba411cf019eb04d5823a74781f7e9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f5fba411cf019eb04d5823a74781f7e9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f5fba411cf019eb04d5823a74781f7e9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f5fba411cf019eb04d5823a74781f7e9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f5fba411cf019eb04d5823a74781f7e9-Reviews.html",
        "metareview": "",
        "pdf_size": 291305,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1746105032771788708&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Computer Science & Engg., University of Minnesota, Twin Cities; Dept. of Computer Science & Engg., University of Minnesota, Twin Cities; Dept. of Computer Science & Engg., University of Minnesota, Twin Cities",
        "aff_domain": "cs.umn.edu;cs.umn.edu;cs.umn.edu",
        "email": "cs.umn.edu;cs.umn.edu;cs.umn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f5fba411cf019eb04d5823a74781f7e9-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "Department of Computer Science & Engineering",
        "aff_unique_url": "https://www.minnesota.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Twin Cities",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Generalized Higher-Order Orthogonal Iteration for Tensor Decomposition and Completion",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4616",
        "id": "4616",
        "author_site": "Yuanyuan Liu, Fanhua Shang, Wei Fan, James Cheng, Hong Cheng",
        "author": "Yuanyuan Liu; Fanhua Shang; Wei Fan; James Cheng; Hong Cheng",
        "abstract": "Low-rank tensor estimation has been frequently applied in many real-world problems. Despite successful applications, existing Schatten 1-norm minimization (SNM) methods may become very slow or even not applicable for large-scale problems. To address this difficulty, we therefore propose an efficient and scalable core tensor Schatten 1-norm minimization method for simultaneous tensor decomposition and completion, with a much lower computational complexity. We first induce the equivalence relation of Schatten 1-norm of a low-rank tensor and its core tensor. Then the Schatten 1-norm of the core tensor is used to replace that of the whole tensor, which leads to a much smaller-scale matrix SNM problem. Finally, an efficient algorithm with a rank-increasing scheme is developed to solve the proposed problem with a convergence guarantee. Extensive experimental results show that our method is usually more accurate than the state-of-the-art methods, and is orders of magnitude faster.",
        "bibtex": "@inproceedings{NIPS2014_3ddbc6f9,\n author = {Liu, Yuanyuan and Shang, Fanhua and Fan, Wei and Cheng, James and Cheng, Hong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalized Higher-Order Orthogonal Iteration for Tensor Decomposition and Completion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3ddbc6f9c020d06c82cf3df9c269a139-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3ddbc6f9c020d06c82cf3df9c269a139-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3ddbc6f9c020d06c82cf3df9c269a139-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3ddbc6f9c020d06c82cf3df9c269a139-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3ddbc6f9c020d06c82cf3df9c269a139-Reviews.html",
        "metareview": "",
        "pdf_size": 293323,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9612088766878692982&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Dept. of Systems Engineering and Engineering Management, The Chinese University of Hong Kong; Dept. of Computer Science and Engineering, The Chinese University of Hong Kong; Huawei Noah's Ark Lab, Hong Kong; Dept. of Computer Science and Engineering, The Chinese University of Hong Kong; Dept. of Systems Engineering and Engineering Management, The Chinese University of Hong Kong",
        "aff_domain": "se.cuhk.edu.hk;se.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;huawei.com",
        "email": "se.cuhk.edu.hk;se.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;huawei.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3ddbc6f9c020d06c82cf3df9c269a139-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Huawei",
        "aff_unique_dep": "Dept. of Systems Engineering and Engineering Management;Noah's Ark Lab",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.huawei.com",
        "aff_unique_abbr": "CUHK;Huawei",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Hong Kong SAR;Hong Kong",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Generalized Unsupervised Manifold Alignment",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4617",
        "id": "4617",
        "author_site": "Zhen Cui, Hong Chang, Shiguang Shan, Xilin Chen",
        "author": "Zhen Cui; Hong Chang; Shiguang Shan; Xilin Chen",
        "abstract": "In this paper, we propose a generalized Unsupervised Manifold Alignment (GUMA) method to build the connections between different but correlated datasets without any known correspondences. Based on the assumption that datasets of the same theme usually have similar manifold structures, GUMA is formulated into an explicit integer optimization problem considering the structure matching and preserving criteria, as well as the feature comparability of the corresponding points in the mutual embedding space. The main benefits of this model include: (1) simultaneous discovery and alignment of manifold structures; (2) fully unsupervised matching without any pre-specified correspondences; (3) efficient iterative alignment without computations in all permutation cases. Experimental results on dataset matching and real-world applications demonstrate the effectiveness and the practicability of our manifold alignment method.",
        "bibtex": "@inproceedings{NIPS2014_f306a296,\n author = {Cui, Zhen and Chang, Hong and Shan, Shiguang and Chen, Xilin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalized Unsupervised Manifold Alignment},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f306a2961c3dcdd21904a759d51f4e22-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f306a2961c3dcdd21904a759d51f4e22-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f306a2961c3dcdd21904a759d51f4e22-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f306a2961c3dcdd21904a759d51f4e22-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f306a2961c3dcdd21904a759d51f4e22-Reviews.html",
        "metareview": "",
        "pdf_size": 143177,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8090877174602295785&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China + School of Computer Science and Technology, Huaqiao University, Xiamen, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China",
        "aff_domain": "vipl.ict.ac.cn;vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "vipl.ict.ac.cn;vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f306a2961c3dcdd21904a759d51f4e22-Abstract.html",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;Huaqiao University",
        "aff_unique_dep": "Institute of Computing Technology;School of Computer Science and Technology",
        "aff_unique_url": "http://www.cas.cn;https://www.hqu.edu.cn",
        "aff_unique_abbr": "CAS;HQU",
        "aff_campus_unique_index": "0+1;0;0;0",
        "aff_campus_unique": "Beijing;Xiamen",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Generative Adversarial Nets",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4618",
        "id": "4618",
        "author_site": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",
        "author": "Ian J. Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio",
        "abstract": "We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.",
        "bibtex": "@inproceedings{NIPS2014_f033ed80,\n author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generative Adversarial Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Reviews.html",
        "metareview": "",
        "pdf_size": 539761,
        "gs_citation": 82368,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11977070277539609369&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 75,
        "aff": "D\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal + Indian Institute of Technology Delhi; D\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019informatique et de recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal",
        "aff_domain": "; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ",
        "github": "http://www.github.com/goodfeli/adversarial",
        "project": "",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f033ed80deb0234979a61f95710dbe25-Abstract.html",
        "aff_unique_index": "0;0;0;0;0;0+1;0;0",
        "aff_unique_norm": "Universit u00e9 de Montr u00eal;Indian Institute of Technology Delhi",
        "aff_unique_dep": "D u00e9partement d\u2019informatique et de recherche op u00e9rationnelle;",
        "aff_unique_url": "https://www.umontreal.ca;https://www.iitd.ac.in",
        "aff_unique_abbr": "UdeM;IIT Delhi",
        "aff_campus_unique_index": "0;0;0;0;0;0+1;0;0",
        "aff_campus_unique": "Montr u00eal;Delhi",
        "aff_country_unique_index": "0;0;0;0;0;0+1;0;0",
        "aff_country_unique": "Canada;India"
    },
    {
        "title": "Global Belief Recursive Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4518",
        "id": "4518",
        "author_site": "Romain Paulus, Richard Socher, Christopher Manning",
        "author": "Romain Paulus; Richard Socher; Christopher D. Manning",
        "abstract": "Recursive Neural Networks have recently obtained state of the art performance on several natural language processing tasks. However, because of their feedforward architecture they cannot correctly predict phrase or word labels that are determined by context. This is a problem in tasks such as aspect-specific sentiment classification which tries to, for instance, predict that the word Android is positive in the sentence Android beats iOS. We introduce global belief recursive neural networks (GB-RNNs) which are based on the idea of extending purely feedforward neural networks to include one feedbackward step during inference. This allows phrase level predictions and representations to give feedback to words. We show the effectiveness of this model on the task of contextual sentiment analysis. We also show that dropout can improve RNN training and that a combination of unsupervised and supervised word vector representations performs better than either alone. The feedbackward step improves F1 performance by 3% over the standard RNN on this task, obtains state-of-the-art performance on the SemEval 2013 challenge and can accurately predict the sentiment of specific entities.",
        "bibtex": "@inproceedings{NIPS2014_a7882003,\n author = {Paulus, Romain and Socher, Richard and Manning, Christopher D},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Global Belief Recursive Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a78820038c6e4576d6b6e5da6779a628-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a78820038c6e4576d6b6e5da6779a628-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a78820038c6e4576d6b6e5da6779a628-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a78820038c6e4576d6b6e5da6779a628-Reviews.html",
        "metareview": "",
        "pdf_size": 345533,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14536355409447473434&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "MetaMind; MetaMind+Stanford University; Stanford University",
        "aff_domain": "metamind.io;metamind.io;stanford.edu",
        "email": "metamind.io;metamind.io;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a78820038c6e4576d6b6e5da6779a628-Abstract.html",
        "aff_unique_index": "0;0+1;1",
        "aff_unique_norm": "Meta;Stanford University",
        "aff_unique_dep": "MetaMind;",
        "aff_unique_url": "https://www.metamind.io;https://www.stanford.edu",
        "aff_unique_abbr": ";Stanford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Global Sensitivity Analysis for MAP Inference in Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4620",
        "id": "4620",
        "author_site": "Jasper De Bock, Cassio P de Campos, Alessandro Antonucci",
        "author": "Jasper De Bock; Cassio P. de Campos; Alessandro Antonucci",
        "abstract": "We study the sensitivity of a MAP configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters. These perturbations are global, in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed. Our main contribution is an exact algorithm that can check whether the MAP configuration is robust with respect to given perturbations. Its complexity is essentially the same as that of obtaining the MAP configuration itself, so it can be promptly used with minimal effort. We use our algorithm to identify the largest global perturbation that does not induce a change in the MAP configuration, and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets. A strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.",
        "bibtex": "@inproceedings{NIPS2014_f9d3c99b,\n author = {De Bock, Jasper and de Campos, Cassio P. and Antonucci, Alessandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Global Sensitivity Analysis for MAP Inference in Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f9d3c99bd6cbf2d694266e7760ee1ed6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f9d3c99bd6cbf2d694266e7760ee1ed6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f9d3c99bd6cbf2d694266e7760ee1ed6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f9d3c99bd6cbf2d694266e7760ee1ed6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f9d3c99bd6cbf2d694266e7760ee1ed6-Reviews.html",
        "metareview": "",
        "pdf_size": 1059918,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11291848116969539124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Ghent University, SYSTeMS; Queen\u2019s University; IDSIA",
        "aff_domain": "ugent.be;qub.ac.uk;idsia.ch",
        "email": "ugent.be;qub.ac.uk;idsia.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f9d3c99bd6cbf2d694266e7760ee1ed6-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Ghent University;Queen's University;Institute of Digital Technologies",
        "aff_unique_dep": "SYSTeMS;;",
        "aff_unique_url": "https://www.ugent.be;https://www.queensu.ca;https://www.idsia.ch",
        "aff_unique_abbr": "UGent;Queen's U;IDSIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Belgium;Canada;Switzerland"
    },
    {
        "title": "Graph Clustering With Missing Data: Convex Algorithms and Analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4621",
        "id": "4621",
        "author_site": "Ramya Korlakai Vinayak, Samet Oymak, Babak Hassibi",
        "author": "Ramya Korlakai Vinayak; Samet Oymak; Babak Hassibi",
        "abstract": "We consider the problem of finding clusters in an unweighted graph, when the graph is partially observed. We analyze two programs, one which works for dense graphs and one which works for both sparse and dense graphs, but requires some a priori knowledge of the total cluster size, that are based on the convex optimization approach for low-rank matrix recovery using nuclear norm minimization. For the commonly used Stochastic Block Model, we obtain \\emph{explicit} bounds on the parameters of the problem (size and sparsity of clusters, the amount of observed data) and the regularization parameter characterize the success and failure of the programs. We corroborate our theoretical findings through extensive simulations. We also run our algorithm on a real data set obtained from crowdsourcing an image classification task on the Amazon Mechanical Turk, and observe significant performance improvement over traditional methods such as k-means.",
        "bibtex": "@inproceedings{NIPS2014_9a2cb485,\n author = {Korlakai Vinayak, Ramya and Oymak, Samet and Hassibi, Babak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Graph Clustering With Missing Data: Convex Algorithms and Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9a2cb485e69e5ed18869dfcab39ab4b6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9a2cb485e69e5ed18869dfcab39ab4b6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/9a2cb485e69e5ed18869dfcab39ab4b6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9a2cb485e69e5ed18869dfcab39ab4b6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9a2cb485e69e5ed18869dfcab39ab4b6-Reviews.html",
        "metareview": "",
        "pdf_size": 517833,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13729747252725877534&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical Engineering, California Institute of Technology, Pasadena, CA 91125; Department of Electrical Engineering, California Institute of Technology, Pasadena, CA 91125; Department of Electrical Engineering, California Institute of Technology, Pasadena, CA 91125",
        "aff_domain": "caltech.edu;caltech.edu;systems.caltech.edu",
        "email": "caltech.edu;caltech.edu;systems.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9a2cb485e69e5ed18869dfcab39ab4b6-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Graphical Models for Recovering Probabilistic and Causal Queries from Missing Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4622",
        "id": "4622",
        "author_site": "Karthika Mohan, Judea Pearl",
        "author": "Karthika Mohan; Judea Pearl",
        "abstract": "We address the problem of deciding whether a causal or probabilistic query is estimable from data corrupted by missing entries, given a model of missingness process. We extend the results of Mohan et al, 2013 by presenting more general conditions for recovering probabilistic queries of the form P(y|x) and P(y,x) as well as causal queries of the form P(y|do(x)). We show that causal queries may be recoverable even when the factors in their identifying estimands are not recoverable. Specifically, we derive graphical conditions for recovering causal effects of the form P(y|do(x)) when Y and its missingness mechanism are not d-separable. Finally, we apply our results to problems of attrition and characterize the recovery of causal effects from data corrupted by attrition.",
        "bibtex": "@inproceedings{NIPS2014_1835d9d1,\n author = {Mohan, Karthika and Pearl, Judea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Graphical Models for Recovering Probabilistic and Causal Queries from Missing Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1835d9d1508eb178b500220a9ddf75a7-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1835d9d1508eb178b500220a9ddf75a7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/1835d9d1508eb178b500220a9ddf75a7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1835d9d1508eb178b500220a9ddf75a7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1835d9d1508eb178b500220a9ddf75a7-Reviews.html",
        "metareview": "",
        "pdf_size": 184015,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14135055381810837810&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Cognitive Systems Laboratory, Computer Science Department, University of California, Los Angeles, CA 90024; Cognitive Systems Laboratory, Computer Science Department, University of California, Los Angeles, CA 90024",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1835d9d1508eb178b500220a9ddf75a7-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Greedy Subspace Clustering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4623",
        "id": "4623",
        "author_site": "Dohyung Park, Constantine Caramanis, Sujay Sanghavi",
        "author": "Dohyung Park; Constantine Caramanis; Sujay Sanghavi",
        "abstract": "We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity be- tween subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.",
        "bibtex": "@inproceedings{NIPS2014_e643fbc8,\n author = {Park, Dohyung and Caramanis, Constantine and Sanghavi, Sujay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Greedy Subspace Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e643fbc8a79cb76b75165fb2550692f6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e643fbc8a79cb76b75165fb2550692f6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e643fbc8a79cb76b75165fb2550692f6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e643fbc8a79cb76b75165fb2550692f6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e643fbc8a79cb76b75165fb2550692f6-Reviews.html",
        "metareview": "",
        "pdf_size": 503061,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16423426874226360062&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. of Electrical and Computer Engineering, The University of Texas at Austin; Dept. of Electrical and Computer Engineering, The University of Texas at Austin; Dept. of Electrical and Computer Engineering, The University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;mail.utexas.edu",
        "email": "utexas.edu;utexas.edu;mail.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e643fbc8a79cb76b75165fb2550692f6-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Dept. of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Grouping-Based Low-Rank Trajectory Completion and 3D Reconstruction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4624",
        "id": "4624",
        "author_site": "Katerina Fragkiadaki, Marta Salas, Pablo Arbelaez, Jitendra Malik",
        "author": "Katerina Fragkiadaki; Marta Salas; Pablo Arbel\u00e1ez; Jitendra Malik",
        "abstract": "Extracting 3D shape of deforming objects in monocular videos, a task known as non-rigid structure-from-motion (NRSfM), has so far been studied only on synthetic datasets and controlled environments. Typically, the objects to reconstruct are pre-segmented, they exhibit limited rotations and occlusions, or full-length trajectories are assumed. In order to integrate NRSfM into current video analysis pipelines, one needs to consider as input realistic -thus incomplete- tracking, and perform spatio-temporal grouping to segment the objects from their surroundings. Furthermore, NRSfM needs to be robust to noise in both segmentation and tracking, e.g., drifting, segmentation",
        "bibtex": "@inproceedings{NIPS2014_5190e987,\n author = {Fragkiadaki, Katerina and Salas, Marta and Arbel\\'{a}ez, Pablo and Malik, Jitendra},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Grouping-Based Low-Rank Trajectory Completion and 3D Reconstruction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5190e987c46a346974e351f96997d640-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5190e987c46a346974e351f96997d640-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5190e987c46a346974e351f96997d640-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5190e987c46a346974e351f96997d640-Reviews.html",
        "metareview": "",
        "pdf_size": 7382183,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14463839825008484819&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "EECS, University of California, Berkeley, CA 94720; Universidad de Zaragoza, Zaragoza, Spain; Universidad de los Andes, Bogota, Colombia; EECS, University of California, Berkeley, CA 94720",
        "aff_domain": "berkeley.edu;unizar.es;uniandes.edu.co;eecs.berkeley.edu",
        "email": "berkeley.edu;unizar.es;uniandes.edu.co;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5190e987c46a346974e351f96997d640-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of California, Berkeley;Universidad de Zaragoza;Universidad de los Andes",
        "aff_unique_dep": "EECS;;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.unizar.es;https://www.unandes.edu.co",
        "aff_unique_abbr": "UC Berkeley;;",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Berkeley;Zaragoza;Bogota",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "United States;Spain;Colombia"
    },
    {
        "title": "Hamming Ball Auxiliary Sampling for Factorial Hidden Markov Models",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4394",
        "id": "4394",
        "author_site": "Michalis Titsias, Christopher Yau",
        "author": "Michalis K. Titsias; Christopher Yau",
        "abstract": "We introduce a novel sampling algorithm for Markov chain Monte Carlo-based Bayesian inference for factorial hidden Markov models. This algorithm is based on an auxiliary variable construction that restricts the model space allowing iterative exploration in polynomial time. The sampling approach overcomes limitations with common conditional Gibbs samplers that use asymmetric updates and become easily trapped in local modes. Instead, our method uses symmetric moves that allows joint updating of the latent sequences and improves mixing. We illustrate the application of the approach with simulated and a real data example.",
        "bibtex": "@inproceedings{NIPS2014_249d963c,\n author = {Titsias, Michalis K. and Yau, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hamming Ball Auxiliary Sampling for Factorial Hidden Markov Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/249d963cf2a1f9539622f86ae66924da-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/249d963cf2a1f9539622f86ae66924da-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/249d963cf2a1f9539622f86ae66924da-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/249d963cf2a1f9539622f86ae66924da-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/249d963cf2a1f9539622f86ae66924da-Reviews.html",
        "metareview": "",
        "pdf_size": 976098,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4525309793511977625&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Informatics, Athens University of Economics and Business; Wellcome Trust Centre for Human Genetics, University of Oxford",
        "aff_domain": "aueb.gr;well.ox.ac.uk",
        "email": "aueb.gr;well.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/249d963cf2a1f9539622f86ae66924da-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Athens University of Economics and Business;University of Oxford",
        "aff_unique_dep": "Department of Informatics;Wellcome Trust Centre for Human Genetics",
        "aff_unique_url": "https://www.aueb.gr;https://www.ox.ac.uk",
        "aff_unique_abbr": "AUEB;Oxford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Athens;Oxford",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Greece;United Kingdom"
    },
    {
        "title": "Hardness of parameter estimation in graphical models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4625",
        "id": "4625",
        "author_site": "Guy Bresler, David Gamarnik, Devavrat Shah",
        "author": "Guy Bresler; David Gamarnik; Devavrat Shah",
        "abstract": "We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan) but no proof was known. Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).",
        "bibtex": "@inproceedings{NIPS2014_325db0cf,\n author = {Bresler, Guy and Gamarnik, David and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hardness of parameter estimation in graphical models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/325db0cfacc5572332b8acaf5ef2c151-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/325db0cfacc5572332b8acaf5ef2c151-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/325db0cfacc5572332b8acaf5ef2c151-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/325db0cfacc5572332b8acaf5ef2c151-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/325db0cfacc5572332b8acaf5ef2c151-Reviews.html",
        "metareview": "",
        "pdf_size": 370632,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3776733157225963034&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Laboratory for Information and Decision Systems, Department of EECS; Sloan School of Management; Laboratory for Information and Decision Systems, Department of EECS",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/325db0cfacc5572332b8acaf5ef2c151-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "How hard is my MDP?\" The distribution-norm to the rescue\"",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4312",
        "id": "4312",
        "author_site": "Odalric-Ambrym Maillard, Timothy A Mann, Shie Mannor",
        "author": "Odalric-Ambrym Maillard; Timothy A. Mann; Shie Mannor",
        "abstract": "In Reinforcement Learning (RL), state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel $p$. In many problems, a good approximation of $p$ is not needed. For instance, if from one state-action pair $(s,a)$, one can only transit to states with the same value, learning $p(\\cdot|s,a)$ accurately is irrelevant (only its support matters). This paper aims at capturing such behavior by defining a novel hardness measure for Markov Decision Processes (MDPs) we call the {\\em distribution-norm}. The distribution-norm w.r.t.~a measure $\\nu$ is defined on zero $\\nu$-mean functions $f$ by the standard variation of $f$ with respect to $\\nu$. We first provide a concentration inequality for the dual of the distribution-norm. This allows us to replace the generic but loose $||\\cdot||_1$ concentration inequalities used in most previous analysis of RL algorithms, to benefit from this new hardness measure. We then show that several common RL benchmarks have low hardness when measured using the new norm. The distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of MDPs.",
        "bibtex": "@inproceedings{NIPS2014_7335f569,\n author = {Maillard, Odalric-Ambrym and Mann, Timothy A. and Mannor, Shie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {How hard is my MDP?\" The distribution-norm to the rescue\"},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7335f569596c706ccdf756fc8f812a94-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7335f569596c706ccdf756fc8f812a94-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7335f569596c706ccdf756fc8f812a94-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7335f569596c706ccdf756fc8f812a94-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7335f569596c706ccdf756fc8f812a94-Reviews.html",
        "metareview": "",
        "pdf_size": 374675,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9820767591382539129&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "The Technion, Haifa, Israel; The Technion, Haifa, Israel; The Technion, Haifa, Israel",
        "aff_domain": "ens-cachan.org;gmail.com;ee.technion.ac.il",
        "email": "ens-cachan.org;gmail.com;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7335f569596c706ccdf756fc8f812a94-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "How transferable are features in deep neural networks?",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4346",
        "id": "4346",
        "author_site": "Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson",
        "author": "Jason Yosinski; Jeff Clune; Yoshua Bengio; Hod Lipson",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
        "bibtex": "@inproceedings{NIPS2014_532a2f85,\n author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {How transferable are features in deep neural networks?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/532a2f85b6977104bc93f8580abbb330-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/532a2f85b6977104bc93f8580abbb330-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/532a2f85b6977104bc93f8580abbb330-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/532a2f85b6977104bc93f8580abbb330-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/532a2f85b6977104bc93f8580abbb330-Reviews.html",
        "metareview": "",
        "pdf_size": 437296,
        "gs_citation": 11843,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8833146761103838326&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/532a2f85b6977104bc93f8580abbb330-Abstract.html"
    },
    {
        "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4626",
        "id": "4626",
        "author_site": "Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, Yoshua Bengio",
        "author": "Yann N. Dauphin; Razvan Pascanu; Caglar Gulcehre; Kyunghyun Cho; Surya Ganguli; Yoshua Bengio",
        "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.",
        "bibtex": "@inproceedings{NIPS2014_04192426,\n author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Reviews.html",
        "metareview": "",
        "pdf_size": 4023083,
        "gs_citation": 1954,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12321526113830855618&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Universit \u00b4e de Montr \u00b4eal; Universit \u00b4e de Montr \u00b4eal; Universit \u00b4e de Montr \u00b4eal; Universit \u00b4e de Montr \u00b4eal; Stanford University; Universit \u00b4e de Montr \u00b4eal + CIFAR Fellow",
        "aff_domain": "iro.umontreal.ca;gmail.com;iro.umontreal.ca;umontreal.ca;standford.edu;umontreal.ca",
        "email": "iro.umontreal.ca;gmail.com;iro.umontreal.ca;umontreal.ca;standford.edu;umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/04192426585542c54b96ba14445be996-Abstract.html",
        "aff_unique_index": "0;0;0;0;1;0+2",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Stanford University;CIFAR",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.umontreal.ca;https://www.stanford.edu;https://www.cifar.ca",
        "aff_unique_abbr": "UdeM;Stanford;CIFAR",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;1;0+0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Improved Distributed Principal Component Analysis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4627",
        "id": "4627",
        "author_site": "Yingyu Liang, Maria-Florina F Balcan, Vandana Kanchanapally, David Woodruff",
        "author": "Maria-Florina Balcan; Vandana Kanchanapally; Yingyu Liang; David Woodruff",
        "abstract": "We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as $k$-means clustering and low rank approximation. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for $k$-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as input-sparsity subspace embeddings with high correctness probability with a dimension and sparsity independent of the error probability, may be of independent interest.",
        "bibtex": "@inproceedings{NIPS2014_e968f164,\n author = {Balcan, Maria-Florina and Kanchanapally, Vandana and Liang, Yingyu and Woodruff, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improved Distributed Principal Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e968f1646c1c6c35422b64c0934772a4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e968f1646c1c6c35422b64c0934772a4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e968f1646c1c6c35422b64c0934772a4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e968f1646c1c6c35422b64c0934772a4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e968f1646c1c6c35422b64c0934772a4-Reviews.html",
        "metareview": "",
        "pdf_size": 451114,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7978452141464117235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Georgia Institute of Technology; Department of Computer Science, Princeton University; Almaden Research Center, IBM Research",
        "aff_domain": "cs.cmu.edu;gatech.edu;cs.princeton.edu;us.ibm.com",
        "email": "cs.cmu.edu;gatech.edu;cs.princeton.edu;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e968f1646c1c6c35422b64c0934772a4-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Carnegie Mellon University;Georgia Institute of Technology;Princeton University;IBM",
        "aff_unique_dep": "School of Computer Science;School of Computer Science;Department of Computer Science;Research",
        "aff_unique_url": "https://www.cmu.edu;https://www.gatech.edu;https://www.princeton.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "CMU;Georgia Tech;Princeton;IBM",
        "aff_campus_unique_index": "0;1;3",
        "aff_campus_unique": "Pittsburgh;Atlanta;;Almaden",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improved Multimodal Deep Learning with Variation of Information",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4628",
        "id": "4628",
        "author_site": "Kihyuk Sohn, Wenling Shang, Honglak Lee",
        "author": "Kihyuk Sohn; Wenling Shang; Honglak Lee",
        "abstract": "Deep learning has been successfully applied to multimodal representation learning problems, with a common strategy to learning joint representations that are shared across multiple modalities on top of layers of modality-specific networks. Nonetheless, there still remains a question how to learn a good association between data modalities; in particular, a good generative model of multimodal data should be able to reason about missing data modality given the rest of data modalities. In this paper, we propose a novel multimodal representation learning framework that explicitly aims this goal. Rather than learning with maximum likelihood, we train the model to minimize the variation of information. We provide a theoretical insight why the proposed learning objective is sufficient to estimate the data-generating joint distribution of multimodal data. We apply our method to restricted Boltzmann machines and introduce learning methods based on contrastive divergence and multi-prediction training. In addition, we extend to deep networks with recurrent encoding structure to finetune the whole network. In experiments, we demonstrate the state-of-the-art visual recognition performance on MIR-Flickr database and PASCAL VOC 2007 database with and without text features.",
        "bibtex": "@inproceedings{NIPS2014_3b8203bb,\n author = {Sohn, Kihyuk and Shang, Wenling and Lee, Honglak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improved Multimodal Deep Learning with Variation of Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3b8203bbc49da2b22f48fb9a15eeed13-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3b8203bbc49da2b22f48fb9a15eeed13-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3b8203bbc49da2b22f48fb9a15eeed13-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3b8203bbc49da2b22f48fb9a15eeed13-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3b8203bbc49da2b22f48fb9a15eeed13-Reviews.html",
        "metareview": "",
        "pdf_size": 6123294,
        "gs_citation": 234,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9555661242272554639&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Michigan Ann Arbor, MI, USA; University of Michigan Ann Arbor, MI, USA; University of Michigan Ann Arbor, MI, USA",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3b8203bbc49da2b22f48fb9a15eeed13-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Incremental Clustering: The Case for Extra Clusters",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4629",
        "id": "4629",
        "author_site": "Margareta Ackerman, Sanjoy Dasgupta",
        "author": "Margareta Ackerman; Sanjoy Dasgupta",
        "abstract": "The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.",
        "bibtex": "@inproceedings{NIPS2014_72aa1632,\n author = {Ackerman, Margareta and Dasgupta, Sanjoy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Incremental Clustering: The Case for Extra Clusters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/72aa1632b83c93a2f680dbb5235f1a83-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/72aa1632b83c93a2f680dbb5235f1a83-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/72aa1632b83c93a2f680dbb5235f1a83-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/72aa1632b83c93a2f680dbb5235f1a83-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/72aa1632b83c93a2f680dbb5235f1a83-Reviews.html",
        "metareview": "",
        "pdf_size": 286222,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15878679820666635797&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Florida State University; UC San Diego",
        "aff_domain": "fsu.edu;eng.ucsd.edu",
        "email": "fsu.edu;eng.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/72aa1632b83c93a2f680dbb5235f1a83-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Florida State University;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.fsu.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "FSU;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Incremental Local Gaussian Regression",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4630",
        "id": "4630",
        "author_site": "Franziska Meier, Philipp Hennig, Stefan Schaal",
        "author": "Franziska Meier; Philipp Hennig; Stefan Schaal",
        "abstract": "Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.",
        "bibtex": "@inproceedings{NIPS2014_9f748277,\n author = {Meier, Franziska and Hennig, Philipp and Schaal, Stefan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Incremental Local Gaussian Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9f74827737e6bb374ff8faf50d88c507-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9f74827737e6bb374ff8faf50d88c507-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9f74827737e6bb374ff8faf50d88c507-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9f74827737e6bb374ff8faf50d88c507-Reviews.html",
        "metareview": "",
        "pdf_size": 409638,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18032080994780172214&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Southern California; Max Planck Institute for Intelligent Systems; University of Southern California + Max Planck Institute for Intelligent Systems",
        "aff_domain": "usc.edu;tue.mpg.de;usc.edu",
        "email": "usc.edu;tue.mpg.de;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9f74827737e6bb374ff8faf50d88c507-Abstract.html",
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "University of Southern California;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";Intelligent Systems",
        "aff_unique_url": "https://www.usc.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "USC;MPI-IS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;1;0+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Inference by Learning: Speeding-up Graphical Model Optimization via a Coarse-to-Fine Cascade of Pruning Classifiers",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4776",
        "id": "4776",
        "author_site": "Bruno Conejo, Nikos Komodakis, Sebastien Leprince, Jean Philippe Avouac",
        "author": "Bruno Conejo; Nikos Komodakis; Sebastien Leprince; Jean Philippe Avouac",
        "abstract": "We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach, refereed as Inference by Learning or IbyL, relies on a multi-scale pruning scheme that progressively reduces the solution space by use of a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our novel framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF. We make our code available on-line.",
        "bibtex": "@inproceedings{NIPS2014_0a33562d,\n author = {Conejo, Bruno and Komodakis, Nikos and Leprince, Sebastien and Avouac, Jean Philippe},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inference by Learning: Speeding-up Graphical Model Optimization via a Coarse-to-Fine Cascade of Pruning Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0a33562d6e9b20a57626befba498ded3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0a33562d6e9b20a57626befba498ded3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0a33562d6e9b20a57626befba498ded3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0a33562d6e9b20a57626befba498ded3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0a33562d6e9b20a57626befba498ded3-Reviews.html",
        "metareview": "",
        "pdf_size": 3386713,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6808644297424162269&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "GPS Division, California Institute of Technology, Pasadena, CA, USA + Universite Paris-Est, Ecole des Ponts ParisTech, Marne-la-Vallee, France; Universite Paris-Est, Ecole des Ponts ParisTech, Marne-la-Vallee, France; GPS Division, California Institute of Technology, Pasadena, CA, USA; GPS Division, California Institute of Technology, Pasadena, CA, USA",
        "aff_domain": "caltech.edu;enpc.fr;caltech.edu;gps.caltech.edu",
        "email": "caltech.edu;enpc.fr;caltech.edu;gps.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0a33562d6e9b20a57626befba498ded3-Abstract.html",
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "California Institute of Technology;Universite Paris-Est",
        "aff_unique_dep": "GPS Division;Ecole des Ponts ParisTech",
        "aff_unique_url": "https://www.caltech.edu;https://www.enpc.fr",
        "aff_unique_abbr": "Caltech;UPE",
        "aff_campus_unique_index": "0+1;1;0;0",
        "aff_campus_unique": "Pasadena;Marne-la-Vallee",
        "aff_country_unique_index": "0+1;1;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Inferring sparse representations of continuous signals with continuous orthogonal matching pursuit",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4631",
        "id": "4631",
        "author_site": "Karin C Knudson, Jacob Yates, Alexander Huk, Jonathan W Pillow",
        "author": "Karin C. Knudson; Jacob L. Yates; Alexander C. Huk; Jonathan W. Pillow",
        "abstract": "Many signals, such as spike trains recorded in multi-channel electrophysiological recordings, may be represented as the sparse sum of translated and scaled copies of waveforms whose timing and amplitudes are of interest. From the aggregate signal, one may seek to estimate the identities, amplitudes, and translations of the waveforms that compose the signal. Here we present a fast method for recovering these identities, amplitudes, and translations. The method involves greedily selecting component waveforms and then refining estimates of their amplitudes and translations, moving iteratively between these steps in a process analogous to the well-known Orthogonal Matching Pursuit (OMP) algorithm. Our approach for modeling translations borrows from Continuous Basis Pursuit (CBP), which we extend in several ways: by selecting a subspace that optimally captures translated copies of the waveforms, replacing the convex optimization problem with a greedy approach, and moving to the Fourier domain to more precisely estimate time shifts. We test the resulting method, which we call Continuous Orthogonal Matching Pursuit (COMP), on simulated and neural data, where it shows gains over CBP in both speed and accuracy.",
        "bibtex": "@inproceedings{NIPS2014_9825c157,\n author = {Knudson, Karin C. and Yates, Jacob L. and Huk, Alexander C. and Pillow, Jonathan W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inferring sparse representations of continuous signals with continuous orthogonal matching pursuit},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9825c157f1333c2bba2747f1c6500b8c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9825c157f1333c2bba2747f1c6500b8c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9825c157f1333c2bba2747f1c6500b8c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9825c157f1333c2bba2747f1c6500b8c-Reviews.html",
        "metareview": "",
        "pdf_size": 389846,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6228528390960029757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Mathematics, The University of Texas at Austin; Department of Neuroscience, The University of Texas at Austin; Center for Perceptual Systems, Departments of Psychology & Neuroscience, The University of Texas at Austin; Princeton Neuroscience Institute and Department of Psychology, Princeton University",
        "aff_domain": "math.utexas.edu;utexas.edu;utexas.edu;princeton.edu",
        "email": "math.utexas.edu;utexas.edu;utexas.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9825c157f1333c2bba2747f1c6500b8c-Abstract.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Texas at Austin;Princeton University",
        "aff_unique_dep": "Department of Mathematics;Princeton Neuroscience Institute and Department of Psychology",
        "aff_unique_url": "https://www.utexas.edu;https://www.princeton.edu",
        "aff_unique_abbr": "UT Austin;Princeton",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Austin;Princeton",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Inferring synaptic conductances from spike trains with a biophysically inspired point process model",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4632",
        "id": "4632",
        "author_site": "Kenneth W Latimer, E.J. Chichilnisky, Fred Rieke, Jonathan W Pillow",
        "author": "Kenneth W. Latimer; E. J. Chichilnisky; Fred Rieke; Jonathan W. Pillow",
        "abstract": "A popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate. However, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs. Here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential. Our work makes two important contributions. First, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. We show that the classic GLM is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite \u201cpush-pull\u201d fashion. Our model can therefore be viewed as a direct extension of the GLM in which we relax these constraints; the resulting model can exhibit shunting as well as hyperpolarizing inhibition, and time-varying changes in both gain and membrane time constant. Second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the GLM. Most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains. We validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells. We show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances.",
        "bibtex": "@inproceedings{NIPS2014_a2a5a5b8,\n author = {Latimer, Kenneth W. and Chichilnisky, E. J. and Rieke, Fred and Pillow, Jonathan W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inferring synaptic conductances from spike trains with a biophysically inspired point process model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a2a5a5b81eee851436243b395b78a908-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a2a5a5b81eee851436243b395b78a908-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a2a5a5b81eee851436243b395b78a908-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a2a5a5b81eee851436243b395b78a908-Reviews.html",
        "metareview": "",
        "pdf_size": 4569049,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9018724256042042590&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "The Institute for Neuroscience, The University of Texas at Austin; Department of Neurosurgery, Hansen Experimental Physics Laboratory, Stanford University; Department of Physiology and Biophysics, Howard Hughes Medical Institute, University of Washington; Princeton Neuroscience Institute, Department of Psychology, Princeton University",
        "aff_domain": "utexas.edu;stanford.edu;u.washington.edu;princeton.edu",
        "email": "utexas.edu;stanford.edu;u.washington.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a2a5a5b81eee851436243b395b78a908-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "University of Texas at Austin;Stanford University;University of Washington;Princeton University",
        "aff_unique_dep": "Institute for Neuroscience;Department of Neurosurgery;Department of Physiology and Biophysics;Department of Psychology",
        "aff_unique_url": "https://www.utexas.edu;https://www.stanford.edu;https://www.washington.edu;https://www.princeton.edu",
        "aff_unique_abbr": "UT Austin;Stanford;UW;Princeton",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Austin;Stanford;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Information-based learning by agents in unbounded state spaces",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4633",
        "id": "4633",
        "author_site": "Shariq A Mobin, James A Arnemann, Fritz Sommer",
        "author": "Shariq A. Mobin; James A. Arnemann; Friedrich T. Sommer",
        "abstract": "The idea that animals might use information-driven planning to explore an unknown environment and build an internal model of it has been proposed for quite some time. Recent work has demonstrated that agents using this principle can efficiently learn models of probabilistic environments with discrete, bounded state spaces. However, animals and robots are commonly confronted with unbounded environments. To address this more challenging situation, we study information-based learning strategies of agents in unbounded state spaces using non-parametric Bayesian models. Specifically, we demonstrate that the Chinese Restaurant Process (CRP) model is able to solve this problem and that an Empirical Bayes version is able to efficiently explore bounded and unbounded worlds by relying on little prior information.",
        "bibtex": "@inproceedings{NIPS2014_87a5f58d,\n author = {Mobin, Shariq A. and Arnemann, James A. and Sommer, Friedrich T.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Information-based learning by agents in unbounded state spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/87a5f58d631d63911c1bc71d6270c5cd-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/87a5f58d631d63911c1bc71d6270c5cd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/87a5f58d631d63911c1bc71d6270c5cd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/87a5f58d631d63911c1bc71d6270c5cd-Reviews.html",
        "metareview": "",
        "pdf_size": 659148,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=410064634126818562&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Redwood Center for Theoretical Neuroscience, University of California, Berkeley; Redwood Center for Theoretical Neuroscience, University of California, Berkeley; Redwood Center for Theoretical Neuroscience, University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/87a5f58d631d63911c1bc71d6270c5cd-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Redwood Center for Theoretical Neuroscience",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Iterative Neural Autoregressive Distribution Estimator NADE-k",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4634",
        "id": "4634",
        "author_site": "Tapani Raiko, Yao Li, Kyunghyun Cho, Yoshua Bengio",
        "author": "Tapani Raiko; Li Yao; KyungHyun Cho; Yoshua Bengio",
        "abstract": "Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in $k$ steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-predictive training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested.",
        "bibtex": "@inproceedings{NIPS2014_b01ea303,\n author = {Raiko, Tapani and Yao, Li and Cho, KyungHyun and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Iterative Neural Autoregressive Distribution Estimator NADE-k},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b01ea3038c58ad1bb73aa015eca90770-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b01ea3038c58ad1bb73aa015eca90770-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b01ea3038c58ad1bb73aa015eca90770-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b01ea3038c58ad1bb73aa015eca90770-Reviews.html",
        "metareview": "",
        "pdf_size": 579688,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16368724010911710149&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b01ea3038c58ad1bb73aa015eca90770-Abstract.html"
    },
    {
        "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4636",
        "id": "4636",
        "author_site": "Jonathan J Tompson, Arjun Jain, Yann LeCun, Christoph Bregler",
        "author": "Jonathan Tompson; Arjun Jain; Yann LeCun; Christoph Bregler",
        "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.",
        "bibtex": "@inproceedings{NIPS2014_893643e2,\n author = {Tompson, Jonathan and Jain, Arjun and LeCun, Yann and Bregler, Christoph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/893643e2dcd4b25212defd18141d58c4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/893643e2dcd4b25212defd18141d58c4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/893643e2dcd4b25212defd18141d58c4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/893643e2dcd4b25212defd18141d58c4-Reviews.html",
        "metareview": "",
        "pdf_size": 1221238,
        "gs_citation": 2108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2160896869094670039&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "New York University; New York University; New York University; New York University",
        "aff_domain": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/893643e2dcd4b25212defd18141d58c4-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Just-In-Time Learning for Fast and Flexible Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4637",
        "id": "4637",
        "author_site": "S. M. Ali Eslami, Danny Tarlow, Pushmeet Kohli, John Winn",
        "author": "S. M. Ali Eslami; Daniel Tarlow; Pushmeet Kohli; John Winn",
        "abstract": "Much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient. The problem is extremely challenging and general inference remains computationally expensive. We seek to address this problem by observing that in most specific applications of a model, we typically only need to perform a small subset of all possible inference computations. Motivated by this, we introduce just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time. Through a series of experiments, we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing.",
        "bibtex": "@inproceedings{NIPS2014_c04f25be,\n author = {Eslami, S. M. Ali and Tarlow, Daniel and Kohli, Pushmeet and Winn, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Just-In-Time Learning for Fast and Flexible Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c04f25be56ab86371563568dce31808f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c04f25be56ab86371563568dce31808f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c04f25be56ab86371563568dce31808f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c04f25be56ab86371563568dce31808f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c04f25be56ab86371563568dce31808f-Reviews.html",
        "metareview": "",
        "pdf_size": 1095927,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7331846466074600391&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c04f25be56ab86371563568dce31808f-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Kernel Mean Estimation via Spectral Filtering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4638",
        "id": "4638",
        "author_site": "Krikamol Muandet, Bharath Sriperumbudur, Bernhard Sch\u00f6lkopf",
        "author": "Krikamol Muandet; Bharath Sriperumbudur; Bernhard Sch\u00f6lkopf",
        "abstract": "The problem of estimating the kernel mean in a reproducing kernel Hilbert space (RKHS) is central to kernel methods in that it is used by classical approaches (e.g., when centering a kernel PCA matrix), and it also forms the core inference step of modern kernel methods (e.g., kernel-based non-parametric tests) that rely on embedding probability distributions in RKHSs. Previous work [1] has shown that shrinkage can help in constructing \u201cbetter\u201d estimators of the kernel mean than the empirical estimator. The present paper studies the consistency and admissibility of the estimators in [1], and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions. Using the kernel PCA basis, we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions. Our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework. The proposed estimators are simple to implement and perform well in practice.",
        "bibtex": "@inproceedings{NIPS2014_099268c3,\n author = {Muandet, Krikamol and Sriperumbudur, Bharath and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Mean Estimation via Spectral Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/099268c3121d49937a67a052c51f865d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/099268c3121d49937a67a052c51f865d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/099268c3121d49937a67a052c51f865d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/099268c3121d49937a67a052c51f865d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/099268c3121d49937a67a052c51f865d-Reviews.html",
        "metareview": "",
        "pdf_size": 255691,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=463268871781302766&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "MPI-IS, T\u00fcbingen; Dept. of Statistics, PSU; MPI-IS, T\u00fcbingen",
        "aff_domain": "tue.mpg.de;psu.edu;tue.mpg.de",
        "email": "tue.mpg.de;psu.edu;tue.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/099268c3121d49937a67a052c51f865d-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Pennsylvania State University",
        "aff_unique_dep": ";Department of Statistics",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://www.psu.edu",
        "aff_unique_abbr": "MPI-IS;PSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "LSDA: Large Scale Detection through Adaptation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4609",
        "id": "4609",
        "author_site": "Judy Hoffman, Sergio Guadarrama, Eric Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko",
        "author": "Judy Hoffman; Sergio Guadarrama; Eric Tzeng; Ronghang Hu; Jeff Donahue; Ross Girshick; Trevor Darrell; Kate Saenko",
        "abstract": "A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at",
        "bibtex": "@inproceedings{NIPS2014_3a2ee4c8,\n author = {Hoffman, Judy and Guadarrama, Sergio and Tzeng, Eric and Hu, Ronghang and Donahue, Jeff and Girshick, Ross and Darrell, Trevor and Saenko, Kate},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {LSDA: Large Scale Detection through Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3a2ee4c801c8820c72af84e6b6c7ad2e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3a2ee4c801c8820c72af84e6b6c7ad2e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3a2ee4c801c8820c72af84e6b6c7ad2e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3a2ee4c801c8820c72af84e6b6c7ad2e-Reviews.html",
        "metareview": "",
        "pdf_size": 1394977,
        "gs_citation": 392,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2452150234195977094&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "EECS, UC Berkeley; EECS, UC Berkeley; EECS, UC Berkeley; EE, Tsinghua University; EECS, UC Berkeley; EECS, UC Berkeley; EECS, UC Berkeley; CS, UMass Lowell",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;mails.tsinghua.edu.cn;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;cs.uml.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;mails.tsinghua.edu.cn;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;cs.uml.edu",
        "github": "",
        "project": "lsda.berkeleyvision.org",
        "author_num": 8,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3a2ee4c801c8820c72af84e6b6c7ad2e-Abstract.html",
        "aff_unique_index": "0;0;0;1;0;0;0;2",
        "aff_unique_norm": "University of California, Berkeley;Tsinghua University;University of Massachusetts Lowell",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences;Department of Electrical Engineering;Computer Science",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tsinghua.edu.cn;https://www.uml.edu",
        "aff_unique_abbr": "UC Berkeley;THU;UMass Lowell",
        "aff_campus_unique_index": "0;0;0;0;0;0;2",
        "aff_campus_unique": "Berkeley;;Lowell",
        "aff_country_unique_index": "0;0;0;1;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Large-Margin Convex Polytope Machine",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4639",
        "id": "4639",
        "author_site": "Alex Kantchelian, Michael C Tschantz, Ling Huang, Peter Bartlett, Anthony D Joseph, J. D. Tygar",
        "author": "Alex Kantchelian; Michael Carl Tschantz; Ling Huang; Peter L. Bartlett; Anthony D. Joseph; J. D. Tygar",
        "abstract": "We present the Convex Polytope Machine (CPM), a novel non-linear learning algorithm for large-scale binary classification tasks. The CPM finds a large margin convex polytope separator which encloses one class. We develop a stochastic gradient descent based algorithm that is amenable to massive datasets, and augment it with a heuristic procedure to avoid sub-optimal local minima. Our experimental evaluations of the CPM on large-scale datasets from distinct domains (MNIST handwritten digit recognition, text topic, and web security) demonstrate that the CPM trains models faster, sometimes several orders of magnitude, than state-of-the-art similar approaches and kernel-SVM methods while achieving comparable or better classification performance. Our empirical results suggest that, unlike prior similar approaches, we do not need to control the number of sub-classifiers (sides of the polytope) to avoid overfitting.",
        "bibtex": "@inproceedings{NIPS2014_320f39ca,\n author = {Kantchelian, Alex and Tschantz, Michael Carl and Huang, Ling and Bartlett, Peter L. and Joseph, Anthony D. and Tygar, J. D.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large-Margin Convex Polytope Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/320f39caebd792d18483222f92c4498e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/320f39caebd792d18483222f92c4498e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/320f39caebd792d18483222f92c4498e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/320f39caebd792d18483222f92c4498e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/320f39caebd792d18483222f92c4498e-Reviews.html",
        "metareview": "",
        "pdf_size": 473706,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11584662728399390215&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley + Datavisor; UC Berkeley; UC Berkeley; UC Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;datavisor.com;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;datavisor.com;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/320f39caebd792d18483222f92c4498e-Abstract.html",
        "aff_unique_index": "0;0;0+1;0;0;0",
        "aff_unique_norm": "University of California, Berkeley;Datavisor",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.datavisor.com",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Large-scale L-BFGS using MapReduce",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4396",
        "id": "4396",
        "author_site": "Weizhu Chen, Zhenghao Wang, Jingren Zhou",
        "author": "Weizhu Chen; Zhenghao Wang; Jingren Zhou",
        "abstract": "L-BFGS has been applied as an effective parameter estimation method for various machine learning algorithms since 1980s. With an increasing demand to deal with massive instances and variables, it is important to scale up and parallelize L-BFGS effectively in a distributed system. In this paper, we study the problem of parallelizing the L-BFGS algorithm in large clusters of tens of thousands of shared-nothing commodity machines. First, we show that a naive implementation of L-BFGS using Map-Reduce requires either a significant amount of memory or a large number of map-reduce steps with negative performance impact. Second, we propose a new L-BFGS algorithm, called Vector-free L-BFGS, which avoids the expensive dot product operations in the two loop recursion and greatly improves computation efficiency with a great degree of parallelism. The algorithm scales very well and enables a variety of machine learning algorithms to handle a massive number of variables over large datasets. We prove the mathematical equivalence of the new Vector-free L-BFGS and demonstrate its excellent performance and scalability using real-world machine learning problems with billions of variables in production clusters.",
        "bibtex": "@inproceedings{NIPS2014_686e9abc,\n author = {Chen, Weizhu and Wang, Zhenghao and Zhou, Jingren},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large-scale L-BFGS using MapReduce},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/686e9abcab1c429b1a85913171cd2376-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/686e9abcab1c429b1a85913171cd2376-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/686e9abcab1c429b1a85913171cd2376-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/686e9abcab1c429b1a85913171cd2376-Reviews.html",
        "metareview": "",
        "pdf_size": 206222,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13532044042289095452&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft; Microsoft; Microsoft",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/686e9abcab1c429b1a85913171cd2376-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Corporation",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Latent Support Measure Machines for Bag-of-Words Data Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4641",
        "id": "4641",
        "author_site": "Yuya Yoshikawa, Tomoharu Iwata, Hiroshi Sawada",
        "author": "Yuya Yoshikawa; Tomoharu Iwata; Hiroshi Sawada",
        "abstract": "In many classification problems, the input is represented as a set of features, e.g., the bag-of-words (BoW) representation of documents. Support vector machines (SVMs) are widely used tools for such classification problems. The performance of the SVMs is generally determined by whether kernel values between data points can be defined properly. However, SVMs for BoW representations have a major weakness in that the co-occurrence of different but semantically similar words cannot be reflected in the kernel calculation. To overcome the weakness, we propose a kernel-based discriminative classifier for BoW data, which we call the latent support measure machine (latent SMM). With the latent SMM, a latent vector is associated with each vocabulary term, and each document is represented as a distribution of the latent vectors for words appearing in the document. To represent the distributions efficiently, we use the kernel embeddings of distributions that hold high order moment information about distributions. Then the latent SMM finds a separating hyperplane that maximizes the margins between distributions of different classes while estimating latent vectors for words to improve the classification performance. In the experiments, we show that the latent SMM achieves state-of-the-art accuracy for BoW text classification, is robust with respect to its own hyper-parameters, and is useful to visualize words.",
        "bibtex": "@inproceedings{NIPS2014_2ee30f32,\n author = {Yoshikawa, Yuya and Iwata, Tomoharu and Sawada, Hiroshi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Latent Support Measure Machines for Bag-of-Words Data Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2ee30f32fc44b88955b02c8a08aa069e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2ee30f32fc44b88955b02c8a08aa069e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2ee30f32fc44b88955b02c8a08aa069e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2ee30f32fc44b88955b02c8a08aa069e-Reviews.html",
        "metareview": "",
        "pdf_size": 1260344,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16021108302266579412&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Nara Institute of Science and Technology; NTT Communication Science Laboratories; NTT Service Evolution Laboratories",
        "aff_domain": "is.naist.jp;lab.ntt.co.jp;lab.ntt.co.jp",
        "email": "is.naist.jp;lab.ntt.co.jp;lab.ntt.co.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2ee30f32fc44b88955b02c8a08aa069e-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Nara Institute of Science and Technology;NTT Communication Science Laboratories;NTT Service Evolution Laboratories",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nist.go.jp;https://www.ntt-csl.com;https://www.ntt.co.jp",
        "aff_unique_abbr": "NIST;NTT CSL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Learning Chordal Markov Networks by Dynamic Programming",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4642",
        "id": "4642",
        "author_site": "Kustaa Kangas, Mikko Koivisto, Teppo Niinim\u00e4ki",
        "author": "Kustaa Kangas; Teppo Niinim\u00e4ki; Mikko Koivisto",
        "abstract": "We present an algorithm for finding a chordal Markov network that maximizes any given decomposable scoring function. The algorithm is based on a recursive characterization of clique trees, and it runs in O(4^n) time for n vertices. On an eight-vertex benchmark instance, our implementation turns out to be about ten million times faster than a recently proposed, constraint satisfaction based algorithm (Corander et al., NIPS 2013). Within a few hours, it is able to solve instances up to 18 vertices, and beyond if we restrict the maximum clique size. We also study the performance of a recent integer linear programming algorithm (Bartlett and Cussens, UAI 2013). Our results suggest that, unless we bound the clique sizes, currently only the dynamic programming algorithm is guaranteed to solve instances with around 15 or more vertices.",
        "bibtex": "@inproceedings{NIPS2014_fdcd4e3d,\n author = {Kangas, Kustaa and Niinim\\\"{a}ki, Teppo and Koivisto, Mikko},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Chordal Markov Networks by Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/fdcd4e3d6f6c8c80d00cd5232785d87b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/fdcd4e3d6f6c8c80d00cd5232785d87b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/fdcd4e3d6f6c8c80d00cd5232785d87b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/fdcd4e3d6f6c8c80d00cd5232785d87b-Reviews.html",
        "metareview": "",
        "pdf_size": 5774629,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2690217415419355015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Helsinki Institute for Information Technology HIIT + Department of Computer Science, University of Helsinki; Helsinki Institute for Information Technology HIIT + Department of Computer Science, University of Helsinki; Helsinki Institute for Information Technology HIIT + Department of Computer Science, University of Helsinki",
        "aff_domain": "cs.helsinki.fi;cs.helsinki.fi;cs.helsinki.fi",
        "email": "cs.helsinki.fi;cs.helsinki.fi;cs.helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/fdcd4e3d6f6c8c80d00cd5232785d87b-Abstract.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Helsinki Institute for Information Technology;University of Helsinki",
        "aff_unique_dep": "HIIT;Department of Computer Science",
        "aff_unique_url": "https://www.hiit.fi;https://www.helsinki.fi",
        "aff_unique_abbr": "HIIT;UH",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Learning Deep Features for Scene Recognition using Places Database",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4398",
        "id": "4398",
        "author_site": "Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, Aude Oliva",
        "author": "Bolei Zhou; Agata Lapedriza; Jianxiong Xiao; Antonio Torralba; Aude Oliva",
        "abstract": "Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",
        "bibtex": "@inproceedings{NIPS2014_19ea3982,\n author = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Deep Features for Scene Recognition using Places Database},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/19ea3982b415d7bb3363917eb3d60c4a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/19ea3982b415d7bb3363917eb3d60c4a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/19ea3982b415d7bb3363917eb3d60c4a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/19ea3982b415d7bb3363917eb3d60c4a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/19ea3982b415d7bb3363917eb3d60c4a-Reviews.html",
        "metareview": "",
        "pdf_size": 2096123,
        "gs_citation": 3870,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18419800374042180760&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 26,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology + Universitat Oberta de Catalunya; Princeton University; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/19ea3982b415d7bb3363917eb3d60c4a-Abstract.html",
        "aff_unique_index": "0;0+1;2;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Universitat Oberta de Catalunya;Princeton University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://web.mit.edu;https://www.uoc.edu;https://www.princeton.edu",
        "aff_unique_abbr": "MIT;UOC;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0;0",
        "aff_country_unique": "United States;Spain"
    },
    {
        "title": "Learning Distributed Representations for Structured Output Prediction",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4400",
        "id": "4400",
        "author_site": "Vivek Srikumar, Christopher D Manning",
        "author": "Vivek Srikumar; Christopher D. Manning",
        "abstract": "In recent years, distributed representations of inputs have led to performance gains in many applications by allowing statistical information to be shared across inputs. However, the predicted outputs (labels, and more generally structures) are still treated as discrete objects even though outputs are often not discrete units of meaning. In this paper, we present a new formulation for structured prediction where we represent individual labels in a structure as dense vectors and allow semantically similar labels to share parameters. We extend this representation to larger structures by defining compositionality using tensor products to give a natural generalization of standard structured prediction approaches. We define a learning objective for jointly learning the model parameters and the label vectors and propose an alternating minimization algorithm for learning. We show that our formulation outperforms structural SVM baselines in two tasks: multiclass document classification and part-of-speech tagging.",
        "bibtex": "@inproceedings{NIPS2014_94b80c78,\n author = {Srikumar, Vivek and Manning, Christopher D},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Distributed Representations for Structured Output Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/94b80c785d78e6d22b491ec5125cf698-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/94b80c785d78e6d22b491ec5125cf698-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/94b80c785d78e6d22b491ec5125cf698-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/94b80c785d78e6d22b491ec5125cf698-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/94b80c785d78e6d22b491ec5125cf698-Reviews.html",
        "metareview": "",
        "pdf_size": 293860,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8336593633617186899&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Utah; Stanford University",
        "aff_domain": "cs.utah.edu;cs.stanford.edu",
        "email": "cs.utah.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/94b80c785d78e6d22b491ec5125cf698-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Utah;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utah.edu;https://www.stanford.edu",
        "aff_unique_abbr": "Utah;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning From Weakly Supervised Data by The Expectation Loss SVM (e-SVM) algorithm",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4643",
        "id": "4643",
        "author_site": "Jun Zhu, Junhua Mao, Alan Yuille",
        "author": "Jun Zhu; Junhua Mao; Alan Yuille",
        "abstract": "In many situations we have some measurement of confidence on",
        "bibtex": "@inproceedings{NIPS2014_bd41ae48,\n author = {Zhu, Jun and Mao, Junhua and Yuille, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning From Weakly Supervised Data by The Expectation Loss SVM (e-SVM) algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/bd41ae48f2464ea2a26a5dcc6b6da12d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/bd41ae48f2464ea2a26a5dcc6b6da12d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/bd41ae48f2464ea2a26a5dcc6b6da12d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/bd41ae48f2464ea2a26a5dcc6b6da12d-Reviews.html",
        "metareview": "",
        "pdf_size": 956900,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1098518492154005263&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Statistics, University of California, Los Angeles; Department of Statistics, University of California, Los Angeles; Department of Statistics, University of California, Los Angeles",
        "aff_domain": "ucla.edu;ucla.edu;stat.ucla.edu",
        "email": "ucla.edu;ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/bd41ae48f2464ea2a26a5dcc6b6da12d-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Generative Models with Visual Attention",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4336",
        "id": "4336",
        "author_site": "Yichuan Charlie Tang, Nitish Srivastava, Russ Salakhutdinov",
        "author": "Yichuan Tang; Nitish Srivastava; Ruslan Salakhutdinov",
        "abstract": "Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.",
        "bibtex": "@inproceedings{NIPS2014_44ae520b,\n author = {Tang, Yichuan and Srivastava, Nitish and Salakhutdinov, Ruslan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Generative Models with Visual Attention},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/44ae520b092f21e95e40e08c80df5a5e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/44ae520b092f21e95e40e08c80df5a5e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/44ae520b092f21e95e40e08c80df5a5e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/44ae520b092f21e95e40e08c80df5a5e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/44ae520b092f21e95e40e08c80df5a5e-Reviews.html",
        "metareview": "",
        "pdf_size": 3931284,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4111955262491058565&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of Toronto, Toronto, Ontario, Canada; Department of Computer Science, University of Toronto, Toronto, Ontario, Canada; Department of Computer Science, University of Toronto, Toronto, Ontario, Canada",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/44ae520b092f21e95e40e08c80df5a5e-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Learning Mixed Multinomial Logit Model from Ordinal Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4644",
        "id": "4644",
        "author_site": "Sewoong Oh, Devavrat Shah",
        "author": "Sewoong Oh; Devavrat Shah",
        "abstract": "Motivated by generating personalized recommendations using ordinal (or preference) data, we study the question of learning a mixture of MultiNomial Logit (MNL) model, a parameterized class of distributions over permutations, from partial ordinal or preference data (e.g. pair-wise comparisons). Despite its long standing importance across disciplines including social choice, operations research and revenue management, little is known about this question. In case of single MNL models (no mixture), computationally and statistically tractable learning from pair-wise comparisons is feasible. However, even learning mixture of two MNL model is infeasible in general. Given this state of affairs, we seek conditions under which it is feasible to learn the mixture model in both computationally and statistically efficient manner. To that end, we present a sufficient condition as well as an efficient algorithm for learning mixed MNL models from partial preferences/comparisons data. In particular, a mixture of $r$ MNL components over $n$ objects can be learnt using samples whose size scales polynomially in $n$ and $r$ (concretely, $n^3 r^{3.5} \\log^4 n$, with $r \\ll n^{2/7}$ when the model parameters are sufficiently {\\em incoherent}). The algorithm has two phases: first, learn the pair-wise marginals for each component using tensor decomposition; second, learn the model parameters for each component using RankCentrality introduced by Negahban et al. In the process of proving these results, we obtain a generalization of existing analysis for tensor decomposition to a more realistic regime where only partial information about each sample is available.",
        "bibtex": "@inproceedings{NIPS2014_b33336b8,\n author = {Oh, Sewoong and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Mixed Multinomial Logit Model from Ordinal Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b33336b853dc43b9f4a96cedf6bd4b30-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b33336b853dc43b9f4a96cedf6bd4b30-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b33336b853dc43b9f4a96cedf6bd4b30-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b33336b853dc43b9f4a96cedf6bd4b30-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b33336b853dc43b9f4a96cedf6bd4b30-Reviews.html",
        "metareview": "",
        "pdf_size": 331241,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7848512067674321916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Industrial and Enterprise Systems Engr., University of Illinois at Urbana-Champaign; Department of Electrical Engineering, Massachussetts Institute of Technology",
        "aff_domain": "illinois.edu;mit.edu",
        "email": "illinois.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b33336b853dc43b9f4a96cedf6bd4b30-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Massachusetts Institute of Technology",
        "aff_unique_dep": "Dept. of Industrial and Enterprise Systems Engr.;Department of Electrical Engineering",
        "aff_unique_url": "https://www illinois.edu;https://web.mit.edu",
        "aff_unique_abbr": "UIUC;MIT",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Urbana-Champaign;Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Mixtures of Ranking Models",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4402",
        "id": "4402",
        "author_site": "Pranjal Awasthi, Avrim Blum, Or Sheffet, Aravindan Vijayaraghavan",
        "author": "Pranjal Awasthi; Avrim Blum; Or Sheffet; Aravindan Vijayaraghavan",
        "abstract": "This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a {\\em Mallows Mixture Model}. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-$k$ prefix in both the rankings. Before this work, even the question of {\\em identifiability} in the case of a mixture of two Mallows models was unresolved.",
        "bibtex": "@inproceedings{NIPS2014_e40e5b78,\n author = {Awasthi, Pranjal and Blum, Avrim and Sheffet, Or and Vijayaraghavan, Aravindan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Mixtures of Ranking Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e40e5b7841b81e794e14fd9be7307c9e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e40e5b7841b81e794e14fd9be7307c9e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e40e5b7841b81e794e14fd9be7307c9e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e40e5b7841b81e794e14fd9be7307c9e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e40e5b7841b81e794e14fd9be7307c9e-Reviews.html",
        "metareview": "",
        "pdf_size": 396297,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6825254078984685078&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Princeton University; Carnegie Mellon University; Harvard University; New York University",
        "aff_domain": "cs.princeton.edu;cs.cmu.edu;seas.harvard.edu;cims.nyu.edu",
        "email": "cs.princeton.edu;cs.cmu.edu;seas.harvard.edu;cims.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e40e5b7841b81e794e14fd9be7307c9e-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Princeton University;Carnegie Mellon University;Harvard University;New York University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.princeton.edu;https://www.cmu.edu;https://www.harvard.edu;https://www.nyu.edu",
        "aff_unique_abbr": "Princeton;CMU;Harvard;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Mixtures of Submodular Functions for Image Collection Summarization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4645",
        "id": "4645",
        "author_site": "Sebastian Tschiatschek, Rishabh K Iyer, Haochen Wei, Jeffrey A Bilmes",
        "author": "Sebastian Tschiatschek; Rishabh Iyer; Haochen Wei; Jeff Bilmes",
        "abstract": "We address the problem of image collection summarization by learning mixtures of submodular functions. We argue that submodularity is very natural to this problem, and we show that a number of previously used scoring functions are submodular \u2014 a property not explicitly mentioned in these publications. We provide classes of submodular functions capturing the necessary properties of summaries, namely coverage, likelihood, and diversity. To learn mixtures of these submodular functions as scoring functions, we formulate summarization as a supervised learning problem using large-margin structured prediction. Furthermore, we introduce a novel evaluation metric, which we call V-ROUGE, for automatic summary scoring. While a similar metric called ROUGE has been successfully applied to document summarization [14], no such metric was known for quantifying the quality of image collection summaries. We provide a new dataset consisting of 14 real-world image collections along with many human-generated ground truth summaries collected using mechanical turk. We also extensively compare our method with previously explored methods for this problem and show that our learning approach outperforms all competitors on this new dataset. This paper provides, to our knowledge, the first systematic approach for quantifying the problem of image collection summarization, along with a new dataset of image collections and human summaries.",
        "bibtex": "@inproceedings{NIPS2014_41321d69,\n author = {Tschiatschek, Sebastian and Iyer, Rishabh and Wei, Haochen and Bilmes, Jeff},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Mixtures of Submodular Functions for Image Collection Summarization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/41321d693c015a6a92f55f29c8a76079-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/41321d693c015a6a92f55f29c8a76079-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/41321d693c015a6a92f55f29c8a76079-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/41321d693c015a6a92f55f29c8a76079-Reviews.html",
        "metareview": "",
        "pdf_size": 2187775,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7615948095337987938&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical Engineering, Graz University of Technology; Department of Electrical Engineering, University of Washington; LinkedIn + Department of Electrical Engineering, University of Washington; Department of Electrical Engineering, University of Washington",
        "aff_domain": "tugraz.at;u.washington.edu;gmail.com;u.washington.edu",
        "email": "tugraz.at;u.washington.edu;gmail.com;u.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/41321d693c015a6a92f55f29c8a76079-Abstract.html",
        "aff_unique_index": "0;1;2+1;1",
        "aff_unique_norm": "Graz University of Technology;University of Washington;LinkedIn Corporation",
        "aff_unique_dep": "Department of Electrical Engineering;Department of Electrical Engineering;",
        "aff_unique_url": "https://www.tugraz.at;https://www.washington.edu;https://www.linkedin.com",
        "aff_unique_abbr": "TUGraz;UW;LinkedIn",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Graz;Seattle;",
        "aff_country_unique_index": "0;1;1+1;1",
        "aff_country_unique": "Austria;United States"
    },
    {
        "title": "Learning Multiple Tasks in Parallel with a Shared Annotator",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4646",
        "id": "4646",
        "author_site": "Haim Cohen, Yacov Crammer",
        "author": "Haim Cohen; Koby Crammer",
        "abstract": "We introduce a new multi-task framework, in which $K$ online learners are sharing a single annotator with limited bandwidth. On each round, each of the $K$ learners receives an input, and makes a prediction about the label of that input. Then, a shared (stochastic) mechanism decides which of the $K$ inputs will be annotated. The learner that receives the feedback (label) may update its prediction rule, and we proceed to the next round. We develop an online algorithm for multi-task binary classification that learns in this setting, and bound its performance in the worst-case setting. Additionally, we show that our algorithm can be used to solve two bandits problems: contextual bandits, and dueling bandits with context, both allowed to decouple exploration and exploitation. Empirical study with OCR data, vowel prediction (VJ project) and document classification, shows that our algorithm outperforms other algorithms, one of which uses uniform allocation, and essentially makes more (accuracy) for the same labour of the annotator.",
        "bibtex": "@inproceedings{NIPS2014_30fbd5e0,\n author = {Cohen, Haim and Crammer, Koby},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Multiple Tasks in Parallel with a Shared Annotator},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/30fbd5e091f51d7cf19153ccd3a4c969-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/30fbd5e091f51d7cf19153ccd3a4c969-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/30fbd5e091f51d7cf19153ccd3a4c969-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/30fbd5e091f51d7cf19153ccd3a4c969-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/30fbd5e091f51d7cf19153ccd3a4c969-Reviews.html",
        "metareview": "",
        "pdf_size": 424488,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16865471598931795490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical Engeneering, The Technion \u2013 Israel Institute of Technology, Haifa, 32000 Israel; Department of Electrical Engeneering, The Technion \u2013 Israel Institute of Technology, Haifa, 32000 Israel",
        "aff_domain": "tx.technion.ac.il;ee.technion.ac.il",
        "email": "tx.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/30fbd5e091f51d7cf19153ccd3a4c969-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion \u2013 Israel Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4462",
        "id": "4462",
        "author_site": "Sergey Levine, Pieter Abbeel",
        "author": "Sergey Levine; Pieter Abbeel",
        "abstract": "We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.",
        "bibtex": "@inproceedings{NIPS2014_c7c9344b,\n author = {Levine, Sergey and Abbeel, Pieter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c7c9344b5a3c0533e29fa69ce807cf08-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c7c9344b5a3c0533e29fa69ce807cf08-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c7c9344b5a3c0533e29fa69ce807cf08-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c7c9344b5a3c0533e29fa69ce807cf08-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c7c9344b5a3c0533e29fa69ce807cf08-Reviews.html",
        "metareview": "",
        "pdf_size": 1325477,
        "gs_citation": 653,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13351610957787428402&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c7c9344b5a3c0533e29fa69ce807cf08-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Optimal Commitment to Overcome Insecurity",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4655",
        "id": "4655",
        "author_site": "Avrim Blum, Nika Haghtalab, Ariel Procaccia",
        "author": "Avrim Blum; Nika Haghtalab; Ariel D. Procaccia",
        "abstract": "Game-theoretic algorithms for physical security have made an impressive real-world impact. These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game, where the attacker observes the defender's strategy and best-responds. In order to build the game model, though, the payoffs of potential attackers for various outcomes must be estimated; inaccurate estimates can lead to significant inefficiencies. We design an algorithm that optimizes the defender's strategy with no prior information, by observing the attacker's responses to randomized deployments of resources and learning his priorities. In contrast to previous work, our algorithm requires a number of queries that is polynomial in the representation of the game.",
        "bibtex": "@inproceedings{NIPS2014_d3eea90e,\n author = {Blum, Avrim and Haghtalab, Nika and Procaccia, Ariel D.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Optimal Commitment to Overcome Insecurity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d3eea90e8e9cff58fc84a4c40d22d95a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d3eea90e8e9cff58fc84a4c40d22d95a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d3eea90e8e9cff58fc84a4c40d22d95a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d3eea90e8e9cff58fc84a4c40d22d95a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d3eea90e8e9cff58fc84a4c40d22d95a-Reviews.html",
        "metareview": "",
        "pdf_size": 318173,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2002797228898181339&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d3eea90e8e9cff58fc84a4c40d22d95a-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Shuffle Ideals Under Restricted Distributions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4647",
        "id": "4647",
        "author": "Dongqu Chen",
        "abstract": "The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set $U$ is the collection of all strings containing some string $u \\in U$ as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper, we study the PAC learnability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.",
        "bibtex": "@inproceedings{NIPS2014_d6457e3a,\n author = {Chen, Dongqu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Shuffle Ideals Under Restricted Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d6457e3aa79c5864bd2928fd01e572be-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d6457e3aa79c5864bd2928fd01e572be-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d6457e3aa79c5864bd2928fd01e572be-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d6457e3aa79c5864bd2928fd01e572be-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d6457e3aa79c5864bd2928fd01e572be-Reviews.html",
        "metareview": "",
        "pdf_size": 275635,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5708767592270442752&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Yale University",
        "aff_domain": "yale.edu",
        "email": "yale.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d6457e3aa79c5864bd2928fd01e572be-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Time-Varying Coverage Functions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4648",
        "id": "4648",
        "author_site": "Nan Du, Yingyu Liang, Maria-Florina F Balcan, Le Song",
        "author": "Nan Du; Yingyu Liang; Maria-Florina Balcan; Le Song",
        "abstract": "Coverage functions are an important class of discrete functions that capture laws of diminishing returns. In this paper, we propose a new problem of learning time-varying coverage functions which arise naturally from applications in social network analysis, machine learning, and algorithmic game theory. We develop a novel parametrization of the time-varying coverage function by illustrating the connections with counting processes. We present an efficient algorithm to learn the parameters by maximum likelihood estimation, and provide a rigorous theoretic analysis of its sample complexity. Empirical experiments from information diffusion in social network analysis demonstrate that with few assumptions about the underlying diffusion process, our method performs significantly better than existing approaches on both synthetic and real world data.",
        "bibtex": "@inproceedings{NIPS2014_d829550b,\n author = {Du, Nan and Liang, Yingyu and Balcan, Maria-Florina and Song, Le},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Time-Varying Coverage Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d829550b42b10dd1cea8947479e39297-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d829550b42b10dd1cea8947479e39297-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d829550b42b10dd1cea8947479e39297-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d829550b42b10dd1cea8947479e39297-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d829550b42b10dd1cea8947479e39297-Reviews.html",
        "metareview": "",
        "pdf_size": 380064,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16049453918134151764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "College of Computing, Georgia Institute of Technology; Department of Computer Science, Princeton University; School of Computer Science, Carnegie Mellon University; College of Computing, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;cs.princeton.edu;cs.cmu.edu;cc.gatech.edu",
        "email": "gatech.edu;cs.princeton.edu;cs.cmu.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d829550b42b10dd1cea8947479e39297-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Georgia Institute of Technology;Princeton University;Carnegie Mellon University",
        "aff_unique_dep": "College of Computing;Department of Computer Science;School of Computer Science",
        "aff_unique_url": "https://www.gatech.edu;https://www.princeton.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Georgia Tech;Princeton;CMU",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Atlanta;;Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning a Concept Hierarchy from Multi-labeled Documents",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4649",
        "id": "4649",
        "author_site": "Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, Jonathan Chang",
        "author": "Viet-An Nguyen; Jordan Boyd-Graber; Philip Resnik; Jonathan Chang",
        "abstract": "While topic models can discover patterns of word usage in large corpora, it is difficult to meld this unsupervised structure with noisy, human-provided labels, especially when the label space is large. In this paper, we present a model-Label to Hierarchy (L2H)-that can induce a hierarchy of user-generated labels and the topics associated with those labels from a set of multi-labeled documents. The model is robust enough to account for missing labels from untrained, disparate annotators and provide an interpretable summary of an otherwise unwieldy label set. We show empirically the effectiveness of L2H in predicting held-out words and labels for unseen documents.",
        "bibtex": "@inproceedings{NIPS2014_67d09ee5,\n author = {Nguyen, Viet-An and Boyd-Graber, Jordan and Resnik, Philip and Chang, Jonathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning a Concept Hierarchy from Multi-labeled Documents},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/67d09ee5fa8a21c3d40d113abe02a658-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/67d09ee5fa8a21c3d40d113abe02a658-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/67d09ee5fa8a21c3d40d113abe02a658-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/67d09ee5fa8a21c3d40d113abe02a658-Reviews.html",
        "metareview": "",
        "pdf_size": 511766,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3492871078387905861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science+Linguistics+UMIACS, Univ. of Maryland, College Park, MD; Computer Science, Univ. of Colorado, Boulder, CO; Computer Science+Linguistics+UMIACS, Univ. of Maryland, College Park, MD; Facebook, Menlo Park, CA",
        "aff_domain": "cs.umd.edu;colorado.edu;umd.edu;fb.com",
        "email": "cs.umd.edu;colorado.edu;umd.edu;fb.com",
        "github": "",
        "project": "http://www.dmoz.org/; http://policyagendas.org/",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/67d09ee5fa8a21c3d40d113abe02a658-Abstract.html",
        "aff_unique_index": "0+1+2;3;0+1+2;4",
        "aff_unique_norm": "Computer Science;Linguistics Department;University of Maryland;University of Colorado Boulder;Meta",
        "aff_unique_dep": "Computer Science Department;Linguistics;UMIACS;Computer Science;Facebook",
        "aff_unique_url": ";;https://www.umd.edu;https://www.colorado.edu;https://www.facebook.com",
        "aff_unique_abbr": ";;UMD;CU Boulder;FB",
        "aff_campus_unique_index": "1;2;1;3",
        "aff_campus_unique": ";College Park;Boulder;Menlo Park",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Learning convolution filters for inverse covariance estimation of neural network connectivity",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4650",
        "id": "4650",
        "author_site": "George Mohler",
        "author": "George O. Mohler",
        "abstract": "We consider the problem of inferring direct neural network connections from Calcium imaging time series. Inverse covariance estimation has proven to be a fast and accurate method for learning macro- and micro-scale network connectivity in the brain and in a recent Kaggle Connectomics competition inverse covariance was the main component of several top ten solutions, including our own and the winning team's algorithm. However, the accuracy of inverse covariance estimation is highly sensitive to signal preprocessing of the Calcium fluorescence time series. Furthermore, brute force optimization methods such as grid search and coordinate ascent over signal processing parameters is a time intensive process, where learning may take several days and parameters that optimize one network may not generalize to networks with different size and parameters. In this paper we show how inverse covariance estimation can be dramatically improved using a simple convolution filter prior to applying sample covariance. Furthermore, these signal processing parameters can be learned quickly using a supervised optimization algorithm. In particular, we maximize a binomial log-likelihood loss function with respect to a convolution filter of the time series and the inverse covariance regularization parameter. Our proposed algorithm is relatively fast on networks the size of those in the competition (1000 neurons), producing AUC scores with similar accuracy to the winning solution in training time under 2 hours on a cpu. Prediction on new networks of the same size is carried out in less than 15 minutes, the time it takes to read in the data and write out the solution.",
        "bibtex": "@inproceedings{NIPS2014_06f714ec,\n author = {Mohler, George O.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning convolution filters for inverse covariance estimation of neural network connectivity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/06f714eca850a0799089c8e9f076ed7b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/06f714eca850a0799089c8e9f076ed7b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/06f714eca850a0799089c8e9f076ed7b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/06f714eca850a0799089c8e9f076ed7b-Reviews.html",
        "metareview": "",
        "pdf_size": 413700,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16308721187313971915&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Mathematics and Computer Science, Santa Clara University University, Santa Clara, CA, USA",
        "aff_domain": "scu.edu",
        "email": "scu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/06f714eca850a0799089c8e9f076ed7b-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Santa Clara University",
        "aff_unique_dep": "Department of Mathematics and Computer Science",
        "aff_unique_url": "https://www.scu.edu",
        "aff_unique_abbr": "SCU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Santa Clara",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning on graphs using Orthonormal Representation is Statistically Consistent",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4651",
        "id": "4651",
        "author_site": "Rakesh Shivanna, Chiranjib Bhattacharyya",
        "author": "Rakesh S; Chiranjib Bhattacharyya",
        "abstract": "Existing research \\cite{reg} suggests that embedding graphs on a unit sphere can be beneficial in learning labels on the vertices of a graph. However the choice of optimal embedding remains an open issue. \\emph{Orthonormal representation} of graphs, a class of embeddings over the unit sphere, was introduced by Lov\\'asz \\cite{lovasz_shannon}. In this paper, we show that there exists orthonormal representations which are statistically consistent over a large class of graphs, including power law and random graphs. This result is achieved by extending the notion of consistency designed in the inductive setting to graph transduction. As part of the analysis, we explicitly derive relationships between the Rademacher complexity measure and structural properties of graphs, such as the chromatic number. We further show the fraction of vertices of a graph $G$, on $n$ nodes, that need to be labelled for the learning algorithm to be consistent, also known as labelled sample complexity, is $ \\Omega\\left(\\frac{\\vartheta(G)}{n}\\right)^{\\frac{1}{4}}$ where $\\vartheta(G)$ is the famous Lov\\'asz~$\\vartheta$ function of the graph. This, for the first time, relates labelled sample complexity to graph connectivity properties, such as the density of graphs. In the multiview setting, whenever individual views are expressed by a graph, it is a well known heuristic that a convex combination of Laplacians \\cite{lap_mv1} tend to improve accuracy. The analysis presented here easily extends to Multiple graph transduction, and helps develop a sound statistical understanding of the heuristic, previously unavailable.",
        "bibtex": "@inproceedings{NIPS2014_7a04f491,\n author = {S, Rakesh and Bhattacharyya, Chiranjib},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning on graphs using Orthonormal Representation is Statistically Consistent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7a04f491d92f638d7e46766715885bdb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7a04f491d92f638d7e46766715885bdb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7a04f491d92f638d7e46766715885bdb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7a04f491d92f638d7e46766715885bdb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7a04f491d92f638d7e46766715885bdb-Reviews.html",
        "metareview": "",
        "pdf_size": 390330,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13959142685136512385&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Electrical Engineering, Indian Institute of Science, Bangalore, 560012, INDIA; Department of CSA, Indian Institute of Science, Bangalore, 560012, INDIA",
        "aff_domain": "gmail.com;csa.iisc.ernet.in",
        "email": "gmail.com;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7a04f491d92f638d7e46766715885bdb-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Learning the Learning Rate for Prediction with Expert Advice",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4652",
        "id": "4652",
        "author_site": "Wouter M Koolen, Tim van Erven, Peter Gr\u00fcnwald",
        "author": "Wouter M. Koolen; Tim van Erven; Peter D. Gr\u00fcnwald",
        "abstract": "Most standard algorithms for prediction with expert advice depend on a parameter called the learning rate. This learning rate needs to be large enough to fit the data well, but small enough to prevent overfitting. For the exponential weights algorithm, a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate, which allow for increasingly aggressive learning. But in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate. To close the gap between theory and practice we introduce an approach to learn the learning rate. Up to a factor that is at most (poly)logarithmic in the number of experts and the inverse of the learning rate, our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available. Our method employs a grid of learning rates, yet runs in linear time regardless of the size of the grid.",
        "bibtex": "@inproceedings{NIPS2014_5cac39b0,\n author = {Koolen, Wouter M. and van Erven, Tim and Gr\\\"{u}nwald, Peter D.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning the Learning Rate for Prediction with Expert Advice},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5cac39b0c5a237a1b54a737155436f0b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5cac39b0c5a237a1b54a737155436f0b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5cac39b0c5a237a1b54a737155436f0b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5cac39b0c5a237a1b54a737155436f0b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5cac39b0c5a237a1b54a737155436f0b-Reviews.html",
        "metareview": "",
        "pdf_size": 331267,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8727163381546905832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Queensland University of Technology and UC Berkeley; Leiden University, the Netherlands; Leiden University and Centrum Wiskunde & Informatica, the Netherlands",
        "aff_domain": "qut.edu.au;timvanerven.nl;cwi.nl",
        "email": "qut.edu.au;timvanerven.nl;cwi.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5cac39b0c5a237a1b54a737155436f0b-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Queensland University of Technology;Leiden University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.qut.edu.au;https://www.universiteitleiden.nl",
        "aff_unique_abbr": "QUT;LU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Australia;Netherlands"
    },
    {
        "title": "Learning to Discover Efficient Mathematical Identities",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4406",
        "id": "4406",
        "author_site": "Wojciech Zaremba, Karol Kurach, Rob Fergus",
        "author": "Wojciech Zaremba; Karol Kurach; Rob Fergus",
        "abstract": "In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a grammar of math operators, we build trees that combine them in different ways, looking for compositions that are analytically equivalent to a target expression but of lower computational complexity. However, as the space of trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neural-network. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation.",
        "bibtex": "@inproceedings{NIPS2014_569e7e21,\n author = {Zaremba, Wojciech and Kurach, Karol and Fergus, Rob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Discover Efficient Mathematical Identities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/569e7e214ffd489841b8fb34763840b2-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/569e7e214ffd489841b8fb34763840b2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/569e7e214ffd489841b8fb34763840b2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/569e7e214ffd489841b8fb34763840b2-Reviews.html",
        "metareview": "",
        "pdf_size": 918944,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3527123610419576702&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Science, Courant Institute, New York University; Google Zurich + Dept. of Computer Science, University of Warsaw; Dept. of Computer Science, Courant Institute, New York University",
        "aff_domain": "cs.nyu.edu;google.com;cs.nyu.edu",
        "email": "cs.nyu.edu;google.com;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/569e7e214ffd489841b8fb34763840b2-Abstract.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "New York University;Google;University of Warsaw",
        "aff_unique_dep": "Dept. of Computer Science;Google;Dept. of Computer Science",
        "aff_unique_url": "https://www.nyu.edu;https://www.google.ch;https://www.uw.edu.pl",
        "aff_unique_abbr": "NYU;Google Zurich;UW",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "New York;Zurich;",
        "aff_country_unique_index": "0;1+2;0",
        "aff_country_unique": "United States;Switzerland;Poland"
    },
    {
        "title": "Learning to Optimize via Information-Directed Sampling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4653",
        "id": "4653",
        "author_site": "Daniel Russo, Benjamin Van Roy",
        "author": "Daniel Russo; Benjamin Van Roy",
        "abstract": "We propose information-directed sampling -- a new algorithm for online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between the square of expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. For the widely studied Bernoulli and linear bandit models, we demonstrate simulation performance surpassing popular approaches, including upper confidence bound algorithms, Thompson sampling, and knowledge gradient. Further, we present simple analytic examples illustrating that information-directed sampling can dramatically outperform upper confidence bound algorithms and Thompson sampling due to the way it measures information gain.",
        "bibtex": "@inproceedings{NIPS2014_90720a2f,\n author = {Russo, Daniel and Van Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Optimize via Information-Directed Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/90720a2fcc41f9332e6a1558da327089-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/90720a2fcc41f9332e6a1558da327089-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/90720a2fcc41f9332e6a1558da327089-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/90720a2fcc41f9332e6a1558da327089-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/90720a2fcc41f9332e6a1558da327089-Reviews.html",
        "metareview": "",
        "pdf_size": 263318,
        "gs_citation": 263,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11489002253960633669&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/90720a2fcc41f9332e6a1558da327089-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Search in Branch and Bound Algorithms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4654",
        "id": "4654",
        "author_site": "He He, Hal Daum\u00e9 III, Jason Eisner",
        "author": "He He; Hal Daum\u00e9 III; Jason Eisner",
        "abstract": "Branch-and-bound is a widely used method in combinatorial optimization, including mixed integer programming, structured prediction and MAP inference. While most work has been focused on developing problem-specific techniques, little is known about how to systematically design the node searching strategy on a branch-and-bound tree. We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch-and-bound. Our strategies are learned by imitation learning. We apply our algorithm to linear programming based branch-and-bound for solving mixed integer programs (MIP). We compare our method with one of the fastest open-source solvers, SCIP; and a very efficient commercial solver, Gurobi. We demonstrate that our approach achieves better solutions faster on four MIP libraries.",
        "bibtex": "@inproceedings{NIPS2014_533d190f,\n author = {He, He and Daum\\'{e}, Hal and Eisner, Jason},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Search in Branch and Bound Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/533d190f5aa2926b2a8a30c8bea0e05d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/533d190f5aa2926b2a8a30c8bea0e05d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/533d190f5aa2926b2a8a30c8bea0e05d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/533d190f5aa2926b2a8a30c8bea0e05d-Reviews.html",
        "metareview": "",
        "pdf_size": 579136,
        "gs_citation": 342,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11803224834311197966&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland; Department of Computer Science, Johns Hopkins University",
        "aff_domain": "cs.umd.edu;cs.umd.edu;cs.jhu.edu",
        "email": "cs.umd.edu;cs.umd.edu;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/533d190f5aa2926b2a8a30c8bea0e05d-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Maryland;Johns Hopkins University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu;https://www.jhu.edu",
        "aff_unique_abbr": "UMD;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning with Fredholm Kernels",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4656",
        "id": "4656",
        "author_site": "Qichao Que, Mikhail Belkin, Yusu Wang",
        "author": "Qichao Que; Mikhail Belkin; Yusu Wang",
        "abstract": "In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the noise assumption\" for semi-supervised learning and provide evidence evidence both theoretical and experimental that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.\"",
        "bibtex": "@inproceedings{NIPS2014_c7d2a71e,\n author = {Que, Qichao and Belkin, Mikhail and Wang, Yusu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Fredholm Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c7d2a71e0e5693e07f9c390a90c44f31-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c7d2a71e0e5693e07f9c390a90c44f31-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c7d2a71e0e5693e07f9c390a90c44f31-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c7d2a71e0e5693e07f9c390a90c44f31-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c7d2a71e0e5693e07f9c390a90c44f31-Reviews.html",
        "metareview": "",
        "pdf_size": 531444,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8166918598354906604&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c7d2a71e0e5693e07f9c390a90c44f31-Abstract.html"
    },
    {
        "title": "Learning with Pseudo-Ensembles",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4657",
        "id": "4657",
        "author_site": "Philip Bachman, Ouais Alsharif, Doina Precup",
        "author": "Philip Bachman; Quais Alsharif; Doina Precup",
        "abstract": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.",
        "bibtex": "@inproceedings{NIPS2014_8844a0d3,\n author = {Bachman, Philip and Alsharif, Quais and Precup, Doina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Pseudo-Ensembles},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8844a0d324ab48fcd4577f1c716f460d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8844a0d324ab48fcd4577f1c716f460d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8844a0d324ab48fcd4577f1c716f460d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8844a0d324ab48fcd4577f1c716f460d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8844a0d324ab48fcd4577f1c716f460d-Reviews.html",
        "metareview": "",
        "pdf_size": 349328,
        "gs_citation": 770,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1526603709533816745&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "McGill University; McGill University; McGill University",
        "aff_domain": "gmail.com;gmail.com;cs.mcgill.ca",
        "email": "gmail.com;gmail.com;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8844a0d324ab48fcd4577f1c716f460d-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Local Decorrelation For Improved Pedestrian Detection",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4658",
        "id": "4658",
        "author_site": "Woonhyun Nam, Piotr Dollar, Joon Hee Han",
        "author": "Woonhyun Nam; Piotr Doll\u00e1r; Joon Hee Han",
        "abstract": "Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.",
        "bibtex": "@inproceedings{NIPS2014_cb14ff88,\n author = {Nam, Woonhyun and Doll\\'{a}r, Piotr and Han, Joon Hee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Local Decorrelation For Improved Pedestrian Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cb14ff88f6287b2c11a1ec3fcf931855-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cb14ff88f6287b2c11a1ec3fcf931855-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cb14ff88f6287b2c11a1ec3fcf931855-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cb14ff88f6287b2c11a1ec3fcf931855-Reviews.html",
        "metareview": "",
        "pdf_size": 1375621,
        "gs_citation": 474,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14007754465970510867&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "StradVision, Inc. + POSTECH, Republic of Korea; Microsoft Research; POSTECH, Republic of Korea",
        "aff_domain": "stradvision.com;microsoft.com;postech.ac.kr",
        "email": "stradvision.com;microsoft.com;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cb14ff88f6287b2c11a1ec3fcf931855-Abstract.html",
        "aff_unique_index": "0+1;2;1",
        "aff_unique_norm": "StradVision;POSTECH;Microsoft",
        "aff_unique_dep": "Inc.;;Microsoft Research",
        "aff_unique_url": "https://www.stradvision.com;https://www.postech.ac.kr;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "StradVision;POSTECH;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "title": "Local Linear Convergence of Forward--Backward under Partial Smoothness",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4659",
        "id": "4659",
        "author_site": "Jingwei Liang, Jalal Fadili, Gabriel Peyr\u00e9",
        "author": "Jingwei Liang; Jalal M. Fadili; Gabriel Peyr\u00e9",
        "abstract": "In this paper, we consider the Forward--Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relatively to an active manifold $\\mathcal{M}$. We propose a generic framework in which we show that the Forward--Backward (i) correctly identifies the active manifold $\\mathcal{M}$ in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.",
        "bibtex": "@inproceedings{NIPS2014_d0eda833,\n author = {Liang, Jingwei and Fadili, Jalal M. and Peyr\\'{e}, Gabriel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Local Linear Convergence of Forward--Backward under Partial Smoothness},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d0eda833f489251837f85ee95478cc64-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d0eda833f489251837f85ee95478cc64-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d0eda833f489251837f85ee95478cc64-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d0eda833f489251837f85ee95478cc64-Reviews.html",
        "metareview": "",
        "pdf_size": 375590,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14063451350469910647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "GREYC, CNRS-ENSICAEN-Univ. Caen; GREYC, CNRS-ENSICAEN-Univ. Caen; CEREMADE, CNRS-Univ. Paris-Dauphine",
        "aff_domain": "greyc.ensicaen.fr;greyc.ensicaen.fr;ceremade.dauphine.fr",
        "email": "greyc.ensicaen.fr;greyc.ensicaen.fr;ceremade.dauphine.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d0eda833f489251837f85ee95478cc64-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "GREYC;CNRS-Univ. Paris-Dauphine",
        "aff_unique_dep": "CNRS-ENSICAEN-Univ. Caen;CEREMADE",
        "aff_unique_url": ";https://www.dauphine.psl.eu",
        "aff_unique_abbr": ";Paris-Dauphine",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Localized Data Fusion for Kernel k-Means Clustering with Application to Cancer Biology",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4660",
        "id": "4660",
        "author_site": "Mehmet G\u00f6nen, Adam A Margolin",
        "author": "Mehmet G\u00f6nen; Adam A. Margolin",
        "abstract": "In many modern applications from, for example, bioinformatics and computer vision, samples have multiple feature representations coming from different data sources. Multiview learning algorithms try to exploit all these available information to obtain a better learner in such scenarios. In this paper, we propose a novel multiple kernel learning algorithm that extends kernel k-means clustering to the multiview setting, which combines kernels calculated on the views in a localized way to better capture sample-specific characteristics of the data. We demonstrate the better performance of our localized data fusion approach on a human colon and rectal cancer data set by clustering patients. Our method finds more relevant prognostic patient groups than global data fusion methods when we evaluate the results with respect to three commonly used clinical biomarkers.",
        "bibtex": "@inproceedings{NIPS2014_5a1ab76e,\n author = {G\\\"{o}nen, Mehmet and Margolin, Adam A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Localized Data Fusion for Kernel k-Means Clustering with Application to Cancer Biology},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5a1ab76e03b84ef43e2b6ec233394e3c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5a1ab76e03b84ef43e2b6ec233394e3c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5a1ab76e03b84ef43e2b6ec233394e3c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5a1ab76e03b84ef43e2b6ec233394e3c-Reviews.html",
        "metareview": "",
        "pdf_size": 1394850,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12047994725820413414&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Biomedical Engineering, Oregon Health & Science University, Portland, OR 97239, USA; Department of Biomedical Engineering, Oregon Health & Science University, Portland, OR 97239, USA",
        "aff_domain": "ohsu.edu;ohsu.edu",
        "email": "ohsu.edu;ohsu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5a1ab76e03b84ef43e2b6ec233394e3c-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon Health & Science University",
        "aff_unique_dep": "Department of Biomedical Engineering",
        "aff_unique_url": "https://www.ohsu.edu",
        "aff_unique_abbr": "OHSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Portland",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Log-Hilbert-Schmidt metric between positive definite operators on Hilbert spaces",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4408",
        "id": "4408",
        "author_site": "Minh Ha Quang, Marco San Biagio, Vittorio Murino",
        "author": "H\u00e0 Quang Minh; Marco San Biagio; Vittorio Murino",
        "abstract": "This paper introduces a novel mathematical and computational framework, namely {\\it Log-Hilbert-Schmidt metric} between positive definite operators on a Hilbert space. This is a generalization of the Log-Euclidean metric on the Riemannian manifold of positive definite matrices to the infinite-dimensional setting. The general framework is applied in particular to compute distances between covariance operators on a Reproducing Kernel Hilbert Space (RKHS), for which we obtain explicit formulas via the corresponding Gram matrices. Empirically, we apply our formulation to the task of multi-category image classification, where each image is represented by an infinite-dimensional RKHS covariance operator. On several challenging datasets, our method significantly outperforms approaches based on covariance matrices computed directly on the original input features, including those using the Log-Euclidean metric, Stein and Jeffreys divergences, achieving new state of the art results.",
        "bibtex": "@inproceedings{NIPS2014_3000e56b,\n author = {Minh, H\\`{a} Quang and San Biagio, Marco and Murino, Vittorio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Log-Hilbert-Schmidt metric between positive definite operators on Hilbert spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3000e56b48442cd23b49e5064bf1a9e6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3000e56b48442cd23b49e5064bf1a9e6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3000e56b48442cd23b49e5064bf1a9e6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3000e56b48442cd23b49e5064bf1a9e6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3000e56b48442cd23b49e5064bf1a9e6-Reviews.html",
        "metareview": "",
        "pdf_size": 329091,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9891598411754423256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3000e56b48442cd23b49e5064bf1a9e6-Abstract.html"
    },
    {
        "title": "Low Rank Approximation Lower Bounds in Row-Update Streams",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4661",
        "id": "4661",
        "author_site": "David Woodruff",
        "author": "David P. Woodruff",
        "abstract": "We study low-rank approximation in the streaming model in which the rows of an $n \\times d$ matrix $A$ are presented one at a time in an arbitrary order. At the end of the stream, the streaming algorithm should output a $k \\times d$ matrix $R$ so that $\\|A-AR^{\\dagger}R\\|_F^2 \\leq (1+\\eps)\\|A-A_k\\|_F^2$, where $A_k$ is the best rank-$k$ approximation to $A$. A deterministic streaming algorithm of Liberty (KDD, 2013), with an improved analysis of Ghashami and Phillips (SODA, 2014), provides such a streaming algorithm using $O(dk/\\epsilon)$ words of space. A natural question is if smaller space is possible. We give an almost matching lower bound of $\\Omega(dk/\\epsilon)$ bits of space, even for randomized algorithms which succeed only with constant probability. Our lower bound matches the upper bound of Ghashami and Phillips up to the word size, improving on a simple $\\Omega(dk)$ space lower bound.",
        "bibtex": "@inproceedings{NIPS2014_1730b5e3,\n author = {Woodruff, David P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Low Rank Approximation Lower Bounds in Row-Update Streams},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1730b5e375aa93bc0ad1f923182a6642-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1730b5e375aa93bc0ad1f923182a6642-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/1730b5e375aa93bc0ad1f923182a6642-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1730b5e375aa93bc0ad1f923182a6642-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1730b5e375aa93bc0ad1f923182a6642-Reviews.html",
        "metareview": "",
        "pdf_size": 379163,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16063658801654828239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IBM Research Almaden",
        "aff_domain": "us.ibm.com",
        "email": "us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1730b5e375aa93bc0ad1f923182a6642-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Almaden",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Low-Rank Time-Frequency Synthesis",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4662",
        "id": "4662",
        "author_site": "C\u00e9dric F\u00e9votte, Matthieu Kowalski",
        "author": "C\u00e9dric F\u00e9votte; Matthieu Kowalski",
        "abstract": "Many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform. In particular, nonnegative matrix factorization (NMF) of the spectrogram -- the (power) magnitude of the short-time Fourier transform (STFT) -- has been considered in many audio applications. In this setting, NMF with the Itakura-Saito divergence was shown to underly a generative Gaussian composite model (GCM) of the STFT, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications. Still, the GCM is not yet a generative model of the raw signal itself, but only of its STFT. The work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure. In particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional NMF setting. We describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.",
        "bibtex": "@inproceedings{NIPS2014_3073554c,\n author = {F\\'{e}votte, C\\'{e}dric and Kowalski, Matthieu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Low-Rank Time-Frequency Synthesis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3073554c5b5472df57e59d9d565ebe13-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3073554c5b5472df57e59d9d565ebe13-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3073554c5b5472df57e59d9d565ebe13-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3073554c5b5472df57e59d9d565ebe13-Reviews.html",
        "metareview": "",
        "pdf_size": 1057989,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10308628076200984640&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Laboratoire Lagrange (CNRS, OCA & Universit \u00b4e de Nice); Laboratoire des Signaux et Syst `emes (CNRS, Sup \u00b4elec & Universit \u00b4e Paris-Sud)",
        "aff_domain": "unice.fr;lss.supelec.fr",
        "email": "unice.fr;lss.supelec.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3073554c5b5472df57e59d9d565ebe13-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Laboratoire Lagrange;Laboratoire des Signaux et Syst\u00e8mes",
        "aff_unique_dep": "CNRS, OCA & Universit\u00e9 de Nice;CNRS, Sup\u00e9lec & Universit\u00e9 Paris-Sud",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Low-dimensional models of neural population activity in sensory cortical circuits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4663",
        "id": "4663",
        "author_site": "Evan Archer, Urs Koster, Jonathan W Pillow, Jakob H Macke",
        "author": "Evan Archer; Urs K\u00f6ster; Jonathan Pillow; Jakob H. Macke",
        "abstract": "Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures the temporal dynamics, effective network connectivity in large population recordings, and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a relatively large number of idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional latent dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations. Inference scales linearly in both population size and recording duration. We apply this model to multi-channel recordings from primary visual cortex and show that it accounts for a large number of individual neural receptive fields using a small number of nonlinear inputs and a low-dimensional dynamical model.",
        "bibtex": "@inproceedings{NIPS2014_5b6d37b6,\n author = {Archer, Evan and K\\\"{o}ster, Urs and Pillow, Jonathan and Macke, Jakob H.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Low-dimensional models of neural population activity in sensory cortical circuits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5b6d37b60d8d916c581ea6cd369eaa7b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5b6d37b60d8d916c581ea6cd369eaa7b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5b6d37b60d8d916c581ea6cd369eaa7b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5b6d37b60d8d916c581ea6cd369eaa7b-Reviews.html",
        "metareview": "",
        "pdf_size": 3910453,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17600417006005636012&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Max Planck Institute for Biological Cybernetics, T \u00a8ubingen + Bernstein Center for Computational Neuroscience, T \u00a8ubingen; Redwood Center for Theoretical Neuroscience, University of California at Berkeley; Princeton Neuroscience Institute, Department of Psychology, Princeton University; Max Planck Institute for Biological Cybernetics, T \u00a8ubingen + Bernstein Center for Computational Neuroscience, T \u00a8ubingen",
        "aff_domain": "tuebingen.mpg.de;nervanasys.com;princeton.edu;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;nervanasys.com;princeton.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5b6d37b60d8d916c581ea6cd369eaa7b-Abstract.html",
        "aff_unique_index": "0+1;2;3;0+1",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Bernstein Center for Computational Neuroscience;University of California, Berkeley;Princeton University",
        "aff_unique_dep": ";;Redwood Center for Theoretical Neuroscience;Department of Psychology",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;;https://www.berkeley.edu;https://www.princeton.edu",
        "aff_unique_abbr": "MPIBC;;UC Berkeley;Princeton",
        "aff_campus_unique_index": "0+0;1;0+0",
        "aff_campus_unique": "T\u00fcbingen;Berkeley;",
        "aff_country_unique_index": "0+0;1;1;0+0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "Magnitude-sensitive preference formation`",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4665",
        "id": "4665",
        "author_site": "Nisheeth Srivastava, Ed Vul, Paul R Schrater",
        "author": "Nisheeth Srivastava; Edward Vul; Paul R Schrater",
        "abstract": "Our understanding of the neural computations that underlie the ability of animals to choose among options has advanced through a synthesis of computational modeling, brain imaging and behavioral choice experiments. Yet, there remains a gulf between theories of preference learning and accounts of the real, economic choices that humans face in daily life, choices that are usually between some amount of money and an item. In this paper, we develop a theory of magnitude-sensitive preference learning that permits an agent to rationally infer its preferences for items compared with money options of different magnitudes. We show how this theory yields classical and anomalous supply-demand curves and predicts choices for a large panel of risky lotteries. Accurate replications of such phenomena without recourse to utility functions suggest that the theory proposed is both psychologically realistic and econometrically viable.",
        "bibtex": "@inproceedings{NIPS2014_06ead039,\n author = {Srivastava, Nisheeth and Vul, Edward and Schrater, Paul R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Magnitude-sensitive preference formation\\textasciigrave },\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/06ead039a193550d1d1d8c4b7f8124ee-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/06ead039a193550d1d1d8c4b7f8124ee-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/06ead039a193550d1d1d8c4b7f8124ee-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/06ead039a193550d1d1d8c4b7f8124ee-Reviews.html",
        "metareview": "",
        "pdf_size": 382640,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3940578315348811008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Psychology, University of San Diego; Department of Psychology, University of San Diego; Dept of Psychology, University of Minnesota",
        "aff_domain": "gmail.com;gmail.com;umn.edu",
        "email": "gmail.com;gmail.com;umn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/06ead039a193550d1d1d8c4b7f8124ee-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of San Diego;University of Minnesota",
        "aff_unique_dep": "Department of Psychology;Dept of Psychology",
        "aff_unique_url": "https://www.sandiego.edu;https://www.minnesota.edu",
        "aff_unique_abbr": "USD;UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Making Pairwise Binary Graphical Models Attractive",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4410",
        "id": "4410",
        "author_site": "Nicholas Ruozzi, Tony Jebara",
        "author": "Nicholas Ruozzi; Tony Jebara",
        "abstract": "Computing the partition function (i.e., the normalizing constant) of a given pairwise binary graphical model is NP-hard in general. As a result, the partition function is typically estimated by approximate inference algorithms such as belief propagation (BP) and tree-reweighted belief propagation (TRBP). The former provides reasonable estimates in practice but has convergence issues. The later has better convergence properties but typically provides poorer estimates. In this work, we propose a novel scheme that has better convergence properties than BP and provably provides better partition function estimates in many instances than TRBP. In particular, given an arbitrary pairwise binary graphical model, we construct a specific ``attractive'' 2-cover. We explore the properties of this special cover and show that it can be used to construct an algorithm with the desired properties.",
        "bibtex": "@inproceedings{NIPS2014_6197257a,\n author = {Ruozzi, Nicholas and Jebara, Tony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Making Pairwise Binary Graphical Models Attractive},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6197257a9577fc60c31e4095fa84e141-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/6197257a9577fc60c31e4095fa84e141-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/6197257a9577fc60c31e4095fa84e141-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/6197257a9577fc60c31e4095fa84e141-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/6197257a9577fc60c31e4095fa84e141-Reviews.html",
        "metareview": "",
        "pdf_size": 297178,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11188588936403862142&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Institute for Data Sciences and Engineering, Columbia University; Department of Computer Science, Columbia University",
        "aff_domain": "columbia.edu;cs.columbia.edu",
        "email": "columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/6197257a9577fc60c31e4095fa84e141-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Institute for Data Sciences and Engineering",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Median Selection Subset Aggregation for Parallel Inference",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4338",
        "id": "4338",
        "author_site": "Xiangyu Wang, Peichao Peng, David B Dunson",
        "author": "Xiangyu Wang; Peichao Peng; David B. Dunson",
        "abstract": "For massive data sets, efficient computation commonly relies on distributed algorithms that store and process subsets of the data on different machines, minimizing communication costs. Our focus is on regression and classification problems involving many features. A variety of distributed algorithms have been proposed in this context, but challenges arise in defining an algorithm with low communication, theoretical guarantees and excellent practical performance in general settings. We propose a MEdian Selection Subset AGgregation Estimator (message) algorithm, which attempts to solve these problems. The algorithm applies feature selection in parallel for each subset using Lasso or another method, calculates the `median' feature inclusion index, estimates coefficients for the selected features in parallel for each subset, and then averages these estimates. The algorithm is simple, involves very minimal communication, scales efficiently in both sample and feature size, and has theoretical guarantees. In particular, we show model selection consistency and coefficient estimation efficiency. Extensive experiments show excellent performance in variable selection, estimation, prediction, and computation time relative to usual competitors.",
        "bibtex": "@inproceedings{NIPS2014_9b81f7d6,\n author = {Wang, Xiangyu and Peng, Peichao and Dunson, David B.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Median Selection Subset Aggregation for Parallel Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9b81f7d69fcc36807ac5fda52d23f0e7-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9b81f7d69fcc36807ac5fda52d23f0e7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/9b81f7d69fcc36807ac5fda52d23f0e7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9b81f7d69fcc36807ac5fda52d23f0e7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9b81f7d69fcc36807ac5fda52d23f0e7-Reviews.html",
        "metareview": "",
        "pdf_size": 163264,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15796505274265947813&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Statistical Science, Duke University; Statistics Department, University of Pennsylvania; Dept. of Statistical Science, Duke University",
        "aff_domain": "stat.duke.edu;yahoo.com;stat.duke.edu",
        "email": "stat.duke.edu;yahoo.com;stat.duke.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9b81f7d69fcc36807ac5fda52d23f0e7-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Duke University;University of Pennsylvania",
        "aff_unique_dep": "Dept. of Statistical Science;Statistics Department",
        "aff_unique_url": "https://www.duke.edu;https://www.upenn.edu",
        "aff_unique_abbr": "Duke;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Message Passing Inference for Large Scale Graphical Models with High Order Potentials",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4666",
        "id": "4666",
        "author_site": "Jian Zhang, Alex Schwing, Raquel Urtasun",
        "author": "Jian Zhang; Alexander G. Schwing; Raquel Urtasun",
        "abstract": "To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods.",
        "bibtex": "@inproceedings{NIPS2014_fb4ca0ed,\n author = {Zhang, Jian and Schwing, Alexander G. and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Message Passing Inference for Large Scale Graphical Models with High Order Potentials},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/fb4ca0edb76d40b81d194ff016a17570-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/fb4ca0edb76d40b81d194ff016a17570-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/fb4ca0edb76d40b81d194ff016a17570-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/fb4ca0edb76d40b81d194ff016a17570-Reviews.html",
        "metareview": "",
        "pdf_size": 876050,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11845010390200914255&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "ETH Zurich; University of Toronto; University of Toronto",
        "aff_domain": "ethz.ch;cs.toronto.edu;cs.toronto.edu",
        "email": "ethz.ch;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/fb4ca0edb76d40b81d194ff016a17570-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "ETH Zurich;University of Toronto",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.utoronto.ca",
        "aff_unique_abbr": "ETHZ;U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Switzerland;Canada"
    },
    {
        "title": "Metric Learning for Temporal Sequence Alignment",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4667",
        "id": "4667",
        "author_site": "R\u00e9mi Lajugie, Damien Garreau, Francis Bach, Sylvain Arlot",
        "author": "Damien Garreau; R\u00e9mi Lajugie; Sylvain Arlot; Francis Bach",
        "abstract": "In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio-to-audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better alignment performance.",
        "bibtex": "@inproceedings{NIPS2014_75fa245a,\n author = {Garreau, Damien and Lajugie, R\\'{e}mi and Arlot, Sylvain and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Metric Learning for Temporal Sequence Alignment},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/75fa245a86d8a358044c66edfc710788-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/75fa245a86d8a358044c66edfc710788-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/75fa245a86d8a358044c66edfc710788-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/75fa245a86d8a358044c66edfc710788-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/75fa245a86d8a358044c66edfc710788-Reviews.html",
        "metareview": "",
        "pdf_size": 832611,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16424786820991030187&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "ENS+SIERRA project-team, D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4erieure (CNRS, INRIA, ENS); INRIA+SIERRA project-team, D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4erieure (CNRS, INRIA, ENS); CNRS+SIERRA project-team, D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4erieure (CNRS, INRIA, ENS); INRIA+SIERRA project-team, D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4erieure (CNRS, INRIA, ENS)",
        "aff_domain": "ens.fr;inria.fr;ens.fr;inria.fr",
        "email": "ens.fr;inria.fr;ens.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/75fa245a86d8a358044c66edfc710788-Abstract.html",
        "aff_unique_index": "0+1;2+1;3+1;2+1",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure;Ecole Normale Sup\u00e9rieure;INRIA;Centre National de la Recherche Scientifique",
        "aff_unique_dep": ";D\u00b4epartement d\u2019Informatique;;",
        "aff_unique_url": "https://www.ens.fr;https://www.ens.fr;https://www.inria.fr;https://www.cnrs.fr",
        "aff_unique_abbr": "ENS;ENS;INRIA;CNRS",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "title": "Mind the Nuisance: Gaussian Process Classification using Privileged Noise",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4668",
        "id": "4668",
        "author_site": "Daniel Hern\u00e1ndez-lobato, Viktoriia Sharmanska, Kristian Kersting, Christoph Lampert, Novi Quadrianto",
        "author": "Daniel Hern\u00e1ndez-Lobato; Viktoriia Sharmanska; Kristian Kersting; Christoph H. Lampert; Novi Quadrianto",
        "abstract": "The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.",
        "bibtex": "@inproceedings{NIPS2014_a60f5f0f,\n author = {Hern\\'{a}ndez-Lobato, Daniel and Sharmanska, Viktoriia and Kersting, Kristian and Lampert, Christoph H. and Quadrianto, Novi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mind the Nuisance: Gaussian Process Classification using Privileged Noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a60f5f0f9348059cabc17bcb0e7686db-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a60f5f0f9348059cabc17bcb0e7686db-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a60f5f0f9348059cabc17bcb0e7686db-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a60f5f0f9348059cabc17bcb0e7686db-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a60f5f0f9348059cabc17bcb0e7686db-Reviews.html",
        "metareview": "",
        "pdf_size": 792642,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11357702217985867741&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": "Universidad Aut\u00f3noma de Madrid; IST Austria; TU Dortmund; IST Austria; SMiLe CLiNiC, University of Sussex",
        "aff_domain": "uam.es;ist.ac.at;cs.tu-dortmund.de;ist.ac.at;sussex.ac.uk",
        "email": "uam.es;ist.ac.at;cs.tu-dortmund.de;ist.ac.at;sussex.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a60f5f0f9348059cabc17bcb0e7686db-Abstract.html",
        "aff_unique_index": "0;1;2;1;3",
        "aff_unique_norm": "Universidad Aut\u00f3noma de Madrid;Institute of Science and Technology Austria;Technische Universit\u00e4t Dortmund;University of Sussex",
        "aff_unique_dep": ";;;SMiLe CLiNiC",
        "aff_unique_url": "https://www.uam.es;https://www.ist.ac.at;https://www.tu-dortmund.de;https://www.sussex.ac.uk",
        "aff_unique_abbr": "UAM;IST Austria;TU Dortmund;Sussex",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;1;3",
        "aff_country_unique": "Spain;Austria;Germany;United Kingdom"
    },
    {
        "title": "Minimax-optimal Inference from Partial Rankings",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4669",
        "id": "4669",
        "author_site": "Bruce Hajek, Sewoong Oh, Jiaming Xu",
        "author": "Bruce Hajek; Sewoong Oh; Jiaming Xu",
        "abstract": "This paper studies the problem of rank aggregation under the Plackett-Luce model. The goal is to infer a global ranking and related scores of the items, based on partial rankings provided by multiple users over multiple subsets of items. A question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error. Without any assumptions on how the items are assigned to users, we derive an oracle lower bound and the Cram\\'er-Rao lower bound of the estimation error. We prove an upper bound on the estimation error achieved by the maximum likelihood estimator, and show that both the upper bound and the Cram\\'er-Rao lower bound inversely depend on the spectral gap of the Laplacian of an appropriately defined comparison graph. Since random comparison graphs are known to have large spectral gaps, this suggests the use of random assignments when we have the control. Precisely, the matching oracle lower bound and the upper bound on the estimation error imply that the maximum likelihood estimator together with a random assignment is minimax-optimal up to a logarithmic factor. We further analyze a popular rank-breaking scheme that decompose partial rankings into pairwise comparisons. We show that even if one applies the mismatched maximum likelihood estimator that assumes independence (on pairwise comparisons that are now dependent due to rank-breaking), minimax optimal performance is still achieved up to a logarithmic factor.",
        "bibtex": "@inproceedings{NIPS2014_daadbd06,\n author = {Hajek, Bruce and Oh, Sewoong and Xu, Jiaming},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimax-optimal Inference from Partial Rankings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/daadbd06d5082478b7677bea9812b575-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/daadbd06d5082478b7677bea9812b575-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/daadbd06d5082478b7677bea9812b575-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/daadbd06d5082478b7677bea9812b575-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/daadbd06d5082478b7677bea9812b575-Reviews.html",
        "metareview": "",
        "pdf_size": 297433,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13462842169321477344&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "UIUC; UIUC; UIUC",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/daadbd06d5082478b7677bea9812b575-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Mode Estimation for High Dimensional Discrete Tree Graphical Models",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4412",
        "id": "4412",
        "author_site": "Chao Chen, Han Liu, Dimitris Metaxas, Tianqi Zhao",
        "author": "Chao Chen; Han Liu; Dimitris N. Metaxas; Tianqi Zhao",
        "abstract": "This paper studies the following problem: given samples from a high dimensional discrete distribution, we want to estimate the leading $(\\delta,\\rho)$-modes of the underlying distributions. A point is defined to be a $(\\delta,\\rho)$-mode if it is a local optimum of the density within a $\\delta$-neighborhood under metric $\\rho$. As we increase the ``scale'' parameter $\\delta$, the neighborhood size increases and the total number of modes monotonically decreases. The sequence of the $(\\delta,\\rho)$-modes reveal intrinsic topographical information of the underlying distributions. Though the mode finding problem is generally intractable in high dimensions, this paper unveils that, if the distribution can be approximated well by a tree graphical model, mode characterization is significantly easier. An efficient algorithm with provable theoretical guarantees is proposed and is applied to applications like data analysis and multiple predictions.",
        "bibtex": "@inproceedings{NIPS2014_e94c843c,\n author = {Chen, Chao and Liu, Han and Metaxas, Dimitris N. and Zhao, Tianqi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mode Estimation for High Dimensional Discrete Tree Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e94c843c4dbd8476cf521ffdd28dfdbd-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e94c843c4dbd8476cf521ffdd28dfdbd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e94c843c4dbd8476cf521ffdd28dfdbd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e94c843c4dbd8476cf521ffdd28dfdbd-Reviews.html",
        "metareview": "",
        "pdf_size": 1055296,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8964931553314830369&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science, Rutgers, The State University of New Jersey; Department of Operations Research and Financial Engineering, Princeton University; Department of Computer Science, Rutgers, The State University of New Jersey; Department of Operations Research and Financial Engineering, Princeton University",
        "aff_domain": "gmail.com;princeton.edu;cs.rutgers.edu;princeton.edu",
        "email": "gmail.com;princeton.edu;cs.rutgers.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e94c843c4dbd8476cf521ffdd28dfdbd-Abstract.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Rutgers University;Princeton University",
        "aff_unique_dep": "Department of Computer Science;Department of Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.rutgers.edu;https://www.princeton.edu",
        "aff_unique_abbr": "Rutgers;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Model-based Reinforcement Learning and the Eluder Dimension",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4670",
        "id": "4670",
        "author_site": "Ian Osband, Benjamin Van Roy",
        "author": "Ian Osband; Benjamin Van Roy",
        "abstract": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.",
        "bibtex": "@inproceedings{NIPS2014_b0a77afa,\n author = {Osband, Ian and Van Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Model-based Reinforcement Learning and the Eluder Dimension},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b0a77afa642922102120b01b1ae6f200-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b0a77afa642922102120b01b1ae6f200-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b0a77afa642922102120b01b1ae6f200-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b0a77afa642922102120b01b1ae6f200-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b0a77afa642922102120b01b1ae6f200-Reviews.html",
        "metareview": "",
        "pdf_size": 704508,
        "gs_citation": 218,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17032668023532561108&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b0a77afa642922102120b01b1ae6f200-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Modeling Deep Temporal Dependencies with Recurrent Grammar Cells\"\"",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4671",
        "id": "4671",
        "author_site": "Vincent Michalski, Roland Memisevic, Kishore Konda",
        "author": "Vincent Michalski; Roland Memisevic; Kishore Konda",
        "abstract": "We propose modeling time series by representing the transformations that take a frame at time t to a frame at time t+1. To this end we show how a bi-linear model of transformations, such as a gated autoencoder, can be turned into a recurrent network, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multiple layers of gating units in a recurrent pyramid makes it possible to represent the \u201dsyntax\u201d of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks.",
        "bibtex": "@inproceedings{NIPS2014_aca342ec,\n author = {Michalski, Vincent and Memisevic, Roland and Konda, Kishore},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modeling Deep Temporal Dependencies with Recurrent Grammar Cells\"\"},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/aca342ec0893b43b016f29ab8d2c6eec-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/aca342ec0893b43b016f29ab8d2c6eec-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/aca342ec0893b43b016f29ab8d2c6eec-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/aca342ec0893b43b016f29ab8d2c6eec-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/aca342ec0893b43b016f29ab8d2c6eec-Reviews.html",
        "metareview": "",
        "pdf_size": 787522,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18381360983521690015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Goethe University Frankfurt, Germany; University of Montreal, Canada; Goethe University Frankfurt, Germany",
        "aff_domain": "rz.uni-frankfurt.de;umontreal.ca;gmail.com",
        "email": "rz.uni-frankfurt.de;umontreal.ca;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/aca342ec0893b43b016f29ab8d2c6eec-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Goethe University Frankfurt;University of Montreal",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-frankfurt.de;https://wwwumontreal.ca",
        "aff_unique_abbr": "GU Frankfurt;UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "title": "Mondrian Forests: Efficient Online Random Forests",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4672",
        "id": "4672",
        "author_site": "Balaji Lakshminarayanan, Daniel Roy, Yee Whye Teh",
        "author": "Balaji Lakshminarayanan; Daniel M. Roy; Yee Whye Teh",
        "abstract": "Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.",
        "bibtex": "@inproceedings{NIPS2014_195c9c07,\n author = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mondrian Forests: Efficient Online Random Forests},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/195c9c0797f42473f2c2f922c4cf52cf-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/195c9c0797f42473f2c2f922c4cf52cf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/195c9c0797f42473f2c2f922c4cf52cf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/195c9c0797f42473f2c2f922c4cf52cf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/195c9c0797f42473f2c2f922c4cf52cf-Reviews.html",
        "metareview": "",
        "pdf_size": 1796446,
        "gs_citation": 308,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12126524186803391652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/195c9c0797f42473f2c2f922c4cf52cf-Abstract.html"
    },
    {
        "title": "Multi-Class Deep Boosting",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4673",
        "id": "4673",
        "author_site": "Vitaly Kuznetsov, Mehryar Mohri, Umar Syed",
        "author": "Vitaly Kuznetsov; Mehryar Mohri; Umar Syed",
        "abstract": "We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble\u2019s mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.",
        "bibtex": "@inproceedings{NIPS2014_b6b0ddc6,\n author = {Kuznetsov, Vitaly and Mohri, Mehryar and Syed, Umar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Class Deep Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b6b0ddc63f8cb9dda203b39c92173268-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b6b0ddc63f8cb9dda203b39c92173268-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b6b0ddc63f8cb9dda203b39c92173268-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b6b0ddc63f8cb9dda203b39c92173268-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b6b0ddc63f8cb9dda203b39c92173268-Reviews.html",
        "metareview": "",
        "pdf_size": 278931,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1439229012111472757&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Courant Institute; Courant Institute + Google Research; Google Research",
        "aff_domain": "cims.nyu.edu;cims.nyu.edu;google.com",
        "email": "cims.nyu.edu;cims.nyu.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b6b0ddc63f8cb9dda203b39c92173268-Abstract.html",
        "aff_unique_index": "0;0+1;1",
        "aff_unique_norm": "Courant Institute of Mathematical Sciences;Google",
        "aff_unique_dep": "Mathematical Sciences;Google Research",
        "aff_unique_url": "https://courant.nyu.edu;https://research.google",
        "aff_unique_abbr": "Courant;Google Research",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multi-Resolution Cascades for Multiclass Object Detection",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4674",
        "id": "4674",
        "author_site": "Ehsan Saberian, Nuno Vasconcelos",
        "author": "Mohammad Saberian; Nuno Vasconcelos",
        "abstract": "An algorithm for learning fast multiclass object detection cascades is introduced. It produces multi-resolution (MRes) cascades, whose early stages are binary target vs. non-target detectors that eliminate false positives, late stages multiclass classifiers that finely discriminate target classes, and middle stages have intermediate numbers of classes, determined in a data-driven manner. This MRes structure is achieved with a new structurally biased boosting algorithm (SBBoost). SBBost extends previous multiclass boosting approaches, whose boosting mechanisms are shown to implement two complementary data-driven biases: 1) the standard bias towards examples difficult to classify, and 2) a bias towards difficult classes. It is shown that structural biases can be implemented by generalizing this class-based bias, so as to encourage the desired MRes structure. This is accomplished through a generalized definition of multiclass margin, which includes a set of bias parameters. SBBoost is a boosting algorithm for maximization of this margin. It can also be interpreted as standard multiclass boosting algorithm augmented with margin thresholds or a cost-sensitive boosting algorithm with costs defined by the bias parameters. A stage adaptive bias policy is then introduced to determine bias parameters in a data driven manner. This is shown to produce MRes cascades that have high detection rate and are computationally efficient. Experiments on multiclass object detection show improved performance over previous solutions.",
        "bibtex": "@inproceedings{NIPS2014_832c4910,\n author = {Saberian, Mohammad and Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Resolution Cascades for Multiclass Object Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/832c491001f90dde4468351fcf3b7961-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/832c491001f90dde4468351fcf3b7961-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/832c491001f90dde4468351fcf3b7961-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/832c491001f90dde4468351fcf3b7961-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/832c491001f90dde4468351fcf3b7961-Reviews.html",
        "metareview": "",
        "pdf_size": 465131,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9914753807979432745&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Yahoo! Labs; Statistical Visual Computing Laboratory, University of California, San Diego",
        "aff_domain": "yahoo-inc.com;ucsd.edu",
        "email": "yahoo-inc.com;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/832c491001f90dde4468351fcf3b7961-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yahoo!;University of California, San Diego",
        "aff_unique_dep": "Yahoo! Labs;Statistical Visual Computing Laboratory",
        "aff_unique_url": "https://yahoo.com;https://ucsd.edu",
        "aff_unique_abbr": "Yahoo!;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multi-Scale Spectral Decomposition of Massive Graphs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4675",
        "id": "4675",
        "author_site": "Si Si, Donghyuk Shin, Inderjit Dhillon, Beresford N Parlett",
        "author": "Si Si; Donghyuk Shin; Inderjit S. Dhillon; Beresford N. Parlett",
        "abstract": "Computing the $k$ dominant eigenvalues and eigenvectors of massive graphs is a key operation in numerous machine learning applications; however, popular solvers suffer from slow convergence, especially when $k$ is reasonably large. In this paper, we propose and analyze a novel multi-scale spectral decomposition method (MSEIGS), which first clusters the graph into smaller clusters whose spectral decomposition can be computed efficiently and independently. We show theoretically as well as empirically that the union of all cluster's subspaces has significant overlap with the dominant subspace of the original graph, provided that the graph is clustered appropriately. Thus, eigenvectors of the clusters serve as good initializations to a block Lanczos algorithm that is used to compute spectral decomposition of the original graph. We further use hierarchical clustering to speed up the computation and adopt a fast early termination strategy to compute quality approximations. Our method outperforms widely used solvers in terms of convergence speed and approximation quality. Furthermore, our method is naturally parallelizable and exhibits significant speedups in shared-memory parallel settings. For example, on a graph with more than 82 million nodes and 3.6 billion edges, MSEIGS takes less than 3 hours on a single-core machine while Randomized SVD takes more than 6 hours, to obtain a similar approximation of the top-50 eigenvectors. Using 16 cores, we can reduce this time to less than 40 minutes.",
        "bibtex": "@inproceedings{NIPS2014_cef83281,\n author = {Si, Si and Shin, Donghyuk and Dhillon, Inderjit S. and Parlett, Beresford N.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Scale Spectral Decomposition of Massive Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cef83281b3d5515b4176fd36d2e298e9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cef83281b3d5515b4176fd36d2e298e9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/cef83281b3d5515b4176fd36d2e298e9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cef83281b3d5515b4176fd36d2e298e9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cef83281b3d5515b4176fd36d2e298e9-Reviews.html",
        "metareview": "",
        "pdf_size": 507726,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10695962869014997831&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Mathematics, University of California, Berkeley",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;math.berkeley.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;math.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cef83281b3d5515b4176fd36d2e298e9-Abstract.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Texas at Austin;University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science;Department of Mathematics",
        "aff_unique_url": "https://www.utexas.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "UT Austin;UC Berkeley",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Austin;Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4676",
        "id": "4676",
        "author_site": "Hanie Sedghi, Anima Anandkumar, Edmond A Jonckheere",
        "author": "Hanie Sedghi; Anima Anandkumar; Edmond Jonckheere",
        "abstract": "In this paper, we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem, our method achieves the minimax rate of $O(s\\log d/T)$ for $s$-sparse problems in $d$ dimensions in $T$ steps, and is thus, unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function, we analyze the multi-step ADMM with multiple blocks. We establish $O(1/T)$ rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise), our convergence rate is minimax-optimal. Thus, we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2014_0197ff74,\n author = {Sedghi, Hanie and Anandkumar, Anima and Jonckheere, Edmond},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0197ff74daa1c383cf9f4e190020f5c4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0197ff74daa1c383cf9f4e190020f5c4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0197ff74daa1c383cf9f4e190020f5c4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0197ff74daa1c383cf9f4e190020f5c4-Reviews.html",
        "metareview": "",
        "pdf_size": 346612,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=662198593230867695&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Univ. of Southern California; University of California; Univ. of Southern California",
        "aff_domain": "usc.edu;uci.edu;usc.edu",
        "email": "usc.edu;uci.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0197ff74daa1c383cf9f4e190020f5c4-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Southern California;University of California",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://www.universityofcalifornia.edu",
        "aff_unique_abbr": "USC;UC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multi-View Perceptron: a Deep Model for Learning Face Identity and View Representations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4551",
        "id": "4551",
        "author_site": "Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang",
        "author": "Zhenyao Zhu; Ping Luo; Xiaogang Wang; Xiaoou Tang",
        "abstract": "Various factors, such as identities, views (poses), and illuminations, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of human brain. Intriguingly, even without accessing 3D data, human not only can recognize face identity, but can also imagine face images of a person under different viewpoints given a single 2D image, making face perception in the brain robust to view changes. In this sense, human brain has learned and encoded 3D face models from 2D images. To take into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and infer a full spectrum of multi-view images in the meanwhile, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.",
        "bibtex": "@inproceedings{NIPS2014_39945d57,\n author = {Zhu, Zhenyao and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-View Perceptron: a Deep Model for Learning Face Identity and View Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/39945d578f616735572174bf5e8f155d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/39945d578f616735572174bf5e8f155d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/39945d578f616735572174bf5e8f155d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/39945d578f616735572174bf5e8f155d-Reviews.html",
        "metareview": "",
        "pdf_size": 1774388,
        "gs_citation": 315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1145971344161527425&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of CVPR, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of CVPR, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Electronic Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of CVPR, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of CVPR, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/39945d578f616735572174bf5e8f155d-Abstract.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese University of Hong Kong;Chinese Academy of Sciences",
        "aff_unique_dep": "Department of Information Engineering;Shenzhen Key Lab of CVPR",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.siat.ac.cn",
        "aff_unique_abbr": "CUHK;CAS",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-scale Graphical Models for Spatio-Temporal Processes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4677",
        "id": "4677",
        "author_site": "Firdaus Janoos, Huseyin Denli, Niranjan Subrahmanya",
        "author": "Firdaus Janoos; Huseyin Denli; Niranjan Subrahmanya",
        "abstract": "Learning the dependency structure between spatially distributed observations of a spatio-temporal process is an important problem in many fields such as geology, geophysics, atmospheric sciences, oceanography, etc. . However, estimation of such systems is complicated by the fact that they exhibit dynamics at multiple scales of space and time arising due to a combination of diffusion and convection/advection. As we show, time-series graphical models based on vector auto-regressive processes are inef\ufb01cient in capturing such multi-scale structure. In this paper, we present a hierarchical graphical model with physically derived priors that better represents the multi-scale character of these dynamical systems. We also propose algorithms to ef\ufb01ciently estimate the interaction structure from data. We demonstrate results on a general class of problems arising in exploration geophysics by discovering graphical structure that is physically meaningful and provide evidence of its advantages over alternative approaches.",
        "bibtex": "@inproceedings{NIPS2014_952e8272,\n author = {Janoos, Firdaus and Denli, Huseyin and Subrahmanya, Niranjan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-scale Graphical Models for Spatio-Temporal Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/952e8272dccb182713fb770733af294f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/952e8272dccb182713fb770733af294f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/952e8272dccb182713fb770733af294f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/952e8272dccb182713fb770733af294f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/952e8272dccb182713fb770733af294f-Reviews.html",
        "metareview": "",
        "pdf_size": 1417897,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4781387184216874170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "ExxonMobil Corporate Strategic Research; ExxonMobil Corporate Strategic Research; ExxonMobil Corporate Strategic Research",
        "aff_domain": "ieee.org; ; ",
        "email": "ieee.org; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/952e8272dccb182713fb770733af294f-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ExxonMobil",
        "aff_unique_dep": "Corporate Strategic Research",
        "aff_unique_url": "https://www.exxonmobil.com",
        "aff_unique_abbr": "ExxonMobil",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multilabel Structured Output Learning with Random Spanning Trees of Max-Margin Markov Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4678",
        "id": "4678",
        "author_site": "Mario Marchand, Hongyu Su, Emilie Morvant, Juho Rousu, John Shawe-Taylor",
        "author": "Mario Marchand; Hongyu Su; Emilie Morvant; Juho Rousu; John Shawe-Taylor",
        "abstract": "We show that the usual score function for conditional Markov networks can be written as the expectation over the scores of their spanning trees. We also show that a small random sample of these output trees can attain a significant fraction of the margin obtained by the complete graph and we provide conditions under which we can perform tractable inference. The experimental results confirm that practical learning is scalable to realistic datasets using this approach.",
        "bibtex": "@inproceedings{NIPS2014_130799de,\n author = {Marchand, Mario and Su, Hongyu and Morvant, Emilie and Rousu, Juho and Shawe-Taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multilabel Structured Output Learning with Random Spanning Trees of Max-Margin Markov Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/130799de861d011345ca384d5116652d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/130799de861d011345ca384d5116652d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/130799de861d011345ca384d5116652d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/130799de861d011345ca384d5116652d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/130799de861d011345ca384d5116652d-Reviews.html",
        "metareview": "",
        "pdf_size": 396271,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10945789864575621418&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "D\u00b4epartement d\u2019informatique et g \u00b4enie logiciel, Universit \u00b4e Laval, Qu\u00b4ebec (QC), Canada; Helsinki Institute for Information Technology, Dept of Information and Computer Science, Aalto University, Finland; LaHC, UMR CNRS 5516, Univ. of St-Etienne, France + IST Austria, Klosterneurburg; Helsinki Institute for Information Technology, Dept of Information and Computer Science, Aalto University, Finland; Department of Computer Science, University College London, London, UK",
        "aff_domain": "ift.ulaval.ca;aalto.fi;univ-st-etienne.fr;aalto.fi;ucl.ac.uk",
        "email": "ift.ulaval.ca;aalto.fi;univ-st-etienne.fr;aalto.fi;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/130799de861d011345ca384d5116652d-Abstract.html",
        "aff_unique_index": "0;1;2+3;1;4",
        "aff_unique_norm": "Universit\u00e9 Laval;Aalto University;University of St-Etienne;Institute of Science and Technology Austria;University College London",
        "aff_unique_dep": "D\u00e9partement d\u2019informatique et g\u00e9nie logiciel;Dept of Information and Computer Science;LaHC, UMR CNRS 5516;;Department of Computer Science",
        "aff_unique_url": "https://www.ulaval.ca;https://www.aalto.fi;https://www.univ-st-etienne.fr;https://www.ist.ac.at;https://www.ucl.ac.uk",
        "aff_unique_abbr": "UL;Aalto;Univ. of St-Etienne;IST Austria;UCL",
        "aff_campus_unique_index": "0;1;3;1;4",
        "aff_campus_unique": "Qu\u00e9bec;Helsinki;;Klosterneurburg;London",
        "aff_country_unique_index": "0;1;2+3;1;4",
        "aff_country_unique": "Canada;Finland;France;Austria;United Kingdom"
    },
    {
        "title": "Multiscale Fields of Patterns",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4679",
        "id": "4679",
        "author_site": "Pedro Felzenszwalb, John G Oberlin",
        "author": "Pedro F. Felzenszwalb; John G. Oberlin",
        "abstract": "We describe a framework for defining high-order image models that can be used in a variety of applications. The approach involves modeling local patterns in a multiscale representation of an image. Local properties of a coarsened image reflect non-local properties of the original image. In the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel. With the multiscale representation we capture the frequency of patterns observed at different scales of resolution. This framework leads to expressive priors that depend on a relatively small number of parameters. For inference and learning we use an MCMC method for block sampling with very large blocks. We evaluate the approach with two example applications. One involves contour detection. The other involves binary segmentation.",
        "bibtex": "@inproceedings{NIPS2014_b3cd73d3,\n author = {Felzenszwalb, Pedro F. and Oberlin, John G.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiscale Fields of Patterns},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b3cd73d353d39e5cf6f6e9ff8d14c87f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b3cd73d353d39e5cf6f6e9ff8d14c87f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b3cd73d353d39e5cf6f6e9ff8d14c87f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b3cd73d353d39e5cf6f6e9ff8d14c87f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b3cd73d353d39e5cf6f6e9ff8d14c87f-Reviews.html",
        "metareview": "",
        "pdf_size": 1375556,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1387819667416022753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Brown University; Brown University",
        "aff_domain": "brown.edu;brown.edu",
        "email": "brown.edu;brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b3cd73d353d39e5cf6f6e9ff8d14c87f-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multitask learning meets tensor factorization: task imputation via convex optimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4680",
        "id": "4680",
        "author_site": "Kishan Wimalawarne, Masashi Sugiyama, Ryota Tomioka",
        "author": "Kishan Wimalawarne; Masashi Sugiyama; Ryota Tomioka",
        "abstract": "We study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices, which can be e.g, (consumer, time). The weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks. Two types of convex relaxations have recently been proposed for the tensor multilinear rank. However, we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous. We propose a new norm, which we call the scaled latent trace norm and analyze the excess risk of all the three norms. The results apply to various settings including matrix and tensor completion, multitask learning, and multilinear multitask learning. Both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.",
        "bibtex": "@inproceedings{NIPS2014_5009efc1,\n author = {Wimalawarne, Kishan and Sugiyama, Masashi and Tomioka, Ryota},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multitask learning meets tensor factorization: task imputation via convex optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5009efc1ebfe58d5a624a154d3d20729-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5009efc1ebfe58d5a624a154d3d20729-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5009efc1ebfe58d5a624a154d3d20729-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5009efc1ebfe58d5a624a154d3d20729-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5009efc1ebfe58d5a624a154d3d20729-Reviews.html",
        "metareview": "",
        "pdf_size": 304922,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9678233522342247669&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Tokyo Institute of Technology; The University of Tokyo; TTI-C",
        "aff_domain": "sg.cs.titech.ac.jp;k.u-tokyo.ac.jp;ttic.edu",
        "email": "sg.cs.titech.ac.jp;k.u-tokyo.ac.jp;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5009efc1ebfe58d5a624a154d3d20729-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Tokyo Institute of Technology;University of Tokyo;Toyota Technological Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.u-tokyo.ac.jp;https://www.tti-c.org",
        "aff_unique_abbr": "Titech;UTokyo;TTI-C",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Multivariate Regression with Calibration",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4681",
        "id": "4681",
        "author_site": "Han Liu, Lie Wang, Tuo Zhao",
        "author": "Han Liu; Lie Wang; Tuo Zhao",
        "abstract": "We propose a new method named calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. Compared to existing methods, CMR calibrates the regularization for each regression task with respect to its noise level so that it is simultaneously tuning insensitive and achieves an improved finite-sample performance. Computationally, we develop an efficient smoothed proximal gradient algorithm which has a worst-case iteration complexity $O(1/\\epsilon)$, where $\\epsilon$ is a pre-specified numerical accuracy. Theoretically, we prove that CMR achieves the optimal rate of convergence in parameter estimation. We illustrate the usefulness of CMR by thorough numerical simulations and show that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR on a brain activity prediction problem and find that CMR is as competitive as the handcrafted model created by human experts.",
        "bibtex": "@inproceedings{NIPS2014_281c09b4,\n author = {Liu, Han and Wang, Lie and Zhao, Tuo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multivariate Regression with Calibration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/281c09b4594c6228d49f663799897178-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/281c09b4594c6228d49f663799897178-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/281c09b4594c6228d49f663799897178-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/281c09b4594c6228d49f663799897178-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/281c09b4594c6228d49f663799897178-Reviews.html",
        "metareview": "",
        "pdf_size": 1299420,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18344201432136668845&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Operations Research and Financial Engineering, Princeton University; Department of Mathematics, Massachusetts Institute of Technology; Department of Computer Science, Johns Hopkins University + Department of Operations Research and Financial Engineering, Princeton University",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/281c09b4594c6228d49f663799897178-Abstract.html",
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "Princeton University;Massachusetts Institute of Technology;Johns Hopkins University",
        "aff_unique_dep": "Department of Operations Research and Financial Engineering;Department of Mathematics;Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu;https://web.mit.edu;https://www.jhu.edu",
        "aff_unique_abbr": "Princeton;MIT;JHU",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multivariate f-divergence Estimation With Confidence",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4682",
        "id": "4682",
        "author_site": "Kevin Moon, Alfred Hero",
        "author": "Kevin R. Moon; Alfred O. Hero III",
        "abstract": "The problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics. While several divergence estimators exist, relatively few have known convergence properties. In particular, even for those estimators whose MSE convergence rates are known, the asymptotic distributions are unknown. We establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples. This estimator has MSE convergence rate of O(1/T), is simple to implement, and performs well in high dimensions. This theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples. We experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error.",
        "bibtex": "@inproceedings{NIPS2014_77bb8bfa,\n author = {Moon, Kevin R. and Hero, Alfred O.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multivariate f-divergence Estimation With Confidence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77bb8bfae1378a4239265b4ee879a319-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/77bb8bfae1378a4239265b4ee879a319-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/77bb8bfae1378a4239265b4ee879a319-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/77bb8bfae1378a4239265b4ee879a319-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/77bb8bfae1378a4239265b4ee879a319-Reviews.html",
        "metareview": "",
        "pdf_size": 196294,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8891012000651975944&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of EECS, University of Michigan, Ann Arbor, MI; Department of EECS, University of Michigan, Ann Arbor, MI",
        "aff_domain": "umich.edu;eecs.umich.edu",
        "email": "umich.edu;eecs.umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/77bb8bfae1378a4239265b4ee879a319-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4683",
        "id": "4683",
        "author_site": "Siu On Chan, Ilias Diakonikolas, Rocco A Servedio, Xiaorui Sun",
        "author": "Siu-On Chan; Ilias Diakonikolas; Rocco A. Servedio; Xiaorui Sun",
        "abstract": "Let $p$ be an unknown and arbitrary probability distribution over $[0 ,1)$. We consider the problem of \\emph{density estimation}, in which a learning algorithm is given i.i.d. draws from $p$ and must (with high probability) output a hypothesis distribution that is close to $p$. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function. In more detail, for any $k$ and $\\eps$, we give an algorithm that makes $\\tilde{O}(k/\\eps^2)$ draws from $p$, runs in $\\tilde{O}(k/\\eps^2)$ time, and outputs a hypothesis distribution $h$ that is piecewise constant with $O(k \\log^2(1/\\eps))$ pieces. With high probability the hypothesis $h$ satisfies $\\dtv(p,h) \\leq C \\cdot \\opt_k(p) + \\eps$, where $\\dtv$ denotes the total variation distance (statistical distance), $C$ is a universal constant, and $\\opt_k(p)$ is the smallest total variation distance between $p$ and any $k$-piecewise constant distribution. The sample size and running time of our algorithm are both optimal up to logarithmic factors. The ``approximation factor'' $C$ that is present in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of $k$ and $\\eps$ can achieve $C < 2$ regardless of what kind of hypothesis distribution it uses.",
        "bibtex": "@inproceedings{NIPS2014_d1a6ff85,\n author = {Chan, Siu-On and Diakonikolas, Ilias and Servedio, Rocco A. and Sun, Xiaorui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d1a6ff856af0080cdbfbd041b1ff4586-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d1a6ff856af0080cdbfbd041b1ff4586-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d1a6ff856af0080cdbfbd041b1ff4586-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d1a6ff856af0080cdbfbd041b1ff4586-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d1a6ff856af0080cdbfbd041b1ff4586-Reviews.html",
        "metareview": "",
        "pdf_size": 474236,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7539703673791464775&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff": "Microsoft Research; University of Edinburgh; Columbia University; Columbia University",
        "aff_domain": "gmail.com;ed.ac.uk;cs.columbia.edu;cs.columbia.edu",
        "email": "gmail.com;ed.ac.uk;cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d1a6ff856af0080cdbfbd041b1ff4586-Abstract.html",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Microsoft;University of Edinburgh;Columbia University",
        "aff_unique_dep": "Microsoft Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.ed.ac.uk;https://www.columbia.edu",
        "aff_unique_abbr": "MSR;Edinburgh;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4684",
        "id": "4684",
        "author_site": "Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, Ashkan Jafarpour",
        "author": "Jayadev Acharya; Ashkan Jafarpour; Alon Orlitsky; Ananda Theertha Suresh",
        "abstract": "Many important distributions are high dimensional, and often they can be modeled as Gaussian mixtures. We derive the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. Based on intuitive spectral reasoning, it approximates mixtures of $k$ spherical Gaussians in $d$-dimensions to within$\\ell_1$ distance $\\epsilon$ using $\\mathcal{O}({dk^9(\\log^2 d)}/{\\epsilon^4})$ samples and $\\mathcal{O}_{k,\\epsilon}(d^3\\log^5 d)$ computation time. Conversely, we show that any estimator requires $\\Omega\\bigl({dk}/{\\epsilon^2}\\bigr)$ samples, hence the algorithm's sample complexity is nearly optimal in the dimension. The implied time-complexity factor \\mathcal{O}_{k,\\epsilon}$ is exponential in $k$, but much smaller than previously known. We also construct a simple estimator for one-dimensional Gaussian mixtures that uses $\\tilde\\mathcal{O}(k /\\epsilon^2)$ samples and $\\tilde\\mathcal{O}((k/\\epsilon)^{3k+1})$ computation time.",
        "bibtex": "@inproceedings{NIPS2014_9d4f24b6,\n author = {Acharya, Jayadev and Jafarpour, Ashkan and Orlitsky, Alon and Suresh, Ananda Theertha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9d4f24b6cc1070a2e533a90309863878-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9d4f24b6cc1070a2e533a90309863878-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/9d4f24b6cc1070a2e533a90309863878-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9d4f24b6cc1070a2e533a90309863878-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9d4f24b6cc1070a2e533a90309863878-Reviews.html",
        "metareview": "",
        "pdf_size": 338125,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11749685856374633509&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "MIT; UC San Diego; UC San Diego; UC San Diego",
        "aff_domain": "mit.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "mit.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9d4f24b6cc1070a2e533a90309863878-Abstract.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "MIT;UCSD",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Near-optimal Reinforcement Learning in Factored MDPs",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4414",
        "id": "4414",
        "author_site": "Ian Osband, Benjamin Van Roy",
        "author": "Ian Osband; Benjamin Van Roy",
        "abstract": "Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer $\\Omega(\\sqrt{SAT})$ regret on some MDP, where $T$ is the elapsed time and $S$ and $A$ are the cardinalities of the state and action spaces. This implies $T = \\Omega(SA)$ time to guarantee a near-optimal policy. In many settings of practical interest, due to the curse of dimensionality, $S$ and $A$ can be so enormous that this learning time is unacceptable. We establish that, if the system is known to be a \\emph{factored} MDP, it is possible to achieve regret that scales polynomially in the number of \\emph{parameters} encoding the factored MDP, which may be exponentially smaller than $S$ or $A$. We provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning (PSRL) and an upper confidence bound algorithm (UCRL-Factored).",
        "bibtex": "@inproceedings{NIPS2014_0f0b653e,\n author = {Osband, Ian and Van Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-optimal Reinforcement Learning in Factored MDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0f0b653ef2261da4d9655441deb6cc55-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0f0b653ef2261da4d9655441deb6cc55-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0f0b653ef2261da4d9655441deb6cc55-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0f0b653ef2261da4d9655441deb6cc55-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0f0b653ef2261da4d9655441deb6cc55-Reviews.html",
        "metareview": "",
        "pdf_size": 676103,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=446942970327906280&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0f0b653ef2261da4d9655441deb6cc55-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Near-optimal sample compression for nearest neighbors",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4685",
        "id": "4685",
        "author_site": "Lee-Ad Gottlieb, Aryeh Kontorovich, Pinhas Nisnevitch",
        "author": "Lee-Ad Gottlieb; Aryeh Kontorovich; Pinhas Nisnevitch",
        "abstract": "We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.",
        "bibtex": "@inproceedings{NIPS2014_3f9d6c56,\n author = {Gottlieb, Lee-Ad and Kontorovich, Aryeh and Nisnevitch, Pinhas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-optimal sample compression for nearest neighbors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3f9d6c56ef5745a62c020d666ab47bdb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3f9d6c56ef5745a62c020d666ab47bdb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3f9d6c56ef5745a62c020d666ab47bdb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3f9d6c56ef5745a62c020d666ab47bdb-Reviews.html",
        "metareview": "",
        "pdf_size": 185348,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5953634239874709685&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science and Mathematics, Ariel University; Computer Science Department, Ben Gurion University; Department of Computer Science and Mathematics, Ariel University",
        "aff_domain": "ariel.ac.il;cs.bgu.ac.il;gmail.com",
        "email": "ariel.ac.il;cs.bgu.ac.il;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3f9d6c56ef5745a62c020d666ab47bdb-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Ariel University;Ben Gurion University",
        "aff_unique_dep": "Department of Computer Science and Mathematics;Computer Science Department",
        "aff_unique_url": "https://www.ariel.ac.il;https://www.bgu.ac.il",
        "aff_unique_abbr": "Ariel U;BGU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Neural Word Embedding as Implicit Matrix Factorization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4686",
        "id": "4686",
        "author_site": "Omer Levy, Yoav Goldberg",
        "author": "Omer Levy; Yoav Goldberg",
        "abstract": "We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.",
        "bibtex": "@inproceedings{NIPS2014_b7866697,\n author = {Levy, Omer and Goldberg, Yoav},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural Word Embedding as Implicit Matrix Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b78666971ceae55a8e87efb7cbfd9ad4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b78666971ceae55a8e87efb7cbfd9ad4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b78666971ceae55a8e87efb7cbfd9ad4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b78666971ceae55a8e87efb7cbfd9ad4-Reviews.html",
        "metareview": "",
        "pdf_size": 288874,
        "gs_citation": 2712,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13881398718713473790&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science, Bar-Ilan University; Department of Computer Science, Bar-Ilan University",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b78666971ceae55a8e87efb7cbfd9ad4-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Bar-Ilan University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.biu.ac.il",
        "aff_unique_abbr": "BIU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Neurons as Monte Carlo Samplers: Bayesian \ufffcInference and Learning in Spiking Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4687",
        "id": "4687",
        "author_site": "Yanping Huang, Rajesh PN Rao",
        "author": "Yanping Huang; Rajesh P.N. Rao",
        "abstract": "We propose a two-layer spiking network capable of performing approximate inference and learning for a hidden Markov model. The lower layer sensory neurons detect noisy measurements of hidden world states. The higher layer neurons with recurrent connections infer a posterior distribution over world states from spike trains generated by sensory neurons. We show how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in the population of inference neurons represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution of hidden state. The model provides a functional explanation for the Poisson-like noise commonly observed in cortical responses. Uncertainties in spike times provide the necessary variability for sampling during inference. Unlike previous models, the hidden world state is not observed by the sensory neurons, and the temporal dynamics of the hidden state is unknown. We demonstrate how this network can sequentially learn the hidden Markov model using a spike-timing dependent Hebbian learning rule and achieve power-law convergence rates.",
        "bibtex": "@inproceedings{NIPS2014_5a60723b,\n author = {Huang, Yanping and Rao, Rajesh P.N.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neurons as Monte Carlo Samplers: Bayesian \ufffcInference and Learning in Spiking Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5a60723b3b37840db1da6a1d8c52c080-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5a60723b3b37840db1da6a1d8c52c080-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5a60723b3b37840db1da6a1d8c52c080-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5a60723b3b37840db1da6a1d8c52c080-Reviews.html",
        "metareview": "",
        "pdf_size": 1330781,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4237620103647017323&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Washington; University of Washington",
        "aff_domain": "cs.uw.edu;cs.uw.edu",
        "email": "cs.uw.edu;cs.uw.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5a60723b3b37840db1da6a1d8c52c080-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "New Rules for Domain Independent Lifted MAP Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4688",
        "id": "4688",
        "author_site": "Happy Mittal, Prasoon Goyal, Vibhav Gogate, Parag Singla",
        "author": "Happy Mittal; Prasoon Goyal; Vibhav Gogate; Parag Singla",
        "abstract": "Lifted inference algorithms for probabilistic first-order logic frameworks such as Markov logic networks (MLNs) have received significant attention in recent years. These algorithms use so called lifting rules to identify symmetries in the first-order representation and reduce the inference problem over a large probabilistic model to an inference problem over a much smaller model. In this paper, we present two new lifting rules, which enable fast MAP inference in a large class of MLNs. Our first rule uses the concept of single occurrence equivalence class of logical variables, which we define in the paper. The rule states that the MAP assignment over an MLN can be recovered from a much smaller MLN, in which each logical variable in each single occurrence equivalence class is replaced by a constant (i.e., an object in the domain of the variable). Our second rule states that we can safely remove a subset of formulas from the MLN if all equivalence classes of variables in the remaining MLN are single occurrence and all formulas in the subset are tautology (i.e., evaluate to true) at extremes (i.e., assignments with identical truth value for groundings of a predicate). We prove that our two new rules are sound and demonstrate via a detailed experimental evaluation that our approach is superior in terms of scalability and MAP solution quality to the state of the art approaches.",
        "bibtex": "@inproceedings{NIPS2014_482db07a,\n author = {Mittal, Happy and Goyal, Prasoon and Gogate, Vibhav and Singla, Parag},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {New Rules for Domain Independent Lifted MAP Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/482db07a506c28ee0f8de0b47b450b25-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/482db07a506c28ee0f8de0b47b450b25-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/482db07a506c28ee0f8de0b47b450b25-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/482db07a506c28ee0f8de0b47b450b25-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/482db07a506c28ee0f8de0b47b450b25-Reviews.html",
        "metareview": "",
        "pdf_size": 348062,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9938097674115327819&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Comp. Sci. & Engg., I.I.T. Delhi, Hauz Khas, New Delhi, 110016, India; Dept. of Comp. Sci. & Engg., I.I.T. Delhi, Hauz Khas, New Delhi, 110016, India; Dept. of Comp. Sci., Univ. of Texas Dallas, Richardson, TX 75080, USA; Dept. of Comp. Sci. & Engg., I.I.T. Delhi, Hauz Khas, New Delhi, 110016, India",
        "aff_domain": "cse.iitd.ac.in;gmail.com;hlt.utdallas.edu;cse.iitd.ac.in",
        "email": "cse.iitd.ac.in;gmail.com;hlt.utdallas.edu;cse.iitd.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/482db07a506c28ee0f8de0b47b450b25-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Delhi;University of Texas at Dallas",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.iitd.ac.in;https://www.utdallas.edu",
        "aff_unique_abbr": "IIT Delhi;UT Dallas",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Delhi;Richardson",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "Non-convex Robust PCA",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4416",
        "id": "4416",
        "author_site": "Praneeth Netrapalli, Niranjan Uma Naresh, Sujay Sanghavi, Animashree Anandkumar, Prateek Jain",
        "author": "Praneeth Netrapalli; U N Niranjan; Sujay Sanghavi; Animashree Anandkumar; Prateek Jain",
        "abstract": "We propose a new provable method for robust PCA, where the task is to recover a low-rank matrix, which is corrupted with sparse perturbations. Our method consists of simple alternating projections onto the set of low rank and sparse matrices with intermediate de-noising steps. We prove correct recovery of the low rank and sparse components under tight recovery conditions, which match those for the state-of-art convex relaxation techniques. Our method is extremely simple to implement and has low computational complexity. For a $m \\times n$ input matrix (say m \\geq n), our method has O(r^2 mn\\log(1/\\epsilon)) running time, where $r$ is the rank of the low-rank component and $\\epsilon$ is the accuracy. In contrast, the convex relaxation methods have a running time O(mn^2/\\epsilon), which is not scalable to large problem instances. Our running time nearly matches that of the usual PCA (i.e. non robust), which is O(rmn\\log (1/\\epsilon)). Thus, we achieve ``best of both the worlds'', viz low computational complexity and provable recovery for robust PCA. Our analysis represents one of the few instances of global convergence guarantees for non-convex methods.",
        "bibtex": "@inproceedings{NIPS2014_3e3d6580,\n author = {Netrapalli, Praneeth and Niranjan, U N and Sanghavi, Sujay and Anandkumar, Animashree and Jain, Prateek},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-convex Robust PCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3e3d6580fc108fb932e5008a94e024a3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3e3d6580fc108fb932e5008a94e024a3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3e3d6580fc108fb932e5008a94e024a3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3e3d6580fc108fb932e5008a94e024a3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3e3d6580fc108fb932e5008a94e024a3-Reviews.html",
        "metareview": "",
        "pdf_size": 492922,
        "gs_citation": 388,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1000060626587110539&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Microsoft Research, Cambridge MA; The University of California at Irvine; The University of Texas at Austin; The University of California at Irvine; Microsoft Research, India",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3e3d6580fc108fb932e5008a94e024a3-Abstract.html",
        "aff_unique_index": "0;1;2;1;0",
        "aff_unique_norm": "Microsoft;University of California, Irvine;University of Texas at Austin",
        "aff_unique_dep": "Microsoft Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.uci.edu;https://www.utexas.edu",
        "aff_unique_abbr": "MSR;UCI;UT Austin",
        "aff_campus_unique_index": "0;1;2;1",
        "aff_campus_unique": "Cambridge;Irvine;Austin;",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "Nonparametric Bayesian inference on multivariate exponential families",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4689",
        "id": "4689",
        "author_site": "William R Vega-Brown, Marek Doniec, Nicholas Roy",
        "author": "William Vega-Brown; Marek Doniec; Nicholas Roy",
        "abstract": "We develop a model by choosing the maximum entropy distribution from the set of models satisfying certain smoothness and independence criteria; we show that inference on this model generalizes local kernel estimation to the context of Bayesian inference on stochastic processes. Our model enables Bayesian inference in contexts when standard techniques like Gaussian process inference are too expensive to apply. Exact inference on our model is possible for any likelihood function from the exponential family. Inference is then highly efficient, requiring only O(log N) time and O(N) space at run time. We demonstrate our algorithm on several problems and show quantifiable improvement in both speed and performance relative to models based on the Gaussian process.",
        "bibtex": "@inproceedings{NIPS2014_a99cb6ad,\n author = {Vega-Brown, William and Doniec, Marek and Roy, Nicholas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Bayesian inference on multivariate exponential families},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a99cb6ad47a9762c9e0c5507508105a9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a99cb6ad47a9762c9e0c5507508105a9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a99cb6ad47a9762c9e0c5507508105a9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a99cb6ad47a9762c9e0c5507508105a9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a99cb6ad47a9762c9e0c5507508105a9-Reviews.html",
        "metareview": "",
        "pdf_size": 497381,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10641400961469446103&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a99cb6ad47a9762c9e0c5507508105a9-Abstract.html"
    },
    {
        "title": "Object Localization based on Structural SVM using Privileged Information",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4691",
        "id": "4691",
        "author_site": "Jan Feyereisl, Suha Kwak, Jeany Son, Bohyung Han",
        "author": "Jan Feyereisl; Suha Kwak; Jeany Son; Bohyung Han",
        "abstract": "We propose a structured prediction algorithm for object localization based on Support Vector Machines (SVMs) using privileged information. Privileged information provides useful high-level knowledge for image understanding and facilitates learning a reliable model even with a small number of training examples. In our setting, we assume that such information is available only at training time since it may be difficult to obtain from visual data accurately without human supervision. Our goal is to improve performance by incorporating privileged information into ordinary learning framework and adjusting model parameters for better generalization. We tackle object localization problem based on a novel structural SVM using privileged information, where an alternating loss-augmented inference procedure is employed to handle the term in the objective function corresponding to privileged information. We apply the proposed algorithm to the Caltech-UCSD Birds 200-2011 dataset, and obtain encouraging results suggesting further investigation into the benefit of privileged information in structured prediction.",
        "bibtex": "@inproceedings{NIPS2014_9add9bd7,\n author = {Feyereisl, Jan and Kwak, Suha and Son, Jeany and Han, Bohyung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Object Localization based on Structural SVM using Privileged Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9add9bd7500e96b5b2d2593c3ba47c64-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9add9bd7500e96b5b2d2593c3ba47c64-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9add9bd7500e96b5b2d2593c3ba47c64-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9add9bd7500e96b5b2d2593c3ba47c64-Reviews.html",
        "metareview": "",
        "pdf_size": 619668,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7746613523396020203&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea; Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea + INRIA\u2013WILLOW Project, Paris, France; Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea; Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea",
        "aff_domain": "gmail.com;inria.fr;postech.ac.kr;postech.ac.kr",
        "email": "gmail.com;inria.fr;postech.ac.kr;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9add9bd7500e96b5b2d2593c3ba47c64-Abstract.html",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "POSTECH;INRIA",
        "aff_unique_dep": "Dept. of Computer Science and Engineering;INRIA\u2013WILLOW Project",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.inria.fr",
        "aff_unique_abbr": "POSTECH;INRIA",
        "aff_campus_unique_index": "0;0+1;0;0",
        "aff_campus_unique": "Pohang;Paris",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "South Korea;France"
    },
    {
        "title": "On Communication Cost of Distributed Statistical Estimation and Dimensionality",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4340",
        "id": "4340",
        "author_site": "Ankit Garg, Tengyu Ma, Huy Nguyen",
        "author": "Ankit Garg; Tengyu Ma; Huy L. Nguy\u1ec5n",
        "abstract": "We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean $\\vectheta$ of an unknown $d$ dimensional gaussian distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among $m$ different machines. The goal is to estimate the mean $\\vectheta$ at the optimal minimax rate while communicating as few bits as possible. We show that in this setting, the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually. Applying this result to previous lower bounds for one dimension in the interactive setting \\cite{ZDJW13} and to our improved bounds for the simultaneous setting, we prove new lower bounds of $\\Omega(md/\\log(m))$ and $\\Omega(md)$ for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively. To complement, we also demonstrate an interactive protocol achieving the minimax squared loss with $O(md)$ bits of communication, which improves upon the simple simultaneous protocol by a logarithmic factor. Given the strong lower bounds in the general setting, we initiate the study of the distributed parameter estimation problems with structured parameters. Specifically, when the parameter is promised to be $s$-sparse, we show a simple thresholding based protocol that achieves the same squared loss while saving a $d/s$ factor of communication. We conjecture that the tradeoff between communication and squared loss demonstrated by this protocol is essentially optimal up to logarithmic factor.",
        "bibtex": "@inproceedings{NIPS2014_29883d52,\n author = {Garg, Ankit and Ma, Tengyu and Nguy\u1ec5n, Huy L.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Communication Cost of Distributed Statistical Estimation and Dimensionality},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/29883d52f2590df7dfb27c69493c91d8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/29883d52f2590df7dfb27c69493c91d8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/29883d52f2590df7dfb27c69493c91d8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/29883d52f2590df7dfb27c69493c91d8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/29883d52f2590df7dfb27c69493c91d8-Reviews.html",
        "metareview": "",
        "pdf_size": 434104,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16540352117284239709&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; Simons Institute, UC Berkeley",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/29883d52f2590df7dfb27c69493c91d8-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Princeton University;University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science;Simons Institute",
        "aff_unique_url": "https://www.princeton.edu;https://simons.berkeley.edu",
        "aff_unique_abbr": "Princeton;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Integrated Clustering and Outlier Detection",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4692",
        "id": "4692",
        "author_site": "Lionel Ott, Linsey Pang, Fabio Ramos, Sanjay Chawla",
        "author": "Lionel Ott; Linsey Pang; Fabio Ramos; Sanjay Chawla",
        "abstract": "We model the joint clustering and outlier detection problem using an extension of the facility location formulation. The advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable. We provide a practical subgradient-based algorithm for the problem and also study the theoretical properties of algorithm in terms of approximation and convergence. Extensive evaluation on synthetic and real data sets attest to both the quality and scalability of our proposed method.",
        "bibtex": "@inproceedings{NIPS2014_ebdb2219,\n author = {Ott, Lionel and Pang, Linsey and Ramos, Fabio and Chawla, Sanjay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Integrated Clustering and Outlier Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ebdb2219c0f03021a1f01fdcb1979f39-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ebdb2219c0f03021a1f01fdcb1979f39-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ebdb2219c0f03021a1f01fdcb1979f39-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ebdb2219c0f03021a1f01fdcb1979f39-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ebdb2219c0f03021a1f01fdcb1979f39-Reviews.html",
        "metareview": "",
        "pdf_size": 352900,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=730587401124167698&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "University of Sydney; University of Sydney; University of Sydney; University of Sydney",
        "aff_domain": "uni.sydney.edu.au;it.usyd.edu.au;sydney.edu.au;sydney.edu.au",
        "email": "uni.sydney.edu.au;it.usyd.edu.au;sydney.edu.au;sydney.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ebdb2219c0f03021a1f01fdcb1979f39-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Sydney",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sydney.edu.au",
        "aff_unique_abbr": "USYD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4693",
        "id": "4693",
        "author_site": "Prateek Jain, Ambuj Tewari, Purushottam Kar",
        "author": "Prateek Jain; Ambuj Tewari; Purushottam Kar",
        "abstract": "The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L_0 constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. Finally, we extend our analysis to the problem of low-rank matrix recovery.",
        "bibtex": "@inproceedings{NIPS2014_e460882d,\n author = {Jain, Prateek and Tewari, Ambuj and Kar, Purushottam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Iterative Hard Thresholding Methods for High-dimensional M-Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e460882d11d622bd04a5af4b66d1ffd3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e460882d11d622bd04a5af4b66d1ffd3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e460882d11d622bd04a5af4b66d1ffd3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e460882d11d622bd04a5af4b66d1ffd3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e460882d11d622bd04a5af4b66d1ffd3-Reviews.html",
        "metareview": "",
        "pdf_size": 318270,
        "gs_citation": 280,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6319562101543000843&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Microsoft Research, INDIA; University of Michigan, Ann Arbor, USA; Microsoft Research, INDIA",
        "aff_domain": "microsoft.com;umich.edu;microsoft.com",
        "email": "microsoft.com;umich.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e460882d11d622bd04a5af4b66d1ffd3-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Microsoft;University of Michigan",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.umich.edu",
        "aff_unique_abbr": "MSR;UM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ann Arbor",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "On Model Parallelization and Scheduling Strategies for Distributed Machine Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4726",
        "id": "4726",
        "author_site": "Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth Gibson, Eric Xing",
        "author": "Seunghak Lee; Jin Kyu Kim; Xun Zheng; Qirong Ho; Garth A. Gibson; Eric P. Xing",
        "abstract": "Distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness. A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms, where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates. We argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment, and theoretical analysis. In this paper, we develop a system for model-parallelism, STRADS, that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs. STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ML. We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling, matrix factorization, and Lasso.",
        "bibtex": "@inproceedings{NIPS2014_186b3d04,\n author = {Lee, Seunghak and Kim, Jin Kyu and Zheng, Xun and Ho, Qirong and Gibson, Garth A. and Xing, Eric P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Model Parallelization and Scheduling Strategies for Distributed Machine Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/186b3d044a8c9898679d98dbd0d9b860-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/186b3d044a8c9898679d98dbd0d9b860-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/186b3d044a8c9898679d98dbd0d9b860-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/186b3d044a8c9898679d98dbd0d9b860-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/186b3d044a8c9898679d98dbd0d9b860-Reviews.html",
        "metareview": "",
        "pdf_size": 387972,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12004784984098514038&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; Institute for Infocomm Research, A*STAR; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;gmail.com;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;gmail.com;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/186b3d044a8c9898679d98dbd0d9b860-Abstract.html",
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;Institute for Infocomm Research",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.cmu.edu;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "CMU;I2R",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "title": "On Multiplicative Multitask Feature Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4694",
        "id": "4694",
        "author_site": "Xin Wang, Jinbo Bi, Shipeng Yu, Jiangwen Sun",
        "author": "Xin Wang; Jinbo Bi; Shipeng Yu; Jiangwen Sun",
        "abstract": "We investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.",
        "bibtex": "@inproceedings{NIPS2014_1f4fead9,\n author = {Wang, Xin and Bi, Jinbo and Yu, Shipeng and Sun, Jiangwen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Multiplicative Multitask Feature Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1f4fead9959b046b360e97432a1fab09-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1f4fead9959b046b360e97432a1fab09-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1f4fead9959b046b360e97432a1fab09-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1f4fead9959b046b360e97432a1fab09-Reviews.html",
        "metareview": "",
        "pdf_size": 6453351,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12412551450066033472&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Dept. of Computer Science & Engineering, University of Connecticut; Dept. of Computer Science & Engineering, University of Connecticut; Health Services Innovation Center, Siemens Healthcare; Dept. of Computer Science & Engineering, University of Connecticut",
        "aff_domain": "engr.uconn.edu;engr.uconn.edu;siemens.com;engr.uconn.edu",
        "email": "engr.uconn.edu;engr.uconn.edu;siemens.com;engr.uconn.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1f4fead9959b046b360e97432a1fab09-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Connecticut;Siemens Healthcare",
        "aff_unique_dep": "Dept. of Computer Science & Engineering;Health Services Innovation Center",
        "aff_unique_url": "https://www.uconn.edu;https://www.siemens-healthineers.com",
        "aff_unique_abbr": "UConn;Siemens",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "On Prior Distributions and Approximate Inference for Structured Variables",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4695",
        "id": "4695",
        "author_site": "Sanmi Koyejo, Rajiv Khanna, Joydeep Ghosh, Russell Poldrack",
        "author": "Oluwasanmi Koyejo; Rajiv Khanna; Joydeep Ghosh; Russell A. Poldrack",
        "abstract": "We present a general framework for constructing prior distributions with structured variables. The prior is defined as the information projection of a base distribution onto distributions supported on the constraint set of interest. In cases where this projection is intractable, we propose a family of parameterized approximations indexed by subsets of the domain. We further analyze the special case of sparse structure. While the optimal prior is intractable in general, we show that approximate inference using convex subsets is tractable, and is equivalent to maximizing a submodular function subject to cardinality constraints. As a result, inference using greedy forward selection provably achieves within a factor of (1-1/e) of the optimal objective value. Our work is motivated by the predictive modeling of high-dimensional functional neuroimaging data. For this task, we employ the Gaussian base distribution induced by local partial correlations and consider the design of priors to capture the domain knowledge of sparse support. Experimental results on simulated data and high dimensional neuroimaging data show the effectiveness of our approach in terms of support recovery and predictive accuracy.",
        "bibtex": "@inproceedings{NIPS2014_3c9d8727,\n author = {Koyejo, Oluwasanmi and Khanna, Rajiv and Ghosh, Joydeep and Poldrack, Russell A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Prior Distributions and Approximate Inference for Structured Variables},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3c9d8727489b6ee1627c86d9bc78d573-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3c9d8727489b6ee1627c86d9bc78d573-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3c9d8727489b6ee1627c86d9bc78d573-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3c9d8727489b6ee1627c86d9bc78d573-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3c9d8727489b6ee1627c86d9bc78d573-Reviews.html",
        "metareview": "",
        "pdf_size": 4900884,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6195418076826081299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Psychology Dept., Stanford; ECE Dept., UT Austin; ECE Dept., UT Austin; Psychology Dept., Stanford",
        "aff_domain": "stanford.edu;utexas.edu;ece.utexas.edu;stanford.edu",
        "email": "stanford.edu;utexas.edu;ece.utexas.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3c9d8727489b6ee1627c86d9bc78d573-Abstract.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Stanford University;University of Texas at Austin",
        "aff_unique_dep": "Department of Psychology;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.utexas.edu",
        "aff_unique_abbr": "Stanford;UT Austin",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Stanford;Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Sparse Gaussian Chain Graph Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4696",
        "id": "4696",
        "author_site": "Calvin McCarter, Seyoung Kim",
        "author": "Calvin McCarter; Seyoung Kim",
        "abstract": "In this paper, we address the problem of learning the structure of Gaussian chain graph models in a high-dimensional space. Chain graph models are generalizations of undirected and directed graphical models that contain a mixed set of directed and undirected edges. While the problem of sparse structure learning has been studied extensively for Gaussian graphical models and more recently for conditional Gaussian graphical models (CGGMs), there has been little previous work on the structure recovery of Gaussian chain graph models. We consider linear regression models and a re-parameterization of the linear regression models using CGGMs as building blocks of chain graph models. We argue that when the goal is to recover model structures, there are many advantages of using CGGMs as chain component models over linear regression models, including convexity of the optimization problem, computational efficiency, recovery of structured sparsity, and ability to leverage the model structure for semi-supervised learning. We demonstrate our approach on simulated and genomic datasets.",
        "bibtex": "@inproceedings{NIPS2014_21c1339d,\n author = {McCarter, Calvin and Kim, Seyoung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Sparse Gaussian Chain Graph Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/21c1339deedba0772fc80581df2eb989-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/21c1339deedba0772fc80581df2eb989-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/21c1339deedba0772fc80581df2eb989-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/21c1339deedba0772fc80581df2eb989-Reviews.html",
        "metareview": "",
        "pdf_size": 344776,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4736638347900369650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Machine Learning Department, Carnegie Mellon University; Lane Center for Computational Biology, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cs.cmu.edu",
        "email": "cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/21c1339deedba0772fc80581df2eb989-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On a Theory of Nonparametric Pairwise Similarity for Clustering: Connecting Clustering to Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4690",
        "id": "4690",
        "author_site": "Yingzhen Yang, Feng Liang, Shuicheng Yan, Zhangyang Wang, Thomas S Huang",
        "author": "Yingzhen Yang; Feng Liang; Shuicheng Yan; Zhangyang Wang; Thomas S. Huang",
        "abstract": "Pairwise clustering methods partition the data space into clusters by the pairwise similarity between data points. The success of pairwise clustering largely depends on the pairwise similarity function defined over the data points, where kernel similarity is broadly used. In this paper, we present a novel pairwise clustering framework by bridging the gap between clustering and multi-class classification. This pairwise clustering framework learns an unsupervised nonparametric classifier from each data partition, and search for the optimal partition of the data by minimizing the generalization error of the learned classifiers associated with the data partitions. We consider two nonparametric classifiers in this framework, i.e. the nearest neighbor classifier and the plug-in classifier. Modeling the underlying data distribution by nonparametric kernel density estimation, the generalization error bounds for both unsupervised nonparametric classifiers are the sum of nonparametric pairwise similarity terms between the data points for the purpose of clustering. Under uniform distribution, the nonparametric similarity terms induced by both unsupervised classifiers exhibit a well known form of kernel similarity. We also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary for Low Density Separation, a widely used criteria for semi-supervised learning and clustering. Based on the derived nonparametric pairwise similarity using the plug-in classifier, we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability, whose superiority is evidenced by the experimental results.",
        "bibtex": "@inproceedings{NIPS2014_e175e8a8,\n author = {Yang, Yingzhen and Liang, Feng and Yan, Shuicheng and Wang, Zhangyang and Huang, Thomas S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On a Theory of Nonparametric Pairwise Similarity for Clustering: Connecting Clustering to Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e175e8a86d28d935be4f43719651f86d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e175e8a86d28d935be4f43719651f86d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e175e8a86d28d935be4f43719651f86d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e175e8a86d28d935be4f43719651f86d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e175e8a86d28d935be4f43719651f86d-Reviews.html",
        "metareview": "",
        "pdf_size": 144904,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17878985178040224352&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; National University of Singapore; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;nus.edu.sg;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;nus.edu.sg;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e175e8a86d28d935be4f43719651f86d-Abstract.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "UIUC;NUS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "title": "On the Computational Efficiency of Training Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4697",
        "id": "4697",
        "author_site": "Roi Livni, Shai Shalev-Shwartz, Ohad Shamir",
        "author": "Roi Livni; Shai Shalev-Shwartz; Ohad Shamir",
        "abstract": "It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training neural networks.",
        "bibtex": "@inproceedings{NIPS2014_9abb5a59,\n author = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Computational Efficiency of Training Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9abb5a595720451e64d761e3a7827814-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9abb5a595720451e64d761e3a7827814-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/9abb5a595720451e64d761e3a7827814-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9abb5a595720451e64d761e3a7827814-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9abb5a595720451e64d761e3a7827814-Reviews.html",
        "metareview": "",
        "pdf_size": 377575,
        "gs_citation": 645,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2755997338821349775&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "The Hebrew University; The Hebrew University; Weizmann Institute of Science",
        "aff_domain": "mail.huji.ac.il;cs.huji.ac.il;weizmann.ac.il",
        "email": "mail.huji.ac.il;cs.huji.ac.il;weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9abb5a595720451e64d761e3a7827814-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Hebrew University of Jerusalem;Weizmann Institute of Science",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.huji.ac.il;https://www.weizmann.org.il",
        "aff_unique_abbr": "HUJI;Weizmann",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "On the Convergence Rate of Decomposable Submodular Function Minimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4791",
        "id": "4791",
        "author_site": "Robert Nishihara, Stefanie Jegelka, Michael Jordan",
        "author": "Robert Nishihara; Stefanie Jegelka; Michael I. Jordan",
        "abstract": "Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of simple\" submodular functions. Empirically, this algorithm performs extremely well, but no theoretical analysis was given. In this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.\"",
        "bibtex": "@inproceedings{NIPS2014_cb65d1b2,\n author = {Nishihara, Robert and Jegelka, Stefanie and Jordan, Michael I.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Convergence Rate of Decomposable Submodular Function Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cb65d1b2fcba48262c087a8fb207a091-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cb65d1b2fcba48262c087a8fb207a091-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/cb65d1b2fcba48262c087a8fb207a091-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cb65d1b2fcba48262c087a8fb207a091-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cb65d1b2fcba48262c087a8fb207a091-Reviews.html",
        "metareview": "",
        "pdf_size": 393766,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15809229409783029396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Electrical Engineering and Computer Science, University of California, Berkeley, CA 94720; Electrical Engineering and Computer Science, University of California, Berkeley, CA 94720; Electrical Engineering and Computer Science, University of California, Berkeley, CA 94720",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cb65d1b2fcba48262c087a8fb207a091-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On the Information Theoretic Limits of Learning Ising Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4698",
        "id": "4698",
        "author_site": "Rashish Tandon, Karthikeyan Shanmugam, Pradeep Ravikumar, Alex Dimakis",
        "author": "Karthikeyan Shanmugam; Rashish Tandon; Alexandros G. Dimakis; Pradeep Ravikumar",
        "abstract": "We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d. samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erdos-Renyi graphs in a certain dense setting.",
        "bibtex": "@inproceedings{NIPS2014_decf622b,\n author = {Shanmugam, Karthikeyan and Tandon, Rashish and Dimakis, Alexandros G. and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Information Theoretic Limits of Learning Ising Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/decf622b56d9be2b25105ec1a2e78d27-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/decf622b56d9be2b25105ec1a2e78d27-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/decf622b56d9be2b25105ec1a2e78d27-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/decf622b56d9be2b25105ec1a2e78d27-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/decf622b56d9be2b25105ec1a2e78d27-Reviews.html",
        "metareview": "",
        "pdf_size": 1829283,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17399763522861704617&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering; Department of Computer Science; Department of Electrical and Computer Engineering; Department of Computer Science",
        "aff_domain": "utexas.edu;cs.utexas.edu;austin.utexas.edu;cs.utexas.edu",
        "email": "utexas.edu;cs.utexas.edu;austin.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/decf622b56d9be2b25105ec1a2e78d27-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Unknown Institution",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "On the Number of Linear Regions of Deep Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4699",
        "id": "4699",
        "author_site": "Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, Yoshua Bengio",
        "author": "Guido Mont\u00fafar; Razvan Pascanu; Kyunghyun Cho; Yoshua Bengio",
        "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.",
        "bibtex": "@inproceedings{NIPS2014_fa6f2a46,\n author = {Mont\\'{u}far, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Number of Linear Regions of Deep Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/fa6f2a469cc4d61a92d96e74617c3d2a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/fa6f2a469cc4d61a92d96e74617c3d2a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/fa6f2a469cc4d61a92d96e74617c3d2a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/fa6f2a469cc4d61a92d96e74617c3d2a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/fa6f2a469cc4d61a92d96e74617c3d2a-Reviews.html",
        "metareview": "",
        "pdf_size": 765633,
        "gs_citation": 2946,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7322764901563502506&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Max Planck Institute for Mathematics in the Sciences; Universit \u00b4e de Montr \u00b4eal; Universit \u00b4e de Montr \u00b4eal; Universit \u00b4e de Montr \u00b4eal, CIFAR Fellow",
        "aff_domain": "mis.mpg.de;iro.umontreal.ca;umontreal.ca;umontreal.ca",
        "email": "mis.mpg.de;iro.umontreal.ca;umontreal.ca;umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/fa6f2a469cc4d61a92d96e74617c3d2a-Abstract.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Max Planck Institute for Mathematics in the Sciences;Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "Mathematics;",
        "aff_unique_url": "https://www.mis.mpg.de;https://www.umontreal.ca",
        "aff_unique_abbr": "MPI MIS;UdeM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "title": "On the Statistical Consistency of Plug-in Classifiers for Non-decomposable Performance Measures",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4700",
        "id": "4700",
        "author_site": "Harikrishna Narasimhan, Rohit Vaish, Shivani Agarwal",
        "author": "Harikrishna Narasimhan; Rohit Vaish; Shivani Agarwal",
        "abstract": "We study consistency properties of algorithms for non-decomposable performance measures that cannot be expressed as a sum of losses on individual data points, such as the F-measure used in text retrieval and several other performance measures used in class imbalanced settings. While there has been much work on designing algorithms for such performance measures, there is limited understanding of the theoretical properties of these algorithms. Recently, Ye et al. (2012) showed consistency results for two algorithms that optimize the F-measure, but their results apply only to an idealized setting, where precise knowledge of the underlying probability distribution (in the form of the",
        "bibtex": "@inproceedings{NIPS2014_3644e33a,\n author = {Narasimhan, Harikrishna and Vaish, Rohit and Agarwal, Shivani},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Statistical Consistency of Plug-in Classifiers for Non-decomposable Performance Measures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3644e33a5161ec5f3997a6acb98d4447-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3644e33a5161ec5f3997a6acb98d4447-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3644e33a5161ec5f3997a6acb98d4447-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3644e33a5161ec5f3997a6acb98d4447-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3644e33a5161ec5f3997a6acb98d4447-Reviews.html",
        "metareview": "",
        "pdf_size": 401094,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12621405313132953722&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore \u2013 560012, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore \u2013 560012, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore \u2013 560012, India",
        "aff_domain": "csa.iisc.ernet.in;csa.iisc.ernet.in;csa.iisc.ernet.in",
        "email": "csa.iisc.ernet.in;csa.iisc.ernet.in;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3644e33a5161ec5f3997a6acb98d4447-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "On the relations of LFPs & Neural Spike Trains",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4701",
        "id": "4701",
        "author_site": "David Carlson, Jana Schaich Borg, Kafui Dzirasa, Lawrence Carin",
        "author": "David E. Carlson; Jana Schaich Borg; Kafui Dzirasa; Lawrence Carin",
        "abstract": "One of the goals of neuroscience is to identify neural networks that correlate with important behaviors, environments, or genotypes. This work proposes a strategy for identifying neural networks characterized by time- and frequency-dependent connectivity patterns, using convolutional dictionary learning that links spike-train data to local field potentials (LFPs) across multiple areas of the brain. Analytical contributions are: (i) modeling dynamic relationships between LFPs and spikes; (ii) describing the relationships between spikes and LFPs, by analyzing the ability to predict LFP data from one region based on spiking information from across the brain; and (iii) development of a clustering methodology that allows inference of similarities in neurons from multiple regions. Results are based on data sets in which spike and LFP data are recorded simultaneously from up to 16 brain regions in a mouse.",
        "bibtex": "@inproceedings{NIPS2014_77937de1,\n author = {Carlson, David E. and Borg, Jana Schaich and Dzirasa, Kafui and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the relations of LFPs \\&amp; Neural Spike Trains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77937de14c86b3568067ab9dd9496e4e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/77937de14c86b3568067ab9dd9496e4e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/77937de14c86b3568067ab9dd9496e4e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/77937de14c86b3568067ab9dd9496e4e-Reviews.html",
        "metareview": "",
        "pdf_size": 534016,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14069483110422877840&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Electrical and Computer Engineering; Department of Psychiatry and Behavioral Sciences; Department of Psychiatry and Behavioral Sciences; Department of Electrical and Computer Engineering",
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/77937de14c86b3568067ab9dd9496e4e-Abstract.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Unknown Institution;Department of Psychiatry and Behavioral Sciences",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Psychiatry and Behavioral Sciences",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Online Decision-Making in General Combinatorial Spaces",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4702",
        "id": "4702",
        "author_site": "Arun Rajkumar, Shivani Agarwal",
        "author": "Arun Rajkumar; Shivani Agarwal",
        "abstract": "We study online combinatorial decision problems, where one must make sequential decisions in some combinatorial space without knowing in advance the cost of decisions on each trial; the goal is to minimize the total regret over some sequence of trials relative to the best fixed decision in hindsight. Such problems have been studied mostly in settings where decisions are represented by Boolean vectors and costs are linear in this representation. Here we study a general setting where costs may be linear in any suitable low-dimensional vector representation of elements of the decision space. We give a general algorithm for such problems that we call low-dimensional online mirror descent (LDOMD); the algorithm generalizes both the Component Hedge algorithm of Koolen et al. (2010), and a recent algorithm of Suehiro et al. (2012). Our study offers a unification and generalization of previous work, and emphasizes the role of the convex polytope arising from the vector representation of the decision space; while Boolean representations lead to 0-1 polytopes, more general vector representations lead to more general polytopes. We study several examples of both types of polytopes. Finally, we demonstrate the benefit of having a general framework for such problems via an application to an online transportation problem; the associated transportation polytopes generalize the Birkhoff polytope of doubly stochastic matrices, and the resulting algorithm generalizes the PermELearn algorithm of Helmbold and Warmuth (2009).",
        "bibtex": "@inproceedings{NIPS2014_d58255c6,\n author = {Rajkumar, Arun and Agarwal, Shivani},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Decision-Making in General Combinatorial Spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d58255c6c264137b64a22eb6a65e691b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d58255c6c264137b64a22eb6a65e691b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d58255c6c264137b64a22eb6a65e691b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d58255c6c264137b64a22eb6a65e691b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d58255c6c264137b64a22eb6a65e691b-Reviews.html",
        "metareview": "",
        "pdf_size": 320854,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6688268935250911652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore 560012, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore 560012, India",
        "aff_domain": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "email": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d58255c6c264137b64a22eb6a65e691b-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Online Optimization for Max-Norm Regularization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4703",
        "id": "4703",
        "author_site": "Jie Shen, Huan Xu, Ping Li",
        "author": "Jie Shen; Huan Xu; Ping Li",
        "abstract": "Max-norm regularizer has been extensively studied in the last decade as it promotes an effective low rank estimation of the underlying data. However, max-norm regularized problems are typically formulated and solved in a batch manner, which prevents it from processing big data due to possible memory bottleneck. In this paper, we propose an online algorithm for solving max-norm regularized problems that is scalable to large problems. Particularly, we consider the matrix decomposition problem as an example, although our analysis can also be applied in other problems such as matrix completion. The key technique in our algorithm is to reformulate the max-norm into a matrix factorization form, consisting of a basis component and a coefficients one. In this way, we can solve the optimal basis and coefficients alternatively. We prove that the basis produced by our algorithm converges to a stationary point asymptotically. Experiments demonstrate encouraging results for the effectiveness and robustness of our algorithm. See the full paper at arXiv:1406.3190.",
        "bibtex": "@inproceedings{NIPS2014_08211bbb,\n author = {Shen, Jie and Xu, Huan and Li, Ping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Optimization for Max-Norm Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/08211bbb6d687bff251342162c6a5f84-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/08211bbb6d687bff251342162c6a5f84-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/08211bbb6d687bff251342162c6a5f84-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/08211bbb6d687bff251342162c6a5f84-Reviews.html",
        "metareview": "",
        "pdf_size": 185514,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4167245601380084812&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Science, Rutgers University; Dept. of Mech. Engineering, National Univ. of Singapore; Dept. of Statistics + Dept. of Computer Science, Rutgers University",
        "aff_domain": "rutgers.edu;nus.edu.sg;stat.rutgers.edu",
        "email": "rutgers.edu;nus.edu.sg;stat.rutgers.edu",
        "github": "",
        "project": "arXiv:1406.3190",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/08211bbb6d687bff251342162c6a5f84-Abstract.html",
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "Rutgers University;National University of Singapore;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science;Department of Mechanical Engineering;Department of Statistics",
        "aff_unique_url": "https://www.rutgers.edu;https://www.nus.edu.sg;",
        "aff_unique_abbr": "Rutgers;NUS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Singapore;"
    },
    {
        "title": "Online and Stochastic Gradient Methods for Non-decomposable Loss Functions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4705",
        "id": "4705",
        "author_site": "Purushottam Kar, Harikrishna Narasimhan, Prateek Jain",
        "author": "Purushottam Kar; Harikrishna Narasimhan; Prateek Jain",
        "abstract": "Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, Prec @k and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes Prec @k, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.",
        "bibtex": "@inproceedings{NIPS2014_9638ddfc,\n author = {Kar, Purushottam and Narasimhan, Harikrishna and Jain, Prateek},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online and Stochastic Gradient Methods for Non-decomposable Loss Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9638ddfc7e3d56a611292c1578b19ff8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9638ddfc7e3d56a611292c1578b19ff8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/9638ddfc7e3d56a611292c1578b19ff8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9638ddfc7e3d56a611292c1578b19ff8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9638ddfc7e3d56a611292c1578b19ff8-Reviews.html",
        "metareview": "",
        "pdf_size": 391937,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17674546301808147089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Microsoft Research, INDIA; Indian Institute of Science, Bangalore, INDIA; Microsoft Research, INDIA",
        "aff_domain": "microsoft.com;csa.iisc.ernet.in;microsoft.com",
        "email": "microsoft.com;csa.iisc.ernet.in;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9638ddfc7e3d56a611292c1578b19ff8-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Microsoft;Indian Institute of Science",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.iisc.ac.in",
        "aff_unique_abbr": "MSR;IISc",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Online combinatorial optimization with stochastic decision sets and adversarial losses",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4706",
        "id": "4706",
        "author_site": "Gergely Neu, Michal Valko",
        "author": "Gergely Neu; Michal Valko",
        "abstract": "Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.",
        "bibtex": "@inproceedings{NIPS2014_06da2cfb,\n author = {Neu, Gergely and Valko, Michal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online combinatorial optimization with stochastic decision sets and adversarial losses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/06da2cfb2088f776d522b5cdafe677ab-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/06da2cfb2088f776d522b5cdafe677ab-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/06da2cfb2088f776d522b5cdafe677ab-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/06da2cfb2088f776d522b5cdafe677ab-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/06da2cfb2088f776d522b5cdafe677ab-Reviews.html",
        "metareview": "",
        "pdf_size": 464150,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11459195773396053750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "SequeL team, INRIA Lille \u2013 Nord Europe, France; SequeL team, INRIA Lille \u2013 Nord Europe, France",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/06da2cfb2088f776d522b5cdafe677ab-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA Lille \u2013 Nord Europe",
        "aff_unique_dep": "SequeL team",
        "aff_unique_url": "https://www.inria.fr/en/centre/lille-nord-europe",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Optimal Neural Codes for Control and Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4707",
        "id": "4707",
        "author_site": "Alex K Susemihl, Ron Meir, Manfred Opper",
        "author": "Alex Susemihl; Manfred Opper; Ron Meir",
        "abstract": "Agents acting in the natural world aim at selecting appropriate actions based on noisy and partial sensory observations. Many behaviors leading to decision making and action selection in a closed loop setting are naturally phrased within a control theoretic framework. Within the framework of optimal Control Theory, one is usually given a cost function which is minimized by selecting a control law based on the observations. While in standard control settings the sensors are assumed fixed, biological systems often gain from the extra flexibility of optimizing the sensors themselves. However, this sensory adaptation is geared towards control rather than perception, as is often assumed. In this work we show that sensory adaptation for control differs from sensory adaptation for perception, even for simple control setups. This implies, consistently with recent experimental results, that when studying sensory adaptation, it is essential to account for the task being performed.",
        "bibtex": "@inproceedings{NIPS2014_8dc533d6,\n author = {Susemihl, Alex and Opper, Manfred and Meir, Ron},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Neural Codes for Control and Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8dc533d625e3a3e3d8096982e15b0eca-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8dc533d625e3a3e3d8096982e15b0eca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8dc533d625e3a3e3d8096982e15b0eca-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8dc533d625e3a3e3d8096982e15b0eca-Reviews.html",
        "metareview": "",
        "pdf_size": 614556,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12531302683348386764&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Technische Universit \u00a8at Berlin + Google; Technische Universit \u00a8at Berlin; Technion - Haifa",
        "aff_domain": "1; 2; 3",
        "email": "1; 2; 3",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8dc533d625e3a3e3d8096982e15b0eca-Abstract.html",
        "aff_unique_index": "0+1;0;2",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;Google;Technion - Israel Institute of Technology",
        "aff_unique_dep": ";Google;",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.google.com;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "TU Berlin;Google;Technion",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Mountain View;Haifa",
        "aff_country_unique_index": "0+1;0;2",
        "aff_country_unique": "Germany;United States;Israel"
    },
    {
        "title": "Optimal Regret Minimization in Posted-Price Auctions with Strategic Buyers",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4418",
        "id": "4418",
        "author_site": "Mehryar Mohri, Andres Munoz",
        "author": "Mehryar Mohri; Andres Mu\u00f1oz Medina",
        "abstract": "We study revenue optimization learning algorithms for posted-price auctions with strategic buyers. We analyze a very broad family of monotone regret minimization algorithms for this problem, which includes the previous best known algorithm, and show that no algorithm in that family admits a strategic regret more favorable than $\\Omega(\\sqrt{T})$. We then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in $O(\\log T)$, an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural analysis and simpler proofs, and the ideas behind its design are general. We also report the results of empirical evaluations comparing our algorithm with the previous best algorithm and show a consistent exponential improvement in several different scenarios.",
        "bibtex": "@inproceedings{NIPS2014_69ed889b,\n author = {Mohri, Mehryar and Medina, Andres Mu\\~{n}oz},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Regret Minimization in Posted-Price Auctions with Strategic Buyers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/69ed889b47b731892f80016959f16ca0-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/69ed889b47b731892f80016959f16ca0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/69ed889b47b731892f80016959f16ca0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/69ed889b47b731892f80016959f16ca0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/69ed889b47b731892f80016959f16ca0-Reviews.html",
        "metareview": "",
        "pdf_size": 390014,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11021447249020699363&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Courant Institute and Google Research; Courant Institute",
        "aff_domain": "cims.nyu.edu;cims.nyu.edu",
        "email": "cims.nyu.edu;cims.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/69ed889b47b731892f80016959f16ca0-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Courant Institute;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Courant Institute;Mathematical Sciences",
        "aff_unique_url": "https://courant.nyu.edu;https://courant.nyu.edu",
        "aff_unique_abbr": "Courant;Courant",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Optimal Teaching for Limited-Capacity Human Learners",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4420",
        "id": "4420",
        "author_site": "Kaustubh R Patil, Jerry Zhu, \u0141ukasz Kope\u0107, Bradley C Love",
        "author": "Kaustubh Raosaheb Patil; Xiaojin Zhu; \u0141ukasz Kope\u0107; Bradley C. Love",
        "abstract": "Basic decisions, such as judging a person as a friend or foe, involve categorizing novel stimuli. Recent work finds that people\u2019s category judgments are guided by a small set of examples that are retrieved from memory at decision time. This limited and stochastic retrieval places limits on human performance for probabilistic classification decisions. In light of this capacity limitation, recent work finds that idealizing training items, such that the saliency of ambiguous cases is reduced, improves human performance on novel test items. One shortcoming of previous work in idealization is that category distributions were idealized in an ad hoc or heuristic fashion. In this contribution, we take a first principles approach to constructing idealized training sets. We apply a machine teaching procedure to a cognitive model that is either limited capacity (as humans are) or unlimited capacity (as most machine learning systems are). As predicted, we find that the machine teacher recommends idealized training sets. We also find that human learners perform best when training recommendations from the machine teacher are based on a limited-capacity model. As predicted, to the extent that the learning model used by the machine teacher conforms to the true nature of human learners, the recommendations of the machine teacher prove effective. Our results provide a normative basis (given capacity constraints) for idealization procedures and offer a novel selection procedure for models of human learning.",
        "bibtex": "@inproceedings{NIPS2014_5138ed99,\n author = {Patil, Kaustubh Raosaheb and Zhu, Xiaojin and Kope\\'{c}, \\L ukasz and Love, Bradley C.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Teaching for Limited-Capacity Human Learners},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5138ed99148ce91255d67df63be46421-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5138ed99148ce91255d67df63be46421-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5138ed99148ce91255d67df63be46421-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5138ed99148ce91255d67df63be46421-Reviews.html",
        "metareview": "",
        "pdf_size": 286955,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6401341109753907752&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Affective Brain Lab, UCL + MIT Sloan Neuroeconomics Lab; Department of Computer Sciences, University of Wisconsin-Madison; Experimental Psychology, University College London; Experimental Psychology, University College London",
        "aff_domain": "gmail.com;cs.wisc.edu;ucl.ac.uk;ucl.ac.uk",
        "email": "gmail.com;cs.wisc.edu;ucl.ac.uk;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5138ed99148ce91255d67df63be46421-Abstract.html",
        "aff_unique_index": "0+1;2;0;0",
        "aff_unique_norm": "University College London;Massachusetts Institute of Technology;University of Wisconsin-Madison",
        "aff_unique_dep": "Affective Brain Lab;Sloan Neuroeconomics Lab;Department of Computer Sciences",
        "aff_unique_url": "https://www.ucl.ac.uk;https://mitsloan.mit.edu;https://www.wisc.edu",
        "aff_unique_abbr": "UCL;MIT;UW-Madison",
        "aff_campus_unique_index": "1;2;3;3",
        "aff_campus_unique": ";Cambridge;Madison;London",
        "aff_country_unique_index": "0+1;1;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Optimal decision-making with time-varying evidence reliability",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4422",
        "id": "4422",
        "author_site": "Jan Drugowitsch, Ruben Moreno-Bote, Alexandre Pouget",
        "author": "Jan Drugowitsch; Rub\u00e9n Moreno-Bote; Alexandre Pouget",
        "abstract": "Previous theoretical and experimental work on optimal decision-making was restricted to the artificial setting of a reliability of the momentary sensory evidence that remained constant within single trials. The work presented here describes the computation and characterization of optimal decision-making in the more realistic case of an evidence reliability that varies across time even within a trial. It shows that, in this case, the optimal behavior is determined by a bound in the decision maker's belief that depends only on the current, but not the past, reliability. We furthermore demonstrate that simpler heuristics fail to match the optimal performance for certain characteristics of the process that determines the time-course of this reliability, causing a drop in reward rate by more than 50%.",
        "bibtex": "@inproceedings{NIPS2014_97b8e790,\n author = {Drugowitsch, Jan and Moreno-Bote, Rub\\'{e}n and Pouget, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal decision-making with time-varying evidence reliability},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/97b8e790d1c6f726710c65420462cce4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/97b8e790d1c6f726710c65420462cce4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/97b8e790d1c6f726710c65420462cce4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/97b8e790d1c6f726710c65420462cce4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/97b8e790d1c6f726710c65420462cce4-Reviews.html",
        "metareview": "",
        "pdf_size": 1580416,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17397655113640580472&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/97b8e790d1c6f726710c65420462cce4-Abstract.html"
    },
    {
        "title": "Optimal prior-dependent neural population codes under shared input noise",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4708",
        "id": "4708",
        "author_site": "Agnieszka Grabska-Barwinska, Jonathan W Pillow",
        "author": "Agnieszka Grabska-Barwi\u0144ska; Jonathan W. Pillow",
        "abstract": "The brain uses population codes to form distributed, noise-tolerant representations of sensory and motor variables. Recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain. However, the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix. Here we analyze population coding under a simple alternative model in which latent input noise\" corrupts the stimulus before it is encoded by the population. This provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons, and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations. We examine prior-dependent, Bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately Gaussian. These analyses extend previous results on independent Poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information. We show that, for homogeneous populations that tile the input domain, optimal tuning curve width depends on the prior, the loss function, the resource constraint, and the amount of input noise. This framework provides a practical testbed for examining issues of optimality, noise, correlation, and coding fidelity in realistic neural populations.\"",
        "bibtex": "@inproceedings{NIPS2014_0d3ec37c,\n author = {Grabska-Barwi\\'{n}ska, Agnieszka and Pillow, Jonathan W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal prior-dependent neural population codes under shared input noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0d3ec37c63fcda06f737f0a3eb8d54ae-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0d3ec37c63fcda06f737f0a3eb8d54ae-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0d3ec37c63fcda06f737f0a3eb8d54ae-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0d3ec37c63fcda06f737f0a3eb8d54ae-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0d3ec37c63fcda06f737f0a3eb8d54ae-Reviews.html",
        "metareview": "",
        "pdf_size": 862324,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2110584023823852707&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Princeton Neuroscience Institute, Department of Psychology, Princeton University",
        "aff_domain": "gatsby.ucl.ac.uk;princeton.edu",
        "email": "gatsby.ucl.ac.uk;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0d3ec37c63fcda06f737f0a3eb8d54ae-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;Princeton University",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Department of Psychology",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.princeton.edu",
        "aff_unique_abbr": "UCL;Princeton",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Optimal rates for k-NN density and mode estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4709",
        "id": "4709",
        "author_site": "Sanjoy Dasgupta, Samory Kpotufe",
        "author": "Sanjoy Dasgupta; Samory Kpotufe",
        "abstract": "We present two related contributions of independent interest: (1) high-probability finite sample rates for $k$-NN density estimation, and (2) practical mode estimators -- based on $k$-NN -- which attain minimax-optimal rates under surprisingly general distributional conditions.",
        "bibtex": "@inproceedings{NIPS2014_a5549f3f,\n author = {Dasgupta, Sanjoy and Kpotufe, Samory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal rates for k-NN density and mode estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a5549f3f66cedf4204ffe35552e5b59c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a5549f3f66cedf4204ffe35552e5b59c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a5549f3f66cedf4204ffe35552e5b59c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a5549f3f66cedf4204ffe35552e5b59c-Reviews.html",
        "metareview": "",
        "pdf_size": 472300,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1076572993270840104&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of California, San Diego, CSE; Princeton University, ORFE + TTI-Chicago",
        "aff_domain": "eng.ucsd.edu;princeton.edu",
        "email": "eng.ucsd.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a5549f3f66cedf4204ffe35552e5b59c-Abstract.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "University of California, San Diego;Princeton University;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Computer Science and Engineering;Operations Research and Financial Engineering;",
        "aff_unique_url": "https://ucsd.edu;https://www.princeton.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UCSD;Princeton;TTI",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "San Diego;;Chicago",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Optimistic Planning in Markov Decision Processes Using a Generative Model",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4710",
        "id": "4710",
        "author_site": "Bal\u00e1zs Sz\u00f6r\u00e9nyi, Gunnar Kedenburg, Remi Munos",
        "author": "Bal\u00e1zs Sz\u00f6r\u00e9nyi; Gunnar Kedenburg; Remi Munos",
        "abstract": "We consider the problem of online planning in a Markov decision process with discounted rewards for any given initial state. We consider the PAC sample complexity problem of computing, with probability $1-\\delta$, an $\\epsilon$-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples). We design an algorithm, called StOP (for Stochastic-Optimistic Planning), based on the optimism in the face of uncertainty\" principle. StOP can be used in the general setting, requires only a generative model, and enjoys a complexity bound that only depends on the local structure of the MDP.\"",
        "bibtex": "@inproceedings{NIPS2014_f3bf7765,\n author = {Sz\\\"{o}r\\'{e}nyi, Bal\\'{a}zs and Kedenburg, Gunnar and Munos, Remi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimistic Planning in Markov Decision Processes Using a Generative Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f3bf7765f40258b5bc2a57e40f96777a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f3bf7765f40258b5bc2a57e40f96777a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f3bf7765f40258b5bc2a57e40f96777a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f3bf7765f40258b5bc2a57e40f96777a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f3bf7765f40258b5bc2a57e40f96777a-Reviews.html",
        "metareview": "",
        "pdf_size": 416431,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12449029469675470337&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "INRIA Lille - Nord Europe, SequeL project, France + MTA-SZTE Research Group on Arti\ufb01cial Intelligence, Hungary; INRIA Lille - Nord Europe, SequeL project, France; INRIA Lille - Nord Europe, SequeL project, France",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f3bf7765f40258b5bc2a57e40f96777a-Abstract.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "INRIA;Hungarian Academy of Sciences - Szeged University",
        "aff_unique_dep": "SequeL project;Research Group on Artificial Intelligence",
        "aff_unique_url": "https://www.inria.fr;https://www.mta.hu/english, https://www.szte.hu/english",
        "aff_unique_abbr": "INRIA;MTA-SZTE AI RG",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Lille - Nord Europe;Szeged",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "France;Hungary"
    },
    {
        "title": "Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4711",
        "id": "4711",
        "author_site": "Sang-Yun Oh, Onkar Dalal, Kshitij Khare, Bala Rajaratnam",
        "author": "Sang-Yun Oh; Onkar Dalal; Kshitij Khare; Bala Rajaratnam",
        "abstract": "Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of $\\ell_1$ penalized estimation in the Gaussian framework. Though many of these approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses cyclic coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing $\\ell_1$-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous pay offs for $\\ell_1$-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.",
        "bibtex": "@inproceedings{NIPS2014_a576b0f3,\n author = {Oh, Sang-Yun and Dalal, Onkar and Khare, Kshitij and Rajaratnam, Bala},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a576b0f35289c373f24a96653de39b8b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a576b0f35289c373f24a96653de39b8b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a576b0f35289c373f24a96653de39b8b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a576b0f35289c373f24a96653de39b8b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a576b0f35289c373f24a96653de39b8b-Reviews.html",
        "metareview": "",
        "pdf_size": 288513,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12192355597670372214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computational Research Division, Lawrence Berkeley National Lab; Stanford University; Department of Statistics, University of Florida; Department of Statistics, Stanford University",
        "aff_domain": "lbl.gov;alumni.stanford.edu;stat.ufl.edu;stanford.edu",
        "email": "lbl.gov;alumni.stanford.edu;stat.ufl.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a576b0f35289c373f24a96653de39b8b-Abstract.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Lawrence Berkeley National Lab;Stanford University;University of Florida",
        "aff_unique_dep": "Computational Research Division;;Department of Statistics",
        "aff_unique_url": "https://www.lbl.gov;https://www.stanford.edu;https://www.ufl.edu",
        "aff_unique_abbr": "LBL;Stanford;UF",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Optimizing Energy Production Using Policy Search and Predictive State Representations",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4424",
        "id": "4424",
        "author_site": "Yuri Grinberg, Doina Precup, Michel Gendreau",
        "author": "Yuri Grinberg; Doina Precup; Michel Gendreau",
        "abstract": "We consider the challenging practical problem of optimizing the power production of a complex of hydroelectric power plants, which involves control over three continuous action variables, uncertainty in the amount of water inflows and a variety of constraints that need to be satisfied. We propose a policy-search-based approach coupled with predictive modelling to address this problem. This approach has some key advantages compared to other alternatives, such as dynamic programming: the policy representation and search algorithm can conveniently incorporate domain knowledge; the resulting policies are easy to interpret, and the algorithm is naturally parallelizable. Our algorithm obtains a policy which outperforms the solution found by dynamic programming both quantitatively and qualitatively.",
        "bibtex": "@inproceedings{NIPS2014_0942e574,\n author = {Grinberg, Yuri and Precup, Doina and Gendreau, Michel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimizing Energy Production Using Policy Search and Predictive State Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0942e5741531db4483d0cc9d6b83ace2-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0942e5741531db4483d0cc9d6b83ace2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0942e5741531db4483d0cc9d6b83ace2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0942e5741531db4483d0cc9d6b83ace2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0942e5741531db4483d0cc9d6b83ace2-Reviews.html",
        "metareview": "",
        "pdf_size": 550883,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3978879676039073107&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science, McGill University; School of Computer Science, McGill University; \u00b4Ecole Polytechnique de Montr \u00b4eal",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca;cirrelt.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca;cirrelt.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0942e5741531db4483d0cc9d6b83ace2-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "McGill University;Ecole Polytechnique de Montr\u00e9al",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.mcgill.ca;https://www.polymtl.ca",
        "aff_unique_abbr": "McGill;Polytechnique Montr\u00e9al",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Montreal;Montr\u00e9al",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Optimizing F-Measures by Cost-Sensitive Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4712",
        "id": "4712",
        "author_site": "Shameem Puthiya Parambath, Nicolas Usunier, Yves Grandvalet",
        "author": "Shameem A. Puthiya Parambath; Nicolas Usunier; Yves Grandvalet",
        "abstract": "We present a theoretical analysis of F-measures for binary, multiclass and multilabel classification. These performance measures are non-linear, but in many scenarios they are pseudo-linear functions of the per-class false negative/false positive rate. Based on this observation, we present a general reduction of F-measure maximization to cost-sensitive classification with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on F-measures, which are asymptotic in nature. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F-measure optimization tasks.",
        "bibtex": "@inproceedings{NIPS2014_5c0314ec,\n author = {Parambath, Shameem A. Puthiya and Usunier, Nicolas and Grandvalet, Yves},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimizing F-Measures by Cost-Sensitive Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5c0314ec1b57fcd36bbb013f3f025868-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5c0314ec1b57fcd36bbb013f3f025868-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5c0314ec1b57fcd36bbb013f3f025868-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5c0314ec1b57fcd36bbb013f3f025868-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5c0314ec1b57fcd36bbb013f3f025868-Reviews.html",
        "metareview": "",
        "pdf_size": 1241969,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6458818690461609337&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5c0314ec1b57fcd36bbb013f3f025868-Abstract.html"
    },
    {
        "title": "Orbit Regularization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4714",
        "id": "4714",
        "author_site": "Renato Negrinho, Andre Martins",
        "author": "Renato Negrinho; Andr\u00e9 F. T. Martins",
        "abstract": "We propose a general framework for regularization based on group majorization. In this framework, a group is defined to act on the parameter space and an orbit is fixed; to control complexity, the model parameters are confined to lie in the convex hull of this orbit (the orbitope). Common regularizers are recovered as particular cases, and a connection is revealed between the recent sorted 1 -norm and the hyperoctahedral group. We derive the properties a group must satisfy for being amenable to optimization with conditional and projected gradient algorithms. Finally, we suggest a continuation strategy for orbit exploration, presenting simulation results for the symmetric and hyperoctahedral groups.",
        "bibtex": "@inproceedings{NIPS2014_888637e6,\n author = {Negrinho, Renato and Martins, Andr\\'{e} F. T.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Orbit Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/888637e6555c5498c2523f2315963e08-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/888637e6555c5498c2523f2315963e08-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/888637e6555c5498c2523f2315963e08-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/888637e6555c5498c2523f2315963e08-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/888637e6555c5498c2523f2315963e08-Reviews.html",
        "metareview": "",
        "pdf_size": 679349,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8325395199533520933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Instituto de Telecomunicac\u00e7\u00f5es, Instituto Superior T\u00e9cnico, 1049\u2013001 Lisboa, Portugal; Instituto de Telecomunicac\u00e7\u00f5es, Instituto Superior T\u00e9cnico, 1049\u2013001 Lisboa, Portugal + Priberam Labs, Alameda D. Afonso Henriques, 41 - 2\u00b0, 1000\u2013123, Lisboa, Portugal",
        "aff_domain": "gmail.com;priberam.pt",
        "email": "gmail.com;priberam.pt",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/888637e6555c5498c2523f2315963e08-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Instituto Superior T\u00e9cnico;Priberam Labs",
        "aff_unique_dep": "Instituto de Telecomunicac\u00e7\u00f5es;",
        "aff_unique_url": "https://www.ist.utl.pt;https://www.priberam.com",
        "aff_unique_abbr": "IST;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lisboa;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Portugal"
    },
    {
        "title": "PAC-Bayesian AUC classification and scoring",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4715",
        "id": "4715",
        "author_site": "James Ridgway, Pierre Alquier, Nicolas Chopin, Feng Liang",
        "author": "James Ridgway; Pierre Alquier; Nicolas Chopin; Feng Liang",
        "abstract": "We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior.",
        "bibtex": "@inproceedings{NIPS2014_eba86bd8,\n author = {Ridgway, James and Alquier, Pierre and Chopin, Nicolas and Liang, Feng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PAC-Bayesian AUC classification and scoring},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/eba86bd8fc6e118a75647a9f89a437b5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/eba86bd8fc6e118a75647a9f89a437b5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/eba86bd8fc6e118a75647a9f89a437b5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/eba86bd8fc6e118a75647a9f89a437b5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/eba86bd8fc6e118a75647a9f89a437b5-Reviews.html",
        "metareview": "",
        "pdf_size": 363003,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1173144159092003869&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "CREST and CEREMADE University Dauphine; CREST (ENSAE); CREST (ENSAE) + HEC Paris; University of Illinois at Urbana-Champaign",
        "aff_domain": "ensae.fr;ucd.ie;ensae.fr;illinois.edu",
        "email": "ensae.fr;ucd.ie;ensae.fr;illinois.edu",
        "github": "",
        "project": "http://www.crest.fr/pagesperso.php?user=3328",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/eba86bd8fc6e118a75647a9f89a437b5-Abstract.html",
        "aff_unique_index": "0;1;1+2;3",
        "aff_unique_norm": "University Paris Dauphine;CREST;HEC Paris;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "CREST and CEREMADE;ENSAE;;",
        "aff_unique_url": "https://www.univ-paris-dauphine.fr;https://www.crest.fr;https://www.hec.edu;https://illinois.edu",
        "aff_unique_abbr": "UPD;CREST;HEC;UIUC",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;0;0+0;1",
        "aff_country_unique": "France;United States"
    },
    {
        "title": "PEWA: Patch-based Exponentially Weighted Aggregation for image denoising",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4716",
        "id": "4716",
        "author": "Charles Kervrann",
        "abstract": "Patch-based methods have been widely used for noise reduction in recent years. In this paper, we propose a general statistical aggregation method which combines image patches denoised with several commonly-used algorithms. We show that weakly denoised versions of the input image obtained with standard methods, can serve to compute an efficient patch-based aggregated estimd aggregation (EWA) estimator. The resulting approach (PEWA) is based on a MCMC sampling and has a nice statistical foundation while producing denoising results that are comparable to the current state-of-the-art. We demonstrate the performance of the denoising algorithm on real images and we compare the results to several competitive methods.",
        "bibtex": "@inproceedings{NIPS2014_3180c224,\n author = {Kervrann, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PEWA: Patch-based Exponentially Weighted Aggregation for image denoising},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3180c2243f2d3667bbe3855854554dcf-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3180c2243f2d3667bbe3855854554dcf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3180c2243f2d3667bbe3855854554dcf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3180c2243f2d3667bbe3855854554dcf-Reviews.html",
        "metareview": "",
        "pdf_size": 5745873,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6277436825637419401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3180c2243f2d3667bbe3855854554dcf-Abstract.html"
    },
    {
        "title": "Parallel Direction Method of Multipliers",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4717",
        "id": "4717",
        "author_site": "Huahua Wang, Arindam Banerjee, Zhi-Quan Luo",
        "author": "Huahua Wang; Arindam Banerjee; Zhi-Quan Luo",
        "abstract": "We consider the problem of minimizing block-separable convex functions subject to linear constraints. While the Alternating Direction Method of Multipliers (ADMM) for two-block linear constraints has been intensively studied both theoretically and empirically, in spite of some preliminary work, effective generalizations of ADMM to multiple blocks is still unclear. In this paper, we propose a parallel randomized block coordinate method named Parallel Direction Method of Multipliers (PDMM) to solve the optimization problems with multi-block linear constraints. PDMM randomly updates some blocks in parallel, behaving like parallel randomized block coordinate descent. We establish the global convergence and the iteration complexity for PDMM with constant step size. We also show that PDMM can do randomized block coordinate descent on overlapping blocks. Experimental results show that PDMM performs better than state-of-the-arts methods in two applications, robust principal component analysis and overlapping group lasso.",
        "bibtex": "@inproceedings{NIPS2014_6ba14530,\n author = {Wang, Huahua and Banerjee, Arindam and Luo, Zhi-Quan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallel Direction Method of Multipliers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6ba1453022da1faf642d4de27241d53f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/6ba1453022da1faf642d4de27241d53f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/6ba1453022da1faf642d4de27241d53f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/6ba1453022da1faf642d4de27241d53f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/6ba1453022da1faf642d4de27241d53f-Reviews.html",
        "metareview": "",
        "pdf_size": 401095,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2186540213078932053&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "University of Minnesota, Twin Cities; University of Minnesota, Twin Cities; University of Minnesota, Twin Cities",
        "aff_domain": "cs.umn.edu;cs.umn.edu;umn.edu",
        "email": "cs.umn.edu;cs.umn.edu;umn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/6ba1453022da1faf642d4de27241d53f-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.minnesota.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Twin Cities",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Parallel Double Greedy Submodular Maximization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4718",
        "id": "4718",
        "author_site": "Xinghao Pan, Stefanie Jegelka, Joseph Gonzalez, Joseph K Bradley, Michael Jordan",
        "author": "Xinghao Pan; Stefanie Jegelka; Joseph Gonzalez; Joseph Bradley; Michael I. Jordan",
        "abstract": "Many machine learning problems can be reduced to the maximization of submodular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. and follows a strongly serial double-greedy logic and program analysis. In this work, we propose two methods to parallelize the double-greedy algorithm. The first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee. The second, concurrency control approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism. As a consequence we explore the trade off space between guaranteed performance and objective optimality. We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.",
        "bibtex": "@inproceedings{NIPS2014_63545404,\n author = {Pan, Xinghao and Jegelka, Stefanie and Gonzalez, Joseph and Bradley, Joseph and Jordan, Michael I.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallel Double Greedy Submodular Maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/63545404a8d4327e42a3416d73647995-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/63545404a8d4327e42a3416d73647995-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/63545404a8d4327e42a3416d73647995-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/63545404a8d4327e42a3416d73647995-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/63545404a8d4327e42a3416d73647995-Reviews.html",
        "metareview": "",
        "pdf_size": 839933,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1081845187378234992&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science + Department of Statistics",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/63545404a8d4327e42a3416d73647995-Abstract.html",
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Department of Statistics",
        "aff_unique_url": "https://web.mit.edu;",
        "aff_unique_abbr": "MIT;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Parallel Feature Selection Inspired by Group Testing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4719",
        "id": "4719",
        "author_site": "Yingbo Zhou, Utkarsh Porwal, Ce Zhang, Hung Q Ngo, XuanLong Nguyen, Christopher R\u00e9, Venu Govindaraju",
        "author": "Yingbo Zhou; Utkarsh Porwal; Ce Zhang; Hung Ngo; XuanLong Nguyen; Christopher R\u00e9; Venu Govindaraju",
        "abstract": "This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.",
        "bibtex": "@inproceedings{NIPS2014_902e3cec,\n author = {Zhou, Yingbo and Porwal, Utkarsh and Zhang, Ce and Ngo, Hung and Nguyen, XuanLong and R\\'{e}, Christopher and Govindaraju, Venu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallel Feature Selection Inspired by Group Testing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/902e3cec1a88efbda8b91d1ddaddfba6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/902e3cec1a88efbda8b91d1ddaddfba6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/902e3cec1a88efbda8b91d1ddaddfba6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/902e3cec1a88efbda8b91d1ddaddfba6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/902e3cec1a88efbda8b91d1ddaddfba6-Reviews.html",
        "metareview": "",
        "pdf_size": 1162941,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9664736620584162742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "CSE Department, SUNY at Buffalo; CSE Department, SUNY at Buffalo; CS Department, University of Wisconsin-Madison; CSE Department, SUNY at Buffalo; EECs Department, University of Michigan; CS Department, Stanford University; CSE Department, SUNY at Buffalo",
        "aff_domain": "buffalo.edu;buffalo.edu;cs.wisc.edu;buffalo.edu;umich.edu;cs.stanford.edu;buffalo.edu",
        "email": "buffalo.edu;buffalo.edu;cs.wisc.edu;buffalo.edu;umich.edu;cs.stanford.edu;buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/902e3cec1a88efbda8b91d1ddaddfba6-Abstract.html",
        "aff_unique_index": "0;0;1;0;2;3;0",
        "aff_unique_norm": "State University of New York at Buffalo;University of Wisconsin-Madison;University of Michigan;Stanford University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Computer Sciences Department;EECs Department;Department of Computer Science",
        "aff_unique_url": "https://www.buffalo.edu;https://www.wisc.edu;https://www.umich.edu;https://www.stanford.edu",
        "aff_unique_abbr": "SUNY Buffalo;UW-Madison;UM;Stanford",
        "aff_campus_unique_index": "0;0;1;0;3;0",
        "aff_campus_unique": "Buffalo;Madison;;Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Parallel Sampling of HDPs using Sub-Cluster Splits",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4664",
        "id": "4664",
        "author_site": "Jason Chang, John Fisher III",
        "author": "Jason Chang; John W. Fisher III",
        "abstract": "We develop a sampling technique for Hierarchical Dirichlet process models. The parallel algorithm builds upon [Chang & Fisher 2013] by proposing large split and merge moves based on learned sub-clusters. The additional global split and merge moves drastically improve convergence in the experimental results. Furthermore, we discover that cross-validation techniques do not adequately determine convergence, and that previous sampling methods converge slower than were previously expected.",
        "bibtex": "@inproceedings{NIPS2014_763d339d,\n author = {Chang, Jason and Fisher III, John W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallel Sampling of HDPs using Sub-Cluster Splits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/763d339d9e1b0caa2ea9ae01d4b9e0d0-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/763d339d9e1b0caa2ea9ae01d4b9e0d0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/763d339d9e1b0caa2ea9ae01d4b9e0d0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/763d339d9e1b0caa2ea9ae01d4b9e0d0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/763d339d9e1b0caa2ea9ae01d4b9e0d0-Reviews.html",
        "metareview": "",
        "pdf_size": 1331739,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10146785777218101865&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "CSAIL, MIT; CSAIL, MIT",
        "aff_domain": "csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/763d339d9e1b0caa2ea9ae01d4b9e0d0-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Parallel Successive Convex Approximation for Nonsmooth Nonconvex Optimization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4720",
        "id": "4720",
        "author_site": "Meisam Razaviyayn, Mingyi Hong, Zhi-Quan Luo, Jong-Shi Pang",
        "author": "Meisam Razaviyayn; Mingyi Hong; Zhi-Quan Luo; Jong-Shi Pang",
        "abstract": "Consider the problem of minimizing the sum of a smooth (possibly non-convex) and a convex (possibly nonsmooth) function involving a large number of variables. A popular approach to solve this problem is the block coordinate descent (BCD) method whereby at each iteration only one variable block is updated while the remaining variables are held fixed. With the recent advances in the developments of the multi-core parallel processing technology, it is desirable to parallelize the BCD method by allowing multiple blocks to be updated simultaneously at each iteration of the algorithm. In this work, we propose an inexact parallel BCD approach where at each iteration, a subset of the variables is updated in parallel by minimizing convex approximations of the original objective function. We investigate the convergence of this parallel BCD method for both randomized and cyclic variable selection rules. We analyze the asymptotic and non-asymptotic convergence behavior of the algorithm for both convex and non-convex objective functions. The numerical experiments suggest that for a special case of Lasso minimization problem, the cyclic block selection rule can outperform the randomized rule.",
        "bibtex": "@inproceedings{NIPS2014_115f841d,\n author = {Razaviyayn, Meisam and Hong, Mingyi and Luo, Zhi-Quan and Pang, Jong-Shi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallel Successive Convex Approximation for Nonsmooth Nonconvex Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/115f841d5edaaef4d084469ea159e3f4-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/115f841d5edaaef4d084469ea159e3f4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/115f841d5edaaef4d084469ea159e3f4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/115f841d5edaaef4d084469ea159e3f4-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/115f841d5edaaef4d084469ea159e3f4-Reviews.html",
        "metareview": "",
        "pdf_size": 347067,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7711465471821091059&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Electrical Engineering Department, Stanford University; Industrial and Manufacturing Systems Engineering, Iowa State University; Department of Electrical and Computer Engineering, University of Minnesota; Department of Industrial and Systems Engineering, University of Southern California",
        "aff_domain": "stanford.edu;iastate.edu;umn.edu;usc.edu",
        "email": "stanford.edu;iastate.edu;umn.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/115f841d5edaaef4d084469ea159e3f4-Abstract.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Stanford University;Iowa State University;University of Minnesota;University of Southern California",
        "aff_unique_dep": "Electrical Engineering Department;Industrial and Manufacturing Systems Engineering;Department of Electrical and Computer Engineering;Department of Industrial and Systems Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.iastate.edu;https://www.umn.edu;https://www.usc.edu",
        "aff_unique_abbr": "Stanford;ISU;UMN;USC",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Stanford;;Los Angeles",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Partition-wise Linear Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4721",
        "id": "4721",
        "author_site": "Hidekazu Oiwa, Ryohei Fujimaki",
        "author": "Hidekazu Oiwa; Ryohei Fujimaki",
        "abstract": "Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partition-specific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models.",
        "bibtex": "@inproceedings{NIPS2014_e3aad2c9,\n author = {Oiwa, Hidekazu and Fujimaki, Ryohei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Partition-wise Linear Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e3aad2c9562a542eaa2b2c81babee37e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e3aad2c9562a542eaa2b2c81babee37e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e3aad2c9562a542eaa2b2c81babee37e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e3aad2c9562a542eaa2b2c81babee37e-Reviews.html",
        "metareview": "",
        "pdf_size": 742900,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12263593022535415525&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Graduate School of Information Science and Technology, The University of Tokyo; NEC Laboratories America",
        "aff_domain": "gmail.com;nec-labs.com",
        "email": "gmail.com;nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e3aad2c9562a542eaa2b2c81babee37e-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Tokyo;NEC Laboratories America",
        "aff_unique_dep": "Graduate School of Information Science and Technology;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.nec-labs.com",
        "aff_unique_abbr": "UTokyo;NEC Labs America",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Tokyo;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "title": "Permutation Diffusion Maps (PDM) with Application to the Image Association Problem in Computer Vision",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4722",
        "id": "4722",
        "author_site": "Deepti Pachauri, Risi Kondor, Gautam Sargur, Vikas Singh",
        "author": "Deepti Pachauri; Risi Kondor; Gautam Sargur; Vikas Singh",
        "abstract": "Consistently matching keypoints across images, and the related problem of finding clusters of nearby images, are critical components of various tasks in Computer Vision, including Structure from Motion (SfM). Unfortunately, occlusion and large repetitive structures tend to mislead most currently used matching algorithms, leading to characteristic pathologies in the final output. In this paper we introduce a new method, Permutations Diffusion Maps (PDM), to solve the matching problem, as well as a related new affinity measure, derived using ideas from harmonic analysis on the symmetric group. We show that just by using it as a preprocessing step to existing SfM pipelines, PDM can greatly improve reconstruction quality on difficult datasets.",
        "bibtex": "@inproceedings{NIPS2014_25283972,\n author = {Pachauri, Deepti and Kondor, Risi and Sargur, Gautam and Singh, Vikas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Permutation Diffusion Maps (PDM) with Application to the Image Association Problem in Computer Vision},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/252839721e444cb4a8e15ceaa9a8776f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/252839721e444cb4a8e15ceaa9a8776f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/252839721e444cb4a8e15ceaa9a8776f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/252839721e444cb4a8e15ceaa9a8776f-Reviews.html",
        "metareview": "",
        "pdf_size": 1660450,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6953495024269333357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Sciences, University of Wisconsin\u2013Madison; Dept. of Computer Science and Dept. of Statistics, The University of Chicago; Dept. of Computer Sciences, University of Wisconsin\u2013Madison; Dept. of Biostatistics & Medical Informatics, University of Wisconsin\u2013Madison",
        "aff_domain": "cs.wisc.edu;uchicago.edu;cs.wisc.edu;biostat.wisc.edu",
        "email": "cs.wisc.edu;uchicago.edu;cs.wisc.edu;biostat.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/252839721e444cb4a8e15ceaa9a8776f-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Wisconsin\u2013Madison;University of Chicago",
        "aff_unique_dep": "Department of Computer Sciences;Dept. of Computer Science",
        "aff_unique_url": "https://www.wisc.edu;https://www.uchicago.edu",
        "aff_unique_abbr": "UW\u2013Madison;UChicago",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Poisson Process Jumping between an Unknown Number of Rates: Application to Neural Spike Data",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4426",
        "id": "4426",
        "author_site": "Florian Stimberg, Andreas Ruttor, Manfred Opper",
        "author": "Florian Stimberg; Andreas Ruttor; Manfred Opper",
        "abstract": "We introduce a model where the rate of an inhomogeneous Poisson process is modified by a Chinese restaurant process. Applying a MCMC sampler to this model allows us to do posterior Bayesian inference about the number of states in Poisson-like data. Our sampler is shown to get accurate results for synthetic data and we apply it to V1 neuron spike data to find discrete firing rate states depending on the orientation of a stimulus.",
        "bibtex": "@inproceedings{NIPS2014_7d94c60b,\n author = {Stimberg, Florian and Ruttor, Andreas and Opper, Manfred},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Poisson Process Jumping between an Unknown Number of Rates: Application to Neural Spike Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7d94c60b41682333643e145cb19573cb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7d94c60b41682333643e145cb19573cb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7d94c60b41682333643e145cb19573cb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7d94c60b41682333643e145cb19573cb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7d94c60b41682333643e145cb19573cb-Reviews.html",
        "metareview": "",
        "pdf_size": 246943,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10952431070187526416&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science, TU Berlin; Computer Science, TU Berlin; Computer Science, TU Berlin",
        "aff_domain": "tu-berlin.de;tu-berlin.de;tu-berlin.de",
        "email": "tu-berlin.de;tu-berlin.de;tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7d94c60b41682333643e145cb19573cb-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www.tu-berlin.de",
        "aff_unique_abbr": "TU Berlin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Positive Curvature and Hamiltonian Monte Carlo",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4723",
        "id": "4723",
        "author_site": "Christof Seiler, Simon Rubinstein-Salzedo, Susan Holmes",
        "author": "Christof Seiler; Simon Rubinstein-Salzedo; Susan Holmes",
        "abstract": "The Jacobi metric introduced in mathematical physics can be used to analyze Hamiltonian Monte Carlo (HMC). In a geometrical setting, each step of HMC corresponds to a geodesic on a Riemannian manifold with a Jacobi metric. Our calculation of the sectional curvature of this HMC manifold allows us to see that it is positive in cases such as sampling from a high dimensional multivariate Gaussian. We show that positive curvature can be used to prove theoretical concentration results for HMC Markov chains.",
        "bibtex": "@inproceedings{NIPS2014_c76d3b26,\n author = {Seiler, Christof and Rubinstein-Salzedo, Simon and Holmes, Susan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Positive Curvature and Hamiltonian Monte Carlo},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c76d3b26eba4f2c2fed5695faaae774f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c76d3b26eba4f2c2fed5695faaae774f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c76d3b26eba4f2c2fed5695faaae774f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c76d3b26eba4f2c2fed5695faaae774f-Reviews.html",
        "metareview": "",
        "pdf_size": 308249,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13759049184168443958&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics; Department of Statistics; Department of Statistics",
        "aff_domain": "stanford.edu;stanford.edu;stat.stanford.edu",
        "email": "stanford.edu;stanford.edu;stat.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c76d3b26eba4f2c2fed5695faaae774f-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University Affiliation Not Specified",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Pre-training of Recurrent Neural Networks via Linear Autoencoders",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4724",
        "id": "4724",
        "author_site": "Luca Pasa, Alessandro Sperduti",
        "author": "Luca Pasa; Alessandro Sperduti",
        "abstract": "We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the input-to-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.",
        "bibtex": "@inproceedings{NIPS2014_7ed42d78,\n author = {Pasa, Luca and Sperduti, Alessandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Pre-training of Recurrent Neural Networks via Linear Autoencoders},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7ed42d78fca4d0ab86a4ad1ca0b5f266-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7ed42d78fca4d0ab86a4ad1ca0b5f266-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7ed42d78fca4d0ab86a4ad1ca0b5f266-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7ed42d78fca4d0ab86a4ad1ca0b5f266-Reviews.html",
        "metareview": "",
        "pdf_size": 1579672,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11703165863807314736&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematics, University of Padova, Italy; Department of Mathematics, University of Padova, Italy",
        "aff_domain": "math.unipd.it;math.unipd.it",
        "email": "math.unipd.it;math.unipd.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7ed42d78fca4d0ab86a4ad1ca0b5f266-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Padova",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.unipd.it",
        "aff_unique_abbr": "UP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy"
    },
    {
        "title": "Predicting Useful Neighborhoods for Lazy Local Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4725",
        "id": "4725",
        "author_site": "Aron Yu, Kristen Grauman",
        "author": "Aron Yu; Kristen Grauman",
        "abstract": "Lazy local learning methods train a classifier on the fly\" at test time, using only a subset of the training instances that are most relevant to the novel test example. The goal is to tailor the classifier to the properties of the data surrounding the test example. Existing methods assume that the instances most useful for building the local model are strictly those closest to the test example. However, this fails to account for the fact that the success of the resulting classifier depends on the full distribution of selected training instances. Rather than simply gather the test example's nearest neighbors, we propose to predict the subset of training data that is jointly relevant to training its local model. We develop an approach to discover patterns between queries and their \"good\" neighborhoods using large-scale multi-label classification with compressed sensing. Given a novel test point, we estimate both the composition and size of the training subset likely to yield an accurate local model. We demonstrate the approach on image classification tasks on SUN and aPascal and show it outperforms traditional global and local approaches.\"",
        "bibtex": "@inproceedings{NIPS2014_d949dac6,\n author = {Yu, Aron and Grauman, Kristen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting Useful Neighborhoods for Lazy Local Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d949dac632d5051120b0362207d92bfa-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d949dac632d5051120b0362207d92bfa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d949dac632d5051120b0362207d92bfa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d949dac632d5051120b0362207d92bfa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d949dac632d5051120b0362207d92bfa-Reviews.html",
        "metareview": "",
        "pdf_size": 2191417,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15980327620430529613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Texas at Austin; University of Texas at Austin",
        "aff_domain": "utexas.edu;cs.utexas.edu",
        "email": "utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d949dac632d5051120b0362207d92bfa-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4428",
        "id": "4428",
        "author_site": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Matthew Hoffman, Zoubin Ghahramani",
        "author": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato; Matthew W. Hoffman; Zoubin Ghahramani",
        "abstract": "We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.",
        "bibtex": "@inproceedings{NIPS2014_6488484c,\n author = {Hern\\'{a}ndez-Lobato, Jos\\'{e} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predictive Entropy Search for Efficient Global Optimization of Black-box Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6488484c982e9af5c35689523ba1abfe-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/6488484c982e9af5c35689523ba1abfe-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/6488484c982e9af5c35689523ba1abfe-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/6488484c982e9af5c35689523ba1abfe-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/6488484c982e9af5c35689523ba1abfe-Reviews.html",
        "metareview": "",
        "pdf_size": 716860,
        "gs_citation": 892,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13320162437836414042&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "University of Cambridge; University of Cambridge; University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/6488484c982e9af5c35689523ba1abfe-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Probabilistic Differential Dynamic Programming",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4727",
        "id": "4727",
        "author_site": "Yunpeng Pan, Evangelos Theodorou",
        "author": "Yunpeng Pan; Evangelos A. Theodorou",
        "abstract": "We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradient-based policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.",
        "bibtex": "@inproceedings{NIPS2014_b1aca8ec,\n author = {Pan, Yunpeng and Theodorou, Evangelos A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Differential Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b1aca8ec83459c8900d531aeee5caa08-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b1aca8ec83459c8900d531aeee5caa08-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b1aca8ec83459c8900d531aeee5caa08-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b1aca8ec83459c8900d531aeee5caa08-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b1aca8ec83459c8900d531aeee5caa08-Reviews.html",
        "metareview": "",
        "pdf_size": 416666,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2009315424684758013&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Daniel Guggenheim School of Aerospace Engineering + Institute for Robotics and Intelligent Machines, Georgia Institute of Technology; Daniel Guggenheim School of Aerospace Engineering + Institute for Robotics and Intelligent Machines, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;ae.gatech.edu",
        "email": "gatech.edu;ae.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b1aca8ec83459c8900d531aeee5caa08-Abstract.html",
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "Daniel Guggenheim School of Aerospace Engineering",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Atlanta",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Probabilistic ODE Solvers with Runge-Kutta Means",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4342",
        "id": "4342",
        "author_site": "Michael Schober, David Duvenaud, Philipp Hennig",
        "author": "Michael Schober; David Duvenaud; Philipp Hennig",
        "abstract": "Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.",
        "bibtex": "@inproceedings{NIPS2014_fa042d3a,\n author = {Schober, Michael and Duvenaud, David and Hennig, Philipp},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic ODE Solvers with Runge-Kutta Means},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/fa042d3ac049f0ae4b2550d956db4d3b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/fa042d3ac049f0ae4b2550d956db4d3b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/fa042d3ac049f0ae4b2550d956db4d3b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/fa042d3ac049f0ae4b2550d956db4d3b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/fa042d3ac049f0ae4b2550d956db4d3b-Reviews.html",
        "metareview": "",
        "pdf_size": 634936,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16932418719344109467&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "MPI for Intelligent Systems, T\u00fcbingen, Germany; Department of Engineering, Cambridge University; MPI for Intelligent Systems, T\u00fcbingen, Germany",
        "aff_domain": "tue.mpg.de;cam.ac.uk;tue.mpg.de",
        "email": "tue.mpg.de;cam.ac.uk;tue.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/fa042d3ac049f0ae4b2550d956db4d3b-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;University of Cambridge",
        "aff_unique_dep": ";Department of Engineering",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://www.cam.ac.uk",
        "aff_unique_abbr": "MPI-IS;Cambridge",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "T\u00fcbingen;Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "title": "Probabilistic low-rank matrix completion on finite alphabets",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4728",
        "id": "4728",
        "author_site": "Jean Lafond, Olga Klopp, Eric Moulines, Joseph Salmon",
        "author": "Jean Lafond; Olga Klopp; \u00c9ric Moulines; Joseph Salmon",
        "abstract": "The task of reconstructing a matrix given a sample of observed entries is known as the \\emph{matrix completion problem}. Such a consideration arises in a wide variety of problems, including recommender systems, collaborative filtering, dimensionality reduction, image processing, quantum physics or multi-class classification to name a few. Most works have focused on recovering an unknown real-valued low-rank matrix from randomly sub-sampling its entries. Here, we investigate the case where the observations take a finite numbers of values, corresponding for examples to ratings in recommender systems or labels in multi-class classification. We also consider a general sampling scheme (non-necessarily uniform) over the matrix entries. The performance of a nuclear-norm penalized estimator is analyzed theoretically. More precisely, we derive bounds for the Kullback-Leibler divergence between the true and estimated distributions. In practice, we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tackle potentially high dimensional settings.",
        "bibtex": "@inproceedings{NIPS2014_17ac4eb3,\n author = {Lafond, Jean and Klopp, Olga and Moulines, \\'{E}ric and Salmon, Joseph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic low-rank matrix completion on finite alphabets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/17ac4eb332d6ac6956ea2e835464e03b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/17ac4eb332d6ac6956ea2e835464e03b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/17ac4eb332d6ac6956ea2e835464e03b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/17ac4eb332d6ac6956ea2e835464e03b-Reviews.html",
        "metareview": "",
        "pdf_size": 286485,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10960147102470519630&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Institut Mines-T\u00e9l\u00e9com + T\u00e9l\u00e9com ParisTech + CNRS LTCI; CREST et MODAL\u2019X + Universit\u00e9 Paris Ouest; Institut Mines-T\u00e9l\u00e9com + T\u00e9l\u00e9com ParisTech + CNRS LTCI; Institut Mines-T\u00e9l\u00e9com + T\u00e9l\u00e9com ParisTech + CNRS LTCI",
        "aff_domain": "telecom-paristech.fr;math.cnrs.fr;telecom-paristech.fr;telecom-paristech.fr",
        "email": "telecom-paristech.fr;math.cnrs.fr;telecom-paristech.fr;telecom-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/17ac4eb332d6ac6956ea2e835464e03b-Abstract.html",
        "aff_unique_index": "0+1+2;3+4;0+1+2;0+1+2",
        "aff_unique_norm": "Institut Mines-T\u00e9l\u00e9com;T\u00e9l\u00e9com ParisTech;CNRS;CREST;Universit\u00e9 Paris Ouest Nanterre La D\u00e9fense",
        "aff_unique_dep": ";;Laboratoire Traitement du signal et des images;;",
        "aff_unique_url": "https://www.imt.fr;https://www.telecom-paristech.fr;https://www.ltci.cnrs.fr;https://www.crest.fr;https://www.u-Paris.fr",
        "aff_unique_abbr": "IMT;TP;LTCI;;UPON",
        "aff_campus_unique_index": ";1;;",
        "aff_campus_unique": ";Nanterre La D\u00e9fense",
        "aff_country_unique_index": "0+0+0;0+0;0+0+0;0+0+0",
        "aff_country_unique": "France"
    },
    {
        "title": "Projecting Markov Random Field Parameters for Fast Mixing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4729",
        "id": "4729",
        "author_site": "Xianghang Liu, Justin Domke",
        "author": "Xianghang Liu; Justin Domke",
        "abstract": "Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense. Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing of univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters.",
        "bibtex": "@inproceedings{NIPS2014_7a2fdc79,\n author = {Liu, Xianghang and Domke, Justin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Projecting Markov Random Field Parameters for Fast Mixing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7a2fdc79835470389db54062c031889d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7a2fdc79835470389db54062c031889d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7a2fdc79835470389db54062c031889d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7a2fdc79835470389db54062c031889d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7a2fdc79835470389db54062c031889d-Reviews.html",
        "metareview": "",
        "pdf_size": 772267,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16691303804773185975&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "NICTA, The University of New South Wales; NICTA, The Australian National University",
        "aff_domain": "nicta.com.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7a2fdc79835470389db54062c031889d-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of New South Wales;Australian National University",
        "aff_unique_dep": "NICTA;NICTA",
        "aff_unique_url": "https://www.unsw.edu.au;https://www.anu.edu.au",
        "aff_unique_abbr": "UNSW;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Projective dictionary pair learning for pattern classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4730",
        "id": "4730",
        "author_site": "Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng",
        "author": "Shuhang Gu; Lei Zhang; Wangmeng Zuo; Xiangchu Feng",
        "abstract": "Discriminative dictionary learning (DL) has been widely studied in various pattern classification problems. Most of the existing DL methods aim to learn a synthesis dictionary to represent the input signal while enforcing the representation coefficients and/or representation residual to be discriminative. However, the $\\ell_0$ or $\\ell_1$-norm sparsity constraint on the representation coefficients adopted in many DL methods makes the training and testing phases time consuming. We propose a new discriminative DL framework, namely projective dictionary pair learning (DPL), which learns a synthesis dictionary and an analysis dictionary jointly to achieve the goal of signal representation and discrimination. Compared with conventional DL methods, the proposed DPL method can not only greatly reduce the time complexity in the training and testing phases, but also lead to very competitive accuracies in a variety of visual classification tasks.",
        "bibtex": "@inproceedings{NIPS2014_7b744eaa,\n author = {Gu, Shuhang and Zhang, Lei and Zuo, Wangmeng and Feng, Xiangchu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Projective dictionary pair learning for pattern classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7b744eaa9b5d7db1348bc0a89eef37f8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7b744eaa9b5d7db1348bc0a89eef37f8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7b744eaa9b5d7db1348bc0a89eef37f8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7b744eaa9b5d7db1348bc0a89eef37f8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7b744eaa9b5d7db1348bc0a89eef37f8-Reviews.html",
        "metareview": "",
        "pdf_size": 1031690,
        "gs_citation": 378,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17122788648782890409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Dept. of Applied Mathematics, Xidian University, Xi'an, China",
        "aff_domain": "comp.polyu.edu.hk;comp.polyu.edu.hk;gmail.com;mail.xidian.edu.cn",
        "email": "comp.polyu.edu.hk;comp.polyu.edu.hk;gmail.com;mail.xidian.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7b744eaa9b5d7db1348bc0a89eef37f8-Abstract.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Hong Kong Polytechnic University;Harbin Institute of Technology;Xidian University",
        "aff_unique_dep": "Dept. of Computing;School of Computer Science and Technology;Dept. of Applied Mathematics",
        "aff_unique_url": "https://www.polyu.edu.hk;http://www.hit.edu.cn/;http://www.xidian.edu.cn",
        "aff_unique_abbr": "PolyU;HIT;Xidian",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Hong Kong;Harbin;Xi'an",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Provable Submodular Minimization using Wolfe's Algorithm",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4344",
        "id": "4344",
        "author_site": "Deeparnab Chakrabarty, Prateek Jain, Pravesh Kothari",
        "author": "Deeparnab Chakrabarty; Prateek Jain; Pravesh Kothari",
        "abstract": "Owing to several applications in large scale learning and vision problems, fast submodular function minimization (SFM) has become a critical problem. Theoretically, unconstrained SFM can be performed in polynomial time (Iwata and Orlin 2009), however these algorithms are not practical. In 1976, Wolfe proposed an algorithm to find the minimum Euclidean norm point in a polytope, and in 1980, Fujishige showed how Wolfe's algorithm can be used for SFM. For general submodular functions, the Fujishige-Wolfe minimum norm algorithm seems to have the best empirical performance. Despite its good practical performance, theoretically very little is known about Wolfe's minimum norm algorithm -- to our knowledge the only result is an exponential time analysis due to Wolfe himself. In this paper we give a maiden convergence analysis of Wolfe's algorithm. We prove that in t iterations, Wolfe's algorithm returns a O(1/t)-approximate solution to the min-norm point. We also prove a robust version of Fujishige's theorem which shows that an O(1/n^2)-approximate solution to the min-norm point problem implies exact submodular minimization. As a corollary, we get the first pseudo-polynomial time guarantee for the Fujishige-Wolfe minimum norm algorithm for submodular function minimization. In particular, we show that the min-norm point algorithm solves SFM in O(n^7F^2)-time, where $F$ is an upper bound on the maximum change a single element can cause in the function value.",
        "bibtex": "@inproceedings{NIPS2014_1dc93987,\n author = {Chakrabarty, Deeparnab and Jain, Prateek and Kothari, Pravesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Provable Submodular Minimization using Wolfe\\textquotesingle s Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1dc9398707356a25bbcf61f7b3aa682e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1dc9398707356a25bbcf61f7b3aa682e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1dc9398707356a25bbcf61f7b3aa682e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1dc9398707356a25bbcf61f7b3aa682e-Reviews.html",
        "metareview": "",
        "pdf_size": 339104,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8801113657366370556&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft Research; Microsoft Research; University of Texas at Austin + Microsoft Research",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1dc9398707356a25bbcf61f7b3aa682e-Abstract.html",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "Microsoft;University of Texas at Austin",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.utexas.edu",
        "aff_unique_abbr": "MSR;UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Provable Tensor Factorization with Missing Data",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4731",
        "id": "4731",
        "author_site": "Prateek Jain, Sewoong Oh",
        "author": "Prateek Jain; Sewoong Oh",
        "abstract": "We study the problem of low-rank tensor factorization in the presence of missing data. We ask the following question: how many sampled entries do we need, to efficiently and exactly reconstruct a tensor with a low-rank orthogonal decomposition? We propose a novel alternating minimization based method which iteratively refines estimates of the singular vectors. We show that under certain standard assumptions, our method can recover a three-mode $n\\times n\\times n$ dimensional rank-$r$ tensor exactly from $O(n^{3/2} r^5 \\log^4 n)$ randomly sampled entries. In the process of proving this result, we solve two challenging sub-problems for tensors with missing data. First, in analyzing the initialization step, we prove a generalization of a celebrated result by Szemer\\'edie et al. on the spectrum of random graphs. Next, we prove global convergence of alternating minimization with a good initialization. Simulations suggest that the dependence of the sample size on dimensionality $n$ is indeed tight.",
        "bibtex": "@inproceedings{NIPS2014_d04d4a15,\n author = {Jain, Prateek and Oh, Sewoong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Provable Tensor Factorization with Missing Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d04d4a1518efb7b7b80c8352d567ecd5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d04d4a1518efb7b7b80c8352d567ecd5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d04d4a1518efb7b7b80c8352d567ecd5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d04d4a1518efb7b7b80c8352d567ecd5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d04d4a1518efb7b7b80c8352d567ecd5-Reviews.html",
        "metareview": "",
        "pdf_size": 339563,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17455578242205864210&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Microsoft Research, Bangalore, India; Dept. of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801",
        "aff_domain": "microsoft.com;illinois.edu",
        "email": "microsoft.com;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d04d4a1518efb7b7b80c8352d567ecd5-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Microsoft Research;Dept. of Industrial and Enterprise Systems Engineering",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india;https://illinois.edu",
        "aff_unique_abbr": "MSR;UIUC",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Bangalore;Urbana",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "Proximal Quasi-Newton for Computationally Intensive L1-regularized M-estimators",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4732",
        "id": "4732",
        "author_site": "Kai Zhong, Ian En-Hsu Yen, Inderjit Dhillon, Pradeep Ravikumar",
        "author": "Kai Zhong; Ian E.H. Yen; Inderjit S. Dhillon; Pradeep Ravikumar",
        "abstract": "We consider the class of optimization problems arising from computationally intensive L1-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the L1-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. L1-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that our proximal quasi-Newton algorithm is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.",
        "bibtex": "@inproceedings{NIPS2014_9815cdc2,\n author = {Zhong, Kai and Yen, Ian E.H. and Dhillon, Inderjit S. and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Proximal Quasi-Newton for Computationally Intensive L1-regularized M-estimators},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9815cdc2dc7a2a0e0b04cddae879c075-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9815cdc2dc7a2a0e0b04cddae879c075-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/9815cdc2dc7a2a0e0b04cddae879c075-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9815cdc2dc7a2a0e0b04cddae879c075-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9815cdc2dc7a2a0e0b04cddae879c075-Reviews.html",
        "metareview": "",
        "pdf_size": 813540,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9563994323203713758&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Institute for Computational Engineering & Sciences; Department of Computer Science; Department of Computer Science; Department of Computer Science",
        "aff_domain": "ices.utexas.edu;cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "ices.utexas.edu;cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9815cdc2dc7a2a0e0b04cddae879c075-Abstract.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Texas at Austin;Unknown Institution",
        "aff_unique_dep": "Institute for Computational Engineering & Sciences;Department of Computer Science",
        "aff_unique_url": "https://ices.utexas.edu;",
        "aff_unique_abbr": "ICES;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "QUIC & DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4733",
        "id": "4733",
        "author_site": "Cho-Jui Hsieh, Inderjit Dhillon, Pradeep Ravikumar, Stephen Becker, Peder A Olsen",
        "author": "Cho-Jui Hsieh; Inderjit S. Dhillon; Pradeep Ravikumar; Stephen Becker; Peder A. Olsen",
        "abstract": "In this paper, we develop a family of algorithms for optimizing superposition-structured\u201d or \u201cdirty\u201d statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization. Most of the current approaches are first-order methods, including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm. Empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.\"",
        "bibtex": "@inproceedings{NIPS2014_377a6f50,\n author = {Hsieh, Cho-Jui and Dhillon, Inderjit S. and Ravikumar, Pradeep and Becker, Stephen and Olsen, Peder A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {QUIC \\&amp; DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/377a6f507bc67aaac04a0eafca076ea2-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/377a6f507bc67aaac04a0eafca076ea2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/377a6f507bc67aaac04a0eafca076ea2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/377a6f507bc67aaac04a0eafca076ea2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/377a6f507bc67aaac04a0eafca076ea2-Reviews.html",
        "metareview": "",
        "pdf_size": 240609,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3816256607515560146&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University of Texas at Austin; University of Texas at Austin; University of Texas at Austin; University of Colorado at Boulder; IBM T.J. Watson Research Center",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;colorado.edu;us.ibm.com",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;colorado.edu;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/377a6f507bc67aaac04a0eafca076ea2-Abstract.html",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "University of Texas at Austin;University of Colorado;IBM",
        "aff_unique_dep": ";;Research Center",
        "aff_unique_url": "https://www.utexas.edu;https://www.colorado.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "UT Austin;CU;IBM",
        "aff_campus_unique_index": "0;0;0;1;2",
        "aff_campus_unique": "Austin;Boulder;T.J. Watson",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Quantized Estimation of Gaussian Sequence Models in Euclidean Balls",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4734",
        "id": "4734",
        "author_site": "Yuancheng Zhu, John Lafferty",
        "author": "Yuancheng Zhu; John Lafferty",
        "abstract": "A central result in statistical theory is Pinsker's theorem, which characterizes the minimax rate in the normal means model of nonparametric estimation. In this paper, we present an extension to Pinsker's theorem where estimation is carried out under storage or communication constraints. In particular, we place limits on the number of bits used to encode an estimator, and analyze the excess risk in terms of this constraint, the signal size, and the noise level. We give sharp upper and lower bounds for the case of a Euclidean ball, which establishes the Pareto-optimal minimax tradeoff between storage and risk in this setting.",
        "bibtex": "@inproceedings{NIPS2014_5581e94a,\n author = {Zhu, Yuancheng and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Quantized Estimation of Gaussian Sequence Models in Euclidean Balls},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5581e94a205dd5f0d5ed21cd508ea9b9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5581e94a205dd5f0d5ed21cd508ea9b9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5581e94a205dd5f0d5ed21cd508ea9b9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5581e94a205dd5f0d5ed21cd508ea9b9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5581e94a205dd5f0d5ed21cd508ea9b9-Reviews.html",
        "metareview": "",
        "pdf_size": 314353,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5807523031345087551&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, University of Chicago; Department of Statistics, University of Chicago",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5581e94a205dd5f0d5ed21cd508ea9b9-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Chicago",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.uchicago.edu",
        "aff_unique_abbr": "UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Quantized Kernel Learning for Feature Matching",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4735",
        "id": "4735",
        "author_site": "Danfeng Qin, Xuanli Chen, Matthieu Guillaumin, Luc V Gool",
        "author": "Danfeng Qin; Xuanli Chen; Matthieu Guillaumin; Luc Van Gool",
        "abstract": "Matching local visual features is a crucial problem in computer vision and its accuracy greatly depends on the choice of similarity measure. As it is generally very difficult to design by hand a similarity or a kernel perfectly adapted to the data of interest, learning it automatically with as few assumptions as possible is preferable. However, available techniques for kernel learning suffer from several limitations, such as restrictive parametrization or scalability. In this paper, we introduce a simple and flexible family of non-linear kernels which we refer to as Quantized Kernels (QK). QKs are arbitrary kernels in the index space of a data quantizer, i.e., piecewise constant similarities in the original feature space. Quantization allows to compress features and keep the learning tractable. As a result, we obtain state-of-the-art matching performance on a standard benchmark dataset with just a few bits to represent each feature dimension. QKs also have explicit non-linear, low-dimensional feature mappings that grant access to Euclidean geometry for uncompressed features.",
        "bibtex": "@inproceedings{NIPS2014_411e39b1,\n author = {Qin, Danfeng and Chen, Xuanli and Guillaumin, Matthieu and Van Gool, Luc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Quantized Kernel Learning for Feature Matching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/411e39b117e885341f25efb8912945f7-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/411e39b117e885341f25efb8912945f7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/411e39b117e885341f25efb8912945f7-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/411e39b117e885341f25efb8912945f7-Reviews.html",
        "metareview": "",
        "pdf_size": 1528932,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10198342828417807644&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "ETH Z\u00fcrich; TU Munich; ETH Z\u00fcrich; ETH Z\u00fcrich",
        "aff_domain": "vision.ee.ethz.ch;tum.de;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;tum.de;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/411e39b117e885341f25efb8912945f7-Abstract.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "ETH Zurich;Technical University of Munich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.tum.de",
        "aff_unique_abbr": "ETHZ;TUM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "title": "RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4430",
        "id": "4430",
        "author_site": "Marek Petrik, Dharmashankar Subramanian",
        "author": "Marek Petrik; Dharmashankar Subramanian",
        "abstract": "We describe how to use robust Markov decision processes for value function approximation with state aggregation. The robustness serves to reduce the sensitivity to the approximation error of sub-optimal policies in comparison to classical methods such as fitted value iteration. This results in reducing the bounds on the gamma-discounted infinite horizon performance loss by a factor of 1/(1-gamma) while preserving polynomial-time computational complexity. Our experimental results show that using the robust representation can significantly improve the solution quality with minimal additional computational cost.",
        "bibtex": "@inproceedings{NIPS2014_10a0a617,\n author = {Petrik, Marek and Subramanian, Dharmashankar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/10a0a61756f0b41fad8270c03da9375d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/10a0a61756f0b41fad8270c03da9375d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/10a0a61756f0b41fad8270c03da9375d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/10a0a61756f0b41fad8270c03da9375d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/10a0a61756f0b41fad8270c03da9375d-Reviews.html",
        "metareview": "",
        "pdf_size": 604692,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2716683235921011935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "IBM T. J. Watson Research Center; IBM T. J. Watson Research Center",
        "aff_domain": "us.ibm.com;us.ibm.com",
        "email": "us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/10a0a61756f0b41fad8270c03da9375d-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T. J. Watson",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Randomized Experimental Design for Causal Graph Discovery",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4432",
        "id": "4432",
        "author_site": "Huining Hu, Zhentao Li, Adrian R Vetta",
        "author": "Huining Hu; Zhentao Li; Adrian Vetta",
        "abstract": "We examine the number of controlled experiments required to discover a causal graph. Hauser and Buhlmann showed that the number of experiments required is logarithmic in the cardinality of maximum undirected clique in the essential graph. Their lower bounds, however, assume that the experiment designer cannot use randomization in selecting the experiments. We show that significant improvements are possible with the aid of randomization \u2013 in an adversarial (worst-case) setting, the designer can then recover the causal graph using at most O(log log n) experiments in expectation. This bound cannot be improved; we show it is tight for some causal graphs. We then show that in a non-adversarial (average-case) setting, even larger improvements are possible: if the causal graph is chosen uniformly at random under a Erd\u00f6s-R\u00e9nyi model then the expected number of experiments to discover the causal graph is constant. Finally, we present computer simulations to complement our theoretic results. Our work exploits a structural characterization of essential graphs by Andersson et al. Their characterization is based upon a set of orientation forcing operations. Our results show a distinction between which forcing operations are most important in worst-case and average-case settings.",
        "bibtex": "@inproceedings{NIPS2014_8d2b9ed0,\n author = {Hu, Huining and Li, Zhentao and Vetta, Adrian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Randomized Experimental Design for Causal Graph Discovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8d2b9ed03b9f254b0f11c388573b7466-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8d2b9ed03b9f254b0f11c388573b7466-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8d2b9ed03b9f254b0f11c388573b7466-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8d2b9ed03b9f254b0f11c388573b7466-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8d2b9ed03b9f254b0f11c388573b7466-Reviews.html",
        "metareview": "",
        "pdf_size": 449836,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15104350834093252641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computer Science, McGill University; LIENS, \u00c9cole Normale Sup\u00e9rieure; Department of Mathematics and Statistics and School of Computer Science, McGill University",
        "aff_domain": "mail.mcgill.ca;ens.fr;math.mcgill.ca",
        "email": "mail.mcgill.ca;ens.fr;math.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8d2b9ed03b9f254b0f11c388573b7466-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "McGill University;\u00c9cole Normale Sup\u00e9rieure",
        "aff_unique_dep": "School of Computer Science;LIENS",
        "aff_unique_url": "https://www.mcgill.ca;https://www.ens.fr",
        "aff_unique_abbr": "McGill;ENS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Montreal;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;France"
    },
    {
        "title": "Ranking via Robust Binary Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4736",
        "id": "4736",
        "author_site": "Hyokun Yun, Parameswaran Raman, S. Vishwanathan",
        "author": "Hyokun Yun; Parameswaran Raman; S. V. N. Vishwanathan",
        "abstract": "We propose RoBiRank, a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification. The algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature. Further, in large scale problems where explicit feature vectors and scores are not given, our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 x 49,824,519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation.",
        "bibtex": "@inproceedings{NIPS2014_a9642528,\n author = {Yun, Hyokun and Raman, Parameswaran and Vishwanathan, S. V. N.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ranking via Robust Binary Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a9642528826e29a29fc6b0fd172d36a3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a9642528826e29a29fc6b0fd172d36a3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/a9642528826e29a29fc6b0fd172d36a3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a9642528826e29a29fc6b0fd172d36a3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a9642528826e29a29fc6b0fd172d36a3-Reviews.html",
        "metareview": "",
        "pdf_size": 637938,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5570896473486798001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Amazon; Department of Computer Science; Department of Computer Science",
        "aff_domain": "amazon.com;ucsc.edu;ucsc.edu",
        "email": "amazon.com;ucsc.edu;ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a9642528826e29a29fc6b0fd172d36a3-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Amazon;Unknown Institution",
        "aff_unique_dep": "Amazon.com, Inc.;Department of Computer Science",
        "aff_unique_url": "https://www.amazon.com;",
        "aff_unique_abbr": "Amazon;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Rates of Convergence for Nearest Neighbor Classification",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4434",
        "id": "4434",
        "author_site": "Kamalika Chaudhuri, Sanjoy Dasgupta",
        "author": "Kamalika Chaudhuri; Sanjoy Dasgupta",
        "abstract": "We analyze the behavior of nearest neighbor classification in metric spaces and provide finite-sample, distribution-dependent rates of convergence under minimal assumptions. These are more general than existing bounds, and enable us, as a by-product, to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known. We illustrate our upper and lower bounds by introducing a new smoothness class customized for nearest neighbor classification. We find, for instance, that under the Tsybakov margin condition the convergence rate of nearest neighbor matches recently established lower bounds for nonparametric classification.",
        "bibtex": "@inproceedings{NIPS2014_2b764b80,\n author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rates of Convergence for Nearest Neighbor Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2b764b803acec2d590f02b160f8a3700-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2b764b803acec2d590f02b160f8a3700-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/2b764b803acec2d590f02b160f8a3700-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2b764b803acec2d590f02b160f8a3700-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2b764b803acec2d590f02b160f8a3700-Reviews.html",
        "metareview": "",
        "pdf_size": 233615,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13712918867459417797&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science and Engineering, University of California, San Diego; Computer Science and Engineering, University of California, San Diego",
        "aff_domain": "cs.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2b764b803acec2d590f02b160f8a3700-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Real-Time Decoding of an Integrate and Fire Encoder",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4737",
        "id": "4737",
        "author_site": "Shreya Saxena, Munther Dahleh",
        "author": "Shreya Saxena; Munther Dahleh",
        "abstract": "Neuronal encoding models range from the detailed biophysically-based Hodgkin Huxley model, to the statistical linear time invariant model specifying firing rates in terms of the extrinsic signal. Decoding the former becomes intractable, while the latter does not adequately capture the nonlinearities present in the neuronal encoding system. For use in practical applications, we wish to record the output of neurons, namely spikes, and decode this signal fast in order to drive a machine, for example a prosthetic device. Here, we introduce a causal, real-time decoder of the biophysically-based Integrate and Fire encoding neuron model. We show that the upper bound of the real-time reconstruction error decreases polynomially in time, and that the L2 norm of the error is bounded by a constant that depends on the density of the spikes, as well as the bandwidth and the decay of the input signal. We numerically validate the effect of these parameters on the reconstruction error.",
        "bibtex": "@inproceedings{NIPS2014_29c4ed5d,\n author = {Saxena, Shreya and Dahleh, Munther},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Real-Time Decoding of an Integrate and Fire Encoder},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/29c4ed5dd426f7a4d854e7c209b9ac25-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/29c4ed5dd426f7a4d854e7c209b9ac25-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/29c4ed5dd426f7a4d854e7c209b9ac25-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/29c4ed5dd426f7a4d854e7c209b9ac25-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/29c4ed5dd426f7a4d854e7c209b9ac25-Reviews.html",
        "metareview": "",
        "pdf_size": 1145203,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3785545144074932967&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/29c4ed5dd426f7a4d854e7c209b9ac25-Abstract.html"
    },
    {
        "title": "Recovery of Coherent Data via Low-Rank Dictionary Pursuit",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4436",
        "id": "4436",
        "author_site": "Guangcan Liu, Ping Li",
        "author": "Guangcan Liu; Ping Li",
        "abstract": "The recently established RPCA method provides a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA is not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when data are strictly low-rank. This is because RPCA ignores clustering structures of the data which are ubiquitous in applications. As the number of cluster grows, the coherence of data keeps increasing, and accordingly, the recovery performance of RPCA degrades. We show that the challenges raised by coherent data (i.e., data with high coherence) could be alleviated by Low-Rank Representation (LRR)~\\cite{tpami",
        "bibtex": "@inproceedings{NIPS2014_2928cc8b,\n author = {Liu, Guangcan and Li, Ping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recovery of Coherent Data via Low-Rank Dictionary Pursuit},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2928cc8b05cf1bf9f7563cb005b1e37e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2928cc8b05cf1bf9f7563cb005b1e37e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2928cc8b05cf1bf9f7563cb005b1e37e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2928cc8b05cf1bf9f7563cb005b1e37e-Reviews.html",
        "metareview": "",
        "pdf_size": 189797,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1568687657977643299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistics and Biostatistics + Department of Computer Science, Rutgers University; Department of Statistics and Biostatistics + Department of Computer Science, Rutgers University",
        "aff_domain": "rutgers.edu;rutgers.edu",
        "email": "rutgers.edu;rutgers.edu",
        "github": "",
        "project": "arXiv:1404.4032",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2928cc8b05cf1bf9f7563cb005b1e37e-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of California, Berkeley;Rutgers University",
        "aff_unique_dep": "Department of Statistics and Biostatistics;Department of Computer Science",
        "aff_unique_url": "https://www.stat.berkeley.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "UC Berkeley;Rutgers",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Recurrent Models of Visual Attention",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4438",
        "id": "4438",
        "author_site": "Volodymyr Mnih, Nicolas Heess, Alex Graves, koray kavukcuoglu",
        "author": "Volodymyr Mnih; Nicolas Heess; Alex Graves; Koray Kavukcuoglu",
        "abstract": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.",
        "bibtex": "@inproceedings{NIPS2014_3e456b31,\n author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recurrent Models of Visual Attention},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3e456b31302cf8210edd4029292a40ad-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3e456b31302cf8210edd4029292a40ad-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3e456b31302cf8210edd4029292a40ad-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3e456b31302cf8210edd4029292a40ad-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3e456b31302cf8210edd4029292a40ad-Reviews.html",
        "metareview": "",
        "pdf_size": 337422,
        "gs_citation": 5160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4636836599580194602&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind",
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3e456b31302cf8210edd4029292a40ad-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Recursive Context Propagation Network for Semantic Scene Labeling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4738",
        "id": "4738",
        "author_site": "Abhishek Sharma, Oncel Tuzel, Ming-Yu Liu",
        "author": "Abhishek Sharma; Oncel Tuzel; Ming-Yu Liu",
        "abstract": "We propose a deep feed-forward neural network architecture for pixel-wise semantic scene labeling. It uses a novel recursive neural network architecture for context propagation, referred to as rCPN. It first maps the local visual features into a semantic space followed by a bottom-up aggregation of local information into a global representation of the entire image. Then a top-down propagation of the aggregated information takes place that enhances the contextual information of each local feature. Therefore, the information from every location in the image is propagated to every other location. Experimental results on Stanford background and SIFT Flow datasets show that the proposed method outperforms previous approaches. It is also orders of magnitude faster than previous methods and takes only 0.07 seconds on a GPU for pixel-wise labeling of a 256x256 image starting from raw RGB pixel values, given the super-pixel mask that takes an additional 0.3 seconds using an off-the-shelf implementation.",
        "bibtex": "@inproceedings{NIPS2014_97f33178,\n author = {Sharma, Abhishek and Tuzel, Oncel and Liu, Ming-Yu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recursive Context Propagation Network for Semantic Scene Labeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/97f3317808342cc15aa5a7d79108743c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/97f3317808342cc15aa5a7d79108743c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/97f3317808342cc15aa5a7d79108743c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/97f3317808342cc15aa5a7d79108743c-Reviews.html",
        "metareview": "",
        "pdf_size": 1426451,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11707005405276688383&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Maryland; Mitsubishi Electric Research Labs (MERL); Mitsubishi Electric Research Labs (MERL)",
        "aff_domain": "cs.umd.edu;merl.com;merl.com",
        "email": "cs.umd.edu;merl.com;merl.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/97f3317808342cc15aa5a7d79108743c-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Maryland;Mitsubishi Electric Research Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.merl.com",
        "aff_unique_abbr": "UMD;MERL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Recursive Inversion Models for Permutations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4739",
        "id": "4739",
        "author_site": "Christopher Meek, Marina Meila",
        "author": "Christopher Meek; Marina Meil\u0103",
        "abstract": "We develop a new exponential family probabilistic model for permutations that can capture hierarchical structure, and that has the well known Mallows and generalized Mallows models as subclasses. We describe how one can do parameter estimation and propose an approach to structure search for this class of models. We provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations.",
        "bibtex": "@inproceedings{NIPS2014_d157fbe3,\n author = {Meek, Christopher and Meil\\u{a}, Marina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recursive Inversion Models for Permutations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d157fbe354aeead90fe6287cbc4a04ca-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d157fbe354aeead90fe6287cbc4a04ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d157fbe354aeead90fe6287cbc4a04ca-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d157fbe354aeead90fe6287cbc4a04ca-Reviews.html",
        "metareview": "",
        "pdf_size": 596876,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6487773483210819308&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Microsoft Research, Redmond, Washington 98052; University of Washington, Seattle, Washington 98195",
        "aff_domain": "microsoft.com;stat.washington.edu",
        "email": "microsoft.com;stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d157fbe354aeead90fe6287cbc4a04ca-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;University of Washington",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.washington.edu",
        "aff_unique_abbr": "MSR;UW",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Redmond;Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Reducing the Rank in Relational Factorization Models by Including Observable Patterns",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4404",
        "id": "4404",
        "author_site": "Maximilian Nickel, Xueyan Jiang, Volker Tresp",
        "author": "Maximilian Nickel; Xueyan Jiang; Volker Tresp",
        "abstract": "Tensor factorizations have become popular methods for learning from multi-relational data. In this context, the rank of a factorization is an important parameter that determines runtime as well as generalization ability. To determine conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings, we propose a novel additive tensor factorization model for learning from latent and observable patterns in multi-relational data and present a scalable algorithm for computing the factorization. Experimentally, we show that the proposed approach does not only improve the predictive performance over pure latent variable methods but that it also reduces the required rank --- and therefore runtime and memory complexity --- significantly.",
        "bibtex": "@inproceedings{NIPS2014_e6c6717f,\n author = {Nickel, Maximilian and Jiang, Xueyan and Tresp, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reducing the Rank in Relational Factorization Models by Including Observable Patterns},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e6c6717ff809b4dba22d1cbfdc1c6696-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e6c6717ff809b4dba22d1cbfdc1c6696-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/e6c6717ff809b4dba22d1cbfdc1c6696-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e6c6717ff809b4dba22d1cbfdc1c6696-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e6c6717ff809b4dba22d1cbfdc1c6696-Reviews.html",
        "metareview": "",
        "pdf_size": 439229,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16630897116934110726&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "LCSL, Poggio Lab, Massachusetts Institute of Technology, Cambridge, MA, USA+Istituto Italiano di Tecnologia, Genova, Italy; Ludwig Maximilian University, Munich, Germany+Siemens AG, Corporate Technology, Munich, Germany; Ludwig Maximilian University, Munich, Germany+Siemens AG, Corporate Technology, Munich, Germany",
        "aff_domain": "mit.edu;siemens.com;siemens.com",
        "email": "mit.edu;siemens.com;siemens.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e6c6717ff809b4dba22d1cbfdc1c6696-Abstract.html",
        "aff_unique_index": "0+1;2+3;2+3",
        "aff_unique_norm": "Massachusetts Institute of Technology;Istituto Italiano di Tecnologia;Ludwig Maximilian University of Munich;Siemens AG",
        "aff_unique_dep": "Poggio Lab;;;Corporate Technology",
        "aff_unique_url": "https://web.mit.edu;https://www.iit.it;https://www.lmu.de;https://www.siemens.com",
        "aff_unique_abbr": "MIT;IIT;LMU;Siemens",
        "aff_campus_unique_index": "0+1;2+2;2+2",
        "aff_campus_unique": "Cambridge;Genova;Munich",
        "aff_country_unique_index": "0+1;2+2;2+2",
        "aff_country_unique": "United States;Italy;Germany"
    },
    {
        "title": "Repeated Contextual Auctions with Strategic Buyers",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4740",
        "id": "4740",
        "author_site": "Kareem Amin, Afshin Rostamizadeh, Umar Syed",
        "author": "Kareem Amin; Afshin Rostamizadeh; Umar Syed",
        "abstract": "Motivated by real-time advertising exchanges, we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer, where the former makes decisions myopically on every round, and the latter may strategically react to our algorithm, forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer\u2019s valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear (O(T^{2/3})) regret in the contextual setting against a surplus-maximizing buyer. We also extend this result to repeated second-price auctions with multiple buyers.",
        "bibtex": "@inproceedings{NIPS2014_6bbb4e56,\n author = {Amin, Kareem and Rostamizadeh, Afshin and Syed, Umar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Repeated Contextual Auctions with Strategic Buyers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6bbb4e56d4e1e0f21a8edbe97b64d537-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/6bbb4e56d4e1e0f21a8edbe97b64d537-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/6bbb4e56d4e1e0f21a8edbe97b64d537-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/6bbb4e56d4e1e0f21a8edbe97b64d537-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/6bbb4e56d4e1e0f21a8edbe97b64d537-Reviews.html",
        "metareview": "",
        "pdf_size": 530954,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14924410609654677655&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Pennsylvania; Google Research; Google Research",
        "aff_domain": "cis.upenn.edu;google.com;google.com",
        "email": "cis.upenn.edu;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/6bbb4e56d4e1e0f21a8edbe97b64d537-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Pennsylvania;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.upenn.edu;https://research.google",
        "aff_unique_abbr": "UPenn;Google Research",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Reputation-based Worker Filtering in Crowdsourcing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4741",
        "id": "4741",
        "author_site": "Srikanth Jagabathula, Lakshminarayanan Subramanian, Ashwin Venkataraman",
        "author": "Srikanth Jagabathula; Lakshminarayanan Subramanian; Ashwin Venkataraman",
        "abstract": "In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of {\\em adversarial} workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowdsourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of {\\em sophisticated} adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.",
        "bibtex": "@inproceedings{NIPS2014_d9280809,\n author = {Jagabathula, Srikanth and Subramanian, Lakshminarayanan and Venkataraman, Ashwin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reputation-based Worker Filtering in Crowdsourcing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d92808093d9ca3e318a58b6b4c7fc703-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d92808093d9ca3e318a58b6b4c7fc703-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d92808093d9ca3e318a58b6b4c7fc703-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d92808093d9ca3e318a58b6b4c7fc703-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d92808093d9ca3e318a58b6b4c7fc703-Reviews.html",
        "metareview": "",
        "pdf_size": 459671,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7859186525169553501&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of IOMS, NYU Stern School of Business; Department of Computer Science, New York University + CTED, New York University Abu Dhabi; Department of Computer Science, New York University + CTED, New York University Abu Dhabi",
        "aff_domain": "stern.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "email": "stern.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d92808093d9ca3e318a58b6b4c7fc703-Abstract.html",
        "aff_unique_index": "0;1+2;1+2",
        "aff_unique_norm": "New York University Stern School of Business;New York University;New York University Abu Dhabi",
        "aff_unique_dep": "Department of IOMS;Department of Computer Science;CTED",
        "aff_unique_url": "https://www.stern.nyu.edu;https://www.nyu.edu;https://nyuad.nyu.edu",
        "aff_unique_abbr": "NYU Stern;NYU;NYUAD",
        "aff_campus_unique_index": "0;0+1;0+1",
        "aff_campus_unique": "New York;Abu Dhabi",
        "aff_country_unique_index": "0;0+1;0+1",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "title": "Restricted Boltzmann machines modeling human choice",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4742",
        "id": "4742",
        "author_site": "Takayuki Osogami, Makoto Otsuka",
        "author": "Takayuki Osogami; Makoto Otsuka",
        "abstract": "We extend the multinomial logit model to represent some of the empirical phenomena that are frequently observed in the choices made by humans. These phenomena include the similarity effect, the attraction effect, and the compromise effect. We formally quantify the strength of these phenomena that can be represented by our choice model, which illuminates the flexibility of our choice model. We then show that our choice model can be represented as a restricted Boltzmann machine and that its parameters can be learned effectively from data. Our numerical experiments with real data of human choices suggest that we can train our choice model in such a way that it represents the typical phenomena of choice.",
        "bibtex": "@inproceedings{NIPS2014_8af87309,\n author = {Osogami, Takayuki and Otsuka, Makoto},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Restricted Boltzmann machines modeling human choice},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8af873095176650d4c5e738e35383498-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8af873095176650d4c5e738e35383498-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8af873095176650d4c5e738e35383498-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8af873095176650d4c5e738e35383498-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8af873095176650d4c5e738e35383498-Reviews.html",
        "metareview": "",
        "pdf_size": 412849,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2647786288209369524&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "IBM Research - Tokyo; IBM Research - Tokyo",
        "aff_domain": "jp.ibm.com;ucla.edu",
        "email": "jp.ibm.com;ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8af873095176650d4c5e738e35383498-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Robust Bayesian Max-Margin Clustering",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4743",
        "id": "4743",
        "author_site": "Changyou Chen, Jun Zhu, Xinhua Zhang",
        "author": "Changyou Chen; Jun Zhu; Xinhua Zhang",
        "abstract": "We present max-margin Bayesian clustering (BMC), a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models, as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.",
        "bibtex": "@inproceedings{NIPS2014_b82ec33e,\n author = {Chen, Changyou and Zhu, Jun and Zhang, Xinhua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Bayesian Max-Margin Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b82ec33e02fb70a089913737150e78cd-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b82ec33e02fb70a089913737150e78cd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b82ec33e02fb70a089913737150e78cd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b82ec33e02fb70a089913737150e78cd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b82ec33e02fb70a089913737150e78cd-Reviews.html",
        "metareview": "",
        "pdf_size": 763234,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12639909658256183515&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Dept. of Electrical and Computer Engineering, Duke University, Durham, NC, USA; State Key Lab of Intelligent Technology & Systems+Tsinghua National TNList Lab+Dept. of Computer Science & Tech., Tsinghua University, Beijing 100084, China; Australian National University (ANU) and National ICT Australia (NICTA), Canberra, Australia",
        "aff_domain": "gmail.com;tsinghua.edu.cn;anu.edu.au",
        "email": "gmail.com;tsinghua.edu.cn;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b82ec33e02fb70a089913737150e78cd-Abstract.html",
        "aff_unique_index": "0;1+2+2;3",
        "aff_unique_norm": "Duke University;State Key Lab of Intelligent Technology & Systems;Tsinghua University;Australian National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;;National TNList Lab;",
        "aff_unique_url": "https://www.duke.edu;;http://www.tsinghua.edu.cn;https://www.anu.edu.au",
        "aff_unique_abbr": "Duke;;Tsinghua;ANU",
        "aff_campus_unique_index": "0;2;3",
        "aff_campus_unique": "Durham;;Beijing;Canberra",
        "aff_country_unique_index": "0;1+1+1;2",
        "aff_country_unique": "United States;China;Australia"
    },
    {
        "title": "Robust Classification Under Sample Selection Bias",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4440",
        "id": "4440",
        "author_site": "Anqi Liu, Brian Ziebart",
        "author": "Anqi Liu; Brian D. Ziebart",
        "abstract": "In many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions. Due to its asymptotic properties, sample-reweighted loss minimization is a commonly employed technique to deal with this difference. However, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias. We develop a framework for robustly learning a probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation. Our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated. We demonstrate the behavior and effectiveness of our approach on synthetic and UCI binary classification tasks.",
        "bibtex": "@inproceedings{NIPS2014_3c5882a7,\n author = {Liu, Anqi and Ziebart, Brian D.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Classification Under Sample Selection Bias},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3c5882a72c072a68dc50d4091eae11df-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3c5882a72c072a68dc50d4091eae11df-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3c5882a72c072a68dc50d4091eae11df-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3c5882a72c072a68dc50d4091eae11df-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3c5882a72c072a68dc50d4091eae11df-Reviews.html",
        "metareview": "",
        "pdf_size": 565829,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15700064850030639937&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "uic.edu;uic.edu",
        "email": "uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3c5882a72c072a68dc50d4091eae11df-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4744",
        "id": "4744",
        "author_site": "Robert A Vandermeulen, Clayton Scott",
        "author": "Robert A. Vandermeulen; Clayton D. Scott",
        "abstract": "While robust parameter estimation has been well studied in parametric density estimation, there has been little investigation into robust density estimation in the nonparametric setting. We present a robust version of the popular kernel density estimator (KDE). As with other estimators, a robust version of the KDE is useful since sample contamination is a common issue with datasets. What ``robustness'' means for a nonparametric density estimate is not straightforward and is a topic we explore in this paper. To construct a robust KDE we scale the traditional KDE and project it to its nearest weighted KDE in the $L^2$ norm. Because the squared $L^2$ norm penalizes point-wise errors superlinearly this causes the weighted KDE to allocate more weight to high density regions. We demonstrate the robustness of the SPKDE with numerical experiments and a consistency result which shows that asymptotically the SPKDE recovers the uncontaminated density under sufficient conditions on the contamination.",
        "bibtex": "@inproceedings{NIPS2014_b4002b41,\n author = {Vandermeulen, Robert A. and Scott, Clayton D.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b4002b417fc4219045de977b02afe425-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b4002b417fc4219045de977b02afe425-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b4002b417fc4219045de977b02afe425-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b4002b417fc4219045de977b02afe425-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b4002b417fc4219045de977b02afe425-Reviews.html",
        "metareview": "",
        "pdf_size": 349198,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13719420859070054073&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of EECS, University of Michigan, Ann Arbor, MI 48109; Department of EECS, University of Michigan, Ann Arbor, MI 48109",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b4002b417fc4219045de977b02afe425-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of EECS",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Robust Logistic Regression and Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4745",
        "id": "4745",
        "author_site": "Jiashi Feng, Huan Xu, Shie Mannor, Shuicheng Yan",
        "author": "Jiashi Feng; Huan Xu; Shie Mannor; Shuicheng Yan",
        "abstract": "We consider logistic regression with arbitrary outliers in the covariate matrix. We propose a new robust logistic regression algorithm, called RoLR, that estimates the parameter through a simple linear programming procedure. We prove that RoLR is robust to a constant fraction of adversarial outliers. To the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees. Besides regression, we apply RoLR to solving binary classification problems where a fraction of training samples are corrupted.",
        "bibtex": "@inproceedings{NIPS2014_4fa05693,\n author = {Feng, Jiashi and Xu, Huan and Mannor, Shie and Yan, Shuicheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Logistic Regression and Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4fa05693882463941c910650ce5442c9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4fa05693882463941c910650ce5442c9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/4fa05693882463941c910650ce5442c9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4fa05693882463941c910650ce5442c9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4fa05693882463941c910650ce5442c9-Reviews.html",
        "metareview": "",
        "pdf_size": 310375,
        "gs_citation": 232,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17732994045884620808&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "EECS Department & ICSI, UC Berkeley; ME Department, National University of Singapore; EE Department, Technion; ECE Department, National University of Singapore",
        "aff_domain": "berkeley.edu;nus.edu.sg;ee.technion.ac.il;nus.edu.sg",
        "email": "berkeley.edu;nus.edu.sg;ee.technion.ac.il;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4fa05693882463941c910650ce5442c9-Abstract.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "University of California, Berkeley;National University of Singapore;Technion",
        "aff_unique_dep": "EECS Department;ME Department;EE Department",
        "aff_unique_url": "https://www.berkeley.edu;https://www.nus.edu.sg;https://www.technion.ac.il",
        "aff_unique_abbr": "UC Berkeley;NUS;Technion",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;1;2;1",
        "aff_country_unique": "United States;Singapore;Israel"
    },
    {
        "title": "Robust Tensor Decomposition with Gross Corruption",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4746",
        "id": "4746",
        "author_site": "Quanquan Gu, Huan Gui, Jiawei Han",
        "author": "Quanquan Gu; Huan Gui; Jiawei Han",
        "abstract": "In this paper, we study the statistical performance of robust tensor decomposition with gross corruption. The observations are noisy realization of the superposition of a low-rank tensor $\\mathcal{W}^*$ and an entrywise sparse corruption tensor $\\mathcal{V}^*$. Unlike conventional noise with bounded variance in previous convex tensor decomposition analysis, the magnitude of the gross corruption can be arbitrary large. We show that under certain conditions, the true low-rank tensor as well as the sparse corruption tensor can be recovered simultaneously. Our theory yields nonasymptotic Frobenius-norm estimation error bounds for each tensor separately. We show through numerical experiments that our theory can precisely predict the scaling behavior in practice.",
        "bibtex": "@inproceedings{NIPS2014_a35e3de3,\n author = {Gu, Quanquan and Gui, Huan and Han, Jiawei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Tensor Decomposition with Gross Corruption},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a35e3de33d5f2c599af18f2ac7db2ab0-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a35e3de33d5f2c599af18f2ac7db2ab0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a35e3de33d5f2c599af18f2ac7db2ab0-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a35e3de33d5f2c599af18f2ac7db2ab0-Reviews.html",
        "metareview": "",
        "pdf_size": 403360,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10556908042490023002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Operations Research and Financial Engineering, Princeton University; Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign",
        "aff_domain": "princeton.edu;illinois.edu;illinois.edu",
        "email": "princeton.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a35e3de33d5f2c599af18f2ac7db2ab0-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Princeton University;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Operations Research and Financial Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu;https://illinois.edu",
        "aff_unique_abbr": "Princeton;UIUC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Rounding-based Moves for Metric Labeling",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4747",
        "id": "4747",
        "author": "M. Pawan Kumar",
        "abstract": "Metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given metric distance function over the label set. Popular methods for solving metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move-making algorithms as special cases.",
        "bibtex": "@inproceedings{NIPS2014_7da7abb4,\n author = {Kumar, M. Pawan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rounding-based Moves for Metric Labeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7da7abb4ff4f64d4c6d9866091bda400-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7da7abb4ff4f64d4c6d9866091bda400-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7da7abb4ff4f64d4c6d9866091bda400-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7da7abb4ff4f64d4c6d9866091bda400-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7da7abb4ff4f64d4c6d9866091bda400-Reviews.html",
        "metareview": "",
        "pdf_size": 148310,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12944711142800256624&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Ecole Centrale Paris & INRIA Saclay",
        "aff_domain": "ecp.fr",
        "email": "ecp.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7da7abb4ff4f64d4c6d9866091bda400-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ecole Centrale Paris",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ecp.fr",
        "aff_unique_abbr": "ECP",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4748",
        "id": "4748",
        "author_site": "Aaron Defazio, Francis Bach, Simon Lacoste-Julien",
        "author": "Aaron Defazio; Francis Bach; Simon Lacoste-Julien",
        "abstract": "In this work we introduce a new fast incremental gradient method SAGA, in the spirit of SAG, SDCA, MISO and SVRG. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.",
        "bibtex": "@inproceedings{NIPS2014_93796419,\n author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/937964195d6fb3a55cd7cc578165f058-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/937964195d6fb3a55cd7cc578165f058-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/937964195d6fb3a55cd7cc578165f058-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/937964195d6fb3a55cd7cc578165f058-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/937964195d6fb3a55cd7cc578165f058-Reviews.html",
        "metareview": "",
        "pdf_size": 418089,
        "gs_citation": 2349,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15785520510117734095&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Ambiata\u2217 + Australian National University, Canberra; INRIA - Sierra Project-Team, \u00b4Ecole Normale Sup \u00b4erieure, Paris, France; INRIA - Sierra Project-Team, \u00b4Ecole Normale Sup \u00b4erieure, Paris, France",
        "aff_domain": "ambiata.com;inria.fr;inria.fr",
        "email": "ambiata.com;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/937964195d6fb3a55cd7cc578165f058-Abstract.html",
        "aff_unique_index": "0+1;2;2",
        "aff_unique_norm": "Ambiata;Australian National University;INRIA",
        "aff_unique_dep": ";;Sierra Project-Team",
        "aff_unique_url": "https://www.ambiata.com;https://www.anu.edu.au;https://www.inria.fr",
        "aff_unique_abbr": ";ANU;INRIA",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";Canberra;Paris",
        "aff_country_unique_index": "0+0;1;1",
        "aff_country_unique": "Australia;France"
    },
    {
        "title": "Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4749",
        "id": "4749",
        "author_site": "Tom Gunter, Michael A Osborne, Roman Garnett, Philipp Hennig, Stephen J Roberts",
        "author": "Tom Gunter; Michael A. Osborne; Roman Garnett; Philipp Hennig; Stephen J. Roberts",
        "abstract": "We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.",
        "bibtex": "@inproceedings{NIPS2014_a0d08267,\n author = {Gunter, Tom and Osborne, Michael A. and Garnett, Roman and Hennig, Philipp and Roberts, Stephen J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a0d08267a0fcee6970544a6d12286691-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a0d08267a0fcee6970544a6d12286691-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a0d08267a0fcee6970544a6d12286691-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a0d08267a0fcee6970544a6d12286691-Reviews.html",
        "metareview": "",
        "pdf_size": 1703415,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11000115737130913200&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Engineering Science, University of Oxford; Engineering Science, University of Oxford; Knowledge Discovery and Machine Learning, University of Bonn; MPI for Intelligent Systems, T\u00fcbingen, Germany; Engineering Science, University of Oxford",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;uni-bonn.de;tuebingen.mpg.de;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;uni-bonn.de;tuebingen.mpg.de;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a0d08267a0fcee6970544a6d12286691-Abstract.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of Oxford;University of Bonn;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Engineering Science;Department of Knowledge Discovery and Machine Learning;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.uni-bonn.de;https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "Oxford;;MPI-IS",
        "aff_campus_unique_index": "0;0;2;0",
        "aff_campus_unique": "Oxford;;T\u00fcbingen",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "title": "Scalable Inference for Neuronal Connectivity from Calcium Imaging",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4442",
        "id": "4442",
        "author_site": "Alyson Fletcher, Sundeep Rangan",
        "author": "Alyson K. Fletcher; Sundeep Rangan",
        "abstract": "Fluorescent calcium imaging provides a potentially powerful tool for inferring connectivity in neural circuits with up to thousands of neurons. However, a key challenge in using calcium imaging for connectivity detection is that current systems often have a temporal response and frame rate that can be orders of magnitude slower than the underlying neural spiking process. Bayesian inference based on expectation-maximization (EM) have been proposed to overcome these limitations, but they are often computationally demanding since the E-step in the EM procedure typically involves state estimation in a high-dimensional nonlinear dynamical system. In this work, we propose a computationally fast method for the state estimation based on a hybrid of loopy belief propagation and approximate message passing (AMP). The key insight is that a neural system as viewed through calcium imaging can be factorized into simple scalar dynamical systems for each neuron with linear interconnections between the neurons. Using the structure, the updates in the proposed hybrid AMP methodology can be computed by a set of one-dimensional state estimation procedures and linear transforms with the connectivity matrix. This yields a computationally scalable method for inferring connectivity of large neural circuits. Simulations of the method on realistic neural networks demonstrate good accuracy with computation times that are potentially significantly faster than current approaches based on Markov Chain Monte Carlo methods.",
        "bibtex": "@inproceedings{NIPS2014_b8cadf31,\n author = {Fletcher, Alyson K and Rangan, Sundeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Inference for Neuronal Connectivity from Calcium Imaging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b8cadf31aee4df42a53173b0843f5abf-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b8cadf31aee4df42a53173b0843f5abf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b8cadf31aee4df42a53173b0843f5abf-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b8cadf31aee4df42a53173b0843f5abf-Reviews.html",
        "metareview": "",
        "pdf_size": 719561,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10502461205688674178&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b8cadf31aee4df42a53173b0843f5abf-Abstract.html"
    },
    {
        "title": "Scalable Kernel Methods via Doubly Stochastic Gradients",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4750",
        "id": "4750",
        "author_site": "Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, Le Song",
        "author": "Bo Dai; Bo Xie; Niao He; Yingyu Liang; Anant Raj; Maria-Florina Balcan; Le Song",
        "abstract": "The general perception is that kernel methods are not scalable, so neural nets become the choice for large-scale nonlinear learning problems. Have we tried hard enough for kernel methods? In this paper, we propose an approach that scales up kernel methods using a novel concept called ``doubly stochastic functional gradients''. Based on the fact that many kernel methods can be expressed as convex optimization problems, our approach solves the optimization problems by making two unbiased stochastic approximations to the functional gradient---one using random training points and another using random features associated with the kernel---and performing descent steps with this noisy functional gradient. Our algorithm is simple, need no commit to a preset number of random features, and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting. We demonstrate that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel Hilbert space in rate O(1/t), and achieves a generalization bound of O(1/\\sqrt{t}). Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show competitive performances of our approach as compared to neural nets in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using convolution features.",
        "bibtex": "@inproceedings{NIPS2014_c6cc81e8,\n author = {Dai, Bo and Xie, Bo and He, Niao and Liang, Yingyu and Raj, Anant and Balcan, Maria-Florina and Song, Le},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Kernel Methods via Doubly Stochastic Gradients},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c6cc81e8589ebb6accf27b78afad82d9-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c6cc81e8589ebb6accf27b78afad82d9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c6cc81e8589ebb6accf27b78afad82d9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c6cc81e8589ebb6accf27b78afad82d9-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c6cc81e8589ebb6accf27b78afad82d9-Reviews.html",
        "metareview": "",
        "pdf_size": 373179,
        "gs_citation": 266,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1106322907072651522&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 25,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Princeton University+Carnegie Mellon University; Georgia Institute of Technology; Carnegie Mellon University; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;cs.princeton.edu;gatech.edu;cs.cmu.edu;cc.gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;cs.princeton.edu;gatech.edu;cs.cmu.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c6cc81e8589ebb6accf27b78afad82d9-Abstract.html",
        "aff_unique_index": "0;0;0;1+2;0;2;0",
        "aff_unique_norm": "Georgia Institute of Technology;Princeton University;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.gatech.edu;https://www.princeton.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Georgia Tech;Princeton;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scalable Methods for Nonnegative Matrix Factorizations of Near-separable Tall-and-skinny Matrices",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4444",
        "id": "4444",
        "author_site": "Austin Benson, Jason D Lee, Bartek Rajwa, David F Gleich",
        "author": "Austin R. Benson; Jason D. Lee; Bartek Rajwa; David F. Gleich",
        "abstract": "Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms scalable for data matrices that have many more rows than columns, so-called tall-and-skinny matrices.\" One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need to read the data matrix only once and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized matrices from scientific computing and bioinformatics.\"",
        "bibtex": "@inproceedings{NIPS2014_dc3db9bc,\n author = {Benson, Austin R. and Lee, Jason D. and Rajwa, Bartek and Gleich, David F.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Methods for Nonnegative Matrix Factorizations of Near-separable Tall-and-skinny Matrices},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/dc3db9bc30886b5c5dfffae313f088e5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/dc3db9bc30886b5c5dfffae313f088e5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/dc3db9bc30886b5c5dfffae313f088e5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/dc3db9bc30886b5c5dfffae313f088e5-Reviews.html",
        "metareview": "",
        "pdf_size": 370281,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4159365944387264996&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "ICME, Stanford University, Stanford, CA; ICME, Stanford University, Stanford, CA; Bindley Biosciences Center, Purdue University, West Lafeyette, IN; Computer Science Department, Purdue University, West Lafeyette, IN",
        "aff_domain": "stanford.edu;stanford.edu;purdue.edu;purdue.edu",
        "email": "stanford.edu;stanford.edu;purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/dc3db9bc30886b5c5dfffae313f088e5-Abstract.html",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Stanford University;Purdue University",
        "aff_unique_dep": "Institute for Computational and Mathematical Engineering;Bindley Biosciences Center",
        "aff_unique_url": "https://www.stanford.edu;https://www.purdue.edu",
        "aff_unique_abbr": "Stanford;Purdue",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Stanford;West Lafayette;West Lafeyette",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scalable Non-linear Learning with Adaptive Polynomial Expansions",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4751",
        "id": "4751",
        "author_site": "Alekh Agarwal, Alina Beygelzimer, Daniel Hsu, John Langford, Matus J Telgarsky",
        "author": "Alekh Agarwal; Alina Beygelzimer; Daniel Hsu; John Langford; Matus Telgarsky",
        "abstract": "Can we effectively learn a nonlinear representation in time comparable to linear learning? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines.",
        "bibtex": "@inproceedings{NIPS2014_4816828e,\n author = {Agarwal, Alekh and Beygelzimer, Alina and Hsu, Daniel and Langford, John and Telgarsky, Matus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Non-linear Learning with Adaptive Polynomial Expansions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4816828eae320da8a286ceb2e70bbeac-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4816828eae320da8a286ceb2e70bbeac-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/4816828eae320da8a286ceb2e70bbeac-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4816828eae320da8a286ceb2e70bbeac-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4816828eae320da8a286ceb2e70bbeac-Reviews.html",
        "metareview": "",
        "pdf_size": 612316,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13449689556908126474&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Microsoft Research; Yahoo! Labs; Columbia University; Microsoft Research; Rutgers University+Microsoft Research",
        "aff_domain": "microsoft.com;yahoo-inc.com;cs.columbia.edu;microsoft.com;cs.ucsd.edu",
        "email": "microsoft.com;yahoo-inc.com;cs.columbia.edu;microsoft.com;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4816828eae320da8a286ceb2e70bbeac-Abstract.html",
        "aff_unique_index": "0;1;2;0;3+0",
        "aff_unique_norm": "Microsoft;Yahoo!;Columbia University;Rutgers University",
        "aff_unique_dep": "Microsoft Research;Yahoo! Labs;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://yahoo.com;https://www.columbia.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "MSR;Yahoo!;Columbia;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scale Adaptive Blind Deblurring",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4752",
        "id": "4752",
        "author_site": "Haichao Zhang, Jianchao Yang",
        "author": "Haichao Zhang; Jianchao Yang",
        "abstract": "The presence of noise and small scale structures usually leads to large kernel estimation errors in blind image deblurring empirically, if not a total failure. We present a scale space perspective on blind deblurring algorithms, and introduce a cascaded scale space formulation for blind deblurring. This new formulation suggests a natural approach robust to noise and small scale structures through tying the estimation across multiple scales and balancing the contributions of different scales automatically by learning from data. The proposed formulation also allows to handle non-uniform blur with a straightforward extension. Experiments are conducted on both benchmark dataset and real-world images to validate the effectiveness of the proposed method. One surprising finding based on our approach is that blur kernel estimation is not necessarily best at the finest scale.",
        "bibtex": "@inproceedings{NIPS2014_ae3a4866,\n author = {Zhang, Haichao and Yang, Jianchao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scale Adaptive Blind Deblurring},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ae3a486695b76a62763f6397a79a0da8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ae3a486695b76a62763f6397a79a0da8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ae3a486695b76a62763f6397a79a0da8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ae3a486695b76a62763f6397a79a0da8-Reviews.html",
        "metareview": "",
        "pdf_size": 794170,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6583320393118444563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Duke University, NC; Adobe Research, CA",
        "aff_domain": "gmail.com;adobe.com",
        "email": "gmail.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ae3a486695b76a62763f6397a79a0da8-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Duke University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.duke.edu;https://research.adobe.com",
        "aff_unique_abbr": "Duke;Adobe",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Durham;CA",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scaling-up Importance Sampling for Markov Logic Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4753",
        "id": "4753",
        "author_site": "Deepak Venugopal, Vibhav Gogate",
        "author": "Deepak Venugopal; Vibhav Gogate",
        "abstract": "Markov Logic Networks (MLNs) are weighted first-order logic templates for generating large (ground) Markov networks. Lifted inference algorithms for them bring the power of logical inference to probabilistic inference. These algorithms operate as much as possible at the compact first-order level, grounding or propositionalizing the MLN only as necessary. As a result, lifted inference algorithms can be much more scalable than propositional algorithms that operate directly on the much larger ground network. Unfortunately, existing lifted inference algorithms suffer from two interrelated problems, which severely affects their scalability in practice. First, for most real-world MLNs having complex structure, they are unable to exploit symmetries and end up grounding most atoms (the grounding problem). Second, they suffer from the evidence problem, which arises because evidence breaks symmetries, severely diminishing the power of lifted inference. In this paper, we address both problems by presenting a scalable, lifted importance sampling-based approach that never grounds the full MLN. Specifically, we show how to scale up the two main steps in importance sampling: sampling from the proposal distribution and weight computation. Scalable sampling is achieved by using an informed, easy-to-sample proposal distribution derived from a compressed MLN-representation. Fast weight computation is achieved by only visiting a small subset of the sampled groundings of each formula instead of all of its possible groundings. We show that our new algorithm yields an asymptotically unbiased estimate. Our experiments on several MLNs clearly demonstrate the promise of our approach.",
        "bibtex": "@inproceedings{NIPS2014_4274adea,\n author = {Venugopal, Deepak and Gogate, Vibhav},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scaling-up Importance Sampling for Markov Logic Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4274adea26e813dd9ea92f9b2ec0eb2e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4274adea26e813dd9ea92f9b2ec0eb2e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4274adea26e813dd9ea92f9b2ec0eb2e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4274adea26e813dd9ea92f9b2ec0eb2e-Reviews.html",
        "metareview": "",
        "pdf_size": 383368,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11423231506384839323&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Texas at Dallas; Department of Computer Science, University of Texas at Dallas",
        "aff_domain": "utdallas.edu;hlt.utdallas.edu",
        "email": "utdallas.edu;hlt.utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4274adea26e813dd9ea92f9b2ec0eb2e-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Dallas",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utdallas.edu",
        "aff_unique_abbr": "UT Dallas",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Dallas",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Searching for Higgs Boson Decay Modes with Deep Learning",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4446",
        "id": "4446",
        "author_site": "Peter Sadowski, Daniel Whiteson, Pierre Baldi",
        "author": "Peter Sadowski; Pierre Baldi; Daniel Whiteson",
        "abstract": "Particle colliders enable us to probe the fundamental nature of matter by observing exotic particles produced by high-energy collisions. Because the experimental measurements from these collisions are necessarily incomplete and imprecise, machine learning algorithms play a major role in the analysis of experimental data. The high-energy physics community typically relies on standardized machine learning software packages for this analysis, and devotes substantial effort towards improving statistical power by hand crafting high-level features derived from the raw collider measurements. In this paper, we train artificial neural networks to detect the decay of the Higgs boson to tau leptons on a dataset of 82 million simulated collision events. We demonstrate that deep neural network architectures are particularly well-suited for this task with the ability to automatically discover high-level features from the data and increase discovery significance.",
        "bibtex": "@inproceedings{NIPS2014_44f58a78,\n author = {Sadowski, Peter and Baldi, Pierre and Whiteson, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Searching for Higgs Boson Decay Modes with Deep Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/44f58a78b6dd03bf9ccb526e7c7538d5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/44f58a78b6dd03bf9ccb526e7c7538d5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/44f58a78b6dd03bf9ccb526e7c7538d5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/44f58a78b6dd03bf9ccb526e7c7538d5-Reviews.html",
        "metareview": "",
        "pdf_size": 603225,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13091553631690183240&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Physics and Astronomy, University of California, Irvine",
        "aff_domain": "uci.edu;ics.uci.edu;uci.edu",
        "email": "uci.edu;ics.uci.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/44f58a78b6dd03bf9ccb526e7c7538d5-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Self-Adaptable Templates for Feature Coding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4754",
        "id": "4754",
        "author_site": "Xavier Boix, Gemma Roig, Salomon Diether, Luc V Gool",
        "author": "Xavier Boix; Gemma Roig; Salomon Diether; Luc Van Gool",
        "abstract": "Hierarchical feed-forward networks have been successfully applied in object recognition. At each level of the hierarchy, features are extracted and encoded, followed by a pooling step. Within this processing pipeline, the common trend is to learn the feature coding templates, often referred as codebook entries, filters, or over-complete basis. Recently, an approach that apparently does not use templates has been shown to obtain very promising results. This is the second-order pooling (O2P). In this paper, we analyze O2P as a coding-pooling scheme. We find that at testing phase, O2P automatically adapts the feature coding templates to the input features, rather than using templates learned during the training phase. From this finding, we are able to bring common concepts of coding-pooling schemes to O2P, such as feature quantization. This allows for significant accuracy improvements of O2P in standard benchmarks of image classification, namely Caltech101 and VOC07.",
        "bibtex": "@inproceedings{NIPS2014_bcd05553,\n author = {Boix, Xavier and Roig, Gemma and Diether, Salomon and Van Gool, Luc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Self-Adaptable Templates for Feature Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/bcd05553c2f1bae90d255e4096ef3e57-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/bcd05553c2f1bae90d255e4096ef3e57-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/bcd05553c2f1bae90d255e4096ef3e57-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/bcd05553c2f1bae90d255e4096ef3e57-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/bcd05553c2f1bae90d255e4096ef3e57-Reviews.html",
        "metareview": "",
        "pdf_size": 345220,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12291035924720499908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Vision Laboratory, ETH Zurich, Switzerland + LCSL, Massachusetts Institute of Technology & Istituto Italiano di Tecnologia, Cambridge, MA; Computer Vision Laboratory, ETH Zurich, Switzerland + LCSL, Massachusetts Institute of Technology & Istituto Italiano di Tecnologia, Cambridge, MA; Computer Vision Laboratory, ETH Zurich, Switzerland; Computer Vision Laboratory, ETH Zurich, Switzerland",
        "aff_domain": "mit.edu;mit.edu;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "mit.edu;mit.edu;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/bcd05553c2f1bae90d255e4096ef3e57-Abstract.html",
        "aff_unique_index": "0+1;0+1;0;0",
        "aff_unique_norm": "ETH Zurich;Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Vision Laboratory;LCSL",
        "aff_unique_url": "https://www.ethz.ch;https://web.mit.edu",
        "aff_unique_abbr": "ETHZ;MIT",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+1;0+1;0;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Self-Paced Learning with Diversity",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4755",
        "id": "4755",
        "author_site": "Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, Alexander Hauptmann",
        "author": "Lu Jiang; Deyu Meng; Shoou-I Yu; Zhenzhong Lan; Shiguang Shan; Alexander G. Hauptmann",
        "abstract": "Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizer. This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.",
        "bibtex": "@inproceedings{NIPS2014_8786a617,\n author = {Jiang, Lu and Meng, Deyu and Yu, Shoou-I and Lan, Zhenzhong and Shan, Shiguang and Hauptmann, Alexander G.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Self-Paced Learning with Diversity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8786a6172486a9582ff0b94492e8f06a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8786a6172486a9582ff0b94492e8f06a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8786a6172486a9582ff0b94492e8f06a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8786a6172486a9582ff0b94492e8f06a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8786a6172486a9582ff0b94492e8f06a-Reviews.html",
        "metareview": "",
        "pdf_size": 754481,
        "gs_citation": 450,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14221479622446595125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University + School of Mathematics and Statistics, Xi\u2019an Jiaotong University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University + Institute of Computing Technology, Chinese Academy of Sciences; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;mail.xjtu.edu.cn;cs.cmu.edu;cs.cmu.edu;ict.ac.cn;cs.cmu.edu",
        "email": "cs.cmu.edu;mail.xjtu.edu.cn;cs.cmu.edu;cs.cmu.edu;ict.ac.cn;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8786a6172486a9582ff0b94492e8f06a-Abstract.html",
        "aff_unique_index": "0;0+1;0;0;0+2;0",
        "aff_unique_norm": "Carnegie Mellon University;Xi'an Jiao Tong University;Chinese Academy of Sciences",
        "aff_unique_dep": "School of Computer Science;School of Mathematics and Statistics;Institute of Computing Technology",
        "aff_unique_url": "https://www.cmu.edu;http://en.xjtu.edu.cn/;http://www.ict.ac.cn",
        "aff_unique_abbr": "CMU;XJTU;CAS",
        "aff_campus_unique_index": "0;0+1;0;0;0;0",
        "aff_campus_unique": "Pittsburgh;Xi'an;",
        "aff_country_unique_index": "0;0+1;0;0;0+1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4756",
        "id": "4756",
        "author_site": "Yichuan Zhang, Charles Sutton",
        "author": "Yichuan Zhang; Charles Sutton",
        "abstract": "Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.",
        "bibtex": "@inproceedings{NIPS2014_a1126573,\n author = {Zhang, Yichuan and Sutton, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a1126573153ad7e9f44ba80e99316482-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a1126573153ad7e9f44ba80e99316482-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a1126573153ad7e9f44ba80e99316482-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a1126573153ad7e9f44ba80e99316482-Reviews.html",
        "metareview": "",
        "pdf_size": 817398,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11031605006874004268&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;inf.ed.ac.uk",
        "email": "sms.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a1126573153ad7e9f44ba80e99316482-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Semi-supervised Learning with Deep Generative Models",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4448",
        "id": "4448",
        "author_site": "Diederik Kingma, Shakir Mohamed, Danilo Jimenez Rezende, Max Welling",
        "author": "Diederik P. Kingma; Danilo J. Rezende; Shakir Mohamed; Max Welling",
        "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.",
        "bibtex": "@inproceedings{NIPS2014_6d42b121,\n author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-supervised Learning with Deep Generative Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6d42b1217a6996997ead5a8398c1f944-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/6d42b1217a6996997ead5a8398c1f944-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/6d42b1217a6996997ead5a8398c1f944-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/6d42b1217a6996997ead5a8398c1f944-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/6d42b1217a6996997ead5a8398c1f944-Reviews.html",
        "metareview": "",
        "pdf_size": 718581,
        "gs_citation": 3895,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17889646460839762357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Machine Learning Group, Univ. of Amsterdam; Google Deepmind; Google Deepmind; Machine Learning Group, Univ. of Amsterdam",
        "aff_domain": "uva.nl;google.com;google.com;uva.nl",
        "email": "uva.nl;google.com;google.com;uva.nl",
        "github": "",
        "project": "http://arxiv.org/abs/1406.5298",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/6d42b1217a6996997ead5a8398c1f944-Abstract.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Amsterdam;DeepMind",
        "aff_unique_dep": "Machine Learning Group;DeepMind",
        "aff_unique_url": "https://www.uva.nl;https://deepmind.com",
        "aff_unique_abbr": "UvA;DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Netherlands;United Kingdom"
    },
    {
        "title": "Sensory Integration and Density Estimation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4757",
        "id": "4757",
        "author_site": "Joseph G Makin, Philip N Sabes",
        "author": "Joseph G. Makin; Philip N. Sabes",
        "abstract": "The integration of partially redundant information from multiple sensors is a standard computational problem for agents interacting with the world. In man and other primates, integration has been shown psychophysically to be nearly optimal in the sense of error minimization. An influential generalization of this notion of optimality is that populations of multisensory neurons should retain all the information from their unisensory afferents about the underlying, common stimulus [1]. More recently, it was shown empirically that a neural network trained to perform latent-variable density estimation, with the activities of the unisensory neurons as observed data, satisfies the information-preservation criterion, even though the model architecture was not designed to match the true generative process for the data [2]. We prove here an analytical connection between these seemingly different tasks, density estimation and sensory integration; that the former implies the latter for the model used in [2]; but that this does not appear to be true for all models.",
        "bibtex": "@inproceedings{NIPS2014_8b88c74a,\n author = {Makin, Joseph G. and Sabes, Philip N.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sensory Integration and Density Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8b88c74ad1c4604cb7ec82f0e61dca91-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8b88c74ad1c4604cb7ec82f0e61dca91-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8b88c74ad1c4604cb7ec82f0e61dca91-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8b88c74ad1c4604cb7ec82f0e61dca91-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8b88c74ad1c4604cb7ec82f0e61dca91-Reviews.html",
        "metareview": "",
        "pdf_size": 326454,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1032800754459897334&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Integrative Neuroscience/Department of Physiology, University of California, San Francisco; Center for Integrative Neuroscience/Department of Physiology, University of California, San Francisco",
        "aff_domain": "phy.ucsf.edu;phy.ucsf.edu",
        "email": "phy.ucsf.edu;phy.ucsf.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8b88c74ad1c4604cb7ec82f0e61dca91-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Francisco",
        "aff_unique_dep": "Department of Physiology",
        "aff_unique_url": "https://www.ucsf.edu",
        "aff_unique_abbr": "UCSF",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Francisco",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sequence to Sequence Learning with Neural Networks",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4348",
        "id": "4348",
        "author_site": "Ilya Sutskever, Oriol Vinyals, Quoc V Le",
        "author": "Ilya Sutskever; Oriol Vinyals; Quoc V. Le",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "bibtex": "@inproceedings{NIPS2014_5a18e133,\n author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sequence to Sequence Learning with Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5a18e133cbf9f257297f410bb7eca942-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5a18e133cbf9f257297f410bb7eca942-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5a18e133cbf9f257297f410bb7eca942-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5a18e133cbf9f257297f410bb7eca942-Reviews.html",
        "metareview": "",
        "pdf_size": 142950,
        "gs_citation": 29204,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13133880703797056141&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 36,
        "aff": "Google; Google; Google",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5a18e133cbf9f257297f410bb7eca942-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sequential Monte Carlo for Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4759",
        "id": "4759",
        "author_site": "Christian Andersson Naesseth, Fredrik Lindsten, Thomas Sch\u00f6n",
        "author": "Christian A. Naesseth; Fredrik Lindsten; Thomas B. Sch\u00f6n",
        "abstract": "We propose a new framework for how to use sequential Monte Carlo (SMC) algorithms for inference in probabilistic graphical models (PGM). Via a sequential decomposition of the PGM we find a sequence of auxiliary distributions defined on a monotonically increasing sequence of probability spaces. By targeting these auxiliary distributions using SMC we are able to approximate the full joint distribution defined by the PGM. One of the key merits of the SMC sampler is that it provides an unbiased estimate of the partition function of the model. We also show how it can be used within a particle Markov chain Monte Carlo framework in order to construct high-dimensional block-sampling algorithms for general PGMs.",
        "bibtex": "@inproceedings{NIPS2014_12d76369,\n author = {Naesseth, Christian A. and Lindsten, Fredrik and Sch\\\"{o}n, Thomas B.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sequential Monte Carlo for Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/12d763696f54acee4f1b4a3e86b89cfc-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/12d763696f54acee4f1b4a3e86b89cfc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/12d763696f54acee4f1b4a3e86b89cfc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/12d763696f54acee4f1b4a3e86b89cfc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/12d763696f54acee4f1b4a3e86b89cfc-Reviews.html",
        "metareview": "",
        "pdf_size": 371739,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12273180576547146889&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Div. of Automatic Control, Link \u00a8oping University, Link \u00a8oping, Sweden; Dept. of Engineering, The University of Cambridge, Cambridge, UK; Dept. of Information Technology, Uppsala University, Uppsala, Sweden",
        "aff_domain": "isy.liu.se;cam.ac.uk;it.uu.se",
        "email": "isy.liu.se;cam.ac.uk;it.uu.se",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/12d763696f54acee4f1b4a3e86b89cfc-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Link\u00f6ping University;University of Cambridge;Uppsala University",
        "aff_unique_dep": "Division of Automatic Control;Dept. of Engineering;Dept. of Information Technology",
        "aff_unique_url": "https://www.liu.se;https://www.cam.ac.uk;https://www.uu.se",
        "aff_unique_abbr": "LiU;Cambridge;UU",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Link\u00f6ping;Cambridge;Uppsala",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Sweden;United Kingdom"
    },
    {
        "title": "SerialRank: Spectral Ranking using Seriation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4760",
        "id": "4760",
        "author_site": "Fajwel Fogel, Alexandre d'Aspremont, Milan Vojnovic",
        "author": "Fajwel Fogel; Alexandre d'Aspremont; Milan Vojnovic",
        "abstract": "We describe a seriation algorithm for ranking a set of n items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact even when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than other scoring methods. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.",
        "bibtex": "@inproceedings{NIPS2014_0fdd9219,\n author = {Fogel, Fajwel and d\\textquotesingle Aspremont, Alexandre and Vojnovic, Milan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SerialRank: Spectral Ranking using Seriation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/0fdd9219a3552881cfe283e8bd759744-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/0fdd9219a3552881cfe283e8bd759744-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/0fdd9219a3552881cfe283e8bd759744-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/0fdd9219a3552881cfe283e8bd759744-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/0fdd9219a3552881cfe283e8bd759744-Reviews.html",
        "metareview": "",
        "pdf_size": 411949,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "C.M.A.P., \u00c9cole Polytechnique, Palaiseau, France; CNRS & D.I., \u00c9cole Normale Sup\u00e9rieure, Paris, France; Microsoft Research, Cambridge, UK",
        "aff_domain": "cmap.polytechnique.fr;ens.fr;microsoft.com",
        "email": "cmap.polytechnique.fr;ens.fr;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/0fdd9219a3552881cfe283e8bd759744-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Ecole Polytechnique;\u00c9cole Normale Sup\u00e9rieure;Microsoft",
        "aff_unique_dep": "C.M.A.P.;;Microsoft Research",
        "aff_unique_url": "https://www.ecp.fr;https://www.ens.fr;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "\u00c9cole Polytechnique;ENS;MSR",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Palaiseau;Paris;Cambridge",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "France;United Kingdom"
    },
    {
        "title": "Shape and Illumination from Shading using the Generic Viewpoint Assumption",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4761",
        "id": "4761",
        "author_site": "Daniel Zoran, Dilip Krishnan, Jos\u00e9 Bento, Bill Freeman",
        "author": "Daniel Zoran; Dilip Krishnan; Jose Bento; William T. Freeman",
        "abstract": "The Generic Viewpoint Assumption (GVA) states that the position of the viewer or the light in a scene is not special. Thus, any estimated parameters from an observation should be stable under small perturbations such as object, viewpoint or light positions. The GVA has been analyzed and quantified in previous works, but has not been put to practical use in actual vision tasks. In this paper, we show how to utilize the GVA to estimate shape and illumination from a single shading image, without the use of other priors. We propose a novel linearized Spherical Harmonics (SH) shading model which enables us to obtain a computationally efficient form of the GVA term. Together with a data term, we build a model whose unknowns are shape and SH illumination. The model parameters are estimated using the Alternating Direction Method of Multipliers embedded in a multi-scale estimation framework. In this prior-free framework, we obtain competitive shape and illumination estimation results under a variety of models and lighting conditions, requiring fewer assumptions than competing methods.",
        "bibtex": "@inproceedings{NIPS2014_f5beec2f,\n author = {Zoran, Daniel and Krishnan, Dilip and Bento, Jose and Freeman, William T.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Shape and Illumination from Shading using the Generic Viewpoint Assumption},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f5beec2f4202f1ba7a9e13f62134b841-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f5beec2f4202f1ba7a9e13f62134b841-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/f5beec2f4202f1ba7a9e13f62134b841-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f5beec2f4202f1ba7a9e13f62134b841-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f5beec2f4202f1ba7a9e13f62134b841-Reviews.html",
        "metareview": "",
        "pdf_size": 1101094,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5837107220747008089&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "CSAIL, MIT; CSAIL, MIT; Boston College; CSAIL, MIT",
        "aff_domain": "mit.edu;mit.edu;bc.edu;mit.edu",
        "email": "mit.edu;mit.edu;bc.edu;mit.edu",
        "github": "",
        "project": "http://www.cogsci.uci.edu/~ddhoff/three-cubes.gif",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f5beec2f4202f1ba7a9e13f62134b841-Abstract.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Boston College",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.bostoncollege.edu",
        "aff_unique_abbr": "MIT;BC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Shaping Social Activity by Incentivizing Users",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4762",
        "id": "4762",
        "author_site": "Mehrdad Farajtabar, Nan Du, Manuel Gomez Rodriguez, Isabel Valera, Hongyuan Zha, Le Song",
        "author": "Mehrdad Farajtabar; Nan Du; Manuel Gomez-Rodriguez; Isabel Valera; Hongyuan Zha; Le Song",
        "abstract": "Events in an online social network can be categorized roughly into endogenous events, where users just respond to the actions of their neighbors within the network, or exogenous events, where users take actions due to drives external to the network. How much external drive should be provided to each user, such that the network activity can be steered towards a target state? In this paper, we model social events using multivariate Hawkes processes, which can capture both endogenous and exogenous event intensities, and derive a time dependent linear relation between the intensity of exogenous events and the overall network activity. Exploiting this connection, we develop a convex optimization framework for determining the required level of external drive in order for the network to reach a desired activity level. We experimented with event data gathered from Twitter, and show that our method can steer the activity of the network more accurately than alternatives.",
        "bibtex": "@inproceedings{NIPS2014_5bce7b2d,\n author = {Farajtabar, Mehrdad and Du, Nan and Gomez-Rodriguez, Manuel and Valera, Isabel and Zha, Hongyuan and Song, Le},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Shaping Social Activity by Incentivizing Users},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5bce7b2ded63483e6f2828e6a9427498-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5bce7b2ded63483e6f2828e6a9427498-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5bce7b2ded63483e6f2828e6a9427498-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5bce7b2ded63483e6f2828e6a9427498-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5bce7b2ded63483e6f2828e6a9427498-Reviews.html",
        "metareview": "",
        "pdf_size": 426370,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12448910949720598106&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Georgia Institute of Technology\u2217; Georgia Institute of Technology\u2217; MPI for Software Systems\u2020; Univ. Carlos III in Madrid\u2021; Georgia Institute of Technology\u2217; Georgia Institute of Technology\u2217",
        "aff_domain": "gatech.edu;gatech.edu;mpi-sws.org;tsc.uc3m.es;cc.gatech.edu;cc.gatech.edu",
        "email": "gatech.edu;gatech.edu;mpi-sws.org;tsc.uc3m.es;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5bce7b2ded63483e6f2828e6a9427498-Abstract.html",
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "Georgia Institute of Technology;Max Planck Institute for Software Systems;Carlos III University of Madrid",
        "aff_unique_dep": ";Software Systems;",
        "aff_unique_url": "https://www.gatech.edu;https://www.mpi-sws.org;https://www.uc3m.es",
        "aff_unique_abbr": "Georgia Tech;MPI-SWS;UC3M",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Madrid",
        "aff_country_unique_index": "0;0;1;2;0;0",
        "aff_country_unique": "United States;Germany;Spain"
    },
    {
        "title": "Signal Aggregate Constraints in Additive Factorial HMMs, with Application to Energy Disaggregation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4763",
        "id": "4763",
        "author_site": "Mingjun Zhong, Nigel Goddard, Charles Sutton",
        "author": "Mingjun Zhong; Nigel Goddard; Charles Sutton",
        "abstract": "Blind source separation problems are difficult because they are inherently unidentifiable, yet the entire goal is to identify meaningful sources. We introduce a way of incorporating domain knowledge into this problem, called signal aggregate constraints (SACs). SACs encourage the total signal for each of the unknown sources to be close to a specified value. This is based on the observation that the total signal often varies widely across the unknown sources, and we often have a good idea of what total values to expect. We incorporate SACs into an additive factorial hidden Markov model (AFHMM) to formulate the energy disaggregation problems where only one mixture signal is assumed to be observed. A convex quadratic program for approximate inference is employed for recovering those source signals. On a real-world energy disaggregation data set, we show that the use of SACs dramatically improves the original AFHMM, and significantly improves over a recent state-of-the art approach.",
        "bibtex": "@inproceedings{NIPS2014_bf64575b,\n author = {Zhong, Mingjun and Goddard, Nigel and Sutton, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Signal Aggregate Constraints in Additive Factorial HMMs, with Application to Energy Disaggregation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/bf64575b42486eb72efdfa0091be60a5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/bf64575b42486eb72efdfa0091be60a5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/bf64575b42486eb72efdfa0091be60a5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/bf64575b42486eb72efdfa0091be60a5-Reviews.html",
        "metareview": "",
        "pdf_size": 208786,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9205530956066863307&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "School of Informatics, University of Edinburgh, United Kingdom; School of Informatics, University of Edinburgh, United Kingdom; School of Informatics, University of Edinburgh, United Kingdom",
        "aff_domain": "inf.ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "inf.ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/bf64575b42486eb72efdfa0091be60a5-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Simple MAP Inference via Low-Rank Relaxations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4764",
        "id": "4764",
        "author_site": "Roy Frostig, Sida Wang, Percy Liang, Christopher D Manning",
        "author": "Roy Frostig; Sida I. Wang; Percy Liang; Christopher D. Manning",
        "abstract": "We focus on the problem of maximum a posteriori (MAP) inference in Markov random fields with binary variables and pairwise interactions. For this common subclass of inference tasks, we consider low-rank relaxations that interpolate between the discrete problem and its full-rank semidefinite relaxation, followed by randomized rounding. We develop new theoretical bounds studying the effect of rank, showing that as the rank grows, the relaxed objective increases but saturates, and that the fraction in objective value retained by the rounded discrete solution decreases. In practice, we show two algorithms for optimizing the low-rank objectives which are simple to implement, enjoy ties to the underlying theory, and outperform existing approaches on benchmark MAP inference tasks.",
        "bibtex": "@inproceedings{NIPS2014_bec00388,\n author = {Frostig, Roy and Wang, Sida I. and Liang, Percy and Manning, Christopher D.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simple MAP Inference via Low-Rank Relaxations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/bec003881f556cf7a949d18178db94a1-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/bec003881f556cf7a949d18178db94a1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/bec003881f556cf7a949d18178db94a1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/bec003881f556cf7a949d18178db94a1-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/bec003881f556cf7a949d18178db94a1-Reviews.html",
        "metareview": "",
        "pdf_size": 576985,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3974337178835650022&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": "Computer Science Department, Stanford University, Stanford, CA, 94305; Computer Science Department, Stanford University, Stanford, CA, 94305; Computer Science Department, Stanford University, Stanford, CA, 94305; Computer Science Department, Stanford University, Stanford, CA, 94305",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/bec003881f556cf7a949d18178db94a1-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4765",
        "id": "4765",
        "author": "Francesco Orabona",
        "abstract": "Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.",
        "bibtex": "@inproceedings{NIPS2014_e3c71f83,\n author = {Orabona, Francesco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e3c71f83184b7aefda401dc278f572c6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e3c71f83184b7aefda401dc278f572c6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e3c71f83184b7aefda401dc278f572c6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e3c71f83184b7aefda401dc278f572c6-Reviews.html",
        "metareview": "",
        "pdf_size": 433645,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14153911922715112749&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e3c71f83184b7aefda401dc278f572c6-Abstract.html"
    },
    {
        "title": "Smoothed Gradients for Stochastic Variational Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4766",
        "id": "4766",
        "author_site": "Stephan Mandt, David Blei",
        "author": "Stephan Mandt; David Blei",
        "abstract": "Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.",
        "bibtex": "@inproceedings{NIPS2014_b1bdc36e,\n author = {Mandt, Stephan and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Smoothed Gradients for Stochastic Variational Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b1bdc36e9d1ee27b5a3b4935eecd3e8a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b1bdc36e9d1ee27b5a3b4935eecd3e8a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b1bdc36e9d1ee27b5a3b4935eecd3e8a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b1bdc36e9d1ee27b5a3b4935eecd3e8a-Reviews.html",
        "metareview": "",
        "pdf_size": 878180,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5460444351070045770&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Physics, Princeton University; Department of Computer Science+Department of Statistics, Columbia University",
        "aff_domain": "princeton.edu;columbia.edu",
        "email": "princeton.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b1bdc36e9d1ee27b5a3b4935eecd3e8a-Abstract.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Princeton University;Unknown Institution;Columbia University",
        "aff_unique_dep": "Department of Physics;Department of Computer Science;Department of Statistics",
        "aff_unique_url": "https://www.princeton.edu;;https://www.columbia.edu",
        "aff_unique_abbr": "Princeton;;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Sparse Bayesian structure learning with \u201cdependent relevance determination\u201d priors",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4767",
        "id": "4767",
        "author_site": "Anqi Wu, Mijung Park, Sanmi Koyejo, Jonathan W Pillow",
        "author": "Anqi Wu; Mijung Park; Oluwasanmi Koyejo; Jonathan W. Pillow",
        "abstract": "In many problem settings, parameter vectors are not merely sparse, but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as \u201cregion sparsity\u201d. Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop efficient approximate inference methods and show substantial improvements over comparable methods (e.g., group lasso and smooth RVM) for both simulated and real datasets from brain imaging.",
        "bibtex": "@inproceedings{NIPS2014_c3997ee5,\n author = {Wu, Anqi and Park, Mijung and Koyejo, Oluwasanmi and Pillow, Jonathan W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Bayesian structure learning with \\textquotedblleft dependent relevance determination\\textquotedblright  priors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c3997ee5599137705bf0e2a7326fd293-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c3997ee5599137705bf0e2a7326fd293-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c3997ee5599137705bf0e2a7326fd293-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c3997ee5599137705bf0e2a7326fd293-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c3997ee5599137705bf0e2a7326fd293-Reviews.html",
        "metareview": "",
        "pdf_size": 9050020,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13870620761198119942&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Princeton Neuroscience Institute, Princeton University; The Gatsby Unit, University College London; Department of Psychology, Stanford University; Princeton Neuroscience Institute, Princeton University",
        "aff_domain": "princeton.edu;gatsby.ucl.ac.uk;stanford.edu;princeton.edu",
        "email": "princeton.edu;gatsby.ucl.ac.uk;stanford.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c3997ee5599137705bf0e2a7326fd293-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Princeton University;University College London;Stanford University",
        "aff_unique_dep": "Princeton Neuroscience Institute;The Gatsby Unit;Department of Psychology",
        "aff_unique_url": "https://www.princeton.edu;https://www.ucl.ac.uk;https://www.stanford.edu",
        "aff_unique_abbr": "Princeton;UCL;Stanford",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Princeton;London;Stanford",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Sparse Multi-Task Reinforcement Learning",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4768",
        "id": "4768",
        "author_site": "Daniele Calandriello, Alessandro Lazaric, Marcello Restelli",
        "author": "Daniele Calandriello; Alessandro Lazaric; Marcello Restelli",
        "abstract": "In multi-task reinforcement learning (MTRL), the objective is to simultaneously learn multiple tasks and exploit their similarity to improve the performance w.r.t.\\ single-task learning. In this paper we investigate the case when all the tasks can be accurately represented in a linear approximation space using the same small subset of the original (large) set of features. This is equivalent to assuming that the weight vectors of the task value functions are \\textit{jointly sparse}, i.e., the set of their non-zero components is small and it is shared across tasks. Building on existing results in multi-task regression, we develop two multi-task extensions of the fitted $Q$-iteration algorithm. While the first algorithm assumes that the tasks are jointly sparse in the given representation, the second one learns a transformation of the features in the attempt of finding a more sparse representation. For both algorithms we provide a sample complexity analysis and numerical simulations.",
        "bibtex": "@inproceedings{NIPS2014_cf80f3e9,\n author = {Calandriello, Daniele and Lazaric, Alessandro and Restelli, Marcello},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Multi-Task Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/cf80f3e9191637641378521b91585f22-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/cf80f3e9191637641378521b91585f22-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/cf80f3e9191637641378521b91585f22-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/cf80f3e9191637641378521b91585f22-Reviews.html",
        "metareview": "",
        "pdf_size": 405900,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15686272090909627717&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "INRIA Lille \u2013 Nord Europe, France; INRIA Lille \u2013 Nord Europe, France; Politecnico di Milano, Italy",
        "aff_domain": "inria.fr;inria.fr;polimi.it",
        "email": "inria.fr;inria.fr;polimi.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/cf80f3e9191637641378521b91585f22-Abstract.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "INRIA;Politecnico di Milano",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.inria.fr;https://www.polimi.it",
        "aff_unique_abbr": "INRIA;Polimi",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lille;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "France;Italy"
    },
    {
        "title": "Sparse PCA via Covariance Thresholding",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4769",
        "id": "4769",
        "author_site": "Yash Deshpande, Andrea Montanari",
        "author": "Yash Deshpande; Andrea Montanari",
        "abstract": "In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension $n\\times p$ and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here that the principal components $\\bv_1,\\dots,\\bv_r$ have at most $k_1, \\cdots, k_q$ non-zero entries respectively, and study the high-dimensional regime in which $p$ is of the same order as $n$. In an influential paper, Johnstone and Lu \\cite{johnstone2004sparse} introduced a simple algorithm that estimates the support of the principal vectors $\\bv_1,\\dots,\\bv_r$ by the largest entries in the diagonal of the empirical covariance. This method can be shown to succeed with high probability if $k_q \\le C_1\\sqrt{n/\\log p}$, and to fail with high probability if $k_q\\ge C_2 \\sqrt{n/\\log p}$ for two constants $0 < C_1,C_2 < \\infty$. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recently proposed by Krauthgamer, Nadler and Vilenchik \\cite{KrauthgamerSPCA}. We confirm empirical evidence presented by these authors and rigorously prove that the algorithm succeeds with high probability for $k$ of order $\\sqrt{n}$. Recent conditional lower bounds \\cite{berthet2013computational} suggest that it might be impossible to do significantly better. The key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before.",
        "bibtex": "@inproceedings{NIPS2014_07a45842,\n author = {Deshpande, Yash and Montanari, Andrea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse PCA via Covariance Thresholding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/07a45842fcab1f6116c50549a437c254-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/07a45842fcab1f6116c50549a437c254-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/07a45842fcab1f6116c50549a437c254-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/07a45842fcab1f6116c50549a437c254-Reviews.html",
        "metareview": "",
        "pdf_size": 421009,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18406419897578937912&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Electrical Engineering, Stanford University; Electrical Engineering and Statistics, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/07a45842fcab1f6116c50549a437c254-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sparse PCA with Oracle Property",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4713",
        "id": "4713",
        "author_site": "Quanquan Gu, Zhaoran Wang, Han Liu",
        "author": "Quanquan Gu; Zhaoran Wang; Han Liu",
        "abstract": "In this paper, we study the estimation of the $k$-dimensional sparse principal subspace of covariance matrix $\\Sigma$ in the high-dimensional setting. We aim to recover the oracle principal subspace solution, i.e., the principal subspace estimator obtained assuming the true support is known a priori. To this end, we propose a family of estimators based on the semidefinite relaxation of sparse PCA with novel regularizations. In particular, under a weak assumption on the magnitude of the population projection matrix, one estimator within this family exactly recovers the true support with high probability, has exact rank-$k$, and attains a $\\sqrt{s/n}$ statistical rate of convergence with $s$ being the subspace sparsity level and $n$ the sample size. Compared to existing support recovery results for sparse PCA, our approach does not hinge on the spiked covariance model or the limited correlation condition. As a complement to the first estimator that enjoys the oracle property, we prove that, another estimator within the family achieves a sharper statistical rate of convergence than the standard semidefinite relaxation of sparse PCA, even when the previous assumption on the magnitude of the projection matrix is violated. We validate the theoretical results by numerical experiments on synthetic datasets.",
        "bibtex": "@inproceedings{NIPS2014_e83fdbad,\n author = {Gu, Quanquan and Wang, Zhaoran and Liu, Han},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse PCA with Oracle Property},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e83fdbad867240ba801da0f647286fe8-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e83fdbad867240ba801da0f647286fe8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e83fdbad867240ba801da0f647286fe8-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e83fdbad867240ba801da0f647286fe8-Reviews.html",
        "metareview": "",
        "pdf_size": 309808,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9262957907290886048&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Operations Research and Financial Engineering, Princeton University; Department of Operations Research and Financial Engineering, Princeton University; Department of Operations Research and Financial Engineering, Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e83fdbad867240ba801da0f647286fe8-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sparse Polynomial Learning and Graph Sketching",
        "status": "Oral",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4350",
        "id": "4350",
        "author_site": "Murat Kocaoglu, Karthikeyan Shanmugam, Alex Dimakis, Adam Klivans",
        "author": "Murat Kocaoglu; Karthikeyan Shanmugam; Alexandros G.Dimakis; Adam Klivans",
        "abstract": "Let $f: \\{-1,1\\}^n \\rightarrow \\mathbb{R}$ be a polynomial with at most $s$ non-zero real coefficients. We give an algorithm for exactly reconstructing $f$ given random examples from the uniform distribution on $\\{-1,1\\}^n$ that runs in time polynomial in $n$ and $2^{s}$ and succeeds if the function satisfies the \\textit{unique sign property}: there is one output value which corresponds to a unique set of values of the participating parities. This sufficient condition is satisfied when every coefficient of $f$ is perturbed by a small random noise, or satisfied with high probability when $s$ parity functions are chosen randomly or when all the coefficients are positive. Learning sparse polynomials over the Boolean domain in time polynomial in $n$ and $2^{s}$ is considered notoriously hard in the worst-case. Our result shows that the problem is tractable for almost all sparse polynomials. Then, we show an application of this result to hypergraph sketching which is the problem of learning a sparse (both in the number of hyperedges and the size of the hyperedges) hypergraph from uniformly drawn random cuts. We also provide experimental results on a real world dataset.",
        "bibtex": "@inproceedings{NIPS2014_78812290,\n author = {Kocaoglu, Murat and Shanmugam, Karthikeyan and G.Dimakis, Alexandros and Klivans, Adam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Polynomial Learning and Graph Sketching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/78812290b46192ce379f0349a4ef0758-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/78812290b46192ce379f0349a4ef0758-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/78812290b46192ce379f0349a4ef0758-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/78812290b46192ce379f0349a4ef0758-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/78812290b46192ce379f0349a4ef0758-Reviews.html",
        "metareview": "",
        "pdf_size": 309489,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10365850872692906888&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering+Department of Computer Science; Department of Electrical and Computer Engineering+Department of Computer Science; Department of Electrical and Computer Engineering+Department of Computer Science; Department of Computer Science",
        "aff_domain": "utexas.edu;utexas.edu;austin.utexas.edu;cs.utexas.edu",
        "email": "utexas.edu;utexas.edu;austin.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/78812290b46192ce379f0349a4ef0758-Abstract.html",
        "aff_unique_index": "0+0;0+0;0+0;0",
        "aff_unique_norm": "Unknown Institution",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;",
        "aff_country_unique": ""
    },
    {
        "title": "Sparse Random Feature Algorithm as Coordinate Descent in Hilbert Space",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4770",
        "id": "4770",
        "author_site": "Ian En-Hsu Yen, Ting-Wei Lin, Shou-De Lin, Pradeep Ravikumar, Inderjit Dhillon",
        "author": "Ian E.H. Yen; Ting-Wei Lin; Shou-De Lin; Pradeep Ravikumar; Inderjit S. Dhillon",
        "abstract": "In this paper, we propose a Sparse Random Feature algorithm, which learns a sparse non-linear predictor by minimizing an $\\ell_1$-regularized objective function over the Hilbert Space induced from kernel function. By interpreting the algorithm as Randomized Coordinate Descent in the infinite-dimensional space, we show the proposed approach converges to a solution comparable within $\\eps$-precision to exact kernel method by drawing $O(1/\\eps)$ number of random features, contrasted to the $O(1/\\eps^2)$-type convergence achieved by Monte-Carlo analysis in current Random Feature literature. In our experiments, the Sparse Random Feature algorithm obtains sparse solution that requires less memory and prediction time while maintains comparable performance on tasks of regression and classification. In the meantime, as an approximate solver for infinite-dimensional $\\ell_1$-regularized problem, the randomized approach converges to better solution than Boosting approach when the greedy step of Boosting cannot be performed exactly.",
        "bibtex": "@inproceedings{NIPS2014_45ba6620,\n author = {Yen, Ian E.H. and Lin, Ting-Wei and Lin, Shou-De and Ravikumar, Pradeep and Dhillon, Inderjit S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Random Feature Algorithm as Coordinate Descent in Hilbert Space},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/45ba6620170d8c1b5d582c2c904a8ace-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/45ba6620170d8c1b5d582c2c904a8ace-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/45ba6620170d8c1b5d582c2c904a8ace-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/45ba6620170d8c1b5d582c2c904a8ace-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/45ba6620170d8c1b5d582c2c904a8ace-Reviews.html",
        "metareview": "",
        "pdf_size": 269648,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16234496107073206746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Texas at Austin; National Taiwan University; National Taiwan University; University of Texas at Austin; University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;csie.ntu.edu.tw;csie.ntu.edu.tw",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu;csie.ntu.edu.tw;csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/45ba6620170d8c1b5d582c2c904a8ace-Abstract.html",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "University of Texas at Austin;National Taiwan University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.ntu.edu.tw",
        "aff_unique_abbr": "UT Austin;NTU",
        "aff_campus_unique_index": "0;1;1;0;0",
        "aff_campus_unique": "Austin;Taiwan",
        "aff_country_unique_index": "0;1;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Sparse Space-Time Deconvolution for Calcium Image Analysis",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4450",
        "id": "4450",
        "author_site": "Ferran Diego Andilla, Fred Hamprecht",
        "author": "Ferran Diego; Fred A. Hamprecht",
        "abstract": "We describe a unified formulation and algorithm to find an extremely sparse representation for Calcium image sequences in terms of cell locations, cell shapes, spike timings and impulse responses. Solution of a single optimization problem yields cell segmentations and activity estimates that are on par with the state of the art, without the need for heuristic pre- or postprocessing. Experiments on real and synthetic data demonstrate the viability of the proposed method.",
        "bibtex": "@inproceedings{NIPS2014_35ab33f5,\n author = {Diego, Ferran and Hamprecht, Fred A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Space-Time Deconvolution for Calcium Image Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/35ab33f5f9a61426560675e75c14cc0b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/35ab33f5f9a61426560675e75c14cc0b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/35ab33f5f9a61426560675e75c14cc0b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/35ab33f5f9a61426560675e75c14cc0b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/35ab33f5f9a61426560675e75c14cc0b-Reviews.html",
        "metareview": "",
        "pdf_size": 5520869,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12428807521660271247&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Heidelberg Collaboratory for Image Processing (HCI) + Interdisciplinary Center for Scienti\ufb01c Computing (IWR), University of Heidelberg, Heidelberg 69115, Germany; Heidelberg Collaboratory for Image Processing (HCI) + Interdisciplinary Center for Scienti\ufb01c Computing (IWR), University of Heidelberg, Heidelberg 69115, Germany",
        "aff_domain": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/35ab33f5f9a61426560675e75c14cc0b-Abstract.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Heidelberg Collaboratory for Image Processing;University of Heidelberg",
        "aff_unique_dep": "Image Processing;Interdisciplinary Center for Scienti\ufb01c Computing (IWR)",
        "aff_unique_url": "https://hci.iwr.uni-heidelberg.de/;https://www.uni-heidelberg.de",
        "aff_unique_abbr": "HCI;Uni Heidelberg",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Heidelberg",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Spatio-temporal Representations of Uncertainty in Spiking Neural Networks",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4452",
        "id": "4452",
        "author_site": "Cristina Savin, Sophie Den\u00e8ve",
        "author": "Cristina Savin; Sophie Deneve",
        "abstract": "It has been long argued that, because of inherent ambiguity and noise, the brain needs to represent uncertainty in the form of probability distributions. The neural encoding of such distributions remains however highly controversial. Here we present a novel circuit model for representing multidimensional real-valued distributions using a spike based spatio-temporal code. Our model combines the computational advantages of the currently competing models for probabilistic codes and exhibits realistic neural responses along a variety of classic measures. Furthermore, the model highlights the challenges associated with interpreting neural activity in relation to behavioral uncertainty and points to alternative population-level approaches for the experimental validation of distributed representations.",
        "bibtex": "@inproceedings{NIPS2014_02a12643,\n author = {Savin, Cristina and Deneve, Sophie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spatio-temporal Representations of Uncertainty in Spiking Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/02a12643ae21d984b93c9df82a9d2152-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/02a12643ae21d984b93c9df82a9d2152-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/02a12643ae21d984b93c9df82a9d2152-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/02a12643ae21d984b93c9df82a9d2152-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/02a12643ae21d984b93c9df82a9d2152-Reviews.html",
        "metareview": "",
        "pdf_size": 4711403,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2796954902087620286&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "IST Austria; Group for Neural Theory, ENS Paris",
        "aff_domain": "ist.ac.at;ens.fr",
        "email": "ist.ac.at;ens.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/02a12643ae21d984b93c9df82a9d2152-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Institute of Science and Technology Austria;\u00c9cole Normale Sup\u00e9rieure",
        "aff_unique_dep": ";Group for Neural Theory",
        "aff_unique_url": "https://www.ist.ac.at;https://www.ens.fr",
        "aff_unique_abbr": "IST Austria;ENS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Paris",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Austria;France"
    },
    {
        "title": "Spectral Clustering of graphs with the Bethe Hessian",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4771",
        "id": "4771",
        "author_site": "Alaa Saade, Florent Krzakala, Lenka Zdeborov\u00e1",
        "author": "Alaa Saade; Florent Krzakala; Lenka Zdeborov\u00e1",
        "abstract": "Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.",
        "bibtex": "@inproceedings{NIPS2014_d8c5fabf,\n author = {Saade, Alaa and Krzakala, Florent and Zdeborov\\'{a}, Lenka},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Clustering of graphs with the Bethe Hessian},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d8c5fabf1a4b215168274283f7c7562c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d8c5fabf1a4b215168274283f7c7562c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d8c5fabf1a4b215168274283f7c7562c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d8c5fabf1a4b215168274283f7c7562c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d8c5fabf1a4b215168274283f7c7562c-Reviews.html",
        "metareview": "",
        "pdf_size": 2014070,
        "gs_citation": 185,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2977266048761935102&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Laboratoire de Physique Statistique, CNRS UMR 8550 + \u00b4Ecole Normale Superieure, 24 Rue Lhomond Paris 75005; Sorbonne Universit \u00b4es, UPMC Univ Paris 06 + Laboratoire de Physique Statistique, CNRS UMR 8550 + \u00b4Ecole Normale Superieure, 24 Rue Lhomond Paris 75005; Institut de Physique Th \u00b4eorique + CEA Saclay and CNRS URA 2306 + 91191 Gif-sur-Yvette, France",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d8c5fabf1a4b215168274283f7c7562c-Abstract.html",
        "aff_unique_index": "0+1;2+0+1;3+4+5",
        "aff_unique_norm": "CNRS UMR 8550;Ecole Normale Superieure;Sorbonne Universit\u00e9s;Institut de Physique Th\u00e9orique;CEA Saclay;Institute for Research in Computer Science and Control (INRIA)",
        "aff_unique_dep": "Laboratoire de Physique Statistique;;;Physics Department;;",
        "aff_unique_url": "https://www.lps.ens.fr;https://www.ens.fr;https://www.sorbonne-universite.fr;https://www.ipt.univ-paris-diderot.fr;https://www.cea.fr;https://www.inria.fr",
        "aff_unique_abbr": "LPS;ENS;Sorbonne;IPT;CEA;INRIA",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Paris;Saclay",
        "aff_country_unique_index": "0+0;0+0+0;0+0+0",
        "aff_country_unique": "France"
    },
    {
        "title": "Spectral Learning of Mixture of Hidden Markov Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4772",
        "id": "4772",
        "author_site": "Cem Subakan, Johannes Traa, Paris Smaragdis",
        "author": "Y. Cem S\u00fcbakan; Johannes Traa; Paris Smaragdis",
        "abstract": "In this paper, we propose a learning approach for the Mixture of Hidden Markov Models (MHMM) based on the Method of Moments (MoM). Computational advantages of MoM make MHMM learning amenable for large data sets. It is not possible to directly learn an MHMM with existing learning approaches, mainly due to a permutation ambiguity in the estimation process. We show that it is possible to resolve this ambiguity using the spectral properties of a global transition matrix even in the presence of estimation noise. We demonstrate the validity of our approach on synthetic and real data.",
        "bibtex": "@inproceedings{NIPS2014_be9203c5,\n author = {S\\\"{u}bakan, Y. Cem and Traa, Johannes and Smaragdis, Paris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Learning of Mixture of Hidden Markov Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/be9203c5f264dfddffee62ce32a861bc-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/be9203c5f264dfddffee62ce32a861bc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/be9203c5f264dfddffee62ce32a861bc-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/be9203c5f264dfddffee62ce32a861bc-Reviews.html",
        "metareview": "",
        "pdf_size": 322582,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18297342933881560111&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign + Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign + Adobe Systems, Inc.",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/be9203c5f264dfddffee62ce32a861bc-Abstract.html",
        "aff_unique_index": "0;0;0+0+1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Adobe",
        "aff_unique_dep": "Department of Computer Science;Adobe Systems",
        "aff_unique_url": "https://illinois.edu;https://www.adobe.com",
        "aff_unique_abbr": "UIUC;Adobe",
        "aff_campus_unique_index": "0;0;0+0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0+0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Spectral Methods for Indian Buffet Process Inference",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4773",
        "id": "4773",
        "author_site": "Hsiao-Yu Tung, Alexander Smola",
        "author": "Hsiao-Yu Fish Tung; Alexander J. Smola",
        "abstract": "The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.",
        "bibtex": "@inproceedings{NIPS2014_219e596e,\n author = {Tung, Hsiao-Yu Fish and Smola, Alexander J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Methods for Indian Buffet Process Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/219e596e4af808699ce63a9f709e661c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/219e596e4af808699ce63a9f709e661c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/219e596e4af808699ce63a9f709e661c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/219e596e4af808699ce63a9f709e661c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/219e596e4af808699ce63a9f709e661c-Reviews.html",
        "metareview": "",
        "pdf_size": 782048,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5534204779714088017&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University + Google",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/219e596e4af808699ce63a9f709e661c-Abstract.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Carnegie Mellon University;Google",
        "aff_unique_dep": "Machine Learning Department;Google",
        "aff_unique_url": "https://www.cmu.edu;https://www.google.com",
        "aff_unique_abbr": "CMU;Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Spectral Methods for Supervised Topic Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4774",
        "id": "4774",
        "author_site": "Yining Wang, Jun Zhu",
        "author": "Yining Wang; Jun Zhu",
        "abstract": "Supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document. Existing inference methods are based on either variational approximation or Monte Carlo sampling. This paper presents a novel spectral decomposition algorithm to recover the parameters of supervised latent Dirichlet allocation (sLDA) models. The Spectral-sLDA algorithm is provably correct and computationally efficient. We prove a sample complexity bound and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on a diverse range of synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the algorithm.",
        "bibtex": "@inproceedings{NIPS2014_c0ea9db4,\n author = {Wang, Yining and Zhu, Jun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Methods for Supervised Topic Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c0ea9db4f03cbaf39f66960fd4fd3d19-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c0ea9db4f03cbaf39f66960fd4fd3d19-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c0ea9db4f03cbaf39f66960fd4fd3d19-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c0ea9db4f03cbaf39f66960fd4fd3d19-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c0ea9db4f03cbaf39f66960fd4fd3d19-Reviews.html",
        "metareview": "",
        "pdf_size": 461077,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17827814326529670443&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Machine Learning Department, Carnegie Mellon University; Dept. of Comp. Sci. & Tech. + Tsinghua National TNList Lab + State Key Lab of Intell. Tech. & Sys., Tsinghua University",
        "aff_domain": "cs.cmu.edu;mail.tsinghua.edu.cn",
        "email": "cs.cmu.edu;mail.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c0ea9db4f03cbaf39f66960fd4fd3d19-Abstract.html",
        "aff_unique_index": "0;1+2+2",
        "aff_unique_norm": "Carnegie Mellon University;University Affiliation;Tsinghua University",
        "aff_unique_dep": "Machine Learning Department;Department of Computer Science and Technology;National TNList Lab",
        "aff_unique_url": "https://www.cmu.edu;;http://www.tsinghua.edu.cn",
        "aff_unique_abbr": "CMU;;Tsinghua",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2+2",
        "aff_country_unique": "United States;;China"
    },
    {
        "title": "Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4454",
        "id": "4454",
        "author_site": "Yuchen Zhang, Xi Chen, Denny Zhou, Michael Jordan",
        "author": "Yuchen Zhang; Xi Chen; Dengyong Zhou; Michael I. Jordan",
        "abstract": "The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.",
        "bibtex": "@inproceedings{NIPS2014_700c82b0,\n author = {Zhang, Yuchen and Chen, Xi and Zhou, Dengyong and Jordan, Michael I.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/700c82b01b20a5e3e160dab2e511c48a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/700c82b01b20a5e3e160dab2e511c48a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/700c82b01b20a5e3e160dab2e511c48a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/700c82b01b20a5e3e160dab2e511c48a-Reviews.html",
        "metareview": "",
        "pdf_size": 314866,
        "gs_citation": 447,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12906368472896573310&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "University of California, Berkeley; New York University; Microsoft Research; University of California, Berkeley",
        "aff_domain": "berkeley.edu;nyu.edu;microsoft.com;berkeley.edu",
        "email": "berkeley.edu;nyu.edu;microsoft.com;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/700c82b01b20a5e3e160dab2e511c48a-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of California, Berkeley;New York University;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.nyu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;NYU;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Spectral k-Support Norm Regularization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4775",
        "id": "4775",
        "author_site": "Andrew McDonald, Massimiliano Pontil, Dimitris Stamos",
        "author": "Andrew M. McDonald; Massimiliano Pontil; Dimitris Stamos",
        "abstract": "The $k$-support norm has successfully been applied to sparse vector prediction problems. We observe that it belongs to a wider class of norms, which we call the box-norms. Within this framework we derive an efficient algorithm to compute the proximity operator of the squared norm, improving upon the original method for the $k$-support norm. We extend the norms from the vector to the matrix setting and we introduce the spectral $k$-support norm. We study its properties and show that it is closely related to the multitask learning cluster norm. We apply the norms to real and synthetic matrix completion datasets. Our findings indicate that spectral $k$-support norm regularization gives state of the art performance, consistently improving over trace norm regularization and the matrix elastic net.",
        "bibtex": "@inproceedings{NIPS2014_874b45af,\n author = {McDonald, Andrew M. and Pontil, Massimiliano and Stamos, Dimitris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral k-Support Norm Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/874b45af5674e42a16712f45e0bcf852-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/874b45af5674e42a16712f45e0bcf852-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/874b45af5674e42a16712f45e0bcf852-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/874b45af5674e42a16712f45e0bcf852-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/874b45af5674e42a16712f45e0bcf852-Reviews.html",
        "metareview": "",
        "pdf_size": 429391,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12010118821899777311&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University College London; Department of Computer Science, University College London; Department of Computer Science, University College London",
        "aff_domain": "cs.ucl.ac.uk;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/874b45af5674e42a16712f45e0bcf852-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Spike Frequency Adaptation Implements Anticipative Tracking in Continuous Attractor Neural Networks",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4777",
        "id": "4777",
        "author_site": "Yuanyuan Mi, C. C. Alan Fung, K. Y. Michael Wong, Si Wu",
        "author": "Yuanyuan Mi; C. C. Alan Fung; K. Y. Michael Wong; Si Wu",
        "abstract": "To extract motion information, the brain needs to compensate for time delays that are ubiquitous in neural signal transmission and processing. Here we propose a simple yet effective mechanism to implement anticipative tracking in neural systems. The proposed mechanism utilizes the property of spike-frequency adaptation (SFA), a feature widely observed in neuronal responses. We employ continuous attractor neural networks (CANNs) as the model to describe the tracking behaviors in neural systems. Incorporating SFA, a CANN exhibits intrinsic mobility, manifested by the ability of the CANN to hold self-sustained travelling waves. In tracking a moving stimulus, the interplay between the external drive and the intrinsic mobility of the network determines the tracking performance. Interestingly, we find that the regime of anticipation effectively coincides with the regime where the intrinsic speed of the travelling wave exceeds that of the external drive. Depending on the SFA amplitudes, the network can achieve either perfect tracking, with zero-lag to the input, or perfect anticipative tracking, with a constant leading time to the input. Our model successfully reproduces experimentally observed anticipative tracking behaviors, and sheds light on our understanding of how the brain processes motion information in a timely manner.",
        "bibtex": "@inproceedings{NIPS2014_57c76ace,\n author = {Mi, Yuanyuan and Fung, C. C. Alan and Wong, K. Y. Michael and Wu, Si},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spike Frequency Adaptation Implements Anticipative Tracking in Continuous Attractor Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/57c76ace5d28ad312d72e9d5923fb097-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/57c76ace5d28ad312d72e9d5923fb097-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/57c76ace5d28ad312d72e9d5923fb097-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/57c76ace5d28ad312d72e9d5923fb097-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/57c76ace5d28ad312d72e9d5923fb097-Reviews.html",
        "metareview": "",
        "pdf_size": 197882,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18261018011707223736&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "State Key Laboratory of Cognitive Neuroscience & Learning, Beijing Normal University, Beijing 100875, China; Department of Physics, The Hong Kong University of Science and Technology, Hong Kong; Department of Physics, The Hong Kong University of Science and Technology, Hong Kong; State Key Laboratory of Cognitive Neuroscience & Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing 100875, China",
        "aff_domain": "bnu.edu.cn;ust.hk;ust.hk;bnu.edu.cn",
        "email": "bnu.edu.cn;ust.hk;ust.hk;bnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/57c76ace5d28ad312d72e9d5923fb097-Abstract.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Beijing Normal University;Hong Kong University of Science and Technology",
        "aff_unique_dep": "State Key Laboratory of Cognitive Neuroscience & Learning;Department of Physics",
        "aff_unique_url": "http://www.bnu.edu.cn;https://www.ust.hk",
        "aff_unique_abbr": "BNU;HKUST",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Beijing;Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4778",
        "id": "4778",
        "author_site": "Deanna Needell, Rachel Ward, Nati Srebro",
        "author": "Deanna Needell; Nathan Srebro; Rachel Ward",
        "abstract": "We improve a recent gurantee of Bach and Moulines on the linear convergence of SGD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SGD can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.",
        "bibtex": "@inproceedings{NIPS2014_b3310bba,\n author = {Needell, Deanna and Srebro, Nathan and Ward, Rachel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b3310bba2be31e673a7ded3386994599-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/b3310bba2be31e673a7ded3386994599-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/b3310bba2be31e673a7ded3386994599-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/b3310bba2be31e673a7ded3386994599-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/b3310bba2be31e673a7ded3386994599-Reviews.html",
        "metareview": "",
        "pdf_size": 393944,
        "gs_citation": 755,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18238265102182480908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Mathematical Sciences, Claremont McKenna College; Toyota Technological Institute at Chicago+Dept. of Computer Science, Technion; Department of Mathematics, Univ. of Texas, Austin",
        "aff_domain": "cmc.edu;ttic.edu;math.utexas.edu",
        "email": "cmc.edu;ttic.edu;math.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/b3310bba2be31e673a7ded3386994599-Abstract.html",
        "aff_unique_index": "0;1+2;3",
        "aff_unique_norm": "Claremont McKenna College;Toyota Technological Institute at Chicago;Technion;University of Texas at Austin",
        "aff_unique_dep": "Department of Mathematical Sciences;;Dept. of Computer Science;Department of Mathematics",
        "aff_unique_url": "https://www.cmc.edu;https://www.tti-chicago.org;https://www.technion.ac.il;https://www.utexas.edu",
        "aff_unique_abbr": "CMC;TTI Chicago;Technion;UT Austin",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Chicago;Austin",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4779",
        "id": "4779",
        "author_site": "Omar Besbes, Yonatan Gur, Assaf Zeevi",
        "author": "Omar Besbes; Yonatan Gur; Assaf Zeevi",
        "abstract": "In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward variation\" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.\"",
        "bibtex": "@inproceedings{NIPS2014_91ba7292,\n author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/91ba7292e5388b90b58d0b839a7f19ec-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/91ba7292e5388b90b58d0b839a7f19ec-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/91ba7292e5388b90b58d0b839a7f19ec-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/91ba7292e5388b90b58d0b839a7f19ec-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/91ba7292e5388b90b58d0b839a7f19ec-Reviews.html",
        "metareview": "",
        "pdf_size": 353622,
        "gs_citation": 542,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13219429085202995782&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Columbia University; Stanford University; Columbia University",
        "aff_domain": "columbia.edu;stanford.edu;gsb.columbia.edu",
        "email": "columbia.edu;stanford.edu;gsb.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/91ba7292e5388b90b58d0b839a7f19ec-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Columbia University;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.stanford.edu",
        "aff_unique_abbr": "Columbia;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Stochastic Network Design in Bidirected Trees",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4780",
        "id": "4780",
        "author_site": "Xiaojian Wu, Daniel Sheldon, Shlomo Zilberstein",
        "author": "Xiaojian Wu; Daniel Sheldon; Shlomo Zilberstein",
        "abstract": "We investigate the problem of stochastic network design in bidirected trees. In this problem, an underlying phenomenon (e.g., a behavior, rumor, or disease) starts at multiple sources in a tree and spreads in both directions along its edges. Actions can be taken to increase the probability of propagation on edges, and the goal is to maximize the total amount of spread away from all sources. Our main result is a rounded dynamic programming approach that leads to a fully polynomial-time approximation scheme (FPTAS), that is, an algorithm that can find (1\u2212\u03b5)-optimal solutions for any problem instance in time polynomial in the input size and 1/\u03b5. Our algorithm outperforms competing approaches on a motivating problem from computational sustainability to remove barriers in river networks to restore the health of aquatic ecosystems.",
        "bibtex": "@inproceedings{NIPS2014_d2c39562,\n author = {Wu, Xiaojian and Sheldon, Daniel and Zilberstein, Shlomo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Network Design in Bidirected Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d2c39562e89531c3e11613fbce984d00-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/d2c39562e89531c3e11613fbce984d00-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/d2c39562e89531c3e11613fbce984d00-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/d2c39562e89531c3e11613fbce984d00-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/d2c39562e89531c3e11613fbce984d00-Reviews.html",
        "metareview": "",
        "pdf_size": 592625,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13230393459451373435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/d2c39562e89531c3e11613fbce984d00-Abstract.html"
    },
    {
        "title": "Stochastic Proximal Gradient Descent with Acceleration Techniques",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4781",
        "id": "4781",
        "author": "Atsushi Nitanda",
        "abstract": "Proximal gradient descent (PGD) and stochastic proximal gradient descent (SPGD) are popular methods for solving regularized risk minimization problems in machine learning and statistics. In this paper, we propose and analyze an accelerated variant of these methods in the mini-batch setting. This method incorporates two acceleration techniques: one is Nesterov's acceleration method, and the other is a variance reduction for the stochastic gradient. Accelerated proximal gradient descent (APG) and proximal stochastic variance reduction gradient (Prox-SVRG) are in a trade-off relationship. We show that our method, with the appropriate mini-batch size, achieves lower overall complexity than both APG and Prox-SVRG.",
        "bibtex": "@inproceedings{NIPS2014_2d6cd90d,\n author = {Nitanda, Atsushi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic Proximal Gradient Descent with Acceleration Techniques},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2d6cd90d4f3fa50e6d9bdbc81a2e3712-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2d6cd90d4f3fa50e6d9bdbc81a2e3712-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/2d6cd90d4f3fa50e6d9bdbc81a2e3712-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2d6cd90d4f3fa50e6d9bdbc81a2e3712-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2d6cd90d4f3fa50e6d9bdbc81a2e3712-Reviews.html",
        "metareview": "",
        "pdf_size": 298735,
        "gs_citation": 326,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8343899366758117443&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "NTT DATA Mathematical Systems Inc.",
        "aff_domain": "msi.co.jp",
        "email": "msi.co.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2d6cd90d4f3fa50e6d9bdbc81a2e3712-Abstract.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "NTT DATA Mathematical Systems Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntt-data.com/",
        "aff_unique_abbr": "NTT DATA MS",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Stochastic variational inference for hidden Markov models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4782",
        "id": "4782",
        "author_site": "Nick Foti, Jason Xu, Dillon Laird, Emily Fox",
        "author": "Nicholas J. Foti; Jason Xu; Dillon Laird; Emily B. Fox",
        "abstract": "Variational inference algorithms have proven successful for Bayesian analysis in large data settings, with recent advances using stochastic variational inference (SVI). However, such methods have largely been studied in independent or exchangeable data settings. We develop an SVI algorithm to learn the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations. We propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. We demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible.",
        "bibtex": "@inproceedings{NIPS2014_63ed2c35,\n author = {Foti, Nicholas J. and Xu, Jason and Laird, Dillon and Fox, Emily B.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic variational inference for hidden Markov models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/63ed2c35352753435bf749c40b0ce171-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/63ed2c35352753435bf749c40b0ce171-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/63ed2c35352753435bf749c40b0ce171-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/63ed2c35352753435bf749c40b0ce171-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/63ed2c35352753435bf749c40b0ce171-Reviews.html",
        "metareview": "",
        "pdf_size": 290919,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9717385134963624667&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Washington; University of Washington; University of Washington; University of Washington",
        "aff_domain": "stat.washington.edu;stat.washington.edu;cs.washington.edu;stat.washington.edu",
        "email": "stat.washington.edu;stat.washington.edu;cs.washington.edu;stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/63ed2c35352753435bf749c40b0ce171-Abstract.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Streaming, Memory Limited Algorithms for Community Detection",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4783",
        "id": "4783",
        "author_site": "Se-Young Yun, marc lelarge, Alexandre Proutiere",
        "author": "Se-Young. Yun; Marc Lelarge; Alexandre Proutiere",
        "abstract": "In this paper, we consider sparse networks consisting of a finite number of non-overlapping communities, i.e. disjoint clusters, so that there is higher density within clusters than across clusters. Both the intra- and inter-cluster edge densities vanish when the size of the graph grows large, making the cluster reconstruction problem nosier and hence difficult to solve. We are interested in scenarios where the network size is very large, so that the adjacency matrix of the graph is hard to manipulate and store. The data stream model in which columns of the adjacency matrix are revealed sequentially constitutes a natural framework in this setting. For this model, we develop two novel clustering algorithms that extract the clusters asymptotically accurately. The first algorithm is {\\it offline}, as it needs to store and keep the assignments of nodes to clusters, and requires a memory that scales linearly with the network size. The second algorithm is {\\it online}, as it may classify a node when the corresponding column is revealed and then discard this information. This algorithm requires a memory growing sub-linearly with the network size. To construct these efficient streaming memory-limited clustering algorithms, we first address the problem of clustering with partial information, where only a small proportion of the columns of the adjacency matrix is observed and develop, for this setting, a new spectral algorithm which is of independent interest.",
        "bibtex": "@inproceedings{NIPS2014_c308206b,\n author = {Yun, Se-Young. and Lelarge, Marc and Proutiere, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Streaming, Memory Limited Algorithms for Community Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c308206b8de48a9af6add45ee4c0f93d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c308206b8de48a9af6add45ee4c0f93d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/c308206b8de48a9af6add45ee4c0f93d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c308206b8de48a9af6add45ee4c0f93d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c308206b8de48a9af6add45ee4c0f93d-Reviews.html",
        "metareview": "",
        "pdf_size": 324037,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11189317358420475865&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "MSR-Inria; Inria & ENS; KTH, EE School / ACL",
        "aff_domain": "inria.fr;ens.fr;kth.se",
        "email": "inria.fr;ens.fr;kth.se",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c308206b8de48a9af6add45ee4c0f93d-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft;INRIA;KTH Royal Institute of Technology",
        "aff_unique_dep": "Microsoft Research - Inria;;Electrical Engineering School",
        "aff_unique_url": "https://www.msr-inria.fr;https://www.inria.fr;https://www.kth.se",
        "aff_unique_abbr": "MSR-Inria;Inria;KTH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "France;Sweden"
    },
    {
        "title": "Structure Regularization for Structured Prediction",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4476",
        "id": "4476",
        "author": "Xu Sun",
        "abstract": "While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via \\emph{structure decomposition}, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.",
        "bibtex": "@inproceedings{NIPS2014_a382aaaa,\n author = {Sun, Xu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structure Regularization for Structured Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a382aaaad2ab9462da85e3b36d5ccfc3-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/a382aaaad2ab9462da85e3b36d5ccfc3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/a382aaaad2ab9462da85e3b36d5ccfc3-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/a382aaaad2ab9462da85e3b36d5ccfc3-Reviews.html",
        "metareview": "",
        "pdf_size": 157279,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6267415059350832644&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "MOE Key Laboratory of Computational Linguistics, Peking University+School of Electronics Engineering and Computer Science, Peking University",
        "aff_domain": "pku.edu.cn",
        "email": "pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/a382aaaad2ab9462da85e3b36d5ccfc3-Abstract.html",
        "aff_unique_index": "0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "MOE Key Laboratory of Computational Linguistics",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Structure learning of antiferromagnetic Ising models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4784",
        "id": "4784",
        "author_site": "Guy Bresler, David Gamarnik, Devavrat Shah",
        "author": "Guy Bresler; David Gamarnik; Devavrat Shah",
        "abstract": "In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. Our first result is an unconditional computational lower bound of $\\Omega (p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree $d$, for the class of statistical algorithms recently introduced by Feldman et al. The construction is related to the notoriously difficult learning parities with noise problem in computational learning theory. Our lower bound shows that the $\\widetilde O(p^{d+2})$ runtime required by Bresler, Mossel, and Sly's exhaustive-search algorithm cannot be significantly improved without restricting the class of models. Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., most recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the \\emph{opposite} behavior: very strong repelling allows efficient learning in time $\\widetilde O(p^2)$. We provide an algorithm whose performance interpolates between $\\widetilde O(p^2)$ and $\\widetilde O(p^{d+2})$ depending on the strength of the repulsion.",
        "bibtex": "@inproceedings{NIPS2014_7d260e35,\n author = {Bresler, Guy and Gamarnik, David and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structure learning of antiferromagnetic Ising models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7d260e353ea63125030e2343c31f87f6-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7d260e353ea63125030e2343c31f87f6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7d260e353ea63125030e2343c31f87f6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7d260e353ea63125030e2343c31f87f6-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7d260e353ea63125030e2343c31f87f6-Reviews.html",
        "metareview": "",
        "pdf_size": 667320,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2339481736571207584&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Laboratory for Information and Decision Systems, Department of EECS; Sloan School of Management; Laboratory for Information and Decision Systems, Department of EECS",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7d260e353ea63125030e2343c31f87f6-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Submodular Attribute Selection for Action Recognition in Video",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4785",
        "id": "4785",
        "author_site": "Jingjing Zheng, Zhuolin Jiang, Rama Chellappa, Jonathon P Phillips",
        "author": "Jinging Zheng; Zhuolin Jiang; Rama Chellappa; P. Jonathon Phillips",
        "abstract": "In real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos. In this work, we encode actions based on attributes that describes actions as high-level concepts: \\textit{e.g.}, jump forward and motion in the air. We base our analysis on two types of action attributes. One type of action attributes is generated by humans. The second type is data-driven attributes, which is learned from data using dictionary learning methods. Attribute-based representation may exhibit high variance due to noisy and redundant attributes. We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set. Three attribute selection criteria are proposed and formulated as a submodular optimization problem. A greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum. Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.",
        "bibtex": "@inproceedings{NIPS2014_41738c37,\n author = {Zheng, Jinging and Jiang, Zhuolin and Chellappa, Rama and Phillips, P. Jonathon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Submodular Attribute Selection for Action Recognition in Video},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/41738c377959589c9634066a7220d6fa-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/41738c377959589c9634066a7220d6fa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/41738c377959589c9634066a7220d6fa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/41738c377959589c9634066a7220d6fa-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/41738c377959589c9634066a7220d6fa-Reviews.html",
        "metareview": "",
        "pdf_size": 341524,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7107982889045347642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "UMIACS, University of Maryland; Noah\u2019s Ark Lab, Huawei Technologies; UMIACS, University of Maryland; National Institute of Standards and Technology",
        "aff_domain": "umiacs.umd.edu;huawei.com;umiacs.umd.edu;nist.gov",
        "email": "umiacs.umd.edu;huawei.com;umiacs.umd.edu;nist.gov",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/41738c377959589c9634066a7220d6fa-Abstract.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Maryland;Huawei;National Institute of Standards and Technology",
        "aff_unique_dep": "UMIACS;Noah\u2019s Ark Lab;",
        "aff_unique_url": "https://www.umd.edu;https://www.huawei.com;https://www.nist.gov",
        "aff_unique_abbr": "UMD;Huawei;NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4392",
        "id": "4392",
        "author_site": "Adarsh Prasad, Stefanie Jegelka, Dhruv Batra",
        "author": "Adarsh Prasad; Stefanie Jegelka; Dhruv Batra",
        "abstract": "To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.",
        "bibtex": "@inproceedings{NIPS2014_8fcd0c8d,\n author = {Prasad, Adarsh and Jegelka, Stefanie and Batra, Dhruv},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/8fcd0c8d0e4335895172454b51bcc506-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/8fcd0c8d0e4335895172454b51bcc506-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/8fcd0c8d0e4335895172454b51bcc506-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/8fcd0c8d0e4335895172454b51bcc506-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/8fcd0c8d0e4335895172454b51bcc506-Reviews.html",
        "metareview": "",
        "pdf_size": 1854015,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1387140821282643759&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "UT Austin; UC Berkeley; Virginia Tech",
        "aff_domain": "cs.utexas.edu;eecs.berkeley.edu;vt.edu",
        "email": "cs.utexas.edu;eecs.berkeley.edu;vt.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/8fcd0c8d0e4335895172454b51bcc506-Abstract.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Texas at Austin;University of California, Berkeley;Virginia Tech",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utexas.edu;https://www.berkeley.edu;https://www.vt.edu",
        "aff_unique_abbr": "UT Austin;UC Berkeley;VT",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Austin;Berkeley;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Subspace Embeddings for the Polynomial Kernel",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4786",
        "id": "4786",
        "author_site": "Haim Avron, Huy Nguyen, David Woodruff",
        "author": "Haim Avron; Huy L. Nguy\u1ec5n; David P. Woodruff",
        "abstract": "Sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms. However, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation. We propose the first {\\em fast} oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel {\\em without} explicitly mapping the data to the high-dimensional space. In particular, we propose an embedding for mappings induced by the polynomial kernel. Using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel PCA of the data, as well as doing approximate kernel principal component regression.",
        "bibtex": "@inproceedings{NIPS2014_ad178e96,\n author = {Avron, Haim and Nguy\u1ec5n, Huy L. and Woodruff, David P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Subspace Embeddings for the Polynomial Kernel},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ad178e96136f045bbb98ea3d76e25e6c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ad178e96136f045bbb98ea3d76e25e6c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ad178e96136f045bbb98ea3d76e25e6c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ad178e96136f045bbb98ea3d76e25e6c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ad178e96136f045bbb98ea3d76e25e6c-Reviews.html",
        "metareview": "",
        "pdf_size": 234967,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6905112303777926709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "IBM T.J. Watson Research Center, Yorktown Heights, NY 10598; Simons Institute, UC Berkeley, Berkeley, CA 94720; IBM Almaden Research Center, San Jose, CA 95120",
        "aff_domain": "us.ibm.com;cs.princeton.edu;us.ibm.com",
        "email": "us.ibm.com;cs.princeton.edu;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ad178e96136f045bbb98ea3d76e25e6c-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "IBM;University of California, Berkeley",
        "aff_unique_dep": "IBM T.J. Watson Research Center;Simons Institute",
        "aff_unique_url": "https://www.ibm.com/research/watson;https://simons.berkeley.edu",
        "aff_unique_abbr": "IBM Watson;UC Berkeley",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Yorktown Heights;Berkeley;San Jose",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Testing Unfaithful Gaussian Graphical Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4787",
        "id": "4787",
        "author_site": "De Wen Soh, Sekhar C Tatikonda",
        "author": "De Wen Soh; Sekhar Tatikonda",
        "abstract": "The global Markov property for Gaussian graphical models ensures graph separation implies conditional independence. Specifically if a node set $S$ graph separates nodes $u$ and $v$ then $X_u$ is conditionally independent of $X_v$ given $X_S$. The opposite direction need not be true, that is, $X_u \\perp X_v \\mid X_S$ need not imply $S$ is a node separator of $u$ and $v$. When it does, the relation $X_u \\perp X_v \\mid X_S$ is called faithful. In this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form $X_i \\perp X_j \\mid X_S$.",
        "bibtex": "@inproceedings{NIPS2014_f32e9b47,\n author = {Soh, De Wen and Tatikonda, Sekhar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Testing Unfaithful Gaussian Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f32e9b47b8956b8204eb644b14203ce5-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/f32e9b47b8956b8204eb644b14203ce5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/f32e9b47b8956b8204eb644b14203ce5-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/f32e9b47b8956b8204eb644b14203ce5-Reviews.html",
        "metareview": "",
        "pdf_size": 296439,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16728555642535490447&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Electrical Engineering, Yale University; Department of Electrical Engineering, Yale University",
        "aff_domain": "yale.edu;yale.edu",
        "email": "yale.edu;yale.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/f32e9b47b8956b8204eb644b14203ce5-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4640",
        "id": "4640",
        "author_site": "Been Kim, Cynthia Rudin, Julie A Shah",
        "author": "Been Kim; Cynthia Rudin; Julie Shah",
        "abstract": "We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the ``quintessential observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.\"",
        "bibtex": "@inproceedings{NIPS2014_411a58eb,\n author = {Kim, Been and Rudin, Cynthia and Shah, Julie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/411a58ebfbf82c8aedcb8a00d0d72e88-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/411a58ebfbf82c8aedcb8a00d0d72e88-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/411a58ebfbf82c8aedcb8a00d0d72e88-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/411a58ebfbf82c8aedcb8a00d0d72e88-Reviews.html",
        "metareview": "",
        "pdf_size": 4789368,
        "gs_citation": 426,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17713576787819236425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/411a58ebfbf82c8aedcb8a00d0d72e88-Abstract.html"
    },
    {
        "title": "The Blinded Bandit: Learning with Adaptive Feedback",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4788",
        "id": "4788",
        "author_site": "Ofer Dekel, Elad Hazan, Tomer Koren",
        "author": "Ofer Dekel; Elad Hazan; Tomer Koren",
        "abstract": "We study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action. Such model of \\emph{adaptive feedback} naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions. This motivates a variant of the multi-armed bandit problem, which we call the \\emph{blinded multi-armed bandit}, in which no feedback is given to the algorithm whenever it switches arms. We develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem. This result stands in stark contrast to another recent result, which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn, and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem. We also extend our results to the general prediction framework of bandit linear optimization, again attaining near-optimal regret bounds.",
        "bibtex": "@inproceedings{NIPS2014_52c102a0,\n author = {Dekel, Ofer and Hazan, Elad and Koren, Tomer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Blinded Bandit: Learning with Adaptive Feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/52c102a09f8e2e1c1cfc3c379c787e5f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/52c102a09f8e2e1c1cfc3c379c787e5f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/52c102a09f8e2e1c1cfc3c379c787e5f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/52c102a09f8e2e1c1cfc3c379c787e5f-Reviews.html",
        "metareview": "",
        "pdf_size": 250975,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8250374427458951089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Microsoft Research; Technion; Technion",
        "aff_domain": "microsoft.com;ie.technion.ac.il;technion.ac.il",
        "email": "microsoft.com;ie.technion.ac.il;technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/52c102a09f8e2e1c1cfc3c379c787e5f-Abstract.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Microsoft;Technion - Israel Institute of Technology",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "MSR;Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "The Infinite Mixture of Infinite Gaussian Mixtures",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4789",
        "id": "4789",
        "author_site": "Halid Z Yerebakan, Bartek Rajwa, Murat Dundar",
        "author": "Halid Z. Yerebakan; Bartek Rajwa; Murat Dundar",
        "abstract": "Dirichlet process mixture of Gaussians (DPMG) has been used in the literature for clustering and density estimation problems. However, many real-world data exhibit cluster distributions that cannot be captured by a single Gaussian. Modeling such data sets by DPMG creates several extraneous clusters even when clusters are relatively well-defined. Herein, we present the infinite mixture of infinite Gaussian mixtures (I2GMM) for more flexible modeling of data sets with skewed and multi-modal cluster distributions. Instead of using a single Gaussian for each cluster as in the standard DPMG model, the generative model of I2GMM uses a single DPMG for each cluster. The individual DPMGs are linked together through centering of their base distributions at the atoms of a higher level DP prior. Inference is performed by a collapsed Gibbs sampler that also enables partial parallelization. Experimental results on several artificial and real-world data sets suggest the proposed I2GMM model can predict clusters more accurately than existing variational Bayes and Gibbs sampler versions of DPMG.",
        "bibtex": "@inproceedings{NIPS2014_5b47430e,\n author = {Yerebakan, Halid Z. and Rajwa, Bartek and Dundar, Murat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Infinite Mixture of Infinite Gaussian Mixtures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5b47430e24a5a1f9fe21f0e8eb814131-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5b47430e24a5a1f9fe21f0e8eb814131-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5b47430e24a5a1f9fe21f0e8eb814131-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5b47430e24a5a1f9fe21f0e8eb814131-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5b47430e24a5a1f9fe21f0e8eb814131-Reviews.html",
        "metareview": "",
        "pdf_size": 735422,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3392140621547768893&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer and Information Science, IUPUI; Bindley Bioscience Center, Purdue University; Department of Computer and Information Science, IUPUI",
        "aff_domain": "cs.iupui.edu;cyto.purdue.edu;cs.iupui.edu",
        "email": "cs.iupui.edu;cyto.purdue.edu;cs.iupui.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5b47430e24a5a1f9fe21f0e8eb814131-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indiana University-Purdue University Indianapolis;Purdue University",
        "aff_unique_dep": "Department of Computer and Information Science;Bindley Bioscience Center",
        "aff_unique_url": "https://www.iupui.edu;https://www.purdue.edu",
        "aff_unique_abbr": "IUPUI;Purdue",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Indianapolis;West Lafayette",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Large Margin Mechanism for Differentially Private Maximization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4790",
        "id": "4790",
        "author_site": "Kamalika Chaudhuri, Daniel Hsu, Shuang Song",
        "author": "Kamalika Chaudhuri; Daniel Hsu; Shuang Song",
        "abstract": "A basic problem in the design of privacy-preserving algorithms is the \\emph{private maximization problem}: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine learning. Previous algorithms for this problem are either range-dependent---i.e., their utility diminishes with the size of the universe---or only apply to very restricted function classes. This work provides the first general purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning.",
        "bibtex": "@inproceedings{NIPS2014_87fbbd93,\n author = {Chaudhuri, Kamalika and Hsu, Daniel and Song, Shuang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Large Margin Mechanism for Differentially Private Maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/87fbbd938c54df75c6adbe0345510c5d-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/87fbbd938c54df75c6adbe0345510c5d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/87fbbd938c54df75c6adbe0345510c5d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/87fbbd938c54df75c6adbe0345510c5d-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/87fbbd938c54df75c6adbe0345510c5d-Reviews.html",
        "metareview": "",
        "pdf_size": 239499,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16429549431361857458&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "UC San Diego; Columbia University; UC San Diego",
        "aff_domain": "cs.ucsd.edu;cs.columbia.edu;eng.ucsd.edu",
        "email": "cs.ucsd.edu;cs.columbia.edu;eng.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/87fbbd938c54df75c6adbe0345510c5d-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UCSD;Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Noisy Power Method: A Meta Algorithm with Applications",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4456",
        "id": "4456",
        "author_site": "Moritz Hardt, Eric Price",
        "author": "Moritz Hardt; Eric Price",
        "abstract": "We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call noisy power method. Our result characterizes the convergence behavior of the algorithm when a large amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (PCA), and privacy-preserving spectral analysis. Our general analysis subsumes several existing ad-hoc convergence bounds and resolves a number of open problems in multiple applications. A recent work of Mitliagkas et al.~(NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. As a second application, we provide an algorithm for differentially private principal component analysis that runs in nearly linear time in the input sparsity and achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013) and leads to strong average-case improvements over the optimal worst-case bound.",
        "bibtex": "@inproceedings{NIPS2014_3c8a075d,\n author = {Hardt, Moritz and Price, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Noisy Power Method: A Meta Algorithm with Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3c8a075d9a17aa1ec1146f92e1eede8f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3c8a075d9a17aa1ec1146f92e1eede8f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3c8a075d9a17aa1ec1146f92e1eede8f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3c8a075d9a17aa1ec1146f92e1eede8f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3c8a075d9a17aa1ec1146f92e1eede8f-Reviews.html",
        "metareview": "",
        "pdf_size": 314607,
        "gs_citation": 251,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14374628759548987968&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "IBM Research Almaden; IBM Research Almaden",
        "aff_domain": "us.ibm.com;cs.utexas.edu",
        "email": "us.ibm.com;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3c8a075d9a17aa1ec1146f92e1eede8f-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Almaden",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The limits of squared Euclidean distance regularization",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4460",
        "id": "4460",
        "author_site": "Michal Derezinski, Manfred K. Warmuth",
        "author": "Micha\u0142 Derezi\u0144ski; Manfred K. Warmuth",
        "abstract": "Some of the simplest loss functions considered in Machine Learning are the square loss, the logistic loss and the hinge loss. The most common family of algorithms, including Gradient Descent (GD) with and without Weight Decay, always predict with a linear combination of the past instances. We give a random construction for sets of examples where the target linear weight vector is trivial to learn but any algorithm from the above family is drastically sub-optimal. Our lower bound on the latter algorithms holds even if the algorithms are enhanced with an arbitrary kernel function. This type of result was known for the square loss. However, we develop new techniques that let us prove such hardness results for any loss function satisfying some minimal requirements on the loss function (including the three listed above). We also show that algorithms that regularize with the squared Euclidean distance are easily confused by random features. Finally, we conclude by discussing related open problems regarding feed forward neural networks. We conjecture that our hardness results hold for any training algorithm that is based on the squared Euclidean distance regularization (i.e. Back-propagation with the Weight Decay heuristic).",
        "bibtex": "@inproceedings{NIPS2014_c61b9b81,\n author = {Derezi\\'{n}ski, Micha\\l  and Warmuth, Manfred K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The limits of squared Euclidean distance regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c61b9b81b2692c80441a81534977a22a-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/c61b9b81b2692c80441a81534977a22a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/c61b9b81b2692c80441a81534977a22a-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/c61b9b81b2692c80441a81534977a22a-Reviews.html",
        "metareview": "",
        "pdf_size": 1021137,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11332575787999639377&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, University of California, Santa Cruz; Computer Science Department, University of California, Santa Cruz",
        "aff_domain": "soe.ucsc.edu;cse.ucsc.edu",
        "email": "soe.ucsc.edu;cse.ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/c61b9b81b2692c80441a81534977a22a-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Santa Cruz",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.ucsc.edu",
        "aff_unique_abbr": "UCSC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Santa Cruz",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Tight Bounds for Influence in Diffusion Networks and Application to Bond Percolation and Epidemiology",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4792",
        "id": "4792",
        "author_site": "Remi Lemonnier, Kevin Scaman, Nicolas Vayatis",
        "author": "R\u00e9mi Lemonnier; Kevin Scaman; Nicolas Vayatis",
        "abstract": "In this paper, we derive theoretical bounds for the long-term influence of a node in an Independent Cascade Model (ICM). We relate these bounds to the spectral radius of a particular matrix and show that the behavior is sub-critical when this spectral radius is lower than 1. More specifically, we point out that, in general networks, the sub-critical regime behaves in O(sqrt(n)) where n is the size of the network, and that this upper bound is met for star-shaped networks. We apply our results to epidemiology and percolation on arbitrary networks, and derive a bound for the critical value beyond which a giant connected component arises. Finally, we show empirically the tightness of our bounds for a large family of networks.",
        "bibtex": "@inproceedings{NIPS2014_2e4ffe19,\n author = {Lemonnier, R\\'{e}mi and Scaman, Kevin and Vayatis, Nicolas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tight Bounds for Influence in Diffusion Networks and Application to Bond Percolation and Epidemiology},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/2e4ffe197475393c15c92fdfb1820cbd-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/2e4ffe197475393c15c92fdfb1820cbd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/2e4ffe197475393c15c92fdfb1820cbd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/2e4ffe197475393c15c92fdfb1820cbd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/2e4ffe197475393c15c92fdfb1820cbd-Reviews.html",
        "metareview": "",
        "pdf_size": 474223,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1334743800492872499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "CMLA \u2013 ENS Cachan, CNRS, France+1000mercis, Paris, France; CMLA \u2013 ENS Cachan, CNRS, France; CMLA \u2013 ENS Cachan, CNRS, France",
        "aff_domain": "cmla.ens-cachan.fr;cmla.ens-cachan.fr;cmla.ens-cachan.fr",
        "email": "cmla.ens-cachan.fr;cmla.ens-cachan.fr;cmla.ens-cachan.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/2e4ffe197475393c15c92fdfb1820cbd-Abstract.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure de Cachan;1000mercis",
        "aff_unique_dep": "CMLA;",
        "aff_unique_url": "https://www.ens-cachan.fr;",
        "aff_unique_abbr": "ENS Cachan;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cachan;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Tight Continuous Relaxation of the Balanced k-Cut Problem",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4793",
        "id": "4793",
        "author_site": "Syama Sundar Rangapuram, Pramod Kaushik Mudrakarta, Matthias Hein",
        "author": "Syama Sundar Rangapuram; Pramod Kaushik Mudrakarta; Matthias Hein",
        "abstract": "Spectral Clustering as a relaxation of the normalized/ratio cut has become one of the standard graph-based clustering methods. Existing methods for the computation of multiple clusters, corresponding to a balanced k-cut of the graph, are either based on greedy techniques or heuristics which have weak connection to the original motivation of minimizing the normalized cut. In this paper we propose a new tight continuous relaxation for any balanced k-cut problem and show that a related recently proposed relaxation is in most cases loose leading to poor performance in practice. For the optimization of our tight continuous relaxation we propose a new algorithm for the hard sum-of-ratios minimization problem which achieves monotonic descent. Extensive comparisons show that our method beats all existing approaches for ratio cut and other balanced k-cut criteria.",
        "bibtex": "@inproceedings{NIPS2014_5cb84044,\n author = {Rangapuram, Syama Sundar and Mudrakarta, Pramod Kaushik and Hein, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tight Continuous Relaxation of the Balanced k-Cut Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5cb840444ee20e2cdc6f15c10cfed80b-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/5cb840444ee20e2cdc6f15c10cfed80b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/5cb840444ee20e2cdc6f15c10cfed80b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/5cb840444ee20e2cdc6f15c10cfed80b-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/5cb840444ee20e2cdc6f15c10cfed80b-Reviews.html",
        "metareview": "",
        "pdf_size": 374744,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1908646122268611751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/5cb840444ee20e2cdc6f15c10cfed80b-Abstract.html"
    },
    {
        "title": "Tight convex relaxations for sparse matrix factorization",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4794",
        "id": "4794",
        "author_site": "Emile Richard, Guillaume R Obozinski, Jean-Philippe Vert",
        "author": "Emile Richard; Guillaume Obozinski; Jean-Philippe Vert",
        "abstract": "Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l_1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.",
        "bibtex": "@inproceedings{NIPS2014_3eb3113f,\n author = {Richard, Emile and Obozinski, Guillaume and Vert, Jean-Philippe},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tight convex relaxations for sparse matrix factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3eb3113f9c4c90df1deb5ae5c209e96f-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3eb3113f9c4c90df1deb5ae5c209e96f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/3eb3113f9c4c90df1deb5ae5c209e96f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3eb3113f9c4c90df1deb5ae5c209e96f-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3eb3113f9c4c90df1deb5ae5c209e96f-Reviews.html",
        "metareview": "",
        "pdf_size": 339358,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15649605814195501907&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3eb3113f9c4c90df1deb5ae5c209e96f-Abstract.html"
    },
    {
        "title": "Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4795",
        "id": "4795",
        "author_site": "Zhaoran Wang, Huanran Lu, Han Liu",
        "author": "Zhaoran Wang; Huanran Lu; Han Liu",
        "abstract": "We provide statistical and computational analysis of sparse Principal Component Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex in nature. Consequently, though its global solution attains the optimal statistical rate of convergence, such solution is computationally intractable to obtain. Meanwhile, although its convex relaxations are tractable to compute, they yield estimators with suboptimal statistical rates of convergence. On the other hand, existing nonconvex optimization procedures, such as greedy methods, lack statistical guarantees. In this paper, we propose a two-stage sparse PCA procedure that attains the optimal principal subspace estimator in polynomial time. The main stage employs a novel algorithm named sparse orthogonal iteration pursuit, which iteratively solves the underlying nonconvex problem. However, our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region, namely the basin of attraction. To obtain the desired initial estimator that falls into this region, we solve a convex formulation of sparse PCA with early stopping. Under an integrated analytic framework, we simultaneously characterize the computational and statistical performance of this two-stage procedure. Computationally, our procedure converges at the rate of $1/\\sqrt{t}$ within the initialization stage, and at a geometric rate within the main stage. Statistically, the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level $s^*$, dimension $d$ and sample size $n$. Our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees.",
        "bibtex": "@inproceedings{NIPS2014_3c8902fd,\n author = {Wang, Zhaoran and Lu, Huanran and Liu, Han},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/3c8902fdf42e2080de33dde7da2b52bd-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/3c8902fdf42e2080de33dde7da2b52bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/3c8902fdf42e2080de33dde7da2b52bd-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/3c8902fdf42e2080de33dde7da2b52bd-Reviews.html",
        "metareview": "",
        "pdf_size": 783278,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3805069455111085143&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/3c8902fdf42e2080de33dde7da2b52bd-Abstract.html"
    },
    {
        "title": "Time--Data Tradeoffs by Aggressive Smoothing",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4796",
        "id": "4796",
        "author_site": "John J Bruer, Joel A Tropp, Volkan Cevher, Stephen Becker",
        "author": "John J. Bruer; Joel A. Tropp; Volkan Cevher; Stephen R. Becker",
        "abstract": "This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.",
        "bibtex": "@inproceedings{NIPS2014_52b214f2,\n author = {Bruer, John J. and Tropp, Joel A. and Cevher, Volkan and Becker, Stephen R.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Time--Data Tradeoffs by Aggressive Smoothing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/52b214f23af6926aa3cc4936814bf6fb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/52b214f23af6926aa3cc4936814bf6fb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/52b214f23af6926aa3cc4936814bf6fb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/52b214f23af6926aa3cc4936814bf6fb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/52b214f23af6926aa3cc4936814bf6fb-Reviews.html",
        "metareview": "",
        "pdf_size": 524075,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4071387121565814944&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Dept. of Computing +Mathematical Sciences, California Institute of Technology; Dept. of Computing +Mathematical Sciences, California Institute of Technology; Laboratory for Information and Inference Systems, EPFL; Dept. of Applied Mathematics, University of Colorado at Boulder",
        "aff_domain": "cms.caltech.edu; ; ; ",
        "email": "cms.caltech.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/52b214f23af6926aa3cc4936814bf6fb-Abstract.html",
        "aff_unique_index": "0+1;0+1;2;3",
        "aff_unique_norm": "University Affiliation Not Specified;California Institute of Technology;EPFL;University of Colorado at Boulder",
        "aff_unique_dep": "Department of Computing;Mathematical Sciences;Laboratory for Information and Inference Systems;Dept. of Applied Mathematics",
        "aff_unique_url": ";https://www.caltech.edu;https://www.epfl.ch;https://www.colorado.edu",
        "aff_unique_abbr": ";Caltech;EPFL;CU Boulder",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Pasadena;Boulder",
        "aff_country_unique_index": "1;1;2;1",
        "aff_country_unique": ";United States;Switzerland"
    },
    {
        "title": "Top Rank Optimization in Linear Time",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4797",
        "id": "4797",
        "author_site": "Nan Li, Rong Jin, Zhi-Hua Zhou",
        "author": "Nan Li; Rong Jin; Zhi-Hua Zhou",
        "abstract": "Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the rank loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.",
        "bibtex": "@inproceedings{NIPS2014_1f1f37ef,\n author = {Li, Nan and Jin, Rong and Zhou, Zhi-Hua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Top Rank Optimization in Linear Time},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1f1f37ef046902cfd7abecc00f2fc9af-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1f1f37ef046902cfd7abecc00f2fc9af-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1f1f37ef046902cfd7abecc00f2fc9af-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1f1f37ef046902cfd7abecc00f2fc9af-Reviews.html",
        "metareview": "",
        "pdf_size": 423896,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7396350062184475525&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China",
        "aff_domain": "lamda.nju.edu.cn;cse.msu.edu;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;cse.msu.edu;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1f1f37ef046902cfd7abecc00f2fc9af-Abstract.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Nanjing University;Michigan State University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;Department of Computer Science and Engineering",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.msu.edu",
        "aff_unique_abbr": "Nanjing U;MSU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Nanjing;East Lansing",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Transportability from Multiple Environments with Limited Experiments: Completeness Results",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4464",
        "id": "4464",
        "author_site": "Elias Bareinboim, Judea Pearl",
        "author": "Elias Bareinboim; Judea Pearl",
        "abstract": "This paper addresses the problem of $mz$-transportability, that is, transferring causal knowledge collected in several heterogeneous domains to a target domain in which only passive observations and limited experimental data can be collected. The paper first establishes a necessary and sufficient condition for deciding the feasibility of $mz$-transportability, i.e., whether causal effects in the target domain are estimable from the information available. It further proves that a previously established algorithm for computing transport formula is in fact complete, that is, failure of the algorithm implies non-existence of a transport formula. Finally, the paper shows that the do-calculus is complete for the $mz$-transportability class.",
        "bibtex": "@inproceedings{NIPS2014_29d8ab58,\n author = {Bareinboim, Elias and Pearl, Judea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Transportability from Multiple Environments with Limited Experiments: Completeness Results},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/29d8ab58bcd65e45a831feeaed051d23-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/29d8ab58bcd65e45a831feeaed051d23-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/29d8ab58bcd65e45a831feeaed051d23-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/29d8ab58bcd65e45a831feeaed051d23-Reviews.html",
        "metareview": "",
        "pdf_size": 279781,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15827354709617467002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science, UCLA; Computer Science, UCLA",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/29d8ab58bcd65e45a831feeaed051d23-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Tree-structured Gaussian Process Approximations",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4466",
        "id": "4466",
        "author_site": "Thang Bui, Richard Turner",
        "author": "Thang Bui; Richard Turner",
        "abstract": "Gaussian process regression can be accelerated by constructing a small pseudo-dataset to summarise the observed data. This idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained. This presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size. In this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints. This is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a Kullback-Leibler (KL) minimisation. Inference and learning can then be performed efficiently using the Gaussian belief propagation algorithm. We demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets. We trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.",
        "bibtex": "@inproceedings{NIPS2014_82666573,\n author = {Bui, Thang and Turner, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tree-structured Gaussian Process Approximations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/82666573a017e318d6c9f3d499f35eba-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/82666573a017e318d6c9f3d499f35eba-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/82666573a017e318d6c9f3d499f35eba-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/82666573a017e318d6c9f3d499f35eba-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/82666573a017e318d6c9f3d499f35eba-Reviews.html",
        "metareview": "",
        "pdf_size": 1032956,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10224721689030052182&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computational and Biological Learning Lab, Department of Engineering, University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK; Computational and Biological Learning Lab, Department of Engineering, University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/82666573a017e318d6c9f3d499f35eba-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4468",
        "id": "4468",
        "author_site": "Jie Wang, Jieping Ye",
        "author": "Jie Wang; Jieping Ye",
        "abstract": "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l1 and l2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. Experiments on both synthetic and real data sets show that TLFre improves the efficiency of SGL by orders of magnitude.",
        "bibtex": "@inproceedings{NIPS2014_dbea76ed,\n author = {Wang, Jie and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/dbea76ed2c0d0e42a285ac78fbe8e57e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/dbea76ed2c0d0e42a285ac78fbe8e57e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/dbea76ed2c0d0e42a285ac78fbe8e57e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/dbea76ed2c0d0e42a285ac78fbe8e57e-Reviews.html",
        "metareview": "",
        "pdf_size": 553207,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15774210519129685637&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Computer Science and Engineering, Arizona State University, Tempe, AZ 85287; Computer Science and Engineering, Arizona State University, Tempe, AZ 85287",
        "aff_domain": "asu.edu;asu.edu",
        "email": "asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/dbea76ed2c0d0e42a285ac78fbe8e57e-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tempe",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4470",
        "id": "4470",
        "author_site": "Karen Simonyan, Andrew Zisserman",
        "author": "Karen Simonyan; Andrew Zisserman",
        "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.",
        "bibtex": "@inproceedings{NIPS2014_ca007296,\n author = {Simonyan, Karen and Zisserman, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Two-Stream Convolutional Networks for Action Recognition in Videos},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ca007296a63f7d1721a2399d56363022-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/ca007296a63f7d1721a2399d56363022-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/ca007296a63f7d1721a2399d56363022-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/ca007296a63f7d1721a2399d56363022-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/ca007296a63f7d1721a2399d56363022-Reviews.html",
        "metareview": "",
        "pdf_size": 705462,
        "gs_citation": 10105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=582514008712420788&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 21,
        "aff": "Visual Geometry Group, University of Oxford; Visual Geometry Group, University of Oxford",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/ca007296a63f7d1721a2399d56363022-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Visual Geometry Group",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Universal Option Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4798",
        "id": "4798",
        "author_site": "hengshuai yao, Csaba Szepesvari, Richard Sutton, Joseph Modayil, Shalabh Bhatnagar",
        "author": "Hengshuai Yao; Csaba Szepesv\u00e1ri; Rich Sutton; Joseph Modayil; Shalabh Bhatnagar",
        "abstract": "We consider the problem of learning models of options for real-time abstract planning, in the setting where reward functions can be specified at any time and their expected returns must be efficiently computed. We introduce a new model for an option that is independent of any reward function, called the {\\it universal option model (UOM)}. We prove that the UOM of an option can construct a traditional option model given a reward function, and the option-conditional return is computed directly by a single dot-product of the UOM with the reward function. We extend the UOM to linear function approximation, and we show it gives the TD solution of option returns and value functions of policies over options. We provide a stochastic approximation algorithm for incrementally learning UOMs from data and prove its consistency. We demonstrate our method in two domains. The first domain is document recommendation, where each user query defines a new reward function and a document's relevance is the expected return of a simulated random-walk through the document's references. The second domain is a real-time strategy game, where the controller must select the best game unit to accomplish dynamically-specified tasks. Our experiments show that UOMs are substantially more efficient in evaluating option returns and policies than previously known methods.",
        "bibtex": "@inproceedings{NIPS2014_7648cbdb,\n author = {Yao, Hengshuai and Szepesv\\'{a}ri, Csaba and Sutton, Rich and Modayil, Joseph and Bhatnagar, Shalabh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Universal Option Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7648cbdbcb6244de84ed49f817669f0e-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7648cbdbcb6244de84ed49f817669f0e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7648cbdbcb6244de84ed49f817669f0e-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7648cbdbcb6244de84ed49f817669f0e-Reviews.html",
        "metareview": "",
        "pdf_size": 826082,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5239983766434152201&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Computer Science and Automation, Indian Institute of Science",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;csa.iisc.ernet.in",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7648cbdbcb6244de84ed49f817669f0e-Abstract.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "University of Alberta;Indian Institute of Science",
        "aff_unique_dep": "Department of Computing Science;Department of Computer Science and Automation",
        "aff_unique_url": "https://www.ualberta.ca;https://www.iisc.ac.in",
        "aff_unique_abbr": "UAlberta;IISc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Canada;India"
    },
    {
        "title": "Unsupervised Deep Haar Scattering on Graphs",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4799",
        "id": "4799",
        "author_site": "Xu Chen, Xiuyuan Cheng, Stephane Mallat",
        "author": "Xu Chen; Xiuyuan Cheng; St\u00e9phane Mallat",
        "abstract": "The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.",
        "bibtex": "@inproceedings{NIPS2014_34fde013,\n author = {Chen, Xu and Cheng, Xiuyuan and Mallat, St\\'{e}phane},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Deep Haar Scattering on Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/34fde01345258939e718af181fc0f996-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/34fde01345258939e718af181fc0f996-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/34fde01345258939e718af181fc0f996-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/34fde01345258939e718af181fc0f996-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/34fde01345258939e718af181fc0f996-Reviews.html",
        "metareview": "",
        "pdf_size": 1603591,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3410759322340359503&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical Engineering, Princeton University, NJ, USA+D\u00b4epartement d\u2019Informatique, \u00b4Ecole Normale Sup \u00b4erieure, Paris, France; D\u00b4epartement d\u2019Informatique, \u00b4Ecole Normale Sup \u00b4erieure, Paris, France; D\u00b4epartement d\u2019Informatique, \u00b4Ecole Normale Sup \u00b4erieure, Paris, France",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/34fde01345258939e718af181fc0f996-Abstract.html",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Princeton University;Ecole Normale Sup\u00e9rieure",
        "aff_unique_dep": "Department of Electrical Engineering;D\u00e9partement d\u2019Informatique",
        "aff_unique_url": "https://www.princeton.edu;https://www.ens.fr",
        "aff_unique_abbr": "Princeton;ENS",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Paris",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Unsupervised Transcription of Piano Music",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4472",
        "id": "4472",
        "author_site": "Taylor Berg-Kirkpatrick, Jacob Andreas, Dan Klein",
        "author": "Taylor Berg-Kirkpatrick; Jacob Andreas; Dan Klein",
        "abstract": "We present a new probabilistic model for transcribing piano music from audio to a symbolic form. Our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data. As a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony. In order to adapt to the properties of a new instrument or acoustic environment being transcribed, we learn recording specific spectral profiles and temporal envelopes in an unsupervised fashion. Our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset F1 on real piano audio.",
        "bibtex": "@inproceedings{NIPS2014_e51e118b,\n author = {Berg-Kirkpatrick, Taylor and Andreas, Jacob and Klein, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Transcription of Piano Music},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/e51e118b28def0cabd5237b4e1fec499-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/e51e118b28def0cabd5237b4e1fec499-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/e51e118b28def0cabd5237b4e1fec499-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/e51e118b28def0cabd5237b4e1fec499-Reviews.html",
        "metareview": "",
        "pdf_size": 820223,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11833513225932733050&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/e51e118b28def0cabd5237b4e1fec499-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised learning of an efficient short-term memory network",
        "status": "Spotlight",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4474",
        "id": "4474",
        "author_site": "Pietro Vertechi, Wieland Brendel, Christian Machens",
        "author": "Pietro Vertechi; Wieland Brendel; Christian K. Machens",
        "abstract": "Learning in recurrent neural networks has been a topic fraught with difficulties and problems. We here report substantial progress in the unsupervised learning of recurrent networks that can keep track of an input signal. Specifically, we show how these networks can learn to efficiently represent their present and past inputs, based on local learning rules only. Our results are based on several key insights. First, we develop a local learning rule for the recurrent weights whose main aim is to drive the network into a regime where, on average, feedforward signal inputs are canceled by recurrent inputs. We show that this learning rule minimizes a cost function. Second, we develop a local learning rule for the feedforward weights that, based on networks in which recurrent inputs already predict feedforward inputs, further minimizes the cost. Third, we show how the learning rules can be modified such that the network can directly encode non-whitened inputs. Fourth, we show that these learning rules can also be applied to a network that feeds a time-delayed version of the network output back into itself. As a consequence, the network starts to efficiently represent both its signal inputs and their history. We develop our main theory for linear networks, but then sketch how the learning rules could be transferred to balanced, spiking networks.",
        "bibtex": "@inproceedings{NIPS2014_7c9da54c,\n author = {Vertechi, Pietro and Brendel, Wieland and Machens, Christian K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised learning of an efficient short-term memory network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7c9da54c6d3759c8bff18e16797627db-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/7c9da54c6d3759c8bff18e16797627db-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/7c9da54c6d3759c8bff18e16797627db-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/7c9da54c6d3759c8bff18e16797627db-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/7c9da54c6d3759c8bff18e16797627db-Reviews.html",
        "metareview": "",
        "pdf_size": 647168,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1666573637057080131&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Champalimaud Neuroscience Programme; Champalimaud Neuroscience Programme; Champalimaud Neuroscience Programme + Centre for Integrative Neuroscience, University of T\u00fcbingen, Germany",
        "aff_domain": "neuro.fchampalimaud.org;neuro.fchampalimaud.org;neuro.fchampalimaud.org",
        "email": "neuro.fchampalimaud.org;neuro.fchampalimaud.org;neuro.fchampalimaud.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/7c9da54c6d3759c8bff18e16797627db-Abstract.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Champalimaud Centre for the Unknown;University of T\u00fcbingen",
        "aff_unique_dep": "Neuroscience Programme;Centre for Integrative Neuroscience",
        "aff_unique_url": "https://www.champalimaud.org;https://www.uni-tuebingen.de",
        "aff_unique_abbr": "CCU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Portugal;Germany"
    },
    {
        "title": "Using Convolutional Neural Networks to Recognize Rhythm \ufffcStimuli from Electroencephalography Recordings",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4800",
        "id": "4800",
        "author_site": "Sebastian Stober, Daniel J Cameron, Jessica A Grahn",
        "author": "Sebastian Stober; Daniel J. Cameron; Jessica A. Grahn",
        "abstract": "Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. We apply convolutional neural networks (CNNs) to analyze and classify EEG data recorded within a rhythm perception study in Kigali, Rwanda which comprises 12 East African and 12 Western rhythmic stimuli \u2013 each presented in a loop for 32 seconds to 13 participants. We investigate the impact of the data representation and the pre-processing steps for this classification tasks and compare different network structures. Using CNNs, we are able to recognize individual rhythms from the EEG with a mean classification accuracy of 24.4% (chance level 4.17%) over all subjects by looking at less than three seconds from a single channel. Aggregating predictions for multiple channels, a mean accuracy of up to 50% can be achieved for individual subjects.",
        "bibtex": "@inproceedings{NIPS2014_57557a58,\n author = {Stober, Sebastian and Cameron, Daniel J. and Grahn, Jessica A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Using Convolutional Neural Networks to Recognize Rhythm \ufffcStimuli from Electroencephalography Recordings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/57557a58f6c5dd13af1713695d3302ba-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/57557a58f6c5dd13af1713695d3302ba-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/57557a58f6c5dd13af1713695d3302ba-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/57557a58f6c5dd13af1713695d3302ba-Reviews.html",
        "metareview": "",
        "pdf_size": 464422,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6236523004979372787&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Brain and Mind Institute, Department of Psychology, Western University; Brain and Mind Institute, Department of Psychology, Western University; Brain and Mind Institute, Department of Psychology, Western University",
        "aff_domain": "uwo.ca;uwo.ca;uwo.ca",
        "email": "uwo.ca;uwo.ca;uwo.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/57557a58f6c5dd13af1713695d3302ba-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Western University",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.uwo.ca",
        "aff_unique_abbr": "Western",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Variational Gaussian Process State-Space Models",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4801",
        "id": "4801",
        "author_site": "Roger Frigola, Yutian Chen, Carl Edward Rasmussen",
        "author": "Roger Frigola; Yutian Chen; Carl E. Rasmussen",
        "abstract": "State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.",
        "bibtex": "@inproceedings{NIPS2014_875b60cf,\n author = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl E.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Gaussian Process State-Space Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/875b60cf697a9e09e12c0f07b982e431-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/875b60cf697a9e09e12c0f07b982e431-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/875b60cf697a9e09e12c0f07b982e431-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/875b60cf697a9e09e12c0f07b982e431-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/875b60cf697a9e09e12c0f07b982e431-Reviews.html",
        "metareview": "",
        "pdf_size": 368405,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6424672062722916335&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/875b60cf697a9e09e12c0f07b982e431-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Weakly-supervised Discovery of Visual Pattern Configurations",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4802",
        "id": "4802",
        "author_site": "Hyun Oh Song, Yong Jae Lee, Stefanie Jegelka, Trevor Darrell",
        "author": "Hyun Oh Song; Yong Jae Lee; Stefanie Jegelka; Trevor Darrell",
        "abstract": "The prominence of weakly labeled data gives rise to a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.",
        "bibtex": "@inproceedings{NIPS2014_9adc21db,\n author = {Song, Hyun Oh and Lee, Yong Jae and Jegelka, Stefanie and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Weakly-supervised Discovery of Visual Pattern Configurations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/9adc21db1702e50c6e41c884382df6cb-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/9adc21db1702e50c6e41c884382df6cb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/9adc21db1702e50c6e41c884382df6cb-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/9adc21db1702e50c6e41c884382df6cb-Reviews.html",
        "metareview": "",
        "pdf_size": 8136599,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18424228051731721159&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Berkeley; University of California, Davis; ; ",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/9adc21db1702e50c6e41c884382df6cb-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;University of California, Davis",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "UC Berkeley;UC Davis",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Berkeley;Davis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Weighted importance sampling for off-policy learning with linear function approximation",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4803",
        "id": "4803",
        "author_site": "Rupam Mahmood, Hado P van Hasselt, Richard Sutton",
        "author": "A. Rupam Mahmood; Hado van Hasselt; Richard S. Sutton",
        "abstract": "Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, \\emph{weighted} importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(lambda). We show empirically that our new WIS-LSTD(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(lambda) (Yu 2010, Bertsekas & Yu 2009).",
        "bibtex": "@inproceedings{NIPS2014_1c64ee92,\n author = {Mahmood, A. Rupam and van Hasselt, Hado and Sutton, Richard S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Weighted importance sampling for off-policy learning with linear function approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/1c64ee92596e8ea5050fc435a1d57459-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/1c64ee92596e8ea5050fc435a1d57459-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/1c64ee92596e8ea5050fc435a1d57459-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/1c64ee92596e8ea5050fc435a1d57459-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/1c64ee92596e8ea5050fc435a1d57459-Reviews.html",
        "metareview": "",
        "pdf_size": 772561,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13507372060016681987&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta; Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta; Reinforcement Learning and Artificial Intelligence Laboratory, University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/1c64ee92596e8ea5050fc435a1d57459-Abstract.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Reinforcement Learning and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Zero-shot recognition with unreliable attributes",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4804",
        "id": "4804",
        "author_site": "Dinesh Jayaraman, Kristen Grauman",
        "author": "Dinesh Jayaraman; Kristen Grauman",
        "abstract": "In principle, zero-shot learning makes it possible to train an object recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses --- even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute\u2019s error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.",
        "bibtex": "@inproceedings{NIPS2014_eacc8e49,\n author = {Jayaraman, Dinesh and Grauman, Kristen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Zero-shot recognition with unreliable attributes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/eacc8e49e3861ca74acc785c95c03cc2-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/eacc8e49e3861ca74acc785c95c03cc2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/eacc8e49e3861ca74acc785c95c03cc2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/eacc8e49e3861ca74acc785c95c03cc2-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/eacc8e49e3861ca74acc785c95c03cc2-Reviews.html",
        "metareview": "",
        "pdf_size": 249097,
        "gs_citation": 358,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10207137288388030908&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Texas at Austin; University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/eacc8e49e3861ca74acc785c95c03cc2-Abstract.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Zeta Hull Pursuits: Learning Nonconvex Data Hulls",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4805",
        "id": "4805",
        "author_site": "Yuanjun Xiong, Wei Liu, Deli Zhao, Xiaoou Tang",
        "author": "Yuanjun Xiong; Wei Liu; Deli Zhao; Xiaoou Tang",
        "abstract": "Selecting a small informative subset from a given dataset, also called column sampling, has drawn much attention in machine learning. For incorporating structured data information into column sampling, research efforts were devoted to the cases where data points are fitted with clusters, simplices, or general convex hulls. This paper aims to study nonconvex hull learning which has rarely been investigated in the literature. In order to learn data-adaptive nonconvex hulls, a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points. Employing this measure, we present a greedy algorithmic framework, dubbed Zeta Hulls, to perform structured column sampling. The process of pursuing a Zeta hull involves the computation of matrix inverse. To accelerate the matrix inversion computation and reduce its space complexity as well, we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique. Extensive experimental results show that data representation learned by Zeta Hulls can achieve state-of-the-art accuracy in text and image classification tasks.",
        "bibtex": "@inproceedings{NIPS2014_4878a052,\n author = {Xiong, Yuanjun and Liu, Wei and Zhao, Deli and Tang, Xiaoou},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Zeta Hull Pursuits: Learning Nonconvex Data Hulls},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/4878a0527a93d431d2637338f51cb4af-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/4878a0527a93d431d2637338f51cb4af-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/4878a0527a93d431d2637338f51cb4af-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/4878a0527a93d431d2637338f51cb4af-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/4878a0527a93d431d2637338f51cb4af-Reviews.html",
        "metareview": "",
        "pdf_size": 369592,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6671156753786158516&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Information Engineering Department, The Chinese University of Hong Kong, Hong Kong; IBM T. J. Watson Research Center, Yorktown Heights, New York, USA; Advanced Algorithm Research Group, HTC, Beijing, China; Information Engineering Department, The Chinese University of Hong Kong, Hong Kong",
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;us.ibm.com;htc.com",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;us.ibm.com;htc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/4878a0527a93d431d2637338f51cb4af-Abstract.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Chinese University of Hong Kong;IBM;HTC",
        "aff_unique_dep": "Information Engineering Department;IBM T. J. Watson Research Center;Advanced Algorithm Research Group",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.ibm.com/research/watson;https://www.htc.com",
        "aff_unique_abbr": "CUHK;IBM Watson;HTC",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Hong Kong SAR;Yorktown Heights;Beijing",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "large scale canonical correlation analysis with iterative least squares",
        "status": "Poster",
        "track": "main",
        "site": "https://nips.cc/virtual/2014/poster/4806",
        "id": "4806",
        "author_site": "Yichao Lu, Dean P Foster",
        "author": "Yichao Lu; Dean P. Foster",
        "abstract": "Canonical Correlation Analysis (CCA) is a widely used statistical tool with both well established theory and favorable performance for a wide range of machine learning problems. However, computing CCA for huge datasets can be very slow since it involves implementing QR decomposition or singular value decomposition of huge matrices. In this paper we introduce L-CCA, an iterative algorithm which can compute CCA fast on huge sparse datasets. Theory on both the asymptotic convergence and finite time accuracy of L-CCA are established. The experiments also show that L-CCA outperform other fast CCA approximation schemes on two real datasets.",
        "bibtex": "@inproceedings{NIPS2014_317fd294,\n author = {Lu, Yichao and Foster, Dean P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {large scale canonical correlation analysis with iterative least squares},\n url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/317fd294bfd5c40816ce48bae30b1d4c-Paper.pdf},\n volume = {27},\n year = {2014}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2014/file/317fd294bfd5c40816ce48bae30b1d4c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2014/file/317fd294bfd5c40816ce48bae30b1d4c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2014/file/317fd294bfd5c40816ce48bae30b1d4c-Metadata.json",
        "review": "https://papers.nips.cc/paper_files/paper/2014/file/317fd294bfd5c40816ce48bae30b1d4c-Reviews.html",
        "metareview": "",
        "pdf_size": 1340075,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11566048643092536213&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Pennsylvania; Yahoo Labs, NYC",
        "aff_domain": "wharton.upenn.edu;foster.net",
        "email": "wharton.upenn.edu;foster.net",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://papers.nips.cc/paper_files/paper/2014/hash/317fd294bfd5c40816ce48bae30b1d4c-Abstract.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Pennsylvania;Yahoo",
        "aff_unique_dep": ";Yahoo Labs",
        "aff_unique_url": "https://www.upenn.edu;https://yahoo.com",
        "aff_unique_abbr": "UPenn;Yahoo Labs",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York City",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    }
]