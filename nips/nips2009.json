[
    {
        "id": "e592e3e604",
        "title": "$L_1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/65ded5353c5ee48d0b7d48c591b8f430-Abstract.html",
        "author": "Arnak Dalalyan; Renaud Keriven",
        "abstract": "We propose a new approach to the problem of robust estimation in multiview geometry. Inspired by recent  advances in the sparse recovery problem of statistics, our estimator is defined as a Bayesian maximum a posteriori  with multivariate Laplace prior on the vector describing the outliers. This leads to an estimator in which  the fidelity to the data is measured by the $L_\\infty$-norm while the regularization is done by the $L_1$-norm.  The proposed procedure is fairly fast since the outlier removal is done by solving one linear program (LP).  An important difference compared to existing algorithms is that for our estimator it is not necessary  to specify neither the number nor the proportion of the outliers. The theoretical results, as well as  the numerical example reported in this work, confirm the efficiency of the proposed approach.",
        "bibtex": "@inproceedings{NIPS2009_65ded535,\n author = {Dalalyan, Arnak and Keriven, Renaud},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {L\\_1-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/65ded5353c5ee48d0b7d48c591b8f430-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 254698,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2962981196444339812&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "IMAGINE/LabIGM, Universit \u00b4e Paris Est - Ecole des Ponts ParisTech, Marne-la-Vall \u00b4ee, France; IMAGINE/LabIGM, Universit \u00b4e Paris Est - Ecole des Ponts ParisTech, Marne-la-Vall \u00b4ee, France",
        "aff_domain": "imagine.enpc.fr;imagine.enpc.fr",
        "email": "imagine.enpc.fr;imagine.enpc.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 Paris Est - Ecole des Ponts ParisTech",
        "aff_unique_dep": "IMAGINE/LabIGM",
        "aff_unique_url": "https://www.enpc.fr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Marne-la-Vall\u00e9e",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "0bdac6c931",
        "title": "3D Object Recognition with Deep Belief Nets",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6e7b33fdea3adc80ebd648fffb665bb8-Abstract.html",
        "author": "Vinod Nair; Geoffrey E. Hinton",
        "abstract": "We introduce a new type of Deep Belief Net and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database(normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error, making it the current best result for NORB.",
        "bibtex": "@inproceedings{NIPS2009_6e7b33fd,\n author = {Nair, Vinod and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {3D Object Recognition with Deep Belief Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 142759,
        "gs_citation": 463,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7195800964875020038&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7685931071",
        "title": "A Bayesian Analysis of Dynamics in Free Recall",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/efe937780e95574250dabe07151bdc23-Abstract.html",
        "author": "Richard Socher; Samuel Gershman; Per Sederberg; Kenneth Norman; Adler J. Perotte; David M. Blei",
        "abstract": "We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject first studies a list of words and then tries to recall them.  To model these data, we draw on both previous psychological research and statistical topic models of text documents.  We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space).  During recall, this context is reinstated and used as a cue for retrieving studied words.  By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory.  We present a particle filter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability.",
        "bibtex": "@inproceedings{NIPS2009_efe93778,\n author = {Socher, Richard and Gershman, Samuel and Sederberg, Per and Norman, Kenneth and Perotte, Adler and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian Analysis of Dynamics in Free Recall},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/efe937780e95574250dabe07151bdc23-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/efe937780e95574250dabe07151bdc23-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/efe937780e95574250dabe07151bdc23-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 296220,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4457333432996251310&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, Stanford University; Department of Psychology, Princeton University; Department of Psychology, Princeton University; Department of Psychology, Princeton University; Department of Computer Science, Princeton University; Department of Psychology, Princeton University",
        "aff_domain": "socher.org;princeton.edu;princeton.edu;princeton.edu;cs.princeton.edu;princeton.edu",
        "email": "socher.org;princeton.edu;princeton.edu;princeton.edu;cs.princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Stanford University;Princeton University",
        "aff_unique_dep": "Department of Computer Science;Department of Psychology",
        "aff_unique_url": "https://www.stanford.edu;https://www.princeton.edu",
        "aff_unique_abbr": "Stanford;Princeton",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5648c51062",
        "title": "A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f85454e8279be180185cac7d243c5eb3-Abstract.html",
        "author": "Lan Du; Lu Ren; Lawrence Carin; David B. Dunson",
        "abstract": "A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred non-parametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efficiently via variational Bayesian analysis, with example results presented on two image databases.",
        "bibtex": "@inproceedings{NIPS2009_f85454e8,\n author = {Du, Lan and Ren, Lu and Carin, Lawrence and Dunson, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f85454e8279be180185cac7d243c5eb3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 312000,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16588927186389047116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering + Statistics Department; Statistics Department; Department of Electrical and Computer Engineering",
        "aff_domain": "ee.duke.edu;ee.duke.edu;stats.duke.edu;ee.duke.edu",
        "email": "ee.duke.edu;ee.duke.edu;stats.duke.edu;ee.duke.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;1;0",
        "aff_unique_norm": "Unknown Institution;Statistics Department",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Statistics Department",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d78e92d793",
        "title": "A Biologically Plausible Model for Rapid Natural Scene Identification",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ccb0989662211f61edae2e26d58ea92f-Abstract.html",
        "author": "Sennay Ghebreab; Steven Scholte; Victor Lamme; Arnold Smeulders",
        "abstract": "Contrast statistics of the majority of natural images conform to a Weibull distribution. This property of natural images may facilitate efficient and very rapid extraction of a scenes visual gist. Here we investigate whether a neural response model based on the Weibull contrast distribution captures visual information that humans use to rapidly identify natural scenes. In a learning phase, we measure EEG activity  of 32 subjects viewing brief flashes of 800 natural scenes. From these neural measurements and the contrast statistics of the natural image stimuli,  we  derive an across subject  Weibull response model. We use this model to predict the responses to a large set of new scenes and estimate which scene the subject viewed by finding the best match between the model predictions and the observed EEG responses. In almost 90 percent of the cases our model accurately predicts the observed scene. Moreover, in most failed cases, the scene mistaken for the observed scene is visually similar to the observed scene itself. These results suggest that Weibull contrast statistics of natural images contain a considerable amount of scene gist information to warrant rapid identification of natural images.",
        "bibtex": "@inproceedings{NIPS2009_ccb09896,\n author = {Ghebreab, Sennay and Scholte, Steven and Lamme, Victor and Smeulders, Arnold},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Biologically Plausible Model for Rapid Natural Scene Identification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ccb0989662211f61edae2e26d58ea92f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1950526,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7683353952172333347&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Intelligent Sensory Information Systems Group, University of Amsterdam, The Netherlands; Intelligent Sensory Information Systems Group, University of Amsterdam, The Netherlands; Cognitive Neuroscience Group, University of Amsterdam, The Netherlands; Cognitive Neuroscience Group, University of Amsterdam, The Netherlands",
        "aff_domain": "uva.nl; ;uva.nl; ",
        "email": "uva.nl; ;uva.nl; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Amsterdam",
        "aff_unique_dep": "Intelligent Sensory Information Systems Group",
        "aff_unique_url": "https://www.uva.nl",
        "aff_unique_abbr": "UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "38a7d6631c",
        "title": "A Data-Driven Approach to Modeling Choice",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/98b297950041a42470269d56260243a1-Abstract.html",
        "author": "Vivek Farias; Srikanth Jagabathula; Devavrat Shah",
        "abstract": "We visit the following fundamental problem: For a `generic model of consumer choice (namely, distributions over preference lists) and a limited amount of data on how consumers actually make decisions (such as marginal preference information), how may one predict revenues from offering a particular assortment of choices? This problem is central to areas within operations research, marketing and econometrics. We present a framework to answer such questions and design a number of tractable algorithms (from a data and computational standpoint) for the same.",
        "bibtex": "@inproceedings{NIPS2009_98b29795,\n author = {Farias, Vivek and Jagabathula, Srikanth and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Data-Driven Approach to Modeling Choice},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/98b297950041a42470269d56260243a1-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/98b297950041a42470269d56260243a1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/98b297950041a42470269d56260243a1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/98b297950041a42470269d56260243a1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146752,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16008375997205903157&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "ORC + Sloan School of Management; LIDS + Department of EECS at MIT; LIDS + Department of EECS at MIT",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;2+1",
        "aff_unique_norm": "Oregon State University;Massachusetts Institute of Technology;Laboratory for Information and Decision Systems",
        "aff_unique_dep": ";Sloan School of Management;",
        "aff_unique_url": "https://oregonstate.edu;https://mitsloan.mit.edu/;http://lids.mit.edu/",
        "aff_unique_abbr": "OSU;MIT Sloan;LIDS",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b9ed41b53e",
        "title": "A Fast, Consistent Kernel Two-Sample Test",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/9246444d94f081e3549803b928260f56-Abstract.html",
        "author": "Arthur Gretton; Kenji Fukumizu; Za\u00efd Harchaoui; Bharath K. Sriperumbudur",
        "abstract": "A kernel embedding of probability distributions into reproducing kernel Hilbert spaces (RKHS) has recently been proposed, which  allows the comparison of two probability measures P and Q based on the distance between their respective embeddings: for a sufficiently rich RKHS, this distance is zero if and only if P and Q coincide. In using this distance as a statistic for a  test of whether two samples are from different distributions, a major difficulty arises in computing the significance threshold, since the empirical statistic has as its null distribution (where P=Q)  an infinite weighted sum of $\\chi^2$ random variables. The main result of the present work is a  novel, consistent estimate of this null distribution, computed from  the eigenspectrum of the Gram matrix on the aggregate sample from P and Q. This estimate may be computed faster than a previous consistent estimate based on the bootstrap.  Another prior approach was to compute the null distribution based on fitting a parametric family with the low order moments of the test statistic: unlike the present work, this heuristic has no guarantee of being accurate or consistent. We verify the performance of our null distribution estimate on both   an artificial example and on high dimensional multivariate data.",
        "bibtex": "@inproceedings{NIPS2009_9246444d,\n author = {Gretton, Arthur and Fukumizu, Kenji and Harchaoui, Za\\\"{\\i}d and Sriperumbudur, Bharath K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Fast, Consistent Kernel Two-Sample Test},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/9246444d94f081e3549803b928260f56-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/9246444d94f081e3549803b928260f56-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/9246444d94f081e3549803b928260f56-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/9246444d94f081e3549803b928260f56-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 137694,
        "gs_citation": 301,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4033524588272036765&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Carnegie Mellon University + MPI for Biological Cybernetics; Inst. of Statistical Mathematics, Tokyo, Japan; Carnegie Mellon University, Pittsburgh, PA, USA; Dept. of ECE, UCSD, La Jolla, CA 92037",
        "aff_domain": "gmail.com;ism.ac.jp;gmail.com;ucsd.edu",
        "email": "gmail.com;ism.ac.jp;gmail.com;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0;3",
        "aff_unique_norm": "Carnegie Mellon University;Max Planck Institute for Biological Cybernetics;Institute of Statistical Mathematics;University of California, San Diego",
        "aff_unique_dep": ";Biological Cybernetics;Statistics;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.cmu.edu;https://www.biological-cybernetics.de;http://www.ism.ac.jp;https://www.ucsd.edu",
        "aff_unique_abbr": "CMU;MPIBC;ISM;UCSD",
        "aff_campus_unique_index": ";1;2;3",
        "aff_campus_unique": ";Tokyo;Pittsburgh;La Jolla",
        "aff_country_unique_index": "0+1;2;0;0",
        "aff_country_unique": "United States;Germany;Japan"
    },
    {
        "id": "9f71448b9e",
        "title": "A Game-Theoretic Approach to Hypergraph Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6c3cf77d52820cd0fe646d38bc2145ca-Abstract.html",
        "author": "Samuel R. Bul\u00f2; Marcello Pelillo",
        "abstract": "Hypergraph clustering refers to the process of extracting maximally coherent groups from a set of objects using high-order (rather than pairwise) similarities. Traditional approaches to this problem are based on the idea of partitioning the input data into a user-defined number of classes, thereby obtaining the clusters as a by-product of the partitioning process. In this paper, we provide a radically different perspective to the problem. In contrast to the classical approach, we attempt to provide a meaningful formalization of the very notion of a cluster and we show that game theory offers an attractive and unexplored perspective that serves well our purpose. Specifically, we show that the hypergraph clustering problem can be naturally cast into a non-cooperative multi-player ``clustering game, whereby the notion of a cluster is equivalent to a classical game-theoretic equilibrium concept. From the computational viewpoint, we show that the problem of finding the equilibria of our clustering game is equivalent to locally optimizing a polynomial function over the standard simplex, and we provide a discrete-time dynamics to perform this optimization. Experiments are presented which show the superiority of our approach over state-of-the-art hypergraph clustering techniques.",
        "bibtex": "@inproceedings{NIPS2009_6c3cf77d,\n author = {Bul\\`{o}, Samuel and Pelillo, Marcello},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Game-Theoretic Approach to Hypergraph Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6c3cf77d52820cd0fe646d38bc2145ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146270,
        "gs_citation": 246,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7956208329705994676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "University of Venice, Italy; University of Venice, Italy",
        "aff_domain": "dsi.unive.it;dsi.unive.it",
        "email": "dsi.unive.it;dsi.unive.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Venice",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unive.it",
        "aff_unique_abbr": "Unive",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "5da8653ab6",
        "title": "A Gaussian Tree Approximation for Integer Least-Squares",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/839ab46820b524afda05122893c2fe8e-Abstract.html",
        "author": "Jacob Goldberger; Amir Leshem",
        "abstract": "This paper proposes a new algorithm for the linear least squares problem where the unknown variables are constrained to be in a finite set.  The factor graph that corresponds to this problem is very loopy; in fact, it is a complete graph. Hence, applying the Belief Propagation (BP) algorithm yields very poor results. The algorithm described here is based on  an optimal  tree approximation of the Gaussian density of the unconstrained linear system. It is shown that even though the approximation is not directly applied to the exact discrete distribution, applying the BP algorithm to the modified factor graph outperforms current methods in terms of both performance and complexity. The improved performance of the proposed algorithm is demonstrated  on the problem of MIMO detection.",
        "bibtex": "@inproceedings{NIPS2009_839ab468,\n author = {Goldberger, Jacob and Leshem, Amir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Gaussian Tree Approximation for Integer Least-Squares},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/839ab46820b524afda05122893c2fe8e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/839ab46820b524afda05122893c2fe8e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/839ab46820b524afda05122893c2fe8e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 87118,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3794192896921792160&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "School of Engineering, Bar-Ilan University; School of Engineering, Bar-Ilan University",
        "aff_domain": "eng.biu.ac.il;eng.biu.ac.il",
        "email": "eng.biu.ac.il;eng.biu.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Bar-Ilan University",
        "aff_unique_dep": "School of Engineering",
        "aff_unique_url": "https://www.biu.ac.il",
        "aff_unique_abbr": "BIU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "99076be5ef",
        "title": "A General Projection Property for Distribution Families",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a684eceee76fc522773286a895bc8436-Abstract.html",
        "author": "Yao-liang Yu; Yuxi Li; Dale Schuurmans; Csaba Szepesv\u00e1ri",
        "abstract": "We prove that linear projections between distribution families with fixed first and second moments are surjective, regardless of dimension. We further extend this result to families that respect additional constraints, such as symmetry, unimodality and log-concavity. By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in different fields. One discovery is that portfolio selection under the worst-case value-at-risk and conditional value-at-risk criteria yields identical portfolios.",
        "bibtex": "@inproceedings{NIPS2009_a684ecee,\n author = {Yu, Yao-liang and Li, Yuxi and Schuurmans, Dale and Szepesv\\'{a}ri, Csaba},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A General Projection Property for Distribution Families},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a684eceee76fc522773286a895bc8436-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a684eceee76fc522773286a895bc8436-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/a684eceee76fc522773286a895bc8436-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a684eceee76fc522773286a895bc8436-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 138767,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3421491639424450474&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "53a7206557",
        "title": "A Generalized Natural Actor-Critic Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/acf4b89d3d503d8252c9c4ba75ddbf6d-Abstract.html",
        "author": "Tetsuro Morimura; Eiji Uchibe; Junichiro Yoshimoto; Kenji Doya",
        "abstract": "Policy gradient Reinforcement Learning (RL) algorithms have received much attention in seeking stochastic policies that maximize the average rewards.  In addition, extensions based on the concept of the Natural Gradient (NG) show promising learning efficiency because these regard metrics for the task. Though there are two candidate metrics, Kakades Fisher Information Matrix (FIM) and Morimuras FIM, all RL algorithms with NG have followed the Kakades approach. In this paper, we describe a generalized Natural Gradient (gNG) by linearly interpolating the two FIMs and propose an efficient implementation for the gNG learning based on a theory of the estimating function, generalized Natural Actor-Critic (gNAC). The gNAC algorithm involves a near optimal auxiliary function to reduce the variance of the gNG estimates. Interestingly, the gNAC can be regarded as a natural extension of the current state-of-the-art NAC algorithm, as long as the interpolating parameter is appropriately selected. Numerical experiments showed that the proposed gNAC algorithm can estimate gNG efficiently and outperformed the NAC algorithm.",
        "bibtex": "@inproceedings{NIPS2009_acf4b89d,\n author = {Morimura, Tetsuro and Uchibe, Eiji and Yoshimoto, Junichiro and Doya, Kenji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Generalized Natural Actor-Critic Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 253246,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7155479512712436621&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "IBM Research \u0096Tokyo, Kanagawa, Japan; Okinawa Institute of Science and Technology, Okinawa, Japan; Okinawa Institute of Science and Technology, Okinawa, Japan; Okinawa Institute of Science and Technology, Okinawa, Japan",
        "aff_domain": "jp.ibm.com;oist.jp;oist.jp;oist.jp",
        "email": "jp.ibm.com;oist.jp;oist.jp;oist.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "IBM;Okinawa Institute of Science and Technology",
        "aff_unique_dep": "IBM Research;",
        "aff_unique_url": "https://www.ibm.com/research;https://www.oist.jp",
        "aff_unique_abbr": "IBM;OIST",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Tokyo;Okinawa",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "fc1cea583f",
        "title": "A Neural Implementation of the Kalman Filter",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6d0f846348a856321729a2f36734d1a7-Abstract.html",
        "author": "Robert Wilson; Leif Finkel",
        "abstract": "There is a growing body of experimental evidence to suggest that the brain is capable of approximating optimal Bayesian inference in the face of noisy input stimuli.  Despite this progress, the neural underpinnings of this computation are still poorly  understood.  In this paper we focus on the problem of Bayesian filtering of stochastic time series.  In particular we introduce a novel neural network, derived from a line attractor architecture, whose dynamics map directly onto those of the Kalman Filter in the limit where the prediction error is small.  When the prediction error is large we show that the network responds robustly to change-points in a way that is qualitatively compatible with the optimal Bayesian model.  The model suggests ways in which probability distributions are encoded in the brain and makes a number of testable experimental predictions.",
        "bibtex": "@inproceedings{NIPS2009_6d0f8463,\n author = {Wilson, Robert and Finkel, Leif},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Neural Implementation of the Kalman Filter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/6d0f846348a856321729a2f36734d1a7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6d0f846348a856321729a2f36734d1a7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 211693,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17379441112679626656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Psychology, Princeton University; Department of Bioengineering, University of Pennsylvania",
        "aff_domain": "princeton.edu; ",
        "email": "princeton.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Princeton University;University of Pennsylvania",
        "aff_unique_dep": "Department of Psychology;Department of Bioengineering",
        "aff_unique_url": "https://www.princeton.edu;https://www.upenn.edu",
        "aff_unique_abbr": "Princeton;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "78d7caf5b5",
        "title": "A Parameter-free Hedging Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2cbca44843a864533ec05b321ae1f9d1-Abstract.html",
        "author": "Kamalika Chaudhuri; Yoav Freund; Daniel J. Hsu",
        "abstract": "We study the problem of decision-theoretic online learning (DTOL). Motivated by practical applications, we focus on DTOL when the number of actions is very large. Previous algorithms for learning in this framework have a tunable learning rate parameter, and a major barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally, particularly when the number of actions is large. In this paper, we offer a clean solution by proposing a novel and completely parameter-free algorithm for DTOL.   In addition, we introduce a new notion of regret, which is more natural for applications with a large number of actions. We show that our algorithm achieves good performance with respect to this new notion of regret; in addition, it also achieves performance close to that of the best bounds achieved by previous algorithms with optimally-tuned parameters, according to previous notions of regret.",
        "bibtex": "@inproceedings{NIPS2009_2cbca448,\n author = {Chaudhuri, Kamalika and Freund, Yoav and Hsu, Daniel J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Parameter-free Hedging Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2cbca44843a864533ec05b321ae1f9d1-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2cbca44843a864533ec05b321ae1f9d1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/2cbca44843a864533ec05b321ae1f9d1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2cbca44843a864533ec05b321ae1f9d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 110784,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=873051114388480096&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "ITA, UC San Diego; CSE, UC San Diego; CSE, UC San Diego",
        "aff_domain": "soe.ucsd.edu;ucsd.edu;cs.ucsd.edu",
        "email": "soe.ucsd.edu;ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Institute for Theoretical and Applied Mechanics",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5843769df8",
        "title": "A Rate Distortion Approach for Semi-Supervised Conditional Random Fields",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e5841df2166dd424a57127423d276bbe-Abstract.html",
        "author": "Yang Wang; Gholamreza Haffari; Shaojun Wang; Greg Mori",
        "abstract": "We propose a novel information theoretic approach for semi-supervised learning of conditional random fields. Our approach defines a training objective that combines the conditional likelihood on labeled data and the mutual information on unlabeled data. Different from previous minimum conditional entropy semi-supervised discriminative learning methods, our approach can be naturally cast into the rate distortion theory framework in information theory. We analyze the tractability of the framework for structured prediction and present a convergent variational training algorithm to defy the combinatorial explosion of terms in the sum over label configurations. Our experimental results show that the rate distortion approach outperforms standard $l_2$ regularization and minimum conditional entropy regularization on both multi-class classification and sequence labeling problems.",
        "bibtex": "@inproceedings{NIPS2009_e5841df2,\n author = {Wang, Yang and Haffari, Gholamreza and Wang, Shaojun and Mori, Greg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Rate Distortion Approach for Semi-Supervised Conditional Random Fields},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e5841df2166dd424a57127423d276bbe-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e5841df2166dd424a57127423d276bbe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e5841df2166dd424a57127423d276bbe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 154029,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15881483149288440935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computing Science, Simon Fraser University; School of Computing Science, Simon Fraser University; Kno.e.sis Center, Wright State University; School of Computing Science, Simon Fraser University",
        "aff_domain": "cs.sfu.ca;cs.sfu.ca;wright.edu;cs.sfu.ca",
        "email": "cs.sfu.ca;cs.sfu.ca;wright.edu;cs.sfu.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Simon Fraser University;Wright State University",
        "aff_unique_dep": "School of Computing Science;Kno.e.sis Center",
        "aff_unique_url": "https://www.sfu.ca;https://www.wright.edu",
        "aff_unique_abbr": "SFU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Burnaby;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "0625ac0874",
        "title": "A Smoothed Approximate Linear Program",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d7322ed717dedf1eb4e6e52a37ea7bcd-Abstract.html",
        "author": "Vijay Desai; Vivek Farias; Ciamac C. Moallemi",
        "abstract": "We present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems. LP approaches to approximate DP naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function. Our program -- the `smoothed approximate linear program -- relaxes this restriction in an appropriate fashion while remaining computationally tractable. Doing so appears to have several advantages: First, we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach. Second, experiments with our approach on a challenging problem (the game of Tetris) show that the approach outperforms the existing LP approach (which has previously been shown to be competitive with several ADP algorithms) by an order of magnitude.",
        "bibtex": "@inproceedings{NIPS2009_d7322ed7,\n author = {Desai, Vijay and Farias, Vivek and Moallemi, Ciamac C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Smoothed Approximate Linear Program},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 444576,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13323404129727628732&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 30,
        "aff": "IEOR, Columbia University; MIT Sloan; GSB, Columbia University",
        "aff_domain": "columbia.edu;mit.edu;gsb.columbia.edu",
        "email": "columbia.edu;mit.edu;gsb.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Columbia University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Industrial Engineering and Operations Research;Sloan School of Management",
        "aff_unique_url": "https://www.columbia.edu;https://mitsloan.mit.edu/",
        "aff_unique_abbr": "Columbia;MIT Sloan",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Cambridge;New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4d1be7dde8",
        "title": "A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/9c3b1830513cc3b8fc4b76635d32e692-Abstract.html",
        "author": "Paris Smaragdis; Madhusudana Shashanka; Bhiksha Raj",
        "abstract": "In this paper we present an algorithm for separating mixed sounds from  a monophonic recording. Our approach makes use of training data which  allows us to learn representations of the types of sounds that compose the  mixture. In contrast to popular methods that attempt to extract com-  pact generalizable models for each sound from training data, we employ  the training data itself as a representation of the sources in the mixture.  We show that mixtures of known sounds can be described as sparse com-  binations of the training data itself, and in doing so produce signi\ufb01cantly  better separation results as compared to similar systems based on compact  statistical models.",
        "bibtex": "@inproceedings{NIPS2009_9c3b1830,\n author = {Smaragdis, Paris and Shashanka, Madhusudana and Raj, Bhiksha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/9c3b1830513cc3b8fc4b76635d32e692-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/9c3b1830513cc3b8fc4b76635d32e692-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/9c3b1830513cc3b8fc4b76635d32e692-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 312931,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16864260057108000472&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Adobe Systems Inc.; Mars Inc.; Carnegie Mellon University",
        "aff_domain": "adobe.com;alum.bu.edu;cs.cmu.edu",
        "email": "adobe.com;alum.bu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Adobe;Mars Inc.;Carnegie Mellon University",
        "aff_unique_dep": "Adobe Systems Incorporated;;",
        "aff_unique_url": "https://www.adobe.com;https://www.mars.com;https://www.cmu.edu",
        "aff_unique_abbr": "Adobe;Mars;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3f6e014b0d",
        "title": "A Stochastic approximation method for inference in probabilistic graphical models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/19ca14e7ea6328a42e0eb13d585e4c22-Abstract.html",
        "author": "Peter Carbonetto; Matthew King; Firas Hamze",
        "abstract": "We describe a new algorithmic framework for inference in probabilistic models, and apply it to inference for latent Dirichlet allocation. Our framework adopts the methodology of variational inference, but unlike existing variational methods such as mean field and expectation propagation it is not restricted to tractable classes of approximating distributions. Our approach can also be viewed as a sequential Monte Carlo (SMC) method, but unlike existing SMC methods there is no need to design the artificial sequence of distributions. Notably, our framework offers a principled means to exchange the variance of an importance sampling estimate for the bias incurred through variational approximation. Experiments on a challenging inference problem in population genetics demonstrate improvements in stability and accuracy over existing methods, and at a comparable cost.",
        "bibtex": "@inproceedings{NIPS2009_19ca14e7,\n author = {Carbonetto, Peter and King, Matthew and Hamze, Firas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Stochastic approximation method for inference in probabilistic graphical models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/19ca14e7ea6328a42e0eb13d585e4c22-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/19ca14e7ea6328a42e0eb13d585e4c22-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/19ca14e7ea6328a42e0eb13d585e4c22-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 278067,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4379361376240097414&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Human Genetics, University of Chicago, Chicago, IL, U.S.A.; Dept. of Botany, University of British Columbia, Vancouver, B.C., Canada; D-Wave Systems, Burnaby, B.C., Canada",
        "aff_domain": "bsd.uchicago.edu;interchange.ubc.ca;dwavesys.com",
        "email": "bsd.uchicago.edu;interchange.ubc.ca;dwavesys.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Chicago;University of British Columbia;D-Wave Systems",
        "aff_unique_dep": "Dept. of Human Genetics;Dept. of Botany;",
        "aff_unique_url": "https://www.uchicago.edu;https://www.ubc.ca;https://www.dwavesys.com/",
        "aff_unique_abbr": "UChicago;UBC;D-Wave",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Chicago;Vancouver;Burnaby",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "4e7d01c380",
        "title": "A joint maximum-entropy model for binary neural population patterns and continuous signals",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract.html",
        "author": "Sebastian Gerwinn; Philipp Berens; Matthias Bethge",
        "abstract": "Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining  the joint second-order statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural firing patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and fitting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the firing patterns of neural ensembles and the stimuli they are processing.",
        "bibtex": "@inproceedings{NIPS2009_be83ab3e,\n author = {Gerwinn, Sebastian and Berens, Philipp and Bethge, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A joint maximum-entropy model for binary neural population patterns and continuous signals},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/be83ab3ecd0db773eb2dc1b0a17836a1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 982206,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17924165683727578194&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "MPI for Biological Cybernetics and University of T\u00fcbingen; MPI for Biological Cybernetics and University of T\u00fcbingen; MPI for Biological Cybernetics and University of T\u00fcbingen",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.biological-cybernetics.de",
        "aff_unique_abbr": "MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2030631e32",
        "title": "A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/dc58e3a306451c9d670adcd37004f48f-Abstract.html",
        "author": "Sahand Negahban; Bin Yu; Martin J. Wainwright; Pradeep K. Ravikumar",
        "abstract": "The estimation of high-dimensional parametric models requires imposing some structure on the models, for instance that they be sparse, or that matrix structured parameters have low rank. A general approach for such structured parametric model estimation is to use regularized M-estimation procedures, which regularize a loss function that measures goodness of fit of the parameters to the data with some regularization function that encourages the assumed structure. In this paper, we aim to provide a unified analysis of such regularized M-estimation procedures. In particular, we report the convergence rates of such estimators in any metric norm. Using just our main theorem, we are able to rederive some of the many existing results, but also obtain a wide range of novel convergence rates results. Our analysis also identifies key properties of loss and regularization functions such as restricted strong convexity, and decomposability, that ensure the corresponding regularized M-estimators have good convergence rates.",
        "bibtex": "@inproceedings{NIPS2009_dc58e3a3,\n author = {Negahban, Sahand and Yu, Bin and Wainwright, Martin J and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/dc58e3a306451c9d670adcd37004f48f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 327075,
        "gs_citation": 1307,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4669736212636661286&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 27,
        "aff": "Department of EECS UC Berkeley; Department of Computer Sciences UT Austin; Department of Statistics + Department of EECS UC Berkeley; Department of Statistics + Department of EECS UC Berkeley",
        "aff_domain": "eecs.berkeley.edu;cs.utexas.edu;eecs.berkeley.edu;stat.berkeley.edu",
        "email": "eecs.berkeley.edu;cs.utexas.edu;eecs.berkeley.edu;stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+0;2+0",
        "aff_unique_norm": "University of California, Berkeley;University of Texas at Austin;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences;Department of Computer Sciences;Department of Statistics",
        "aff_unique_url": "https://www.berkeley.edu;https://www.utexas.edu;",
        "aff_unique_abbr": "UC Berkeley;UT Austin;",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Berkeley;Austin;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "d7bbf6b938",
        "title": "AUC optimization and the two-sample problem",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html",
        "author": "Nicolas Vayatis; Marine Depecker; St\u00e9phan J. Cl\u00e9men\u00e7con",
        "abstract": "The purpose of the paper is to explore the connection between multivariate homogeneity tests and $\\auc$ optimization. The latter problem has recently received much attention in the statistical learning literature. From the elementary observation that, in the two-sample problem setup, the null assumption corresponds to the situation where the area under the optimal ROC curve is equal to 1/2, we propose a two-stage testing method based on data splitting. A nearly optimal scoring function in the AUC sense is first learnt from one of the two half-samples. Data from the remaining half-sample are then projected onto the real line and eventually ranked according to the scoring function computed at the first stage. The last step amounts to performing a standard Mann-Whitney Wilcoxon  test in the one-dimensional framework. We show that the learning step of the procedure does not affect the consistency of the test as well as its properties in terms of power, provided the ranking produced is accurate enough in the AUC sense. The results of a numerical experiment are eventually displayed in order to show the efficiency of the method.",
        "bibtex": "@inproceedings{NIPS2009_40008b9a,\n author = {Vayatis, Nicolas and Depecker, Marine and Cl\\'{e}men\\c{c}con, St\\'{e}phan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {AUC optimization and the two-sample problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/40008b9a5380fcacce3976bf7c08af5b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 248074,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9908166110251268099&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141; Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141; ENS Cachan & UniverSud - CMLA UMR CNRS 8536",
        "aff_domain": "telecom-paristech.fr;telecom-paristech.fr;cmla.ens-cachan.fr",
        "email": "telecom-paristech.fr;telecom-paristech.fr;cmla.ens-cachan.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Telecom ParisTech;\u00c9cole Normale Sup\u00e9rieure de Cachan",
        "aff_unique_dep": "LTCI UMR Institut Telecom/CNRS 5141;CMLA UMR CNRS 8536",
        "aff_unique_url": "https://www.telecom-paristech.fr;https://www.ens-cachan.fr",
        "aff_unique_abbr": "TP;ENS Cachan",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cachan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "6121193f90",
        "title": "Abstraction and Relational learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/96da2f590cd7246bbde0051047b0d6f7-Abstract.html",
        "author": "Charles Kemp; Alan Jern",
        "abstract": "Part of",
        "bibtex": "@inproceedings{NIPS2009_96da2f59,\n author = {Kemp, Charles and Jern, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Abstraction and Relational learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/96da2f590cd7246bbde0051047b0d6f7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 390877,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2721910298064608040&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Psychology, Carnegie Mellon University; Department of Psychology, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu",
        "email": "cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8b26c1cd15",
        "title": "Accelerated Gradient Methods for Stochastic Optimization and Online Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html",
        "author": "Chonghai Hu; Weike Pan; James T. Kwok",
        "abstract": "Regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., $\\ell_1$-regularizer). Gradient descent methods, though highly scalable and easy to implement, are known to converge slowly on these problems. In this paper, we develop novel accelerated gradient methods for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic optimization with both convex and strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, resulting in a simple but powerful algorithm.",
        "bibtex": "@inproceedings{NIPS2009_ec5aa0b7,\n author = {Hu, Chonghai and Pan, Weike and Kwok, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accelerated Gradient Methods for Stochastic Optimization and Online Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ec5aa0b7846082a2415f0902f0da88f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 199890,
        "gs_citation": 243,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8299926996278630063&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology + Department of Mathematics, Zhejiang University; Department of Computer Science and Engineering, Hong Kong University of Science and Technology",
        "aff_domain": "gmail.com;cse.ust.hk;cse.ust.hk",
        "email": "gmail.com;cse.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Zhejiang University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Mathematics",
        "aff_unique_url": "https://www.ust.hk;http://www.zju.edu.cn",
        "aff_unique_abbr": "HKUST;ZJU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "03f474ce92",
        "title": "Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a1519de5b5d44b31a01de013b9b51a80-Abstract.html",
        "author": "Baback Moghaddam; Mohammad Emtiyaz Khan; Kevin P. Murphy; Benjamin M. Marlin",
        "abstract": "In this paper we make several contributions towards accelerating approximate Bayesian structural inference for non-decomposable GGMs. Our first contribution is to show how to efficiently compute a BIC or Laplace approximation to the marginal likelihood of non-decomposable graphs using convex methods for precision matrix estimation. This optimization technique can be used as a fast scoring function inside standard Stochastic Local Search (SLS) for generating posterior samples. Our second contribution is a novel framework for efficiently generating large sets of high-quality graph topologies without performing local search. This graph proposal method, which we call Neighborhood Fusion\" (NF), samples candidate Markov blankets at each node using sparse regression techniques. Our final contribution is a hybrid method combining the complementary strengths of NF and SLS. Experimental results in structural recovery and prediction tasks demonstrate that NF and hybrid NF/SLS out-perform state-of-the-art local search methods, on both synthetic and real-world datasets, when realistic computational limits are imposed.\"",
        "bibtex": "@inproceedings{NIPS2009_a1519de5,\n author = {Moghaddam, Baback and Khan, Mohammad Emtiyaz and Murphy, Kevin P and Marlin, Benjamin M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a1519de5b5d44b31a01de013b9b51a80-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 233249,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17517546146249825134&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Jet Propulsion Laboratory, California Institute of Technology; Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia",
        "aff_domain": "jpl.nasa.gov;cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "jpl.nasa.gov;cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "California Institute of Technology;University of British Columbia",
        "aff_unique_dep": "Jet Propulsion Laboratory;Department of Computer Science",
        "aff_unique_url": "https://www.caltech.edu;https://www.ubc.ca",
        "aff_unique_abbr": "Caltech;UBC",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Pasadena;Vancouver",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "8556ea2fe0",
        "title": "Adapting to the Shifting Intent of Search Queries",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6aab1270668d8cac7cef2566a1c5f569-Abstract.html",
        "author": "Umar Syed; Aleksandrs Slivkins; Nina Mishra",
        "abstract": "Search engines today present results that are often oblivious to recent shifts in intent. For example, the meaning of the query independence day shifts in early July to a US holiday and to a movie around the time of the box office release.  While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 1/2 the search queries.  This paper shows that the signals a search engine receives can be used to both determine that a shift in intent happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions.  We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.",
        "bibtex": "@inproceedings{NIPS2009_6aab1270,\n author = {Syed, Umar and Slivkins, Aleksandrs and Mishra, Nina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adapting to the Shifting Intent of Search Queries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6aab1270668d8cac7cef2566a1c5f569-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 170842,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10494220806254000107&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Pennsylvania; Microsoft Research; Microsoft Research",
        "aff_domain": "cis.upenn.edu;microsoft.com;microsoft.com",
        "email": "cis.upenn.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "https://arxiv.org/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Pennsylvania;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.upenn.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UPenn;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6124ab327d",
        "title": "Adaptive Design Optimization in Experiments with People",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6cd67d9b6f0150c77bda2eda01ae484c-Abstract.html",
        "author": "Daniel Cavagnaro; Jay Myung; Mark A. Pitt",
        "abstract": "In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments.  It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efficiently as possible in simulation experiments.  In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to find designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes.  Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation.",
        "bibtex": "@inproceedings{NIPS2009_6cd67d9b,\n author = {Cavagnaro, Daniel and Myung, Jay and Pitt, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Design Optimization in Experiments with People},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6cd67d9b6f0150c77bda2eda01ae484c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6cd67d9b6f0150c77bda2eda01ae484c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6cd67d9b6f0150c77bda2eda01ae484c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3125578,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16662025812842932140&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Psychology, Ohio State University; Department of Psychology, Ohio State University; Department of Psychology, Ohio State University",
        "aff_domain": "osu.edu;osu.edu;osu.edu",
        "email": "osu.edu;osu.edu;osu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ohio State University",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "31e3e227b9",
        "title": "Adaptive Regularization for Transductive Support Vector Machine",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8a0e1141fd37fa5b98d5bb769ba1a7cc-Abstract.html",
        "author": "Zenglin Xu; Rong Jin; Jianke Zhu; Irwin King; Michael Lyu; Zhirong Yang",
        "abstract": "We discuss the framework of Transductive Support Vector Machine (TSVM) from the perspective of the regularization strength induced by the unlabeled data. In this framework, SVM and TSVM can be regarded as a learning machine without regularization and one with full regularization from the unlabeled data, respectively. Therefore, to supplement this framework of the regularization strength, it is necessary to introduce data-dependant partial regularization. To this end, we reformulate TSVM into a form with controllable regularization strength, which includes SVM and TSVM as special cases. Furthermore, we introduce a method of adaptive regularization that is data dependant and is based on the smoothness assumption. Experiments on a set of benchmark data sets indicate the promising results of the proposed work compared with state-of-the-art TSVM algorithms.",
        "bibtex": "@inproceedings{NIPS2009_8a0e1141,\n author = {Xu, Zenglin and Jin, Rong and Zhu, Jianke and King, Irwin and Lyu, Michael and Yang, Zhirong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Regularization for Transductive Support Vector Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 141297,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13342876507129037079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Cluster MMCI Saarland Univ. & MPI INF; Computer Sci. & Eng. Michigan State Univ.; Computer Vision Lab ETH Zurich; Computer Science & Engineering The Chinese Univ. of Hong Kong; Computer Science & Engineering The Chinese Univ. of Hong Kong; Information & Computer Science Helsinki Univ. of Technology",
        "aff_domain": "mpi-inf.mpg.de;cse.msu.edu;vision.ee.ethz.ch;cse.cuhk.edu.hk;cse.cuhk.edu.hk;tkk.fi",
        "email": "mpi-inf.mpg.de;cse.msu.edu;vision.ee.ethz.ch;cse.cuhk.edu.hk;cse.cuhk.edu.hk;tkk.fi",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;3;4",
        "aff_unique_norm": "Saarland University;Michigan State University;ETH Zurich;Chinese University of Hong Kong;Helsinki University of Technology",
        "aff_unique_dep": "Cluster of Excellence on Multimodal Computing and Interaction (MMCI);Department of Computer Science & Engineering;Computer Vision Lab;Computer Science & Engineering;Information & Computer Science",
        "aff_unique_url": "https://www.uni-saarland.de;https://www.msu.edu;https://www.ethz.ch;https://www.cuhk.edu.hk;https://www.hut.fi",
        "aff_unique_abbr": "Saarland Univ.;MSU;ETHZ;CUHK;HUT",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";Zurich;Hong Kong SAR",
        "aff_country_unique_index": "0;1;2;3;3;4",
        "aff_country_unique": "Germany;United States;Switzerland;China;Finland"
    },
    {
        "id": "a6a99965e5",
        "title": "Adaptive Regularization of Weight Vectors",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8ebda540cbcc4d7336496819a46a1b68-Abstract.html",
        "author": "Koby Crammer; Alex Kulesza; Mark Dredze",
        "abstract": "We present AROW, a new online learning algorithm that combines several properties of successful : large margin training, confidence weighting, and the capacity to handle non-separable data. AROW performs adaptive regularization of the prediction function upon seeing each new instance, allowing it to perform especially well in the presence of label noise.  We derive a mistake bound, similar in form to the second order perceptron bound, which does not assume separability. We also relate our algorithm to recent confidence-weighted online learning techniques and empirically show that AROW achieves state-of-the-art performance and notable robustness in the case of non-separable data.",
        "bibtex": "@inproceedings{NIPS2009_8ebda540,\n author = {Crammer, Koby and Kulesza, Alex and Dredze, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Regularization of Weight Vectors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8ebda540cbcc4d7336496819a46a1b68-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8ebda540cbcc4d7336496819a46a1b68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8ebda540cbcc4d7336496819a46a1b68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1020032,
        "gs_citation": 402,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10048550914769125174&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical Enginering, The Technion, Haifa, 32000 Israel; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104; Human Language Tech. Center of Excellence, Johns Hopkins University, Baltimore, MD 21211",
        "aff_domain": "ee.technion.ac.il;cis.upenn.edu;cs.jhu.edu",
        "email": "ee.technion.ac.il;cis.upenn.edu;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Technion;University of Pennsylvania;Johns Hopkins University",
        "aff_unique_dep": "Department of Electrical Engineering;Department of Computer and Information Science;Human Language Tech. Center of Excellence",
        "aff_unique_url": "http://www.technion.ac.il;https://www.upenn.edu;https://www.jhu.edu",
        "aff_unique_abbr": "Technion;UPenn;JHU",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Haifa;Philadelphia;Baltimore",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "49f9b267b9",
        "title": "An Additive Latent Feature Model for Transparent Object Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html",
        "author": "Mario Fritz; Gary Bradski; Sergey Karayev; Trevor Darrell; Michael J. Black",
        "abstract": "Existing methods for recognition of object instances and categories based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization.  The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance.  We model transparent local patch appearance using an additive  model of latent factors: background factors due to scene content,  and factors which capture a local edge energy distribution characteristic of the refraction.  We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment.",
        "bibtex": "@inproceedings{NIPS2009_e46de7e1,\n author = {Fritz, Mario and Bradski, Gary and Karayev, Sergey and Darrell, Trevor and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Additive Latent Feature Model for Transparent Object Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1032675,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11268443319058601489&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "UC Berkeley; Brown University; Willow Garage; UC Berkeley; UC Berkeley",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "University of California, Berkeley;Brown University;Willow Garage",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.brown.edu;http://willowgarage.com/",
        "aff_unique_abbr": "UC Berkeley;Brown;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ec1e34a072",
        "title": "An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html",
        "author": "Douglas Eck; Yoshua Bengio; Aaron C. Courville",
        "abstract": "The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an infinite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer defines a conditional \\emph{factorial} prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment.",
        "bibtex": "@inproceedings{NIPS2009_1e6e0a04,\n author = {Eck, Douglas and Bengio, Yoshua and Courville, Aaron C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/1e6e0a04d20f50967c64dac2d639a577-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1e6e0a04d20f50967c64dac2d639a577-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 788373,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2845514957305767152&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Operations Research University of Montreal; Department of Computer Science and Operations Research University of Montreal; Department of Computer Science and Operations Research University of Montreal",
        "aff_domain": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca",
        "email": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Montreal",
        "aff_unique_dep": "Department of Computer Science and Operations Research",
        "aff_unique_url": "https://wwwumontreal.ca",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "80e1848bd9",
        "title": "An Integer Projected Fixed Point Method for Graph Matching and MAP Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/fc2c7c47b918d0c2d792a719dfb602ef-Abstract.html",
        "author": "Marius Leordeanu; Martial Hebert; Rahul Sukthankar",
        "abstract": "Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efficiently. Recent graph matching algorithms are based on a general quadratic programming formulation, that takes in consideration both unary and second-order terms reflecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. In this case the problem is NP-hard and a lot of effort has been spent in finding efficiently approximate solutions by relaxing the constraints of the original problem. Most algorithms find optimal continuous solutions of the modified problem, ignoring during the optimization the original discrete constraints. The continuous solution is quickly binarized at the end, but very little attention is put into this final discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efficient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art algorithms and it also significantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by outperforming ICM and Max-Product Belief Propagation.",
        "bibtex": "@inproceedings{NIPS2009_fc2c7c47,\n author = {Leordeanu, Marius and Hebert, Martial and Sukthankar, Rahul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Integer Projected Fixed Point Method for Graph Matching and MAP Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/fc2c7c47b918d0c2d792a719dfb602ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1076027,
        "gs_citation": 426,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8987641813328939918&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Intel Labs Pittsburgh",
        "aff_domain": "gmail.com;ri.cmu.edu;cs.cmu.edu",
        "email": "gmail.com;ri.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Intel",
        "aff_unique_dep": "Robotics Institute;Intel Labs",
        "aff_unique_url": "https://www.cmu.edu;https://www.intel.com",
        "aff_unique_abbr": "CMU;Intel",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2ac4fe9ca1",
        "title": "An LP View of the M-best MAP problem",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6d3a2d24eb109dddf78374fe5d0ee067-Abstract.html",
        "author": "Menachem Fromer; Amir Globerson",
        "abstract": "We consider the problem of finding the M assignments with maximum probability in a probabilistic graphical model. We show how this problem can be formulated as a linear program (LP) on a particular polytope. We prove that, for tree graphs (and junction trees in general), this polytope has a particularly simple form and differs from the marginal polytope in a single inequality constraint. We use this characterization to provide an approximation scheme for non-tree graphs, by using the set of spanning trees over such graphs. The method we present puts the M -best inference problem in the context of LP relaxations, which have recently received considerable attention and have proven useful in solving difficult inference problems. We show empirically that our method often finds the provably exact M best configurations for problems of high tree-width. A common task in probabilistic modeling is finding the assignment with maximum probability given a model. This is often referred to as the MAP (maximum a-posteriori) problem. Of particular interest is the case of MAP in graphical models, i.e., models where the probability factors into a product over small subsets of variables. For general models, this is an NP-hard problem [11], and thus approximation algorithms are required. Of those, the class of LP based relaxations has recently received considerable attention [3, 5, 18]. In fact, it has been shown that some problems (e.g., fixed backbone protein design) can be solved exactly via sequences of increasingly tighter LP relaxations [13]. In many applications, one is interested not only in the MAP assignment but also in the M maximum probability assignments [19]. For example, in a protein design problem, we might be interested in the M amino acid sequences that are most stable on a given backbone structure [2]. In cases where the MAP problem is tractable, one can devise tractable algorithms for the M best problem [8, 19]. Specifically, for low tree-width graphs, this can be done via a variant of max-product [19]. However, when finding MAPs is not tractable, it is much less clear how to approximate the M best case. One possible approach is to use loopy max-product to obtain approximate max-marginals and use those to approximate the M best solutions [19]. However, this is largely a heuristic and does not provide any guarantees in terms of optimality certificates or bounds on the optimal values. LP approximations to MAP do enjoy such guarantees. Specifically, they provide upper bounds on the MAP value and optimality certificates. Furthermore, they often work for graphs with large tree-width [13]. The goal of the current work is to leverage the power of LP relaxations to the M best case. We begin by focusing on the problem of finding the second best solution. We show how it can be formulated as an LP over a polytope we call the \"assignment-excluding marginal polytope\". In the general case, this polytope may require an exponential number of inequalities, but we prove that when the graph is a tree it has a very compact representation. We proceed to use this result to obtain approximations to the second best problem, and show how these can be tightened in various ways. Next, we show how M best assignments can be found by relying on algorithms for 1",
        "bibtex": "@inproceedings{NIPS2009_6d3a2d24,\n author = {Fromer, Menachem and Globerson, Amir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An LP View of the M-best MAP problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6d3a2d24eb109dddf78374fe5d0ee067-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6d3a2d24eb109dddf78374fe5d0ee067-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6d3a2d24eb109dddf78374fe5d0ee067-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 197186,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15976020209092669723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem; School of Computer Science and Engineering, The Hebrew University of Jerusalem",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "bfe1bd07be",
        "title": "An Online Algorithm for Large Scale Image Similarity Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e97ee2054defb209c35fe4dc94599061-Abstract.html",
        "author": "Gal Chechik; Uri Shalit; Varun Sharma; Samy Bengio",
        "abstract": "Learning a measure of similarity between pairs of objects is a fundamental problem in machine learning. It stands in the core of classification methods like kernel machines, and is particularly useful for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, current approaches for learning similarity may not scale to large datasets with high dimensionality, especially when imposing metric constraints on the learned similarity. We describe OASIS, a method for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features. Scalability is achieved through online learning of a bilinear model over sparse representations using a large margin criterion and an efficient hinge loss cost. OASIS is accurate at a wide range of scales: on a standard benchmark with thousands of images, it is more precise than state-of-the-art methods, and faster by orders of magnitude. On 2 million images collected from the web, OASIS can be trained within 3 days on a single CPU. The non-metric similarities learned by OASIS can be transformed into metric similarities, achieving higher precisions than similarities that are learned as metrics in the first place. This suggests an approach for learning a metric from data that is larger by an order of magnitude than was handled before.",
        "bibtex": "@inproceedings{NIPS2009_e97ee205,\n author = {Chechik, Gal and Shalit, Uri and Sharma, Varun and Bengio, Samy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Online Algorithm for Large Scale Image Similarity Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e97ee2054defb209c35fe4dc94599061-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 392528,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5823920578930581826&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Google, Mountain View, CA; Google, Bengalooru, Karnataka, India; ICNC, The Hebrew University, Israel; Google, Mountain View, CA",
        "aff_domain": "google.com;google.com;mail.huji.ac.il;google.com",
        "email": "google.com;google.com;mail.huji.ac.il;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Google;Hebrew University",
        "aff_unique_dep": "Google;ICNC",
        "aff_unique_url": "https://www.google.com;https://www.huji.ac.il",
        "aff_unique_abbr": "Google;HUJI",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Mountain View;Bengalooru;",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "United States;India;Israel"
    },
    {
        "id": "0052fd571d",
        "title": "Analysis of SVM with Indefinite Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/7f1de29e6da19d22b51c68001e7e0e54-Abstract.html",
        "author": "Yiming Ying; Colin Campbell; Mark Girolami",
        "abstract": "The recent introduction of indefinite SVM by Luss and dAspremont [15] has effectively demonstrated SVM classification with a non-positive semi-definite  kernel (indefinite kernel).  This paper studies the properties of the objective function introduced there. In particular, we show that the objective function  is continuously differentiable and its gradient can be explicitly computed. Indeed, we further show that its gradient is Lipschitz continuous.  The main idea behind our analysis is that the objective function is smoothed by the penalty term, in its saddle (min-max) representation, measuring the distance between the indefinite kernel matrix and the proxy positive semi-definite one. Our elementary result greatly facilitates the application of gradient-based algorithms. Based on our analysis,  we further develop  Nesterovs smooth optimization approach [16,17] for indefinite SVM which has an optimal convergence rate for smooth problems. Experiments on various benchmark datasets validate our analysis and demonstrate the efficiency of our proposed algorithms.",
        "bibtex": "@inproceedings{NIPS2009_7f1de29e,\n author = {Ying, Yiming and Campbell, Colin and Girolami, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analysis of SVM with Indefinite Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/7f1de29e6da19d22b51c68001e7e0e54-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 208902,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8981931238921883468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3bde2195ac",
        "title": "Anomaly Detection with Score functions based on Nearest Neighbor Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/996a7fa078cc36c46d02f9af3bef918b-Abstract.html",
        "author": "Manqi Zhao; Venkatesh Saligrama",
        "abstract": "We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data. Anomalies are declared whenever the score of a test sample falls below q, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, q, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.",
        "bibtex": "@inproceedings{NIPS2009_996a7fa0,\n author = {Zhao, Manqi and Saligrama, Venkatesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Anomaly Detection with Score functions based on Nearest Neighbor Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/996a7fa078cc36c46d02f9af3bef918b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 287167,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11091462163779790208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "ECE Dept., Boston University, Boston, MA 02215; ECE Dept., Boston University, Boston, MA 02215",
        "aff_domain": "bu.edu;bu.edu",
        "email": "bu.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "ECE Dept.",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "73c8f83134",
        "title": "Approximating MAP by Compensating for Structural Relaxations",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html",
        "author": "Arthur Choi; Adnan Darwiche",
        "abstract": "We introduce a new perspective on approximations to the maximum a posteriori (MAP) task in probabilistic graphical models, that is based on simplifying a given instance, and then tightening the approximation.  First, we start with a structural relaxation of the original model.  We then infer from the relaxation its deficiencies, and compensate for them.  This perspective allows us to identify two distinct classes of approximations.  First, we find that max-product belief propagation can be viewed as a way to compensate for a relaxation, based on a particular idealized case for exactness.  We identify a second approach to compensation that is based on a more refined idealized case, resulting in a new approximation with distinct properties.  We go on to propose a new class of algorithms that, starting with a relaxation, iteratively yields tighter approximations.",
        "bibtex": "@inproceedings{NIPS2009_cfbce4c1,\n author = {Choi, Arthur and Darwiche, Adnan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximating MAP by Compensating for Structural Relaxations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/cfbce4c1d7c425baf21d6b6f2babe6be-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/cfbce4c1d7c425baf21d6b6f2babe6be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1567723,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3608336857739807002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a54dc6a9bd",
        "title": "Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/bdb106a0560c4e46ccc488ef010af787-Abstract.html",
        "author": "Sundeep Rangan; Vivek Goyal; Alyson K. Fletcher",
        "abstract": "The replica method is a non-rigorous but widely-used technique from statistical physics used in the asymptotic analysis of many large random nonlinear problems.  This paper applies the replica method to non-Gaussian MAP estimation.  It is shown that with large random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector ``decouples as n scalar MAP estimators.  The result is a counterpart to Guo and Verdus replica analysis on MMSE estimation. The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding and zero-norm estimation.  In the case of lasso estimation, the scalar estimator reduces to a soft-thresholding operator and for zero-norm estimation it reduces to a hard-threshold.  Among other benefits, the replica method provides a computationally tractable method for exactly computing various performance metrics including MSE and sparsity recovery.",
        "bibtex": "@inproceedings{NIPS2009_bdb106a0,\n author = {Rangan, Sundeep and Goyal, Vivek and Fletcher, Alyson K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/bdb106a0560c4e46ccc488ef010af787-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 107961,
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8457914177417576672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 26,
        "aff": "Qualcomm Technologies, Bedminster, NJ; University of California, Berkeley, Berkeley, CA; Mass. Inst. of Tech., Cambridge, MA",
        "aff_domain": "qualcomm.com;eecs.berkeley.edu;mit.edu",
        "email": "qualcomm.com;eecs.berkeley.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Qualcomm Technologies;University of California, Berkeley;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.qualcomm.com;https://www.berkeley.edu;https://www.mit.edu",
        "aff_unique_abbr": "Qualcomm;UC Berkeley;MIT",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Bedminster;Berkeley;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "177890f84b",
        "title": "Asymptotically Optimal Regularization in Smooth Parametric Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/43fa7f58b7eac7ac872209342e62e8f1-Abstract.html",
        "author": "Percy Liang; Guillaume Bouchard; Francis R. Bach; Michael I. Jordan",
        "abstract": "Many types of regularization schemes have been employed in statistical learning, each one motivated by some assumption about the problem domain.  In this paper, we present a unified asymptotic analysis of smooth regularizers, which allows us to see how the validity of these assumptions impacts the success of a particular regularizer.  In addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework.  We apply our analysis to several examples, including hybrid generative-discriminative learning and multi-task learning.",
        "bibtex": "@inproceedings{NIPS2009_43fa7f58,\n author = {Liang, Percy S and Bouchard, Guillaume and Bach, Francis and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Asymptotically Optimal Regularization in Smooth Parametric Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/43fa7f58b7eac7ac872209342e62e8f1-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/43fa7f58b7eac7ac872209342e62e8f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/43fa7f58b7eac7ac872209342e62e8f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 215213,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7492867496837528873&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "University of California, Berkeley; INRIA - \u00c9cole Normale Sup\u00e9rieure, France; Xerox Research Centre Europe, France; University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;ens.fr;xrce.xerox.com;cs.berkeley.edu",
        "email": "cs.berkeley.edu;ens.fr;xrce.xerox.com;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of California, Berkeley;INRIA;Xerox Research Centre Europe",
        "aff_unique_dep": ";\u00c9cole Normale Sup\u00e9rieure;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.inria.fr;https://www.xerox.com/en-us/innovation/research-centers/europe",
        "aff_unique_abbr": "UC Berkeley;INRIA;XRCE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "a3925135b1",
        "title": "Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0f840be9b8db4d3fbd5ba2ce59211f55-Abstract.html",
        "author": "Andreas Bartels; Matthew Blaschko; Jacquelyn A. Shelton",
        "abstract": "Resting state activity is brain activation that arises in the absence of any task, and is usually measured in awake subjects during prolonged fMRI scanning sessions where the only instruction given is to close the eyes and do nothing.  It has been recognized in recent years that resting state activity is implicated in a wide variety of brain function.  While certain networks of brain areas have different levels of activation at rest and during a task, there is nevertheless significant similarity between activations in the two cases.  This suggests that recordings of resting state activity can be used as a source of unlabeled data to augment discriminative regression techniques in a semi-supervised setting.  We evaluate this setting empirically yielding three main results: (i) regression tends to be improved by the use of Laplacian regularization even when no additional unlabeled data are available, (ii) resting state data may have a similar marginal distribution to that recorded during the execution of a visual processing task reinforcing the hypothesis that these conditions have similar types of activation, and (iii) this source of information can be broadly exploited to improve the robustness of empirical inference in fMRI studies, an inherently data poor domain.",
        "bibtex": "@inproceedings{NIPS2009_0f840be9,\n author = {Bartels, Andreas and Blaschko, Matthew and Shelton, Jacquelyn},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0f840be9b8db4d3fbd5ba2ce59211f55-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4971026,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=467155574181226104&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Visual Geometry Group, Department of Engineering Science, University of Oxford; Max Planck Institute for Biological Cybernetics, Fakult\u00e4t f\u00fcr Informations- und Kognitionswissenschaften, Universit\u00e4t T\u00fcbingen; Max Planck Institute for Biological Cybernetics, Centre for Integrative Neuroscience, Universit\u00e4t T\u00fcbingen",
        "aff_domain": "robots.ox.ac.uk;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "robots.ox.ac.uk;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Oxford;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Department of Engineering Science;Fakult\u00e4t f\u00fcr Informations- und Kognitionswissenschaften",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "Oxford;MPIBK",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Oxford;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "47ddcc8f97",
        "title": "Bayesian Belief Polarization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/3435c378bb76d4357324dd7e69f3cd18-Abstract.html",
        "author": "Alan Jern; Kai-min Chang; Charles Kemp",
        "abstract": "Situations in which people with opposing prior beliefs observe the same evidence and then strengthen those existing beliefs are frequently offered as evidence of human irrationality. This phenomenon, termed belief polarization, is typically assumed to be non-normative. We demonstrate, however, that a variety of cases of belief polarization are consistent with a Bayesian approach to belief revision. Simulation results indicate that belief polarization is not only possible but relatively common within the class of Bayesian models that we consider.",
        "bibtex": "@inproceedings{NIPS2009_3435c378,\n author = {Jern, Alan and Chang, Kai-min and Kemp, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Belief Polarization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/3435c378bb76d4357324dd7e69f3cd18-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/3435c378bb76d4357324dd7e69f3cd18-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 161681,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11131880585072582353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Psychology, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Department of Psychology, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cs.cmu.edu;cmu.edu",
        "email": "cmu.edu;cs.cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "aa9a83f598",
        "title": "Bayesian Nonparametric Models on Decomposable Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4e4b5fbbbb602b6d35bea8460aa8f8e5-Abstract.html",
        "author": "Francois Caron; Arnaud Doucet",
        "abstract": "Over recent years Dirichlet processes and the associated Chinese restaurant process (CRP) have found many applications in clustering while the Indian buffet process (IBP) is increasingly used to describe latent feature models. In the clustering case, we associate to each data point a latent allocation variable. These latent variables can share the same value and this induces a partition of the data set. The CRP is a prior distribution on such partitions.  In latent feature models, we associate to each data point a potentially infinite number of binary latent variables indicating the possession of some features and the IBP is a prior distribution on the associated infinite binary matrix. These prior distributions are attractive because they ensure exchangeability (over samples). We propose here extensions of these models to decomposable graphs. These models have appealing properties and can be easily learned using Monte Carlo techniques.",
        "bibtex": "@inproceedings{NIPS2009_4e4b5fbb,\n author = {Caron, Francois and Doucet, Arnaud},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Nonparametric Models on Decomposable Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 265932,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7102742330559524874&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "INRIA Bordeaux Sud\u2013Ouest + Institut de Math \u00b4ematiques de Bordeaux + University of Bordeaux, France; Departments of Computer Science & Statistics, University of British Columbia, Vancouver, Canada + The Institute of Statistical Mathematics, Tokyo, Japan",
        "aff_domain": "inria.fr;cs.ubc.ca",
        "email": "inria.fr;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;3+4",
        "aff_unique_norm": "INRIA;Institut de Math\u00e9matiques de Bordeaux;University of Bordeaux;University of British Columbia;Institute of Statistical Mathematics",
        "aff_unique_dep": ";Department of Mathematics;;Departments of Computer Science & Statistics;",
        "aff_unique_url": "https://www.inria.fr;https://www.ima.u-bordeaux.fr;https://www.u-bordeaux.fr;https://www.ubc.ca;https://www.ism.ac.jp",
        "aff_unique_abbr": "INRIA;IMB;UBordeaux;UBC;ISM",
        "aff_campus_unique_index": "0;2+3",
        "aff_campus_unique": "Bordeaux;;Vancouver;Tokyo",
        "aff_country_unique_index": "0+0+0;1+2",
        "aff_country_unique": "France;Canada;Japan"
    },
    {
        "id": "29a609cb80",
        "title": "Bayesian Source Localization with the Multivariate Laplace Prior",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html",
        "author": "Marcel V. Gerven; Botond Cseke; Robert Oostenveld; Tom Heskes",
        "abstract": "We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the specification of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efficient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior.",
        "bibtex": "@inproceedings{NIPS2009_e7b24b11,\n author = {Gerven, Marcel and Cseke, Botond and Oostenveld, Robert and Heskes, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Source Localization with the Multivariate Laplace Prior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 551488,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14779021282063317948&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "74fe42d81d",
        "title": "Bayesian Sparse Factor Models and DAGs Inference and Comparison",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/9b698eb3105bd82528f23d0c92dedfc0-Abstract.html",
        "author": "Ricardo Henao; Ole Winther",
        "abstract": "In this paper we present a novel approach to learn directed acyclic graphs (DAG) and factor models within the same framework while also allowing for model comparison between them. For this purpose, we exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identifiability and predictive densities to perform the model comparison. We require identifiability to be able to produce variable orderings leading to valid DAGs and sparsity to learn the structures. The effectiveness of our approach is demonstrated through extensive experiments on artificial and biological data showing that our approach outperform a number of state of the art methods.",
        "bibtex": "@inproceedings{NIPS2009_9b698eb3,\n author = {Henao, Ricardo and Winther, Ole},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Sparse Factor Models and DAGs Inference and Comparison},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/9b698eb3105bd82528f23d0c92dedfc0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/9b698eb3105bd82528f23d0c92dedfc0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 163902,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4435220089184791570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "DTU Informatics, Technical University of Denmark+Bioinformatics Centre, University of Copenhagen; DTU Informatics, Technical University of Denmark+Bioinformatics Centre, University of Copenhagen",
        "aff_domain": "binf.ku.dk;imm.dtu.dk",
        "email": "binf.ku.dk;imm.dtu.dk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Technical University of Denmark;University of Copenhagen",
        "aff_unique_dep": "DTU Informatics;Bioinformatics Centre",
        "aff_unique_url": "https://www.dtu.dk;https://www.ku.dk",
        "aff_unique_abbr": "DTU;UCPH",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "b2a217a999",
        "title": "Bayesian estimation of orientation preference maps",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ef0d3930a7b6c95bd2b32ed45989c61f-Abstract.html",
        "author": "Sebastian Gerwinn; Leonard White; Matthias Kaschube; Matthias Bethge; Jakob H. Macke",
        "abstract": "Imaging techniques such as optical imaging of intrinsic signals, 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial scales. Here, we present Bayesian methods based on Gaussian processes for extracting topographic maps from functional imaging data.  In particular, we focus on the estimation of orientation preference maps (OPMs) from intrinsic signal imaging data.  We model the underlying map as a bivariate Gaussian process, with a prior covariance function that reflects known properties of OPMs, and a noise covariance adjusted to the data.  The posterior mean can be interpreted as an optimally smoothed estimate of the map, and can be used for model based interpolations of the map from sparse measurements.  By sampling from the posterior distribution, we can get error bars on statistical properties such as preferred orientations,  pinwheel locations or -counts.  Finally, the use of an explicit probabilistic model facilitates interpretation of parameters and provides the basis for decoding studies.  We demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex.",
        "bibtex": "@inproceedings{NIPS2009_ef0d3930,\n author = {Gerwinn, Sebastian and White, Leonard and Kaschube, Matthias and Bethge, Matthias and Macke, Jakob H},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian estimation of orientation preference maps},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ef0d3930a7b6c95bd2b32ed45989c61f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 489480,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13900104549267861038&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "MPI for Biological Cybernetics and University of T\u00fcbingen; MPI for Biological Cybernetics and University of T\u00fcbingen; Duke Institute for Brain Sciences, Duke University; Lewis-Sigler Institute for Integrative Genomics and Department of Physics, Princeton University; MPI for Biological Cybernetics and University of T\u00fcbingen",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;mc.duke.edu;princeton.edu;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;mc.duke.edu;princeton.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Duke University;Princeton University",
        "aff_unique_dep": ";Duke Institute for Brain Sciences;Lewis-Sigler Institute for Integrative Genomics",
        "aff_unique_url": "https://www.biological-cybernetics.de;https://www.duke.edu;https://www.princeton.edu",
        "aff_unique_abbr": "MPIBC;Duke;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "adc42cb36b",
        "title": "Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/93db85ed909c13838ff95ccfa94cebd9-Abstract.html",
        "author": "Tomasz Malisiewicz; Alyosha Efros",
        "abstract": "The use of context is critical for scene understanding in computer vision, where the recognition of an object is driven by both local appearance and the objects relationship to other elements of the scene (context).  Most current approaches rely on modeling the relationships between object categories as a source of context. In this paper we seek to move beyond categories to provide a richer appearance-based model of context.  We present an exemplar-based model of objects and their relationships, the Visual Memex, that encodes both local appearance and 2D spatial context between object instances. We evaluate our model on Torralbas proposed Context Challenge against a baseline category-based system. Our experiments suggest that moving beyond categories for context modeling appears to be quite beneficial, and may be the critical missing ingredient in scene understanding systems.",
        "bibtex": "@inproceedings{NIPS2009_93db85ed,\n author = {Malisiewicz, Tomasz and Efros, Alyosha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/93db85ed909c13838ff95ccfa94cebd9-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/93db85ed909c13838ff95ccfa94cebd9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/93db85ed909c13838ff95ccfa94cebd9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 633555,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4620933639941276086&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bb693cd4d3",
        "title": "Beyond Convexity: Online Submodular Minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f770b62bc8f42a0b66751fe636fc6eb0-Abstract.html",
        "author": "Elad Hazan; Satyen Kale",
        "abstract": "We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efficient and are Hannan-consistent in both the full information and bandit settings.",
        "bibtex": "@inproceedings{NIPS2009_f770b62b,\n author = {Hazan, Elad and Kale, Satyen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond Convexity: Online Submodular Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f770b62bc8f42a0b66751fe636fc6eb0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 188555,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5987553277885459766&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "IBM Almaden Research Center; Yahoo! Research",
        "aff_domain": "us.ibm.com;yahoo-inc.com",
        "email": "us.ibm.com;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IBM;Yahoo!",
        "aff_unique_dep": "Research Center;Yahoo! Research",
        "aff_unique_url": "https://www.ibm.com/research;https://research.yahoo.com",
        "aff_unique_abbr": "IBM;Yahoo!",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Almaden;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "419554183d",
        "title": "Bilinear classifiers for visual recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/147702db07145348245dc5a2f2fe5683-Abstract.html",
        "author": "Hamed Pirsiavash; Deva Ramanan; Charless C. Fowlkes",
        "abstract": "We describe an algorithm for learning bilinear SVMs. Bilinear classifiers are a discriminative variant of bilinear models, which capture the dependence of data on multiple factors. Such models are particularly appropriate for visual data that is better represented as a matrix or tensor, rather than a vector. Matrix encodings allow for more natural regularization through rank restriction. For example, a rank-one scanning-window classifier yields a separable filter. Low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time. We learn low-rank models with bilinear classifiers. We also use bilinear classifiers for transfer learning by sharing linear factors between different classification tasks. Bilinear classifiers are trained with biconvex programs. Such programs are optimized with coordinate descent, where each coordinate step requires solving a convex program - in our case, we use a standard off-the-shelf SVM solver. We demonstrate bilinear SVMs on difficult problems of people detection in video sequences and action classification of video sequences, achieving state-of-the-art results in both.",
        "bibtex": "@inproceedings{NIPS2009_147702db,\n author = {Pirsiavash, Hamed and Ramanan, Deva and Fowlkes, Charless},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bilinear classifiers for visual recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/147702db07145348245dc5a2f2fe5683-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/147702db07145348245dc5a2f2fe5683-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/147702db07145348245dc5a2f2fe5683-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 595715,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18010294779956699880&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computer Science, University of California at Irvine; Department of Computer Science, University of California at Irvine; Department of Computer Science, University of California at Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bc861f2680",
        "title": "Boosting with Spatial Regularization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f2217062e9a397a1dca429e7d70bc6ca-Abstract.html",
        "author": "Yongxin Xi; Uri Hasson; Peter J. Ramadge; Zhen J. Xiang",
        "abstract": "By adding a spatial regularization kernel to a standard loss function formulation of the boosting problem, we develop a framework for spatially informed boosting.  From this regularized loss framework we derive an efficient boosting algorithm that uses additional weights/priors on the base classifiers.  We prove that the proposed algorithm exhibits a ``grouping effect, which encourages the selection of all spatially local, discriminative base classifiers.  The algorithms primary advantage is in applications where the trained classifier is used to identify the spatial pattern of discriminative information, e.g. the voxel selection problem in fMRI.  We demonstrate the algorithms performance on various data sets.",
        "bibtex": "@inproceedings{NIPS2009_f2217062,\n author = {Xi, Yongxin and Hasson, Uri and Ramadge, Peter J and Xiang, Zhen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Boosting with Spatial Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/f2217062e9a397a1dca429e7d70bc6ca-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f2217062e9a397a1dca429e7d70bc6ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 376951,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9588531619966390099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering, Princeton University, Princeton NJ, USA; Department of Electrical Engineering, Princeton University, Princeton NJ, USA; Department of Psychology, and Neuroscience Institute, Princeton University, Princeton NJ, USA; Department of Electrical Engineering, Princeton University, Princeton NJ, USA",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dcbc3d7f0f",
        "title": "Bootstrapping from Game Tree Search",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/389bc7bb1e1c2a5e7e147703232a88f6-Abstract.html",
        "author": "Joel Veness; David Silver; Alan Blair; William Uther",
        "abstract": "In this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function, by updating the heuristic towards the values computed by an alpha-beta search. Our algorithm differs from previous approaches to learning from search, such as Samuels checkers player and the TD-Leaf algorithm, in two key ways. First, we update all nodes in the search tree, rather than a single node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function. We implemented our algorithm in a chess program Meep, using a linear heuristic function. After initialising its weight vector to small random values, Meep was able to learn high quality weights from self-play alone. When tested online against human opponents, Meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play.",
        "bibtex": "@inproceedings{NIPS2009_389bc7bb,\n author = {Veness, Joel and Silver, David and Blair, Alan and Uther, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bootstrapping from Game Tree Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/389bc7bb1e1c2a5e7e147703232a88f6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 203690,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=139483045995533140&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "University of NSW and NICTA; University of Alberta; NICTA and the University of NSW; University of NSW and NICTA",
        "aff_domain": "cse.unsw.edu.au;cs.ualberta.ca;nicta.com.au;cse.unsw.edu.au",
        "email": "cse.unsw.edu.au;cs.ualberta.ca;nicta.com.au;cse.unsw.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of New South Wales;University of Alberta",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unsw.edu.au;https://www.ualberta.ca",
        "aff_unique_abbr": "UNSW;UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Australia;Canada"
    },
    {
        "id": "c2bde66387",
        "title": "Breaking Boundaries Between Induction Time and Diagnosis Time Active Information Acquisition",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/fc221309746013ac554571fbd180e1c8-Abstract.html",
        "author": "Ashish Kapoor; Eric Horvitz",
        "abstract": "There has been a clear distinction between induction or training time and diagnosis time active information acquisition. While active learning during induction focuses on acquiring data that promises to provide the best classification model, the goal at diagnosis time focuses completely on next features to observe about the test case at hand in order to make better predictions about the case. We introduce a model and inferential methods that breaks this distinction. The methods can be used to extend case libraries under a budget but, more fundamentally, provide a framework for guiding agents to collect data under scarce resources, focused by diagnostic challenges. This extension to active learning leads to a new class of policies for real-time diagnosis, where recommended information-gathering sequences include actions that simultaneously seek new data for the case at hand and for cases in the training set.",
        "bibtex": "@inproceedings{NIPS2009_fc221309,\n author = {Kapoor, Ashish and Horvitz, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Breaking Boundaries Between Induction Time and Diagnosis Time Active Information Acquisition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/fc221309746013ac554571fbd180e1c8-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/fc221309746013ac554571fbd180e1c8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/fc221309746013ac554571fbd180e1c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 544720,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14080999442764606583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Microsoft Research; Microsoft Research",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e24abd72fa",
        "title": "Canonical Time Warping for Alignment of Human Behavior",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html",
        "author": "Feng Zhou; Fernando Torre",
        "abstract": "Alignment of time series is an important problem to solve in many scientific disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of the behavior between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping for temporal alignment; and (ii) it extends CCA to allow local spatial deformations. We show CTWs effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of two people with similar facial expressions. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on dynamic time warping.",
        "bibtex": "@inproceedings{NIPS2009_2ca65f58,\n author = {Zhou, Feng and Torre, Fernando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Canonical Time Warping for Alignment of Human Behavior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4759082,
        "gs_citation": 323,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17873220578687093477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "www.f-zhou.com;cs.cmu.edu",
        "email": "www.f-zhou.com;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4ace910e78",
        "title": "Clustering sequence sets for motif discovery",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4e0928de075538c593fbdabb0c5ef2c3-Abstract.html",
        "author": "Jong K. Kim; Seungjin Choi",
        "abstract": "Most of existing methods for DNA motif discovery consider only a single set of sequences to find an over-represented motif. In contrast, we consider multiple sets of sequences where we group sets associated with the same motif into a cluster, assuming that each set involves a single motif. Clustering sets of sequences yields clusters of coherent motifs, improving signal-to-noise ratio or enabling us to identify multiple motifs. We present a probabilistic model for DNA motif discovery where we identify multiple motifs through searching for patterns which are shared across multiple sets of sequences. Our model infers cluster-indicating latent variables and learns motifs simultaneously, where these two tasks interact with each other. We show that our model can handle various motif discovery problems, depending on how to construct multiple sets of sequences. Experiments on three different problems for discovering DNA motifs emphasize the useful behavior and confirm the substantial gains over existing methods where only single set of sequences is considered.",
        "bibtex": "@inproceedings{NIPS2009_4e0928de,\n author = {Kim, Jong and Choi, Seungjin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering sequence sets for motif discovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4e0928de075538c593fbdabb0c5ef2c3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 511118,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8595470249478115897&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Pohang University of Science and Technology; Department of Computer Science, Pohang University of Science and Technology",
        "aff_domain": "postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Pohang University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pohang",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "1a6d77418f",
        "title": "Code-specific policy gradient rules for spiking neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/04ecb1fa28506ccb6f72b12c0245ddbc-Abstract.html",
        "author": "Henning Sprekeler; Guillaume Hennequin; Wulfram Gerstner",
        "abstract": "Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to influence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for  three different example codes - a spike count code, a spike timing code and the most general ``full spike train code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features  belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems.",
        "bibtex": "@inproceedings{NIPS2009_04ecb1fa,\n author = {Sprekeler, Henning and Hennequin, Guillaume and Gerstner, Wulfram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Code-specific policy gradient rules for spiking neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/04ecb1fa28506ccb6f72b12c0245ddbc-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/04ecb1fa28506ccb6f72b12c0245ddbc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/04ecb1fa28506ccb6f72b12c0245ddbc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 655278,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11293755492099840688&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Laboratory for Computational Neuroscience, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne; Laboratory for Computational Neuroscience, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne; Laboratory for Computational Neuroscience, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
        "aff_domain": "epfl.ch; ; ",
        "email": "epfl.ch; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "Laboratory for Computational Neuroscience",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "a7edd23b9a",
        "title": "Complexity of Decentralized Control: Special Cases",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/fec8d47d412bcbeece3d9128ae855a7a-Abstract.html",
        "author": "Martin Allen; Shlomo Zilberstein",
        "abstract": "The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases.  Some reductions in complexity have been achieved by exploiting independence relations in some models.  We show that these results are somewhat limited:  when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case.",
        "bibtex": "@inproceedings{NIPS2009_fec8d47d,\n author = {Allen, Martin and Zilberstein, Shlomo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Complexity of Decentralized Control: Special Cases},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/fec8d47d412bcbeece3d9128ae855a7a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/fec8d47d412bcbeece3d9128ae855a7a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 173075,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=103968099834227810&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Connecticut College, New London, CT 06320; Department of Computer Science, University of Massachusetts, Amherst, MA 01003",
        "aff_domain": "conncoll.edu;cs.umass.edu",
        "email": "conncoll.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Connecticut College;University of Massachusetts Amherst",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.connecticutcollege.edu;https://www.umass.edu",
        "aff_unique_abbr": "Conn College;UMass Amherst",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "New London;Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c3a6dbd2b3",
        "title": "Compositionality of optimal control laws",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/3eb71f6293a2a31f3569e10af6552658-Abstract.html",
        "author": "Emanuel Todorov",
        "abstract": "We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. The primitives are themselves feedback controllers pursuing their own agendas. They are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task. The resulting composite control law is provably optimal when the problem belongs to a certain class. This class is rather general and yet has a number of unique properties - one of which is that the Bellman equation can be made linear even for non-linear or discrete dynamics. This gives rise to the compositionality developed here. In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i.e. non-linear mixtures of linear-quadratic regulators) without requiring the final cost to be quadratic. More generally, a natural set of control primitives can be constructed by applying SVD to Greens function of the Bellman equation. We illustrate the theory in the context of human arm movements. The ideas of optimality and compositionality are both very prominent in the field of motor control, yet they are hard to reconcile. Our work makes this possible.",
        "bibtex": "@inproceedings{NIPS2009_3eb71f62,\n author = {Todorov, Emanuel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Compositionality of optimal control laws},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/3eb71f6293a2a31f3569e10af6552658-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/3eb71f6293a2a31f3569e10af6552658-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/3eb71f6293a2a31f3569e10af6552658-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 492884,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9437566848331349870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Applied Mathematics and Computer Science & Engineering, University of Washington",
        "aff_domain": "cs.washington.edu",
        "email": "cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Applied Mathematics and Computer Science & Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e2bd0ac00b",
        "title": "Compressed Least-Squares Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/01882513d5fa7c329e940dda99b12147-Abstract.html",
        "author": "Odalric Maillard; R\u00e9mi Munos",
        "abstract": "We consider the problem of learning, from K input data, a regression function in a function space of high dimension N using projections onto a random subspace of lower dimension M. From any linear approximation algorithm using empirical risk minimization (possibly penalized), we provide bounds on the excess risk of the estimate computed in the projected subspace (compressed domain) in terms of the excess risk of the estimate built in the high-dimensional space (initial domain). We apply the analysis to the ordinary Least-Squares regression and show that by choosing M=O(\\sqrt{K}), the estimation error (for the quadratic loss) of the ``Compressed Least Squares Regression is O(1/\\sqrt{K}) up to logarithmic factors. We also discuss the numerical complexity of several algorithms (both in initial and compressed domains) as a function of N, K, and M.",
        "bibtex": "@inproceedings{NIPS2009_01882513,\n author = {Maillard, Odalric and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Compressed Least-Squares Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/01882513d5fa7c329e940dda99b12147-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/01882513d5fa7c329e940dda99b12147-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/01882513d5fa7c329e940dda99b12147-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/01882513d5fa7c329e940dda99b12147-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 204618,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6325633147681835458&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "SequeL Project, INRIA Lille - Nord Europe, France; SequeL Project, INRIA Lille - Nord Europe, France",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA Lille - Nord Europe",
        "aff_unique_dep": "SequeL Project",
        "aff_unique_url": "https://www.inria.fr/en/centre/lille-nord-europe",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "4aa35fdd18",
        "title": "Conditional Neural Fields",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html",
        "author": "Jian Peng; Liefeng Bo; Jinbo Xu",
        "abstract": "Conditional random fields (CRF) are quite successful on sequence labeling tasks such as natural language processing and biological sequence analysis. CRF models use linear potential functions to represent the relationship between input features and outputs. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and outputs is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input features and outputs we propose Conditional Neural Fields (CNF), a new conditional probabilistic graphical model for sequence labeling. Our CNF model extends CRF by adding one (or possibly several) middle layer between input features and outputs. The middle layer consists of a number of hidden parameterized gates, each acting as a local neural network node or feature extractor to capture the nonlinear relationship between input features and outputs. Therefore, conceptually this CNF model is much more expressive than the linear CRF model. To better control the complexity of the CNF model, we also present a hyperparameter optimization procedure within the evidence framework. Experiments on two widely-used benchmarks indicate that this CNF model performs significantly better than a number of popular methods. In particular, our CNF model is the best among about ten machine learning methods for protein secondary tructure prediction and also among a few of the best methods for handwriting recognition.",
        "bibtex": "@inproceedings{NIPS2009_e820a45f,\n author = {Peng, Jian and Bo, Liefeng and Xu, Jinbo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Conditional Neural Fields},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e820a45f1dfc7b95282d10b6087e11c0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 171201,
        "gs_citation": 231,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13084845385035355254&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI Chicago",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "476de32764",
        "title": "Conditional Random Fields with High-Order Features for Sequence Labeling",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/94f6d7e04a4d452035300f18b984988c-Abstract.html",
        "author": "Nan Ye; Wee S. Lee; Hai L. Chieu; Dan Wu",
        "abstract": "Dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems. However, only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we show that it is possible to design efficient inference algorithms for a conditional random field using features that depend on long consecutive label sequences (high-order features), as long as the number of distinct label sequences in the features used is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective.",
        "bibtex": "@inproceedings{NIPS2009_94f6d7e0,\n author = {Ye, Nan and Lee, Wee and Chieu, Hai and Wu, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Conditional Random Fields with High-Order Features for Sequence Labeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/94f6d7e04a4d452035300f18b984988c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/94f6d7e04a4d452035300f18b984988c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 661854,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11345283237759072219&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore; DSO National Laboratories; Singapore MIT Alliance, National University of Singapore",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg;dso.org.sg;nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg;dso.org.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "National University of Singapore;DSO National Laboratories",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.dso.org.sg",
        "aff_unique_abbr": "NUS;DSO",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Singapore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "ba9cb61c86",
        "title": "Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html",
        "author": "Roy Anati; Kostas Daniilidis",
        "abstract": "We present a system which constructs a topological map of an environment given a sequence of images. This system includes a novel image similarity score which uses dynamic programming to match images using both the appearance and relative positions of local features simultaneously. Additionally an MRF is constructed to model the probability of loop-closures. A locally optimal labeling is found using Loopy-BP. Finally we outline a method to generate a topological map from loop closure data. Results are presented on four urban sequences and one indoor sequence.",
        "bibtex": "@inproceedings{NIPS2009_303ed4c6,\n author = {Anati, Roy and Daniilidis, Kostas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/303ed4c69846ab36c2904d3ba8573050-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1005512,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10850996732213455365&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "62544742d8",
        "title": "Construction of Nonparametric Bayesian Models from Parametric Bayes Equations",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/892c91e0a653ba19df81a90f89d99bcd-Abstract.html",
        "author": "Peter Orbanz",
        "abstract": "We consider the general problem of constructing nonparametric   Bayesian models on infinite-dimensional random objects, such   as functions, infinite graphs or infinite permutations.   The problem has generated much interest in machine learning,   where it is treated heuristically, but has not been    studied in full generality in nonparametric Bayesian statistics, which tends to   focus on models over probability distributions.    Our approach applies a standard tool of stochastic process   theory, the construction of stochastic processes from their   finite-dimensional marginal distributions.    The main contribution of the paper is a generalization   of the classic Kolmogorov extension theorem to conditional   probabilities.   This extension allows a rigorous construction of nonparametric Bayesian models   from systems of finite-dimensional, parametric Bayes equations.   Using this approach, we show (i)   how existence of a conjugate posterior for    the nonparametric model can be guaranteed by choosing   conjugate finite-dimensional models in the construction, (ii) how the   mapping to the posterior parameters of the nonparametric   model can be explicitly determined, and (iii) that   the construction of conjugate models in essence requires the   finite-dimensional models to be in the exponential family.   As an application of our constructive framework,    we derive a model on infinite   permutations, the nonparametric Bayesian analogue of a model   recently proposed for the analysis of rank data.",
        "bibtex": "@inproceedings{NIPS2009_892c91e0,\n author = {Orbanz, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Construction of Nonparametric Bayesian Models from Parametric Bayes Equations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/892c91e0a653ba19df81a90f89d99bcd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/892c91e0a653ba19df81a90f89d99bcd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 108478,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5007472838331546979&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Cambridge+ETH Zurich",
        "aff_domain": "eng.cam.ac.uk",
        "email": "eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "University of Cambridge;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ethz.ch",
        "aff_unique_abbr": "Cambridge;ETHZ",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+1",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "cdb78dfedf",
        "title": "Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html",
        "author": "Hamid R. Maei; Csaba Szepesv\u00e1ri; Shalabh Bhatnagar; Doina Precup; David Silver; Richard S. Sutton",
        "abstract": "We introduce the first  temporal-difference  learning algorithms that converge  with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD($\\lambda$), Q-learning and Sarsa have been used successfully with function approximation in many applications.  However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al (2009a,b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman-error, and algorithms that perform stochastic gradient-descent on this function. In this paper, we generalize their work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the  asymptotic almost-sure convergence  of both algorithms for any finite Markov decision process and any smooth value function approximator, under usual stochastic approximation conditions. The computational complexity per iteration scales linearly with the number of parameters of the approximator. The algorithms are incremental and are guaranteed to converge to locally optimal solutions.",
        "bibtex": "@inproceedings{NIPS2009_3a15c7d0,\n author = {Maei, Hamid and Szepesv\\'{a}ri, Csaba and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/3a15c7d0bbe60300a39f76f8a5ba6896-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/3a15c7d0bbe60300a39f76f8a5ba6896-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 544972,
        "gs_citation": 373,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2756124880537887188&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dd6e2ecc99",
        "title": "Convex Relaxation of Mixture Regression with Efficient Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/7250eb93b3c18cc9daa29cf58af7a004-Abstract.html",
        "author": "Novi Quadrianto; John Lim; Dale Schuurmans; Tib\u00e9rio S. Caetano",
        "abstract": "We develop a convex relaxation of maximum a posteriori estimation of a mixture of regression models. Although our relaxation involves a semidefinite matrix variable, we reformulate the problem to eliminate the need for general semidefinite programming. In particular, we provide two reformulations that admit fast algorithms. The first is a max-min spectral reformulation exploiting quasi-Newton descent. The second is a min-min reformulation consisting of fast alternating steps of closed-form updates. We evaluate the methods against Expectation-Maximization in a real problem of motion segmentation from video data.",
        "bibtex": "@inproceedings{NIPS2009_7250eb93,\n author = {Quadrianto, Novi and Lim, John and Schuurmans, Dale and Caetano, Tib\\'{e}rio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convex Relaxation of Mixture Regression with Efficient Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/7250eb93b3c18cc9daa29cf58af7a004-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/7250eb93b3c18cc9daa29cf58af7a004-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2524337,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2751249746817167483&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "NICTA - Australian National University; NICTA - Australian National University; NICTA - Australian National University; University of Alberta",
        "aff_domain": "nicta.com.au;nicta.com.au;nicta.com.au;cs.ualberta.ca",
        "email": "nicta.com.au;nicta.com.au;nicta.com.au;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Australian National University;University of Alberta",
        "aff_unique_dep": "NICTA;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.ualberta.ca",
        "aff_unique_abbr": "ANU;UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Australia;Canada"
    },
    {
        "id": "fa0e8b49af",
        "title": "Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4ea06fbc83cdd0a06020c35d50e1e89a-Abstract.html",
        "author": "Arno Onken; Steffen Gr\u00fcnew\u00e4lder; Klaus Obermayer",
        "abstract": "The linear correlation coefficient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefficient is in general insufficient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-fire neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We find that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25% and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poisson-like marginals is well characterized by the linear correlation coefficient and verify it for different copula-based models.",
        "bibtex": "@inproceedings{NIPS2009_4ea06fbc,\n author = {Onken, Arno and Gr\\\"{u}new\\\"{a}lder, Steffen and Obermayer, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/4ea06fbc83cdd0a06020c35d50e1e89a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4ea06fbc83cdd0a06020c35d50e1e89a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 364176,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10415072850097593658&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Technische Universit \u00a8at Berlin/ BCCN Berlin; University College London; Technische Universit \u00a8at Berlin/BCCN Berlin",
        "aff_domain": "cs.tu-berlin.de;cs.ucl.ac.uk;cs.tu-berlin.de",
        "email": "cs.tu-berlin.de;cs.ucl.ac.uk;cs.tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;University College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.ucl.ac.uk",
        "aff_unique_abbr": "TU Berlin;UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berlin;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "066731feb9",
        "title": "DUOL: A Double Updating Approach for Online Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/013d407166ec4fa56eb1e1f8cbe183b9-Abstract.html",
        "author": "Peilin Zhao; Steven C. Hoi; Rong Jin",
        "abstract": "In most online learning algorithms, the weights assigned to the misclassified examples (or support vectors) remain unchanged during the entire learning process. This is clearly insufficient since when a new misclassified example is added to the pool of support vectors, we generally expect it to affect the weights for the existing support vectors. In this paper, we propose a new online learning method, termed Double Updating Online Learning\", or \"DUOL\" for short. Instead of only assigning a fixed weight to the misclassified example received in current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be significantly improved by the proposed online learning method. Encouraging experimental results show that the proposed technique is in general considerably more effective than the state-of-the-art online learning algorithms.\"",
        "bibtex": "@inproceedings{NIPS2009_013d4071,\n author = {Zhao, Peilin and Hoi, Steven and Jin, Rong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {DUOL: A Double Updating Approach for Online Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/013d407166ec4fa56eb1e1f8cbe183b9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 339885,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9605156559749718283&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "School of Comp. Eng., Nanyang Tech. University, Singapore 639798; School of Comp. Eng., Nanyang Tech. University, Singapore 639798; Dept. of Comp. Sci. & Eng., Michigan State University, East Lansing, MI, 48824",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;cse.msu.edu",
        "email": "ntu.edu.sg;ntu.edu.sg;cse.msu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Nanyang Technological University;Michigan State University",
        "aff_unique_dep": "School of Computer Engineering;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.msu.edu",
        "aff_unique_abbr": "NTU;MSU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Singapore;East Lansing",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "35dd3872a9",
        "title": "Data-driven calibration of linear estimators with minimal penalties",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html",
        "author": "Sylvain Arlot; Francis R. Bach",
        "abstract": "This paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. We propose a new algorithm which first estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. Then, plugging our variance estimate in Mallows $C_L$ penalty is proved to lead to an algorithm satisfying an oracle inequality. Simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation.",
        "bibtex": "@inproceedings{NIPS2009_9c838d2e,\n author = {Arlot, Sylvain and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Data-driven calibration of linear estimators with minimal penalties},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/9c838d2e45b2ad1094d42f4ef36764f6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/9c838d2e45b2ad1094d42f4ef36764f6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120173,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13054136453992328234&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a14072f4b2",
        "title": "Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/3b8a614226a953a8cd9526fca6fe9ba5-Abstract.html",
        "author": "Chong Wang; David M. Blei",
        "abstract": "We present a nonparametric hierarchical Bayesian model of document collections that decouples sparsity and smoothness in the component distributions (i.e., the ``topics). In the sparse topic model (STM), each topic is represented by a bank of selector variables that determine which terms appear in the topic. Thus each topic is associated with a subset of the vocabulary, and topic smoothness is modeled on this subset. We develop an efficient Gibbs sampler for the STM that includes a general-purpose method for sampling from a Dirichlet mixture with a combinatorial number of components. We demonstrate the STM on four real-world datasets. Compared to traditional approaches, the empirical results show that STMs give better predictive performance with simpler inferred models.",
        "bibtex": "@inproceedings{NIPS2009_3b8a6142,\n author = {Wang, Chong and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/3b8a614226a953a8cd9526fca6fe9ba5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/3b8a614226a953a8cd9526fca6fe9ba5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/3b8a614226a953a8cd9526fca6fe9ba5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/3b8a614226a953a8cd9526fca6fe9ba5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 196121,
        "gs_citation": 177,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11944759182158814694&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computer Science Department, Princeton University; Computer Science Department, Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d5a0c6394",
        "title": "Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0f96613235062963ccde717b18f97592-Abstract.html",
        "author": "Anne Hsu; Thomas L. Griffiths",
        "abstract": "A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing  restrictions on verb alternations, without negative  evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate  language-specific knowledge. However, recently, researchers have shown  that statistical models are capable of learning complex rules from only  positive evidence. These two kinds of learnability analyses differ in their assumptions about the role of the distribution from which linguistic  input is generated.  The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution  from which the sentences are generated, analogous to discriminative  approaches in machine learning. The latter assume that learners are trying  to estimate a generative model, with sentences being sampled from that  model. We show that these two learning approaches differ in their use of implicit negative evidence -- the absence of a sentence -- when learning  verb alternations, and demonstrate that human learners can produce results  consistent with the predictions of both approaches, depending on the  context in which the learning problem is presented.",
        "bibtex": "@inproceedings{NIPS2009_0f966132,\n author = {Hsu, Anne and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0f96613235062963ccde717b18f97592-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0f96613235062963ccde717b18f97592-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0f96613235062963ccde717b18f97592-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101123,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2600785118535998236&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Psychology, University of California, Berkeley; Department of Psychology, University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1b2ade1f3a",
        "title": "Directed Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html",
        "author": "Yi-hao Kao; Benjamin V. Roy; Xiang Yan",
        "abstract": "When used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. Empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. We propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. We also develop a theory that motivates the algorithm.",
        "bibtex": "@inproceedings{NIPS2009_0c74b7f7,\n author = {Kao, Yi-hao and Roy, Benjamin and Yan, Xiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Directed Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 192981,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1041360062496814433&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "42f5b9c8a3",
        "title": "Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6c8349cc7260ae62e3b1396831a8398f-Abstract.html",
        "author": "Shuang-hong Yang; Hongyuan Zha; Bao-gang Hu",
        "abstract": "We propose Dirichlet-Bernoulli Alignment (DBA), a generative model for corpora in which each pattern (e.g., a document) contains a set of instances (e.g., paragraphs in the document) and belongs to multiple classes. By casting predefined classes as latent Dirichlet variables (i.e., instance level labels), and modeling the multi-label of each pattern as Bernoulli variables conditioned on the weighted empirical average of topic assignments, DBA automatically aligns the latent topics discovered from data to human-defined classes. DBA is useful for both pattern classification and instance disambiguation, which are tested on text classification and named entity disambiguation for web search queries respectively.",
        "bibtex": "@inproceedings{NIPS2009_6c8349cc,\n author = {Yang, Shuang-hong and Zha, Hongyuan and Hu, Bao-gang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6c8349cc7260ae62e3b1396831a8398f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6c8349cc7260ae62e3b1396831a8398f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6c8349cc7260ae62e3b1396831a8398f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 136612,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1530766855248005302&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "College of Computing, Georgia Tech; College of Computing, Georgia Tech; NLPR & LIAMA, Chinese Academy of Sciences",
        "aff_domain": "gatech.edu;cc.gatech.edu;nlpr.ia.ac.cn",
        "email": "gatech.edu;cc.gatech.edu;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Georgia Institute of Technology;Chinese Academy of Sciences",
        "aff_unique_dep": "College of Computing;NLPR & LIAMA",
        "aff_unique_url": "https://www.gatech.edu;http://www.cas.cn",
        "aff_unique_abbr": "Georgia Tech;CAS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "e64e76b4ff",
        "title": "Discrete MDL Predicts in Total Variation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d3d9446802a44259755d38e6d163e820-Abstract.html",
        "author": "Marcus Hutter",
        "abstract": "The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identifiability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed.",
        "bibtex": "@inproceedings{NIPS2009_d3d94468,\n author = {Hutter, Marcus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discrete MDL Predicts in Total Variation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d3d9446802a44259755d38e6d163e820-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d3d9446802a44259755d38e6d163e820-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d3d9446802a44259755d38e6d163e820-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 255297,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9466955001478850478&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "RSISE @ ANU and SML @ NICTA",
        "aff_domain": "hutter1.net",
        "email": "hutter1.net",
        "github": "",
        "project": "www.hutter1.net",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "Research School of Information Sciences and Engineering",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "7218c3f449",
        "title": "Discriminative Network Models of Schizophrenia",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/788d986905533aba051261497ecffcbb-Abstract.html",
        "author": "Irina Rish; Benjamin Thyreau; Bertrand Thirion; Marion Plaze; Marie-laure Paillere-martinot; Catherine Martelli; Jean-luc Martinot; Jean-baptiste Poline; Guillermo A. Cecchi",
        "abstract": "Schizophrenia is a complex psychiatric disorder that has eluded a characterization in terms of local abnormalities of brain activity, and is hypothesized to affect the collective, ``emergent working of the brain. We propose a novel data-driven approach to capture emergent features using functional brain networks [Eguiluzet al] extracted from fMRI data, and demonstrate its advantage over traditional region-of-interest (ROI) and local, task-specific linear activation analyzes. Our results suggest that schizophrenia is indeed associated with disruption of global, emergent brain properties related to its functioning as a network, which cannot be explained by alteration of local activation patterns. Moreover, further exploitation of interactions by sparse Markov Random Field classifiers shows clear gain over linear methods, such as Gaussian Naive Bayes and SVM, allowing to reach 86% accuracy (over 50% baseline - random guess), which is quite remarkable given that it is based on a single fMRI experiment using a simple auditory task.",
        "bibtex": "@inproceedings{NIPS2009_788d9869,\n author = {Rish, Irina and Thyreau, Benjamin and Thirion, Bertrand and Plaze, Marion and Paillere-martinot, Marie-laure and Martelli, Catherine and Martinot, Jean-luc and Poline, Jean-baptiste and Cecchi, Guillermo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative Network Models of Schizophrenia},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/788d986905533aba051261497ecffcbb-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/788d986905533aba051261497ecffcbb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/788d986905533aba051261497ecffcbb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/788d986905533aba051261497ecffcbb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 686358,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17184052794040500198&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; Neurospin, CEA, Saclay, France; INRIA, Saclay, France; INSERM - CEA - Univ. Paris Sud, Research Unit U.797, Neuroimaging & Psychiatry, SHFJ & Neurospin, Orsay, France; AP-HP, Adolescent Psychopathology and Medicine Dept., Maison de Solenn, Cochin Hospital, University Paris Descartes, F-75014 Paris, France; Departement de Psychiatrie et d\u2019Addictologie, Centre Hospitalier Paul Brousse, Villejuif, France; INSERM - CEA - Univ. Paris Sud, Research Unit U.797, Neuroimaging & Psychiatry, SHFJ & Neurospin, Orsay, France; Neurospin, CEA, Saclay, France",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;3;4;5;3;1",
        "aff_unique_norm": "IBM;Commissariat \u00e0 l'\u00c9nergie Atomique et aux \u00c9nergies Alternatives (CEA);INRIA;University of Paris Sud;University Paris Descartes;Centre Hospitalier Paul Brousse",
        "aff_unique_dep": "IBM T. J. Watson Research Center;Neurospin;;Research Unit U.797, Neuroimaging & Psychiatry;Adolescent Psychopathology and Medicine Dept.;Departement de Psychiatrie et d\u2019Addictologie",
        "aff_unique_url": "https://www.ibm.com/research/watson;https://www.cea.fr;https://www.inria.fr;https://www.universite-paris-sud.fr;https://www.universite-paris-descartes.fr;",
        "aff_unique_abbr": "IBM Watson;CEA;INRIA;Paris Sud;UPD;",
        "aff_campus_unique_index": "0;0;1;1;2;3;4;2;1",
        "aff_campus_unique": "Yorktown Heights;Saclay;Orsay;Paris;Villejuif",
        "aff_country_unique_index": "0;0;1;1;1;1;1;1;1",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "2db05d823b",
        "title": "Distribution Matching for Transduction",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2bb232c0b13c774965ef8558f0fbd615-Abstract.html",
        "author": "Novi Quadrianto; James Petterson; Alex J. Smola",
        "abstract": "Many transductive inference algorithms assume that distributions over training and test estimates should be related, e.g. by providing a large margin of separation on both sets. We use this idea to design a transduction algorithm which can be used without modification for classification, regression, and structured estimation. At its heart we exploit the fact that for a good learner the distributions over the outputs on training and test sets should match. This is a classical two-sample problem which can be solved efficiently in its most general form by using distance measures in Hilbert Space. It turns out that a number of existing heuristics can be viewed as special cases of our approach.",
        "bibtex": "@inproceedings{NIPS2009_2bb232c0,\n author = {Quadrianto, Novi and Petterson, James and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distribution Matching for Transduction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2bb232c0b13c774965ef8558f0fbd615-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 211562,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5287104996420008210&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "RSISE, ANU & SML, NICTA; RSISE, ANU & SML, NICTA; Yahoo! Research",
        "aff_domain": "gmail.com;nicta.com.au;smola.org",
        "email": "gmail.com;nicta.com.au;smola.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Australian National University;Yahoo!",
        "aff_unique_dep": "Research School of Information Sciences and Engineering;Yahoo! Research",
        "aff_unique_url": "https://www.anu.edu.au;https://research.yahoo.com",
        "aff_unique_abbr": "ANU;Yahoo!",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "4fcf5776e4",
        "title": "Distribution-Calibrated Hierarchical Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d707329bece455a462b58ce00d1194c9-Abstract.html",
        "author": "Ofer Dekel",
        "abstract": "While many advances have already been made on the topic of hierarchical classi-  \ufb01cation learning, we take a step back and examine how a hierarchical classi\ufb01ca-  tion problem should be formally de\ufb01ned. We pay particular attention to the fact  that many arbitrary decisions go into the design of the the label taxonomy that  is provided with the training data, and that this taxonomy is often unbalanced.  We correct this problem by using the data distribution to calibrate the hierarchical  classi\ufb01cation loss function. This distribution-based correction must be done with  care, to avoid introducing unmanagable statstical dependencies into the learning  problem. This leads us off the beaten path of binomial-type estimation and into  the uncharted waters of geometric-type estimation. We present a new calibrated  de\ufb01nition of statistical risk for hierarchical classi\ufb01cation, an unbiased geometric  estimator for this risk, and a new algorithmic reduction from hierarchical classi\ufb01-  cation to cost-sensitive classi\ufb01cation.",
        "bibtex": "@inproceedings{NIPS2009_d707329b,\n author = {Dekel, Ofer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distribution-Calibrated Hierarchical Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d707329bece455a462b58ce00d1194c9-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d707329bece455a462b58ce00d1194c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d707329bece455a462b58ce00d1194c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 160431,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16232969777399806546&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Microsoft Research",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "482c08a174",
        "title": "Dual Averaging Method for Regularized Stochastic Learning and Online Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/7cce53cf90577442771720a370c3c723-Abstract.html",
        "author": "Lin Xiao",
        "abstract": "We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as L1-norm for sparsity. We develop a new online algorithm, the regularized dual averaging method, that can explicitly exploit the regularization structure in an online setting. In particular, at each iteration, the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term, not just its subgradient. This method achieves the optimal convergence rate and often enjoys a low complexity per iteration similar as the standard stochastic gradient method. Computational experiments are presented for the special case of sparse online learning using L1-regularization.",
        "bibtex": "@inproceedings{NIPS2009_7cce53cf,\n author = {Xiao, Lin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dual Averaging Method for Regularized Stochastic Learning and Online Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/7cce53cf90577442771720a370c3c723-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/7cce53cf90577442771720a370c3c723-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/7cce53cf90577442771720a370c3c723-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 513082,
        "gs_citation": 1082,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11833722259496946463&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Microsoft Research, Redmond, WA 98052",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Redmond",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2141e588c7",
        "title": "Efficient Bregman Range Search",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/fe131d7f5a6b38b23cc967316c13dae2-Abstract.html",
        "author": "Lawrence Cayton",
        "abstract": "We develop an algorithm for efficient range search when the notion of dissimilarity is given by a Bregman divergence.  The range search task is to return all points in a potentially large database that are within some specified distance of a query.  It arises in many learning algorithms such as locally-weighted regression, kernel density estimation, neighborhood graph-based algorithms, and in tasks like outlier detection and information retrieval.  In metric spaces, efficient range search-like algorithms based on spatial data structures have been deployed on a variety of statistical tasks.  Here we describe the first algorithm for range search for an arbitrary Bregman divergence.   This broad class of dissimilarity measures includes the relative entropy,  Mahalanobis distance, Itakura-Saito divergence, and a variety of matrix divergences.  Metric methods cannot be directly applied since Bregman divergences do not in general satisfy the triangle inequality.  We derive geometric properties of Bregman divergences that yield an efficient algorithm for range search based on a recently proposed space decomposition for Bregman divergences.",
        "bibtex": "@inproceedings{NIPS2009_fe131d7f,\n author = {Cayton, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Bregman Range Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/fe131d7f5a6b38b23cc967316c13dae2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 706886,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17814251633663138922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Max Planck Institute for Biological Cybernetics",
        "aff_domain": "tuebingen.mpg.de",
        "email": "tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Biological Cybernetics",
        "aff_unique_url": "https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "MPIBC",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "4298b12d3c",
        "title": "Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d81f9c1be2e08964bf9f24b15f0e4900-Abstract.html",
        "author": "Ryan Mcdonald; Mehryar Mohri; Nathan Silberman; Dan Walker; Gideon S. Mann",
        "abstract": "Training conditional maximum entropy models on massive data requires significant time and computational resources. In this paper, we investigate three common distributed training strategies: distributed gradient, majority voting ensembles, and parameter mixtures. We analyze the worst-case runtime and resource costs of each and present a theoretical foundation for the convergence of parameters under parameter mixtures, the most efficient strategy. We present large-scale experiments comparing the different strategies and demonstrate that parameter mixtures over independent models use fewer resources and achieve comparable loss as compared to standard approaches.",
        "bibtex": "@inproceedings{NIPS2009_d81f9c1b,\n author = {Mcdonald, Ryan and Mohri, Mehryar and Silberman, Nathan and Walker, Dan and Mann, Gideon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 186849,
        "gs_citation": 369,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8629511673691054400&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Google; Google; Courant Institute and Google; Google; NLP Lab, Brigham Young University",
        "aff_domain": "google.com;google.com;cims.nyu.edu;google.com;cs.byu.edu",
        "email": "google.com;google.com;cims.nyu.edu;google.com;cs.byu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "Google;Courant Institute of Mathematical Sciences;Brigham Young University",
        "aff_unique_dep": "Google;Mathematical Sciences;NLP Lab",
        "aff_unique_url": "https://www.google.com;https://courant.nyu.edu;https://www.byu.edu",
        "aff_unique_abbr": "Google;Courant;BYU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "171ae755fb",
        "title": "Efficient Learning using Forward-Backward Splitting",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html",
        "author": "Yoram Singer; John C. Duchi",
        "abstract": "We describe, analyze, and experiment with a new framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we first perform an {\\em unconstrained} gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase. This yields a simple yet effective algorithm for both batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as $\\ell_1$. We derive concrete and very simple algorithms for minimization of loss functions with $\\ell_1$, $\\ell_2$, $\\ell_2^2$, and $\\ell_\\infty$ regularization. We also show how to construct efficient algorithms for mixed-norm $\\ell_1/\\ell_q$ regularization. We further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets.",
        "bibtex": "@inproceedings{NIPS2009_621bf66d,\n author = {Singer, Yoram and Duchi, John C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Learning using Forward-Backward Splitting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/621bf66ddb7c962aa0d22ac97d69b793-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/621bf66ddb7c962aa0d22ac97d69b793-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 145542,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13441847550518151765&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University of California Berkeley; Google",
        "aff_domain": "cs.berkeley.edu;google.com",
        "email": "cs.berkeley.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.berkeley.edu;https://www.google.com",
        "aff_unique_abbr": "UC Berkeley;Google",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Berkeley;Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dda41b7cf7",
        "title": "Efficient Match Kernel between Sets of Features for Visual Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4daa3db355ef2b0e64b472968cb70f0d-Abstract.html",
        "author": "Liefeng Bo; Cristian Sminchisescu",
        "abstract": "In visual recognition, the images are frequently modeled as sets of local features (bags). We show that bag of words, a common method to handle such cases, can be viewed as a special match kernel, which counts 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse. It is, therefore, appealing to design match kernels that more accurately measure the similarity between local features. However, it is impractical to use such kernels on large datasets due to their significant computational cost. To address this problem, we propose an efficient match kernel (EMK), which maps local features to a low dimensional feature space, average the resulting feature vectors to form a set-level feature, then apply a linear classifier. The local feature maps are learned so that their inner products preserve, to the best possible, the values of the specified kernel function. EMK is linear both in the number of images and in the number of local features. We demonstrate that EMK is extremely efficient and achieves the current state of the art performance on three difficult real world datasets: Scene-15, Caltech-101 and Caltech-256.",
        "bibtex": "@inproceedings{NIPS2009_4daa3db3,\n author = {Bo, Liefeng and Sminchisescu, Cristian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Match Kernel between Sets of Features for Visual Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4daa3db355ef2b0e64b472968cb70f0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 218074,
        "gs_citation": 322,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7384361018375060169&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Toyota Technological Institute at Chicago; University of Bonn",
        "aff_domain": "tti-c.org; sminchisescu.ins.uni-bonn.de",
        "email": "tti-c.org; sminchisescu.ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Bonn",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.uni-bonn.de/",
        "aff_unique_abbr": "TTI Chicago;UBonn",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "e1127ae461",
        "title": "Efficient Moments-based Permutation Tests",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/afd4836712c5e77550897e25711e1d96-Abstract.html",
        "author": "Chunxiao Zhou; Huixia J. Wang; Yongmei M. Wang",
        "abstract": "In this paper, we develop an efficient moments-based permutation test approach to improve the system\u2019s efficiency by approximating the permutation distribution of the test statistic with Pearson distribution series. This approach involves the calculation of the first four moments of the permutation distribution. We propose a novel recursive method to derive these moments theoretically and analytically without any permutation.  Experimental results using different test statistics are demonstrated using simulated data and real data. The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson distribution approximation to achieve both accuracy and efficiency.",
        "bibtex": "@inproceedings{NIPS2009_afd48367,\n author = {Zhou, Chunxiao and Wang, Huixia and Wang, Yongmei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Moments-based Permutation Tests},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/afd4836712c5e77550897e25711e1d96-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/afd4836712c5e77550897e25711e1d96-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/afd4836712c5e77550897e25711e1d96-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 355786,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9326175493011160233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Electrical and Computer Eng. + Dept. of Statistics, University of Illinois at Urbana-Champaign; Dept. of Statistics, North Carolina State University; Dept s. of Statistics, Psychology, and Bioengineering, University of Illinois at Urbana-Champaign",
        "aff_domain": "gmail.com;stat.ncsu.edu;illinois.edu",
        "email": "gmail.com;stat.ncsu.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;1",
        "aff_unique_norm": "University of California, Berkeley;University of Illinois Urbana-Champaign;North Carolina State University",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences;Dept. of Statistics;Dept. of Statistics",
        "aff_unique_url": "https://www.berkeley.edu;https://illinois.edu;https://www.ncsu.edu",
        "aff_unique_abbr": "UC Berkeley;UIUC;NCSU",
        "aff_campus_unique_index": "0+1;1",
        "aff_campus_unique": "Berkeley;Urbana-Champaign;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "80e097449c",
        "title": "Efficient Recovery of Jointly Sparse Vectors",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4ca82782c5372a547c104929f03fe7a9-Abstract.html",
        "author": "Liang Sun; Jun Liu; Jianhui Chen; Jieping Ye",
        "abstract": "We consider the reconstruction of sparse signals in the multiple measurement vector (MMV) model,in which the signal, represented as a matrix, consists of a set of jointly sparse vectors. MMV is an extension of the single measurement vector (SMV) model employed in standard compressive sensing (CS). Recent theoretical studies focus on the convex relaxation of the MMV problem based on the $(2,1)$-norm minimization, which is an extension of the well-known $1$-norm minimization employed in SMV. However, the resulting convex optimization problem in MMV is significantly much more difficult to solve than the one in SMV. Existing algorithms reformulate it as a second-order cone programming (SOCP) or semidefinite programming (SDP), which is computationally expensive to solve for problems of moderate size. In this paper, we propose a new (dual) reformulation of the convex optimization problem in MMV and develop an efficient algorithm based on the prox-method. Interestingly, our theoretical analysis reveals the close connection between the proposed reformulation and multiple kernel learning. Our simulation studies demonstrate the scalability of the proposed algorithm.",
        "bibtex": "@inproceedings{NIPS2009_4ca82782,\n author = {Sun, Liang and Liu, Jun and Chen, Jianhui and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Recovery of Jointly Sparse Vectors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4ca82782c5372a547c104929f03fe7a9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 240069,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10645896017921021527&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "07f05907ae",
        "title": "Efficient and Accurate Lp-Norm Multiple Kernel Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html",
        "author": "Marius Kloft; Ulf Brefeld; Pavel Laskov; Klaus-Robert M\u00fcller; Alexander Zien; S\u00f6ren Sonnenburg",
        "abstract": "Learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown. Previous approaches to multiple kernel learning (MKL) promote sparse kernel combinations and hence support interpretability. Unfortunately, L1-norm MKL is hardly observed to outperform trivial baselines in practical applications. To allow for robust kernel mixtures, we generalize MKL to arbitrary Lp-norms. We devise new insights on the connection between  several existing MKL formulations and develop two efficient interleaved optimization strategies for arbitrary p>1. Empirically, we demonstrate that the interleaved optimization strategies are much faster compared to the traditionally used wrapper approaches. Finally, we apply Lp-norm MKL to real-world problems from computational biology, showing that non-sparse MKL achieves accuracies that go beyond the state-of-the-art.",
        "bibtex": "@inproceedings{NIPS2009_d516b136,\n author = {Kloft, Marius and Brefeld, Ulf and Laskov, Pavel and M\\\"{u}ller, Klaus-Robert and Zien, Alexander and Sonnenburg, S\\\"{o}ren},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient and Accurate Lp-Norm Multiple Kernel Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/d516b13671a4179d9b7b458a6ebdeb92-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d516b13671a4179d9b7b458a6ebdeb92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 209726,
        "gs_citation": 359,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7642591711971963499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "322c532355",
        "title": "Ensemble Nystrom Method",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html",
        "author": "Sanjiv Kumar; Mehryar Mohri; Ameet Talwalkar",
        "abstract": "A crucial technique for scaling kernel methods to very large data sets reaching or exceeding millions of instances is based on low-rank approximation of kernel matrices. We introduce a new family of algorithms based on mixtures of Nystrom approximations, ensemble Nystrom algorithms, that yield more accurate low-rank approximations than the standard Nystrom method. We give a detailed study of multiple variants of these algorithms based on simple averaging, an exponential weight method, or regression-based methods. We also present a theoretical analysis of these algorithms, including novel error bounds guaranteeing a better convergence rate than the standard Nystrom method. Finally, we report the results of extensive experiments with several data sets containing up to 1M points demonstrating the signi\ufb01cant performance improvements gained over the standard Nystrom approximation.",
        "bibtex": "@inproceedings{NIPS2009_a49e9411,\n author = {Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ensemble Nystrom Method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a49e9411d64ff53eccfdd09ad10a15b3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 108392,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15282639732977051200&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Google Research; Courant Institute and Google Research; Courant Institute of Mathematical Sciences",
        "aff_domain": "google.com;cs.nyu.edu;cs.nyu.edu",
        "email": "google.com;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;Courant Institute;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Google Research;Courant Institute;Mathematical Sciences",
        "aff_unique_url": "https://research.google;https://courant.nyu.edu;https://cims.nyu.edu",
        "aff_unique_abbr": "Google Research;Courant;CIMS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "67b7e093ba",
        "title": "Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0768281a05da9f27df178b5c39a51263-Abstract.html",
        "author": "Amarnag Subramanya; Jeff A. Bilmes",
        "abstract": "We prove certain theoretical properties of a graph-regularized transductive learning objective that is based on minimizing a Kullback-Leibler divergence based loss. These include showing that the iterative alternating minimization procedure used to minimize the objective converges to the correct solution and deriving a test for convergence. We also propose a graph node ordering algorithm that is cache cognizant and leads to a linear speedup in parallel computations. This ensures that the algorithm scales to large data sets. By making use of empirical evaluation on the TIMIT and Switchboard I corpora, we show this approach is able to out-perform other state-of-the-art SSL approaches. In one instance, we solve a problem on a 120 million node graph.",
        "bibtex": "@inproceedings{NIPS2009_0768281a,\n author = {Subramanya, Amarnag and Bilmes, Jeff A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0768281a05da9f27df178b5c39a51263-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0768281a05da9f27df178b5c39a51263-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 345127,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3088340578187144613&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical Engineering, University of Washington, Seattle; Department of Electrical Engineering, University of Washington, Seattle",
        "aff_domain": "ee.washington.edu;ee.washington.edu",
        "email": "ee.washington.edu;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "537c2a39bd",
        "title": "Estimating image bases for visual image reconstruction from human brain activity",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/dc5689792e08eb2e219dce49e64c885b-Abstract.html",
        "author": "Yusuke Fujiwara; Yoichi Miyawaki; Yukiyasu Kamitani",
        "abstract": "Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-defined visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were fixed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.",
        "bibtex": "@inproceedings{NIPS2009_dc568979,\n author = {Fujiwara, Yusuke and Miyawaki, Yoichi and Kamitani, Yukiyasu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimating image bases for visual image reconstruction from human brain activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/dc5689792e08eb2e219dce49e64c885b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/dc5689792e08eb2e219dce49e64c885b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/dc5689792e08eb2e219dce49e64c885b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 159481,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10790871621248402456&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "ATR Computational Neuroscience Laboratories; National Institute of Information and Communications Technology; ATR Computational Neuroscience Laboratories",
        "aff_domain": "gmail.com;atr.jp;atr.jp",
        "email": "gmail.com;atr.jp;atr.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "ATR Computational Neuroscience Laboratories;National Institute of Information and Communications Technology",
        "aff_unique_dep": "Computational Neuroscience;",
        "aff_unique_url": "https://www.atr.jp/en/research/labs/cnl/;https://www.nict.go.jp/",
        "aff_unique_abbr": "ATR CNL;NICT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "251de390e1",
        "title": "Evaluating multi-class learning strategies in a generative hierarchical framework for object detection",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html",
        "author": "Sanja Fidler; Marko Boben; Ales Leonardis",
        "abstract": "Multiple object class learning and detection is a challenging problem due to the large number of object classes and their high visual variability. Specialized detectors usually excel in performance, while joint representations optimize sharing and reduce inference time --- but are complex to train. Conveniently, sequential learning of categories cuts down training time by transferring existing knowledge to novel classes, but cannot fully exploit the richness of shareability and might depend on ordering in learning. In hierarchical frameworks these issues have been little explored. In this paper, we show how different types of multi-class learning can be done within one generative hierarchical framework and provide a rigorous experimental analysis of various object class learning strategies as the number of classes grows. Specifically, we propose, evaluate and compare three important types of multi-class learning: 1.) independent training of individual categories, 2.) joint training of classes, 3.) sequential learning of classes. We explore and compare their computational behavior (space and time) and detection performance as a function of the number of learned classes on several recognition data sets.",
        "bibtex": "@inproceedings{NIPS2009_a8baa565,\n author = {Fidler, Sanja and Boben, Marko and Leonardis, Ales},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Evaluating multi-class learning strategies in a generative hierarchical framework for object detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a8baa56554f96369ab93e4f3bb068c22-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 347381,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6214553519516566460&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Faculty of Computer and Information Science, University of Ljubljana, Slovenia",
        "aff_domain": "fri.uni-lj.si;fri.uni-lj.si;fri.uni-lj.si",
        "email": "fri.uni-lj.si;fri.uni-lj.si;fri.uni-lj.si",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Ljubljana",
        "aff_unique_dep": "Faculty of Computer and Information Science",
        "aff_unique_url": "https://www.fcis.unilj.si",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Slovenia"
    },
    {
        "id": "36ca186e6b",
        "title": "Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d79aac075930c83c2f1e369a511148fe-Abstract.html",
        "author": "Ed Vul; George Alvarez; Joshua B. Tenenbaum; Michael J. Black",
        "abstract": "Multiple object tracking is a task commonly used to investigate the architecture of human visual attention. Human participants show a distinctive pattern of successes and failures in tracking experiments that is often attributed to limits on an object system, a tracking module, or other specialized cognitive structures. Here we use a computational analysis of the task of object tracking to ask which human failures arise from cognitive limitations and which are consequences of inevitable perceptual uncertainty in the tracking task. We find that many human performance phenomena, measured through novel behavioral experiments, are naturally produced by the operation of our ideal observer model (a Rao-Blackwelized particle filter). The tradeoff between the speed and number of objects being tracked, however, can only arise from the allocation of a flexible cognitive resource, which can be formalized as either memory or attention.",
        "bibtex": "@inproceedings{NIPS2009_d79aac07,\n author = {Vul, Ed and Alvarez, George and Tenenbaum, Joshua and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d79aac075930c83c2f1e369a511148fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 617217,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12977283391009280352&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Psychology, Harvard University",
        "aff_domain": "mit.edu;mit.edu;mit.edu;wjh.harvard.edu",
        "email": "mit.edu;mit.edu;mit.edu;wjh.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Harvard University",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences;Department of Psychology",
        "aff_unique_url": "https://web.mit.edu;https://www.harvard.edu",
        "aff_unique_abbr": "MIT;Harvard",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "21e2bb012d",
        "title": "Exploring Functional Connectivities of the Human Brain using Multivariate Information Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8248a99e81e752cb9b41da3fc43fbe7f-Abstract.html",
        "author": "Barry Chai; Dirk Walther; Diane Beck; Li Fei-fei",
        "abstract": "In this study, we present a method for estimating the mutual information for a localized pattern of fMRI data. We show that taking a multivariate information approach to voxel selection leads to a decoding accuracy that surpasses an univariate inforamtion approach and other standard voxel selection methods. Furthermore,we extend the multivariate mutual information theory to measure the functional connectivity between distributed brain regions. By jointly estimating the information shared by two sets of voxels we can reliably map out the connectivities in the human brain during experiment conditions. We validated our approach on a 6-way scene categorization fMRI experiment. The multivariate information analysis is able to \ufb01nd strong information \ufb02ow between PPA and RSC, which con\ufb01rms existing neuroscience studies on scenes. Furthermore, by exploring over the whole brain, our method identifies other interesting ROIs that share information with the PPA, RSC scene network,suggesting interesting future work for neuroscientists.",
        "bibtex": "@inproceedings{NIPS2009_8248a99e,\n author = {Chai, Barry and Walther, Dirk and Beck, Diane and Fei-fei, Li},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exploring Functional Connectivities of the Human Brain using Multivariate Information Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8248a99e81e752cb9b41da3fc43fbe7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 603041,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13436170858612133567&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Department, Stanford University; Beckman Institute, University of Illinois at Urbana-Champaign + Psychology Department, University of Illinois at Urbana-Champaign; Beckman Institute, University of Illinois at Urbana-Champaign + Psychology Department, University of Illinois at Urbana-Champaign; Computer Science Department, Stanford University + Psychology Department, University of Illinois at Urbana-Champaign",
        "aff_domain": "cs.stanford.edu;illinois.edu;illinois.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;illinois.edu;illinois.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+1;1+1;0+1",
        "aff_unique_norm": "Stanford University;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Computer Science Department;Beckman Institute",
        "aff_unique_url": "https://www.stanford.edu;https://www.illinois.edu",
        "aff_unique_abbr": "Stanford;UIUC",
        "aff_campus_unique_index": "0;1+1;1+1;0+1",
        "aff_campus_unique": "Stanford;Urbana-Champaign",
        "aff_country_unique_index": "0;0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "70e5b8af40",
        "title": "Exponential Family Graph Matching and Ranking",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html",
        "author": "James Petterson; Jin Yu; Julian J. Mcauley; Tib\u00e9rio S. Caetano",
        "abstract": "We present a method for learning max-weight matching predictors in bipartite graphs. The method consists of performing maximum a posteriori estimation in exponential families with sufficient statistics that encode permutations and data features. Although inference is in general hard, we show that for one very relevant application - document ranking - exact inference is efficient. For general model instances, an appropriate sampler is readily available. Contrary to existing max-margin matching models, our approach is statistically consistent and, in addition, experiments with increasing sample sizes indicate superior improvement over such models. We apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning document ranking, in which we obtain state-of-the-art results, in particular improving on max-margin variants. The drawback of this method with respect to max-margin alternatives is its runtime for large graphs, which is high comparatively.",
        "bibtex": "@inproceedings{NIPS2009_85d8ce59,\n author = {Petterson, James and Yu, Jin and Mcauley, Julian and Caetano, Tib\\'{e}rio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exponential Family Graph Matching and Ranking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/85d8ce590ad8981ca2c8286f79f59954-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/85d8ce590ad8981ca2c8286f79f59954-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 683380,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13483697341511641358&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5f37d30ae9",
        "title": "Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/884d247c6f65a96a7da4d1105d584ddd-Abstract.html",
        "author": "Yicong Meng; Bertram E. Shi",
        "abstract": "We extend the concept of phase tuning, a ubiquitous mechanism in sensory neurons including motion and disparity detection neurons, to the motion contrast detection.  We demonstrate that motion contrast can be detected by phase shifts between motion neuronal responses in different spatial regions. By constructing the differential motion opponency in response to motions in two different spatial regions, varying motion contrasts can be detected, where similar motion is detected by zero phase shifts and differences in motion by non-zero phase shifts.  The model can exhibit either enhancement or suppression of responses by either different or similar motion in the surrounding.  A primary advantage of the model is that the responses are selective to relative motion instead of absolute motion, which could model neurons found in neurophysiological experiments responsible for motion pop-out detection.",
        "bibtex": "@inproceedings{NIPS2009_884d247c,\n author = {Meng, Yicong and Shi, Bertram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/884d247c6f65a96a7da4d1105d584ddd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 356315,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:J_nJQxttNSwJ:scholar.google.com/&scioq=Extending+Phase+Mechanism+to+Differential+Motion+Opponency+for+Motion+Pop-out&hl=en&as_sdt=0,33",
        "gs_version_total": 7,
        "aff": "Department of Electronic and Computer Engineering Hong Kong University of Science and Technology Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering Hong Kong University of Science and Technology Clear Water Bay, Kowloon, Hong Kong",
        "aff_domain": "ust.hk;ust.hk",
        "email": "ust.hk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Electronic and Computer Engineering",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "8cc22c2e56",
        "title": "FACTORIE: Probabilistic Programming via Imperatively Defined Factor Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/847cc55b7032108eee6dd897f3bca8a5-Abstract.html",
        "author": "Andrew McCallum; Karl Schultz; Sameer Singh",
        "abstract": "Discriminatively trained undirected graphical models have had wide empirical success, and there has been increasing interest in toolkits that ease their application to complex relational data.  The power in relational models is in their repeated structure and tied parameters; at issue is how to define these structures in a powerful and flexible way. Rather than using a declarative language, such as SQL or first-order logic, we advocate using an imperative language to express various aspects of model structure, inference, and learning.  By combining the traditional, declarative, statistical semantics of factor graphs with imperative definitions of their construction and operation, we allow the user to mix declarative and procedural domain knowledge, and also gain significant efficiencies.  We have implemented such imperatively defined factor graphs in a system we call Factorie, a software library for an object-oriented, strongly-typed, functional language.  In experimental comparisons to Markov Logic Networks on joint segmentation and coreference, we find our approach to be 3-15 times faster while reducing error by 20-25%-achieving a new state of the art.",
        "bibtex": "@inproceedings{NIPS2009_847cc55b,\n author = {McCallum, Andrew and Schultz, Karl and Singh, Sameer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {FACTORIE: Probabilistic Programming via Imperatively Defined Factor Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/847cc55b7032108eee6dd897f3bca8a5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/847cc55b7032108eee6dd897f3bca8a5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/847cc55b7032108eee6dd897f3bca8a5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 153945,
        "gs_citation": 295,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6107023413717939309&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f44b643f7d",
        "title": "Factor Modeling for Advertisement Targeting",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1bb91f73e9d31ea2830a5e73ce3ed328-Abstract.html",
        "author": "Ye Chen; Michael Kapralov; John Canny; Dmitry Y. Pavlov",
        "abstract": "We adapt a probabilistic latent variable model, namely GaP (Gamma-Poisson), to ad targeting in the contexts of sponsored search (SS) and behaviorally targeted (BT) display advertising. We also approach the important problem of ad positional bias by formulating a one-latent-dimension GaP factorization. Learning from click-through data is intrinsically large scale, even more so for ads. We scale up the algorithm to terabytes of real-world SS and BT data that contains hundreds of millions of users and hundreds of thousands of features, by leveraging the scalability characteristics of the algorithm and the inherent structure of the problem including data sparsity and locality. Specifically, we demonstrate two somewhat orthogonal philosophies of scaling algorithms to large-scale problems, through the SS and BT implementations, respectively. Finally, we report the experimental results using Yahoos vast datasets, and show that our approach substantially outperform the state-of-the-art methods in prediction accuracy. For BT in particular, the ROC area achieved by GaP is exceeding 0.95, while one prior approach using Poisson regression yielded 0.83. For computational performance, we compare a single-node sparse implementation with a parallel implementation using Hadoop MapReduce, the results are counterintuitive yet quite interesting. We therefore provide insights into the underlying principles of large-scale learning.",
        "bibtex": "@inproceedings{NIPS2009_1bb91f73,\n author = {Chen, Ye and Kapralov, Michael and Canny, John and Pavlov, Dmitry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Factor Modeling for Advertisement Targeting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1bb91f73e9d31ea2830a5e73ce3ed328-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1bb91f73e9d31ea2830a5e73ce3ed328-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1bb91f73e9d31ea2830a5e73ce3ed328-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 740089,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1950484895555583807&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "eBay Inc. + Yahoo! Labs; Stanford University; Yandex Labs + Yahoo! Labs; University of California, Berkeley + Yahoo! Labs",
        "aff_domain": "ebay.com;stanford.edu;yandex-team.ru;cs.berkeley.edu",
        "email": "ebay.com;stanford.edu;yandex-team.ru;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3+1;4+1",
        "aff_unique_norm": "eBay Inc.;Yahoo!;Stanford University;Yandex;University of California, Berkeley",
        "aff_unique_dep": ";Yahoo! Labs;;Yandex Labs;",
        "aff_unique_url": "https://www.ebayinc.com;https://yahoo.com;https://www.stanford.edu;https://yandex.com;https://www.berkeley.edu",
        "aff_unique_abbr": "eBay;Yahoo!;Stanford;Yandex;UC Berkeley",
        "aff_campus_unique_index": ";1;;2",
        "aff_campus_unique": ";Stanford;Berkeley",
        "aff_country_unique_index": "0+0;0;1+0;0+0",
        "aff_country_unique": "United States;Russian Federation"
    },
    {
        "id": "98e90084ec",
        "title": "Fast Graph Laplacian Regularized Kernel Learning via Semidefinite\u2013Quadratic\u2013Linear Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/96ea64f3a1aa2fd00c72faacf0cb8ac9-Abstract.html",
        "author": "Xiao-ming Wu; Anthony M. So; Zhenguo Li; Shuo-yen R. Li",
        "abstract": "Kernel learning is a powerful framework for nonlinear data modeling. Using the kernel trick, a number of problems have been formulated as semidefinite programs (SDPs). These include Maximum Variance Unfolding (MVU) (Weinberger et al., 2004) in nonlinear dimensionality reduction, and Pairwise Constraint Propagation (PCP) (Li et al., 2008) in constrained clustering. Although in theory SDPs can be efficiently solved, the high computational complexity incurred in numerically processing the huge linear matrix inequality constraints has rendered the SDP approach unscalable. In this paper, we show that a large class of kernel learning problems can be reformulated as semidefinite-quadratic-linear programs (SQLPs), which only contain a simple positive semidefinite constraint, a second-order cone constraint and a number of linear constraints. These constraints are much easier to process numerically, and the gain in speedup over previous approaches is at least of the order $m^{2.5}$, where m is the matrix dimension. Experimental results are also presented to show the superb computational efficiency of our approach.",
        "bibtex": "@inproceedings{NIPS2009_96ea64f3,\n author = {Wu, Xiao-ming and So, Anthony and Li, Zhenguo and Li, Shuo-yen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Graph Laplacian Regularized Kernel Learning via Semidefinite\\textendash Quadratic\\textendash Linear Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 256855,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18248144755289107395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Dept. of IE, The Chinese University of Hong Kong; Dept. of SE&EM, The Chinese University of Hong Kong; Dept. of IE, The Chinese University of Hong Kong; Dept. of IE, The Chinese University of Hong Kong",
        "aff_domain": "ie.cuhk.edu.hk;se.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;se.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Dept. of IE",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "b483c0f2a0",
        "title": "Fast Image Deconvolution using Hyper-Laplacian Priors",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/3dd48ab31d016ffcbf3314df2b3cb9ce-Abstract.html",
        "author": "Dilip Krishnan; Rob Fergus",
        "abstract": "The heavy-tailed distribution of gradients in natural scenes have proven effective priors for a range of problems such as denoising, deblurring and super-resolution. However, the use of sparse distributions makes the problem non-convex and impractically slow to solve for multi-megapixel images. In this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use hyper-Laplacian priors. We adopt an alternating minimization scheme where one of the two phases is a non-convex problem that is separable over pixels. This per-pixel sub-problem may be solved with a lookup table (LUT). Alternatively, for two specific values of \u03b1, 1/2 and 2/3 an analytic solution can be found, by finding the roots of a cubic and quartic polynomial, respectively. Our approach (using either LUTs or analytic formulae) is able to deconvolve a 1 megapixel image in less than \u223c3 seconds, achieving comparable quality to existing methods such as iteratively reweighted least squares (IRLS) that take \u223c20 minutes. Furthermore, our method is quite general and can easily be extended to related image processing problems, beyond the deconvolution application demonstrated.",
        "bibtex": "@inproceedings{NIPS2009_3dd48ab3,\n author = {Krishnan, Dilip and Fergus, Rob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Image Deconvolution using Hyper-Laplacian Priors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 380903,
        "gs_citation": 1804,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8134592984605123717&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Computer Science, Courant Institute, New York University; Dept. of Computer Science, Courant Institute, New York University",
        "aff_domain": "cs.nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "583f0051a8",
        "title": "Fast Learning from Non-i.i.d. Observations",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a89cf525e1d9f04d16ce31165e139a4b-Abstract.html",
        "author": "Ingo Steinwart; Andreas Christmann",
        "abstract": "We prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from  $\\a$-mixing processes. To illustrate this oracle inequality, we use it to derive learning rates for some learning methods including least squares SVMs. Since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed (i.i.d.) processes, it turns out that these learning rates are close to the optimal rates known in the i.i.d. case.",
        "bibtex": "@inproceedings{NIPS2009_a89cf525,\n author = {Steinwart, Ingo and Christmann, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Learning from Non-i.i.d. Observations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a89cf525e1d9f04d16ce31165e139a4b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 228514,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12268592858905878091&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Information Sciences Group CCS-3, Los Alamos National Laboratory, Los Alamos, NM 87545, USA; University of Bayreuth, Department of Mathematics, D-95440 Bayreuth",
        "aff_domain": "lanl.gov;uni-bayreuth.de",
        "email": "lanl.gov;uni-bayreuth.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Los Alamos National Laboratory;University of Bayreuth",
        "aff_unique_dep": "Information Sciences Group CCS-3;Department of Mathematics",
        "aff_unique_url": "https://www.lanl.gov;https://www.uni-bayreuth.de",
        "aff_unique_abbr": "LANL;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Alamos;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "4ae0ea867b",
        "title": "Fast subtree kernels on graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0a49e3c3a03ebde64f85c0bacd8a08e2-Abstract.html",
        "author": "Nino Shervashidze; Karsten Borgwardt",
        "abstract": "In this article, we propose fast subtree kernels on graphs. On graphs with n nodes and m edges and maximum degree d, these kernels comparing subtrees of height h can be computed in O(mh), whereas the classic subtree kernel by Ramon & G\u00a8artner scales as O(n24dh). Key to this ef\ufb01ciency is the observation that the Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with labeled graphs, scale up easily to large graphs and outperform state-of-the-art graph ker- nels on several classi\ufb01cation benchmark datasets in terms of accuracy and runtime.",
        "bibtex": "@inproceedings{NIPS2009_0a49e3c3,\n author = {Shervashidze, Nino and Borgwardt, Karsten},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast subtree kernels on graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0a49e3c3a03ebde64f85c0bacd8a08e2-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0a49e3c3a03ebde64f85c0bacd8a08e2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0a49e3c3a03ebde64f85c0bacd8a08e2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 171356,
        "gs_citation": 348,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3517385452931329062&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Interdepartmental Bioinformatics Group; Max Planck Institutes T\u00fcbingen, Germany",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Interdepartmental Bioinformatics Group;Max Planck Institute",
        "aff_unique_dep": "Bioinformatics;",
        "aff_unique_url": ";https://www.mpg.de",
        "aff_unique_abbr": ";MPI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Germany"
    },
    {
        "id": "435eee7fdb",
        "title": "Fast, smooth and adaptive regression in metric spaces",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/31b3b31a1c2f8a370206f111127c0dbd-Abstract.html",
        "author": "Samory Kpotufe",
        "abstract": "It was recently shown that certain nonparametric regressors can escape the curse of dimensionality in the sense that their convergence rates adapt to the intrinsic dimension of data (\\cite{BL:65, SK:77}). We prove some stronger results in more general settings. In particular, we consider a regressor which, by combining aspects of both tree-based regression and kernel regression, operates on a general metric space, yields a smooth function, and evaluates in time $O(\\log n)$. We derive a tight convergence rate of the form $n^{-2/(2+d)}$ where $d$ is the Assouad dimension of the input space.",
        "bibtex": "@inproceedings{NIPS2009_31b3b31a,\n author = {Kpotufe, Samory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast, smooth and adaptive regression in metric spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/31b3b31a1c2f8a370206f111127c0dbd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 280690,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4440402450582253554&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "UCSD CSE",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://cse.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0fa86dd2ad",
        "title": "Filtering Abstract Senses From Image Search Results",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/208e43f0e45c4c78cafadb83d2888cb6-Abstract.html",
        "author": "Kate Saenko; Trevor Darrell",
        "abstract": "Part of",
        "bibtex": "@inproceedings{NIPS2009_208e43f0,\n author = {Saenko, Kate and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Filtering Abstract Senses From Image Search Results},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/208e43f0e45c4c78cafadb83d2888cb6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1934300,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=783126075536961032&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "MIT CSAIL, Cambridge, MA; UC Berkeley EECS and ICSI, Berkeley, CA",
        "aff_domain": "csail.mit.edu;eecs.berkeley.edu",
        "email": "csail.mit.edu;eecs.berkeley.edu",
        "github": "",
        "project": "http://www.semantic-robot-vision-challenge.org",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of California, Berkeley",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;Electrical Engineering and Computer Sciences",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "MIT CSAIL;UC Berkeley",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ecfffc8a02",
        "title": "Free energy score space",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/08d98638c6fcd194a4b1e6992063e944-Abstract.html",
        "author": "Alessandro Perina; Marco Cristani; Umberto Castellani; Vittorio Murino; Nebojsa Jojic",
        "abstract": "Score functions induced by generative models extract fixed-dimension feature vectors from different-length data observations by subsuming the process of data generation, projecting them in highly informative spaces called score spaces. In this way, standard discriminative classifiers are proved to achieve higher performances than a solely generative or discriminative approach. In this paper, we present a novel score space that exploits the free energy associated to a generative model through a score function. This function aims at capturing both the uncertainty of the model learning and ``local compliance  of data observations with respect to the generative process. Theoretical justifications and convincing comparative classification results on various generative models prove the goodness of the proposed strategy.",
        "bibtex": "@inproceedings{NIPS2009_08d98638,\n author = {Perina, Alessandro and Cristani, Marco and Castellani, Umberto and Murino, Vittorio and Jojic, Nebojsa},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Free energy score space},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/08d98638c6fcd194a4b1e6992063e944-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 277872,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12139961999263109753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Verona, Italy + Microsoft Research, Redmond, WA; Department of Computer Science, University of Verona, Italy + IIT, Italian Institute of Technology, Genova, Italy; Department of Computer Science, University of Verona, Italy; Department of Computer Science, University of Verona, Italy + IIT, Italian Institute of Technology, Genova, Italy; Microsoft Research, Redmond, WA",
        "aff_domain": "univr.it;univr.it;univr.it;univr.it;microsoft.com",
        "email": "univr.it;univr.it;univr.it;univr.it;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2;0;0+2;1",
        "aff_unique_norm": "University of Verona;Microsoft;Italian Institute of Technology",
        "aff_unique_dep": "Department of Computer Science;Microsoft Research;",
        "aff_unique_url": "https://www.univr.it;https://www.microsoft.com/en-us/research;https://www.iit.it",
        "aff_unique_abbr": ";MSR;IIT",
        "aff_campus_unique_index": "1;2;2;1",
        "aff_campus_unique": ";Redmond;Genova",
        "aff_country_unique_index": "0+1;0+0;0;0+0;1",
        "aff_country_unique": "Italy;United States"
    },
    {
        "id": "14412c7f44",
        "title": "From PAC-Bayes Bounds to KL Regularization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html",
        "author": "Pascal Germain; Alexandre Lacasse; Mario Marchand; Sara Shanian; Fran\u00e7ois Laviolette",
        "abstract": "We show that convex KL-regularized objective functions are obtained from a PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs classifier that upper-bound the standard zero-one loss used for the weighted majority vote. By restricting ourselves to a class of posteriors, that we call quasi uniform, we propose a simple coordinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show that standard ell",
        "bibtex": "@inproceedings{NIPS2009_250cf8b5,\n author = {Germain, Pascal and Lacasse, Alexandre and Marchand, Mario and Shanian, Sara and Laviolette, Fran\\c{c}ois},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From PAC-Bayes Bounds to KL Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/250cf8b51c773f3f8dc8b4be867a9a02-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 194890,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15823924467321464327&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science and Software Engineering, Laval University, Qu\u00e9bec (QC), Canada; Department of Computer Science and Software Engineering, Laval University, Qu\u00e9bec (QC), Canada; Department of Computer Science and Software Engineering, Laval University, Qu\u00e9bec (QC), Canada; Department of Computer Science and Software Engineering, Laval University, Qu\u00e9bec (QC), Canada; Department of Computer Science and Software Engineering, Laval University, Qu\u00e9bec (QC), Canada",
        "aff_domain": "ift.ulaval.ca;ift.ulaval.ca;ift.ulaval.ca;ift.ulaval.ca;ift.ulaval.ca",
        "email": "ift.ulaval.ca;ift.ulaval.ca;ift.ulaval.ca;ift.ulaval.ca;ift.ulaval.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Laval University",
        "aff_unique_dep": "Department of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.laval.ca",
        "aff_unique_abbr": "Laval",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Qu\u00e9bec",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "3afe00072c",
        "title": "Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/eb163727917cbba1eea208541a643e74-Abstract.html",
        "author": "Steven Chase; Andrew Schwartz; Wolfgang Maass; Robert A. Legenstein",
        "abstract": "The control of neuroprosthetic devices from the activity of motor cortex neurons benefits from learning effects where the function of these neurons is adapted to the control task. It was recently shown that tuning properties of neurons in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity. In particular, it was shown that the tuning curves of those neurons whose preferred directions had been misinterpreted changed more than those of other neurons. In this article, we show that the experimentally observed self-tuning properties of the system can be explained on the basis of a simple learning rule. This learning rule utilizes neuronal noise for exploration and performs Hebbian weight updates that are modulated by a global reward signal. In contrast to most previously proposed reward-modulated Hebbian learning rules, this rule does not require extraneous knowledge about what is noise and what is signal. The learning rule is able to optimize the performance of the model system within biologically realistic periods of time and under high noise levels. When the neuronal noise is fitted to experimental data, the model produces learning effects similar to those found in monkey experiments.",
        "bibtex": "@inproceedings{NIPS2009_eb163727,\n author = {Chase, Steven and Schwartz, Andrew and Maass, Wolfgang and Legenstein, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/eb163727917cbba1eea208541a643e74-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/eb163727917cbba1eea208541a643e74-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/eb163727917cbba1eea208541a643e74-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 138160,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2778450209740916154&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Institute for Theoretical Computer Science, Graz University of Technology, Austria; Department of Neurobiology, University of Pittsburgh + Center for the Neural Basis of Cognition + Department of Statistics, Carnegie Mellon University; Department of Neurobiology, University of Pittsburgh + Center for the Neural Basis of Cognition; Institute for Theoretical Computer Science, Graz University of Technology, Austria",
        "aff_domain": "igi.tugraz.at; ; ; ",
        "email": "igi.tugraz.at; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2+3;1+2;0",
        "aff_unique_norm": "Graz University of Technology;University of Pittsburgh;Center for the Neural Basis of Cognition;Carnegie Mellon University",
        "aff_unique_dep": "Institute for Theoretical Computer Science;Department of Neurobiology;;Department of Statistics",
        "aff_unique_url": "https://www.tugraz.at;https://www.pitt.edu;;https://www.cmu.edu",
        "aff_unique_abbr": "TU Graz;Pitt;;CMU",
        "aff_campus_unique_index": "0;;;0",
        "aff_campus_unique": "Graz;",
        "aff_country_unique_index": "0;1+1+1;1+1;0",
        "aff_country_unique": "Austria;United States"
    },
    {
        "id": "87f76f5ee6",
        "title": "Gaussian process regression with Student-t likelihood",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/13fe9d84310e77f13a6d184dbf1232f3-Abstract.html",
        "author": "Jarno Vanhatalo; Pasi Jyl\u00e4nki; Aki Vehtari",
        "abstract": "In the Gaussian process regression the observation model is commonly assumed to be Gaussian, which is convenient in computational perspective. However, the drawback is that the predictive accuracy of the model can be significantly compromised if the observations are contaminated by outliers. A robust observation model, such as the Student-t distribution, reduces the influence of outlying observations and improves the predictions. The problem, however, is the analytically intractable inference. In this work, we discuss the properties of a Gaussian process regression model with the Student-t likelihood and utilize the Laplace approximation for approximate inference. We compare our approach to a variational approximation and a Markov chain Monte Carlo scheme, which utilize the commonly used scale mixture representation of the Student-t distribution.",
        "bibtex": "@inproceedings{NIPS2009_13fe9d84,\n author = {Vanhatalo, Jarno and Jyl\\\"{a}nki, Pasi and Vehtari, Aki},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian process regression with Student-t likelihood},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/13fe9d84310e77f13a6d184dbf1232f3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 442464,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12315419076819867646&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Biomedical Engineering and Computational Science, Helsinki University of Technology, Finland; Department of Biomedical Engineering and Computational Science, Helsinki University of Technology, Finland; Department of Biomedical Engineering and Computational Science, Helsinki University of Technology, Finland",
        "aff_domain": "tkk.fi;tkk.fi;tkk.fi",
        "email": "tkk.fi;tkk.fi;tkk.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Helsinki University of Technology",
        "aff_unique_dep": "Department of Biomedical Engineering and Computational Science",
        "aff_unique_url": "https://www.aalto.fi/en",
        "aff_unique_abbr": "HUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "9bd599afa0",
        "title": "Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/fbd7939d674997cdb4692d34de8633c4-Abstract.html",
        "author": "Kian M. Chai",
        "abstract": "We provide some insights into how task correlations in multi-task Gaussian process (GP) regression affect the generalization error and the learning curve.  We analyze the asymmetric two-task case, where a secondary task is to help the learning of a primary task. Within this setting, we give bounds on the generalization error and the learning curve of the primary task. Our approach admits intuitive understandings of the multi-task GP by relating it to single-task GPs. For the case of one-dimensional input-space under optimal sampling with data only for the secondary task, the limitations of multi-task GP can be quantified explicitly.",
        "bibtex": "@inproceedings{NIPS2009_fbd7939d,\n author = {Chai, Kian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/fbd7939d674997cdb4692d34de8633c4-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/fbd7939d674997cdb4692d34de8633c4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/fbd7939d674997cdb4692d34de8633c4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/fbd7939d674997cdb4692d34de8633c4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 296504,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13240964637354705757&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk",
        "email": "ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f83408f025",
        "title": "Graph Zeta Function in the Bethe Free Energy and Loopy Belief Propagation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/b6f0479ae87d244975439c6124592772-Abstract.html",
        "author": "Yusuke Watanabe; Kenji Fukumizu",
        "abstract": "We propose a new approach to the analysis of Loopy Belief Propagation (LBP) by establishing a formula that connects the Hessian of the Bethe free energy with the edge zeta function. The formula has a number of theoretical implications on LBP. It is applied to give a sufficient condition that the Hessian of the Bethe free energy is positive definite, which shows non-convexity for graphs with multiple cycles. The formula clarifies the relation between the local stability of a fixed point of LBP and local minima of the Bethe free energy. We also propose a new approach to the uniqueness of LBP fixed point, and show various conditions of uniqueness.",
        "bibtex": "@inproceedings{NIPS2009_b6f0479a,\n author = {Watanabe, Yusuke and Fukumizu, Kenji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Graph Zeta Function in the Bethe Free Energy and Loopy Belief Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/b6f0479ae87d244975439c6124592772-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/b6f0479ae87d244975439c6124592772-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/b6f0479ae87d244975439c6124592772-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/b6f0479ae87d244975439c6124592772-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 134382,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1340758075253431033&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "The Institute of Statistical Mathematics; The Institute of Statistical Mathematics",
        "aff_domain": "ism.ac.jp;ism.ac.jp",
        "email": "ism.ac.jp;ism.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institute of Statistical Mathematics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ism.ac.jp",
        "aff_unique_abbr": "ISM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "f49c4e365b",
        "title": "Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d7a728a67d909e714c0774e22cb806f2-Abstract.html",
        "author": "Jing Gao; Feng Liang; Wei Fan; Yizhou Sun; Jiawei Han",
        "abstract": "Little work has been done to directly combine the outputs of multiple supervised and unsupervised models. However, it can increase the accuracy and applicability of ensemble methods. First, we can boost the diversity of classification ensemble by incorporating multiple clustering outputs, each of which provides grouping constraints for the joint label predictions of a set of related objects. Secondly, ensemble of supervised models is limited in applications which have no access to raw data but to the meta-level model outputs. In this paper, we aim at calculating a consolidated classification solution for a set of objects by maximizing the consensus among both supervised predictions and unsupervised grouping constraints. We seek a global optimal label assignment for the target objects, which is different from the result of traditional majority voting and model combination approaches. We cast the problem into an optimization problem on a bipartite graph, where the objective function favors smoothness in the conditional probability estimates over the graph, as well as penalizes deviation from initial labeling of supervised models. We solve the problem through iterative propagation of conditional probability estimates among neighboring nodes, and interpret the method as conducting a constrained embedding in a transformed space, as well as a ranking on the graph. Experimental results on three real applications demonstrate the benefits of the proposed method over existing alternatives.",
        "bibtex": "@inproceedings{NIPS2009_d7a728a6,\n author = {Gao, Jing and Liang, Feng and Fan, Wei and Sun, Yizhou and Han, Jiawei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/d7a728a67d909e714c0774e22cb806f2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d7a728a67d909e714c0774e22cb806f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 256427,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9809584045370803921&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; IBM TJ Watson Research Center; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;us.ibm.com;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;us.ibm.com;illinois.edu;illinois.edu",
        "github": "",
        "project": "http://ews.uiuc.edu/~jinggao3/nips09bgcm.htm",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;IBM",
        "aff_unique_dep": ";Research Center",
        "aff_unique_url": "https://illinois.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "UIUC;IBM",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Urbana-Champaign;TJ Watson",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cefeb8089a",
        "title": "Group Sparse Coding",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/3b3dbaf68507998acd6a5a5254ab2d76-Abstract.html",
        "author": "Samy Bengio; Fernando Pereira; Yoram Singer; Dennis Strelow",
        "abstract": "Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency, the proposed approach yields better mean average precision in classification.",
        "bibtex": "@inproceedings{NIPS2009_3b3dbaf6,\n author = {Bengio, Samy and Pereira, Fernando and Singer, Yoram and Strelow, Dennis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Group Sparse Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/3b3dbaf68507998acd6a5a5254ab2d76-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 109871,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=248902551758598833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Google Mountain View, CA; Google Mountain View, CA; Google Mountain View, CA; Google Mountain View, CA",
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "660c882b6d",
        "title": "Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/dd45045f8c68db9f54e70c67048d32e8-Abstract.html",
        "author": "Grzegorz Swirszcz; Naoki Abe; Aurelie C. Lozano",
        "abstract": "We consider the problem of variable group selection for least squares regression, namely, that of selecting groups of variables for best regression performance, leveraging and adhering to a natural grouping structure within the explanatory variables. We show that this problem can be efficiently addressed by using a certain greedy style algorithm. More precisely, we propose the Group Orthogonal Matching Pursuit algorithm (Group-OMP), which extends the standard OMP procedure (also referred to as ``forward greedy feature selection algorithm for least squares regression) to perform stage-wise group variable selection. We prove that under certain conditions Group-OMP can identify the correct (groups of) variables. We also provide an upperbound on the $l_\\infty$ norm of the difference between the estimated regression coefficients and the true coefficients. Experimental results on simulated and real world datasets indicate that Group-OMP compares favorably to Group Lasso, OMP and Lasso, both in terms of variable selection and prediction accuracy.",
        "bibtex": "@inproceedings{NIPS2009_dd45045f,\n author = {Swirszcz, Grzegorz and Abe, Naoki and Lozano, Aurelie C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/dd45045f8c68db9f54e70c67048d32e8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 269085,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13151214234648665934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5968c6310e",
        "title": "Heavy-Tailed Symmetric Stochastic Neighbor Embedding",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html",
        "author": "Zhirong Yang; Irwin King; Zenglin Xu; Erkki Oja",
        "abstract": "Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data visualization.  Currently, the most popular implementation, t-SNE, is restricted to a particular Student t-distribution as its embedding distribution. Moreover, it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size, momentum, etc., in finding its optimum. In this paper, we propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) method, which is a generalization of the t-SNE to accommodate various heavy-tailed embedding similarity functions. With this generalization, we are presented with two difficulties.  The first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected. Our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions. Based on this finding, we present a parameterized subset of similarity functions for choosing the best tail-heaviness for HSSNE; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies, one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-SNE implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-SNE.",
        "bibtex": "@inproceedings{NIPS2009_2291d2ec,\n author = {Yang, Zhirong and King, Irwin and Xu, Zenglin and Oja, Erkki},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Heavy-Tailed Symmetric Stochastic Neighbor Embedding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 533173,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1718887955822336716&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "The Chinese University of Hong Kong + Helsinki University of Technology; The Chinese University of Hong Kong; The Chinese University of Hong Kong + Saarland University & MPI for Informatics; Helsinki University of Technology",
        "aff_domain": "tkk.fi;cse.cuhk.edu.hk;cse.cuhk.edu.hk;tkk.fi",
        "email": "tkk.fi;cse.cuhk.edu.hk;cse.cuhk.edu.hk;tkk.fi",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0+2;1",
        "aff_unique_norm": "Chinese University of Hong Kong;Helsinki University of Technology;Saarland University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.helsinki.fi/en;https://www.uni-saarland.de",
        "aff_unique_abbr": "CUHK;HUT;Saarland U",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0+1;0;0+2;1",
        "aff_country_unique": "China;Finland;Germany"
    },
    {
        "id": "dae3842be2",
        "title": "Help or Hinder: Bayesian Models of Social Goal Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/52292e0c763fd027c6eba6b8f494d2eb-Abstract.html",
        "author": "Tomer Ullman; Chris Baker; Owen Macindoe; Owain Evans; Noah Goodman; Joshua B. Tenenbaum",
        "abstract": "Everyday social interactions are heavily influenced by our snap judgments about others goals.  Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is",
        "bibtex": "@inproceedings{NIPS2009_52292e0c,\n author = {Ullman, Tomer and Baker, Chris and Macindoe, Owen and Evans, Owain and Goodman, Noah and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Help or Hinder: Bayesian Models of Social Goal Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/52292e0c763fd027c6eba6b8f494d2eb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272426,
        "gs_citation": 253,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8791047880004161057&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu/bcs/",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b19a5f9805",
        "title": "Heterogeneous multitask learning with joint sparsity constraints",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/58c54802a9fb9526cd0923353a34a7ae-Abstract.html",
        "author": "Xiaolin Yang; Seyoung Kim; Eric P. Xing",
        "abstract": "Multitask learning addressed the problem of learning related tasks whose information can be shared each other. Traditional problem usually deal with homogeneous tasks such as regression, classification individually. In this paper we consider the problem learning multiple related tasks where tasks consist of both continuous and discrete outputs from a common set of input variables that lie in a high-dimensional space. All of the tasks are related in the sense that they share the same set of relevant input variables, but the amount of influence of each input on different outputs may vary. We formulate this problem as a combination of linear regression and logistic regression and model the joint sparsity as L1/Linf and L1/L2-norm of the model parameters. Among several possible applications, our approach addresses an important open problem in genetic association mapping, where we are interested in discovering genetic markers that influence multiple correlated traits jointly. In our experiments, we demonstrate our method in the scenario of association mapping, using simulated and asthma data, and show that the algorithm can effectively recover the relevant inputs with respect to all of the tasks.",
        "bibtex": "@inproceedings{NIPS2009_58c54802,\n author = {Yang, Xiaolin and Kim, Seyoung and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Heterogeneous multitask learning with joint sparsity constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/58c54802a9fb9526cd0923353a34a7ae-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 357035,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14901868104148240890&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "stat.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "stat.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "52db8e6b04",
        "title": "Hierarchical Learning of Dimensional Biases in Human Categorization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/731c83db8d2ff01bdc000083fd3c3740-Abstract.html",
        "author": "Adam Sanborn; Nick Chater; Katherine A. Heller",
        "abstract": "Existing models of categorization typically represent to-be-classified items as points in a multidimensional space. While from a mathematical point of view, an infinite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions, and have a strong preference to generalize along the axes of these dimensions, but not diagonally\". What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Specifically, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. Our model can be viewed as a type of transformed Dirichlet process mixture model, where it is the learning of the base distribution of the Dirichlet process which allows dimensional generalization.The learning behaviour of our model captures the developmental shift from roughly \"isotropic\" for children to the axis-aligned generalization that adults show.\"",
        "bibtex": "@inproceedings{NIPS2009_731c83db,\n author = {Sanborn, Adam and Chater, Nick and Heller, Katherine A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Learning of Dimensional Biases in Human Categorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/731c83db8d2ff01bdc000083fd3c3740-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 987206,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2683026936460915899&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Engineering, University of Cambridge; Gatsby Computational Neuroscience Unit, University College London; Cognitive, Perceptual and Brain Sciences, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Cambridge;University College London",
        "aff_unique_dep": "Department of Engineering;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Cambridge;UCL",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Cambridge;London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "62e4b609bf",
        "title": "Hierarchical Mixture of Classification Experts Uncovers Interactions between Brain Regions",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a86c450b76fb8c371afead6410d55534-Abstract.html",
        "author": "Bangpeng Yao; Dirk Walther; Diane Beck; Li Fei-fei",
        "abstract": "The human brain can be described as containing a number of functional regions. For a given task, these regions, as well as the connections between them, play a key role in information processing in the brain. However, most existing multi-voxel pattern analysis approaches either treat multiple functional regions as one large uniform region or several independent regions, ignoring the connections between regions. In this paper, we propose to model such connections in an Hidden Conditional Random Field (HCRF) framework, where the classifier of one region of interest (ROI) makes predictions based on not only its voxels but also the classifier predictions from ROIs that it connects to. Furthermore, we propose a structural learning method in the HCRF framework to automatically uncover the connections between ROIs. Experiments on fMRI data acquired while human subjects viewing images of natural scenes show that our model can improve the top-level (the classifier combining information from all ROIs) and ROI-level prediction accuracy, as well as uncover some meaningful connections between ROIs.",
        "bibtex": "@inproceedings{NIPS2009_a86c450b,\n author = {Yao, Bangpeng and Walther, Dirk and Beck, Diane and Fei-fei, Li},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Mixture of Classification Experts Uncovers Interactions between Brain Regions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a86c450b76fb8c371afead6410d55534-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a86c450b76fb8c371afead6410d55534-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a86c450b76fb8c371afead6410d55534-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 451648,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13128336096683214674&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Computer Science Department, Stanford University; Beckman Institute, University of Illinois at Urbana-Champaign + Psychology Department, University of Illinois at Urbana-Champaign; Beckman Institute, University of Illinois at Urbana-Champaign + Psychology Department, University of Illinois at Urbana-Champaign; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;illinois.edu;illinois.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;illinois.edu;illinois.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+1;1+1;0",
        "aff_unique_norm": "Stanford University;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Computer Science Department;Beckman Institute",
        "aff_unique_url": "https://www.stanford.edu;https://www.illinois.edu",
        "aff_unique_abbr": "Stanford;UIUC",
        "aff_campus_unique_index": "0;1+1;1+1;0",
        "aff_campus_unique": "Stanford;Urbana-Champaign",
        "aff_country_unique_index": "0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4ff9f4f990",
        "title": "Hierarchical Modeling of Local Image Features through $L_p$-Nested Symmetric Distributions",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/c0e190d8267e36708f955d7ab048990d-Abstract.html",
        "author": "Matthias Bethge; Eero P. Simoncelli; Fabian H. Sinz",
        "abstract": "We introduce a new family of distributions, called $L_p${\\em -nested symmetric distributions}, whose densities access the data exclusively through a hierarchical cascade of $L_p$-norms. This class generalizes the family of spherically and $L_p$-spherically symmetric distributions which have recently been successfully used for natural image modeling. Similar to those distributions it allows for a nonlinear mechanism to reduce the dependencies between its variables. With suitable choices of the parameters and norms, this family also includes the Independent Subspace Analysis (ISA) model, which has been proposed as a means of deriving filters that mimic complex cells found in mammalian primary visual cortex. $L_p$-nested distributions are easy to estimate and allow us to explore the variety of models between ISA and the $L_p$-spherically symmetric models. Our main findings are that, without a preprocessing step of contrast gain control, the independent subspaces of ISA are in fact more dependent than the individual filter coefficients within a subspace and, with contrast gain control, where ISA finds more than one subspace, the filter responses were almost independent anyway.",
        "bibtex": "@inproceedings{NIPS2009_c0e190d8,\n author = {Bethge, Matthias and Simoncelli, Eero and Sinz, Fabian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Modeling of Local Image Features through L\\_p-Nested Symmetric Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/c0e190d8267e36708f955d7ab048990d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/c0e190d8267e36708f955d7ab048990d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4493479,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14738909797796885416&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Max Planck Institute for Biological Cybernetics; Center for Neural Science, and Courant Institute of Mathematical Sciences, New York University + Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics",
        "aff_domain": "tuebingen.mpg.de;nyu.edu;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;nyu.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;New York University",
        "aff_unique_dep": "Biological Cybernetics;Center for Neural Science",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;https://www.nyu.edu",
        "aff_unique_abbr": "MPIBC;NYU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "9ff164394d",
        "title": "Human Rademacher Complexity",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html",
        "author": "Xiaojin Zhu; Bryan R. Gibson; Timothy T. Rogers",
        "abstract": "We propose to use Rademacher complexity, originally developed in computational learning theory, as a measure of human learning capacity.  Rademacher complexity measures a learners ability to fit random data, and can be used to bound the learners true error based on the observed training sample error.  We first review the definition of Rademacher complexity and its generalization bound.  We then describe a learning the noise\" procedure to experimentally measure human Rademacher complexities.  The results from empirical studies showed that: (i) human Rademacher complexity can be successfully measured, (ii) the complexity depends on the domain and training sample size in intuitive ways, (iii) human learning respects the generalization bounds, (iv) the bounds can be useful in predicting the danger of overfitting in human learning.  Finally, we discuss the potential applications of human Rademacher complexity in cognitive science.\"",
        "bibtex": "@inproceedings{NIPS2009_f7664060,\n author = {Zhu, Jerry and Gibson, Bryan and Rogers, Timothy T},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Human Rademacher Complexity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/f7664060cc52bc6f3d620bcedc94a4b6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f7664060cc52bc6f3d620bcedc94a4b6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 125703,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6008091986471335410&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Sciences; Department of Psychology; Department of Computer Sciences",
        "aff_domain": "cs.wisc.edu;wisc.edu;cs.wisc.edu",
        "email": "cs.wisc.edu;wisc.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Wisconsin-Madison;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Sciences;Department of Psychology",
        "aff_unique_url": "https://www.cs.wisc.edu;",
        "aff_unique_abbr": "UW-Madison;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "e87093f92e",
        "title": "Improving Existing Fault Recovery Policies",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/70c639df5e30bdee440e4cdf599fec2b-Abstract.html",
        "author": "Guy Shani; Christopher Meek",
        "abstract": "Automated recovery from failures is a key component in the management of large data centers. Such systems typically employ a hand-made controller created by an expert. While such controllers capture many important aspects of the recovery process, they are often not systematically optimized to reduce costs such as server downtime. In this paper we explain how to use data gathered from the interactions of the hand-made controller with the system, to create an optimized controller. We suggest learning an indefinite horizon Partially Observable Markov Decision Process, a model for decision making under uncertainty, and solve it using a point-based algorithm. We describe the complete process, starting with data gathering, model learning, model checking procedures, and computing a policy. While our paper focuses on a specific domain, our method is applicable to other systems that use a hand-coded, imperfect controllers.",
        "bibtex": "@inproceedings{NIPS2009_70c639df,\n author = {Shani, Guy and Meek, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improving Existing Fault Recovery Policies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/70c639df5e30bdee440e4cdf599fec2b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/70c639df5e30bdee440e4cdf599fec2b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/70c639df5e30bdee440e4cdf599fec2b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 164741,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4792618492618300428&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Information Systems Engineering, Ben Gurion University, Beer-Sheva, Israel; Microsoft Research, One Microsoft Way, Redmond, WA",
        "aff_domain": "bgu.ac.il;microsoft.com",
        "email": "bgu.ac.il;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Ben Gurion University;Microsoft",
        "aff_unique_dep": "Department of Information Systems Engineering;Research",
        "aff_unique_url": "https://www.bgu.ac.il;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "BGU;MSR",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Beer-Sheva;Redmond",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "e377ea7640",
        "title": "Indian Buffet Processes with Power-law Behavior",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f1b6f2857fb6d44dd73c7041e0aa0f19-Abstract.html",
        "author": "Yee W. Teh; Dilan Gorur",
        "abstract": "The Indian buffet process (IBP) is an exchangeable distribution over binary matrices used in Bayesian nonparametric featural models.  In this paper we propose a three-parameter generalization of the IBP exhibiting power-law behavior.  We achieve this by generalizing the beta process (the de Finetti measure of the IBP) to the \\emph{stable-beta process} and deriving the IBP corresponding to it.  We find interesting relationships between the stable-beta process and the Pitman-Yor process (another stochastic process used in Bayesian nonparametric models with interesting power-law properties).  We show that our power-law IBP is a good model for word occurrences in documents with improved performance over the normal IBP.",
        "bibtex": "@inproceedings{NIPS2009_f1b6f285,\n author = {Teh, Yee and Gorur, Dilan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Indian Buffet Processes with Power-law Behavior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 350840,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16734231248920117339&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "021d7df170",
        "title": "Individuation, Identification and Object Discovery",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/38af86134b65d0f10fe33d30dd76442e-Abstract.html",
        "author": "Charles Kemp; Alan Jern; Fei Xu",
        "abstract": "Humans are typically able to infer how many objects their environment contains and to recognize when the same object is encountered twice.  We present a simple statistical model that helps to explain these abilities and evaluate it in three behavioral experiments.  Our first experiment suggests that humans rely on prior knowledge when deciding whether an object token has been previously encountered. Our second and third experiments suggest that humans can infer how many objects they have seen and can learn about categories and their properties even when they are uncertain about which tokens are instances of the same object.",
        "bibtex": "@inproceedings{NIPS2009_38af8613,\n author = {Kemp, Charles and Jern, Alan and Xu, Fei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Individuation, Identification and Object Discovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/38af86134b65d0f10fe33d30dd76442e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 330665,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16823970068161029035&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Psychology, Carnegie Mellon University; Department of Psychology, Carnegie Mellon University; Department of Psychology, University of California, Berkeley",
        "aff_domain": "cmu.edu;cmu.edu;berkeley.edu",
        "email": "cmu.edu;cmu.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;University of California, Berkeley",
        "aff_unique_dep": "Department of Psychology;Department of Psychology",
        "aff_unique_url": "https://www.cmu.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "CMU;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ed8ad4d94e",
        "title": "Information-theoretic lower bounds on the oracle complexity of convex optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2387337ba1e0b0249ba90f55b2ba2521-Abstract.html",
        "author": "Alekh Agarwal; Martin J. Wainwright; Peter L. Bartlett; Pradeep K. Ravikumar",
        "abstract": "Despite the large amount of literature on upper bounds on complexity of convex analysis, surprisingly little is known about the fundamental hardness of these problems. The extensive use of convex optimization in machine learning and statistics makes such an understanding critical to understand fundamental computational limits of learning and estimation. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We improve upon known results and obtain tight minimax complexity estimates for some function classes. We also discuss implications of these results to the understanding the inherent complexity of large-scale learning and estimation problems.",
        "bibtex": "@inproceedings{NIPS2009_2387337b,\n author = {Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Information-theoretic lower bounds on the oracle complexity of convex optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2387337ba1e0b0249ba90f55b2ba2521-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 173062,
        "gs_citation": 553,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16481506039463236513&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 25,
        "aff": "Computer Science Division, UC Berkeley; Computer Science Division, Department of Statistics, UC Berkeley; Department of Computer Sciences, UT Austin; Department of EECS, Department of Statistics, UC Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.utexas.edu;eecs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.utexas.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of California, Berkeley;University of Texas at Austin",
        "aff_unique_dep": "Computer Science Division;Department of Computer Sciences",
        "aff_unique_url": "https://www.berkeley.edu;https://www.utexas.edu",
        "aff_unique_abbr": "UC Berkeley;UT Austin",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Berkeley;Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4edad49fb6",
        "title": "Inter-domain Gaussian Processes for Sparse Inference using Inducing Features",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/5ea1649a31336092c05438df996a3e59-Abstract.html",
        "author": "Miguel L\u00e1zaro-Gredilla; An\u00edbal Figueiras-Vidal",
        "abstract": "We present a general inference framework for inter-domain Gaussian Processes (GPs), focusing on its usefulness to build sparse GP models. The state-of-the-art sparse GP model introduced by Snelson and Ghahramani in [1] relies on finding a small, representative pseudo data set of m elements (from the same domain as the n available data elements) which is able to explain existing data well, and then uses it to perform inference. This reduces inference and model selection computation time from O(n^3) to O(m^2n), where m << n. Inter-domain GPs can be used to find a (possibly more compact) representative set of features lying in a different domain, at the same computational cost. Being able to specify a different domain for the representative features allows to incorporate prior knowledge about relevant characteristics of data and detaches the functional form of the covariance and basis functions. We will show how previously existing models fit into this framework and will use it to develop two new sparse GP models. Tests on large, representative regression data sets suggest that significant improvement can be achieved, while retaining computational efficiency.",
        "bibtex": "@inproceedings{NIPS2009_5ea1649a,\n author = {L\\'{a}zaro-Gredilla, Miguel and Figueiras-Vidal, An\\'{\\i}bal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inter-domain Gaussian Processes for Sparse Inference using Inducing Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/5ea1649a31336092c05438df996a3e59-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/5ea1649a31336092c05438df996a3e59-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/5ea1649a31336092c05438df996a3e59-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 215912,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16369686703537382683&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dep. Signal Processing & Communications, Universidad Carlos III de Madrid, SPAIN; Dep. Signal Processing & Communications, Universidad Carlos III de Madrid, SPAIN",
        "aff_domain": "tsc.uc3m.es;tsc.uc3m.es",
        "email": "tsc.uc3m.es;tsc.uc3m.es",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universidad Carlos III de Madrid",
        "aff_unique_dep": "Department of Signal Processing & Communications",
        "aff_unique_url": "https://www.uc3m.es",
        "aff_unique_abbr": "UC3M",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "724149c5f3",
        "title": "Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/685ac8cadc1be5ac98da9556bc1c8d9e-Abstract.html",
        "author": "Kenji Fukumizu; Arthur Gretton; Gert R. Lanckriet; Bernhard Sch\u00f6lkopf; Bharath K. Sriperumbudur",
        "abstract": "Embeddings of probability measures into reproducing kernel Hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities. In particular, the distance between embeddings (the maximum mean discrepancy, or MMD) has several key advantages over many classical metrics on distributions, namely easy computability, fast convergence and low bias of finite sample estimates. An important requirement of the embedding RKHS is that it be characteristic: in this case, the MMD between two distributions is zero if and only if the distributions coincide. Three new results on the MMD are introduced in the present study. First, it is established that MMD corresponds to the optimal risk of a kernel classifier, thus forming a natural link between the distance between distributions and their ease of classification. An important consequence is that a kernel must be characteristic to guarantee classifiability between distributions in the RKHS. Second, the class of characteristic kernels is broadened to incorporate all strictly positive definite kernels: these include non-translation invariant kernels and kernels on non-compact domains. Third, a generalization of the MMD is proposed for families of kernels, as the supremum over MMDs on a class of kernels (for instance the Gaussian kernels with different bandwidths). This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classifier. The generalized MMD is shown to have consistent finite sample estimates, and its performance is demonstrated on a homogeneity testing example.",
        "bibtex": "@inproceedings{NIPS2009_685ac8ca,\n author = {Fukumizu, Kenji and Gretton, Arthur and Lanckriet, Gert and Sch\\\"{o}lkopf, Bernhard and Sriperumbudur, Bharath K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/685ac8cadc1be5ac98da9556bc1c8d9e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/685ac8cadc1be5ac98da9556bc1c8d9e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/685ac8cadc1be5ac98da9556bc1c8d9e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/685ac8cadc1be5ac98da9556bc1c8d9e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 252117,
        "gs_citation": 294,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7143156798799014294&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Department of ECE, UC San Diego, La Jolla, USA; The Institute of Statistical Mathematics, Tokyo, Japan; Carnegie Mellon University, MPI for Biological Cybernetics; Department of ECE, UC San Diego, La Jolla, USA; MPI for Biological Cybernetics, T\u00a8ubingen, Germany",
        "aff_domain": "ucsd.edu;ism.ac.jp;gmail.com;ece.ucsd.edu;tuebingen.mpg.de",
        "email": "ucsd.edu;ism.ac.jp;gmail.com;ece.ucsd.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;3",
        "aff_unique_norm": "University of California, San Diego;Institute of Statistical Mathematics;Carnegie Mellon University;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;;;Biological Cybernetics",
        "aff_unique_url": "https://www.ucsd.edu;https://www.ism.ac.jp;https://www.cmu.edu;https://www.biological-cybernetics.de",
        "aff_unique_abbr": "UCSD;ISM;CMU;MPIBC",
        "aff_campus_unique_index": "0;1;0;3",
        "aff_campus_unique": "La Jolla;Tokyo;;T\u00fcbingen",
        "aff_country_unique_index": "0;1;0;0;2",
        "aff_country_unique": "United States;Japan;Germany"
    },
    {
        "id": "79f4060de1",
        "title": "Kernel Methods for Deep Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html",
        "author": "Youngmin Cho; Lawrence K. Saul",
        "abstract": "We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets.  These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs).  We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures.  On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.",
        "bibtex": "@inproceedings{NIPS2009_5751ec3e,\n author = {Cho, Youngmin and Saul, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Methods for Deep Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 769272,
        "gs_citation": 1028,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13780742961636406443&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Engineering, University of California, San Diego; Department of Computer Science and Engineering, University of California, San Diego",
        "aff_domain": "cs.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f8a28c95bf",
        "title": "Kernels and learning curves for Gaussian process regression on random graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/92cc227532d17e56e07902b254dfad10-Abstract.html",
        "author": "Peter Sollich; Matthew Urry; Camille Coti",
        "abstract": "We investigate how well Gaussian process regression can learn functions defined on graphs, using large regular random graphs as a paradigmatic example. Random-walk based kernels are shown to have some surprising properties: within the standard approximation of a locally tree-like graph structure, the kernel does not become constant, i.e.neighbouring function values do not become fully correlated, when the lengthscale $\\sigma$ of the kernel is made large. Instead the kernel attains a non-trivial limiting form, which we calculate. The fully correlated limit is reached only once loops become relevant, and we estimate where the crossover to this regime occurs. Our main subject are learning curves of Bayes error versus training set size. We show that these are qualitatively well predicted by a simple approximation using only the spectrum of a large tree as input, and generically scale with $n/V$, the number of training examples per vertex. We also explore how this behaviour changes once kernel lengthscales are large enough for loops to become important.",
        "bibtex": "@inproceedings{NIPS2009_92cc2275,\n author = {Sollich, Peter and Urry, Matthew and Coti, Camille},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernels and learning curves for Gaussian process regression on random graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/92cc227532d17e56e07902b254dfad10-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/92cc227532d17e56e07902b254dfad10-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/92cc227532d17e56e07902b254dfad10-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/92cc227532d17e56e07902b254dfad10-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 350602,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8198122636983647796&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "King\u2019s College London, Department of Mathematics; King\u2019s College London, Department of Mathematics; INRIA Saclay \u02c6Ile de France",
        "aff_domain": "kcl.ac.uk;kcl.ac.uk; ",
        "email": "kcl.ac.uk;kcl.ac.uk; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "King's College London;INRIA",
        "aff_unique_dep": "Department of Mathematics;",
        "aff_unique_url": "https://www.kcl.ac.uk;https://www.inria.fr",
        "aff_unique_abbr": "KCL;INRIA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Saclay",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "id": "43dab7e01e",
        "title": "Know Thy Neighbour: A Normative Theory of Synaptic Depression",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e4bb4c5173c2ce17fd8fcd40041c068f-Abstract.html",
        "author": "Jean-pascal Pfister; Peter Dayan; M\u00e1t\u00e9 Lengyel",
        "abstract": "Synapses exhibit an extraordinary degree of short-term malleability, with release probabilities and effective synaptic strengths changing markedly over multiple timescales. From the perspective of a fixed computational operation in a network, this seems like a most unacceptable degree of added noise. We suggest an alternative theory according to which short term synaptic plasticity plays a normatively-justifiable role. This theory starts from the commonplace observation that the spiking of a neuron is an incomplete, digital, report of the analog quantity that contains all the critical information, namely its membrane potential. We suggest that one key task for a synapse is to solve the inverse problem of estimating the pre-synaptic membrane potential from the spikes it receives and prior  expectations, as in a recursive filter. We show that short-term synaptic depression has canonical dynamics which closely resemble those required for optimal estimation, and that it indeed supports high quality estimation. Under this account, the local postsynaptic potential and the level of synaptic resources track the (scaled) mean and variance of the estimated presynaptic membrane potential. We make  experimentally testable predictions for how the statistics of subthreshold membrane potential fluctuations and the form of spiking non-linearity should be related to the properties of short-term plasticity in any particular cell type.",
        "bibtex": "@inproceedings{NIPS2009_e4bb4c51,\n author = {Pfister, Jean-pascal and Dayan, Peter and Lengyel, M\\'{a}t\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Know Thy Neighbour: A Normative Theory of Synaptic Depression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/e4bb4c5173c2ce17fd8fcd40041c068f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e4bb4c5173c2ce17fd8fcd40041c068f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 900830,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14555285697197767316&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Computational & Biological Learning Lab, Department of Engineering, University of Cambridge; Gatsby Computational Neuroscience Unit, UCL; Computational & Biological Learning Lab, Department of Engineering, University of Cambridge",
        "aff_domain": "eng.cam.ac.uk;gatsby.ucl.ac.uk;eng.cam.ac.uk",
        "email": "eng.cam.ac.uk;gatsby.ucl.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Cambridge;University College London",
        "aff_unique_dep": "Department of Engineering;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Cambridge;UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "8fbd92a801",
        "title": "Label Selection on Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/90794e3b050f815354e3e29e977a88ab-Abstract.html",
        "author": "Andrew Guillory; Jeff A. Bilmes",
        "abstract": "We investigate methods for selecting sets of labeled vertices for use in predicting the labels of vertices on a graph.  We specifically study methods which choose a single batch of labeled vertices (i.e. offline, non sequential methods).  In this setting, we find common graph smoothness assumptions directly motivate simple label selection methods with interesting theoretical guarantees. These methods bound prediction error in terms of the smoothness of the true labels with respect to the graph.  Some of these bounds give new motivations for previously proposed algorithms, and some suggest new algorithms which we evaluate.  We show improved performance over baseline methods on several real world data sets.",
        "bibtex": "@inproceedings{NIPS2009_90794e3b,\n author = {Guillory, Andrew and Bilmes, Jeff A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Label Selection on Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/90794e3b050f815354e3e29e977a88ab-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/90794e3b050f815354e3e29e977a88ab-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/90794e3b050f815354e3e29e977a88ab-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/90794e3b050f815354e3e29e977a88ab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 397918,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4396835175746741996&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Washington; Department of Electrical Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;ee.washington.edu",
        "email": "cs.washington.edu;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b68ad38ed0",
        "title": "Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1f50893f80d6830d62765ffad7721742-Abstract.html",
        "author": "Finale Doshi-velez; Shakir Mohamed; Zoubin Ghahramani; David A. Knowles",
        "abstract": "Nonparametric Bayesian models provide a framework for flexible probabilistic modelling of complex datasets. Unfortunately, Bayesian inference methods often require high-dimensional averages and can be slow to compute, especially with the potentially unbounded representations associated with nonparametric models. We address the challenge of scaling nonparametric Bayesian inference to the increasingly large datasets found in real-world applications, focusing on the case of parallelising inference in the Indian Buffet Process (IBP).  Our approach divides a large data set between multiple processors.  The processors use message passing to compute likelihoods in an asynchronous, distributed fashion and to propagate statistics about the global Bayesian posterior.  This novel MCMC sampler is the first parallel inference scheme for IBP-based models, scaling to datasets orders of magnitude larger than had previously been possible.",
        "bibtex": "@inproceedings{NIPS2009_1f50893f,\n author = {Doshi-velez, Finale and Mohamed, Shakir and Ghahramani, Zoubin and Knowles, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1f50893f80d6830d62765ffad7721742-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1f50893f80d6830d62765ffad7721742-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1f50893f80d6830d62765ffad7721742-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 318282,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8022303325169102009&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "University of Cambridge; University of Cambridge; University of Cambridge; University of Cambridge",
        "aff_domain": "alum.mit.edu;cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "email": "alum.mit.edu;cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "7cf8ba0804",
        "title": "Lattice Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html",
        "author": "Eric Garcia; Maya Gupta",
        "abstract": "We present a new empirical risk minimization framework for approximating functions from training samples for low-dimensional regression applications where a lattice (look-up table) is stored and interpolated at run-time for an efficient hardware implementation. Rather than evaluating a fitted function at the lattice nodes without regard to the fact that samples will be interpolated, the proposed lattice regression approach estimates the lattice to minimize the interpolation error on the given training samples. Experiments show that lattice regression can reduce mean test error  compared to Gaussian process regression for digital color management of printers, an application for which linearly interpolating a look-up table (LUT) is standard. Simulations confirm that lattice regression performs consistently better than the naive approach to learning the lattice, particularly when the density of training samples is low.",
        "bibtex": "@inproceedings{NIPS2009_4b025079,\n author = {Garcia, Eric and Gupta, Maya},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Lattice Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4b0250793549726d5c1ea3906726ebfe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 241983,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14717614516506393055&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering, University of Washington; Department of Electrical Engineering, University of Washington",
        "aff_domain": "ee.washington.edu;ee.washington.edu",
        "email": "ee.washington.edu;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7868974b02",
        "title": "Learning Brain Connectivity of Alzheimer's Disease from Neuroimaging Data",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1141938ba2c2b13f5505d7c424ebae5f-Abstract.html",
        "author": "Shuai Huang; Jing Li; Liang Sun; Jun Liu; Teresa Wu; Kewei Chen; Adam Fleisher; Eric Reiman; Jieping Ye",
        "abstract": "Recent advances in neuroimaging techniques provide great potentials for effective diagnosis of Alzheimer\u2019s disease (AD), the most common form of dementia. Previous studies have shown that AD is closely related to alternation in the functional brain network, i.e., the functional connectivity among different brain regions. In this paper, we consider the problem of learning functional brain connectivity from neuroimaging, which holds great promise for identifying image-based markers used to distinguish Normal Controls (NC), patients with Mild Cognitive Impairment (MCI), and patients with AD.  More specifically, we study sparse inverse covariance estimation (SICE), also known as exploratory Gaussian graphical models, for brain connectivity modeling. In particular, we apply SICE to learn and analyze functional brain connectivity patterns from different subject groups, based on a key property of SICE, called the \u201cmonotone property\u201d we established in this paper. Our experimental results on neuroimaging PET data of 42 AD, 116 MCI, and 67 NC subjects reveal several interesting connectivity patterns consistent with literature findings, and also some new patterns that can help the knowledge discovery of AD.",
        "bibtex": "@inproceedings{NIPS2009_1141938b,\n author = {Huang, Shuai and Li, Jing and Sun, Liang and Liu, Jun and Wu, Teresa and Chen, Kewei and Fleisher, Adam and Reiman, Eric and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Brain Connectivity of Alzheimer\\textquotesingle s Disease from Neuroimaging Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1141938ba2c2b13f5505d7c424ebae5f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 985171,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16261941347119305637&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5dac131181",
        "title": "Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html",
        "author": "Lei Wu; Rong Jin; Steven C. Hoi; Jianke Zhu; Nenghai Yu",
        "abstract": "Learning distance functions with side information plays a key role in many machine learning and data mining applications. Conventional approaches often assume a Mahalanobis distance function. These approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a fixed metric for the entire input space and therefore are unable to handle heterogeneous data. In this paper, we propose a novel scheme that learns nonlinear Bregman distance functions from side information using a non-parametric approach that is similar to support vector machines. The proposed scheme avoids the assumption of fixed metric because its local distance metric is implicitly derived from the Hessian matrix of a convex function that is used to generate the Bregman distance function. We present an efficient learning algorithm for the proposed scheme for distance function learning. The extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning, and (ii) is computationally efficient for high dimensional data.",
        "bibtex": "@inproceedings{NIPS2009_2f2b2656,\n author = {Wu, Lei and Jin, Rong and Hoi, Steven and Zhu, Jianke and Yu, Nenghai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2f2b265625d76a6704b08093c652fd79-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 220058,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6960176681739763653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "School of Computer Engineering, Nanyang Technological University, Singapore + Univeristy of Science and Technology of China, P.R. China; Department of Computer Science & Engineering, Michigan State University; School of Computer Engineering, Nanyang Technological University, Singapore; Computer Vision Lab, ETH Zurich, Swiss; Univeristy of Science and Technology of China, P.R. China",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0;3;1",
        "aff_unique_norm": "Nanyang Technological University;University of Science and Technology of China;Michigan State University;ETH Zurich",
        "aff_unique_dep": "School of Computer Engineering;;Department of Computer Science & Engineering;Computer Vision Lab",
        "aff_unique_url": "https://www.ntu.edu.sg;http://www.ustc.edu.cn;https://www.msu.edu;https://www.ethz.ch",
        "aff_unique_abbr": "NTU;USTC;MSU;ETHZ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Singapore;",
        "aff_country_unique_index": "0+1;2;0;3;1",
        "aff_country_unique": "Singapore;China;United States;Switzerland"
    },
    {
        "id": "e9012c64d9",
        "title": "Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/15d4e891d784977cacbfcbb00c48f133-Abstract.html",
        "author": "Natasha Singh-miller; Michael Collins",
        "abstract": "We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P(y|a), when the number of labels y is large and the labels share some underlying structure. We propose a method for learning error-correcting output codes (ECOCs) to model the similarity between labels within a nearest neighbor framework. The learned ECOCs and nearest neighbor information are used to provide conditional probability estimates. We apply these estimates to the problem of acoustic modeling for speech recognition. We demonstrate an absolute reduction in word error rate (WER) of 0.9% (a 2.5% relative reduction in WER) on a lecture recognition task over a state-of-the-art baseline GMM model.",
        "bibtex": "@inproceedings{NIPS2009_15d4e891,\n author = {Singh-miller, Natasha and Collins, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Label Embeddings for Nearest-Neighbor Multi-class Classification with an Application to Speech Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/15d4e891d784977cacbfcbb00c48f133-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 75343,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5670770915400226740&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;csail.mit.edu",
        "email": "mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5e9c90df48",
        "title": "Learning Non-Linear Combinations of Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e7f8a7fb0b77bcb3b283af5be021448f-Abstract.html",
        "author": "Corinna Cortes; Mehryar Mohri; Afshin Rostamizadeh",
        "abstract": "This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. It analyzes this problem in the case of regression and the kernel ridge regression algorithm. It examines the corresponding learning kernel optimization problem, shows how that minimax problem can be reduced to a simpler minimization problem, and proves that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.",
        "bibtex": "@inproceedings{NIPS2009_e7f8a7fb,\n author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Non-Linear Combinations of Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e7f8a7fb0b77bcb3b283af5be021448f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131641,
        "gs_citation": 379,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4042860058252972939&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Google Research; Courant Institute and Google; Courant Institute and Google",
        "aff_domain": "google.com;cims.nyu.edu;cs.nyu.edu",
        "email": "google.com;cims.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Google;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Google Research;Mathematical Sciences",
        "aff_unique_url": "https://research.google;https://courant.nyu.edu",
        "aff_unique_abbr": "Google Research;Courant",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e35561d840",
        "title": "Learning a Small Mixture of Trees",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e2a2dcc36a08a345332c751b2f2e476c-Abstract.html",
        "author": "M. P. Kumar; Daphne Koller",
        "abstract": "The problem of approximating a given probability distribution using a simpler distribution plays an important role in several areas of machine learning, e.g. variational inference and classification. Within this context, we consider the task of learning a mixture of tree distributions. Although mixtures of trees can be learned by minimizing the KL-divergence using an EM algorithm, its success depends heavily on the initialization. We propose an efficient strategy for obtaining a good initial set of trees that attempts to cover the entire observed distribution by minimizing the $\\alpha$-divergence with $\\alpha = \\infty$. We formulate the problem using the fractional covering framework and present a convergent sequential algorithm that only relies on solving a convex program at each iteration. Compared to previous methods, our approach results in a significantly smaller mixture of trees that provides similar or better accuracies. We demonstrate the usefulness of our approach by learning pictorial structures for face recognition.",
        "bibtex": "@inproceedings{NIPS2009_e2a2dcc3,\n author = {Kumar, M. and Koller, Daphne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning a Small Mixture of Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e2a2dcc36a08a345332c751b2f2e476c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 344430,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15747304184255699239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3246367bf6",
        "title": "Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f79921bbae40a577928b76d2fc3edc2a-Abstract.html",
        "author": "Massih R. Amini; Nicolas Usunier; Cyril Goutte",
        "abstract": "We address the problem of learning classifiers when observations have multiple views, some of which may not be observed for all examples.  We assume the existence of view generating functions which may complete the missing views in an approximate way.  This situation corresponds for example to learning text classifiers from multilingual collections where documents are not available in all languages.  In that case, Machine Translation (MT) systems may be used to translate each document in the missing languages.  We derive a generalization error bound for classifiers learned on examples with multiple artificially created views. Our result uncovers a trade-off between the size of the training set, the number of views, and the quality of the view generating functions. As a consequence, we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning.  An extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning.  Experimental results on a subset of the Reuters RCV1/RCV2 collections support our findings by showing that additional views obtained from MT may significantly improve the classification performance in the cases identified by our trade-off.",
        "bibtex": "@inproceedings{NIPS2009_f79921bb,\n author = {Amini, Massih R. and Usunier, Nicolas and Goutte, Cyril},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 118697,
        "gs_citation": 454,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=149209612197078043&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Interactive Language Technologies Group, National Research Council Canada; Laboratoire d\u2019Informatique de Paris 6, Universit\u00b4e Pierre et Marie Curie, France; Interactive Language Technologies Group, National Research Council Canada",
        "aff_domain": "cnrc-nrc.gc.ca;lip6.fr;cnrc-nrc.gc.ca",
        "email": "cnrc-nrc.gc.ca;lip6.fr;cnrc-nrc.gc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "National Research Council Canada;Universit\u00b4e Pierre et Marie Curie",
        "aff_unique_dep": "Interactive Language Technologies Group;Laboratoire d\u2019Informatique de Paris 6",
        "aff_unique_url": "https://www.nrc-cnrc.gc.ca;https://www.upmc.fr",
        "aff_unique_abbr": "NRC-CNRC;UPMC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Paris",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;France"
    },
    {
        "id": "dc2137d577",
        "title": "Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/cf67355a3333e6e143439161adc2d82e-Abstract.html",
        "author": "Tom Ouyang; Randall Davis",
        "abstract": "We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach significantly improves recognition performance.",
        "bibtex": "@inproceedings{NIPS2009_cf67355a,\n author = {Ouyang, Tom and Davis, Randall},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/cf67355a3333e6e143439161adc2d82e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/cf67355a3333e6e143439161adc2d82e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/cf67355a3333e6e143439161adc2d82e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 483297,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12302825510908815669&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c8e6427f40",
        "title": "Learning in Markov Random Fields using Tempered Transitions",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/b7ee6f5f9aa5cd17ca1aea43ce848496-Abstract.html",
        "author": "Ruslan Salakhutdinov",
        "abstract": "Markov random fields (MRFs), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables. Maximum likelihood learning in MRFs is hard due to the presence of the global normalizing constant. In this paper we consider a class of stochastic approximation algorithms of Robbins-Monro type that uses Markov chain Monte Carlo to do approximate maximum likelihood learning. We show that using MCMC operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large densely-connected MRFs. Our results on MNIST and NORB datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data and perform well on digit and object recognition tasks.",
        "bibtex": "@inproceedings{NIPS2009_b7ee6f5f,\n author = {Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning in Markov Random Fields using Tempered Transitions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 483977,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2682355449559910353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Brain and Cognitive Sciences and CSAIL, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu",
        "email": "mit.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Brain and Cognitive Sciences and Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "55a9e0aa38",
        "title": "Learning models of object structure",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/024d7f84fff11dd7e8d9c510137a2381-Abstract.html",
        "author": "Joseph Schlecht; Kobus Barnard",
        "abstract": "We present an approach for learning stochastic geometric models of object categories from single view images. We focus here on models expressible as a spatially contiguous assemblage of blocks. Model topologies are learned across groups of images, and one or more such topologies is linked to an object category (e.g.  chairs). Fitting learned topologies to an image can be used to identify the object class, as well as detail its geometry. The latter goes beyond labeling objects, as it provides the geometric structure of particular instances.  We learn the models using joint statistical inference over structure parameters, camera parameters, and instance parameters. These produce an image likelihood through a statistical imaging model. We use trans-dimensional sampling to explore topology hypotheses, and alternate between Metropolis-Hastings and stochastic dynamics to explore instance parameters. Experiments on images of furniture objects such as tables and chairs suggest that this is an effective approach for learning models that encode simple representations of category geometry and the statistics thereof, and support inferring both category and geometry on held out single view images.",
        "bibtex": "@inproceedings{NIPS2009_024d7f84,\n author = {Schlecht, Joseph and Barnard, Kobus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning models of object structure},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/024d7f84fff11dd7e8d9c510137a2381-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1643708,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16658415510900930437&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, University of Arizona; Department of Computer Science, University of Arizona",
        "aff_domain": "cs.arizona.edu;cs.arizona.edu",
        "email": "cs.arizona.edu;cs.arizona.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Arizona",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.arizona.edu",
        "aff_unique_abbr": "UArizona",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "552d9c3e4d",
        "title": "Learning to Explore and Exploit in POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e58cc5ca94270acaceed13bc82dfedf7-Abstract.html",
        "author": "Chenghui Cai; Xuejun Liao; Lawrence Carin",
        "abstract": "A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments. The method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the specific problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.",
        "bibtex": "@inproceedings{NIPS2009_e58cc5ca,\n author = {Cai, Chenghui and Liao, Xuejun and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Explore and Exploit in POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e58cc5ca94270acaceed13bc82dfedf7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 424523,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9433664242179689848&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ee76e7e736",
        "title": "Learning to Hash with Binary Reconstructive Embeddings",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6602294be910b1e3c4571bd98c4d5484-Abstract.html",
        "author": "Brian Kulis; Trevor Darrell",
        "abstract": "Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.  In this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the Hamming distances of the corresponding binary embeddings.  We develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings.  Unlike existing methods such as semantic hashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data.  We present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques.",
        "bibtex": "@inproceedings{NIPS2009_6602294b,\n author = {Kulis, Brian and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Hash with Binary Reconstructive Embeddings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6602294be910b1e3c4571bd98c4d5484-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 141049,
        "gs_citation": 1047,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5701737475834151642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "UCBerkeley EECS and ICSI; UCBerkeley EECS and ICSI",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ca0fbc01b4",
        "title": "Learning to Rank by Optimizing NDCG Measure",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/b3967a0e938dc2a6340e258630febd5a-Abstract.html",
        "author": "Hamed Valizadegan; Rong Jin; Ruofei Zhang; Jianchang Mao",
        "abstract": "Learning to rank is a relatively new field of study, aiming to learn a ranking function from a set of training data with relevancy labels. The ranking algorithms are often evaluated using Information Retrieval measures, such as Normalized Discounted Cumulative Gain [1] and Mean Average Precision [2]. Until recently, most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures. The main difficulty in direct optimization of these measures is that they depend on the ranks of documents, not the numerical values output by the ranking function. We propose a probabilistic framework that addresses this challenge by optimizing the expectation of NDCG over all the possible permutations of documents. A relaxation strategy is used to approximate the average of NDCG over the space of permutation, and a bound optimization approach is proposed to make the computation efficient. Extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets.",
        "bibtex": "@inproceedings{NIPS2009_b3967a0e,\n author = {Valizadegan, Hamed and Jin, Rong and Zhang, Ruofei and Mao, Jianchang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Rank by Optimizing NDCG Measure},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/b3967a0e938dc2a6340e258630febd5a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 226498,
        "gs_citation": 286,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7374590158466403697&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science and Engineering, Michigan State University; Computer Science and Engineering, Michigan State University; Advertising Sciences, Yahoo! Labs; Advertising Sciences, Yahoo! Labs",
        "aff_domain": "cse.msu.edu;cse.msu.edu;yahoo-inc.com;yahoo-inc.com",
        "email": "cse.msu.edu;cse.msu.edu;yahoo-inc.com;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Michigan State University;Yahoo! Labs",
        "aff_unique_dep": "Computer Science and Engineering;Advertising Sciences",
        "aff_unique_url": "https://www.msu.edu;https://labs.yahoo.com",
        "aff_unique_abbr": "MSU;Yahoo! Labs",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "East Lansing;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "58cb486994",
        "title": "Learning transport operators for image manifolds",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a1d50185e7426cbb0acad1e6ca74b9aa-Abstract.html",
        "author": "Benjamin Culpepper; Bruno A. Olshausen",
        "abstract": "We describe a method for learning a group of continuous transformation operators  to traverse smooth nonlinear manifolds. The method is applied to model how  natural images change over time and scale. The group of continuous transform  operators is represented by a basis that is adapted to the statistics of the data so  that the in\ufb01nitesimal generator for a measurement orbit can be produced by a  linear combination of a few basis elements. We illustrate how the method can be  used to ef\ufb01ciently code time-varying images by describing changes across time  and scale in terms of the learned operators.",
        "bibtex": "@inproceedings{NIPS2009_a1d50185,\n author = {Culpepper, Benjamin and Olshausen, Bruno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning transport operators for image manifolds},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/a1d50185e7426cbb0acad1e6ca74b9aa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a1d50185e7426cbb0acad1e6ca74b9aa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1120674,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18358931698604263102&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of EECS, Computer Science Division, University of California, Berkeley; Helen Wills Neuroscience Institute & School of Optometry, University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;berkeley.edu",
        "email": "cs.berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of EECS, Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d1ca3860f9",
        "title": "Learning with Compressible Priors",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/5c936263f3428a40227908d5a3847c0b-Abstract.html",
        "author": "Volkan Cevher",
        "abstract": "We describe probability distributions, dubbed compressible priors, whose independent and identically distributed (iid) realizations result in compressible signals. A signal is compressible when sorted magnitudes of its coefficients exhibit a power-law decay so that the signal can be well-approximated by a sparse signal. Since compressible signals live close to sparse signals, their intrinsic information can be stably embedded via simple non-adaptive linear projections into a much lower dimensional space whose dimension grows logarithmically with the ambient signal dimension. By using order statistics, we show that N-sample iid realizations of generalized Pareto, Student\u2019s t, log-normal, Frechet, and log-logistic distributions are compressible, i.e., they have a constant expected decay rate, which is independent of N. In contrast, we show that generalized Gaussian distribution with shape parameter q is compressible only in restricted cases since the expected decay rate of its N-sample iid realizations decreases with N as 1/[q log(N/q)]. We use compressible priors as a scaffold to build new iterative sparse signal recovery algorithms based on Bayesian inference arguments. We show how tuning of these algorithms explicitly depends on the parameters of the compressible prior of the signal, and how to learn the parameters of the signal\u2019s compressible prior on the fly during recovery.",
        "bibtex": "@inproceedings{NIPS2009_5c936263,\n author = {Cevher, Volkan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Compressible Priors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/5c936263f3428a40227908d5a3847c0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 346713,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17812733061939769834&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Rice University",
        "aff_domain": "rice.edu",
        "email": "rice.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Rice University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rice.edu",
        "aff_unique_abbr": "Rice",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3f069271c2",
        "title": "Linear-time Algorithms for Pairwise Statistical Problems",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2421fcb1263b9530df88f7f002e78ea5-Abstract.html",
        "author": "Parikshit Ram; Dongryeol Lee; William March; Alexander G. Gray",
        "abstract": "Several key computational bottlenecks in machine learning involve pairwise distance computations, including all-nearest-neighbors (finding the nearest neighbor(s) for each point, e.g.  in manifold learning) and kernel summations (e.g. in kernel density estimation or kernel machines).  We consider the general, bichromatic case for these problems, in addition to the scientific problem of N-body potential calculation.  In this paper we show for the first time O(N) worst case runtimes for practical algorithms for these problems based on the cover tree data structure (Beygelzimer, Kakade, Langford, 2006).",
        "bibtex": "@inproceedings{NIPS2009_2421fcb1,\n author = {Ram, Parikshit and Lee, Dongryeol and March, William and Gray, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linear-time Algorithms for Pairwise Statistical Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2421fcb1263b9530df88f7f002e78ea5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2421fcb1263b9530df88f7f002e78ea5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2421fcb1263b9530df88f7f002e78ea5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101312,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9961736576912263181&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "239969f9c4",
        "title": "Linearly constrained Bayesian matrix factorization for blind source separation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/371bce7dc83817b7893bcdeed13799b5-Abstract.html",
        "author": "Mikkel Schmidt",
        "abstract": "We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efficient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of non-negative matrix factorization and factor analysis.  The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.",
        "bibtex": "@inproceedings{NIPS2009_371bce7d,\n author = {Schmidt, Mikkel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linearly constrained Bayesian matrix factorization for blind source separation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/371bce7dc83817b7893bcdeed13799b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 183033,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2494816984871877228&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Engineering, University of Cambridge",
        "aff_domain": "imm.dtu.dk",
        "email": "imm.dtu.dk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f247db7a03",
        "title": "Local Rules for Global MAP: When Do They Work ?",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html",
        "author": "Kyomin Jung; Pushmeet Kohli; Devavrat Shah",
        "abstract": "We consider the question of computing Maximum A Posteriori (MAP) assignment in an arbitrary pair-wise Markov Random Field (MRF). We present a randomized iterative algorithm based on simple local updates. The algorithm, starting with an arbitrary initial assignment, updates it in each iteration by first, picking a random node, then selecting an (appropriately chosen) random local neighborhood and optimizing over this local neighborhood. Somewhat surprisingly, we show that this algorithm finds a near optimal assignment within $2n\\ln n$ iterations on average and with high probability for {\\em any} $n$ node pair-wise MRF with {\\em geometry} (i.e. MRF graph with polynomial growth) with the approximation error depending on (in a reasonable manner) the geometric growth rate of the graph and the average radius of the local neighborhood -- this allows for a graceful tradeoff between the complexity of the algorithm and the approximation error. Through extensive simulations, we show that our algorithm finds extremely good approximate solutions for various kinds of MRFs with geometry.",
        "bibtex": "@inproceedings{NIPS2009_eecca5b6,\n author = {Jung, Kyomin and Kohli, Pushmeet and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Local Rules for Global MAP: When Do They Work ?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/eecca5b6365d9607ee5a9d336962c534-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 187230,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2448595148910961432&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "KAIST; Microsoft Research; MIT",
        "aff_domain": "kaist.edu;microsoft.com;mit.edu",
        "email": "kaist.edu;microsoft.com;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Microsoft;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.microsoft.com/en-us/research;https://web.mit.edu",
        "aff_unique_abbr": "KAIST;MSR;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "837af1fd99",
        "title": "Locality-sensitive binary codes from shift-invariant kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html",
        "author": "Maxim Raginsky; Svetlana Lazebnik",
        "abstract": "This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.",
        "bibtex": "@inproceedings{NIPS2009_a5e00132,\n author = {Raginsky, Maxim and Lazebnik, Svetlana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Locality-sensitive binary codes from shift-invariant kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a5e00132373a7031000fd987a3c9f87b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 400997,
        "gs_citation": 815,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15313563320251816300&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Duke University, Durham, NC 27708; UNC Chapel Hill, Chapel Hill, NC 27599",
        "aff_domain": "duke.edu;cs.unc.edu",
        "email": "duke.edu;cs.unc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Duke University;University of North Carolina at Chapel Hill",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.duke.edu;https://www.unc.edu",
        "aff_unique_abbr": "Duke;UNC Chapel Hill",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Durham;Chapel Hill",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d8f111bf3a",
        "title": "Localizing Bugs in Program Executions with Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f64eac11f2cd8f0efa196f8ad173178e-Abstract.html",
        "author": "Laura Dietz; Valentin Dallmeier; Andreas Zeller; Tobias Scheffer",
        "abstract": "We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the  Bernoulli graph model on data of the software projects AspectJ and Rhino.",
        "bibtex": "@inproceedings{NIPS2009_f64eac11,\n author = {Dietz, Laura and Dallmeier, Valentin and Zeller, Andreas and Scheffer, Tobias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Localizing Bugs in Program Executions with Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/f64eac11f2cd8f0efa196f8ad173178e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f64eac11f2cd8f0efa196f8ad173178e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 385831,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11507554441366923165&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Max-Planck Institute for Computer Science, Saarbruecken, Germany; Saarland University, Saarbruecken, Germany; Saarland University, Saarbruecken, Germany; Potsdam University, Potsdam, Germany",
        "aff_domain": "mpi-inf.mpg.de;cs.uni-saarland.de;cs.uni-saarland.de;cs.uni-potsdam.de",
        "email": "mpi-inf.mpg.de;cs.uni-saarland.de;cs.uni-saarland.de;cs.uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Max-Planck Institute for Computer Science;Saarland University;Potsdam University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://mpi-sws.org;https://www.uni-saarland.de;https://www.uni-potsdam.de",
        "aff_unique_abbr": "MPI-SWS;UdS;",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Saarbruecken;Potsdam",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "f7670ac0bd",
        "title": "Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/b1563a78ec59337587f6ab6397699afc-Abstract.html",
        "author": "Garvesh Raskutti; Bin Yu; Martin J. Wainwright",
        "abstract": "This paper uses information-theoretic techniques to determine minimax rates for estimating nonparametric sparse additive regression models under high-dimensional scaling.  We assume an additive decomposition of the form $f^*(X_1, \\ldots, X_p) = \\sum_{j \\in S} h_j(X_j)$, where each component function $h_j$ lies in some Hilbert Space $\\Hilb$ and $S \\subset \\{1, \\ldots, \\pdim \\}$ is an unknown subset with cardinality $\\s = |S$.  Given $\\numobs$ i.i.d. observations of $f^*(X)$ corrupted with white Gaussian noise where the covariate vectors $(X_1, X_2, X_3,...,X_{\\pdim})$ are drawn with i.i.d. components from some distribution $\\mP$, we determine tight lower bounds on the minimax rate for estimating the regression function with respect to squared $\\LTP$ error. The main result shows that the minimax rates are $\\max{\\big(\\frac{\\s \\log \\pdim / \\s}{n}, \\LowerRateSq \\big)}$.  The first term reflects the difficulty of performing \\emph{subset selection} and is independent of the Hilbert space $\\Hilb$; the second term $\\LowerRateSq$ is an \\emph{\\s-dimensional estimation} term, depending only on the low dimension $\\s$ but not the ambient dimension $\\pdim$, that captures the difficulty of estimating a sum of $\\s$ univariate functions in the Hilbert space $\\Hilb$.  As a special case, if $\\Hilb$ corresponds to the $\\m$-th order Sobolev space $\\SobM$ of functions that are $m$-times differentiable, the $\\s$-dimensional estimation term takes the form $\\LowerRateSq \\asymp \\s \\; n^{-2\\m/(2\\m+1)}$. The minimax rates are compared with rates achieved by an $\\ell_1$-penalty based approach, it can be shown that a certain $\\ell_1$-based approach achieves the minimax optimal rate.",
        "bibtex": "@inproceedings{NIPS2009_b1563a78,\n author = {Raskutti, Garvesh and Yu, Bin and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/b1563a78ec59337587f6ab6397699afc-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/b1563a78ec59337587f6ab6397699afc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/b1563a78ec59337587f6ab6397699afc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120458,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1652088649762755986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c423532be7",
        "title": "Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a2557a7b2e94197ff767970b67041697-Abstract.html",
        "author": "Keith Bush; Joelle Pineau",
        "abstract": "Interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by first principles, and are only partially observable. If partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning.  We experiment with manifold embeddings as the reconstructed observable state-space of an off-line, model-based reinforcement learning approach to control. We demonstrate the embedding of a system changes as a result of learning and that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system.  We apply this approach in simulation to learn a neurostimulation policy that is more efficient in treating epilepsy than conventional policies.  We then demonstrate the learned policy completely suppressing seizures in real-world neurostimulation experiments on actual animal brain slices.",
        "bibtex": "@inproceedings{NIPS2009_a2557a7b,\n author = {Bush, Keith and Pineau, Joelle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a2557a7b2e94197ff767970b67041697-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a2557a7b2e94197ff767970b67041697-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a2557a7b2e94197ff767970b67041697-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2230590,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6967315458583921132&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "63b8c8f6ff",
        "title": "Manifold Regularization for SIR with Rate Root-n Convergence",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a516a87cfcaef229b342c437fe2b95f7-Abstract.html",
        "author": "Wei Bian; Dacheng Tao",
        "abstract": "In this paper, we study the manifold regularization for the Sliced Inverse Regression (SIR). The manifold regularization improves the standard SIR in two aspects: 1) it encodes the local geometry for SIR and 2) it enables SIR to deal with transductive and semi-supervised learning problems. We prove that the proposed graph Laplacian based regularization is convergent at rate root-n. The projection directions of the regularized SIR are optimized by using a conjugate gradient method on the Grassmann manifold. Experimental results support our theory.",
        "bibtex": "@inproceedings{NIPS2009_a516a87c,\n author = {Bian, Wei and Tao, Dacheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Manifold Regularization for SIR with Rate Root-n Convergence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a516a87cfcaef229b342c437fe2b95f7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 293566,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2716277909062913115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Engineering, Nanyang Technological University, Singapore, 639798; School of Computer Engineering, Nanyang Technological University, Singapore, 639798",
        "aff_domain": "pmail.ntu.edu.sg;ntu.edu.sg",
        "email": "pmail.ntu.edu.sg;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Computer Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "8e88e5af5e",
        "title": "Matrix Completion from Noisy Entries",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/aa942ab2bfa6ebda4840e7360ce6e7ef-Abstract.html",
        "author": "Raghunandan Keshavan; Andrea Montanari; Sewoong Oh",
        "abstract": "Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the \u2018Netflix problem\u2019) to structure-from-motion and positioning. We study a low complexity algorithm introduced in [1], based on a combination of spectral techniques and manifold optimization, that we call here OPTSPACE. We prove performance guarantees that are order-optimal in a number of circumstances.",
        "bibtex": "@inproceedings{NIPS2009_aa942ab2,\n author = {Keshavan, Raghunandan and Montanari, Andrea and Oh, Sewoong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Matrix Completion from Noisy Entries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 103434,
        "gs_citation": 896,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17715290248652110327&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "107c55995c",
        "title": "Matrix Completion from Power-Law Distributed Samples",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1fc214004c9481e4c8073e85323bfd4b-Abstract.html",
        "author": "Raghu Meka; Prateek Jain; Inderjit S. Dhillon",
        "abstract": "The low-rank matrix completion problem is a fundamental problem with many important applications. Recently, Candes & Recht, Keshavan et al. and Candes & Tao obtained the first non-trivial theoretical results for the problem assuming that the observed entries are sampled uniformly at random. Unfortunately, most real-world datasets do not satisfy this assumption, but instead exhibit power-law distributed samples. In this paper, we propose a graph theoretic approach to matrix completion that solves the problem for more realistic sampling models. Our method is easier to analyze than previous methods with the analysis reducing to computing the threshold for complete cascades in random graphs, a problem of independent interest. By analyzing the graph theoretic problem, we show that our method achieves exact recovery when the observed entries are sampled from the Chung-Lu-Vu model, which can generate power-law distributed graphs. We also hypothesize that our algorithm solves the matrix completion problem from an optimal number of entries for the popular preferential attachment model and provide strong empirical evidence for the claim. Furthermore, our method is easier to implement and is substantially faster than existing methods. We demonstrate the effectiveness of our method on examples when the low-rank matrix is sampled according to the prevalent random graph models for complex networks and also on the Netflix challenge dataset.",
        "bibtex": "@inproceedings{NIPS2009_1fc21400,\n author = {Meka, Raghu and Jain, Prateek and Dhillon, Inderjit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Matrix Completion from Power-Law Distributed Samples},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1fc214004c9481e4c8073e85323bfd4b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1fc214004c9481e4c8073e85323bfd4b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1fc214004c9481e4c8073e85323bfd4b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1184832,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15298903695353596527&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Sciences, University of Texas at Austin; Department of Computer Sciences, University of Texas at Austin; Department of Computer Sciences, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Sciences",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "69ef16e2ee",
        "title": "Maximin affinity learning of image segmentation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/68d30a9594728bc39aa24be94b319d21-Abstract.html",
        "author": "Kevin Briggman; Winfried Denk; Sebastian Seung; Moritz N. Helmstaedter; Srinivas C. Turaga",
        "abstract": "Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure. The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.",
        "bibtex": "@inproceedings{NIPS2009_68d30a95,\n author = {Briggman, Kevin and Denk, Winfried and Seung, Sebastian and Helmstaedter, Moritz and Turaga, Srinivas C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maximin affinity learning of image segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/68d30a9594728bc39aa24be94b319d21-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/68d30a9594728bc39aa24be94b319d21-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/68d30a9594728bc39aa24be94b319d21-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 792058,
        "gs_citation": 166,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7858407743072258426&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "MIT; Max-Planck Insitute for Medical Research; Max-Planck Insitute for Medical Research; Max-Planck Insitute for Medical Research; MIT+HHMI",
        "aff_domain": "mit.edu; ; ; ; ",
        "email": "mit.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0+2",
        "aff_unique_norm": "Massachusetts Institute of Technology;Max Planck Institute for Medical Research;HHMI",
        "aff_unique_dep": ";Medical Research;",
        "aff_unique_url": "https://web.mit.edu;https://www.mpimr.de;https://www.hhmi.org",
        "aff_unique_abbr": "MIT;MPI-Medical Research;HHMI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0+0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "bdb7b1ea9a",
        "title": "Maximum likelihood trajectories for continuous-time Markov chains",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/afda332245e2af431fb7b672a68b659d-Abstract.html",
        "author": "Theodore J. Perkins",
        "abstract": "Continuous-time Markov chains are used to model systems in which transitions between states as well as the time the system spends in each state are random.  Many computational problems related to such chains have been solved, including determining state distributions as a function of time, parameter estimation, and control.  However, the problem of inferring most likely trajectories, where a trajectory is a sequence of states as well as the amount of time spent in each state, appears unsolved.  We study three versions of this problem: (i) an initial value problem, in which an initial state is given and we seek the most likely trajectory until a given final time, (ii) a boundary value problem, in which initial and final states and times are given, and we seek the most likely trajectory connecting them, and (iii) trajectory inference under partial observability, analogous to finding maximum likelihood trajectories for hidden Markov models.  We show that maximum likelihood trajectories are not always well-defined, and describe a polynomial time test for well-definedness.  When well-definedness holds, we show that each of the three problems can be solved in polynomial time, and we develop efficient dynamic programming algorithms for doing so.",
        "bibtex": "@inproceedings{NIPS2009_afda3322,\n author = {Perkins, Theodore},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maximum likelihood trajectories for continuous-time Markov chains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/afda332245e2af431fb7b672a68b659d-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/afda332245e2af431fb7b672a68b659d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/afda332245e2af431fb7b672a68b659d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 225461,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5057050737366694159&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Ottawa Hospital Research Institute, Ottawa, Ontario, Canada",
        "aff_domain": "ohri.ca",
        "email": "ohri.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ottawa Hospital Research Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ohri.ca",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ottawa",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "6d3ed92e66",
        "title": "Measuring Invariances in Deep Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/428fca9bc1921c25c5121f9da7815cde-Abstract.html",
        "author": "Ian Goodfellow; Honglak Lee; Quoc V. Le; Andrew Saxe; Andrew Y. Ng",
        "abstract": "For many computer vision applications, the ideal image feature would be invariant to multiple confounding image properties, such as illumination and viewing angle. Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, outside of using these learning algorithms in a classi\ufb01er, they can be sometimes dif\ufb01cult to evaluate. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different image transforms. We \ufb01nd that deep autoencoders become invariant to increasingly complex image transformations with depth. This further justi\ufb01es the use of \u201cdeep\u201d vs. \u201cshallower\u201d representations. Our performance metrics agree with existing measures of invariance. Our evaluation metrics can also be used to evaluate future work in unsupervised deep learning, and thus help the development of future algorithms.",
        "bibtex": "@inproceedings{NIPS2009_428fca9b,\n author = {Goodfellow, Ian and Lee, Honglak and Le, Quoc and Saxe, Andrew and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Measuring Invariances in Deep Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/428fca9bc1921c25c5121f9da7815cde-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/428fca9bc1921c25c5121f9da7815cde-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/428fca9bc1921c25c5121f9da7815cde-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 129235,
        "gs_citation": 619,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10810592820351129263&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0737b2a78d",
        "title": "Measuring model complexity with the prior predictive",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f7cade80b7cc92b991cf4d2806d6bd78-Abstract.html",
        "author": "Wolf Vanpaemel",
        "abstract": "In the last few decades, model complexity has received a lot of press. While many methods have been proposed that jointly measure a model\u2019s descriptive adequacy and its complexity, few measures exist that measure complexity in itself. Moreover, existing measures ignore the parameter prior, which is an inherent part of the model and affects the complexity. This paper presents a stand alone measure for model complexity, that takes the number of parameters, the functional form, the range of the parameters and the parameter prior into account. This Prior Predictive Complexity (PPC) is an intuitive and easy to compute measure. It starts from the observation that model complexity is the property of the model that enables it to fit a wide range of outcomes. The PPC then measures how wide this range exactly is.",
        "bibtex": "@inproceedings{NIPS2009_f7cade80,\n author = {Vanpaemel, Wolf},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Measuring model complexity with the prior predictive},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f7cade80b7cc92b991cf4d2806d6bd78-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 183996,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9769890415553430484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Psychology, University of Leuven, Belgium",
        "aff_domain": "psy.kuleuven.be",
        "email": "psy.kuleuven.be",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Leuven",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.kuleuven.be",
        "aff_unique_abbr": "KU Leuven",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "754686a5ef",
        "title": "Modeling Social Annotation Data with Content Relevance using a Topic Model",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/39059724f73a9969845dfe4146c5660e-Abstract.html",
        "author": "Tomoharu Iwata; Takeshi Yamada; Naonori Ueda",
        "abstract": "We propose a probabilistic topic model for analyzing and extracting content-related annotations from noisy annotated discrete data such as web pages stored in social bookmarking services. In these services, since users can attach annotations freely, some annotations do not describe the semantics of the content, thus they are noisy,  i.e.  not content-related. The extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classification and image recognition, or can improve information retrieval performance. The proposed model is a generative model for content and annotations, in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content. We demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.",
        "bibtex": "@inproceedings{NIPS2009_39059724,\n author = {Iwata, Tomoharu and Yamada, Takeshi and Ueda, Naonori},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modeling Social Annotation Data with Content Relevance using a Topic Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/39059724f73a9969845dfe4146c5660e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/39059724f73a9969845dfe4146c5660e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/39059724f73a9969845dfe4146c5660e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131067,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13844966905516712907&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e353459759",
        "title": "Modeling the spacing effect in sequential category learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1e48c4420b7073bc11916c6c1de226bb-Abstract.html",
        "author": "Hongjing Lu; Matthew Weiden; Alan L. Yuille",
        "abstract": "We develop a Bayesian sequential model for category learning. The sequential model updates two category parameters, the mean and the variance, over time. We define conjugate temporal priors to enable closed form solutions to be obtained. This model can be easily extended to supervised and unsupervised learning involving multiple categories. To model the spacing effect, we introduce a generic prior in the temporal updating stage to capture a learning preference, namely, less change for repetition and more change for variation. Finally, we show how this approach can be generalized to efficiently performmodel selection to decide whether observations are from one or multiple categories.",
        "bibtex": "@inproceedings{NIPS2009_1e48c442,\n author = {Lu, Hongjing and Weiden, Matthew and Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modeling the spacing effect in sequential category learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1e48c4420b7073bc11916c6c1de226bb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 158419,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10157758584828510870&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Psychology & Statistics; Department of Psychology; Department of Statistics, Computer Science & Psychology, University of California, Los Angeles",
        "aff_domain": "ucla.edu;ucla.edu;stat.ucla.edu",
        "email": "ucla.edu;ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University Affiliation Not Specified;University of California, Los Angeles",
        "aff_unique_dep": "Department of Psychology & Statistics;Department of Statistics, Computer Science & Psychology",
        "aff_unique_url": ";https://www.ucla.edu",
        "aff_unique_abbr": ";UCLA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "d6e5842d2b",
        "title": "Modelling Relational Data using Bayesian Clustered Tensor Factorization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/5705e1164a8394aace6018e27d20d237-Abstract.html",
        "author": "Ilya Sutskever; Joshua B. Tenenbaum; Ruslan Salakhutdinov",
        "abstract": "We consider the problem of learning probabilistic models for complex relational structures between various types of objects.  A model can help us ``understand a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true.  Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have better predictive performance on large data sets.  We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework.  Inference is fully Bayesian but scales well to large data sets.  The model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data.",
        "bibtex": "@inproceedings{NIPS2009_5705e116,\n author = {Sutskever, Ilya and Tenenbaum, Joshua and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modelling Relational Data using Bayesian Clustered Tensor Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/5705e1164a8394aace6018e27d20d237-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/5705e1164a8394aace6018e27d20d237-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/5705e1164a8394aace6018e27d20d237-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2542685,
        "gs_citation": 312,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5656485262033401501&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "University of Toronto; MIT; MIT",
        "aff_domain": "cs.utoronto.ca;mit.edu;mit.edu",
        "email": "cs.utoronto.ca;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Toronto;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utoronto.ca;https://web.mit.edu",
        "aff_unique_abbr": "U of T;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "6eac3588b8",
        "title": "Monte Carlo Sampling for Regret Minimization in Extensive Games",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html",
        "author": "Marc Lanctot; Kevin Waugh; Martin Zinkevich; Michael Bowling",
        "abstract": "Sequential decision-making with multiple agents and imperfect information is commonly modeled as an extensive game.  One efficient method for computing Nash equilibria in large, zero-sum, imperfect information games is counterfactual regret minimization (CFR). In the domain of poker, CFR has proven effective, particularly when using a domain-specific augmentation involving chance outcome sampling.  In this paper, we describe a general family of domain independent CFR sample-based algorithms called Monte Carlo counterfactual regret minimization (MCCFR) of which the original and poker-specific versions are special cases. We start by showing that MCCFR performs the same regret updates as CFR on expectation. Then, we introduce two sampling schemes: {\\it outcome sampling} and {\\it external sampling}, showing that both have bounded overall regret with high  probability. Thus, they can compute an approximate equilibrium using self-play. Finally, we prove a new tighter bound on the regret for the original CFR algorithm and relate this new bound to MCCFRs bounds. We show empirically that, although the sample-based algorithms require more iterations, their lower cost per iteration can lead to dramatically faster convergence in various games.",
        "bibtex": "@inproceedings{NIPS2009_00411460,\n author = {Lanctot, Marc and Waugh, Kevin and Zinkevich, Martin and Bowling, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Monte Carlo Sampling for Regret Minimization in Extensive Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/00411460f7c92d2124a67ea0f4cb5f85-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/00411460f7c92d2124a67ea0f4cb5f85-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 222840,
        "gs_citation": 423,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15610162481684970513&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computing Science, University of Alberta; School of Computer Science, Carnegie Mellon University; Yahoo! Research; Department of Computing Science, University of Alberta",
        "aff_domain": "ualberta.ca;cs.cmu.edu;yahoo-inc.com;cs.ualberta.ca",
        "email": "ualberta.ca;cs.cmu.edu;yahoo-inc.com;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Alberta;Carnegie Mellon University;Yahoo!",
        "aff_unique_dep": "Department of Computing Science;School of Computer Science;Yahoo! Research",
        "aff_unique_url": "https://www.ualberta.ca;https://www.cmu.edu;https://research.yahoo.com",
        "aff_unique_abbr": "UAlberta;CMU;Yahoo!",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "e9446d12af",
        "title": "Multi-Label Prediction via Compressed Sensing",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/67974233917cea0e42a49a2fb7eb4cf4-Abstract.html",
        "author": "Daniel J. Hsu; Sham M. Kakade; John Langford; Tong Zhang",
        "abstract": "We consider multi-label prediction problems with large output spaces under the assumption of output sparsity \u2013 that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subprob- lems need only be logarithmic in the total number of possible labels, making this approach radically more ef\ufb01cient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.",
        "bibtex": "@inproceedings{NIPS2009_67974233,\n author = {Hsu, Daniel J and Kakade, Sham M and Langford, John and Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Label Prediction via Compressed Sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 86412,
        "gs_citation": 534,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4209140439188757783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 26,
        "aff": "U C San Diego; TTI-Chicago; Yahoo! Research; Rutgers University",
        "aff_domain": "cs.ucsd.edu;tti-c.org;hunch.net;rci.rutgers.edu",
        "email": "cs.ucsd.edu;tti-c.org;hunch.net;rci.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "University of California, San Diego;Toyota Technological Institute at Chicago;Yahoo!;Rutgers University",
        "aff_unique_dep": ";;Yahoo! Research;",
        "aff_unique_url": "https://ucsd.edu;https://www.tti-chicago.org;https://research.yahoo.com;https://www.rutgers.edu",
        "aff_unique_abbr": "UCSD;TTI;Yahoo!;Rutgers",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "San Diego;Chicago;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9dce1b3a12",
        "title": "Multi-Label Prediction via Sparse Infinite CCA",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html",
        "author": "Piyush Rai; Hal Daume",
        "abstract": "Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efficacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction.",
        "bibtex": "@inproceedings{NIPS2009_09c6c378,\n author = {Rai, Piyush and Daume, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Label Prediction via Sparse Infinite CCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/09c6c3783b4a70054da74f2538ed47c6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 135417,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15732565348387292258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computing, University of Utah; School of Computing, University of Utah",
        "aff_domain": "cs.utah.edu;cs.utah.edu",
        "email": "cs.utah.edu;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Utah",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.utah.edu",
        "aff_unique_abbr": "U of U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Utah",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9fc04b14d7",
        "title": "Multi-Step Dyna Planning for Policy Evaluation and Control",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/c52f1bd66cc19d05628bd8bf27af3ad6-Abstract.html",
        "author": "Hengshuai Yao; Shalabh Bhatnagar; Dongcui Diao; Richard S. Sutton; Csaba Szepesv\u00e1ri",
        "abstract": "We extend Dyna planning architecture for policy evaluation and control in two significant aspects. First, we introduce a multi-step Dyna planning that projects the simulated state/feature many steps into the future. Our multi-step Dyna is based on a multi-step model, which we call the {\\em $\\lambda$-model}. The $\\lambda$-model interpolates between the one-step model and an infinite-step model, and can be learned efficiently online. Second, we use for Dyna control a dynamic multi-step model that is able to predict the results of a sequence of greedy actions and track the optimal policy in the long run. Experimental results show that Dyna using the multi-step model evaluates a policy faster than using single-step models; Dyna control algorithms using the dynamic tracking model are much faster than model-free algorithms; further, multi-step Dyna control algorithms enable the policy and value function to converge much faster to their optima than single-step Dyna algorithms.",
        "bibtex": "@inproceedings{NIPS2009_c52f1bd6,\n author = {Yao, Hengshuai and Bhatnagar, Shalabh and Diao, Dongcui and Sutton, Richard S and Szepesv\\'{a}ri, Csaba},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Step Dyna Planning for Policy Evaluation and Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/c52f1bd66cc19d05628bd8bf27af3ad6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/c52f1bd66cc19d05628bd8bf27af3ad6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 106609,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=774167389946012846&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a4d8a32514",
        "title": "Multiple Incremental Decremental Learning of Support Vector Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e6b4b2a746ed40e1af829d1fa82daa10-Abstract.html",
        "author": "Masayuki Karasuyama; Ichiro Takeuchi",
        "abstract": "We propose a multiple incremental decremental algorithm  of Support Vector Machine (SVM). Conventional single  cremental decremental SVM can update the trained model  efficiently when single data point is added to or removed  from the training set. When we add and/or remove multiple  data points, this algorithm is time-consuming because we  need to repeatedly apply it to each data point. The roposed  algorithm is computationally more efficient when multiple  data points are added and/or removed simultaneously. The  single incremental decremental algorithm is built on an  optimization technique called parametric programming.  We extend the idea and introduce multi-parametric  programming for developing the proposed algorithm.  Experimental results on synthetic and real data sets indicate that the proposed algorithm can significantly  reduce the computational cost of multiple incremental  decremental operation. Our approach is especially useful  for online SVM learning in which we need to remove old  data points and add new data points in a short amount of  time.",
        "bibtex": "@inproceedings{NIPS2009_e6b4b2a7,\n author = {Karasuyama, Masayuki and Takeuchi, Ichiro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiple Incremental Decremental Learning of Support Vector Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e6b4b2a746ed40e1af829d1fa82daa10-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 251909,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1682789196923899625&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Engineering, Nagoya Institute of Technology; Department of Engineering, Nagoya Institute of Technology",
        "aff_domain": "ics.nitech.ac.jp;nitech.ac.jp",
        "email": "ics.nitech.ac.jp;nitech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nagoya Institute of Technology",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.nitech.ac.jp",
        "aff_unique_abbr": "NIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "fd93b10fd3",
        "title": "Nash Equilibria of Static Prediction Games",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
        "author": "Michael Br\u00fcckner; Tobias Scheffer",
        "abstract": "The standard assumption of identically distributed training and test data can be violated when an adversary can exercise some control over the generation of the test data. In a prediction game, a learner produces a predictive model while an adversary may alter the distribution of input data. We study single-shot prediction games in which the cost functions of learner and adversary are not necessarily antagonistic. We identify conditions under which the prediction game has a unique Nash equilibrium, and derive algorithms that will find the equilibrial prediction models. In a case study, we explore properties of Nash-equilibrial prediction models for email spam filtering empirically.",
        "bibtex": "@inproceedings{NIPS2009_c399862d,\n author = {Br\\\"{u}ckner, Michael and Scheffer, Tobias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nash Equilibria of Static Prediction Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/c399862d3b9d6b76c8436e924a68c45b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/c399862d3b9d6b76c8436e924a68c45b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 273623,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11289130503611107824&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Potsdam, Germany; University of Potsdam, Germany",
        "aff_domain": "cs.uni-potsdam.de;cs.uni-potsdam.de",
        "email": "cs.uni-potsdam.de;cs.uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Potsdam",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-potsdam.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "743f8ce1b3",
        "title": "Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/754dda4b1ba34c6fa89716b85d68532b-Abstract.html",
        "author": "Lei Shi; Thomas L. Griffiths",
        "abstract": "The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which fire at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and oblique effect.",
        "bibtex": "@inproceedings{NIPS2009_754dda4b,\n author = {Shi, Lei and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/754dda4b1ba34c6fa89716b85d68532b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/754dda4b1ba34c6fa89716b85d68532b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/754dda4b1ba34c6fa89716b85d68532b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1660302,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10830752926736768032&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Helen Wills Neuroscience Institute, University of California, Berkeley; Department of Psychology, University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Helen Wills Neuroscience Institute",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b823da7a7d",
        "title": "Neurometric function analysis of population codes",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/03c6b06952c750899bb03d998e631860-Abstract.html",
        "author": "Philipp Berens; Sebastian Gerwinn; Alexander Ecker; Matthias Bethge",
        "abstract": "The relative merits of different population coding schemes have mostly been analyzed in the framework of stimulus reconstruction using Fisher Information. Here, we consider the case of stimulus discrimination in a two alternative forced choice paradigm and compute neurometric functions in terms of the minimal discrimination error and the Jensen-Shannon information to study neural population codes. We first explore the relationship between minimum discrimination error, Jensen-Shannon Information and Fisher Information and show that the discrimination framework is more informative about the coding accuracy than Fisher Information as it defines an error for any pair of possible stimuli. In particular, it includes Fisher Information as a special case. Second, we use the framework to study population codes of angular variables. Specifically, we assess the impact of different noise correlations structures on coding accuracy in long versus short decoding time windows. That is, for long time window we use the common Gaussian noise approximation. To address the case of short time windows we analyze the Ising model with identical noise correlation structure. In this way, we provide a new rigorous framework for assessing the functional consequences of noise correlation structures for the representational accuracy of neural population codes that is in particular applicable to short-time population coding.",
        "bibtex": "@inproceedings{NIPS2009_03c6b069,\n author = {Berens, Philipp and Gerwinn, Sebastian and Ecker, Alexander and Bethge, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neurometric function analysis of population codes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/03c6b06952c750899bb03d998e631860-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/03c6b06952c750899bb03d998e631860-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/03c6b06952c750899bb03d998e631860-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 240337,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15932425512781875565&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Biological Cybernetics",
        "aff_unique_url": "https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "68d91b71e8",
        "title": "No evidence for active sparsification in the visual cortex",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html",
        "author": "Pietro Berkes; Ben White; Jozsef Fiser",
        "abstract": "The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development, and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.",
        "bibtex": "@inproceedings{NIPS2009_2b24d495,\n author = {Berkes, Pietro and White, Ben and Fiser, Jozsef},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {No evidence for active sparsification in the visual cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/2b24d495052a8ce66358eb576b8912c8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2b24d495052a8ce66358eb576b8912c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 344417,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1063756417917210921&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "78ce19eed8",
        "title": "Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8e296a067a37563370ded05f5a3bf3ec-Abstract.html",
        "author": "Zhi Yang; Qi Zhao; Edward Keefer; Wentai Liu",
        "abstract": "Studying signal and noise properties of recorded neural data is critical in developing more efficient algorithms to recover the encoded information. Important issues exist in this research including the variant spectrum spans of neural spikes that make it difficult to choose a global optimal bandpass filter. Also, multiple sources produce aggregated noise that deviates from the conventional white Gaussian noise. In this work, the spectrum variability of spikes is addressed, based on which the concept of adaptive bandpass filter that fits the spectrum of individual spikes is proposed. Multiple noise sources have been studied through analytical models as well as empirical measurements. The dominant noise source is identified as neuron noise followed by interface noise of the electrode. This suggests that major efforts to reduce noise from electronics are not well spent. The measured noise from in vivo experiments shows a family of 1/f^{x} (x=1.5\\pm 0.5) spectrum that can be reduced using noise shaping techniques. In summary, the methods of adaptive bandpass filtering and noise shaping together result in several dB signal-to-noise ratio (SNR) enhancement.",
        "bibtex": "@inproceedings{NIPS2009_8e296a06,\n author = {Yang, Zhi and Zhao, Qi and Keefer, Edward and Liu, Wentai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8e296a067a37563370ded05f5a3bf3ec-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8e296a067a37563370ded05f5a3bf3ec-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/8e296a067a37563370ded05f5a3bf3ec-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8e296a067a37563370ded05f5a3bf3ec-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3770957,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4875959211312407255&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of California at Santa Cruz; California Institute of Technology; UT Southwestern Medical Center + Plexon Inc; University of California at Santa Cruz",
        "aff_domain": "soe.ucsc.edu; ; ; ",
        "email": "soe.ucsc.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3;0",
        "aff_unique_norm": "University of California, Santa Cruz;California Institute of Technology;University of Texas Southwestern Medical Center;Plexon Inc",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ucsc.edu;https://www.caltech.edu;https://www.utsouthwestern.edu;https://www.plexon.com",
        "aff_unique_abbr": "UCSC;Caltech;UT Southwestern;",
        "aff_campus_unique_index": "0;1;;0",
        "aff_campus_unique": "Santa Cruz;Pasadena;",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b35f88c526",
        "title": "Noisy Generalized Binary Search",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/556f391937dfd4398cbac35e050a2177-Abstract.html",
        "author": "Robert Nowak",
        "abstract": "This paper addresses the problem of noisy Generalized Binary Search (GBS).  GBS is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries.  At each step, a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets, a natural generalization of the idea underlying classic binary search.  GBS is used in many applications, including fault testing, machine diagnostics, disease diagnosis, job scheduling, image processing, computer vision, and active learning. In most of these cases, the responses to queries can be noisy.  Past work has provided a partial characterization of GBS, but existing noise-tolerant versions of GBS are suboptimal in terms of sample complexity.  This paper presents the first optimal algorithm for noisy GBS and demonstrates its application to learning multidimensional threshold functions.",
        "bibtex": "@inproceedings{NIPS2009_556f3919,\n author = {Nowak, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Noisy Generalized Binary Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/556f391937dfd4398cbac35e050a2177-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/556f391937dfd4398cbac35e050a2177-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/556f391937dfd4398cbac35e050a2177-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 193083,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15347314547411783063&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Wisconsin-Madison",
        "aff_domain": "ece.wisc.edu",
        "email": "ece.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3ea74dc205",
        "title": "Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/cfecdb276f634854f3ef915e2e980c31-Abstract.html",
        "author": "Mingyuan Zhou; Haojun Chen; Lu Ren; Guillermo Sapiro; Lawrence Carin; John W. Paisley",
        "abstract": "Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non-stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.",
        "bibtex": "@inproceedings{NIPS2009_cfecdb27,\n author = {Zhou, Mingyuan and Chen, Haojun and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence and Paisley, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/cfecdb276f634854f3ef915e2e980c31-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/cfecdb276f634854f3ef915e2e980c31-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/cfecdb276f634854f3ef915e2e980c31-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 263941,
        "gs_citation": 333,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7157364698504834111&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University + Department of Electrical and Computer Engineering, University of Minnesota; Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University",
        "aff_domain": "ee.duke.edu;ee.duke.edu;ee.duke.edu;ee.duke.edu;umn.edu;ee.duke.edu",
        "email": "ee.duke.edu;ee.duke.edu;ee.duke.edu;ee.duke.edu;umn.edu;ee.duke.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1;0;0",
        "aff_unique_norm": "Duke University;University of Minnesota",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu;https://www.umn.edu",
        "aff_unique_abbr": "Duke;UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c8b45eaa88",
        "title": "Non-stationary continuous dynamic Bayesian networks",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0ff39bbbf981ac0151d340c9aa40e63e-Abstract.html",
        "author": "Marco Grzegorczyk; Dirk Husmeier",
        "abstract": "Dynamic Bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data. The standard approach is based on the assumption of a homogeneous Markov chain, which is not valid in many real-world scenarios. Recent research efforts addressing this shortcoming have considered undirected graphs, directed graphs for discretized data, or over-flexible models that lack any information sharing between time series segments. In the present article, we propose a non-stationary dynamic Bayesian network for continuous data, in which parameters are allowed to vary between segments, and in which a common network structure provides essential information sharing across segments.  Our model is based on a Bayesian change-point process, and we apply a variant of the allocation sampler of Nobile and Fearnside to infer the number and location of the change-points.",
        "bibtex": "@inproceedings{NIPS2009_0ff39bbb,\n author = {Grzegorczyk, Marco and Husmeier, Dirk},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-stationary continuous dynamic Bayesian networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/0ff39bbbf981ac0151d340c9aa40e63e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0ff39bbbf981ac0151d340c9aa40e63e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 172515,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17152237549420748249&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Statistics, TUDortmund University, 44221 Dortmund, Germany; Biomathematics & Statistics Scotland (BioSS), JCMB, The King\u2019s Buildings, Edinburgh EH93JZ, United Kingdom",
        "aff_domain": "statistik.tu-dortmund.de;bioss.ac.uk",
        "email": "statistik.tu-dortmund.de;bioss.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "TUDortmund University;Biomathematics & Statistics Scotland",
        "aff_unique_dep": "Department of Statistics;Biomathematics & Statistics",
        "aff_unique_url": "https://www.tu-dortmund.de;",
        "aff_unique_abbr": "TUD;BioSS",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Dortmund;Edinburgh",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "1d3b9c4909",
        "title": "Nonlinear Learning using Local Coordinate Coding",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2afe4567e1bf64d32a5527244d104cea-Abstract.html",
        "author": "Kai Yu; Tong Zhang; Yihong Gong",
        "abstract": "This paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds, which includes a phase of unsupervised basis learning and a phase of supervised function learning. The learned bases provide a set of anchor points to form a local coordinate system, such that each data point x on the manifold can be locally approximated by a linear combination of its nearby anchor points, and the linear weights become its local coordinate coding. We show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme, and the approximation quality is ensured by the locality of such coding. The method turns a difficult nonlinear learning problem into a simple global linear learning problem, which overcomes some drawbacks of traditional local learning methods.",
        "bibtex": "@inproceedings{NIPS2009_2afe4567,\n author = {Yu, Kai and Zhang, Tong and Gong, Yihong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonlinear Learning using Local Coordinate Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/2afe4567e1bf64d32a5527244d104cea-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2afe4567e1bf64d32a5527244d104cea-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1140545,
        "gs_citation": 958,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11115714309462468456&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "NEC Laboratories America; Rutgers University; NEC Laboratories America",
        "aff_domain": "sv.nec-labs.com;stat.rutgers.edu;sv.nec-labs.com",
        "email": "sv.nec-labs.com;stat.rutgers.edu;sv.nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "NEC Laboratories America;Rutgers University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nec-labs.com;https://www.rutgers.edu",
        "aff_unique_abbr": "NEC Labs America;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a687a635bd",
        "title": "Nonlinear directed acyclic structure learning with weakly additive noise models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html",
        "author": "Arthur Gretton; Peter Spirtes; Robert E. Tillman",
        "abstract": "The recently proposed \\emph{additive noise model} has advantages over previous structure learning algorithms, when attempting to recover some true data generating mechanism, since it (i) does not assume linearity or Gaussianity and (ii) can recover a unique DAG rather than an equivalence class. However, its original extension to the multivariate case required enumerating all possible DAGs, and for some special distributions, e.g. linear Gaussian, the model is invertible and thus cannot be used for structure learning. We present a new approach which combines a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the equivalence class. This results in a more computationally efficient approach that is useful for arbitrary distributions even when additive noise models are invertible. Experiments with synthetic and real data show that this method is more accurate than previous methods when data are nonlinear and/or non-Gaussian.",
        "bibtex": "@inproceedings{NIPS2009_83fa5a43,\n author = {Gretton, Arthur and Spirtes, Peter and Tillman, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonlinear directed acyclic structure learning with weakly additive noise models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/83fa5a432ae55c253d0e60dbfa716723-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 418648,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6526943932994348507&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Carnegie Mellon University; Carnegie Mellon University + MPI for Biological Cybernetics; Carnegie Mellon University",
        "aff_domain": "cmu.edu;gmail.com;andrew.cmu.edu",
        "email": "cmu.edu;gmail.com;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Carnegie Mellon University;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": ";Biological Cybernetics",
        "aff_unique_url": "https://www.cmu.edu;https://www.biological-cybernetics.de",
        "aff_unique_abbr": "CMU;MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "0f69375f3f",
        "title": "Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/185e65bc40581880c4f2c82958de8cfe-Abstract.html",
        "author": "Cosmin Bejan; Matthew Titsworth; Andrew Hickl; Sanda Harabagiu",
        "abstract": "We present a sequence of unsupervised, nonparametric Bayesian models for clustering  complex linguistic objects. In this approach, we consider a potentially infinite number of features and categorical outcomes. We evaluate these models for the task  of within- and cross-document event coreference on two corpora.  All the models we investigated show significant improvements when compared against an existing baseline for this task.",
        "bibtex": "@inproceedings{NIPS2009_185e65bc,\n author = {Bejan, Cosmin and Titsworth, Matthew and Hickl, Andrew and Harabagiu, Sanda},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/185e65bc40581880c4f2c82958de8cfe-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/185e65bc40581880c4f2c82958de8cfe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/185e65bc40581880c4f2c82958de8cfe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 149182,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5343831240804136242&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Human Language Technology Research Institute, University of Texas at Dallas; Language Computer Corporation, Richardson, Texas; Language Computer Corporation, Richardson, Texas; Human Language Technology Research Institute, University of Texas at Dallas",
        "aff_domain": "hlt.utdallas.edu; ; ; ",
        "email": "hlt.utdallas.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Texas at Dallas;Language Computer Corporation",
        "aff_unique_dep": "Human Language Technology Research Institute;",
        "aff_unique_url": "https://www.utdallas.edu;",
        "aff_unique_abbr": "UT Dallas;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Dallas;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ab111a9519",
        "title": "Nonparametric Bayesian Texture Learning and Synthesis",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f7e6c85504ce6e82442c770f7c8606f0-Abstract.html",
        "author": "Long Zhu; Yuanahao Chen; Bill Freeman; Antonio Torralba",
        "abstract": "We present a nonparametric Bayesian method for texture learning and synthesis.  A texture image is represented by a 2D-Hidden Markov Model (2D-HMM) where the hidden states correspond to the cluster labeling of textons and the transition matrix encodes their spatial layout (the compatibility between adjacent textons). 2D-HMM is coupled with the Hierarchical Dirichlet process (HDP) which allows the number of textons and the complexity of transition matrix grow as the input texture becomes irregular. The HDP makes use of Dirichlet process prior which favors regular textures by penalizing the model complexity. This framework (HDP-2D-HMM) learns the texton vocabulary and their spatial layout jointly and automatically.  The HDP-2D-HMM results in a compact  representation of textures which allows fast texture synthesis with comparable rendering quality over the state-of-the-art image-based rendering methods. We also show that HDP-2D-HMM can be applied to perform image segmentation and synthesis.",
        "bibtex": "@inproceedings{NIPS2009_f7e6c855,\n author = {Zhu, Long and Chen, Yuanahao and Freeman, Bill and Torralba, Antonio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Bayesian Texture Learning and Synthesis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f7e6c85504ce6e82442c770f7c8606f0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 646486,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10656182947595957187&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CSAIL, MIT; Department of Statistics, UCLA + CSAIL, MIT; CSAIL, MIT; CSAIL, MIT",
        "aff_domain": "csail.mit.edu;stat.ucla.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;stat.ucla.edu;csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of California, Los Angeles",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;Department of Statistics",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.ucla.edu",
        "aff_unique_abbr": "MIT;UCLA",
        "aff_campus_unique_index": "0;1+0;0;0",
        "aff_campus_unique": "Cambridge;Los Angeles",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e94a201fc1",
        "title": "Nonparametric Greedy Algorithms for the Sparse Learning Problem",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0cb929eae7a499e50248a3a78f7acfc7-Abstract.html",
        "author": "Han Liu; Xi Chen",
        "abstract": "This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate regression, we propose an algorithm called generalized forward regression. Both of them simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-of-the-art competitors, including the LASSO, a nonparametric version of the LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called the Foba. Some theoretical justifications are also provided.",
        "bibtex": "@inproceedings{NIPS2009_0cb929ea,\n author = {Liu, Han and Chen, Xi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Greedy Algorithms for the Sparse Learning Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0cb929eae7a499e50248a3a78f7acfc7-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0cb929eae7a499e50248a3a78f7acfc7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0cb929eae7a499e50248a3a78f7acfc7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 214135,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6998785021978701802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "850f9a2aa6",
        "title": "Nonparametric Latent Feature Models for Link Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/437d7d1d97917cd627a34a6a0fb41136-Abstract.html",
        "author": "Kurt Miller; Michael I. Jordan; Thomas L. Griffiths",
        "abstract": "As the availability and importance of relational data -- such as the friendships summarized on a social networking website -- increases, it becomes increasingly important to have good models for such data.  The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting nonparametric Bayesian methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable -- latent features -- using a nonparametric Bayesian technique to simultaneously infer the number of features at the same time we learn which entities have each feature. The greater expressiveness of this approach allows us to improve link prediction on three datasets.",
        "bibtex": "@inproceedings{NIPS2009_437d7d1d,\n author = {Miller, Kurt and Jordan, Michael and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Latent Feature Models for Link Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/437d7d1d97917cd627a34a6a0fb41136-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/437d7d1d97917cd627a34a6a0fb41136-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/437d7d1d97917cd627a34a6a0fb41136-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 201008,
        "gs_citation": 515,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5369929087149243229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "EECS, University of California, Berkeley, CA 94720; Psychology and Cognitive Science, University of California, Berkeley, CA 94720; EECS and Statistics, University of California, Berkeley, CA 94720",
        "aff_domain": "cs.berkeley.edu;berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "EECS",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f5416d359a",
        "title": "Occlusive Components Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d67d8ab4f4c10bf22aa353e27879133c-Abstract.html",
        "author": "J\u00f6rg L\u00fccke; Richard Turner; Maneesh Sahani; Marc Henniges",
        "abstract": "We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research field of multiple-cause component extraction methods.",
        "bibtex": "@inproceedings{NIPS2009_d67d8ab4,\n author = {L\\\"{u}cke, J\\\"{o}rg and Turner, Richard and Sahani, Maneesh and Henniges, Marc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Occlusive Components Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/d67d8ab4f4c10bf22aa353e27879133c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d67d8ab4f4c10bf22aa353e27879133c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 180771,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5027957489153424625&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Frankfurt Institutefor Advanced Studies + Goethe-University Frankfurt, Germany; Gatsby Computational Neuroscience Unit, UCL, UK; Gatsby Computational Neuroscience Unit, UCL, UK; Frankfurt Institutefor Advanced Studies + Goethe-University Frankfurt, Germany",
        "aff_domain": "fias.uni-frankfurt.de;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;fias.uni-frankfurt.de",
        "email": "fias.uni-frankfurt.de;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;fias.uni-frankfurt.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;2;0+1",
        "aff_unique_norm": "Frankfurt Institute for Advanced Studies;Goethe University Frankfurt;University College London",
        "aff_unique_dep": ";;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.fias.uni-frankfurt.de/;https://www.uni-frankfurt.de;https://www.ucl.ac.uk",
        "aff_unique_abbr": "FIAS;GU Frankfurt;UCL",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Frankfurt",
        "aff_country_unique_index": "0+0;1;1;0+0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "758a5ca252",
        "title": "On Invariance in Hierarchical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2ab56412b1163ee131e1246da0955bd1-Abstract.html",
        "author": "Jake Bouvrie; Lorenzo Rosasco; Tomaso Poggio",
        "abstract": "A goal of central importance in the study of hierarchical models for object recognition -- and indeed the visual cortex -- is that of understanding quantitatively the trade-off between invariance and selectivity, and how invariance and discrimination properties contribute towards providing an improved representation useful for learning from data. In this work we provide a general group-theoretic framework for characterizing and understanding invariance in a family of hierarchical models.  We show that by taking an algebraic perspective, one can provide a concise set of conditions which must be met to establish invariance, as well as a constructive prescription for meeting those conditions. Analyses in specific cases of particular relevance to computer vision and text processing are given, yielding insight into how and when invariance can be achieved. We find that the minimal sets of transformations intrinsic to the hierarchical model needed to support a particular invariance can be clearly described, thereby encouraging efficient computational implementations.",
        "bibtex": "@inproceedings{NIPS2009_2ab56412,\n author = {Bouvrie, Jake and Rosasco, Lorenzo and Poggio, Tomaso},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Invariance in Hierarchical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/2ab56412b1163ee131e1246da0955bd1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2ab56412b1163ee131e1246da0955bd1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 339511,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6741215990942997951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5ac148e6e4",
        "title": "On Learning Rotations",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/82cec96096d4281b7c95cd7e74623496-Abstract.html",
        "author": "Raman Arora",
        "abstract": "An algorithm is presented for online learning of rotations. The proposed algorithm involves matrix exponentiated gradient updates and is motivated by the Von Neumann divergence. The additive updates are skew-symmetric matrices with trace zero which comprise the Lie algebra of the rotation group. The orthogonality and unit determinant of the matrix  parameter are preserved using matrix logarithms and exponentials and the algorithm lends itself to interesting interpretations in terms of the computational topology of the compact Lie groups. The stability and the computational complexity of the algorithm are discussed.",
        "bibtex": "@inproceedings{NIPS2009_82cec960,\n author = {Arora, Raman},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Learning Rotations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/82cec96096d4281b7c95cd7e74623496-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 187682,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2419892254280247354&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "594b853591",
        "title": "On Stochastic and Worst-case Models for Investing",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/26337353b7962f533d78c762373b3318-Abstract.html",
        "author": "Elad Hazan; Satyen Kale",
        "abstract": "In practice, most investing is done assuming a probabilistic model of stock price returns known as the Geometric Brownian Motion (GBM). While it is often an acceptable approximation, the GBM model is not always valid empirically. This motivates a worst-case approach to investing, called universal portfolio management, where the objective is to maximize wealth relative to the wealth earned by the best fixed portfolio in hindsight.  In this paper we tie the two approaches, and design an investment strategy which is universal in the worst-case, and yet capable of exploiting the mostly valid GBM model. Our method is based on new and improved regret bounds for online convex optimization with exp-concave loss functions.",
        "bibtex": "@inproceedings{NIPS2009_26337353,\n author = {Hazan, Elad and Kale, Satyen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Stochastic and Worst-case Models for Investing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/26337353b7962f533d78c762373b3318-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/26337353b7962f533d78c762373b3318-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/26337353b7962f533d78c762373b3318-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 216442,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12753231500948844771&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "IBM Almaden Research Center; Yahoo! Research",
        "aff_domain": "cs.princeton.edu;yahoo-inc.com",
        "email": "cs.princeton.edu;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IBM;Yahoo!",
        "aff_unique_dep": "Research Center;Yahoo! Research",
        "aff_unique_url": "https://www.ibm.com/research;https://research.yahoo.com",
        "aff_unique_abbr": "IBM;Yahoo!",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Almaden;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "381383d712",
        "title": "On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html",
        "author": "Saketha N. Jagarlapudi; Dinesh G; Raman S; Chiranjib Bhattacharyya; Aharon Ben-tal; Ramakrishnan K.r.",
        "abstract": "Motivated from real world problems, like object categorization, we study a particular mixed-norm regularization for Multiple Kernel Learning (MKL). It is assumed that the given set of kernels are grouped into distinct components where each component is crucial for the learning task at hand. The formulation hence employs $l_\\infty$ regularization for promoting combinations at the component level and $l_1$ regularization for promoting sparsity among kernels in each component. While previous attempts have formulated this as a non-convex problem, the formulation given here is an instance of non-smooth convex optimization problem which admits an efficient Mirror-Descent (MD) based procedure. The MD procedure optimizes over product of simplexes, which is not a well-studied case in literature. Results on real-world datasets show that the new MKL formulation is well-suited for object categorization tasks and that the MD based algorithm outperforms state-of-the-art MKL solvers like \\texttt{simpleMKL} in terms of computational effort.",
        "bibtex": "@inproceedings{NIPS2009_d86ea612,\n author = {Jagarlapudi, Saketha and G, Dinesh and S, Raman and Bhattacharyya, Chiranjib and Ben-tal, Aharon and K.r., Ramakrishnan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/d86ea612dec96096c5e0fcc8dd42ab6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 212128,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6516716785916151212&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Computer Science & Engg., Indian Institute of Technology, Bombay; Dept. of Computer Science & Automation, Indian Institute of Science, Bangalore; Dept. of Computer Science & Automation, Indian Institute of Science, Bangalore; Dept. of Computer Science & Automation, Indian Institute of Science, Bangalore; Faculty of Industrial Engg. & Management, Technion, Haifa; Dept. of Electrical Engg., Indian Institute of Science, Bangalore",
        "aff_domain": "cse.iitb.ac.in;csa.iisc.ernet.in;csa.iisc.ernet.in;csa.iisc.ernet.in;ie.technion.ac.il;ee.iisc.ernet.in",
        "email": "cse.iitb.ac.in;csa.iisc.ernet.in;csa.iisc.ernet.in;csa.iisc.ernet.in;ie.technion.ac.il;ee.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;2;1",
        "aff_unique_norm": "Indian Institute of Technology Bombay;Indian Institute of Science;Technion - Israel Institute of Technology",
        "aff_unique_dep": "Department of Computer Science & Engineering;Dept. of Computer Science & Automation;Faculty of Industrial Engineering and Management",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.iisc.ac.in;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "IIT Bombay;IISc;Technion",
        "aff_campus_unique_index": "0;1;1;1;2;1",
        "aff_campus_unique": "Bombay;Bangalore;Haifa",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "India;Israel"
    },
    {
        "id": "c8b45900fb",
        "title": "On the Convergence of the Concave-Convex Procedure",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html",
        "author": "Gert R. Lanckriet; Bharath K. Sriperumbudur",
        "abstract": "The concave-convex procedure (CCCP) is a majorization-minimization algorithm that solves d.c. (difference of convex functions) programs as a sequence of convex programs. In machine learning, CCCP is extensively used in many learning algorithms like sparse support vector machines (SVMs), transductive SVMs, sparse principal component analysis, etc. Though widely used in many applications, the convergence behavior of CCCP has not gotten a lot of specific attention. Yuille and Rangarajan analyzed its convergence in their original paper, however, we believe the analysis is not complete. Although the convergence of CCCP can be derived from the convergence of the d.c. algorithm (DCA), their proof is more specialized and technical than actually required for the specific case of CCCP. In this paper, we follow a different reasoning and show how Zangwills global convergence theory of iterative algorithms provides a natural framework to prove the convergence of CCCP, allowing a more elegant and simple proof. This underlines Zangwills theory as a powerful and general framework to deal with the convergence issues of iterative algorithms, after also being used to prove the convergence of algorithms like expectation-maximization, generalized alternating minimization, etc. In this paper, we provide a rigorous analysis of the convergence of CCCP by addressing these questions: (i) When does CCCP find a local minimum or a stationary point of the d.c. program under consideration? (ii) When does the sequence generated by CCCP converge? We also present an open problem on the issue of local convergence of CCCP.",
        "bibtex": "@inproceedings{NIPS2009_8b5040a8,\n author = {Lanckriet, Gert and Sriperumbudur, Bharath K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Convergence of the Concave-Convex Procedure},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8b5040a8a5baf3e0e67386c2e3a9b903-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 218256,
        "gs_citation": 325,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6426624218937766110&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego",
        "aff_domain": "ucsd.edu;ece.ucsd.edu",
        "email": "ucsd.edu;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "835b508577",
        "title": "Online Learning of Assignments",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html",
        "author": "Matthew Streeter; Daniel Golovin; Andreas Krause",
        "abstract": "Which ads should we display in sponsored search in order to maximize our revenue?  How should we dynamically rank information sources to maximize value of information?  These applications exhibit strong diminishing returns: Selection of redundant ads and information sources decreases their marginal utility.  We show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one.  We present an efficient algorithm for this general problem and analyze it in the no-regret model.  Our algorithm is equipped with strong theoretical guarantees, with a performance ratio that converges to the optimal constant of 1-1/e.  We empirically evaluate our algorithms on two real-world online optimization problems on the web: ad allocation with submodular utilities, and dynamically ranking blogs to detect information cascades.",
        "bibtex": "@inproceedings{NIPS2009_e0c64119,\n author = {Streeter, Matthew and Golovin, Daniel and Krause, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning of Assignments},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e0c641195b27425bb056ac56f8953d24-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 250179,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8529726289793979792&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff": "Google, Inc.; Carnegie Mellon University; California Institute of Technology",
        "aff_domain": "google.com;cs.cmu.edu;caltech.edu",
        "email": "google.com;cs.cmu.edu;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;Carnegie Mellon University;California Institute of Technology",
        "aff_unique_dep": "Google;;",
        "aff_unique_url": "https://www.google.com;https://www.cmu.edu;https://www.caltech.edu",
        "aff_unique_abbr": "Google;CMU;Caltech",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Mountain View;;Pasadena",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dc4c31b05a",
        "title": "Optimal Scoring for Unsupervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html",
        "author": "Zhihua Zhang; Guang Dai",
        "abstract": "We are often interested in casting classification and clustering problems in a regression framework, because it is feasible to achieve some statistical properties in this framework by imposing some penalty criteria. In this paper we illustrate optimal scoring, which was originally proposed for performing Fisher linear discriminant analysis by regression, in the application of unsupervised learning. In particular, we devise a novel clustering algorithm that we call optimal discriminant clustering (ODC). We associate our algorithm with the existing unsupervised learning algorithms such as spectral clustering, discriminative clustering and sparse principal component analysis. Thus, our work shows that optimal scoring  provides a new approach to the implementation of  unsupervised learning. This approach facilitates the development of new unsupervised learning algorithms.",
        "bibtex": "@inproceedings{NIPS2009_a8e864d0,\n author = {Zhang, Zhihua and Dai, Guang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Scoring for Unsupervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a8e864d04c95572d1aece099af852d0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 304259,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6121902638519966623&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "College of Computer Science & Technology, Zhejiang University; College of Computer Science & Technology, Zhejiang University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Computer Science & Technology",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "03c33997d9",
        "title": "Optimal context separation of spiking haptic signals by second-order somatosensory neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/57aeee35c98205091e18d1140e9f38cf-Abstract.html",
        "author": "Romain Brasselet; Roland Johansson; Angelo Arleo",
        "abstract": "We study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve fibers to second-order somatosensory neurons in the cuneate nucleus (CN). The CN is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans. The efficiency of the haptic discrimination process is quantified by a novel definition of entropy that takes into full account the metrical properties of the spike train space. This measure proves to be a suitable decoding scheme for generalizing the classical Shannon entropy to spike-based neural codes. It permits an assessment of neurotransmission in the presence of a large output space (i.e. hundreds of spike trains) with 1 ms temporal precision. It is shown that the CN population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the first afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms. This study suggests that the CN may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system.",
        "bibtex": "@inproceedings{NIPS2009_57aeee35,\n author = {Brasselet, Romain and Johansson, Roland and Arleo, Angelo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal context separation of spiking haptic signals by second-order somatosensory neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/57aeee35c98205091e18d1140e9f38cf-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/57aeee35c98205091e18d1140e9f38cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1143220,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13348575350200832190&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "CNRS - UPMC Univ Paris 6, UMR 7102; UMEA Univ, Dept Integr Medical Biology; CNRS - UPMC Univ Paris 6, UMR 7102",
        "aff_domain": "upmc.fr;physiol.umu.se;upmc.fr",
        "email": "upmc.fr;physiol.umu.se;upmc.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "UPMC Univ Paris 6;Ume\u00e5 University",
        "aff_unique_dep": "UMR 7102;Department of Integrative Medical Biology",
        "aff_unique_url": "https://www.upmc.fr;https://www.umu.se",
        "aff_unique_abbr": "UPMC;UMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;Sweden"
    },
    {
        "id": "32f49ab692",
        "title": "Optimizing Multi-Class Spatio-Spectral Filters via Bayes Error Estimation for EEG Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/c16a5320fa475530d9583c34fd356ef5-Abstract.html",
        "author": "Wenming Zheng; Zhouchen Lin",
        "abstract": "The method of common spatio-spectral patterns (CSSPs) is an extension of common spatial patterns (CSPs) by utilizing the technique of delay embedding to alleviate the adverse effects of noises and artifacts on the electroencephalogram (EEG) classification. Although the CSSPs method has shown to be more powerful than the CSPs method in the EEG classification, this method is only suitable for two-class EEG classification problems. In this paper, we generalize the two-class CSSPs method to multi-class cases. To this end, we first develop a novel theory of multi-class Bayes error estimation and then present the multi-class CSSPs (MCSSPs) method based on this Bayes error theoretical framework. By minimizing the estimated closed-form Bayes error, we obtain the optimal spatio-spectral filters of MCSSPs. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on the data set of BCI competition 2005. The experimental results show that our method significantly outperforms the previous multi-class CSPs (MCSPs) methods in the EEG classification.",
        "bibtex": "@inproceedings{NIPS2009_c16a5320,\n author = {Zheng, Wenming and Lin, Zhouchen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimizing Multi-Class Spatio-Spectral Filters via Bayes Error Estimation for EEG Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/c16a5320fa475530d9583c34fd356ef5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/c16a5320fa475530d9583c34fd356ef5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/c16a5320fa475530d9583c34fd356ef5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 118003,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17084846003614062685&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Research Center for Learning Science, Southeast University; Microsoft Research Asia",
        "aff_domain": "seu.edu.cn;microsoft.com",
        "email": "seu.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Southeast University;Microsoft",
        "aff_unique_dep": "Research Center for Learning Science;Research",
        "aff_unique_url": "https://www.seu.edu.cn/;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": ";MSR Asia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "e166a076d0",
        "title": "Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8d6dc35e506fc23349dd10ee68dabb64-Abstract.html",
        "author": "Sundeep Rangan; Alyson K. Fletcher",
        "abstract": "Orthogonal matching pursuit (OMP) is a widely used greedy algorithm for recovering sparse vectors from linear measurements.  A well-known analysis of Tropp and Gilbert shows that OMP can recover a k-sparse n-dimensional real vector from m = 4k log(n) noise-free random linear measurements with a probability that goes to one as n goes to infinity. This work shows strengthens this result by showing that a lower number of measurements, m = 2k log(n-k), is in fact sufficient for asymptotic recovery. Moreover, this number of measurements is also sufficient for detection of the sparsity pattern (support) of the vector with measurement errors provided the signal-to-noise ratio (SNR) scales to infinity. The scaling m = 2k log(n-k) exactly matches the number of measurements required by the more complex lasso for signal recovery.",
        "bibtex": "@inproceedings{NIPS2009_8d6dc35e,\n author = {Rangan, Sundeep and Fletcher, Alyson K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8d6dc35e506fc23349dd10ee68dabb64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 105595,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4262457348245536580&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, Berkeley; Qualcomm Technologies",
        "aff_domain": "eecs.berkeley.edu;qualcomm.com",
        "email": "eecs.berkeley.edu;qualcomm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Qualcomm Technologies",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.qualcomm.com",
        "aff_unique_abbr": "UC Berkeley;Qualcomm",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d48891e41a",
        "title": "Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html",
        "author": "Feng Yan; Ningyi Xu; Yuan Qi",
        "abstract": "The recent emergence of Graphics Processing Units (GPUs) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data. In this work, we consider the problem of parallelizing two inference methods on GPUs for latent Dirichlet Allocation (LDA) models, collapsed Gibbs sampling (CGS) and collapsed variational Bayesian (CVB). To address limited memory constraints on GPUs, we propose a novel data partitioning scheme that effectively reduces the memory cost. Furthermore, the partitioning scheme balances the computational cost on each multiprocessor and enables us to easily avoid memory access conflicts. We also use data streaming to handle extremely large datasets. Extensive experiments showed that our parallel inference methods consistently produced LDA models with the same predictive power as sequential training methods did but with 26x speedup for CGS and 196x speedup for CVB on a GPU with 30 multiprocessors; actually the speedup is almost linearly scalable with the number of multiprocessors available. The proposed partitioning scheme and data streaming can be easily ported to many other models in machine learning.",
        "bibtex": "@inproceedings{NIPS2009_ed265bc9,\n author = {Yan, Feng and Xu, Ningyi and Qi, Yuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ed265bc903a5a097f61d3ec064d96d2e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 270338,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=732885777416900944&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "43ecfcfabe",
        "title": "Particle-based Variational Inference for Continuous Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/5ec91aac30eae62f4140325d09b9afd0-Abstract.html",
        "author": "Andrew Frank; Padhraic Smyth; Alexander T. Ihler",
        "abstract": "Since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions defined on discrete random variables. Improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference.  However, extending these methods  to continuous-valued systems has lagged behind.  While several methods have been developed to use belief propagation on systems with continuous values, they have not as yet incorporated the recent advances for discrete variables. In this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to perform inference in continuous systems.  The resulting algorithms behave similarly to their purely discrete counterparts, extending the benefits of these more advanced inference techniques to the continuous domain.",
        "bibtex": "@inproceedings{NIPS2009_5ec91aac,\n author = {Frank, Andrew and Smyth, Padhraic and Ihler, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Particle-based Variational Inference for Continuous Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/5ec91aac30eae62f4140325d09b9afd0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/5ec91aac30eae62f4140325d09b9afd0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/5ec91aac30eae62f4140325d09b9afd0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 349142,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7293571934477131518&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Science, Univ. of California, Irvine; Dept. of Computer Science, Univ. of California, Irvine; Dept. of Computer Science, Univ. of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3ca81a9c19",
        "title": "Perceptual Multistability as Markov Chain Monte Carlo Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/692f93be8c7a41525c0baf2076aecfb4-Abstract.html",
        "author": "Samuel Gershman; Ed Vul; Joshua B. Tenenbaum",
        "abstract": "While many perceptual and cognitive phenomena are well described in terms of Bayesian inference, the necessary computations are intractable at the scale of real-world tasks, and it remains unclear how the human mind approximates Bayesian inference algorithmically. We explore the proposal that for some tasks, humans use a form of Markov Chain Monte Carlo to approximate the posterior distribution over hidden variables.  As a case study, we show how several phenomena of perceptual multistability can be explained as MCMC inference in simple graphical models for low-level vision.",
        "bibtex": "@inproceedings{NIPS2009_692f93be,\n author = {Gershman, Samuel and Vul, Ed and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Perceptual Multistability as Markov Chain Monte Carlo Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/692f93be8c7a41525c0baf2076aecfb4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 996168,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14211800127780997099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Psychology and Neuroscience Institute, Princeton University; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_domain": "princeton.edu;mit.edu;mit.edu",
        "email": "princeton.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Princeton University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Psychology and Neuroscience Institute;Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://www.princeton.edu;https://web.mit.edu",
        "aff_unique_abbr": "Princeton;MIT",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f7a0d7db21",
        "title": "Periodic Step Size Adaptation for Single Pass On-line Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/884d79963bd8bc0ae9b13a1aa71add73-Abstract.html",
        "author": "Chun-nan Hsu; Yu-ming Chang; Hanshen Huang; Yuh-jye Lee",
        "abstract": "It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks.",
        "bibtex": "@inproceedings{NIPS2009_884d7996,\n author = {Hsu, Chun-nan and Chang, Yu-ming and Huang, Hanshen and Lee, Yuh-jye},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Periodic Step Size Adaptation for Single Pass On-line Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/884d79963bd8bc0ae9b13a1aa71add73-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 156403,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2431680029138977845&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Institute of Information Science, Academia Sinica, Taipei 115, Taiwan + USC/Information Sciences Institute, Marina del Rey, CA 90292, USA; Institute of Information Science, Academia Sinica, Taipei 115, Taiwan; Institute of Information Science, Academia Sinica, Taipei 115, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei 106, Taiwan",
        "aff_domain": "isi.edu; ; ;",
        "email": "isi.edu; ; ;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;2",
        "aff_unique_norm": "Academia Sinica;University of Southern California;National Taiwan University of Science and Technology",
        "aff_unique_dep": "Institute of Information Science;Information Sciences Institute;Department of Computer Science and Information Engineering",
        "aff_unique_url": "https://www.sinica.edu.tw;https://isi.usc.edu;https://www.ntust.edu.tw",
        "aff_unique_abbr": "AS;USC ISI;NTUST",
        "aff_campus_unique_index": "0+1;0;0;0",
        "aff_campus_unique": "Taiwan;Marina del Rey",
        "aff_country_unique_index": "0+1;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "7fce5ba72f",
        "title": "Polynomial Semantic Indexing",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/7504adad8bb96320eb3afdd4df6e1f60-Abstract.html",
        "author": "Bing Bai; Jason Weston; David Grangier; Ronan Collobert; Kunihiko Sadamasa; Yanjun Qi; Corinna Cortes; Mehryar Mohri",
        "abstract": "We present a class of nonlinear (polynomial) models that are discriminatively trained to directly map from the word content in a query-document or document-document pair to a ranking score. Dealing with polynomial models on word features is computationally challenging. We propose a low rank (but diagonal preserving) representation of our polynomial models to induce feasible memory and computation requirements. We provide an empirical study on retrieval tasks based on Wikipedia documents, where we obtain state-of-the-art performance while providing realistically scalable methods.",
        "bibtex": "@inproceedings{NIPS2009_7504adad,\n author = {Bai, Bing and Weston, Jason and Grangier, David and Collobert, Ronan and Sadamasa, Kunihiko and Qi, Yanjun and Cortes, Corinna and Mohri, Mehryar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Polynomial Semantic Indexing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/7504adad8bb96320eb3afdd4df6e1f60-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 164976,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9474239618169678467&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "NEC Labs America, Princeton, NJ; NEC Labs America, Princeton, NJ + Google Research, New York, NY; NEC Labs America, Princeton, NJ; NEC Labs America, Princeton, NJ; NEC Labs America, Princeton, NJ; NEC Labs America, Princeton, NJ; Google Research, New York, NY; Google Research, New York, NY + NYU Courant Institute, New York, NY",
        "aff_domain": "nec-labs.com;google.com;nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com;google.com;cs.nyu.edu",
        "email": "nec-labs.com;google.com;nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com;google.com;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;0;0;1;1+2",
        "aff_unique_norm": "NEC Labs America;Google;New York University",
        "aff_unique_dep": ";Google Research;Courant Institute of Mathematical Sciences",
        "aff_unique_url": "https://www.nec-labs.com;https://research.google;https://www.courant.nyu.edu",
        "aff_unique_abbr": "NEC LA;Google Research;NYU",
        "aff_campus_unique_index": "0;0+1;0;0;0;0;1;1+1",
        "aff_campus_unique": "Princeton;New York",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0c49167f68",
        "title": "Positive Semidefinite Metric Learning with Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/051e4e127b92f5d98d3c79b195f2b291-Abstract.html",
        "author": "Chunhua Shen; Junae Kim; Lei Wang; Anton Hengel",
        "abstract": "The learning of appropriate distance metrics is a critical problem in classification. In this work, we propose a boosting-based technique, termed BoostMetric, for learning a Mahalanobis distance metric. One of the primary difficulties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semidefinite. Semidefinite programming is sometimes used to enforce this constraint, but does not scale well. BoostMetric is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices.  BoostMetric thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints.  Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.",
        "bibtex": "@inproceedings{NIPS2009_051e4e12,\n author = {Shen, Chunhua and Kim, Junae and Wang, Lei and Hengel, Anton},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Positive Semidefinite Metric Learning with Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/051e4e127b92f5d98d3c79b195f2b291-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/051e4e127b92f5d98d3c79b195f2b291-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/051e4e127b92f5d98d3c79b195f2b291-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 195708,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14129245123769716013&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5cd3d467b4",
        "title": "Posterior vs Parameter Sparsity in Latent Variable Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html",
        "author": "Kuzman Ganchev; Ben Taskar; Fernando Pereira; Jo\u00e3o Gama",
        "abstract": "In this paper we explore the problem of biasing unsupervised models to favor sparsity. We extend the posterior regularization framework [8] to encourage the  model to achieve posterior sparsity on the unlabeled training data. We apply this new method to learn \ufb01rst-order HMMs for unsupervised part-of-speech (POS) tagging, and show that HMMs learned this way consistently and signi\ufb01cantly out-performs both EM-trained HMMs, and HMMs with a sparsity-inducing Dirichlet prior trained by variational EM. We evaluate these HMMs on three languages \u2014 English, Bulgarian and Portuguese \u2014 under four conditions. We \ufb01nd that our  method always improves performance with respect to both baselines, while variational Bayes actually degrades performance in most cases. We increase accuracy with respect to EM by 2.5%-8.7% absolute and we see improvements even in a semisupervised condition where a limited dictionary is provided.",
        "bibtex": "@inproceedings{NIPS2009_8f1d4362,\n author = {Ganchev, Kuzman and Taskar, Ben and Pereira, Fernando and Gama, Jo\\~{a}o},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Posterior vs Parameter Sparsity in Latent Variable Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/8f1d43620bc6bb580df6e80b0dc05c48-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8f1d43620bc6bb580df6e80b0dc05c48-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 225867,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7151092365521211712&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2870203e10",
        "title": "Potential-Based Agnostic Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/13f9896df61279c928f19721878fac41-Abstract.html",
        "author": "Varun Kanade; Adam Kalai",
        "abstract": "We prove strong noise-tolerance properties of a potential-based boosting algorithm, similar to MadaBoost (Domingo and Watanabe, 2000) and SmoothBoost (Servedio, 2003). Our analysis is in the agnostic framework of Kearns, Schapire and Sellie (1994), giving polynomial-time guarantees in presence of arbitrary noise. A remarkable feature of our algorithm is that it can be implemented without reweighting examples, by randomly relabeling them instead. Our boosting theorem gives, as easy corollaries, alternative derivations of two recent non-trivial results in computational learning theory: agnostically learning decision trees (Gopalan et al, 2008) and agnostically learning halfspaces (Kalai et al, 2005). Experiments suggest that the algorithm performs similarly to Madaboost.",
        "bibtex": "@inproceedings{NIPS2009_13f9896d,\n author = {Kanade, Varun and Kalai, Adam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Potential-Based Agnostic Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/13f9896df61279c928f19721878fac41-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/13f9896df61279c928f19721878fac41-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/13f9896df61279c928f19721878fac41-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/13f9896df61279c928f19721878fac41-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 193309,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4616708815700605991&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Microsoft Research; Harvard University",
        "aff_domain": "microsoft.com;fas.harvard.edu",
        "email": "microsoft.com;fas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;Harvard University",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.harvard.edu",
        "aff_unique_abbr": "MSR;Harvard",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ffb70857cc",
        "title": "Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6bc24fc1ab650b25b4114e93a98f1eba-Abstract.html",
        "author": "Harold Pashler; Nicholas Cepeda; Robert Lindsey; Ed Vul; Michael Mozer",
        "abstract": "When individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a significant impact on memory retention.  Behavioral experiments have shown a nonmonotonic relationship between spacing and retention:  short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals.  Appropriate spacing of study can double retention on educationally relevant time scales.  We introduce a Multiscale Context Model (MCM) that is able to predict the influence of a particular study schedule on retention for specific material.  MCMs prediction is based on empirical data characterizing forgetting of the material following a single study session.  MCM is a synthesis of two existing memory models (Staddon, Chelaru, & Higa, 2002; Raaijmakers, 2003).  On the surface, these  models are unrelated and incompatible, but we show they share a core feature  that allows them to be integrated.  MCM can determine study schedules that  maximize the durability of learning, and has implications for education  and training.  MCM can be cast either as a neural network with inputs that  fluctuate over time, or as a cascade of leaky integrators.  MCM is  intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, Shadmehr, 2007), yet MCM is better able to account for human  declarative memory.",
        "bibtex": "@inproceedings{NIPS2009_6bc24fc1,\n author = {Pashler, Harold and Cepeda, Nicholas and Lindsey, Robert V and Vul, Ed and Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6bc24fc1ab650b25b4114e93a98f1eba-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6bc24fc1ab650b25b4114e93a98f1eba-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6bc24fc1ab650b25b4114e93a98f1eba-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 199922,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4755423851974325271&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Dept. of Computer Science, University of Colorado; Dept. of Psychology, UCSD; Dept. of Psychology, York University; Dept. of Computer Science, University of Colorado; Dept. of Brain and Cognitive Sciences, MIT",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;3",
        "aff_unique_norm": "University of Colorado;University of California, San Diego;York University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Dept. of Computer Science;Department of Psychology;Dept. of Psychology;Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://www.colorado.edu;https://www.ucsd.edu;https://www.yorku.ca;https://www.mit.edu",
        "aff_unique_abbr": "CU;UCSD;York U;MIT",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";San Diego;Cambridge",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "ea48c2cfa9",
        "title": "Probabilistic Relational PCA",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html",
        "author": "Wu-jun Li; Dit-Yan Yeung; Zhihua Zhang",
        "abstract": "One crucial assumption made by both principal component analysis (PCA) and probabilistic PCA (PPCA) is that the instances are independent and identically distributed (i.i.d.). However, this common i.i.d. assumption is unreasonable for relational data. In this paper, by explicitly modeling covariance between instances as derived from the relational information, we propose a novel probabilistic dimensionality reduction method, called probabilistic relational PCA (PRPCA), for relational data analysis. Although the i.i.d. assumption is no longer adopted in PRPCA, the learning algorithms for PRPCA can still be devised easily like those for PPCA which makes explicit use of the i.i.d. assumption. Experiments on real-world data sets show that PRPCA can effectively utilize the relational information to dramatically outperform PCA and achieve state-of-the-art performance.",
        "bibtex": "@inproceedings{NIPS2009_69adc1e1,\n author = {Li, Wu-jun and Yeung, Dit-Yan and Zhang, Zhihua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Relational PCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/69adc1e107f7f7d035d7baf04342e1ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 212516,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2372380441076275162&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Comp. Sci. and Eng., Hong Kong University of Science and Technology, Hong Kong, China; Dept. of Comp. Sci. and Eng., Hong Kong University of Science and Technology, Hong Kong, China; School of Comp. Sci. and Tech., Zhejiang University, Zhejiang 310027, China",
        "aff_domain": "cse.ust.hk;cse.ust.hk;cs.zju.edu.cn",
        "email": "cse.ust.hk;cse.ust.hk;cs.zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Zhejiang University",
        "aff_unique_dep": "Department of Computer Science and Engineering;School of Computer Science and Technology",
        "aff_unique_url": "https://www.ust.hk;http://www.zju.edu.cn",
        "aff_unique_abbr": "HKUST;ZJU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Hong Kong;Zhejiang",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "eb969ae46a",
        "title": "Quantification and the language of thought",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/82161242827b703e6acf9c726942a1e4-Abstract.html",
        "author": "Charles Kemp",
        "abstract": "Many researchers have suggested that the psychological complexity of a concept is related to the length of its representation in a language of thought.  As yet, however, there are few concrete proposals about the nature of this language. This paper makes one such proposal: the language of thought allows first order quantification (quantification over objects) more readily than second-order quantification (quantification over features). To support this proposal we present behavioral results from a concept learning study inspired by the work of Shepard, Hovland and Jenkins.\"",
        "bibtex": "@inproceedings{NIPS2009_82161242,\n author = {Kemp, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Quantification and the language of thought},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/82161242827b703e6acf9c726942a1e4-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/82161242827b703e6acf9c726942a1e4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/82161242827b703e6acf9c726942a1e4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 281510,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6282280523945715058&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Psychology, Carnegie Mellon University",
        "aff_domain": "cmu.edu",
        "email": "cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5e9104929f",
        "title": "Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e515df0d202ae52fcebb14295743063b-Abstract.html",
        "author": "Alexandre Bouchard-c\u00f4t\u00e9; Slav Petrov; Dan Klein",
        "abstract": "Pruning can massively accelerate the computation of feature expectations in large models.  However, any single pruning mask will introduce bias.  We present a novel approach which employs a randomized sequence of pruning masks. Formally, we apply auxiliary variable MCMC sampling to generate this sequence of masks, thereby gaining theoretical guarantees about convergence. Because each mask is generally able to skip large portions of an underlying dynamic program, our approach is particularly compelling for high-degree algorithms.  Empirically, we demonstrate our method on bilingual parsing, showing decreasing bias as more masks are incorporated, and outperforming fixed tic-tac-toe pruning.",
        "bibtex": "@inproceedings{NIPS2009_e515df0d,\n author = {Bouchard-c\\^{o}t\\'{e}, Alexandre and Petrov, Slav and Klein, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e515df0d202ae52fcebb14295743063b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e515df0d202ae52fcebb14295743063b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e515df0d202ae52fcebb14295743063b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1154884,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4586822901260559678&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Division, University of California at Berkeley; Google Research + University of California at Berkeley; Computer Science Division, University of California at Berkeley",
        "aff_domain": "cs.berkeley.edu;google.com;cs.berkeley.edu",
        "email": "cs.berkeley.edu;google.com;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "University of California, Berkeley;Google",
        "aff_unique_dep": "Computer Science Division;Google Research",
        "aff_unique_url": "https://www.berkeley.edu;https://research.google",
        "aff_unique_abbr": "UC Berkeley;Google Research",
        "aff_campus_unique_index": "0;1+0;0",
        "aff_campus_unique": "Berkeley;Mountain View",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f4411fea7",
        "title": "Rank-Approximate Nearest Neighbor Search: Retaining Meaning and Speed in High Dimensions",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ddb30680a691d157187ee1cf9e896d03-Abstract.html",
        "author": "Parikshit Ram; Dongryeol Lee; Hua Ouyang; Alexander G. Gray",
        "abstract": "The long-standing problem of efficient nearest-neighbor (NN) search has ubiquitous applications ranging from astrophysics to MP3 fingerprinting to bioinformatics to movie recommendations.  As the dimensionality of the dataset increases, exact NN search becomes computationally prohibitive; (1+eps)-distance-approximate NN search can provide large speedups but risks losing the meaning of NN search present in the ranks (ordering) of the distances. This paper presents a simple, practical algorithm allowing the user to, for the first time, directly control the true accuracy of NN search (in terms of ranks) while still achieving the large speedups over exact NN.  Experiments with high-dimensional datasets show that it often achieves faster and more accurate results than the best-known distance-approximate method, with much more stable behavior.",
        "bibtex": "@inproceedings{NIPS2009_ddb30680,\n author = {Ram, Parikshit and Lee, Dongryeol and Ouyang, Hua and Gray, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rank-Approximate Nearest Neighbor Search: Retaining Meaning and Speed in High Dimensions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ddb30680a691d157187ee1cf9e896d03-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 95239,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2435002592562810504&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ffafa2e241",
        "title": "Ranking Measures and Loss Functions in Learning to Rank",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2f55707d4193dc27118a0f19a1985716-Abstract.html",
        "author": "Wei Chen; Tie-yan Liu; Yanyan Lan; Zhi-ming Ma; Hang Li",
        "abstract": "Learning to rank has become an important research topic in machine learning. While most learning-to-rank methods learn the ranking function by minimizing the loss functions, it is the ranking measures (such as NDCG and MAP) that are used to evaluate the performance of the learned ranking function. In this work, we reveal the relationship between ranking measures and loss functions in learning-to-rank methods, such as Ranking SVM, RankBoost, RankNet, and ListMLE. We show that these loss functions are upper bounds of the measure-based ranking errors. As a result, the minimization of these loss functions will lead to the maximization of the ranking measures. The key to obtaining this result is to model ranking as a sequence of classification tasks, and define a so-called essential loss as the weighted sum of the classification errors of individual tasks in the sequence. We have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods. Our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors. Experimental results on benchmark datasets show that the modifications can lead to better ranking performance, demonstrating the correctness of our analysis.",
        "bibtex": "@inproceedings{NIPS2009_2f55707d,\n author = {Chen, Wei and Liu, Tie-yan and Lan, Yanyan and Ma, Zhi-ming and Li, Hang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ranking Measures and Loss Functions in Learning to Rank},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2f55707d4193dc27118a0f19a1985716-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2f55707d4193dc27118a0f19a1985716-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2f55707d4193dc27118a0f19a1985716-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 107657,
        "gs_citation": 293,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8766785085331722643&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Chinese Academy of sciences; Microsoft Research Asia; Chinese Academy of sciences; Chinese Academy of sciences; Microsoft Research Asia",
        "aff_domain": "amss.ac.cn;micorsoft.com;amss.ac.cn;amt.ac.cn;micorsoft.com",
        "email": "amss.ac.cn;micorsoft.com;amss.ac.cn;amt.ac.cn;micorsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "Chinese Academy of Sciences;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "http://www.cas.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "CAS;MSR Asia",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "a93b7ab0e2",
        "title": "Reading Tea Leaves: How Humans Interpret Topic Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html",
        "author": "Jonathan Chang; Sean Gerrish; Chong Wang; Jordan L. Boyd-graber; David M. Blei",
        "abstract": "Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.",
        "bibtex": "@inproceedings{NIPS2009_f92586a2,\n author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reading Tea Leaves: How Humans Interpret Topic Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 700727,
        "gs_citation": 3625,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10065652925831485830&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Facebook; Institute for Advanced Computer Studies, University of Maryland; Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "facebook.com;umiacs.umd.edu;cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "email": "facebook.com;umiacs.umd.edu;cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;2",
        "aff_unique_norm": "Meta;University of Maryland;Princeton University",
        "aff_unique_dep": "Facebook, Inc.;Institute for Advanced Computer Studies;Department of Computer Science",
        "aff_unique_url": "https://www.facebook.com;https://www.umd.edu;https://www.princeton.edu",
        "aff_unique_abbr": "FB;UMD;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0113966fc2",
        "title": "Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME)",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8f7d807e1f53eff5f9efbe5cb81090fb-Abstract.html",
        "author": "Tao Hu; Anthony Leonardo; Dmitri B. Chklovskii",
        "abstract": "One of the central problems in neuroscience is reconstructing synaptic connectivity in neural circuits. Synapses onto a neuron can be probed by sequentially stimulating potentially pre-synaptic neurons while monitoring the membrane voltage of the post-synaptic neuron. Reconstructing a large neural circuit using such a \u201cbrute force\u201d approach is rather time-consuming and inefficient because the connectivity in neural circuits is sparse. Instead, we propose to measure a post-synaptic neuron\u2019s voltage while stimulating simultaneously multiple randomly chosen potentially pre-synaptic neurons. To extract the weights of individual synaptic connections we apply a decoding algorithm recently developed for compressive sensing. Compared to the brute force approach, our method promises significant time savings that grow with the size of the circuit. We use computer simulations to find optimal stimulation parameters and explore the feasibility of our reconstruction method under realistic experimental conditions including noise and non-linear synaptic integration. Multiple-neuron stimulation allows reconstructing synaptic connectivity just from the spiking activity of post-synaptic neurons, even when sub-threshold voltage is unavailable. By using calcium indicators, voltage-sensitive dyes, or multi-electrode arrays one could monitor activity of multiple post-synaptic neurons simultaneously, thus mapping their synaptic inputs in parallel, potentially reconstructing a complete neural circuit.",
        "bibtex": "@inproceedings{NIPS2009_8f7d807e,\n author = {Hu, Tao and Leonardo, Anthony and Chklovskii, Dmitri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME)},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8f7d807e1f53eff5f9efbe5cb81090fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 268270,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8755184083815038746&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "acb924caf0",
        "title": "Region-based Segmentation and Object Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a7aeed74714116f3b292a982238f83d2-Abstract.html",
        "author": "Stephen Gould; Tianshi Gao; Daphne Koller",
        "abstract": "Object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other. However, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving classification of many parts of the scene ambiguous. In this work, we propose a hierarchical region-based approach to joint object detection and image segmentation. Our approach reasons about pixels, regions and objects in a coherent probabilistic model. Importantly, our model gives a single unified description of the scene. We explain every pixel in the image and enforce global consistency between all variables in our model. We run experiments on challenging vision datasets and show significant improvement over state-of-the-art object detection accuracy.",
        "bibtex": "@inproceedings{NIPS2009_a7aeed74,\n author = {Gould, Stephen and Gao, Tianshi and Koller, Daphne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Region-based Segmentation and Object Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a7aeed74714116f3b292a982238f83d2-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a7aeed74714116f3b292a982238f83d2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a7aeed74714116f3b292a982238f83d2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 257855,
        "gs_citation": 302,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3234955208397496200&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Electrical Engineering, Stanford University; Department of Electrical Engineering, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5e6844d1d7",
        "title": "Regularized Distance Metric Learning:Theory and Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a666587afda6e89aec274a3657558a27-Abstract.html",
        "author": "Rong Jin; Shijun Wang; Yang Zhou",
        "abstract": "In this paper, we examine the generalization error of regularized distance metric learning. We show that with appropriate constraints, the generalization error of regularized distance metric learning could be independent from the dimensionality, making it suitable for handling high dimensional data. In addition, we present an efficient online learning algorithm for regularized distance metric learning. Our empirical studies with data classification and face recognition show that the proposed algorithm is (i) effective for distance metric learning when compared to the state-of-the-art methods, and (ii) efficient and robust for high dimensional data.",
        "bibtex": "@inproceedings{NIPS2009_a666587a,\n author = {Jin, Rong and Wang, Shijun and Zhou, Yang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regularized Distance Metric Learning:Theory and Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a666587afda6e89aec274a3657558a27-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a666587afda6e89aec274a3657558a27-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/a666587afda6e89aec274a3657558a27-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a666587afda6e89aec274a3657558a27-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 110386,
        "gs_citation": 245,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1821738328817776886&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Science & Engineering, Michigan State University, East Lansing, MI 48824; Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892; Dept. of Computer Science & Engineering, Michigan State University, East Lansing, MI 48824",
        "aff_domain": "cse.msu.edu;cc.nih.gov;msu.edu",
        "email": "cse.msu.edu;cc.nih.gov;msu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Michigan State University;National Institutes of Health",
        "aff_unique_dep": "Department of Computer Science & Engineering;Radiology and Imaging Sciences",
        "aff_unique_url": "https://www.msu.edu;https://www.nih.gov",
        "aff_unique_abbr": "MSU;NIH",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "East Lansing;Bethesda",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bf5eb792e4",
        "title": "Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/08c5433a60135c32e34f46a71175850c-Abstract.html",
        "author": "Stefan Klampfl; Wolfgang Maass",
        "abstract": "Many models for computations in recurrent networks of neurons assume that the network state moves from some initial state to some fixed point attractor or limit cycle that represents the output of the computation. However experimental data show that in response to a sensory stimulus the network state moves from its initial state through a trajectory of network states and eventually returns to the initial state, without reaching an attractor or limit cycle in between. This type of network response, where salient information about external stimuli is encoded in characteristic trajectories of continuously varying network states, raises the question how a neural system could compute with such code, and arrive for example at a temporally stable classification of the external stimulus. We show that a known unsupervised learning algorithm, Slow Feature Analysis (SFA), could be an important ingredient for extracting stable information from these network trajectories. In fact, if sensory stimuli are more often followed by another stimulus from the same class than by a stimulus from another class, SFA approaches the classification capability of Fishers Linear Discriminant (FLD), a powerful algorithm for supervised learning. We apply this principle to simulated cortical microcircuits, and show that it enables readout neurons to learn discrimination of spoken digits and detection of repeating firing patterns within a stream of spike trains with the same firing statistics, without requiring any supervision for learning.",
        "bibtex": "@inproceedings{NIPS2009_08c5433a,\n author = {Klampfl, Stefan and Maass, Wolfgang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/08c5433a60135c32e34f46a71175850c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/08c5433a60135c32e34f46a71175850c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/08c5433a60135c32e34f46a71175850c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1046877,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17941723320030420327&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "InstituteforTheoreticalComputerScience, GrazUniversityofTechnology; InstituteforTheoreticalComputerScience, GrazUniversityofTechnology",
        "aff_domain": "igi.tugraz.at;igi.tugraz.at",
        "email": "igi.tugraz.at;igi.tugraz.at",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Institute for Theoretical Computer Science",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TUGraz",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "61aaf0f2c3",
        "title": "Replicated Softmax: an Undirected Topic Model",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/31839b036f63806cba3f47b93af8ccb5-Abstract.html",
        "author": "Geoffrey E. Hinton; Ruslan Salakhutdinov",
        "abstract": "We show how to model documents as bags of words using family of two-layer, undirected graphical models. Each member of the family has the same number of binary hidden units but a different number of ``softmax visible units. All of the softmax units in all of the models in the family share the same weights to the binary hidden units. We describe efficient inference and learning procedures for such a family. Each member of the family models the probability distribution of documents of a specific length as a product of topic-specific distributions rather than as a mixture and this gives much better generalization than Latent Dirichlet Allocation for modeling the log probabilities of held-out documents. The low-dimensional topic vectors learned by the undirected family are also much better than LDA topic vectors for retrieving documents that are similar to a query document. The learned topics are more general than those found by LDA because precision is achieved by intersecting many general topics rather than by selecting a single precise topic to generate each word.",
        "bibtex": "@inproceedings{NIPS2009_31839b03,\n author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Replicated Softmax: an Undirected Topic Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/31839b036f63806cba3f47b93af8ccb5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131316,
        "gs_citation": 688,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8289384845546106281&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Brain and Cognitive Sciences and CSAIL, Massachusetts Institute of Technology; Department of Computer Science, University of Toronto",
        "aff_domain": "mit.edu;cs.toronto.edu",
        "email": "mit.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Toronto",
        "aff_unique_dep": "Brain and Cognitive Sciences and Computer Science and Artificial Intelligence Laboratory;Department of Computer Science",
        "aff_unique_url": "https://web.mit.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "MIT;U of T",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Toronto",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "a8bd4087bc",
        "title": "Rethinking LDA: Why Priors Matter",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0d0871f0806eae32d30983b62252da50-Abstract.html",
        "author": "Hanna M. Wallach; David M. Mimno; Andrew McCallum",
        "abstract": "Implementations of topic models typically use symmetric Dirichlet priors with fixed concentration parameters, with the implicit assumption that such smoothing parameters\" have little practical effect. In this paper, we explore several classes of structured priors for topic models. We find that an asymmetric Dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior, while an asymmetric prior over the topic-word distributions provides no real benefit. Approximation of this prior structure through simple, efficient hyperparameter optimization steps is sufficient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language. Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.\"",
        "bibtex": "@inproceedings{NIPS2009_0d0871f0,\n author = {Wallach, Hanna and Mimno, David and McCallum, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rethinking LDA: Why Priors Matter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0d0871f0806eae32d30983b62252da50-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0d0871f0806eae32d30983b62252da50-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/0d0871f0806eae32d30983b62252da50-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0d0871f0806eae32d30983b62252da50-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 999343,
        "gs_citation": 1038,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4784006211071822360&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003; Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003; Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003",
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "http://rexa.info/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f37bf5d34",
        "title": "Riffled Independence for Ranked Data",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html",
        "author": "Jonathan Huang; Carlos Guestrin",
        "abstract": "Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called riffled independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efficient inference and reducing sample complexity. In riffled independence, one draws two permutations independently, then performs the riffle shuffle, common in card games, to combine the two permutations to form a single permutation. In ranking, riffled independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using riffled independence within Fourier-theoretic frameworks which have been explored by a number of recent papers.",
        "bibtex": "@inproceedings{NIPS2009_6cfe0e61,\n author = {Huang, Jonathan and Guestrin, Carlos},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Riffled Independence for Ranked Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/6cfe0e6127fa25df2a0ef2ae1067d915-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6cfe0e6127fa25df2a0ef2ae1067d915-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 262649,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10619843391570236883&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5a40fcbb4b",
        "title": "Robust Nonparametric Regression with Metric-Space Valued Output",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/92977ae4d2ba21425a59afb269c2a14e-Abstract.html",
        "author": "Matthias Hein",
        "abstract": "Motivated by recent developments in manifold-valued regression we propose a family of nonparametric kernel-smoothing estimators with metric-space valued output including a robust median type estimator and the classical Frechet mean. Depending on the choice of the output space and the chosen metric the estimator reduces to partially well-known procedures for multi-class classification, multivariate regression in Euclidean space, regression with manifold-valued output and even some cases of structured output learning. In this paper we focus on the case of regression with manifold-valued input and output. We show pointwise and Bayes consistency for all estimators in the family for the case of manifold-valued output and illustrate the robustness properties of the estimator with experiments.",
        "bibtex": "@inproceedings{NIPS2009_92977ae4,\n author = {Hein, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Nonparametric Regression with Metric-Space Valued Output},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/92977ae4d2ba21425a59afb269c2a14e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/92977ae4d2ba21425a59afb269c2a14e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 286571,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5032640286828327995&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Saarland University",
        "aff_domain": "cs.uni-sb.de",
        "email": "cs.uni-sb.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "Saarland U",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "1b20b5c6af",
        "title": "Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/c45147dee729311ef5b5c3003946c48f-Abstract.html",
        "author": "John Wright; Arvind Ganesh; Shankar Rao; Yigang Peng; Yi Ma",
        "abstract": "Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized \u201crobust principal component analysis\u201d problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the error entries E can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efficiently and exactly recovered from most error sign-and-support patterns, by solving a simple convex program. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.",
        "bibtex": "@inproceedings{NIPS2009_c45147de,\n author = {Wright, John and Ganesh, Arvind and Rao, Shankar and Peng, Yigang and Ma, Yi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/c45147dee729311ef5b5c3003946c48f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/c45147dee729311ef5b5c3003946c48f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 274748,
        "gs_citation": 2272,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5347391055780784580&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Visual Computing Group, Microsoft Research Asia; Visual Computing Group, Microsoft Research Asia; Visual Computing Group, Microsoft Research Asia; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;uiuc.edu;uiuc.edu",
        "email": "microsoft.com;microsoft.com;microsoft.com;uiuc.edu;uiuc.edu",
        "github": "",
        "project": "http://perception.csl.illinois.edu/matrix-rank/home.html",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Microsoft;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Visual Computing Group;Coordinated Science Laboratory",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia;https://www illinois.edu",
        "aff_unique_abbr": "MSRA;UIUC",
        "aff_campus_unique_index": "0;0;0;1;1",
        "aff_campus_unique": "Asia;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "e6aea80484",
        "title": "Robust Value Function Approximation Using Bilinear Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ec5decca5ed3d6b8079e2e7e7bacc9f2-Abstract.html",
        "author": "Marek Petrik; Shlomo Zilberstein",
        "abstract": "Existing value function approximation methods have been successfully used in many applications, but they often lack useful a priori error bounds. We propose approximate bilinear programming, a new formulation of value function approximation that provides strong a priori guarantees. In particular, it provably finds an approximate value function that minimizes the Bellman residual. Solving a bilinear program optimally is NP hard, but this is unavoidable because the Bellman-residual minimization itself is NP hard. We, therefore, employ and analyze a common approximate algorithm for bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on a simple benchmark problem.",
        "bibtex": "@inproceedings{NIPS2009_ec5decca,\n author = {Petrik, Marek and Zilberstein, Shlomo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Value Function Approximation Using Bilinear Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 253478,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15113432345047069269&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University of Massachusetts; Department of Computer Science, University of Massachusetts",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5fd2213619",
        "title": "STDP enables spiking neurons to detect hidden causes of their inputs",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a5cdd4aa0048b187f7182f1b9ce7a6a7-Abstract.html",
        "author": "Bernhard Nessler; Michael Pfeiffer; Wolfgang Maass",
        "abstract": "The principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits, and how spike-timing-dependent plasticity (STDP) of synaptic weights could generate and maintain  this computational function, are unknown. We show here that STDP, in conjunction with a stochastic soft winner-take-all (WTA) circuit, induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses (or causes\") of the high-dimensional spike patterns of hundreds of pre-synaptic neurons. Hence these  neurons will fire after learning whenever the current input best matches their internal model. The resulting computational function of soft WTA circuits, a common network motif of cortical microcircuits, could therefore be a drastic dimensionality reduction of information streams, together with the autonomous creation of internal models for the probability distributions of their input patterns. We show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles. In particular, we show that STDP is able to approximate a stochastic online Expectation-Maximization (EM) algorithm for modeling the input data. A corresponding result is shown for Hebbian learning in artificial neural networks.\"",
        "bibtex": "@inproceedings{NIPS2009_a5cdd4aa,\n author = {Nessler, Bernhard and Pfeiffer, Michael and Maass, Wolfgang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {STDP enables spiking neurons to detect hidden causes of their inputs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 205858,
        "gs_citation": 157,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16972082276322574792&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "25faeb9f84",
        "title": "Segmenting Scenes by Matching Image Composites",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/fba9d88164f3e2d9109ee770223212a0-Abstract.html",
        "author": "Bryan Russell; Alyosha Efros; Josef Sivic; Bill Freeman; Andrew Zisserman",
        "abstract": "In this paper, we investigate how similar images sharing the same global description can help with unsupervised scene segmentation in an image.  In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes.  This allows for a better explanation of the input scenes.  We perform MRF-based segmentation that optimizes over matches, while respecting boundary information.  The recovered segments are then used to re-query a large database of images to retrieve better matches for the target region. We show improved performance in detecting occluding boundaries over previous methods on data gathered from the LabelMe database.",
        "bibtex": "@inproceedings{NIPS2009_fba9d881,\n author = {Russell, Bryan and Efros, Alyosha and Sivic, Josef and Freeman, Bill and Zisserman, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Segmenting Scenes by Matching Image Composites},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/fba9d88164f3e2d9109ee770223212a0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 868480,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16915647522742026703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "INRIA\u2217; Carnegie Mellon University; INRIA\u2217; CSAIL MIT; University of Oxford + INRIA\u2217",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;3+0",
        "aff_unique_norm": "INRIA;Carnegie Mellon University;Massachusetts Institute of Technology;University of Oxford",
        "aff_unique_dep": ";;Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.inria.fr;https://www.cmu.edu;https://www.csail.mit.edu;https://www.ox.ac.uk",
        "aff_unique_abbr": "INRIA;CMU;MIT;Oxford",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;0;1;2+0",
        "aff_country_unique": "France;United States;United Kingdom"
    },
    {
        "id": "f93f2e4350",
        "title": "Semi-Supervised Learning in Gigantic Image Collections",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1651cf0d2f737d7adeab84d339dbabd3-Abstract.html",
        "author": "Rob Fergus; Yair Weiss; Antonio Torralba",
        "abstract": "With the advent of the Internet it is now possible to collect   hundreds of millions of images. These images come with varying   degrees of label information.",
        "bibtex": "@inproceedings{NIPS2009_1651cf0d,\n author = {Fergus, Rob and Weiss, Yair and Torralba, Antonio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-Supervised Learning in Gigantic Image Collections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1651cf0d2f737d7adeab84d339dbabd3-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1651cf0d2f737d7adeab84d339dbabd3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1651cf0d2f737d7adeab84d339dbabd3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 551300,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Courant Institute, NYU; School of Computer Science, Hebrew University; CSAIL, EECS, MIT",
        "aff_domain": "cs.nyu.edu;huji.ac.il;csail.mit.edu",
        "email": "cs.nyu.edu;huji.ac.il;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "New York University;Hebrew University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Courant Institute;School of Computer Science;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.courant.nyu.edu;http://www.huji.ac.il;https://www.csail.mit.edu",
        "aff_unique_abbr": "NYU;HUJI;MIT",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "New York;;Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "6ee9a72568",
        "title": "Semi-supervised Learning using Sparse Eigenfunction Bases",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/82b8a3434904411a9fdc43ca87cee70c-Abstract.html",
        "author": "Kaushik Sinha; Mikhail Belkin",
        "abstract": "We present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices.  It turns out that  when the \\emph{cluster assumption} holds, that is, when the high density regions are sufficiently separated by  low density valleys, each high density area corresponds to a unique representative eigenvector. Linear combination of such eigenvectors (or, more precisely, of their Nystrom extensions) provide good candidates for good classification functions. By first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data  with Lasso to select a classifier in the span of these eigenvectors, we obtain a classifier, which has a very sparse representation in this basis. Importantly, the sparsity appears naturally from the  cluster assumption. Experimental results on a number  of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm (Lasso in the Kernel PCA basis).",
        "bibtex": "@inproceedings{NIPS2009_82b8a343,\n author = {Sinha, Kaushik and Belkin, Mikhail},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-supervised Learning using Sparse Eigenfunction Bases},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/82b8a3434904411a9fdc43ca87cee70c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 318755,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3359796745324662353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Computer Science and Engineering, Ohio State University; Dept. of Computer Science and Engineering, Ohio State University",
        "aff_domain": "cse.ohio-state.edu;cse.ohio-state.edu",
        "email": "cse.ohio-state.edu;cse.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ohio State University",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9db8ae0c21",
        "title": "Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f4552671f8909587cf485ea990207f3b-Abstract.html",
        "author": "Kwang I. Kim; Florian Steinke; Matthias Hein",
        "abstract": "Semi-supervised regression based on the graph Laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power. Outgoing from these observations we propose to use the second-order Hessian energy for semi-supervised regression which overcomes both of these problems, in particular, if the data lies on or close to a low-dimensional submanifold in the feature space, the Hessian energy prefers functions which vary ``linearly with respect to the natural parameters in the data. This property makes it also particularly suited for the task of semi-supervised dimensionality reduction where the goal is to find the natural parameters in the data based on a few labeled points. The experimental result suggest that our method is superior to semi-supervised regression using Laplacian regularization and standard supervised methods and is particularly suited for semi-supervised dimensionality reduction.",
        "bibtex": "@inproceedings{NIPS2009_f4552671,\n author = {Kim, Kwang and Steinke, Florian and Hein, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f4552671f8909587cf485ea990207f3b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f4552671f8909587cf485ea990207f3b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/f4552671f8909587cf485ea990207f3b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f4552671f8909587cf485ea990207f3b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1227634,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9032256399098064105&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Saarland University Saarbr\u00fccken, Germany; Siemens AG Corporate Technology Munich, Germany + MPI for Biological Cybernetics, Germany; Department of Computer Science, Saarland University Saarbr\u00fccken, Germany",
        "aff_domain": "cs.uni-sb.de;siemens.com;cs.uni-sb.de",
        "email": "cs.uni-sb.de;siemens.com;cs.uni-sb.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Saarland University;Siemens AG;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Department of Computer Science;Corporate Technology;",
        "aff_unique_url": "https://www.uni-saarland.de;https://www.siemens.com;https://www.biological-cybernetics.de",
        "aff_unique_abbr": ";Siemens;MPIBC",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Saarbr\u00fccken;Munich;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "19b9615d70",
        "title": "Sensitivity analysis in HMMs with application to likelihood maximization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/16a5cdae362b8d27a1d8f8c7b78b4330-Abstract.html",
        "author": "Pierre-arnaud Coquelin; Romain Deguest; R\u00e9mi Munos",
        "abstract": "This paper considers a sensitivity analysis in Hidden Markov Models with continuous state and observation spaces. We propose an Infinitesimal Perturbation Analysis (IPA) on the filtering distribution with respect to some parameters of the model. We describe a methodology for using any algorithm that estimates the filtering density, such as Sequential Monte Carlo methods, to design an algorithm that estimates its gradient. The resulting IPA estimator is proven to be asymptotically unbiased, consistent and has computational complexity linear in the number of particles. We consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations. We derive an IPA estimator for the gradient of the log-likelihood, which may be used in a gradient method for the purpose of likelihood maximization. We illustrate the method with several numerical experiments.",
        "bibtex": "@inproceedings{NIPS2009_16a5cdae,\n author = {Coquelin, Pierre-arnaud and Deguest, Romain and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sensitivity analysis in HMMs with application to likelihood maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/16a5cdae362b8d27a1d8f8c7b78b4330-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 125126,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2272464863401965917&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 18,
        "aff": "Vekia, Lille, France; Columbia University, New York City, NY 10027 + CMAP, Ecole Polytechnique, France; INRIA Lille - Nord Europe, Sequel Project, France",
        "aff_domain": "vekia.fr;columbia.edu;inria.fr",
        "email": "vekia.fr;columbia.edu;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;3",
        "aff_unique_norm": "Vekia;Columbia University;Ecole Polytechnique;INRIA",
        "aff_unique_dep": ";;CMAP;Sequel Project",
        "aff_unique_url": ";https://www.columbia.edu;https://www.polytechnique.edu;https://www.inria.fr",
        "aff_unique_abbr": ";Columbia;Polytechnique;INRIA",
        "aff_campus_unique_index": "0;1;3",
        "aff_campus_unique": "Lille;New York City;;Lille - Nord Europe",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "434e023517",
        "title": "Sequential effects reflect parallel learning of multiple environmental regularities",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/522a9ae9a99880d39e5daec35375e999-Abstract.html",
        "author": "Matthew Wilder; Matt Jones; Michael Mozer",
        "abstract": "Across a wide range of cognitive tasks, recent experience in\ufb02uences behavior. For example, when individuals repeatedly perform a simple two-alternative forced-choice task (2AFC), response latencies vary dramatically based on the immediately preceding trial sequence. These sequential effects have been interpreted as adaptation to the statistical structure of an uncertain, changing environment (e.g. Jones & Sieck, 2003; Mozer, Kinoshita, & Shettel, 2007; Yu & Cohen, 2008). The Dynamic Belief Model (DBM) (Yu & Cohen, 2008) explains sequential effects in 2AFC tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence (repetition rates) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial. Experimental results suggest that \ufb01rst-order statistics (base rates) also in\ufb02uence sequential effects. We propose a model that learns both \ufb01rst- and second-order sequence properties, each according to the basic principles of the DBM but under a uni\ufb01ed inferential framework. This model, the Dynamic Belief Mixture Model (DBM2), obtains precise, parsimonious \ufb01ts to data. Furthermore, the model predicts dissociations in behavioral (Maloney, Dal Martello, Sahm, & Spillmann, 2005) and electrophysiological studies (Jentzsch & Sommer, 2002), supporting the psychological and neurobiological reality of its two components.",
        "bibtex": "@inproceedings{NIPS2009_522a9ae9,\n author = {Wilder, Matthew and Jones, Matt and Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sequential effects reflect parallel learning of multiple environmental regularities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/522a9ae9a99880d39e5daec35375e999-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 621312,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15020630429044812694&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Computer Science; Dept. of Psychology; Dept. of Computer Science",
        "aff_domain": "colorado.edu;colorado.edu;colorado.edu",
        "email": "colorado.edu;colorado.edu;colorado.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e871426bc6",
        "title": "Sharing Features among Dynamical Systems with Beta Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4b04a686b0ad13dce35fa99fa4161c65-Abstract.html",
        "author": "Emily B. Fox; Michael I. Jordan; Erik B. Sudderth; Alan S. Willsky",
        "abstract": "We propose a Bayesian nonparametric approach to relating multiple time series via a set of latent, dynamical behaviors.  Using a beta process prior, we allow data-driven selection of the size of this set, as well as the pattern with which behaviors are shared among time series.  Via the Indian buffet process representation of the beta process predictive distributions, we develop an exact Markov chain Monte Carlo inference method.  In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals.  We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising unsupervised segmentation of visual motion capture data.",
        "bibtex": "@inproceedings{NIPS2009_4b04a686,\n author = {Fox, Emily and Jordan, Michael and Sudderth, Erik and Willsky, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sharing Features among Dynamical Systems with Beta Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/4b04a686b0ad13dce35fa99fa4161c65-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4b04a686b0ad13dce35fa99fa4161c65-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1006138,
        "gs_citation": 188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6618110870206279414&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Electrical Engineering & Computer Science, Massachusetts Institute of Technology; Computer Science, Brown University; Electrical Engineering & Computer Science and Statistics, University of California, Berkeley; Electrical Engineering & Computer Science, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;cs.brown.edu;cs.berkeley.edu;mit.edu",
        "email": "mit.edu;cs.brown.edu;cs.berkeley.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Brown University;University of California, Berkeley",
        "aff_unique_dep": "Electrical Engineering & Computer Science;Computer Science;Electrical Engineering & Computer Science and Statistics",
        "aff_unique_url": "https://web.mit.edu;https://www.brown.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "MIT;Brown;UC Berkeley",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Cambridge;;Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e2538e898b",
        "title": "Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e0cf1f47118daebc5b16269099ad7347-Abstract.html",
        "author": "George Konidaris; Andrew G. Barto",
        "abstract": "We introduce skill chaining, a skill discovery method for reinforcement learning agents in continuous domains, that builds chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates skills that result in performance benefits in a challenging continuous domain.",
        "bibtex": "@inproceedings{NIPS2009_e0cf1f47,\n author = {Konidaris, George and Barto, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/e0cf1f47118daebc5b16269099ad7347-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e0cf1f47118daebc5b16269099ad7347-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 336344,
        "gs_citation": 406,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18382427864599558065&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "Computer Science Department, University of Massachusetts Amherst; Computer Science Department, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "828c8dd779",
        "title": "Slow Learners are Fast",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/b55ec28c52d5f6205684a473a2193564-Abstract.html",
        "author": "Martin Zinkevich; John Langford; Alex J. Smola",
        "abstract": "Online learning algorithms have impressive convergence properties   when it comes to risk minimization and convex games on very large   problems. However, they are inherently sequential in their design   which prevents them from taking advantage of modern multi-core   architectures. In this paper we prove that online learning with   delayed updates converges well, thereby facilitating parallel online   learning.",
        "bibtex": "@inproceedings{NIPS2009_b55ec28c,\n author = {Zinkevich, Martin and Langford, John and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Slow Learners are Fast},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/b55ec28c52d5f6205684a473a2193564-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 323825,
        "gs_citation": 332,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1441780924368199789&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Machine Learning; Yahoo! Labs; Australian National University",
        "aff_domain": "yahoo-inc.com;yahoo-inc.com;yahoo-inc.com",
        "email": "yahoo-inc.com;yahoo-inc.com;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Machine Learning;Yahoo!;Australian National University",
        "aff_unique_dep": ";Yahoo! Labs;",
        "aff_unique_url": ";https://yahoo.com;https://www.anu.edu.au",
        "aff_unique_abbr": ";Yahoo!;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";United States;Australia"
    },
    {
        "id": "ce67793579",
        "title": "Slow, Decorrelated Features for Pretraining Complex Cell-like Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/043c3d7e489c69b48737cc0c92d0f3a2-Abstract.html",
        "author": "Yoshua Bengio; James S. Bergstra",
        "abstract": "We introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area V1.  A single-hidden-layer neural network of this kind of model achieves 1.5% error on MNIST.  We also introduce an existing criterion for learning slow, decorrelated features as a pretraining strategy for image models.  This pretraining strategy results in orientation-selective features, similar to the receptive fields of complex cells.  With this pretraining, the same single-hidden-layer model achieves better generalization error, even though the pretraining sample distribution is very different from the fine-tuning distribution.  To implement this pretraining strategy, we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features.",
        "bibtex": "@inproceedings{NIPS2009_043c3d7e,\n author = {Bengio, Yoshua and Bergstra, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Slow, Decorrelated Features for Pretraining Complex Cell-like Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/043c3d7e489c69b48737cc0c92d0f3a2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1569021,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8870510384768144966&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Montreal; University of Montreal",
        "aff_domain": "umontreal.ca;umontreal.ca",
        "email": "umontreal.ca;umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Montreal",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wwwumontreal.ca",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "27af294404",
        "title": "Solving Stochastic Games",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html",
        "author": "Liam M. Dermed; Charles L. Isbell",
        "abstract": "Solving multi-agent reinforcement learning problems has proven difficult because of the lack of tractable algorithms.  We provide the first approximation algorithm which solves stochastic games to within $\\epsilon$ relative error of the optimal game-theoretic solution, in time polynomial in $1/\\epsilon$. Our algorithm extends Murrays and Gordon\u2019s (2007) modified Bellman equation which determines the \\emph{set} of all possible achievable utilities; this provides us a truly general framework for multi-agent learning. Further, we empirically validate our algorithm and find the computational cost to be orders of magnitude less than what the theory predicts.",
        "bibtex": "@inproceedings{NIPS2009_5680522b,\n author = {Dermed, Liam and Isbell, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Solving Stochastic Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/5680522b8e2bb01943234bce7bf84534-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1342018,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8459884951909077354&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "College of Computing, Georgia Tech; College of Computing, Georgia Tech",
        "aff_domain": "cc.gatech.edu;cc.gatech.edu",
        "email": "cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "College of Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2ec760e8ea",
        "title": "Sparse Estimation Using General Likelihoods and Non-Factorial Priors",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1ce927f875864094e3906a4a0b5ece68-Abstract.html",
        "author": "David P. Wipf; Srikantan S. Nagarajan",
        "abstract": "Finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood (or data fit) term and a prior (or penalty function) that favors sparsity.  While typically the prior is factorial, here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efficient, globally-convergent reweighted $\\ell_1$ minimization procedure.  The first method under consideration arises from the sparse Bayesian learning (SBL) framework.  Although based on a highly non-convex underlying cost function, in the context of canonical sparse estimation problems, we prove uniform superiority of this method over the Lasso in that, (i) it can never do worse, and (ii) for any dictionary and sparsity profile, there will always exist cases where it does better.  These results challenge the prevailing reliance on strictly convex penalty functions for finding sparse solutions.  We then derive a new non-factorial variant with similar properties that exhibits further performance improvements in empirical tests.  For both of these methods, as well as traditional factorial analogs, we demonstrate the effectiveness of reweighted $\\ell_1$-norm algorithms in handling more general sparse estimation problems involving classification, group feature selection, and non-negativity constraints.  As a byproduct of this development, a rigorous reformulation of sparse Bayesian classification (e.g., the relevance vector machine) is derived that, unlike the original, involves no approximation steps and descends a well-defined objective function.",
        "bibtex": "@inproceedings{NIPS2009_1ce927f8,\n author = {Wipf, David and Nagarajan, Srikantan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Estimation Using General Likelihoods and Non-Factorial Priors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1ce927f875864094e3906a4a0b5ece68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 133291,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7518075353919253457&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1d4393343a",
        "title": "Sparse Metric Learning via Smooth Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html",
        "author": "Yiming Ying; Kaizhu Huang; Colin Campbell",
        "abstract": "In this paper we study the problem of learning a low-dimensional (sparse)  distance matrix.  We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is  non-convex.  We then show that it can be equivalently formulated  as a convex saddle (min-max) problem. From this saddle representation, we develop an efficient smooth optimization approach for sparse metric learning although the learning model is based on a non-differential loss function. This smooth optimization approach has an optimal convergence rate of $O(1 /\\ell^2)$ for smooth problems where $\\ell$ is the iteration number. Finally, we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets.",
        "bibtex": "@inproceedings{NIPS2009_37bc2f75,\n author = {Ying, Yiming and Huang, Kaizhu and Campbell, Colin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Metric Learning via Smooth Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/37bc2f75bf1bcfe8450a1a41c200364c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 230842,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3750917035493463247&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Engineering Mathematics, University of Bristol; National Laboratory of Pattern Recognition, Institute of Automation, The Chinese Academy of Sciences; Department of Engineering Mathematics, University of Bristol",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Bristol;Chinese Academy of Sciences",
        "aff_unique_dep": "Department of Engineering Mathematics;Institute of Automation",
        "aff_unique_url": "https://www.bristol.ac.uk;http://www.ia.cas.cn",
        "aff_unique_abbr": "UoB;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "d078ecc37b",
        "title": "Sparse and Locally Constant Gaussian Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/37693cfc748049e45d87b8c7d8b9aacd-Abstract.html",
        "author": "Jean Honorio; Dimitris Samaras; Nikos Paragios; Rita Goldstein; Luis E. Ortiz",
        "abstract": "Locality information is crucial in datasets where each variable corresponds to a measurement in a manifold (silhouettes, motion trajectories, 2D and 3D images). Although these datasets are typically under-sampled and high-dimensional, they often need to be represented with low-complexity statistical models, which are comprised of only the important probabilistic dependencies in the datasets. Most methods attempt to reduce model complexity by enforcing structure sparseness. However, sparseness cannot describe inherent regularities in the structure. Hence, in this paper we first propose a new class of Gaussian graphical models which, together with sparseness, imposes local constancy through ${\\ell}_1$-norm penalization. Second, we propose an efficient algorithm which decomposes the strictly convex maximum likelihood estimation into a sequence of problems with closed form solutions. Through synthetic experiments, we evaluate the closeness of the recovered models to the ground truth. We also test the generalization performance of our method in a wide range of complex real-world datasets and demonstrate that it can capture useful structures such as the rotation and shrinking of a beating heart, motion correlations between body parts during walking and functional interactions of brain regions. Our method outperforms the state-of-the-art structure learning techniques for Gaussian graphical models both for small and large datasets.",
        "bibtex": "@inproceedings{NIPS2009_37693cfc,\n author = {Honorio, Jean and Samaras, Dimitris and Paragios, Nikos and Goldstein, Rita and Ortiz, Luis E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse and Locally Constant Gaussian Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/37693cfc748049e45d87b8c7d8b9aacd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/37693cfc748049e45d87b8c7d8b9aacd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 333483,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11591351673114311328&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University; Laboratoire MAS, Ecole Centrale Paris; Medical Department, Brookhaven National Laboratory",
        "aff_domain": "cs.sunysb.edu;cs.sunysb.edu;cs.sunysb.edu;ecp.fr;bnl.gov",
        "email": "cs.sunysb.edu;cs.sunysb.edu;cs.sunysb.edu;ecp.fr;bnl.gov",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Stony Brook University;Ecole Centrale Paris;Brookhaven National Laboratory",
        "aff_unique_dep": "Department of Computer Science;Laboratoire MAS;Medical Department",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.ecp.fr;https://www.bnl.gov",
        "aff_unique_abbr": "SBU;ECP;BNL",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Stony Brook;Paris;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "e55ac6558e",
        "title": "Sparsistent Learning of Varying-coefficient Models with Structural Changes",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html",
        "author": "Mladen Kolar; Le Song; Eric P. Xing",
        "abstract": "To estimate the changing structure of a varying-coefficient   varying-structure (VCVS) model remains an important and open problem   in dynamic system modelling, which includes learning trajectories of   stock prices, or uncovering the topology of an evolving gene   network. In this paper, we investigate sparsistent learning of a   sub-family of this model --- piecewise constant VCVS models. We   analyze two main issues in this problem: inferring time points where   structural changes occur and estimating model structure (i.e., model   selection) on each of the constant segments. We propose a two-stage   adaptive procedure, which first identifies jump points of structural   changes and then identifies relevant covariates to a response on   each of the segments. We provide an asymptotic analysis of the   procedure, showing that with the increasing sample size, number of   structural changes, and number of variables, the true model can be   consistently selected. We demonstrate the performance of the method   on synthetic data and apply it to the brain computer interface   dataset. We also consider how this applies to structure estimation   of time-varying probabilistic graphical models.",
        "bibtex": "@inproceedings{NIPS2009_7bcdf75a,\n author = {Kolar, Mladen and Song, Le and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparsistent Learning of Varying-coefficient Models with Structural Changes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/7bcdf75ad237b8e02e301f4091fb6bc8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/7bcdf75ad237b8e02e301f4091fb6bc8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 135870,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13071364181831033204&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "19b4acbda1",
        "title": "Spatial Normalized Gamma Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0537fb40a68c18da59a35c2bfe1ca554-Abstract.html",
        "author": "Vinayak Rao; Yee W. Teh",
        "abstract": "Dependent Dirichlet processes (DPs) are dependent sets of random measures, each being marginally Dirichlet process distributed. They are used in Bayesian nonparametric models when the usual exchangebility assumption does not hold. We propose a simple and general framework to construct dependent DPs by marginalizing and normalizing a single gamma process over an extended space. The result is a set of DPs, each located at a point in a space such that  neighboring DPs are more dependent. We describe Markov chain Monte Carlo inference, involving the typical Gibbs sampling and three different Metropolis-Hastings proposals to speed up convergence. We report an empirical study of convergence speeds on a synthetic dataset and demonstrate an application of the model to topic modeling through time.",
        "bibtex": "@inproceedings{NIPS2009_0537fb40,\n author = {Rao, Vinayak and Teh, Yee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spatial Normalized Gamma Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0537fb40a68c18da59a35c2bfe1ca554-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0537fb40a68c18da59a35c2bfe1ca554-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0537fb40a68c18da59a35c2bfe1ca554-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 304646,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5344371442691173881&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "9ff1f6c1d1",
        "title": "Speaker Comparison with Inner Product Discriminant Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html",
        "author": "Zahi Karam; Douglas Sturim; William M. Campbell",
        "abstract": "Speaker comparison, the process of finding the speaker similarity between two speech signals, occupies a central role in a variety of applications---speaker verification, clustering, and identification. Speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process.  For a given speech signal, feature vectors are produced and used to adapt a Gaussian mixture model (GMM).  Speaker comparison can then be viewed as the process of compensating and finding metrics on the space of adapted models.  We propose a framework, inner product discriminant functions (IPDFs), which extends many common techniques for speaker comparison: support vector machines, joint factor analysis, and linear scoring.  The framework uses inner products between the parameter vectors of GMM models motivated by several statistical methods.  Compensation of nuisances is performed via linear transforms on GMM parameter vectors.  Using the IPDF framework, we show that many current techniques are simple variations of each other.  We demonstrate, on a 2006 NIST speaker recognition evaluation task, new scoring methods using IPDFs which produce excellent error rates and require significantly less computation than current techniques.",
        "bibtex": "@inproceedings{NIPS2009_bac9162b,\n author = {Karam, Zahi and Sturim, Douglas and Campbell, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Speaker Comparison with Inner Product Discriminant Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/bac9162b47c56fc8a4d2a519803d51b3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 90730,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14193635886161226341&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "MIT Lincoln Laboratory, Lexington, MA 02420; DSPG, MIT RLE, Cambridge, MA + MIT Lincoln Laboratory, Lexington, MA; MIT Lincoln Laboratory, Lexington, MA 02420",
        "aff_domain": "ll.mit.edu;mit.edu;ll.mit.edu",
        "email": "ll.mit.edu;mit.edu;ll.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology Lincoln Laboratory;Massachusetts Institute of Technology",
        "aff_unique_dep": "Lincoln Laboratory;Research Laboratory of Electronics",
        "aff_unique_url": "https://www.ll.mit.edu;https://web.mit.edu",
        "aff_unique_abbr": "MIT LL;MIT",
        "aff_campus_unique_index": "0;1+0;0",
        "aff_campus_unique": "Lexington;Cambridge",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8e4b19a890",
        "title": "Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6d70cb65d15211726dcce4c0e971e21c-Abstract.html",
        "author": "Matthias Seeger",
        "abstract": "We show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices, by performing convex variational inference on a large scale non-Gaussian linear dynamical system, tracking dominating directions of posterior covariance without imposing any factorization constraints. Our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels. In a first study, designs are found that improve significantly on others chosen independently for each slice or drawn at random.",
        "bibtex": "@inproceedings{NIPS2009_6d70cb65,\n author = {Seeger, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/6d70cb65d15211726dcce4c0e971e21c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6d70cb65d15211726dcce4c0e971e21c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 597377,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5542093136429352349&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Saarland University and Max Planck Institute for Informatics",
        "aff_domain": "mmci.uni-saarland.de",
        "email": "mmci.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "Saarland U",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "c18cc183f1",
        "title": "Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/68ce199ec2c5517597ce0a4d89620f55-Abstract.html",
        "author": "Boaz Nadler; Nathan Srebro; Xueyuan Zhou",
        "abstract": "We study the behavior of the popular Laplacian Regularization method for Semi-Supervised Learning at the regime of a fixed number of labeled points but a large number of unlabeled points.  We show that in $\\R^d$, $d \\geq 2$, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function.  We also contrast the method with the Laplacian Eigenvector method, and discuss the ``smoothness assumptions associated with this alternate method.",
        "bibtex": "@inproceedings{NIPS2009_68ce199e,\n author = {Nadler, Boaz and Srebro, Nathan and Zhou, Xueyuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/68ce199ec2c5517597ce0a4d89620f55-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 468026,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Dept. of Computer Science and Applied Mathematics, Weizmann Institute of Science; Toyota Technological Institute; Dept. of Computer Science, University of Chicago",
        "aff_domain": "weizmann.ac.il;uchicago.edu;cs.uchicago.edu",
        "email": "weizmann.ac.il;uchicago.edu;cs.uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Weizmann Institute of Science;Toyota Technological Institute;University of Chicago",
        "aff_unique_dep": "Dept. of Computer Science and Applied Mathematics;;Dept. of Computer Science",
        "aff_unique_url": "https://www.weizmann.ac.il;https://www.tti.ac.jp;https://www.uchicago.edu",
        "aff_unique_abbr": "Weizmann;TTI;UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Israel;Japan;United States"
    },
    {
        "id": "7042e45d2b",
        "title": "Statistical Consistency of Top-k Ranking",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ba2fd310dcaa8781a9a652a31baf3c68-Abstract.html",
        "author": "Fen Xia; Tie-yan Liu; Hang Li",
        "abstract": "This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches. Most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important. This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we define a top-k ranking framework, where the true loss (and thus the risks) are defined on the basis of top-k subgroup of permutations. This framework can include the permutation-level ranking framework proposed in previous work as a special case. Based on the new framework, we derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions. Experimental results show that after the modifications, the methods can work significantly better than their original versions.",
        "bibtex": "@inproceedings{NIPS2009_ba2fd310,\n author = {Xia, Fen and Liu, Tie-yan and Li, Hang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Statistical Consistency of Top-k Ranking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ba2fd310dcaa8781a9a652a31baf3c68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 84024,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15547131302040047728&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Institute of Automation, Chinese Academy of Sciences; Microsoft Research Asia; Microsoft Research Asia",
        "aff_domain": "ia.ac.cn;microsoft.com;microsoft.com",
        "email": "ia.ac.cn;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Chinese Academy of Sciences;Microsoft",
        "aff_unique_dep": "Institute of Automation;Research",
        "aff_unique_url": "http://www.ia.cas.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "CAS;MSR Asia",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "27ee5968a3",
        "title": "Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/be3159ad04564bfb90db9e32851ebf9c-Abstract.html",
        "author": "Ruben Coen-cagli; Peter Dayan; Odelia Schwartz",
        "abstract": "A central hypothesis about early visual processing is that it represents inputs in a coordinate system matched to the statistics of natural scenes. Simple versions of this lead to Gabor-like receptive fields and divisive gain modulation from local surrounds; these have led to influential neural and psychological models of visual processing. However, these accounts are based on an incomplete view of the visual context surrounding each point. Here, we consider an approximate model of linear and non-linear correlations between the responses of spatially distributed Gabor-like receptive fields, which, when trained on an ensemble of natural scenes, unifies a range of spatial context effects. The full model accounts for neural surround data in primary visual cortex (V1), provides a statistical foundation for perceptual phenomena associated with Lis (2002) hypothesis that V1 builds a saliency map, and fits data on the tilt illusion.",
        "bibtex": "@inproceedings{NIPS2009_be3159ad,\n author = {Coen-cagli, Ruben and Dayan, Peter and Schwartz, Odelia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/be3159ad04564bfb90db9e32851ebf9c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 496052,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16167833176167633063&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "AECOM; GCNU, UCL; AECOM",
        "aff_domain": "aecom.yu.edu;gatsby.ucl.ac.uk;aecom.yu.edu",
        "email": "aecom.yu.edu;gatsby.ucl.ac.uk;aecom.yu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "AECOM;University College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.aecom.com;https://www.ucl.ac.uk",
        "aff_unique_abbr": "AECOM;UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "eeeed0d0be",
        "title": "Strategy Grafting in Extensive Games",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e0ec453e28e061cc58ac43f91dc2f3f0-Abstract.html",
        "author": "Kevin Waugh; Nolan Bard; Michael Bowling",
        "abstract": "Extensive games are often used to model the interactions of multiple agents within an environment.  Much recent work has focused on increasing the size of an extensive game that can be feasibly solved.  Despite these improvements, many interesting games are still too large for such techniques.  A common approach for computing strategies in these large games is to first employ an abstraction technique to reduce the original game to an abstract game that is of a manageable size.  This abstract game is then solved and the resulting strategy is used in the original game.  Most top programs in recent AAAI Computer Poker Competitions use this approach.  The trend in this competition has been that strategies found in larger abstract games tend to beat strategies found in smaller abstract games.  These larger abstract games have more expressive strategy spaces and therefore contain better strategies.  In this paper we present a new method for computing strategies in large games.  This method allows us to compute more expressive strategies without increasing the size of abstract games that we are required to solve.  We demonstrate the power of the approach experimentally in both small and large games, while also providing a theoretical justification for the resulting improvement.",
        "bibtex": "@inproceedings{NIPS2009_e0ec453e,\n author = {Waugh, Kevin and Bard, Nolan and Bowling, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Strategy Grafting in Extensive Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e0ec453e28e061cc58ac43f91dc2f3f0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e0ec453e28e061cc58ac43f91dc2f3f0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e0ec453e28e061cc58ac43f91dc2f3f0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 169098,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13445712277760329860&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, Carnegie Mellon University; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.cmu.edu;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.cmu.edu;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Carnegie Mellon University;University of Alberta",
        "aff_unique_dep": "Department of Computer Science;Department of Computing Science",
        "aff_unique_url": "https://www.cmu.edu;https://www.ualberta.ca",
        "aff_unique_abbr": "CMU;UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "d255552e2c",
        "title": "Streaming Pointwise Mutual Information",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/185c29dc24325934ee377cfda20e414c-Abstract.html",
        "author": "Benjamin V. Durme; Ashwin Lall",
        "abstract": "Recent work has led to the ability to perform space ef\ufb01cient, approximate counting  over large vocabularies in a streaming context. Motivated by the existence of data  structures of this type, we explore the computation of associativity scores, other-  wise known as pointwise mutual information (PMI), in a streaming context. We  give theoretical bounds showing the impracticality of perfect online PMI compu-  tation, and detail an algorithm with high expected accuracy. Experiments on news  articles show our approach gives high accuracy on real world data.",
        "bibtex": "@inproceedings{NIPS2009_185c29dc,\n author = {Durme, Benjamin and Lall, Ashwin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Streaming Pointwise Mutual Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/185c29dc24325934ee377cfda20e414c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/185c29dc24325934ee377cfda20e414c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/185c29dc24325934ee377cfda20e414c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 344382,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=241857374758644315&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of Rochester; Georgia Institute of Technology",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Rochester;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rochester.edu;https://www.gatech.edu",
        "aff_unique_abbr": "U of R;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d186aa8635",
        "title": "Streaming k-means approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4f16c818875d9fcb6867c7bdc89be7eb-Abstract.html",
        "author": "Nir Ailon; Ragesh Jaiswal; Claire Monteleoni",
        "abstract": "We provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting. We make no assumptions about the data, and  our algorithm is very light-weight in terms of memory, and computation. This setting is applicable to unsupervised learning on massive data sets, or resource-constrained devices. The two main ingredients of our theoretical work are:  a derivation of an extremely simple pseudo-approximation batch algorithm for k-means, in which the algorithm is allowed to output more than k centers (based on the recent k-means++\"), and a streaming clustering algorithm in which batch clustering algorithms are performed on small inputs (fitting in memory) and combined in a hierarchical manner.  Empirical evaluations on real and simulated data reveal the practical utility of our method.\"",
        "bibtex": "@inproceedings{NIPS2009_4f16c818,\n author = {Ailon, Nir and Jaiswal, Ragesh and Monteleoni, Claire},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Streaming k-means approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4f16c818875d9fcb6867c7bdc89be7eb-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4f16c818875d9fcb6867c7bdc89be7eb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/4f16c818875d9fcb6867c7bdc89be7eb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4f16c818875d9fcb6867c7bdc89be7eb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 366564,
        "gs_citation": 284,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10623308770470217706&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Google Research; Columbia University; Columbia University",
        "aff_domain": "google.com;gmail.com;ccls.columbia.edu",
        "email": "google.com;gmail.com;ccls.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Google;Columbia University",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.columbia.edu",
        "aff_unique_abbr": "Google Research;Columbia",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cbc90f8057",
        "title": "Structural inference affects depth perception in the context of potential occlusion",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html",
        "author": "Ian Stevenson; Konrad Koerding",
        "abstract": "In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target \u2013 in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then fit the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We find that the model accurately predicts shifts in subject\u2019s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception.",
        "bibtex": "@inproceedings{NIPS2009_f197002b,\n author = {Stevenson, Ian and Koerding, Konrad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structural inference affects depth perception in the context of potential occlusion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f197002b9a0853eca5e046d9ca4663d5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4253144,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=96362046890046822&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Physical Medicine and Rehabilitation, Northwestern University; Department of Physical Medicine and Rehabilitation, Northwestern University",
        "aff_domain": "northwestern.edu; ",
        "email": "northwestern.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Northwestern University",
        "aff_unique_dep": "Department of Physical Medicine and Rehabilitation",
        "aff_unique_url": "https://www.northwestern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "740b46a33e",
        "title": "Structured output regression for detection with partial truncation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2a38a4a9316c49e5a833517c45d31070-Abstract.html",
        "author": "Andrea Vedaldi; Andrew Zisserman",
        "abstract": "We develop a structured output model for object category detection that explicitly accounts for alignment, multiple aspects and partial truncation in both training and inference. The model is formulated as large margin learning with latent variables and slack rescaling, and both training and inference are computationally efficient. We make the following contributions: (i) we note that extending the Structured Output Regression formulation of Blaschko and Lampert (ECCV 2008) to include a bias term significantly improves performance; (ii) that alignment (to account for small rotations and anisotropic scalings) can be included as a latent variable and efficiently determined and implemented; (iii) that the latent variable extends to multiple aspects (e.g. left facing, right facing, front) with the same formulation; and (iv), most significantly for performance, that truncated and truncated instances can be included in both training and inference with an explicit truncation mask. We demonstrate the method by training and testing on the PASCAL VOC 2007 data set -- training includes the truncated examples, and in testing object instances are detected at multiple scales, alignments, and with significant truncations.",
        "bibtex": "@inproceedings{NIPS2009_2a38a4a9,\n author = {Vedaldi, Andrea and Zisserman, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured output regression for detection with partial truncation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2a38a4a9316c49e5a833517c45d31070-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 426274,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5105879421058646891&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Engineering, University of Oxford, Oxford, UK; Department of Engineering, University of Oxford, Oxford, UK",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "8aececfe79",
        "title": "Subject independent EEG-based BCI decoding",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html",
        "author": "Siamac Fazli; Cristian Grozea; Marton Danoczy; Benjamin Blankertz; Florin Popescu; Klaus-Robert M\u00fcller",
        "abstract": "In the quest to make Brain Computer Interfacing (BCI) more usable, dry electrodes have emerged that get rid of the initial 30 minutes required for placing an electrode cap.  Another time consuming step is the required individualized adaptation to the BCI user, which involves another 30 minutes calibration for assessing a subjects brain signature. In this paper we aim to also remove this calibration proceedure from BCI setup time by means of machine learning. In particular, we harvest a large database of EEG BCI motor imagination recordings (83 subjects) for constructing a library of subject-specific spatio-temporal filters and derive a subject independent BCI classifier. Our offline results indicate that BCI-na{i}ve users could start real-time BCI use with no prior calibration at only a very moderate performance loss.\"",
        "bibtex": "@inproceedings{NIPS2009_8a1e808b,\n author = {Fazli, Siamac and Grozea, Cristian and Danoczy, Marton and Blankertz, Benjamin and Popescu, Florin and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Subject independent EEG-based BCI decoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/8a1e808b55fde9455cb3d8857ed88389-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 817416,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4636422179128558571&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5df188325b",
        "title": "Submanifold density estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/2ac2406e835bd49c70469acae337d292-Abstract.html",
        "author": "Arkadas Ozakin; Alexander G. Gray",
        "abstract": "Kernel density estimation is the most widely-used practical method for accurate nonparametric density estimation. However, long-standing worst-case theoretical results showing that its performance worsens exponentially with the dimension of the data have quashed its application to modern high-dimensional datasets for decades. In practice, it has been recognized that often such data have a much lower-dimensional intrinsic structure. We propose a small modification to kernel density estimation for estimating probability density functions on Riemannian submanifolds of Euclidean space. Using ideas from Riemannian geometry, we prove the consistency of this modified estimator and show that the convergence rate is determined by the intrinsic dimension of the submanifold. We conclude with empirical results demonstrating the behavior predicted by our theory.",
        "bibtex": "@inproceedings{NIPS2009_2ac2406e,\n author = {Ozakin, Arkadas and Gray, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Submanifold density estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/2ac2406e835bd49c70469acae337d292-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/2ac2406e835bd49c70469acae337d292-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/2ac2406e835bd49c70469acae337d292-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 119917,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1792215807836715677&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Georgia Tech Research Institute + Georgia Institute of Technology; College of Computing, Georgia Institute of Technology",
        "aff_domain": "gtri.gatech.edu;cc.gatech.edu",
        "email": "gtri.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "Georgia Tech Research Institute;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gtri.gatech.edu;https://www.gatech.edu",
        "aff_unique_abbr": "GTRI;Georgia Tech",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Atlanta",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "de46db426c",
        "title": "Submodularity Cuts and Applications",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html",
        "author": "Yoshinobu Kawahara; Kiyohito Nagano; Koji Tsuda; Jeff A. Bilmes",
        "abstract": "Several key problems in machine learning, such as feature selection and active learning, can be formulated as submodular set function maximization.  We present herein a novel algorithm for maximizing a submodular set function under a cardinality constraint --- the algorithm is based on a cutting-plane method and is implemented as an iterative small-scale binary-integer linear programming procedure. It is well known that this problem is NP-hard, and the approximation factor achieved by the greedy algorithm is the theoretical limit for polynomial time. As for (non-polynomial time) exact algorithms that perform reasonably in practice, there has been very little in the literature although the problem is quite important for many applications. Our algorithm is guaranteed to find the exact solution in finite iterations, and it converges fast in practice due to the efficiency of the cutting-plane mechanism. Moreover, we also provide a method that produces successively decreasing upper-bounds of the optimal solution, while our algorithm provides successively increasing lower-bounds.  Thus, the accuracy of the current solution can be estimated at any point, and the algorithm can be stopped early once a desired degree of tolerance is met.  We evaluate our algorithm on sensor placement and feature selection applications showing good performance.",
        "bibtex": "@inproceedings{NIPS2009_9ad6aaed,\n author = {Kawahara, Yoshinobu and Nagano, Kiyohito and Tsuda, Koji and Bilmes, Jeff A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Submodularity Cuts and Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/9ad6aaed513b73148b7d49f70afcfb32-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/9ad6aaed513b73148b7d49f70afcfb32-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146886,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12183571073928042962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "The Inst. of Scientific and Industrial Res. (ISIR), Osaka Univ., Japan; Dept. of Math. and Comp. Sci., Tokyo Inst. of Technology, Japan; Comp. Bio. Research Center, AIST, Japan; Dept. of Electrical Engineering, Univ. of Washington, USA",
        "aff_domain": "ar.sanken.osaka-u.ac.jp;is.titech.ac.jp;aist.go.jp;u.washington.edu",
        "email": "ar.sanken.osaka-u.ac.jp;is.titech.ac.jp;aist.go.jp;u.washington.edu",
        "github": "",
        "project": "http://www.ar.sanken.osaka-u.ac.jp/kawahara/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Osaka University;Tokyo Institute of Technology;Advanced Institute of Science and Technology;University of Washington",
        "aff_unique_dep": "Institute of Scientific and Industrial Research;Department of Mathematics and Computer Science;Computational Biology Research Center;Department of Electrical Engineering",
        "aff_unique_url": "https://www.osaka-u.ac.jp;https://www.titech.ac.jp;https://www.aist.go.jp;https://www.washington.edu",
        "aff_unique_abbr": "Osaka U;Tokyo Tech;AIST;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "68b48e6b8d",
        "title": "Sufficient Conditions for Agnostic Active Learnable",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/b1d10e7bafa4421218a51b1e1f1b0ba2-Abstract.html",
        "author": "Liwei Wang",
        "abstract": "We study pool-based active learning in the presence of noise, i.e. the agnostic setting. Previous works have shown that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have advantage. In this paper, we propose intuitively reasonable sufficient conditions under which agnostic active learning algorithm is strictly superior to passive supervised learning. We show that under some noise condition, if the classification boundary and the underlying distribution are smooth to a finite order, active learning achieves polynomial improvement in the label complexity; if the boundary and the distribution are infinitely smooth, the improvement is exponential.",
        "bibtex": "@inproceedings{NIPS2009_b1d10e7b,\n author = {Wang, Liwei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sufficient Conditions for Agnostic Active Learnable},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 135761,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12129474870166333897&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "KeyLaboratory of Machine Perception, MOE, School of Electronics Engineering and Computer Science, Peking University",
        "aff_domain": "cis.pku.edu.cn",
        "email": "cis.pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of Electronics Engineering and Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "6c3950106d",
        "title": "The 'tree-dependent components' of natural scenes are edge filters",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/37f0e884fbad9667e38940169d0a3c95-Abstract.html",
        "author": "Daniel Zoran; Yair Weiss",
        "abstract": "We propose a new model for natural image statistics. Instead of minimizing dependency between components of natural images, we maximize a simple form of dependency in the form of tree-dependency. By learning filters and tree structures which are best suited for natural images we observe that the resulting filters are edge filters, similar to the famous ICA on natural images results. Calculating the likelihood of the model requires estimating the squared output of pairs of filters connected in the tree. We observe that after learning, these pairs of filters are predominantly of similar orientations but different phases, so their joint energy resembles models of complex cells.",
        "bibtex": "@inproceedings{NIPS2009_37f0e884,\n author = {Zoran, Daniel and Weiss, Yair},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The \\textquotesingle tree-dependent components\\textquotesingle  of natural scenes are edge filters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/37f0e884fbad9667e38940169d0a3c95-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/37f0e884fbad9667e38940169d0a3c95-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/37f0e884fbad9667e38940169d0a3c95-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 679372,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1779771966224574164&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Interdisciplinary Center for Neural Computation, Hebrew University of Jerusalem; School of Computer Science, Hebrew University of Jerusalem",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "Interdisciplinary Center for Neural Computation",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "3829ab704a",
        "title": "The Infinite Partially Observable Markov Decision Process",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ebd9629fc3ae5e9f6611e2ee05a31cef-Abstract.html",
        "author": "Finale Doshi-velez",
        "abstract": "The Partially Observable Markov Decision Process (POMDP) framework has proven useful in planning domains that require balancing actions that increase an agents knowledge and actions that increase an agents reward.  Unfortunately, most POMDPs are complex structures with a large number of parameters.  In many realworld problems, both the structure and the parameters are difficult to specify from domain knowledge alone.  Recent work in Bayesian reinforcement learning has made headway in learning POMDP models; however, this work has largely focused on learning the parameters of the POMDP model.  We define an infinite POMDP (iPOMDP) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and explicitly models only visited states.  We demonstrate the iPOMDP utility on several standard problems.",
        "bibtex": "@inproceedings{NIPS2009_ebd9629f,\n author = {Doshi-velez, Finale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Infinite Partially Observable Markov Decision Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 132790,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10768110427383167189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Cambridge University",
        "aff_domain": "alum.mit.edu",
        "email": "alum.mit.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "9ff388933a",
        "title": "The Ordered Residual Kernel for Robust Motion Subspace Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/b337e84de8752b27eda3a12363109e80-Abstract.html",
        "author": "Tat-jun Chin; Hanzi Wang; David Suter",
        "abstract": "We present a novel and highly effective approach for multi-body motion segmentation. Drawing inspiration from robust statistical model fitting, we estimate putative subspace hypotheses from the data. However, instead of ranking them we encapsulate the hypotheses in a novel Mercer kernel which elicits the potential of two point trajectories to have emerged from the same subspace. The kernel permits the application of well-established statistical learning methods for effective outlier rejection, automatic recovery of the number of motions and accurate segmentation of the point trajectories. The method operates well under severe outliers arising from spurious trajectories or mistracks. Detailed experiments on a recent benchmark dataset (Hopkins 155) show that our method is superior to other state-of-the-art approaches in terms of recovering the number of motions, segmentation accuracy, robustness against gross outliers and computational efficiency.",
        "bibtex": "@inproceedings{NIPS2009_b337e84d,\n author = {Chin, Tat-jun and Wang, Hanzi and Suter, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Ordered Residual Kernel for Robust Motion Subspace Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/b337e84de8752b27eda3a12363109e80-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/b337e84de8752b27eda3a12363109e80-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/b337e84de8752b27eda3a12363109e80-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 256847,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8029276471808738462&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, The University of Adelaide, South Australia; School of Computer Science, The University of Adelaide, South Australia; School of Computer Science, The University of Adelaide, South Australia",
        "aff_domain": "cs.adelaide.edu.au;cs.adelaide.edu.au;cs.adelaide.edu.au",
        "email": "cs.adelaide.edu.au;cs.adelaide.edu.au;cs.adelaide.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Adelaide",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "b2e6caba13",
        "title": "The Wisdom of Crowds in the Recollection of Order Information",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4c27cea8526af8cfee3be5e183ac9605-Abstract.html",
        "author": "Mark Steyvers; Brent Miller; Pernille Hemmer; Michael D. Lee",
        "abstract": "When individuals independently recollect events or retrieve facts from memory, how can we aggregate these retrieved memories to reconstruct the actual set of events or facts? In this research, we report the performance of individuals in a series of general knowledge tasks, where the goal is to reconstruct from memory the order of historic events, or the order of items along some physical dimension. We introduce two Bayesian models for aggregating order information based on a Thurstonian approach and Mallows model. Both models assume that each individuals reconstruction is based on either a random permutation of the unobserved ground truth, or by a pure guessing strategy. We apply MCMC to make inferences about the underlying truth and the strategies employed by individuals. The models demonstrate a wisdom of crowds\" effect, where the aggregated orderings are closer to the true ordering than the orderings of the best individual.\"",
        "bibtex": "@inproceedings{NIPS2009_4c27cea8,\n author = {Steyvers, Mark and Miller, Brent and Hemmer, Pernille and Lee, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Wisdom of Crowds in the Recollection of Order Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/4c27cea8526af8cfee3be5e183ac9605-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4c27cea8526af8cfee3be5e183ac9605-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 567848,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10446814622235985332&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Cognitive Sciences, University of California Irvine; Department of Cognitive Sciences, University of California Irvine; Department of Cognitive Sciences, University of California Irvine; Department of Cognitive Sciences, University of California Irvine",
        "aff_domain": "uci.edu; ; ; ",
        "email": "uci.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Cognitive Sciences",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d2e141cd83",
        "title": "Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/92fb0c6d1758261f10d052e6e2c1123c-Abstract.html",
        "author": "Shuheng Zhou",
        "abstract": "Given $n$ noisy samples with $p$ dimensions, where $n \\ll p$, we show that the multi-stage thresholding procedures can accurately estimate a sparse vector $\\beta \\in \\R^p$ in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very significant values of $s$, which is the number of non-zero elements in the true parameter $\\beta$. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if $X$ obeys a uniform uncertainty principle and if the true parameter is sufficiently sparse, the Gauss-Dantzig selector (Cand\\{e}s-Tao 07) achieves the $\\ell_2$ loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufficiently sparse model.",
        "bibtex": "@inproceedings{NIPS2009_92fb0c6d,\n author = {Zhou, Shuheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/92fb0c6d1758261f10d052e6e2c1123c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/92fb0c6d1758261f10d052e6e2c1123c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/92fb0c6d1758261f10d052e6e2c1123c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 143727,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4402295319368065957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Seminarf \u00a8ur Statistik, ETH Z\u00a8urich",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Semanarf\u00fcr Statistik",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Z\u00fcrich",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2064829d6c",
        "title": "Time-Varying Dynamic Bayesian Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a67f096809415ca1c9f112d96d27689b-Abstract.html",
        "author": "Le Song; Mladen Kolar; Eric P. Xing",
        "abstract": "Directed graphical models such as Bayesian networks are a favored formalism to model the dependency structures in complex multivariate systems such as those encountered in biology and neural sciences. When the system is undergoing dynamic transformation, often a temporally rewiring network is needed for capturing the dynamic causal influences between covariates. In this paper, we propose a time-varying dynamic Bayesian network (TV-DBN) for modeling the structurally varying directed dependency structures underlying non-stationary biological/neural time series. This is a challenging problem due the non-stationarity and sample scarcity of the time series. We present a kernel reweighted $\\ell_1$ regularized auto-regressive procedure for learning the TV-DBN model. Our method enjoys nice properties such as computational efficiency and provable asymptotic consistency. Applying TV-DBN to time series measurements during yeast cell cycle and brain response to visual stimuli reveals interesting dynamics underlying the respective biological systems.",
        "bibtex": "@inproceedings{NIPS2009_a67f0968,\n author = {Song, Le and Kolar, Mladen and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Time-Varying Dynamic Bayesian Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a67f096809415ca1c9f112d96d27689b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a67f096809415ca1c9f112d96d27689b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/a67f096809415ca1c9f112d96d27689b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a67f096809415ca1c9f112d96d27689b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 911997,
        "gs_citation": 268,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15764782877855704835&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d07f61438",
        "title": "Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f3f27a324736617f20abbf2ffd806f6d-Abstract.html",
        "author": "Jonathan W. Pillow",
        "abstract": "Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a {\\it conditional renewal} (CR) model for neural spike trains. This model captures both real and rescaled-time effects, and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with $\\kappa \\neq1$), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-fit tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data.",
        "bibtex": "@inproceedings{NIPS2009_f3f27a32,\n author = {Pillow, Jonathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f3f27a324736617f20abbf2ffd806f6d-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f3f27a324736617f20abbf2ffd806f6d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f3f27a324736617f20abbf2ffd806f6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1162168,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5875734612022080753&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Departments of Psychology and Neurobiology, University of Texas at Austin",
        "aff_domain": "mail.utexas.edu",
        "email": "mail.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Departments of Psychology and Neurobiology",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2bbcf49920",
        "title": "Toward Provably Correct Feature Selection in Arbitrary Domains",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html",
        "author": "Dimitris Margaritis",
        "abstract": "In this paper we address the problem of provably correct feature selection in arbitrary domains.  An optimal solution to the problem is a Markov boundary, which is a minimal set of features that make the probability distribution of a target variable conditionally invariant to the state of all other features in the domain.  While numerous algorithms for this problem have been proposed, their theoretical correctness and practical behavior under arbitrary probability distributions is unclear.  We address this by introducing the Markov Boundary Theorem that precisely characterizes the properties of an ideal Markov boundary, and use it to develop algorithms that learn a more general boundary that can capture complex interactions that only appear when the values of multiple features are considered together.  We introduce two algorithms: an exact, provably correct one as well a more practical randomized anytime version, and show that they perform well on artificial as well as benchmark and real-world data sets.  Throughout the paper we make minimal assumptions that consist of only a general set of axioms that hold for every probability distribution, which gives these algorithms universal applicability.",
        "bibtex": "@inproceedings{NIPS2009_6da37dd3,\n author = {Margaritis, Dimitris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Toward Provably Correct Feature Selection in Arbitrary Domains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 108022,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14126979299818206663&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c2634f5a02",
        "title": "Tracking Dynamic Sources of Malicious Activity at Internet Scale",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/b6edc1cd1f36e45daf6d7824d7bb2283-Abstract.html",
        "author": "Shobha Venkataraman; Avrim Blum; Dawn Song; Subhabrata Sen; Oliver Spatscheck",
        "abstract": "We formulate and address the problem of discovering dynamic malicious regions on the Internet. We model this problem as one of adaptively pruning a known decision tree, but with additional challenges: (1) severe space requirements, since the underlying decision tree has over 4 billion leaves, and (2) a changing target function, since malicious activity on the Internet is dynamic. We present a novel algorithm that addresses this problem, by putting together a number of different ``experts algorithms and online paging algorithms. We prove guarantees on our algorithms performance as a function of the best possible pruning of a similar size, and our experiments show that our algorithm achieves high accuracy on large real-world data sets, with significant improvements over existing approaches.",
        "bibtex": "@inproceedings{NIPS2009_b6edc1cd,\n author = {Venkataraman, Shobha and Blum, Avrim and Song, Dawn and Sen, Subhabrata and Spatscheck, Oliver},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tracking Dynamic Sources of Malicious Activity at Internet Scale},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/b6edc1cd1f36e45daf6d7824d7bb2283-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/b6edc1cd1f36e45daf6d7824d7bb2283-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/b6edc1cd1f36e45daf6d7824d7bb2283-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 221776,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17556896732232564721&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "AT&T Labs\u2013Research; Carnegie Mellon University; University of California, Berkeley; AT&T Labs\u2013Research; AT&T Labs\u2013Research",
        "aff_domain": "research.att.com;cs.cmu.edu;cs.berkeley.edu;research.att.com;research.att.com",
        "email": "research.att.com;cs.cmu.edu;cs.berkeley.edu;research.att.com;research.att.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "AT&T Labs;Carnegie Mellon University;University of California, Berkeley",
        "aff_unique_dep": "Research;;",
        "aff_unique_url": "https://www.att.com/labs/research;https://www.cmu.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "AT&T Labs;CMU;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9ac93b60da",
        "title": "Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/e205ee2a5de471a70c1fd1b46033a75f-Abstract.html",
        "author": "Khashayar Rohanimanesh; Sameer Singh; Andrew McCallum; Michael J. Black",
        "abstract": "Large, relational factor graphs with structure defined by   first-order logic or other languages give rise to notoriously   difficult inference problems.  Because unrolling the structure   necessary to represent distributions over all hypotheses has   exponential blow-up, solutions are often derived from MCMC. However,   because of limitations in the design and parameterization of the   jump function, these sampling-based methods suffer from local   minima|the system must transition through lower-scoring   configurations before arriving at a better MAP solution. This paper   presents a new method of explicitly selecting fruitful downward   jumps by leveraging reinforcement learning (RL). Rather than setting   parameters to maximize the likelihood of the training data,   parameters of the factor graph are treated as a log-linear function   approximator and learned with temporal difference (TD); MAP   inference is performed by executing the resulting policy on held out   test data. Our method allows efficient gradient updates since only   factors in the neighborhood of variables affected by an action need   to be computed|we bypass the need to compute marginals entirely.   Our method provides dramatic empirical success, producing new   state-of-the-art results on a complex joint model of ontology   alignment, with a 48\\% reduction in error over state-of-the-art in   that domain.",
        "bibtex": "@inproceedings{NIPS2009_e205ee2a,\n author = {Rohanimanesh, Khashayar and Singh, Sameer and McCallum, Andrew and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/e205ee2a5de471a70c1fd1b46033a75f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 205794,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12271006883005348454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "52e4b90095",
        "title": "Unsupervised Detection of Regions of Interest Using Iterative Link Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a87ff679a2f3e71d9181a67b7542122c-Abstract.html",
        "author": "Gunhee Kim; Antonio Torralba",
        "abstract": "This paper proposes a fast and scalable alternating optimization technique to detect regions of interest (ROIs) in cluttered Web images without labels. The proposed approach discovers highly probable regions of  object instances by iteratively repeating the following two functions: (1) choose the exemplar set (i.e. small number of high ranked reference ROIs) across the dataset and (2) refine the ROIs of each image with respect to the exemplar set. These two subproblems are formulated as ranking in two different similarity networks of ROI hypotheses by link analysis. The experiments with the PASCAL 06 dataset show that our unsupervised localization performance is better than one of state-of-the-art techniques and comparable to supervised methods. Also, we test the scalability of our approach with five objects in Flickr dataset consisting of more than 200,000 images.",
        "bibtex": "@inproceedings{NIPS2009_a87ff679,\n author = {Kim, Gunhee and Torralba, Antonio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Detection of Regions of Interest Using Iterative Link Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a87ff679a2f3e71d9181a67b7542122c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 557126,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15081619343556182125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science, Carnegie Mellon University; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
        "aff_domain": "cs.cmu.edu;csail.mit.edu",
        "email": "cs.cmu.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Massachusetts Institute of Technology",
        "aff_unique_dep": "School of Computer Science;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.cmu.edu;https://www.csail.mit.edu",
        "aff_unique_abbr": "CMU;MIT",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pittsburgh;Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ab339fd3cb",
        "title": "Unsupervised Feature Selection for the $k$-means Clustering Problem",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/c51ce410c124a10e0db5e4b97fc2af39-Abstract.html",
        "author": "Christos Boutsidis; Petros Drineas; Michael W. Mahoney",
        "abstract": "We present a novel feature selection algorithm for the $k$-means clustering problem. Our algorithm is randomized and, assuming an accuracy parameter $\\epsilon \\in (0,1)$, selects and appropriately rescales in an unsupervised manner $\\Theta(k \\log(k / \\epsilon) / \\epsilon^2)$ features from a dataset of arbitrary dimensions. We prove that, if we run any $\\gamma$-approximate $k$-means algorithm ($\\gamma \\geq 1$) on the features selected using our method, we can find a $(1+(1+\\epsilon)\\gamma)$-approximate partition with high probability.",
        "bibtex": "@inproceedings{NIPS2009_c51ce410,\n author = {Boutsidis, Christos and Drineas, Petros and Mahoney, Michael W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Feature Selection for the k-means Clustering Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/c51ce410c124a10e0db5e4b97fc2af39-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/c51ce410c124a10e0db5e4b97fc2af39-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/c51ce410c124a10e0db5e4b97fc2af39-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 331989,
        "gs_citation": 224,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2963672306798296156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180; Department of Mathematics, Stanford University, Stanford, CA 94305; Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180",
        "aff_domain": "cs.rpi.edu;cs.stanford.edu;cs.rpi.edu",
        "email": "cs.rpi.edu;cs.stanford.edu;cs.rpi.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;Stanford University",
        "aff_unique_dep": "Department of Computer Science;Department of Mathematics",
        "aff_unique_url": "https://www.rpi.edu;https://www.stanford.edu",
        "aff_unique_abbr": "RPI;Stanford",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Troy;Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0a2655c99b",
        "title": "Unsupervised feature learning for audio classification using convolutional deep belief networks",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/a113c1ecd3cace2237256f4c712f61b5-Abstract.html",
        "author": "Honglak Lee; Peter Pham; Yan Largman; Andrew Y. Ng",
        "abstract": "In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. For the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations trained from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.",
        "bibtex": "@inproceedings{NIPS2009_a113c1ec,\n author = {Lee, Honglak and Pham, Peter and Largman, Yan and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised feature learning for audio classification using convolutional deep belief networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a113c1ecd3cace2237256f4c712f61b5-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/a113c1ecd3cace2237256f4c712f61b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/a113c1ecd3cace2237256f4c712f61b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 372411,
        "gs_citation": 1610,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2046036768079393393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c06fb4e65a",
        "title": "Variational Gaussian-process factor analysis for modeling spatio-temporal data",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/4a47d2983c8bd392b120b627e0e1cab4-Abstract.html",
        "author": "Jaakko Luttinen; Alexander Ilin",
        "abstract": "We present a probabilistic latent factor model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.",
        "bibtex": "@inproceedings{NIPS2009_4a47d298,\n author = {Luttinen, Jaakko and Ilin, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Gaussian-process factor analysis for modeling spatio-temporal data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/4a47d2983c8bd392b120b627e0e1cab4-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/4a47d2983c8bd392b120b627e0e1cab4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/4a47d2983c8bd392b120b627e0e1cab4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1145599,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3491867267053169220&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Adaptive Informatics Research Center, Helsinki University of Technology, Finland; Adaptive Informatics Research Center, Helsinki University of Technology, Finland",
        "aff_domain": "tkk.fi;tkk.fi",
        "email": "tkk.fi;tkk.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Helsinki University of Technology",
        "aff_unique_dep": "Adaptive Informatics Research Center",
        "aff_unique_url": "https://www.hut.fi",
        "aff_unique_abbr": "HUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "fe0de31d7e",
        "title": "Variational Inference for the Nested Chinese Restaurant Process",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract.html",
        "author": "Chong Wang; David M. Blei",
        "abstract": "The nested Chinese restaurant process (nCRP) is a powerful nonparametric Bayesian model for learning tree-based hierarchies from data. Since its posterior distribution is intractable, current inference methods have all relied on MCMC sampling. In this paper, we develop an alternative inference technique based on variational methods. To employ variational methods, we derive a tree-based stick-breaking construction of the nCRP mixture model, and a novel variational algorithm that efficiently explores a posterior over a large set of combinatorial structures. We demonstrate the use of this approach for text and hand written digits modeling, where we show we can adapt the nCRP to continuous data as well.",
        "bibtex": "@inproceedings{NIPS2009_ca46c1b9,\n author = {Wang, Chong and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Inference for the Nested Chinese Restaurant Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/ca46c1b9512a7a8315fa3c5a946e8265-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 230997,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3725564863814171913&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, Princeton University; Computer Science Department, Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9337df0082",
        "title": "Which graphical models are difficult to learn?",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/22fb0cee7e1f3bde58293de743871417-Abstract.html",
        "author": "Andrea Montanari; Jose A. Pereira",
        "abstract": "We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity  algorithms systematically fail when the Markov random field  develops long-range correlations. More precisely, this phenomenon  appears to be related to the Ising model phase transition  (although it does not coincide with it).",
        "bibtex": "@inproceedings{NIPS2009_22fb0cee,\n author = {Montanari, Andrea and Pereira, Jose},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Which graphical models are difficult to learn?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/22fb0cee7e1f3bde58293de743871417-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/22fb0cee7e1f3bde58293de743871417-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 157637,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7425865451041326500&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical Engineering, Stanford University; Department of Electrical Engineering and Department of Statistics, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5169846ba3",
        "title": "White Functionals for Anomaly Detection in Dynamical Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/0188e8b8b014829e2fa0f430f0a95961-Abstract.html",
        "author": "Marco Cuturi; Jean-philippe Vert; Alexandre D'aspremont",
        "abstract": "We propose new methodologies to detect anomalies in discrete-time processes taking values in a set. The method is based on the inference of functionals whose evaluations on successive states visited by the process have low autocorrelations. Deviations from this behavior are used to flag anomalies. The candidate functionals are estimated in a subset of a reproducing kernel Hilbert space associated with the set where the process takes values. We provide experimental results which show that these techniques compare favorably with other algorithms.",
        "bibtex": "@inproceedings{NIPS2009_0188e8b8,\n author = {Cuturi, Marco and Vert, Jean-philippe and D\\textquotesingle aspremont, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {White Functionals for Anomaly Detection in Dynamical Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/0188e8b8b014829e2fa0f430f0a95961-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/0188e8b8b014829e2fa0f430f0a95961-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/0188e8b8b014829e2fa0f430f0a95961-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 278435,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8601914390329639971&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff": "ORFE - Princeton University; Mines ParisTech + Institut Curie + INSERM U900; ORFE - Princeton University",
        "aff_domain": "princeton.edu;mines.org;princeton.edu",
        "email": "princeton.edu;mines.org;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2+3;0",
        "aff_unique_norm": "Princeton University;MINES ParisTech;Institut Curie;Institut National de la Sant\u00e9 et de la Recherche M\u00e9dicale",
        "aff_unique_dep": "Operations Research and Financial Engineering;;;Unit\u00e9 900",
        "aff_unique_url": "https://www.princeton.edu;https://www.mines-paristech.fr;https://www.institut-curie.org;https://www.inserm.fr",
        "aff_unique_abbr": "Princeton;Mines ParisTech;Institut Curie;INSERM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+1+1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "72d19a8ade",
        "title": "Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/f899139df5e1059396431415e770c6dd-Abstract.html",
        "author": "Jacob Whitehill; Ting-fan Wu; Jacob Bergsma; Javier R. Movellan; Paul L. Ruvolo",
        "abstract": "Modern machine learning-based approaches to computer vision require very large databases of labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector). While the collection of these large databases is becoming a bottleneck, new Internet-based services that allow labelers from around the world to be easily hired and managed provide a promising solution.  However, using these services to label large databases brings with it new theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used ``Majority Vote heuristic for inferring image labels, and is robust to both adversarial and noisy labelers.",
        "bibtex": "@inproceedings{NIPS2009_f899139d,\n author = {Whitehill, Jacob and Wu, Ting-fan and Bergsma, Jacob and Movellan, Javier and Ruvolo, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/f899139df5e1059396431415e770c6dd-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/f899139df5e1059396431415e770c6dd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/f899139df5e1059396431415e770c6dd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/f899139df5e1059396431415e770c6dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 644480,
        "gs_citation": 1514,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14409115865258228521&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Machine Perception Laboratory, University of California, San Diego; Machine Perception Laboratory, University of California, San Diego; Machine Perception Laboratory, University of California, San Diego; Machine Perception Laboratory, University of California, San Diego; Machine Perception Laboratory, University of California, San Diego",
        "aff_domain": "mplab.ucsd.edu;mplab.ucsd.edu;mplab.ucsd.edu;mplab.ucsd.edu;mplab.ucsd.edu",
        "email": "mplab.ucsd.edu;mplab.ucsd.edu;mplab.ucsd.edu;mplab.ucsd.edu;mplab.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Machine Perception Laboratory",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "84f05f6f34",
        "title": "Who\u2019s Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/16e6a3326dd7d868cbc926602a61e4d0-Abstract.html",
        "author": "Jie Luo; Barbara Caputo; Vittorio Ferrari",
        "abstract": "Given a corpus of news items consisting of images accompanied by text captions, we want to find out `",
        "bibtex": "@inproceedings{NIPS2009_16e6a332,\n author = {Luo, Jie and Caputo, Barbara and Ferrari, Vittorio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Who\u2019s Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/16e6a3326dd7d868cbc926602a61e4d0-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/16e6a3326dd7d868cbc926602a61e4d0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/16e6a3326dd7d868cbc926602a61e4d0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1177960,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=675684801141881187&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Idiap and EPF Lausanne; Idiap Research Institute; ETH Zurich",
        "aff_domain": "idiap.ch;idiap.ch;vision.ee.ethz.ch",
        "email": "idiap.ch;idiap.ch;vision.ee.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Idiap Research Institute;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.idiap.ch;https://www.ethz.ch",
        "aff_unique_abbr": "Idiap;ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "c49a1a9180",
        "title": "Zero-shot Learning with Semantic Output Codes",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html",
        "author": "Mark Palatucci; Dean Pomerleau; Geoffrey E. Hinton; Tom M. Mitchell",
        "abstract": "We consider the problem of zero-shot learning, where the goal is to learn a classifier $f: X \\rightarrow Y$ that must predict novel values of $Y$ that were omitted from the training set. To achieve this, we define the notion of a semantic output code classifier (SOC) which utilizes a knowledge base of semantic properties of $Y$ to extrapolate to novel classes. We provide a formalism for this type of classifier and study its theoretical properties in a PAC framework, showing conditions under which the classifier can accurately predict novel classes.  As a case study, we build a SOC classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.",
        "bibtex": "@inproceedings{NIPS2009_1543843a,\n author = {Palatucci, Mark and Pomerleau, Dean and Hinton, Geoffrey E and Mitchell, Tom M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Zero-shot Learning with Semantic Output Codes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/1543843a4723ed2ab08e18053ae6dc5b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 191424,
        "gs_citation": 1391,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9659772690571084059&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Robotics Institute, Carnegie Mellon University; Intel Labs; Computer Science Department, University of Toronto; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;intel.com;cs.toronto.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;intel.com;cs.toronto.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Carnegie Mellon University;Intel;University of Toronto",
        "aff_unique_dep": "Robotics Institute;Intel Labs;Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu;https://www.intel.com;https://www.utoronto.ca",
        "aff_unique_abbr": "CMU;Intel;U of T",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Pittsburgh;;Toronto",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "474aa6a1b8",
        "title": "fMRI-Based Inter-Subject Cortical Alignment Using Functional Connectivity",
        "site": "https://papers.nips.cc/paper_files/paper/2009/hash/9f396fe44e7c05c16873b05ec425cbad-Abstract.html",
        "author": "Bryan Conroy; Ben Singer; James Haxby; Peter J. Ramadge",
        "abstract": "The inter-subject alignment of functional MRI (fMRI) data is important for improving the statistical power of fMRI group analyses. In contrast to existing anatomically-based methods, we propose a novel multi-subject algorithm that derives a functional correspondence by aligning spatial patterns of functional connectivity across a set of subjects. We test our method on fMRI data collected during a movie viewing experiment. By cross-validating the results of our algorithm, we show that the correspondence successfully generalizes to a secondary movie dataset not used to derive the alignment.",
        "bibtex": "@inproceedings{NIPS2009_9f396fe4,\n author = {Conroy, Bryan and Singer, Ben and Haxby, James and Ramadge, Peter J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {fMRI-Based Inter-Subject Cortical Alignment Using Functional Connectivity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf},\n volume = {22},\n year = {2009}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2009/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2009/file/9f396fe44e7c05c16873b05ec425cbad-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2009/file/9f396fe44e7c05c16873b05ec425cbad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 439639,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4137654166605655002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    }
]