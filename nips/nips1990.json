[
    {
        "id": "9a79208392",
        "title": "A B-P ANN Commodity Trader",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/ad13a2a07ca4b7642959dc0c4c740ab6-Abstract.html",
        "author": "Joseph E. Collard",
        "abstract": "An  Artificial  Neural  Network  recognize  a  buy/sell  particular  commodity  Propagation  of  errors  algorithm  was  used  to  encode  the  the  Long/Short  desired  output  and  18  fundamental  variables  plus  6  (or  18)  Trained  on  one  technical  variables  into  year  of  past  data  to  predict  long/short  market  positions  in  the  future  that  would  have  made  $10,301  profit  on  an  investment  of  less  than  $1000. \n\nrelationship  between",
        "bibtex": "@inproceedings{NIPS1990_ad13a2a0,\n author = {Collard, Joseph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A B-P ANN Commodity Trader},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/ad13a2a07ca4b7642959dc0c4c740ab6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 866296,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16140760777532856997&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Martingale Research Corporation",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Martingale Research Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "15149dfb47",
        "title": "A Comparative Study of the Practical Characteristics of Neural Network and Conventional Pattern Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html",
        "author": "Kenney Ng; Richard P Lippmann",
        "abstract": "Seven different pattern classifiers were implemented on a serial computer  and compared using artificial and speech recognition tasks. Two neural  network (radial basis function and high order polynomial GMDH network)  and five conventional classifiers (Gaussian mixture, linear tree, K nearest  neighbor, KD-tree, and condensed K nearest neighbor) were evaluated.  Classifiers were chosen to be representative of different approaches to pat(cid:173) tern classification and to complement and extend those evaluated in a  previous study (Lee and Lippmann, 1989). This and the previous study  both demonstrate that classification error rates can be equivalent across  different classifiers when they are powerful enough to form minimum er(cid:173) ror decision regions, when they are properly tuned, and when sufficient  training data is available. Practical characteristics such as training time,  classification time, and memory requirements, however, can differ by or(cid:173) ders of magnitude. These results suggest that the selection of a classifier  for a particular task should be guided not so much by small differences in  error rate, but by practical considerations concerning memory usage, com(cid:173) putational resources, ease of implementation, and restrictions on training  and classification times.",
        "bibtex": "@inproceedings{NIPS1990_66368270,\n author = {Ng, Kenney and Lippmann, Richard P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Comparative Study of the Practical Characteristics of Neural Network and Conventional Pattern Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/66368270ffd51418ec58bd793f2d9b1b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1652624,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18414898036931903777&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Lincoln Laboratory, MIT; BBN Systems and Technologies",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;BBN Technologies",
        "aff_unique_dep": "Lincoln Laboratory;",
        "aff_unique_url": "https://www.mit.edu;https://www.bbn.com",
        "aff_unique_abbr": "MIT;BBN",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7f6ace8cad",
        "title": "A Connectionist Learning Control Architecture for Navigation",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/dc912a253d1e9ba40e2c597ed2376640-Abstract.html",
        "author": "Jonathan R. Bachrach",
        "abstract": "A novel learning control architecture is used for navigation. A sophisti(cid:173) cated test-bed is used to simulate a cylindrical robot with a sonar belt  in a planar environment. The task is short-range homing in the pres(cid:173) ence of obstacles. The robot receives no global information and assumes  no comprehensive world model. Instead the robot receives only sensory  information which is inherently limited. A connectionist architecture is  presented which incorporates a large amount of a priori knowledge in the  form of hard-wired networks, architectural constraints, and initial weights.  Instead of hard-wiring static potential fields from object models, myarchi(cid:173) tecture learns sensor-based potential fields, automatically adjusting them  to avoid local minima and to produce efficient homing trajectories. It does  this without object models using only sensory information. This research  demonstrates the use of a large modular architecture on a difficult task.",
        "bibtex": "@inproceedings{NIPS1990_dc912a25,\n author = {Bachrach, Jonathan R.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Connectionist Learning Control Architecture for Navigation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/dc912a253d1e9ba40e2c597ed2376640-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1342998,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11706347603114444337&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer and Information Science, University of Massachusetts",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8623c1eec4",
        "title": "A Delay-Line Based Motion Detection Chip",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/142949df56ea8ae0be8b5306971900a4-Abstract.html",
        "author": "Tim Horiuchi; John Lazzaro; Andrew Moore; Christof Koch",
        "abstract": "Inspired by a visual motion detection model for the ra.bbit retina  and by a computational architecture used for early audition in the  barn owl, we have designed a chip that employs a correlation model  to report the one-dimensional field motion of a scene in real time.  Using subthreshold analog VLSI techniques, we have fabricated and  successfully tested a 8000 transistor chip using a standard MOSIS  process.",
        "bibtex": "@inproceedings{NIPS1990_142949df,\n author = {Horiuchi, Tim and Lazzaro, John and Moore, Andrew and Koch, Christof},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Delay-Line Based Motion Detection Chip},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/142949df56ea8ae0be8b5306971900a4-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/142949df56ea8ae0be8b5306971900a4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/142949df56ea8ae0be8b5306971900a4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1421229,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13538712304293648144&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3e29a84e70",
        "title": "A Framework for the Cooperation of Learning Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/a8c88a0055f636e4a163a5e3d16adab7-Abstract.html",
        "author": "L\u00e9on Bottou; Patrick Gallinari",
        "abstract": "We introduce a framework  for  training architectures composed of several  modules. This framework,  which  uses a statistical formulation  of learning  systems,  provides  a  unique  formalism  for  describing  many  classical  connectionist  algorithms  as  well  as  complex  systems  where  several  algorithms interact. It allows to design hybrid systems which combine the  advantages of connectionist algorithms as well as other learning algorithms.",
        "bibtex": "@inproceedings{NIPS1990_a8c88a00,\n author = {Bottou, L\\'{e}on and Gallinari, Patrick},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Framework for the Cooperation of Learning Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a8c88a0055f636e4a163a5e3d16adab7-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/a8c88a0055f636e4a163a5e3d16adab7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/a8c88a0055f636e4a163a5e3d16adab7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1422672,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14923838171700954480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Laboratoire de Recherche en Informatique; Universite de Paris XI",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Laboratoire de recherche en informatique;Universit\u00e9 Paris-Sud",
        "aff_unique_dep": "Computer Science Research Laboratory;",
        "aff_unique_url": ";https://www.u-psud.fr",
        "aff_unique_abbr": ";UPS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "a2b57f9395",
        "title": "A Lagrangian Approach to Fixed Points",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/b7b16ecf8ca53723593894116071700c-Abstract.html",
        "author": "Eric Mjolsness; Willard L. Miranker",
        "abstract": "We  present  a  new  way  to  derive  dissipative,  optimizing  dynamics  from  the  Lagrangian formulation of mechanics.  It can  be  used  to  obtain  both  standard  and  novel  neural  net  dynamics  for  optimization  problems.  To  demonstrate this we  derive standard descent  dynamics as well as nonstan(cid:173) dard variants that introduce  a  computational attention mechanism.",
        "bibtex": "@inproceedings{NIPS1990_b7b16ecf,\n author = {Mjolsness, Eric and Miranker, Willard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Lagrangian Approach to Fixed Points},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b7b16ecf8ca53723593894116071700c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/b7b16ecf8ca53723593894116071700c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/b7b16ecf8ca53723593894116071700c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1178413,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1793049695159355353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Yale University; IBM Watson Research Center, Yorktown Heights, NY 10598",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yale University;IBM",
        "aff_unique_dep": "Department of Computer Science;IBM Watson Research Center",
        "aff_unique_url": "https://www.yale.edu;https://www.ibm.com/watson",
        "aff_unique_abbr": "Yale;IBM Watson",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Yorktown Heights",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4516bf7853",
        "title": "A Method for the Efficient Design of Boltzmann Machines for Classiffication Problems",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/d1f255a373a3cef72e03aa9d980c7eca-Abstract.html",
        "author": "Ajay Gupta; Wolfgang Maass",
        "abstract": "We introduce a method for the efficient design of a Boltzmann machine (or  a Hopfield net) that computes an arbitrary given Boolean function f . This  method is based on an efficient simulation of acyclic circuits with threshold  gates by Boltzmann machines. As a consequence we can show that various  concrete Boolean functions f that are relevant for classification problems  can be computed by scalable Boltzmann machines that are guaranteed  to converge to their global maximum configuration with high probability  after constantly many steps.",
        "bibtex": "@inproceedings{NIPS1990_d1f255a3,\n author = {Gupta, Ajay and Maass, Wolfgang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Method for the Efficient Design of Boltzmann Machines for Classiffication Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/d1f255a373a3cef72e03aa9d980c7eca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1661736,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:zjpYdCM2rZMJ:scholar.google.com/&scioq=A+Method+for+the+Efficient+Design+of+Boltzmann+Machines+for+Classiffication+Problems&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Department of Mathematics, Statistics, and Computer Science, University of Illinois at Chicago; Department of Mathematics, Statistics, and Computer Science, University of Illinois at Chicago + Department of Computer Science, University of Chicago",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of Illinois at Chicago;University of Chicago",
        "aff_unique_dep": "Department of Mathematics, Statistics, and Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu;https://www.uchicago.edu",
        "aff_unique_abbr": "UIC;UChicago",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2a9b8e774e",
        "title": "A Model of Distributed Sensorimotor Control in the Cockroach Escape Turn",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/f85454e8279be180185cac7d243c5eb3-Abstract.html",
        "author": "R.D. Beer; G. J. Kacmarcik; R.E. Ritzmann; H.J. Chiel",
        "abstract": "In response to a puff of wind, the American cockroach turns away and runs.  The circuit underlying the initial turn of this escape response consists of  three populations of individually identifiable nerve cells and appears to em(cid:173) ploy distributed representations in its operation. We have reconstructed  several neuronal and behavioral properties of this system using simplified  neural network models and the backpropagation learning algorithm con(cid:173) strained by known structural characteristics of the circuitry. In order to  test and refine the model, we have also compared the model's responses to  various lesions with the insect's responses to similar lesions.",
        "bibtex": "@inproceedings{NIPS1990_f85454e8,\n author = {Beer, R.D. and Kacmarcik, G. and Ritzmann, R.E. and Chiel, H.J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Model of Distributed Sensorimotor Control in the Cockroach Escape Turn},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/f85454e8279be180185cac7d243c5eb3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1708986,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16354491601984628659&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "eba9fa23dc",
        "title": "A Multiscale Adaptive Network Model of Motion Computation in Primates",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/9dfcd5e558dfa04aaf37f137a1d9d3e5-Abstract.html",
        "author": "H. Taichi Wang; Bimal Mathur; Christof Koch",
        "abstract": "We  demonstrate  a  multiscale  adaptive  network  model  of motion  computation in primate area MT. The model consists of two stages:  (l)  local velocities are measured across multiple spatio-temporal channels,  and (2) the optical flow  field  is computed by a  network of direction(cid:173) selective neurons  at multiple  spatial  resolutions.  This model  embeds  the computational efficiency of Multigrid algorithms within a parallel  network as well as adaptively  computes  the most reliable estimate of  the flow  field across different spatial scales. Our model neurons show  the same nonclassical receptive field properties as Allman's type I MT  neurons.  Since local velocities are measured across multiple channels,  various  channels  often  provide  conflicting  measurements  to  the  network. We have incorporated a  veto scheme for conflict resolution.  This mechanism provides a novel explanation for the spatial frequency  dependency of the psychophysical phenomenon called Motion Capture.",
        "bibtex": "@inproceedings{NIPS1990_9dfcd5e5,\n author = {Wang, H. and Mathur, Bimal and Koch, Christof},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Multiscale Adaptive Network Model of Motion Computation in Primates},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9dfcd5e558dfa04aaf37f137a1d9d3e5-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/9dfcd5e558dfa04aaf37f137a1d9d3e5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/9dfcd5e558dfa04aaf37f137a1d9d3e5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1426340,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13695772295987253050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cc668bbfd4",
        "title": "A Neural Expert System with Automated Extraction of Fuzzy If-Then Rules and Its Application to Medical Diagnosis",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/82cec96096d4281b7c95cd7e74623496-Abstract.html",
        "author": "Yoichi Hayashi",
        "abstract": "This paper proposes ajuzzy neural expert system (FNES) with the  following two functions: (1) Generalization of the information derived  from the training data and embodiment of knowledge in the form of the  fuzzy neural network; (2) Extraction of fuzzy If-Then rules with  linguistic relative importance of each proposition in an antecedent  (I f -part) from a trained neural network. This paper also gives a  method to extract automatically fuzzy If-Then rules from the trained  neural network. To prove the effectiveness and validity of the proposed  fuzzy neural expert system. a fuzzy neural expert system for medical  diagnosis has been developed.",
        "bibtex": "@inproceedings{NIPS1990_82cec960,\n author = {Hayashi, Yoichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Neural Expert System with Automated Extraction of Fuzzy If-Then Rules and Its Application to Medical Diagnosis},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/82cec96096d4281b7c95cd7e74623496-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1749638,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2630997896104967695&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1915302402",
        "title": "A Neural Network Approach for Three-Dimensional Object Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/8bf1211fd4b7b94528899de0a43b9fb3-Abstract.html",
        "author": "Volker Tresp",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1990_8bf1211f,\n author = {Tresp, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Neural Network Approach for Three-Dimensional Object Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/8bf1211fd4b7b94528899de0a43b9fb3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1434841,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7704526006127724578&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Siemens AG, Central Research and Development",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Siemens AG",
        "aff_unique_dep": "Central Research and Development",
        "aff_unique_url": "https://www.siemens.com",
        "aff_unique_abbr": "Siemens",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "e56330e307",
        "title": "A Novel Approach to Prediction of the 3-Dimensional Structures of Protein Backbones by Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/bca82e41ee7b0833588399b1fcd177c7-Abstract.html",
        "author": "Henrik Fredholm; Henrik Bohr; Jakob Bohr; S\u00f8ren Brunak; Rodney M. J. Cotterill; Benny Lautrup; Steffen B. Petersen",
        "abstract": "Three-dimensional (3D) structures of protein backbones have been pre(cid:173) dicted using neural networks. A feed forward neural network was trained  on a class of functionally, but not structurally, homologous proteins, us(cid:173) ing backpropagation learning. The network generated tertiary structure  information in the form of binary distance constraints for the Co atoms  in the protein backbone. The binary distance between two Co atoms was  o if the distance between them was less than a certain threshold distance,  and 1 otherwise. The distance constraints predicted by the trained neu(cid:173) ral network were utilized to generate a folded conformation of the protein  backbone, using a steepest descent minimization approach.",
        "bibtex": "@inproceedings{NIPS1990_bca82e41,\n author = {Fredholm, Henrik and Bohr, Henrik and Bohr, Jakob and Brunak, S\\o ren and Cotterill, Rodney and Lautrup, Benny and Petersen, Steffen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Novel Approach to Prediction of the 3-Dimensional Structures of Protein Backbones by Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bca82e41ee7b0833588399b1fcd177c7-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/bca82e41ee7b0833588399b1fcd177c7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/bca82e41ee7b0833588399b1fcd177c7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1552222,
        "gs_citation": 194,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15667896031111567590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e69b302608",
        "title": "A Recurrent Neural Network Model of Velocity Storage in the Vestibulo-Ocular Reflex",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html",
        "author": "Thomas J. Anastasio",
        "abstract": "A three-layered neural  network model was used to  explore  the  organization of  the  vestibulo-ocular  reflex  (VOR).  The  dynamic  model  was  trained  using  recurrent back-propagation to produce compensatory, long duration eye muscle  motoneuron  outputs  in  response  to  short  duration  vestibular  afferent  head  velocity  inputs.  The  network  learned  to  produce  this  response  prolongation,  known  as  velocity  storage,  by  developing  complex,  lateral  inhibitory  interac(cid:173) tions among the interneurons.  These had the low baseline, long time constant,  rectified  and  skewed  responses  that  are  characteristic  of  real  VOR  inter(cid:173) neurons.  The  model  suggests  that  all  of these  features  are  interrelated  and  result from lateral inhibition.",
        "bibtex": "@inproceedings{NIPS1990_eddea82a,\n author = {Anastasio, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Recurrent Neural Network Model of Velocity Storage in the Vestibulo-Ocular Reflex},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/eddea82ad2755b24c4e168c5fc2ebd40-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1454960,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12518891301288212828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Otolaryngology, University of Southern California School of Medicine",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Department of Otolaryngology",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "415a449417",
        "title": "A Recurrent Neural Network for Word Identification from Continuous Phoneme Strings",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/24b16fede9a67c9251d3e7c7161c83ac-Abstract.html",
        "author": "Robert B. Allen; Candace A. Kamm",
        "abstract": "A  neural  network  architecture  was  designed  for  locating  word  boundaries  and  identifying  words  from  phoneme  sequences.  This  architecture  was  tested  in  three  sets  of  studies.  First,  a  highly  redundant  corpus  with  a  restricted  vocabulary was  generated and the network was trained with a limited number of  phonemic variations for the words  in the corpus.  Tests of network performance  on a transfer set yielded a very low error rate.  In a second study, a network was  trained  to  identify  words  from  expert  transcriptions  of speech.  On a  transfer  test,  error  rate  for  correct  simultaneous  identification  of  words  and  word  boundaries was  18%.  The third study used the output of a phoneme classifier as  the input to the word and  word boundary identification network.  The error rate  on a transfer test set was 49% for this task.  Overall, these studies provide a first  step at identifying words in connected discourse with a neural network.",
        "bibtex": "@inproceedings{NIPS1990_24b16fed,\n author = {Allen, Robert and Kamm, Candace},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Recurrent Neural Network for Word Identification from Continuous Phoneme Strings},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/24b16fede9a67c9251d3e7c7161c83ac-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/24b16fede9a67c9251d3e7c7161c83ac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/24b16fede9a67c9251d3e7c7161c83ac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1878272,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10755200148433715089&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Bellcore; Bellcore",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Bellcore",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bellcore.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8adf36ba53",
        "title": "A Reinforcement Learning Variant for Control Scheduling",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/357a6fdf7642bf815a88822c447d9dc4-Abstract.html",
        "author": "Aloke Guha",
        "abstract": "We  present  an  algorithm  based  on  reinforcement  and  state  recurrence  learning  techniques  to  solve  control  scheduling  problems.  In  particular,  we  have  devised  a  simple  learning  scheme  called  \"handicapped  learning\",  in  which  the  weights  of the  associative  search  element  are  reinforced,  either  positively  or negatively,  such  that the  system  is forced  to  move  towards the  desired  setpoint  in  the  shortest possible  trajectory.  To  improve  the  learning  rate,  a  variable  reinforcement  scheme  is  employed:  negative  reinforcement  values are  varied depending  on  whether the failure  occurs in  handicapped or  normal  mode  of operation.  Furthermore,  to  realize  a  simulated  annealing  scheme  for  accelerated  learning,  if the  system  visits  the  same  failed  state  successively,  the  negative  reinforcement  value  is  increased.  In  examples  studied,  these  learning  schemes  have  demonstrated  high  learning  rates,  and  therefore may prove useful  for  in-situ learning.",
        "bibtex": "@inproceedings{NIPS1990_357a6fdf,\n author = {Guha, Aloke},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Reinforcement Learning Variant for Control Scheduling},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/357a6fdf7642bf815a88822c447d9dc4-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/357a6fdf7642bf815a88822c447d9dc4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/357a6fdf7642bf815a88822c447d9dc4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1465418,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14836491389504453993&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Honeywell Sensor and System Development Center",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Honeywell",
        "aff_unique_dep": "Sensor and System Development Center",
        "aff_unique_url": "https://www.honeywell.com",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "af6690089c",
        "title": "A Second-Order Translation, Rotation and Scale Invariant Neural Network",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/c3e878e27f52e2a57ace4d9a76fd9acf-Abstract.html",
        "author": "Shelly D. D. Goggin; Kristina M. Johnson; Karl E. Gustafson",
        "abstract": "A second-order architecture is presented  here for  translation, rotation and  scale  invariant processing  of 2-D  images  mapped  to  n  input  units.  This  new architecture has a complexity of O( n) weights as opposed to the O( n 3 )  weights usually required  for  a  third-order,  rotation invariant architecture.  The reduction  in complexity is due  to the use  of discrete  frequency  infor(cid:173) mation.  Simulations show favorable  comparisons to other neural  network  architectures.",
        "bibtex": "@inproceedings{NIPS1990_c3e878e2,\n author = {Goggin, Shelly and Johnson, Kristina and Gustafson, Karl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Second-Order Translation, Rotation and Scale Invariant Neural Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c3e878e27f52e2a57ace4d9a76fd9acf-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/c3e878e27f52e2a57ace4d9a76fd9acf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/c3e878e27f52e2a57ace4d9a76fd9acf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1424663,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4164837471200117686&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Optoelectronic Computing Systems Center and Department of Electrical and Computer Engineering, University of Colorado at Boulder; Optoelectronic Computing Systems Center and Department of Electrical and Computer Engineering, University of Colorado at Boulder; Department of Mathematics",
        "aff_domain": "boulder.colorado.edu; ; ",
        "email": "boulder.colorado.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Colorado at Boulder;Mathematics Department",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Mathematics",
        "aff_unique_url": "https://www.colorado.edu;",
        "aff_unique_abbr": "CU Boulder;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boulder;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "d4f1b5a52d",
        "title": "A Short-Term Memory Architecture for the Learning of Morphophonemic Rules",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/41ae36ecb9b3eee609d05b90c14222fb-Abstract.html",
        "author": "Michael Gasser; Chan-Do Lee",
        "abstract": "Despite its successes,  Rumelhart and McClelland's (1986)  well-known ap(cid:173) proach to the learning of morphophonemic rules  suffers from two deficien(cid:173) cies:  (1)  It performs  the  artificial  task  of associating  forms  with  forms  rather  than  perception  or  production.  (2)  It is  not  constrained  in  ways  that humans learners  are.  This paper describes  a  model which  addresses  both objections.  Using  a  simple recurrent  architecture  which  takes  both  forms  and  \"meanings\"  as  inputs,  the  model  learns  to  generate  verbs  in  one  or  another  \"tense\",  given  arbitrary  meanings,  and  to  recognize  the  tenses  of verbs.  Furthermore,  it fails  to learn  reversal  processes  unknown  in human language.",
        "bibtex": "@inproceedings{NIPS1990_41ae36ec,\n author = {Gasser, Michael and Lee, Chan-Do},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Short-Term Memory Architecture for the Learning of Morphophonemic Rules},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/41ae36ecb9b3eee609d05b90c14222fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1671064,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1747898957443980192&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, Indiana University; Computer Science Department, Indiana University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a001e02e7b",
        "title": "A Theory for Neural Networks with Time Delays",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/6c524f9d5d7027454a783c841250ba71-Abstract.html",
        "author": "Bert de Vries; Jos\u00e9 Carlos Pr\u00edncipe",
        "abstract": "We present a new neural network model for processing of temporal  patterns.  This  model,  the  gamma  neural model,  is as  general  as  a  convolution  delay  model  with  arbitrary  weight  kernels  w(t).  We  show  that  the  gamma  model  can  be  formulated  as  a  (partially  prewired)  additive  model.  A  temporal  hebbian  learning  rule  is  derived  and  we  establish  links  to  related  existing  models  for  temporal processing.",
        "bibtex": "@inproceedings{NIPS1990_6c524f9d,\n author = {de Vries, Bert and Pr\\'{\\i}ncipe, Jos\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Theory for Neural Networks with Time Delays},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/6c524f9d5d7027454a783c841250ba71-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1167781,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17868016108787695210&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical Engineering, University of Horida, CSE 447, Gainesville, FL 32611; Department of Electrical Engineering, University of Horida, CSE 444, Gainesville, FL 32611",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Gainesville",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9f891c0ce2",
        "title": "A VLSI Neural Network for Color Constancy",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Abstract.html",
        "author": "Andrew W. Moore; John Allman; Geoffrey Fox; Rodney Goodman",
        "abstract": "A system for color correction has been designed, built, and tested suc(cid:173) cessfully; the essential components are three custom chips built using sub(cid:173) threshold analog CMOS VLSI. The system, based on Land's Retinex the(cid:173) ory of color constancy, produces colors similar in many respects to those  produced by the visual system. Resistive grids implemented in analog  VLSI perform the smoothing operation central to the algorithm at video  rates. With the electronic system, the strengths and weaknesses of the  algorithm are explored.",
        "bibtex": "@inproceedings{NIPS1990_f8c1f23d,\n author = {Moore, Andrew and Allman, John and Fox, Geoffrey and Goodman, Rodney},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A VLSI Neural Network for Color Constancy},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1754525,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1819535845248771010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9fc45d7710",
        "title": "A competitive modular connectionist architecture",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/f74909ace68e51891440e4da0b65a70c-Abstract.html",
        "author": "Robert A. Jacobs; Michael I. Jordan",
        "abstract": "We describe a multi-network, or modular, connectionist architecture that  captures that fact that many tasks have structure at a level of granularity  intermediate to that assumed by local and global function approximation  schemes. The main innovation of the architecture is that it combines  associative and competitive learning in order to learn task decompositions.  A task decomposition is discovered by forcing the networks comprising the  architecture to compete to learn the training patterns. As a result of the  competition, different networks learn different training patterns and, thus,  learn to partition the input space. The performance of the architecture on  a \"what\" and \"where\" vision task and on a multi-payload robotics task  are presented.",
        "bibtex": "@inproceedings{NIPS1990_f74909ac,\n author = {Jacobs, Robert and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A competitive modular connectionist architecture},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f74909ace68e51891440e4da0b65a70c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/f74909ace68e51891440e4da0b65a70c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/f74909ace68e51891440e4da0b65a70c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1442660,
        "gs_citation": 235,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8293916855177295013&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology; Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain & Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "645d9cb3bf",
        "title": "A four neuron circuit accounts for change sensitive inhibition in salamander retina",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/cf004fdc76fa1a4f25f62e0eb5261ca3-Abstract.html",
        "author": "Jeffrey L. Teeters; Frank H. Eeckman; Frank S. Werblin",
        "abstract": "In salamander retina, the response of On-Off ganglion cells to a central  flash is reduced by movement in the receptive field surround. Through  computer simulation of a 2-D model which takes into account their  anatomical and physiological properties, we show that interactions  between four neuron types (two bipolar and two amacrine) may be  responsible for the generation and lateral conductance of this change  sensitive inhibition. The model shows that the four neuron circuit can  account for previously observed movement sensitive reductions in  ganglion cell sensitivity and allows visualization and prediction of the  spatio-temporal pattern of activity in change sensitive retinal cells.",
        "bibtex": "@inproceedings{NIPS1990_cf004fdc,\n author = {Teeters, Jeffrey and Eeckman, Frank and Werblin, Frank},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A four neuron circuit accounts for change sensitive inhibition in salamander retina},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1722031,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=154184011214061298&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8ccfdf1df6",
        "title": "ALCOVE: A Connectionist Model of Human Category Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/8fe0093bb30d6f8c31474bd0764e6ac0-Abstract.html",
        "author": "John K. Kruschke",
        "abstract": "ALCOVE  is  a  connectionist  model  of human  category  learning  that  fits  a  broad spectrum of human learning data.  Its architecture is  based on well(cid:173) established  psychological  theory,  and  is  related  to  networks  using  radial  basis functions.  From the perspective of cognitive psychology,  ALCOVE can  be construed as a combination of exemplar-based representation and error(cid:173) driven  learning.  From the perspective of connectionism,  it can  be seen  as  incorporating constraints into back-propagation networks  appropriate for  modelling human learning.",
        "bibtex": "@inproceedings{NIPS1990_8fe0093b,\n author = {Kruschke, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {ALCOVE: A Connectionist Model of Human Category Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8fe0093bb30d6f8c31474bd0764e6ac0-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/8fe0093bb30d6f8c31474bd0764e6ac0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/8fe0093bb30d6f8c31474bd0764e6ac0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1598646,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13010457739620599726&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Psychology and Cognitive Science Program, Indiana University, Bloomington IN 47405-4201 USA",
        "aff_domain": "ucs.indiana.edu",
        "email": "ucs.indiana.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "Department of Psychology and Cognitive Science Program",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Bloomington",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5f33e7f497",
        "title": "ART2/BP architecture for adaptive estimation of dynamic processes",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/11b9842e0a271ff252c1903e7132cd68-Abstract.html",
        "author": "Einar S\u00f8rheim",
        "abstract": "The goal has been to construct a supervised artificial neural network that  learns incrementally an unknown mapping. As a result a network con(cid:173) sisting of a combination of ART2 and backpropagation is proposed and  is called an \"ART2/BP\" network. The ART2 network is used to build  and focus a supervised backpropagation network. The ART2/BP network  has the advantage of being able to dynamically expand itself in response  to input patterns containing new information. Simulation results show  that the ART2/BP network outperforms a classical maximum likelihood  method for the estimation of a discrete dynamic and nonlinear transfer  function.",
        "bibtex": "@inproceedings{NIPS1990_11b9842e,\n author = {S\\o rheim, Einar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {ART2/BP architecture for adaptive estimation of dynamic processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/11b9842e0a271ff252c1903e7132cd68-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/11b9842e0a271ff252c1903e7132cd68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/11b9842e0a271ff252c1903e7132cd68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1356091,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9436344457393617990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d3d4e3c2f3",
        "title": "Adaptive Range Coding",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/caf1a3dfb505ffed0d024130f58c5cfa-Abstract.html",
        "author": "Bruce E. Rosen; James M. Goodwin; Jacques J. Vidal",
        "abstract": "these",
        "bibtex": "@inproceedings{NIPS1990_caf1a3df,\n author = {Rosen, Bruce and Goodwin, James and Vidal, Jacques},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Adaptive Range Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/caf1a3dfb505ffed0d024130f58c5cfa-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/caf1a3dfb505ffed0d024130f58c5cfa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/caf1a3dfb505ffed0d024130f58c5cfa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1288080,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10799508707287850436&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e554e4bff3",
        "title": "Adaptive Spline Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/0d0fd7c6e093f7b804fa0150b875b868-Abstract.html",
        "author": "Jerome H. Friedman",
        "abstract": "A network based on splines is  described.  It automatically adapts the num(cid:173) ber of units, unit parameters, and the architecture of the network for  each  application.",
        "bibtex": "@inproceedings{NIPS1990_0d0fd7c6,\n author = {Friedman, Jerome},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Adaptive Spline Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/0d0fd7c6e093f7b804fa0150b875b868-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1933372,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7271380100523171629&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9e69d04be1",
        "title": "Adjoint-Functions and Temporal Learning Algorithms in Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/25b2822c2f5a3230abfadd476e8b04c9-Abstract.html",
        "author": "N. Toomarian; J. Barhen",
        "abstract": "The development of learning algorithms is generally based upon the min(cid:173) imization of an energy function. It is a fundamental requirement to com(cid:173) pute the gradient of this energy function with respect to the various pa(cid:173) rameters of the neural architecture, e.g., synaptic weights, neural gain,etc.  In principle, this requires solving a system of nonlinear equations for each  parameter of the model, which is computationally very expensive. A new  methodology for neural learning of time-dependent nonlinear mappings is  presented. It exploits the concept of adjoint operators to enable a fast  global computation of the network's response to perturbations in all the  systems parameters. The importance of the time boundary conditions of  the adjoint functions is discussed. An algorithm is presented in which  the adjoint sensitivity equations are solved simultaneously (Le., forward  in time) along with the nonlinear dynamics of the neural networks. This  methodology makes real-time applications and hardware implementation  of temporal learning feasible.",
        "bibtex": "@inproceedings{NIPS1990_25b2822c,\n author = {Toomarian, N. and Barhen, J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Adjoint-Functions and Temporal Learning Algorithms in Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/25b2822c2f5a3230abfadd476e8b04c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1727567,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6581619578818127396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Jet Propulsion Laboratory, California Institute of Technology; Jet Propulsion Laboratory, California Institute of Technology",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Jet Propulsion Laboratory",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c622c0f978",
        "title": "An Analog VLSI Chip for Finding Edges from Zero-crossings",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/0deb1c54814305ca9ad266f53bc82511-Abstract.html",
        "author": "Wyeth Bair; Christof Koch",
        "abstract": "We  have  designed  and  tested  a  one-dimensional  64  pixel,  analog  CMOS  VLSI chip which localizes intensity edges in real-time.  This device exploits  on-chip photoreceptors and the natural filtering properties of resistive net(cid:173) works to implement a  scheme similar  to  and  motivated  by  the Difference  of Gaussians (DOG) operator proposed by Marr and Hildreth (1980).  Our  chip computes the  zero-crossings associated  with  the difference of two ex(cid:173) ponential weighting functions.  If the  derivative  across  this  zero-crossing  is  above  a  threshold,  an edge  is  reported.  Simulations  indicate  that  this  technique will  extend well to two dimensions.",
        "bibtex": "@inproceedings{NIPS1990_0deb1c54,\n author = {Bair, Wyeth and Koch, Christof},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Analog VLSI Chip for Finding Edges from Zero-crossings},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/0deb1c54814305ca9ad266f53bc82511-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1147569,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4996836243235555446&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computation and Neural Systems Program, Caltech 216-76, Pasadena, CA 91125; Computation and Neural Systems Program, Caltech 216-76, Pasadena, CA 91125",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Computation and Neural Systems Program",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f13cb5509c",
        "title": "An Analog VLSI Splining Network",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/58238e9ae2dd305d79c2ebc8c1883422-Abstract.html",
        "author": "Daniel B. Schwartz; Vijay K. Samalam",
        "abstract": "We  have produced  a  VLSI  circuit capable of learning  to approximate ar(cid:173) bitrary  smooth  of a  single  variable  using  a  technique  closely  related  to  splines.  The circuit effectively  has  512 knots space on  a  uniform grid and  has full support for  learning.  The circuit  also can be used  to approximate  multi-variable functions as sum of splines.",
        "bibtex": "@inproceedings{NIPS1990_58238e9a,\n author = {Schwartz, Daniel and Samalam, Vijay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Analog VLSI Splining Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/58238e9ae2dd305d79c2ebc8c1883422-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1523742,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6981879356878867320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9db10656ec",
        "title": "An Attractor Neural Network Model of Recall and Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/8e98d81f8217304975ccb23337bb5761-Abstract.html",
        "author": "Eytan Ruppin; Yehezkel Yeshurun",
        "abstract": "This  work  presents  an  Attractor  Neural  Network  (ANN)  model  of Re(cid:173) call  and  Recognition.  It is  shown  that  an  ANN  model  can  qualitatively  account  for  a  wide  range  of experimental  psychological  data  pertaining  to  the  these  two  main  aspects  of memory  access.  Certain  psychological  phenomena  are  accounted  for,  including  the  effects  of list-length,  word(cid:173) frequency,  presentation  time,  context  shift,  and  aging.  Thereafter,  the  probabilities of successful  Recall  and  Recognition  are  estimated,  in  order  to  possibly enable further  quantitative examination of the model.",
        "bibtex": "@inproceedings{NIPS1990_8e98d81f,\n author = {Ruppin, Eytan and Yeshurun, Yehezkel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Attractor Neural Network Model of Recall and Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8e98d81f8217304975ccb23337bb5761-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/8e98d81f8217304975ccb23337bb5761-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/8e98d81f8217304975ccb23337bb5761-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1564688,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10043438321116781014&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, School of Mathematical Sciences, Sackler Faculty of Exact Sciences, Tel Aviv University; Department of Computer Science, School of Mathematical Sciences, Sackler Faculty of Exact Sciences, Tel Aviv University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "b7a685c6cd",
        "title": "Analog Computation at a Critical Point: A Novel Function for Neuronal Oscillations?",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html",
        "author": "Leonid Kruglyak; William Bialek",
        "abstract": "\\Ve show that a simple spin system bia.sed at its critical point can en(cid:173) code spatial characteristics of external signals, sHch as the dimensions of  \"objects\" in the visual field. in the temporal correlation functions of indi(cid:173) vidual spins. Qualit.ative arguments suggest that regularly firing neurons  should be described by a planar spin of unit lengt.h. and such XY models  exhibit critical dynamics over a broad range of parameters. \\Ve show how  to extract these spins from spike trains and then mea'3ure t.he interaction  Hamilt.onian using simulations of small dusters of cells. Static correla(cid:173) tions among spike trains obtained from simulations of large arrays of cells  are in agreement with the predictions from these Hamiltonians, and dy(cid:173) namic correlat.ions display the predicted encoding of spatial information.  \\Ve suggest that this novel representation of object dinwnsions in temporal  correlations may be relevant t.o recent experiment.s on oscillatory neural  firing in the visual cortex.",
        "bibtex": "@inproceedings{NIPS1990_e0c64119,\n author = {Kruglyak, Leonid and Bialek, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Analog Computation at a Critical Point: A Novel Function for Neuronal Oscillations?},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/e0c641195b27425bb056ac56f8953d24-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1580122,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10407910346903468611&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California at Berkeley; University of California at Berkeley + NEC Research Institute",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of California, Berkeley;NEC Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.neclab.eu",
        "aff_unique_abbr": "UC Berkeley;NEC RI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4ce9ce3c9b",
        "title": "Analog Neural Networks as Decoders",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/352fe25daf686bdb4edca223c921acea-Abstract.html",
        "author": "Ruth Erlanson; Yaser Abu-Mostafa",
        "abstract": "Analog neural networks with feedback can be used to implement l((cid:173) Winner-Take-All (KWTA) networks. In turn, KWTA networks can be  used as decoders of a class of nonlinear error-correcting codes. By in(cid:173) terconnecting such KWTA networks, we can construct decoders capable  of decoding more powerful codes. We consider several families of inter(cid:173) connected KWTA networks, analyze their performance in terms of coding  theory metrics, and consider the feasibility of embedding such networks in  VLSI technologies.",
        "bibtex": "@inproceedings{NIPS1990_352fe25d,\n author = {Erlanson, Ruth and Abu-Mostafa, Yaser},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Analog Neural Networks as Decoders},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/352fe25daf686bdb4edca223c921acea-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/352fe25daf686bdb4edca223c921acea-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/352fe25daf686bdb4edca223c921acea-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 861512,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14337716725596204813&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Electrical Engineering, California Institute of Technology, Pasadena, CA 91125; Dept. of Electrical Engineering, California Institute of Technology, Pasadena, CA 91125 + Hughes Network Systems, 10790 Roselle St., San Diego, CA 92121",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "California Institute of Technology;Hughes Network Systems",
        "aff_unique_dep": "Dept. of Electrical Engineering;",
        "aff_unique_url": "https://www.caltech.edu;https://www.hughesnet.com",
        "aff_unique_abbr": "Caltech;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6ee2bec62c",
        "title": "Applications of Neural Networks in Video Signal Processing",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/158f3069a435b314a80bdcb024f8e422-Abstract.html",
        "author": "John C. Pearson; Clay D. Spence; Ronald Sverdlove",
        "abstract": "Although color TV is an established technology, there are a number of  longstanding problems for which neural networks may be suited. Impulse  noise is such a problem, and a modular neural network approach is pre(cid:173) sented in this paper. The training and analysis was done on conventional  computers, while real-time simulations were performed on a massively par(cid:173) allel computer called the Princeton Engine. The network approach was  compared to a conventional alternative, a median filter. Real-time simula(cid:173) tions and quantitative analysis demonstrated the technical superiority of  the neural system. Ongoing work is investigating the complexity and cost  of implementing this system in hardware.",
        "bibtex": "@inproceedings{NIPS1990_158f3069,\n author = {Pearson, John and Spence, Clay D. and Sverdlove, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Applications of Neural Networks in Video Signal Processing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/158f3069a435b314a80bdcb024f8e422-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/158f3069a435b314a80bdcb024f8e422-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/158f3069a435b314a80bdcb024f8e422-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1645463,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16309983341855208280&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3a88b72217",
        "title": "Associative Memory in a Network of `Biological' Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/41f1f19176d383480afa65d325c06ed0-Abstract.html",
        "author": "Wulfram Gerstner",
        "abstract": "The Hopfield network (Hopfield,  1982,1984) provides a simple model of an  associative memory in  a neuronal structure.  This model, however, is based  on highly artificial assumptions, especially the use of formal-two state neu(cid:173) rons  (Hopfield,  1982) or graded-response  neurons  (Hopfield,  1984).  \\Vhat  happens if we  replace  the formal neurons  by 'real' biological neurons?  \\Ve  address  this question  in  two steps.  First, we  show  that a simple model of  a  neuron  can  capture  all  relevant features  of neuron  spiking,  i. e., a  wide  range of spiking frequencies  and a realistic distribution of interspike inter(cid:173) vals.  Second, we construct an associative memory by linking these neurons  together.  The analytical solution for  a  large  and fully  connected  network  shows that the  Hopfield solution  is  valid only for  neurons  with  a short re(cid:173) fractory  period.  If the refractory  period  is  longer  than  a  crit.ical  duration  ie,  the  solutions  are  qualitatively different.  The  associative  character  of  the solutions, however,  is  preserved.",
        "bibtex": "@inproceedings{NIPS1990_41f1f191,\n author = {Gerstner, Wulfram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Associative Memory in a Network of \\textasciigrave Biological\\textquotesingle  Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/41f1f19176d383480afa65d325c06ed0-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/41f1f19176d383480afa65d325c06ed0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/41f1f19176d383480afa65d325c06ed0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1299033,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9425087637935242727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Physics, University of California, Berkeley, CA 94720 + Physik-Department der TU Muenchen, Institut fuer Theoretische Physik, D-8046 Garching bei Muenchen",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "University of California, Berkeley;Technische Universitaet Muenchen",
        "aff_unique_dep": "Department of Physics;Physik-Department, Institut fuer Theoretische Physik",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tum.de",
        "aff_unique_abbr": "UC Berkeley;TUM",
        "aff_campus_unique_index": "0+1",
        "aff_campus_unique": "Berkeley;Garching bei Muenchen",
        "aff_country_unique_index": "0+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "a76cb6b4d1",
        "title": "Asymptotic slowing down of the nearest-neighbor classifier",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/c042f4db68f23406c6cecf84a7ebb0fe-Abstract.html",
        "author": "Robert R. Snapp; Demetri Psaltis; Santosh S. Venkatesh",
        "abstract": "If patterns are drawn from an n-dimensional feature space according to a  probability distribution that obeys a weak smoothness criterion, we show  that the probability that a random input pattern is misclassified by a  nearest-neighbor classifier using M random reference patterns asymptoti(cid:173) cally satisfies",
        "bibtex": "@inproceedings{NIPS1990_c042f4db,\n author = {Snapp, Robert and Psaltis, Demetri and Venkatesh, Santosh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Asymptotic slowing down of the nearest-neighbor classifier},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/c042f4db68f23406c6cecf84a7ebb0fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1524293,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7409929167451756160&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a3dace948c",
        "title": "Back Propagation Implementation on the Adaptive Solutions CNAPS Neurocomputer Chip",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/beed13602b9b0e6ecb5b568ff5058f07-Abstract.html",
        "author": "Hal McCartor",
        "abstract": "The Adaptive Solutions CN APS architecture chip is a general purpose  neurocomputer chip. It has 64 processors, each with 4 K bytes of local  memory, running at 25 megahertz. It is capable of implementing most  current neural network algorithms with on chip learning. This paper dis(cid:173) cusses the implementation of the Back Propagation algorithm on an array  of these chips and shows performance figures from a clock accurate hard(cid:173) ware simulator. An eight chip configuration on one board can update 2.3  billion connections per second in learning mode and process 9.6 billion  connections per second in feed forward mode.",
        "bibtex": "@inproceedings{NIPS1990_beed1360,\n author = {McCartor, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Back Propagation Implementation on the Adaptive Solutions CNAPS Neurocomputer Chip},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/beed13602b9b0e6ecb5b568ff5058f07-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 843346,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1469599085638523102&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Adaptive Solutions Inc.",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Adaptive Solutions Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d4a335f4d9",
        "title": "Back Propagation is Sensitive to Initial Conditions",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html",
        "author": "John F. Kolen; Jordan B. Pollack",
        "abstract": "functions  with",
        "bibtex": "@inproceedings{NIPS1990_1543843a,\n author = {Kolen, John and Pollack, Jordan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Back Propagation is Sensitive to Initial Conditions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/1543843a4723ed2ab08e18053ae6dc5b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1990395,
        "gs_citation": 568,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7100852602254383049&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "Laboratory for Artificial Intelligence Research, The Ohio State University, Columbus. OH 43210. USA; Laboratory for Artificial Intelligence Research, The Ohio State University, Columbus. OH 43210. USA",
        "aff_domain": "cis.ohio-state.edu;cis.ohio-state.edu",
        "email": "cis.ohio-state.edu;cis.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ohio State University",
        "aff_unique_dep": "Laboratory for Artificial Intelligence Research",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Columbus",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3983877a28",
        "title": "Basis-Function Trees as a Generalization of Local Variable Selection Methods for Function Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html",
        "author": "Terence D. Sanger",
        "abstract": "Local variable selection has proven to be a powerful technique for ap(cid:173) proximating functions in high-dimensional spaces. It is used in several  statistical methods, including CART, ID3, C4, MARS, and others (see the  bibliography for references to these algorithms). In this paper I present  a tree-structured network which is a generalization of these techniques.  The network provides a framework for understanding the behavior of such  algorithms and for modifying them to suit particular applications.",
        "bibtex": "@inproceedings{NIPS1990_40008b9a,\n author = {Sanger, Terence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Basis-Function Trees as a Generalization of Local Variable Selection Methods for Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/40008b9a5380fcacce3976bf7c08af5b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1481848,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14121241782612686231&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. Electrical Engineering and Computer Science, Massachusetts Institute of Technology",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "499dc61cba",
        "title": "Bumptrees for Efficient Function, Constraint and Classification Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html",
        "author": "Stephen M. Omohundro",
        "abstract": "A  new  class of data  structures called \"bumptrees\" is described.  These  structures  are  useful  for  efficiently  implementing  a  number  of neural  network related operations.  An empirical comparison with radial  basis  functions  is presented on a  robot ann mapping learning task.  Applica(cid:173) tions to density estimation. classification. and constraint representation  and learning are also outlined.",
        "bibtex": "@inproceedings{NIPS1990_950a4152,\n author = {Omohundro, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Bumptrees for Efficient Function, Constraint and Classification Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/950a4152c2b4aa3ad78bdd6b366cc179-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/950a4152c2b4aa3ad78bdd6b366cc179-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/950a4152c2b4aa3ad78bdd6b366cc179-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1623243,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9123763881832429167&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "International Computer Science Institute",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "International Computer Science Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.icsi.berkeley.edu/",
        "aff_unique_abbr": "ICSI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1c924a1ef2",
        "title": "CAM Storage of Analog Patterns and Continuous Sequences with 3N2 Weights",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/6855456e2fe46a9d49d3d3af4f57443d-Abstract.html",
        "author": "Bill Baird; Frank Eeckman",
        "abstract": "A simple architecture and algorithm for analytically guaranteed associa(cid:173) tive memory storage of analog patterns, continuous sequences, and chaotic  attractors in the same network is described. A matrix inversion determines  network weights, given prototype patterns to be stored. There are N units  of capacity in an N node network with 3N 2 weights. It costs one unit per  static attractor, two per Fourier component of each sequence, and four per  chaotic attractor. There are no spurious attractors, and there is a Lia(cid:173) punov function in a special coordinate system which governs the approach  of transient states to stored trajectories. Unsupervised or supervised incre(cid:173) mental learning algorithms for pattern classification, such as competitive  learning or bootstrap Widrow-Hoff can easily be implemented. The archi(cid:173) tecture can be \"folded\" into a recurrent network with higher order weights  that can be used as a model of cortex that stores oscillatory and chaotic  attractors by a Hebb rule. Hierarchical sensory-motor control networks  may be constructed of interconnected \"cortical patches\" of these network  modules. Network performance is being investigated by application to the  problem of real time handwritten digit recognition.",
        "bibtex": "@inproceedings{NIPS1990_6855456e,\n author = {Baird, Bill and Eeckman, Frank},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {CAM Storage of Analog Patterns and Continuous Sequences with 3N2 Weights},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6855456e2fe46a9d49d3d3af4f57443d-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/6855456e2fe46a9d49d3d3af4f57443d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/6855456e2fe46a9d49d3d3af4f57443d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1797705,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14419483253979673795&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Dept Mathematics and Dept Molecular and Cell Biology, 129 LSA, U.C.Berkeley, Berkeley, Ca. 94720; Lawrence Livermore National Laboratory, P.O. Box 808 (L-426), Livermore, Ca. 94550",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Lawrence Livermore National Laboratory",
        "aff_unique_dep": "Dept Mathematics;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.llnl.gov",
        "aff_unique_abbr": "UC Berkeley;LLNL",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Berkeley;Livermore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "67b59d3de6",
        "title": "Can neural networks do better than the Vapnik-Chervonenkis bounds?",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/816b112c6105b3ebd537828a39af4818-Abstract.html",
        "author": "David Cohn; Gerald Tesauro",
        "abstract": "\\Ve describe a series of careful llumerical experiments which measure the  average generalization capability of neural networks trained on a variety of  simple functions. These experiments are designed to test whether average  generalization performance can surpass the worst-case bounds obtained  from formal learning theory using the Vapnik-Chervonenkis dimension  (Blumer et al., 1989). We indeed find that, in some cases, the average  generalization is significantly better than the VC bound: the approach to  perfect performance is exponential in the number of examples m, rather  than the 11m result of the bound. In other cases, we do find the 11m  behavior of the VC bound, and in these cases, the numerical prefactor is  closely related to prefactor contained in the bound.",
        "bibtex": "@inproceedings{NIPS1990_816b112c,\n author = {Cohn, David and Tesauro, Gerald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Can neural networks do better than the Vapnik-Chervonenkis bounds?},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/816b112c6105b3ebd537828a39af4818-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/816b112c6105b3ebd537828a39af4818-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/816b112c6105b3ebd537828a39af4818-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1633792,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8116124620859458839&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Dept. of Compo Sci. & Eng., University of Washington, Seattle, WA 98195; IBM Watson Research Center, P.O. Box 704, Yorktown Heights, NY 10598",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Washington;IBM",
        "aff_unique_dep": "Department of Computer Science & Engineering;IBM Watson Research Center",
        "aff_unique_url": "https://www.washington.edu;https://www.ibm.com/watson",
        "aff_unique_abbr": "UW;IBM Watson",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Seattle;Yorktown Heights",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ae10e53cbe",
        "title": "Chaitin-Kolmogorov Complexity and Generalization in Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/28f0b864598a1291557bed248a998d4e-Abstract.html",
        "author": "Barak A. Pearlmutter; Ronald Rosenfeld",
        "abstract": "We  present  a  unified  framework  for  a  number  of different  ways  of failing  to  generalize  properly.  During  learning,  sources  of random  information  contaminate  the  network,  effectively  augmenting  the  training  data  with  random information. The complexity of the function computed is therefore  increased,  and generalization is degraded.  We analyze replicated networks,  in  which  a  number of identical networks are independently trained on  the  same data and their results averaged.  We conclude that replication almost  always results in a decrease in the expected complexity of the network, and  that  replication  therefore  increases  expected  generalization.  Simulations  confirming the effect  are  also presented.",
        "bibtex": "@inproceedings{NIPS1990_28f0b864,\n author = {Pearlmutter, Barak and Rosenfeld, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Chaitin-Kolmogorov Complexity and Generalization in Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/28f0b864598a1291557bed248a998d4e-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/28f0b864598a1291557bed248a998d4e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/28f0b864598a1291557bed248a998d4e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1447060,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10288511882313205442&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cea696d8a9",
        "title": "Cholinergic Modulation May Enhance Cortical Associative Memory Function",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/b2eb7349035754953b57a32e2841bda5-Abstract.html",
        "author": "Michael E. Hasselmo; Brooke P. Anderson; James M. Bower",
        "abstract": "Combining neuropharmacological experiments with computational model(cid:173) ing, we have shown that cholinergic modulation may enhance associative  memory function in piriform (olfactory) cortex. We have shown that the  acetylcholine analogue carbachol selectively suppresses synaptic transmis(cid:173) sion between cells within piriform cortex, while leaving input connections  unaffected. When tested in a computational model of piriform cortex,  this selective suppression, applied during learning, enhances associative  memory performance.",
        "bibtex": "@inproceedings{NIPS1990_b2eb7349,\n author = {Hasselmo, Michael and Anderson, Brooke and Bower, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Cholinergic Modulation May Enhance Cortical Associative Memory Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b2eb7349035754953b57a32e2841bda5-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/b2eb7349035754953b57a32e2841bda5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/b2eb7349035754953b57a32e2841bda5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1490875,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14432287182912677052&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computation and Neural Systems, Caltech 139-74, Pasadena, CA 91125; Computation and Neural Systems, Caltech 139-74, Pasadena, CA 91125; Computation and Neural Systems, Caltech 216-76, Pasadena, CA 91125",
        "aff_domain": "sma.ug.cns.caltech.edu;hope.caltech.edu; ",
        "email": "sma.ug.cns.caltech.edu;hope.caltech.edu; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Computation and Neural Systems",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f88fcf2383",
        "title": "Closed-Form Inversion of Backpropagation Networks: Theory and Optimization Issues",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/06eb61b839a0cefee4967c67ccb099dc-Abstract.html",
        "author": "Michael L. Rossen",
        "abstract": "We describe a closed-form technique for mapping the output of a trained  backpropagation network int.o input activity space. The mapping is an in(cid:173) verse mapping in the sense that, when the image of the mapping in input  activity space is propagat.ed forward through the normal network dynam(cid:173) ics, it reproduces the output used to generate that image. When more  than one such inverse mappings exist, our inverse ma.pping is special in  that it has no projection onto the nullspace of the activation flow opera(cid:173) tor for the entire network. An important by-product of our calculation,  when more than one invel'se mappings exist, is an orthogonal basis set of  a significant portion of the activation flow operator nullspace. This basis  set can be used to obtain an alternate inverse mapping that is optimized  for a particular rea.l-world application.",
        "bibtex": "@inproceedings{NIPS1990_06eb61b8,\n author = {Rossen, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Closed-Form Inversion of Backpropagation Networks: Theory and Optimization Issues},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/06eb61b839a0cefee4967c67ccb099dc-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/06eb61b839a0cefee4967c67ccb099dc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/06eb61b839a0cefee4967c67ccb099dc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1038069,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17295019009098519384&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "HNC, Inc.",
        "aff_domain": "amos.ucsd.edu",
        "email": "amos.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "HNC, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "77cf8d7470",
        "title": "Compact EEPROM-based Weight Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/6ecbdd6ec859d284dc13885a37ce8d81-Abstract.html",
        "author": "A. Kramer; C. K. Sin; R. Chu; P. K. Ko",
        "abstract": "We are focusing on the development of a  highly compact neural net weight  function based on the use of EEPROM devices.  These devices have already  proven  useful  for  analog  weight storage,  but  existing  designs  rely  on  the  use  of conventional voltage multiplication as the weight function,  requiring  additional  transistors  per  synapse.  A  parasitic  capacitance  between  the  floating gate and the drain of the EEPROM structure leads to an unusual  J-V characteristic  which can  be used  to advantage in  designing a compact  synapse.  This  novel  behavior  is  well  characterized  by  a  model  we  have  developed.  A single-device circuit results in a  1-quadrant synapse function  which  is  nonlinear,  though  monotonic.  A  simple  extension  employing  2  EEPROMs  results  in  a  2  quadrant  function  which  is  much  more  linear.  This approach offers  the  potential for  more than a  ten-fold  increase  in  the  density of neural net  implementations.",
        "bibtex": "@inproceedings{NIPS1990_6ecbdd6e,\n author = {Kramer, A. and Sin, C. and Chu, R. and Ko, P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Compact EEPROM-based Weight Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/6ecbdd6ec859d284dc13885a37ce8d81-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1495653,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10283227999235579997&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "17b7e4b724",
        "title": "Comparison of three classification techniques: CART, C4.5 and Multi-Layer Perceptrons",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/1068c6e4c8051cfd4e9ea8072e3189e2-Abstract.html",
        "author": "A. C. Tsoi; R. A. Pearson",
        "abstract": "In this paper, after some introductory remarks into the classification prob(cid:173) lem as considered in various research communities, and some discussions  concerning some of the reasons for ascertaining the performances of the  three chosen algorithms, viz., CART (Classification and Regression Tree),  C4.5 (one of the more recent versions of a popular induction tree tech(cid:173) nique known as ID3), and a multi-layer perceptron (MLP), it is proposed  to compare the performances of these algorithms under two criteria: classi(cid:173) fication and generalisation. It is found that, in general, the MLP has better  classification and generalisation accuracies compared with the other two  algorithms.",
        "bibtex": "@inproceedings{NIPS1990_1068c6e4,\n author = {Tsoi, A. C. and Pearson, R.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Comparison of three classification techniques: CART, C4.5 and Multi-Layer Perceptrons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/1068c6e4c8051cfd4e9ea8072e3189e2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1675332,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5821264031038265604&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Electrical Engineering + Department of Computer Science, University of Queensland, St Lucia, Queensland 4072, Australia; Aust Defence Force Academy, Campbell, ACT 2600, Australia",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Institution not specified;University of Queensland;Australian Defence Force Academy",
        "aff_unique_dep": "Department of Electrical Engineering;Department of Computer Science;",
        "aff_unique_url": ";https://www.uq.edu.au;https://www.adfa.edu.au",
        "aff_unique_abbr": ";UQ;ADFA",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";St Lucia;Campbell",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";Australia"
    },
    {
        "id": "a527073934",
        "title": "Computing with Arrays of Bell-Shaped and Sigmoid Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/b9228e0962a78b84f3d5d92f4faa000b-Abstract.html",
        "author": "Pierre Baldi",
        "abstract": "We consider feed-forward neural networks with one non-linear hidden layer  and  linear  output units.  The transfer  function  in the hidden  layer  are  ei(cid:173) ther  bell-shaped  or sigmoid.  In the bell-shaped  case,  we  show  how  Bern(cid:173) stein polynomials on one hand and the theory of the heat equation on the  other  are  relevant  for  understanding the  properties  of the  corresponding  networks.  In  particular,  these  techniques  yield  simple proofs of universal  approximation properties, i.e.  of the fact that any reasonable function can  be approximated to any degree of precision by a linear combination of bell(cid:173) shaped functions.  In addition, in  this  framework  the  problem of learning  is equivalent to the problem of reversing the time course of a diffusion pro(cid:173) cess.  The  results  obtained in  the bell-shaped  case  can  then  be  applied  to  the case  of sigmoid  transfer functions  in the hidden layer,  yielding similar  universality  results.  A  conjecture  related  to the problem of generalization  is  briefly  examined.",
        "bibtex": "@inproceedings{NIPS1990_b9228e09,\n author = {Baldi, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Computing with Arrays of Bell-Shaped and Sigmoid Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b9228e0962a78b84f3d5d92f4faa000b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/b9228e0962a78b84f3d5d92f4faa000b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/b9228e0962a78b84f3d5d92f4faa000b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1717074,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9321345614197691036&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3e93286fd8",
        "title": "Connection Topology and Dynamics in Lateral Inhibition Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/13f9896df61279c928f19721878fac41-Abstract.html",
        "author": "C.M Marcus; F. R. Waugh; R. M. Westervelt",
        "abstract": "We show analytically how the stability of two-dimensional lateral  inhibition neural networks depends on the local connection topology.  For various network topologies, we calculate the critical time delay for  the onset of oscillation in continuous-time networks and present  analytic phase diagrams characterizing the dynamics of discrete-time  networks.",
        "bibtex": "@inproceedings{NIPS1990_13f9896d,\n author = {Marcus, C.M and Waugh, F. and Westervelt, R.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Connection Topology and Dynamics in Lateral Inhibition Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/13f9896df61279c928f19721878fac41-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/13f9896df61279c928f19721878fac41-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/13f9896df61279c928f19721878fac41-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1591698,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13158885030978531613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4add76a8d4",
        "title": "Connectionist Approaches to the Use of Markov Models for Speech Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/c058f544c737782deacefa532d9add4c-Abstract.html",
        "author": "Herv\u00e9 Bourlard; Nelson Morgan; Chuck Wooters",
        "abstract": "Previous  work  has  shown  the  ability  of  Multilayer  Perceptrons  (MLPs) to estimate emission probabilities for Hidden Markov Mod(cid:173) els  (HMMs).  The advantages of a  speech recognition system incor(cid:173) porating  both  MLPs  and  HMMs  are  the  best  discrimination  and  the  ability  to  incorporate  multiple  sources  of evidence  (features,  temporal context) without restrictive assumptions of distributions  or  statistical  independence.  This  paper  presents  results  on  the  speaker-dependent portion of DARPA's English language Resource  Management  database.  Results  support  the  previously  reported  utility of MLP probability estimation for  continuous speech recog(cid:173) nition.  An additional approach we  are pursuing is to use  MLPs as  nonlinear predictors for autoregressive HMMs.  While this is shown  to  be  more  compatible  with  the  HMM  formalism,  it  still  suffers  from several limitations.  This approach  is  generalized  to  take ac(cid:173) count of time correlation between successive observations, without  any restrictive assumptions about the driving noise.",
        "bibtex": "@inproceedings{NIPS1990_c058f544,\n author = {Bourlard, Herv\\'{e} and Morgan, Nelson and Wooters, Chuck},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Connectionist Approaches to the Use of Markov Models for Speech Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c058f544c737782deacefa532d9add4c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/c058f544c737782deacefa532d9add4c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/c058f544c737782deacefa532d9add4c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1779706,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3530859773683439506&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "L & H Speechproducts; IntI. Compo Sc. Institute; IntI. Compo Sc. Institute",
        "aff_domain": "t; ~; ~",
        "email": "t; ~; ~",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "L & H Speechproducts;International Computer Science Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.icsi.berkeley.edu/",
        "aff_unique_abbr": ";ICSI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "1adbc9b50a",
        "title": "Connectionist Implementation of a Theory of Generalization",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/efe937780e95574250dabe07151bdc23-Abstract.html",
        "author": "Roger N. Shepard; Sheila Kannappan",
        "abstract": "Empirically,  generalization between  a  training  and  a  test  stimulus  falls  off in  close approximation to  an  exponential decay  function  of distance between  the  two stimuli in the \"stimulus space\" obtained by multidimensional scaling.  Math(cid:173) ematically, this result is derivable from  the assumption that an  individual takes  the  training  stimulus  to  belong  to  a  \"consequential\"  region  that includes  that  stimulus but is  otherwise of unknown  location, size, and  shape in the stimulus  space (Shepard, 1987).  As the individual gains additional information about the  consequential region-by finding other stimuli to be consequential or nOl-the  theory  predicts  the shape  of the generalization  function  to  change  toward  the  function relating actual probability of the consequence to location in the stimulus  space.  This paper describes a natural connectionist implementation of the theory,  and illustrates how implications of the theory for generalization, discrimination,  and classification learning can be explored by connectionist simulation.",
        "bibtex": "@inproceedings{NIPS1990_efe93778,\n author = {Shepard, Roger and Kannappan, Sheila},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Connectionist Implementation of a Theory of Generalization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/efe937780e95574250dabe07151bdc23-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/efe937780e95574250dabe07151bdc23-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/efe937780e95574250dabe07151bdc23-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1797083,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5602547192924481774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Psychology, Stanford University; Department of Physics, Harvard University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Harvard University",
        "aff_unique_dep": "Department of Psychology;Department of Physics",
        "aff_unique_url": "https://www.stanford.edu;https://www.harvard.edu",
        "aff_unique_abbr": "Stanford;Harvard",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Stanford;Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5a3a08fd8a",
        "title": "Connectionist Music Composition Based on Melodic and Stylistic Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/75fc093c0ee742f6dddaa13fff98f104-Abstract.html",
        "author": "Michael Mozer; Todd Soukup",
        "abstract": "We describe  a  recurrent  connectionist  network,  called CONCERT,  that  uses  a  set  of  melodies  written  in  a  given  style  to  compose  new  melodies  in  that  style. CONCERT  is  an extension of a traditional algorithmic composition tech(cid:173) nique  in  which  transition  tables specify  the  probability  of the  next  note  as  a  function of previous context.  A central ingredient of CONCERT  is the use of a  psychologically-grounded representation of pitch.",
        "bibtex": "@inproceedings{NIPS1990_75fc093c,\n author = {Mozer, Michael C and Soukup, Todd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Connectionist Music Composition Based on Melodic and Stylistic Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/75fc093c0ee742f6dddaa13fff98f104-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1908325,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7054568184625065984&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Institute of Cognitive Science, University of Colorado, Boulder, CO 80309-0430; Department of Electrical and Computer Engineering, University of Colorado, Boulder, CO 80309-0425",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Colorado Boulder;University of Colorado",
        "aff_unique_dep": "Department of Computer Science and Institute of Cognitive Science;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.colorado.edu;https://www.colorado.edu",
        "aff_unique_abbr": "CU Boulder;CU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a1a6d6b2c6",
        "title": "Constructing Hidden Units using Examples and Queries",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/34ed066df378efacc9b924ec161e7639-Abstract.html",
        "author": "Eric B. Baum; Kevin J. Lang",
        "abstract": "While the network loading problem for 2-layer threshold nets is  NP-hard when learning from examples alone (as with backpropaga(cid:173) tion), (Baum, 91) has now proved that a learner can employ queries  to evade the hidden unit credit assignment problem and PAC-load  nets with up to four hidden units in polynomial time. Empirical  tests show that the method can also learn far more complicated  functions such as randomly generated networks with 200 hidden  units. The algorithm easily approximates Wieland's 2-spirals func(cid:173) tion using a single layer of 50 hidden units, and requires only 30  minutes of CPU time to learn 200-bit parity to 99.7% accuracy.",
        "bibtex": "@inproceedings{NIPS1990_34ed066d,\n author = {Baum, Eric and Lang, Kevin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Constructing Hidden Units using Examples and Queries},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/34ed066df378efacc9b924ec161e7639-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/34ed066df378efacc9b924ec161e7639-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/34ed066df378efacc9b924ec161e7639-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1673124,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15131217495439118453&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "36e38c1c90",
        "title": "Continuous Speech Recognition by Linked Predictive Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/aa942ab2bfa6ebda4840e7360ce6e7ef-Abstract.html",
        "author": "Joe Tebelskis; Alex Waibel; Bojan Petek; Otto Schmidbauer",
        "abstract": "We present a large vocabulary, continuous speech recognition system based  on  Linked  Predictive  Neural  Networks  (LPNN's).  The system  uses  neu(cid:173) ral  networks  as  predictors  of speech  frames,  yielding  distortion  measures  which  are  used  by  the  One Stage DTW algorithm to perform  continuous  speech  recognition.  The system,  already  deployed  in  a  Speech  to Speech  Translation system, currently achieves 95%,  58%,  and 39% word accuracy  on  tasks  with  perplexity  5,  111,  and  402  respectively,  outperforming sev(cid:173) eral simple HMMs  that  we  tested.  We  also  found  that  the  accuracy  and  speed of the LPNN can be slightly improved by the judicious use of hidden  control  inputs.  We  conclude  by  discussing  the  strengths  and  weaknesses  of the predictive approach.",
        "bibtex": "@inproceedings{NIPS1990_aa942ab2,\n author = {Tebelskis, Joe and Waibel, Alex and Petek, Bojan and Schmidbauer, Otto},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Continuous Speech Recognition by Linked Predictive Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1731520,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12707052213675323592&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "49ad2bc53b",
        "title": "Convergence of a Neural Network Classifier",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/f4f6dce2f3a0f9dada0c2b5b66452017-Abstract.html",
        "author": "John S. Baras; Anthony LaVigna",
        "abstract": "In  this  paper,  we  prove  that  the  vectors  in  the  LVQ  learning  algorithm  converge.  We  do  this  by  showing  that  the  learning  algorithm  performs  stochastic  approximation.  Convergence  is  then  obtained  by  identifying  the  appropriate  conditions  on  the  learning  rate  and  on  the  underlying  statistics of the  classification  problem.  We  also  present  a  modification  to  the  learning algorithm which  we  argue  results  in  convergence  of the  LVQ  error to the Bayesian optimal error as  the appropriate parameters become  large.",
        "bibtex": "@inproceedings{NIPS1990_f4f6dce2,\n author = {Baras, John and LaVigna, Anthony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Convergence of a Neural Network Classifier},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f4f6dce2f3a0f9dada0c2b5b66452017-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/f4f6dce2f3a0f9dada0c2b5b66452017-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/f4f6dce2f3a0f9dada0c2b5b66452017-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1202976,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13108452169910718650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Systems Research Center, University of Maryland, College Park, Maryland 20705; Systems Research Center, University of Maryland, College Park, Maryland 20705",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Systems Research Center",
        "aff_unique_url": "https://www.umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "16a7422463",
        "title": "Design and Implementation of a High Speed CMAC Neural Network Using Programmable CMOS Logic Cell Arrays",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Abstract.html",
        "author": "W. Thomas Miller III; Brian A. Box; Erich C. Whitney; James M. Glynn",
        "abstract": "A high speed implementation of the CMAC neural network was designed  using dedicated CMOS logic. This technology was then used to implement  two general purpose CMAC associative memory boards for the VME bus.  Each board implements up to 8 independent CMAC networks with a total  of one million adjustable weights. Each CMAC network can be configured  to have from 1 to 512 integer inputs and from 1 to 8 integer outputs.  Response times for typical CMAC networks are well below 1 millisecond,  making the networks sufficiently fast for most robot control problems, and  many pattern recognition and signal processing problems.",
        "bibtex": "@inproceedings{NIPS1990_4f4adcbf,\n author = {Miller, W. and Box, Brian and Whitney, Erich and Glynn, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Design and Implementation of a High Speed CMAC Neural Network Using Programmable CMOS Logic Cell Arrays},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1232286,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9420993774066165814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "24894781d5",
        "title": "Designing Linear Threshold Based Neural Network Pattern Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html",
        "author": "Terrence L. Fine",
        "abstract": "The three problems that concern us are identifying a natural domain of  pattern classification applications of feed forward neural networks, select(cid:173) ing an appropriate feedforward network architecture, and assessing the  tradeoff between network complexity, training set size, and statistical reli(cid:173) ability as measured by the probability of incorrect classification. We close  with some suggestions, for improving the bounds that come from Vapnik(cid:173) Chervonenkis theory, that can narrow, but not close, the chasm between  theory and practice.",
        "bibtex": "@inproceedings{NIPS1990_5a4b25aa,\n author = {Fine, Terrence L.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Designing Linear Threshold Based Neural Network Pattern Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/5a4b25aaed25c2ee1b74de72dc03c14e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1713522,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12826956164223806603&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Electrical Engineering, Cornell University, Ithaca, NY 14853",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "School of Electrical Engineering",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ithaca",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "16d91d39b6",
        "title": "Development and Spatial Structure of Cortical Feature Maps: A Model Study",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/ef0d3930a7b6c95bd2b32ed45989c61f-Abstract.html",
        "author": "Klaus Obermayer; Helge Ritter; Klaus Schulten",
        "abstract": "K.  Schulten  Beckman -Insti t u te  University  of Illinois  Urbana, IL  61801",
        "bibtex": "@inproceedings{NIPS1990_ef0d3930,\n author = {Obermayer, Klaus and Ritter, Helge and Schulten, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Development and Spatial Structure of Cortical Feature Maps: A Model Study},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/ef0d3930a7b6c95bd2b32ed45989c61f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1739089,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12563249765629772391&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ca9eea2ffc",
        "title": "Direct memory access using two cues: Finding the intersection of sets in a connectionist model",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html",
        "author": "Janet Wiles; Michael S. Humphreys; John D. Bain; Simon Dennis",
        "abstract": "For lack of alternative models, search and decision processes have provided the  dominant paradigm for human memory access using two or more cues, despite  evidence against search as an access process (Humphreys, Wiles & Bain, 1990).  We present an alternative process to search, based on calculating the intersection  of sets of targets activated by two or more cues. Two methods of computing  the intersection are presented, one using information about the possible targets,  the other constraining the cue-target strengths in the memory matrix. Analysis  using orthogonal vectors to represent the cues and targets demonstrates the  competence of both processes, and simulations using sparse distributed  representations demonstrate the performance of the latter process for tasks  involving 2 and 3 cues.",
        "bibtex": "@inproceedings{NIPS1990_a49e9411,\n author = {Wiles, Janet and Humphreys, Michael and Bain, John and Dennis, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Direct memory access using two cues: Finding the intersection of sets in a connectionist model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/a49e9411d64ff53eccfdd09ad10a15b3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1709388,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11723355773303262443&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Departments of Psychology and Computer Science, University of Queensland QLD 4072 Australia; Departments of Psychology and Computer Science, University of Queensland QLD 4072 Australia; Departments of Psychology and Computer Science, University of Queensland QLD 4072 Australia; Departments of Psychology and Computer Science, University of Queensland QLD 4072 Australia",
        "aff_domain": "psych.psy.uq.oz.au; ; ; ",
        "email": "psych.psy.uq.oz.au; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Queensland",
        "aff_unique_dep": "Departments of Psychology and Computer Science",
        "aff_unique_url": "https://www.uq.edu.au",
        "aff_unique_abbr": "UQ",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "QLD",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "0d2f3feaa4",
        "title": "Discovering Discrete Distributed Representations with Iterative Competitive Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/819f46e52c25763a55cc642422644317-Abstract.html",
        "author": "Michael Mozer",
        "abstract": "Competitive learning is an unsupervised algorithm that classifies input pat(cid:173) terns into mutually exclusive clusters. In a neural net framework, each clus(cid:173) ter is represented by a processing unit that competes with others in a winner(cid:173) take-all pool for an input pattern. I present a simple extension to the algo(cid:173) rithm that allows it to construct discrete, distributed representations. Discrete  representations are useful because they are relatively easy to analyze and  their information content can readily be measured. Distributed representa(cid:173) tions are useful because they explicitly encode similarity. The basic idea is  to apply competitive learning iteratively to an input pattern, and after each  stage to subtract from the input pattern the component that was captured in  the representation at that stage. This component is simply the weight vector  of the winning unit of the competitive pool. The subtraction procedure forces  competitive pools at different stages to encode different aspects of the input.  The algorithm is essentially the same as a traditional data compression tech(cid:173) nique known as multistep vector quantization, although the neural net per(cid:173) spective suggests potentially powerful extensions to that approach.",
        "bibtex": "@inproceedings{NIPS1990_819f46e5,\n author = {Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Discovering Discrete Distributed Representations with Iterative Competitive Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/819f46e52c25763a55cc642422644317-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/819f46e52c25763a55cc642422644317-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/819f46e52c25763a55cc642422644317-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1639612,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11359121800119413223&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Institute of Cognitive Science, University of Colorado, Boulder, CO 80309-0430",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Colorado Boulder",
        "aff_unique_dep": "Department of Computer Science and Institute of Cognitive Science",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU Boulder",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1cf5b8dd79",
        "title": "Discovering Viewpoint-Invariant Relationships That Characterize Objects",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html",
        "author": "Richard S. Zemel; Geoffrey E. Hinton",
        "abstract": "Using  an  unsupervised  learning  procedure,  a  network  is  trained on  an en(cid:173) semble of images of the same two-dimensional object at different positions,  orientations  and  sizes.  Each  half of  the  network  \"sees\"  one  fragment  of  the object, and  tries to produce  as  output a set of 4 parameters that have  high mutual information with the 4 parameters output by  the other half of  the network.  Given the ensemble of training patterns, the 4 parameters on  which the two halves of the network can agree are the position, orientation,  and  size  of the  whole  object,  or  some  recoding  of them.  After  training,  the network can reject  instances of other shapes by  using the fact  that the  predictions  made  by  its  two  halves  disagree.  If two  competing  networks  are trained on  an unlabelled mixture of images of two objects, they  cluster  the training cases on the basis of the objects' shapes,  independently of the  position, orientation, and size.",
        "bibtex": "@inproceedings{NIPS1990_6faa8040,\n author = {Zemel, Richard and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Discovering Viewpoint-Invariant Relationships That Characterize Objects},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/6faa8040da20ef399b63a72d0e4ab575-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1729454,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7127819763326456855&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "297743cd74",
        "title": "Discrete Affine Wavelet Transforms For Anaylsis And Synthesis Of Feedfoward Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html",
        "author": "Y. C. Pati; P. S. Krishnaprasad",
        "abstract": "In this paper we show that discrete affine wavelet transforms can provide  a tool for the analysis and synthesis of standard feedforward neural net(cid:173) works. It is shown that wavelet frames for L2(IR) can be constructed based  upon sigmoids. The spatia-spectral localization property of wavelets can  be exploited in defining the topology and determining the weights of a  feedforward network. Training a network constructed using the synthe(cid:173) sis procedure described here involves minimization of a convex cost func(cid:173) tional and therefore avoids pitfalls inherent in standard backpropagation  algorithms. Extension of these methods to L2(IRN) is also discussed.",
        "bibtex": "@inproceedings{NIPS1990_f2fc9902,\n author = {Pati, Y. and Krishnaprasad, P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Discrete Affine Wavelet Transforms For Anaylsis And Synthesis Of Feedfoward Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/f2fc990265c712c49d51a18a32b39f0c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1338072,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12726516305662325178&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Systems Research Center and Department of Electrical Engineering, University of Maryland, College Park, MD 20742; Systems Research Center and Department of Electrical Engineering, University of Maryland, College Park, MD 20742",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "92528583ff",
        "title": "Distributed Recursive Structure Processing",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/8cb22bdd0b7ba1ab13d742e22eed8da2-Abstract.html",
        "author": "Geraldine Legendre; Yoshiro Miyata; Paul Smolensky",
        "abstract": "Harmonic grammar (Legendre,  et al., 1990) is a connectionist theory of lin(cid:173) guistic  well-formed ness  based on the assumption  that the well-formedness  of a  sentence  can  be  measured  by  the  harmony  (negative  energy)  of the  corresponding  connectionist  state.  Assuming  a  lower-level  connectionist  network that obeys a  few  general connectionist  principles  but is  otherwise  unspecified,  we  construct  a  higher-level  network  with  an  equivalent  har(cid:173) mony function  that captures the most linguistically relevant global aspects  of the  lower  level  network.  In  this  paper,  we  extend  the  tensor  product  representation  (Smolensky  1990)  to fully  recursive  representations  of re(cid:173) cursively  structured objects like  sentences  in  the lower-level  network.  We  show  theoretically  and  with  an  example  the  power  of the  new  technique  for  parallel distributed  structure  processing.",
        "bibtex": "@inproceedings{NIPS1990_8cb22bdd,\n author = {Legendre, Geraldine and Miyata, Yoshiro and Smolensky, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Distributed Recursive Structure Processing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1712770,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18009071939397139751&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Linguistics, University of Colorado Boulder, CO 80309-0430; Optoelectronic Computing Systems Center; Department of Computer Science",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Colorado Boulder;Optoelectronic Computing Systems Center;Unknown Institution",
        "aff_unique_dep": "Department of Linguistics;;Department of Computer Science",
        "aff_unique_url": "https://www.colorado.edu;;",
        "aff_unique_abbr": "CU Boulder;;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Boulder;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "723ce6a60b",
        "title": "Dynamics of Generalization in Linear Perceptrons",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/0bb4aec1710521c12ee76289d9440817-Abstract.html",
        "author": "Anders Krogh; John A. Hertz",
        "abstract": "We study the evolution of the generalization ability of a  simple linear per(cid:173) ceptron with N  inputs which learns to imitate a  \"teacher perceptron\".  The  system is  trained  on p  =  aN  binary  example  inputs  and  the  generaliza(cid:173) tion  ability measured by testing for  agreement with  the teacher on all 2N  possible  binary input  patterns.  The dynamics may  be solved  analytically  and  exhibits  a  phase  transition  from  imperfect  to  perfect  generalization  at  a  =  1.  Except  at  this  point  the  generalization  ability  approaches  its  asymptotic  value  exponentially,  with critical slowing  down near  the  tran(cid:173) sition;  the  relaxation  time  is  ex  (1  - y'a)-2.  Right  at  the  critical  point,  1  the  approach  to  perfect  generalization  follows  a  power  law  ex  t - '2.  In  the presence of noise,  the generalization ability is  degraded  by  an amount  ex  (va - 1)-1 just above a  =  1.",
        "bibtex": "@inproceedings{NIPS1990_0bb4aec1,\n author = {Krogh, Anders and Hertz, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Dynamics of Generalization in Linear Perceptrons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/0bb4aec1710521c12ee76289d9440817-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1282193,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8124892268249464843&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Niels Bohr Institute; NORDITA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Niels Bohr Institute;Nordic Institute for Theoretical Physics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://nbi.ku.dk;https://www.nordita.org",
        "aff_unique_abbr": "NBI;NORDITA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Denmark;Sweden"
    },
    {
        "id": "96e634e798",
        "title": "Dynamics of Learning in Recurrent Feature-Discovery Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/8d3bba7425e7c98c50f52ca1b52d3735-Abstract.html",
        "author": "Todd K. Leen",
        "abstract": "The  self-organization  of  recurrent  feature-discovery  networks  is  studied  from the perspective of dynamical systems.  Bifurcation theory reveals pa(cid:173) rameter regimes in which multiple equilibria or limit cycles coexist with the  equilibrium at which  the networks  perform principal component analysis.",
        "bibtex": "@inproceedings{NIPS1990_8d3bba74,\n author = {Leen, Todd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Dynamics of Learning in Recurrent Feature-Discovery Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/8d3bba7425e7c98c50f52ca1b52d3735-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1458322,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15843214217986195534&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Engineering, Oregon Graduate Institute of Science & Technology",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Oregon Graduate Institute of Science & Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ogi.edu",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d061695c0d",
        "title": "EMPATH: Face, Emotion, and Gender Recognition Using Holons",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/52720e003547c70561bf5e03b95aa99f-Abstract.html",
        "author": "Garrison W. Cottrell; Janet Metcalfe",
        "abstract": "The  dimens~onali~y of a  set Off  160 1:~ :a:~s ~~\u00b7.10 .  female  subjects  IS  reduced  ........ .  network  The extracted features do not correspond to  in previ~us face  recognition systems (KaR\u00b7 na~e, 19~;)y' ......\u2022\u2022.\u2022..  f~tures we  call  holons.  The  hol.ons  are fV~~ t~!  ..... .  ..  ......\\  d'  tances  between  facial  elements.",
        "bibtex": "@inproceedings{NIPS1990_52720e00,\n author = {Cottrell, Garrison and Metcalfe, Janet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {EMPATH: Face, Emotion, and Gender Recognition Using Holons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/52720e003547c70561bf5e03b95aa99f-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/52720e003547c70561bf5e03b95aa99f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/52720e003547c70561bf5e03b95aa99f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2359257,
        "gs_citation": 417,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2591846774129739715&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5c7e1b3b71",
        "title": "Evaluation of Adaptive Mixtures of Competing Experts",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html",
        "author": "Steven J. Nowlan; Geoffrey E. Hinton",
        "abstract": "We  compare  the  performance  of the  modular  architecture,  composed  of  competing  expert  networks,  suggested  by  Jacobs,  Jordan,  Nowlan  and  Hinton  (1991)  to  the  performance  of a  single  back-propagation  network  on  a  complex,  but  low-dimensional,  vowel  recognition  task.  Simulations  reveal that this system is capable of uncovering interesting decompositions  in  a  complex  task.  The  type  of decomposition  is  strongly  influenced  by  the  nature  of the  input  to  the  gating  network  that  decides  which  expert  to  use  for  each  case.  The  modular  architecture  also exhibits consistently  better  generalization on many variations of the  task.",
        "bibtex": "@inproceedings{NIPS1990_432aca3a,\n author = {Nowlan, Steven and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Evaluation of Adaptive Mixtures of Competing Experts},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1608471,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16325542600062120151&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Dept., University of Toronto; Computer Science Dept., University of Toronto",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Computer Science Dept.",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "a357f9902d",
        "title": "Evolution and Learning in Neural Networks: The Number and Distribution of Learning Trials Affect the Rate of Evolution",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/b6f0479ae87d244975439c6124592772-Abstract.html",
        "author": "Ron Keesing; David G. Stork",
        "abstract": "and",
        "bibtex": "@inproceedings{NIPS1990_b6f0479a,\n author = {Keesing, Ron and Stork, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Evolution and Learning in Neural Networks: The Number and Distribution of Learning Trials Affect the Rate of Evolution},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b6f0479ae87d244975439c6124592772-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/b6f0479ae87d244975439c6124592772-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/b6f0479ae87d244975439c6124592772-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1585811,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2050505148514019016&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "455202e726",
        "title": "Exploiting Syllable Structure in a Connectionist Phonology Model",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/d709f38ef758b5066ef31b18039b8ce5-Abstract.html",
        "author": "David S. Touretzky; Deirdre W. Wheeler",
        "abstract": "In  a  previous  paper  (Touretzky  &  Wheeler,  1990a) we showed  how  adding  a  clustering operation to a connectionist phonology model produced a parallel pro(cid:173) cessing account of certain \"iterative\" phenomena.  In this paper we show how the  addition of a second structuring primitive, syllabification, greatly increases  the  power of the model.  We present examples from  a non-Indo-European language  that appear to require rule ordering to at least a depth of four.  By adding syllab(cid:173) ification circuitry to structure the model's perception of the input string, we are  able to handle these examples with only two derivational steps.  We conclude that  in phonology, derivation can be largely replaced by structuring.",
        "bibtex": "@inproceedings{NIPS1990_d709f38e,\n author = {Touretzky, David and Wheeler, Deirdre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Exploiting Syllable Structure in a Connectionist Phonology Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d709f38ef758b5066ef31b18039b8ce5-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/d709f38ef758b5066ef31b18039b8ce5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/d709f38ef758b5066ef31b18039b8ce5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1595637,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=437049336297543488&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9ad1d690a2",
        "title": "Exploratory Feature Extraction in Speech Signals",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/320722549d1751cf3f247855f937b982-Abstract.html",
        "author": "Nathan Intrator",
        "abstract": "A  novel  unsupervised  neural  network  for  dimensionality  reduction  which  seeks  directions  emphasizing  multimodality is  presented,  and  its  connec(cid:173) tion  to exploratory projection pursuit  methods is  discussed.  This leads to  a  new  statistical insight  to the  synaptic  modification  equations  governing  learning in  Bienenstock,  Cooper,  and  Munro  (BCM)  neurons  (1982).  The  importance  of a  dimensionality  reduction  principle  based  solely  on  distinguishing  features,  is  demonstrated  using  a  linguistically  motivated  phoneme  recognition  experiment,  and  compared  with  feature  extraction  using  back-propagation network.",
        "bibtex": "@inproceedings{NIPS1990_32072254,\n author = {Intrator, Nathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Exploratory Feature Extraction in Speech Signals},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/320722549d1751cf3f247855f937b982-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/320722549d1751cf3f247855f937b982-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/320722549d1751cf3f247855f937b982-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1584046,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13261260560085735258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Neural Science, Brown University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Center for Neural Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "24c37257e0",
        "title": "Extensions of a Theory of Networks for Approximation and Learning: Outliers and Negative Examples",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/3ad7c2ebb96fcba7cda0cf54a2e802f5-Abstract.html",
        "author": "Federico Girosi; Tomaso Poggio; Bruno Caprile",
        "abstract": "Learning an input-output mapping from a set of examples can be regarded  as synthesizing an approximation of a multi-dimensional function. From  this point of view, this form of learning is closely related to regularization  theory, and we have previously shown (Poggio and Girosi, 1990a, 1990b)  the equivalence between reglilari~at.ioll and a. class of three-layer networks  that we call regularization networks. In this note, we ext.end the theory  by introducing ways of",
        "bibtex": "@inproceedings{NIPS1990_3ad7c2eb,\n author = {Girosi, Federico and Poggio, Tomaso and Caprile, Bruno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Extensions of a Theory of Networks for Approximation and Learning: Outliers and Negative Examples},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3ad7c2ebb96fcba7cda0cf54a2e802f5-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/3ad7c2ebb96fcba7cda0cf54a2e802f5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/3ad7c2ebb96fcba7cda0cf54a2e802f5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1355179,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16650846671422846256&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "AI Lab. M.I.T., Cambridge, MA 02139; AI Lab. M.LT., Cambridge, MA 021:39; I.R.S.T., Povo, Italy, 38050",
        "aff_domain": "; ;",
        "email": "; ;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Istituto di Ricerca Scienze e Tecnologie",
        "aff_unique_dep": "AI Lab;",
        "aff_unique_url": "https://www.mit.edu;",
        "aff_unique_abbr": "MIT;IRST",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Cambridge;Povo",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Italy"
    },
    {
        "id": "708dfa7b6e",
        "title": "Feedback Synapse to Cone and Light Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/a02ffd91ece5e7efeb46db8f10a74059-Abstract.html",
        "author": "Josef Skrzypek",
        "abstract": "Light  adaptation  (LA)  allows  cone  vIslOn  to  remain  functional  between  twilight  and  the  brightest  time  of day  even  though,  at  anyone  time,  their  intensity-response (I-R)  characteristic  is  limited  to  3  log  units of the  stimu(cid:173) lating  light.  One mechanism  underlying  LA, was localized  in  the outer seg(cid:173) ment of an isolated cone (1,2). We found that by adding annular illhmination,  an  I-R  characteristic  of a  cone  can  be  shifted  along  the  intensity  domain.  Neural network involving feedback  synapse from  horizontal cells to cones is  involved  to  be  in  register  with  ambient  light  level  of  the  periphery.  An  equivalent  electrical  circuit  with  three  different  transmembrane  channels  leakage,  photocurrent  and  feedback  was  used  to  model static  behavior of a  cone.  SPICE simulation showed  that interactions between feedback  synapse  and  the  light  sensitive  conductance  in  the  outer  segment  can  shift  the  I-R  curves along  the  intensity domain, provided  that phototransduction  mechan(cid:173) ism is not saturated during maximally hyperpolarized light response.",
        "bibtex": "@inproceedings{NIPS1990_a02ffd91,\n author = {Skrzypek, Josef},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Feedback Synapse to Cone and Light Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/a02ffd91ece5e7efeb46db8f10a74059-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1683111,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2524856374962687709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dba70752ad",
        "title": "Flight Control in the Dragonfly: A Neurobiological Simulation",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/a666587afda6e89aec274a3657558a27-Abstract.html",
        "author": "William E. Faller; Marvin W. Luttges",
        "abstract": "Neural network simulations of the dragonfly flight neurocontrol system  have  been  developed  to  understand  how  this  insect  uses  complex,  unsteady  aerodynamics.  The  simulation  networks  account  for  the  ganglionic  spatial  distribution  of  cells  as  well  as  the  physiologic  operating range and the stochastic cellular fIring history of each neuron.  In  addition  the  motor  neuron  firing  patterns,  \"flight  command  sequences\", were utilized. Simulation training was targeted against both  the  cellular  and  flight  motor  neuron  firing  patterns.  The  trained  networks  accurately  resynthesized  the  intraganglionic  cellular firing  patterns. These in  tum controlled the  motor neuron fIring patterns that  drive  wing  musculature  during  flight.  Such  networks  provide  both  neurobiological analysis tools and fIrst  generation controls for  the  use  of \"unsteady\" aerodynamics.",
        "bibtex": "@inproceedings{NIPS1990_a666587a,\n author = {Faller, William and Luttges, Marvin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Flight Control in the Dragonfly: A Neurobiological Simulation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a666587afda6e89aec274a3657558a27-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/a666587afda6e89aec274a3657558a27-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/a666587afda6e89aec274a3657558a27-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1645950,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6943485397079124625&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Aerospace Engineering Sciences, University of Colorado, Boulder, Colorado 80309-0429; Aerospace Engineering Sciences, University of Colorado, Boulder, Colorado 80309-0429",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Colorado Boulder",
        "aff_unique_dep": "Aerospace Engineering Sciences",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU Boulder",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "686fc1776f",
        "title": "From Speech Recognition to Spoken Language Understanding: The Development of the MIT SUMMIT and VOYAGER Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/bbf94b34eb32268ada57a3be5062fe7d-Abstract.html",
        "author": "Victor Zue; James Glass; David Goodine; Lynette Hirschman; Hong Leung; Michael Phillips; Joseph Polifroni; Stephanie Seneff",
        "abstract": "Spoken language is one of the most natural, efficient, flexible, and econom(cid:173) ical means of communication among humans. As computers play an ever  increasing role in our lives, it is important that we address the issue of  providing a graceful human-machine interface through spoken language.  In this paper, we will describe our recent efforts in moving beyond the  scope of speech recognition into the realm of spoken-language understand(cid:173) ing. Specifically, we report on the development of an urban navigation and  exploration system called VOYAGER, an application which we have used as  a basis for performing research in spoken-language understanding.",
        "bibtex": "@inproceedings{NIPS1990_bbf94b34,\n author = {Zue, Victor and Glass, James and Goodine, David and Hirschman, Lynette and Leung, Hong and Phillips, Michael and Polifroni, Joseph and Seneff, Stephanie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {From Speech Recognition to Spoken Language Understanding: The Development of the MIT SUMMIT and VOYAGER Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/bbf94b34eb32268ada57a3be5062fe7d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1795303,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2258860711191849572&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8d9c476b03",
        "title": "Further Studies of a Model for the Development and Regeneration of Eye-Brain Maps",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/fccb60fb512d13df5083790d64c4d5dd-Abstract.html",
        "author": "Jack D. Cowan; A. E. Friedman",
        "abstract": "We describe a computational model of the development and regenera(cid:173) tion of specific eye-brain circuits. The model comprises a self-organiz(cid:173) ing map-forming network which uses local Hebb rules, constrained by  (genetically determined) molecular markers. Various simulations of  the development and regeneration of eye-brain maps in fish and frogs  are described, in particular successful simulations of experiments by  Schmidt-Cicerone-Easter; Meyer; and Y oon.",
        "bibtex": "@inproceedings{NIPS1990_fccb60fb,\n author = {Cowan, Jack and Friedman, A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Further Studies of a Model for the Development and Regeneration of Eye-Brain Maps},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fccb60fb512d13df5083790d64c4d5dd-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/fccb60fb512d13df5083790d64c4d5dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/fccb60fb512d13df5083790d64c4d5dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1540227,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8730015907684498703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Mathematics, Committee on Neurobiology, and Brain Research Institute, The University of Chicago; Department of Mathematics, Committee on Neurobiology, and Brain Research Institute, The University of Chicago",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Chicago",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.uchicago.edu",
        "aff_unique_abbr": "UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bfe1994016",
        "title": "Generalization Dynamics in LMS Trained Linear Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html",
        "author": "Yves Chauvin",
        "abstract": "For a simple linear case, a mathematical analysis of the training and gener(cid:173) alization (validation)  performance of networks trained by gradient descent  on a Least Mean Square cost function is provided as a function of the learn(cid:173) ing parameters and of the statistics of the training data base.  The analysis  predicts  that generalization error dynamics  are very dependent  on  a  pri(cid:173) ori initial weights.  In particular, the generalization error might sometimes  weave within a computable range during extended training.  In some cases,  the analysis provides bounds on the optimal number of training cycles  for  minimal  validation error.  For a  speech  labeling  task,  predicted  weaving  effects  were qualitatively tested  and  observed  by computer simulations in  networks trained by the linear and non-linear back-propagation algorithm.",
        "bibtex": "@inproceedings{NIPS1990_01386bd6,\n author = {Chauvin, Yves},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Generalization Dynamics in LMS Trained Linear Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/01386bd6d8e091c2ab4c7c7de644d37b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1466511,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10622225854407268677&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7a98ddd2dd",
        "title": "Generalization Properties of Radial Basis Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/6883966fd8f918a4aa29be29d2c386fb-Abstract.html",
        "author": "Sherif M. Botros; Christopher G. Atkeson",
        "abstract": "We examine the ability of radial basis functions  (RBFs) to generalize.  We  compare the performance of several types of RBFs.  We use the inverse dy(cid:173) namics of an idealized  two-joint  arm as  a  test case.  We find  that without  a  proper  choice of a  norm for  the inputs,  RBFs have  poor  generalization  properties.  A simple global scaling of the input variables greatly improves  performance.  We suggest some efficient methods to approximate this dis(cid:173) tance  metric.",
        "bibtex": "@inproceedings{NIPS1990_6883966f,\n author = {Botros, Sherif and Atkeson, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Generalization Properties of Radial Basis Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6883966fd8f918a4aa29be29d2c386fb-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/6883966fd8f918a4aa29be29d2c386fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/6883966fd8f918a4aa29be29d2c386fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1538510,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5342470297271399329&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Brain and Cognitive Sciences Department and the Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Brain and Cognitive Sciences Department and the Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Brain and Cognitive Sciences Department, Artificial Intelligence Laboratory",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2c6e525011",
        "title": "Generalization by Weight-Elimination with Application to Forecasting",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html",
        "author": "Andreas S. Weigend; David E. Rumelhart; Bernardo A. Huberman",
        "abstract": "Inspired by the information theoretic idea of minimum description length, we add  a term  to the back propagation cost function that penalizes network complexity.  We  give  the  details  of the  procedure,  called  weight-elimination,  describe  its  dynamics, and clarify the meaning of the parameters involved. From a Bayesian  perspective,  the complexity term  can  be usefully interpreted as  an  assumption  about prior distribution of the weights.  We  use  this  procedure  to  predict  the  sunspot time series and the notoriously noisy series of currency exchange rates.",
        "bibtex": "@inproceedings{NIPS1990_bc6dc48b,\n author = {Weigend, Andreas and Rumelhart, David and Huberman, Bernardo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Generalization by Weight-Elimination with Application to Forecasting},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bc6dc48b743dc5d013b1abaebd2faed2-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/bc6dc48b743dc5d013b1abaebd2faed2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/bc6dc48b743dc5d013b1abaebd2faed2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2015140,
        "gs_citation": 1052,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14435058591602583314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "259cd7ce20",
        "title": "Grouping Contours by Iterated Pairing Network",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/1651cf0d2f737d7adeab84d339dbabd3-Abstract.html",
        "author": "Amnon Shashua; Shimon Ullman",
        "abstract": "Shimon Ullman",
        "bibtex": "@inproceedings{NIPS1990_1651cf0d,\n author = {Shashua, Amnon and Ullman, Shimon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Grouping Contours by Iterated Pairing Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1651cf0d2f737d7adeab84d339dbabd3-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/1651cf0d2f737d7adeab84d339dbabd3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/1651cf0d2f737d7adeab84d339dbabd3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1680540,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1162839460321768580&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "M.I.T. Artificial Intelligence Lab., NE43-737 and Department of Brain and Cognitive Science; M.I.T. Artificial Intelligence Lab., NE43-737 and Department of Brain and Cognitive Science",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Artificial Intelligence Lab.",
        "aff_unique_url": "https://www.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1297674e55",
        "title": "How Receptive Field Parameters Affect Neural Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/d296c101daa88a51f6ca8cfc1ac79b50-Abstract.html",
        "author": "Bartlett W. Mel; Stephen M. Omohundro",
        "abstract": "We  identify  the three  principle factors  affecting  the  performance of learn(cid:173) ing by  networks  with  localized  units:  unit noise,  sample density,  and  the  structure of the target function.  We then  analyze the effect  of unit recep(cid:173) tive  field  parameters  on  these  factors  and  use  this  analysis  to  propose  a  new  learning algorithm which  dynamically alters receptive field  properties  during learning.",
        "bibtex": "@inproceedings{NIPS1990_d296c101,\n author = {Mel, Bartlett and Omohundro, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {How Receptive Field Parameters Affect Neural Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/d296c101daa88a51f6ca8cfc1ac79b50-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1519412,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1601578478721055950&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "CNS Program, Caltech, 216-76, Pasadena, CA 91125; ICSI, 1947 Center St., Suite 600, Berkeley, CA 94704",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "California Institute of Technology;International Computer Science Institute",
        "aff_unique_dep": "CNS Program;",
        "aff_unique_url": "https://www.caltech.edu;https://www.icsi.berkeley.edu/",
        "aff_unique_abbr": "Caltech;ICSI",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pasadena;Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dbaeb0409c",
        "title": "Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/d9fc5b73a8d78fad3d6dffe419384e70-Abstract.html",
        "author": "Richard S. Sutton",
        "abstract": "This is a summary of results with Dyna, a class of architectures for intel(cid:173) ligent systems based on approximating dynamic programming methods.  Dyna architectures integrate trial-and-error (reinforcement) learning and  execution-time planning into a single process operating alternately on the  world and on a learned forward model of the world. We describe and  show results for two Dyna architectures, Dyna-AHC and Dyna-Q. Using a  navigation task, results are shown for a simple Dyna-AHC system which  simultaneously learns by trial and error, learns a world model, and plans  optimal routes using the evolving world model. We show that Dyna-Q  architectures (based on Watkins's Q-Iearning) are easy to adapt for use in  changing environments.",
        "bibtex": "@inproceedings{NIPS1990_d9fc5b73,\n author = {Sutton, Richard S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1942578,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12856116594030745523&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "GTE Laboratories Incorporated",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "GTE Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gte.com",
        "aff_unique_abbr": "GTE Labs",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "454bff6b47",
        "title": "Integrated Segmentation and Recognition of Hand-Printed Numerals",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html",
        "author": "James D. Keeler; David E. Rumelhart; Wee Kheng Leow",
        "abstract": "Neural  network  algorithms  have  proven  useful  for  recognition  of individ(cid:173) ual,  segmented  characters.  However,  their recognition  accuracy  has  been  limited by  the  accuracy  of the  underlying  segmentation  algorithm.  Con(cid:173) ventional,  rule-based  segmentation  algorithms  encounter  difficulty  if the  characters  are touching, broken,  or noisy.  The problem in these situations  is  that  often  one  cannot  properly  segment  a  character  until  it  is  recog(cid:173) nized yet  one cannot  properly recognize  a  character until it is  segmented.  We present here  a  neural network algorithm that simultaneously segments  and recognizes  in an  integrated  system.  This  algorithm has several  novel  features:  it uses  a supervised learning algorithm (backpropagation), but is  able to take position-independent information as  targets and self-organize  the  activities  of the units  in  a  competitive fashion  to infer the  positional  information.  We  demonstrate  this  ability with  overlapping  hand-printed  numerals.",
        "bibtex": "@inproceedings{NIPS1990_e46de7e1,\n author = {Keeler, James and Rumelhart, David and Leow, Wee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Integrated Segmentation and Recognition of Hand-Printed Numerals},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1685400,
        "gs_citation": 239,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7255972558246081460&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "MCC; Psychology Department, Stanford University; MCC+University of Texas",
        "aff_domain": "mcc.com;mcc.com; ",
        "email": "mcc.com;mcc.com; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "MCC;Stanford University;University of Texas",
        "aff_unique_dep": ";Psychology Department;",
        "aff_unique_url": ";https://www.stanford.edu;https://www.utexas.edu",
        "aff_unique_abbr": ";Stanford;UT",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "3c4bf36ea6",
        "title": "Interaction Among Ocularity, Retinotopy and On-center/Off-center Pathways During Development",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html",
        "author": "Shigeru Tanaka",
        "abstract": "The development of projections from the retinas to the cortex is  mathematically analyzed according to the previously proposed  thermodynamic formulation of the self-organization of neural networks.  Three types of submodality included in the visual afferent pathways are  assumed in two models: model (A), in which the ocularity and retinotopy  are considered separately, and model (B), in which on-center/off-center  pathways are considered in addition to ocularity and retinotopy. Model (A)  shows striped ocular dominance spatial patterns and, in ocular dominance  histograms, reveals a dip in the binocular bin. Model (B) displays  spatially modulated irregular patterns and shows single-peak behavior in  the histograms. When we compare the simulated results with the observed  results, it is evident that the ocular dominance spatial patterns and  histograms for models (A) and (B) agree very closely with those seen in  monkeys and cats.",
        "bibtex": "@inproceedings{NIPS1990_5737c6ec,\n author = {Tanaka, Shigeru},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Interaction Among Ocularity, Retinotopy and On-center/Off-center Pathways During Development},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1751108,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14812274755100780050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ab064c8821",
        "title": "Kohonen Networks and Clustering: Comparative Performance in Color Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/371bce7dc83817b7893bcdeed13799b5-Abstract.html",
        "author": "Wesley Snyder; Daniel Nissman; David Van den Bout; Griff Bilbro",
        "abstract": "The problem of color clustering is defined and shown to be a problem of  assigning a large number (hundreds of thousands) of 3-vectors to a  small number (256) of clusters. Finding those clusters in such a way that  they best represent a full color image using only 256 distinct colors is a  burdensome computational problem. In this paper, the problem is solved  using \"classical\" techniques -- k-means clustering, vector quantization  (which turns out to be the same thing in this application), competitive  learning, and Kohonen self-organizing feature maps. Quality of the  result is judged subjectively by how much the pseudo-color result  resembles the true color image, by RMS quantization error, and by run  time. The Kohonen map provides the best solution.",
        "bibtex": "@inproceedings{NIPS1990_371bce7d,\n author = {Snyder, Wesley and Nissman, Daniel and Van den Bout, David and Bilbro, Griff},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Kohonen Networks and Clustering: Comparative Performance in Color Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/371bce7dc83817b7893bcdeed13799b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1581773,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7507288262377197466&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "28dbbbece2",
        "title": "Language Induction by Phase Transition in Dynamical Recognizers",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/26e359e83860db1d11b6acca57d8ea88-Abstract.html",
        "author": "Jordan B. Pollack",
        "abstract": "A higher order recurrent neural network architecture learns to recognize and  generate languages after being  \"trained\"  on categorized exemplars.  Studying  these  networks  from  the  perspective  of  dynamical  systems  yields  two  interesting  discoveries:  First,  a  longitudinal  examination  of  the  learning  process  illustrates  a  new  form  of mechanical  inference:  Induction  by  phase  transition.  A  small  weight  adjustment  causes  a  \"bifurcation\"  in  the  limit  behavior of the network. This phase transition corresponds to the onset of the  network's  capacity  for  generalizing  to  arbitrary-length  strings.  Second,  a  study of the  automata resulting  from  the  acquisition  of previously published  languages  indicates  that  while  the architecture  is  NOT  guaranteed  to  find  a  minimal  finite  automata  consistent  with  the  given  exemplars,  which  is  an  NP-Hard  problem,  the  architecture  does  appear capable  of generating  non(cid:173) regular languages by exploiting fractal and chaotic dynamics. I end the paper  with  a  hypothesis  relating  linguistic  generative  capacity  to  the  behavioral  regimes of non-linear dynamical systems.",
        "bibtex": "@inproceedings{NIPS1990_26e359e8,\n author = {Pollack, Jordan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Language Induction by Phase Transition in Dynamical Recognizers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/26e359e83860db1d11b6acca57d8ea88-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1727478,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11522274203566639191&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3f80158317",
        "title": "Leaning by Combining Memorization and Gradient Descent",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/89f0fd5c927d466d6ec9a21b9ac34ffa-Abstract.html",
        "author": "John C. Platt",
        "abstract": "We have created a radial basis function network that allocates a  new computational unit whenever an unusual pattern is presented  to the network. The network learns by allocating new units and  adjusting the parameters of existing units. If the network performs  poorly on a presented pattern, then a new unit is allocated which  memorizes the response to the presented pattern. If the network  performs well on a presented pattern, then the network parameters  are updated using standard LMS gradient descent. For predicting  the Mackey Glass chaotic time series, our network learns much  faster than do those using back-propagation and uses a comparable  number of synapses.",
        "bibtex": "@inproceedings{NIPS1990_89f0fd5c,\n author = {Platt, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Leaning by Combining Memorization and Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1412288,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10910491851625564821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Synaptics, Inc.",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Synaptics, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.synaptics.com",
        "aff_unique_abbr": "Synaptics",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e442c69b57",
        "title": "Learning Theory and Experiments with Competitive Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/fb7b9ffa5462084c5f4e7e85a093e6d7-Abstract.html",
        "author": "Griff L. Bilbro; David E. van den Bout",
        "abstract": "We  apply  the  theory  of Tishby,  Levin,  and Sol1a  (TLS)  to two problems.  First  we analyze  an elementary problem for  which we find  the predictions  consistent  with  conventional  statistical  results.  Second  we  numerically  examine  the  more realistic  problem of training a  competitive net  to learn  a  probability  density  from  samples.  We  find  TLS  useful  for  predicting  average  training  behavior.",
        "bibtex": "@inproceedings{NIPS1990_fb7b9ffa,\n author = {Bilbro, Griff and van den Bout, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Learning Theory and Experiments with Competitive Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1362767,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7131244378606932245&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "North Carolina State University; North Carolina State University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "North Carolina State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ncsu.edu",
        "aff_unique_abbr": "NCSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dea89bb62b",
        "title": "Learning Time-varying Concepts",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/69cb3ea317a32c4e6143e665fdb20b14-Abstract.html",
        "author": "Anthony Kuh; Thomas Petsche; Ronald L. Rivest",
        "abstract": "This work extends computational learning theory to situations in which concepts  vary over time, e.g., system identification of a time-varying plant. We have  extended formal definitions of concepts and learning to provide a framework  in which an algorithm can track a concept as it evolves over time. Given  this framework and focusing on memory-based algorithms, we have derived  some PAC-style sample complexity results that determine, for example, when  tracking is feasible. We have also used a similar framework and focused on  incremental tracking algorithms for which we have derived some bounds on  the mistake or error rates for some specific concept classes.",
        "bibtex": "@inproceedings{NIPS1990_69cb3ea3,\n author = {Kuh, Anthony and Petsche, Thomas and Rivest, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Learning Time-varying Concepts},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/69cb3ea317a32c4e6143e665fdb20b14-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/69cb3ea317a32c4e6143e665fdb20b14-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/69cb3ea317a32c4e6143e665fdb20b14-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1765464,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12651290426965394423&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. of Electrical Eng., U. of Hawaii at Manoa; Siemens Corp. Research; Lab. for Computer Sci., MIT",
        "aff_domain": "wiliki.eng.hawaii.edu;learning.siemens.com;theory.lcs.mit.edu",
        "email": "wiliki.eng.hawaii.edu;learning.siemens.com;theory.lcs.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Hawaii at Manoa;Siemens Corporation;Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering;Research;Computer Science",
        "aff_unique_url": "https://www.hawaii.edu;https://www.siemens.com;https://www.mit.edu",
        "aff_unique_abbr": "UH Manoa;Siemens;MIT",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Manoa;;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "23e1ffb54a",
        "title": "Learning Trajectory and Force Control of an Artificial Muscle Arm by Parallel-hierarchical Neural Network Model",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/3fe94a002317b5f9259f82690aeea4cd-Abstract.html",
        "author": "Masazumi Katayama; Mitsuo Kawato",
        "abstract": "We propose a new parallel-hierarchical neural network model to enable motor  learning for simultaneous control of both trajectory and force. by integrating  Hogan's control method and our previous neural network control model using a  feedback-error-learning scheme. Furthermore. two hierarchical control laws  which apply to the model, are derived by using the Moore-Penrose pseudo(cid:173) inverse matrix. One is related to the minimum muscle-tension-change trajectory  and the other is related to the minimum motor-command-change trajectory. The  human arm is redundant at the dynamics level since joint torque is generated by  agonist and antagonist muscles. Therefore, acquisition of the inverse model is  an ill-posed problem. However. the combination of these control laws and  feedback-error-learning resolve the ill-posed problem. Finally. the efficiency of  the parallel-hierarchical neural network model is shown by learning experiments  using an artificial muscle arm and computer simulations.",
        "bibtex": "@inproceedings{NIPS1990_3fe94a00,\n author = {Katayama, Masazumi and Kawato, Mitsuo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Learning Trajectory and Force Control of an Artificial Muscle Arm by Parallel-hierarchical Neural Network Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/3fe94a002317b5f9259f82690aeea4cd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1587908,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17435301059608052425&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Cognitive Processes Department, ATR Auditory and Visual Perception Research Laboratories; Cognitive Processes Department, ATR Auditory and Visual Perception Research Laboratories",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ATR Auditory and Visual Perception Research Laboratories",
        "aff_unique_dep": "Cognitive Processes Department",
        "aff_unique_url": "https://www.atr.jp",
        "aff_unique_abbr": "ATR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "7283e6280f",
        "title": "Learning to See Rotation and Dilation with a Hebb Rule",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/5ef698cd9fe650923ea331c15af3b160-Abstract.html",
        "author": "Martin I. Sereno; Margaret E. Sereno",
        "abstract": "Previous work (M.I. Sereno, 1989; cf. M.E. Sereno, 1987) showed that a  feedforward network with area VI-like input-layer units and a Hebb rule  can develop area MT-like second layer units that solve the aperture  problem for pattern motion. The present study extends this earlier work  to more complex motions. Saito et al. (1986) showed that neurons with  large receptive fields in macaque visual area MST are sensitive to  different senses of rotation and dilation, irrespective of the receptive field  location of the movement singularity. A network with an MT-like  second layer was trained and tested on combinations of rotating, dilating,  and translating patterns. Third-layer units learn to detect specific senses  of rotation or dilation in a position-independent fashion, despite having  position-dependent direction selectivity within their receptive fields.",
        "bibtex": "@inproceedings{NIPS1990_5ef698cd,\n author = {Sereno, Martin and Sereno, Margaret},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Learning to See Rotation and Dilation with a Hebb Rule},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/5ef698cd9fe650923ea331c15af3b160-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1627056,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17664283583850858996&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Cognitive Science D-015, University of California, San Diego; Cognitive Science D-015, University of California, San Diego",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Cognitive Science",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "137ed29b44",
        "title": "Lg Depth Estimation and Ripple Fire Characterization Using Artificial Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/c86a7ee3d8ef0b551ed58e354a836f2b-Abstract.html",
        "author": "John L. Perry; Douglas R. Baumgardt",
        "abstract": "This srudy has demonstrated how artificial neural networks (ANNs) can  be used to characterize seismic sources using high-frequency regional  seismic data. We have taken the novel approach of using ANNs as a  research tool for obtaining seismic source information, specifically  depth of focus for earthquakes and ripple-fire characteristics for  economic blasts, rather than as just a feature classifier between  earthquake and explosion populations. Overall, we have found that  ANNs have potential applications to seismic event characterization and  identification, beyond just as a feature classifier. In future studies, these  techniques should be applied to actual data of regional seismic events  recorded at the new regional seismic arrays. The results of this study  indicates that an ANN should be evaluated as part of an operational  seismic event identification system.",
        "bibtex": "@inproceedings{NIPS1990_c86a7ee3,\n author = {Perry, John and Baumgardt, Douglas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Lg Depth Estimation and Ripple Fire Characterization Using Artificial Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/c86a7ee3d8ef0b551ed58e354a836f2b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1751678,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17567525552398684226&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "ENSCO, Inc. Signal Analysis and Systems Division; ENSCO, Inc. Signal Analysis and Systems Division",
        "aff_domain": "dewey.css.gov; ",
        "email": "dewey.css.gov; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ENSCO, Inc.",
        "aff_unique_dep": "Signal Analysis and Systems Division",
        "aff_unique_url": "https://www.ensco.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7bf77dacc3",
        "title": "Modeling Time Varying Systems Using Hidden Control Neural Architecture",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html",
        "author": "Esther Levin",
        "abstract": "Multi-layered  neural  networks  have  recently  been  proposed  for  non(cid:173) linear  prediction  and  system  modeling.  Although  proven  successful  for  modeling  time  invariant nonlinear systems,  the  inability  of neural  networks  to  characterize  temporal  variability  has  so  far  been  an  obstacle  in  applying  them  to  complicated  non stationary  signals,  such  as  speech.  In  this  paper  we  present  a  network  architecture,  called  \"Hidden  Control  Neural  Network\"  (HCNN),  for  modeling  signals  generated  by  nonlinear  dynamical  systems  with  restricted  time  variability.  The approach  taken  here  is  to  allow  the  mapping  that  is  implemented  by  a  multi  layered  neural  network  to  change  with  time  as  a  function  of an  additional  control  input  signal.  This  network  is  trained  using  an  algorithm  that  is  based  on  \"back-propagation\"  and  segmentation  algorithms  for  estimating  the  unknown  control  together  with  the  network's  parameters.  The HCNN  approach  was  applied  to  several  tasks  including  modeling  of  time-varying  nonlinear  systems  and  speaker-independent  recognition  of  connected  digits,  yielding  a  word accuracy of 99.1 %.",
        "bibtex": "@inproceedings{NIPS1990_00411460,\n author = {Levin, Esther},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Modeling Time Varying Systems Using Hidden Control Neural Architecture},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/00411460f7c92d2124a67ea0f4cb5f85-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1966430,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10419695760799578664&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "AT&T Bell Laboratories Speech Research Department Murray Hill, NJ 07974 USA",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "AT&T Bell Laboratories",
        "aff_unique_dep": "Speech Research Department",
        "aff_unique_url": "https://www.att.com/labs/research",
        "aff_unique_abbr": "AT&T Labs",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Murray Hill",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b621758a3e",
        "title": "Multi-Layer Perceptrons with B-Spline Receptive Field Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/94f6d7e04a4d452035300f18b984988c-Abstract.html",
        "author": "Stephen H. Lane; Marshall Flax; David Handelman; Jack Gelfand",
        "abstract": "Multi-layer perceptrons are often slow to learn nonlinear functions  with complex local structure due to the global nature of their function  approximations. It is shown that standard multi-layer perceptrons are  actually a special case of a more general network formulation that  incorporates B-splines into the node computations. This allows novel  spline network architectures to be developed that can combine the  generalization capabilities and scaling properties of global multi-layer  feedforward networks with the computational efficiency and learning  speed of local computational paradigms. Simulation results are  presented for the well known spiral problem of Weiland and of Lang  and Witbrock to show the effectiveness of the Spline Net approach.",
        "bibtex": "@inproceedings{NIPS1990_94f6d7e0,\n author = {Lane, Stephen and Flax, Marshall and Handelman, David and Gelfand, Jack},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Multi-Layer Perceptrons with B-Spline Receptive Field Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/94f6d7e04a4d452035300f18b984988c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1766295,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8724021062894281366&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7c0fb248a3",
        "title": "Natural Dolphin Echo Recognition Using an Integrator Gateway Network",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html",
        "author": "Herbert L. Roitblat; Patrick W. B. Moore; Paul E. Nachtigall; Ralph H. Penner",
        "abstract": "We have been studying the performance of a bottlenosed dolphin on  a delayed matching-to-sample task to gain insight into the processes and  mechanisms that the animal uses during echolocation. The dolphin  recognizes targets by emitting natural sonar signals and listening to the  echoes that return. This paper describes a novel neural network  architecture, called an integrator gateway network, that we have de(cid:173) veloped to account for this performance. The integrator gateway  network combines information from multiple echoes to classify targets  with about 90% accuracy. In contrast, a standard backpropagation  network performed with only about 63% accuracy.",
        "bibtex": "@inproceedings{NIPS1990_6da37dd3,\n author = {Roitblat, Herbert and Moore, Patrick and Nachtigall, Paul and Penner, Ralph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Natural Dolphin Echo Recognition Using an Integrator Gateway Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1870940,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6276897319397786811&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b438184c0c",
        "title": "Navigating through Temporal Difference",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/8d7d8ee069cb0cbbf816bbb65d56947e-Abstract.html",
        "author": "Peter Dayan",
        "abstract": "Barto, Sutton and Watkins [2] introduced a grid task as a didactic ex(cid:173) ample of temporal difference planning and asynchronous dynamical pre>(cid:173) gramming. This paper considers the effects of changing the coding of the  input stimulus, and demonstrates that the self-supervised learning of a  particular form of hidden unit representation improves performance.",
        "bibtex": "@inproceedings{NIPS1990_8d7d8ee0,\n author = {Dayan, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Navigating through Temporal Difference},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8d7d8ee069cb0cbbf816bbb65d56947e-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/8d7d8ee069cb0cbbf816bbb65d56947e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/8d7d8ee069cb0cbbf816bbb65d56947e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1589910,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2519440492601664007&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Centre for Cognitive Science & Department of Physics, University of Edinburgh",
        "aff_domain": "dayantcns.ed.ac.uk",
        "email": "dayantcns.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "Centre for Cognitive Science & Department of Physics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "13a84c6f19",
        "title": "Neural Dynamics of Motion Segmentation and Grouping",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/310dcbbf4cce62f762a2aaa148d556bd-Abstract.html",
        "author": "Ennio Mingolla",
        "abstract": "A  neural  network  model  of motion segmentation  by  visual  cortex  is  de(cid:173) scribed.  The  model  clarifies  how  preprocessing  of  motion  signals  by  a  Motion Oriented Contrast Filter (MOC  Filter)  is joined to long-range co(cid:173) operative motion mechanisms in a  motion Cooperative Competitive Loop  (CC Loop)  to control phenomena such as as induced  motion, motion cap(cid:173) ture,  and motion aftereffects.  The total model system is  a  motion Bound(cid:173) ary Contour System (BCS)  that is computed in parallel with a static BCS  before  both systems cooperate  to generate  a  boundary  representation for  three dimensional visual form perception.  The present investigations clari(cid:173) fy how the static BCS can be modified for use in motion segmentation prob(cid:173) lems, notably for analyzing how ambiguous local movements (the aperture  problem) on a  complex moving shape  are suppressed  and actively reorga(cid:173) nized  into a  coherent global motion signal.",
        "bibtex": "@inproceedings{NIPS1990_310dcbbf,\n author = {Mingolla, Ennio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Dynamics of Motion Segmentation and Grouping},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/310dcbbf4cce62f762a2aaa148d556bd-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/310dcbbf4cce62f762a2aaa148d556bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/310dcbbf4cce62f762a2aaa148d556bd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1547050,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:d1Ma-Hl92YIJ:scholar.google.com/&scioq=Neural+Dynamics+of+Motion+Segmentation+and+Grouping&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Center for Adaptive Systems, and Cognitive and Neural Systems Program, Boston University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Center for Adaptive Systems, and Cognitive and Neural Systems Program",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cb6e670520",
        "title": "Neural Network Application to Diagnostics and Control of Vehicle Control Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/b3967a0e938dc2a6340e258630febd5a-Abstract.html",
        "author": "Kenneth A. Marko",
        "abstract": "Diagnosis of faults in complex, real-time control systems is a  complicated task that has resisted solution by traditional methods. We  have shown that neural networks can be successfully employed to  diagnose faults in digitally controlled powertrain systems. This paper  discusses the means we use to develop the appropriate databases for  training and testing in order to select the optimum network architectures  and to provide reasonable estimates of the classification accuracy of  these networks on new samples of data. Recent work applying neural  nets to adaptive control of an active suspension system is presented.",
        "bibtex": "@inproceedings{NIPS1990_b3967a0e,\n author = {Marko, Kenneth},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Network Application to Diagnostics and Control of Vehicle Control Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/b3967a0e938dc2a6340e258630febd5a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1879428,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14302642295572104456&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Ford Motor Company",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ford Motor Company",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ford.com",
        "aff_unique_abbr": "Ford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1c87bc5804",
        "title": "Neural Network Implementation of Admission Control",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/b83aac23b9528732c23cc7352950e880-Abstract.html",
        "author": "Rodolfo A. Milito; Isabelle Guyon; Sara A. Solla",
        "abstract": "A feedforward layered network implements a mapping required to control an  unknown stochastic nonlinear dynamical system. Training is based on a  novel approach that combines stochastic approximation ideas with back(cid:173) propagation. The method is applied to control admission into a queueing sys(cid:173) tem operating in a time-varying environment.",
        "bibtex": "@inproceedings{NIPS1990_b83aac23,\n author = {Milito, Rodolfo and Guyon, Isabelle and Solla, Sara},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Network Implementation of Admission Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b83aac23b9528732c23cc7352950e880-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/b83aac23b9528732c23cc7352950e880-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/b83aac23b9528732c23cc7352950e880-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1496575,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17639634897346617236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c33b2608ad",
        "title": "Neural Networks Structured for Control Application to Aircraft Landing",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/0584ce565c824b7b7f50282d9a19945b-Abstract.html",
        "author": "Charles Schley; Yves Chauvin; Van Henkle; Richard Golden",
        "abstract": "We present a generic neural network architecture capable of con(cid:173) trolling non-linear plants. The network is composed of dynamic.  parallel, linear maps gated by non-linear switches. Using a recur(cid:173) rent form of the back-propagation algorithm, control is achieved  by optimizing the control gains and task-adapted switch parame(cid:173) ters. A mean quadratic cost function computed across a nominal  plant trajectory is minimized along with performance constraint  penalties. The approach is demonstrated for a control task con(cid:173) sisting of landing a commercial aircraft in difficult wind conditions.  We show that the network yields excellent performance while re(cid:173) maining within acceptable damping response constraints.",
        "bibtex": "@inproceedings{NIPS1990_0584ce56,\n author = {Schley, Charles and Chauvin, Yves and Henkle, Van and Golden, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Networks Structured for Control Application to Aircraft Landing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0584ce565c824b7b7f50282d9a19945b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/0584ce565c824b7b7f50282d9a19945b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/0584ce565c824b7b7f50282d9a19945b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1790274,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5228013240102285742&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e710ef92be",
        "title": "Note on Learning Rate Schedules for Stochastic Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/18d8042386b79e2c279fd162df0205c8-Abstract.html",
        "author": "Christian Darken; John E. Moody",
        "abstract": "We  present  and  compare  learning  rate  schedules  for  stochastic  gradient  descent,  a  general  algorithm  which  includes  LMS,  on-line  backpropaga(cid:173) tion  and  k-means  clustering  as  special  cases.  We  introduce  \"search-then(cid:173) converge\"  type  schedules  which  outperform  the  classical  constant  and  \"running average\"  (1ft) schedules both in speed of convergence and quality  of solution.",
        "bibtex": "@inproceedings{NIPS1990_18d80423,\n author = {Darken, Christian and Moody, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Note on Learning Rate Schedules for Stochastic Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/18d8042386b79e2c279fd162df0205c8-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/18d8042386b79e2c279fd162df0205c8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/18d8042386b79e2c279fd162df0205c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1344435,
        "gs_citation": 256,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7013789687439561442&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Yale University; Yale University",
        "aff_domain": ";cs.yale.edu",
        "email": ";cs.yale.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "03729a4331",
        "title": "On Stochastic Complexity and Admissible Models for Neural Network Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/ddb30680a691d157187ee1cf9e896d03-Abstract.html",
        "author": "Padhraic Smyth",
        "abstract": "Given some  training data how  should we  choose a particular network clas(cid:173) sifier  from  a  family  of networks  of different  complexities?  In  this  paper  we  discuss how  the application of stochastic complexity theory to classifier  design problems can provide some insights into this problem.  In particular  we  introduce  the  notion  of admissible  models  whereby  the  complexity  of  models  under consideration is  affected  by  (among other factors)  the class  entropy,  the  amount  of training  data,  and  our  prior  belief.  In  particular  we  discuss the implications of these results with respect to neural architec(cid:173) tures and demonstrate the approach on real data from  a medical diagnosis  task.",
        "bibtex": "@inproceedings{NIPS1990_ddb30680,\n author = {Smyth, Padhraic},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {On Stochastic Complexity and Admissible Models for Neural Network Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/ddb30680a691d157187ee1cf9e896d03-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1641876,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8783322455280941804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Communications Systems Research, Jet Propulsion Laboratory, California Institute of Technology",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Communications Systems Research",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "31d8b76911",
        "title": "On the Circuit Complexity of Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html",
        "author": "V. P. Roychowdhury; K. Y. Siu; A. Orlitsky; T. Kailath",
        "abstract": "'~le introduce a geometric approach for investigating the power of threshold  circuits. Viewing n-variable boolean functions as vectors in 'R'2\", we invoke  tools from linear algebra and linear programming to derive new results on  the realizability of boolean functions using threshold gat.es.  Using this approach, one can obtain: (1) upper-bounds on the number of  spurious memories in HopfielJ networks, and on the number of functions  implementable by a depth-d threshold circuit; (2) a lower bound on the  number of ort.hogonal input. functions required to implement. a threshold  function; (3) a necessary condit.ion for an arbit.rary set of input. functions to  implement a threshold function; (4) a lower bound on the error introduced  in approximating boolean functions using sparse polynomials; (5) a limit  on the effectiveness of the only known lower-bound technique (based on  computing correlations among boolean functions) for the depth of thresh(cid:173) old circuit.s implement.ing boolean functions, and (6) a constructive proof  that every boolean function f of n input variables is a threshold function  of polynomially many input functions, none of which is significantly cor(cid:173) related with f. Some of these results lead t.o genera.lizations of key results  concerning threshold circuit complexity, particularly t.hose that are based  on the so-called spectral or Ha.rmonic analysis approach. Moreover, our  geometric approach yields simple proofs, based on elementary results from  linear algebra, for many of these earlier results.",
        "bibtex": "@inproceedings{NIPS1990_8dd48d6a,\n author = {Roychowdhury, V. P. and Siu, K. Y. and Orlitsky, A. and Kailath, T.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {On the Circuit Complexity of Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/8dd48d6a2e2cad213179a3992c0be53c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1710453,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4574566673309120033&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8f7279a379",
        "title": "Optimal Filtering in the Salamander Retina",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html",
        "author": "Fred Rieke; W. Geoffrey Owen; William Bialek",
        "abstract": "The dark-adapted  visual  system can  count photons  wit h  a  reliability  lim(cid:173) ited by thermal noise  in  the rod photoreceptors - the processing circuitry  bet.ween  t.he  rod  cells  and the brain is  essentially noiseless  and  in fact  may  be  close  to  optimal.  Here  we  design  an  optimal  signal  processor  which  estimates  the  time-varying  light  intensit.y  at  the  retina  based  on  the  rod  signals.  \\Ve show  that.  the first stage of optimal signal processing  involves  passing  the  rod  cell  out.put.  t.hrough  a  linear filter  with  characteristics de(cid:173) termined  entirely  by  the  rod  signal  and  noise  spectra.  This  filter  is  very  general;  in  fact  it.  is  the  first  st.age  in  any  visual  signal  processing  task  at.  10\\'  photon  flux.  \\Ve  iopntify  the  output  of this  first-st.age  filter  wit.h  the  intracellular  voltage  response of the  bipolar  celL  the  first  anatomical  st.age  in  retinal  signal  processing.  From  recent.  data on  tiger  salamander  phot.oreceptors  we  extract  t.he  relevant.  spect.ra  and  make  parameter-free,  quantit.ative predictions of the bipolar celll'esponse to a dim, diffuse flash.  Agreement  wit.h  experiment  is  essentially  perfect.  As  far  as  we  know  this  is  the  first  successful  predicti ve  t.heory  for  neural  dynamics.",
        "bibtex": "@inproceedings{NIPS1990_019d385e,\n author = {Rieke, Fred and Owen, W. and Bialek, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Optimal Filtering in the Salamander Retina},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/019d385eb67632a7e958e23f24bd07d7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1562508,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17131718668152560401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7843c5c262",
        "title": "Optimal Sampling of Natural Images: A Design Principle for the Visual System",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/f61d6947467ccd3aa5af24db320235dd-Abstract.html",
        "author": "William Bialek; Daniel L. Ruderman; A. Zee",
        "abstract": "We formulate the problem of optimizing the sampling of natural images  using an array of linear filters. Optimization of information capacity is  constrained by the noise levels of the individual channels and by a penalty  for the construction of long-range interconnections in the array. At low  signal-to-noise ratios the optimal filter characteristics correspond to bound  states of a Schrodinger equation in which the signal spectrum plays the  role of the potential. The resulting optimal filters are remarkably similar  to those observed in the mammalian visual cortex and the retinal ganglion  cells of lower vertebrates. The observed scale invariance of natural images  plays an essential role in this construction.",
        "bibtex": "@inproceedings{NIPS1990_f61d6947,\n author = {Bialek, William and Ruderman, Daniel and Zee, A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Optimal Sampling of Natural Images: A Design Principle for the Visual System},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/f61d6947467ccd3aa5af24db320235dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1484348,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8174615795623573301&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3ccc2a27ce",
        "title": "Order Reduction for Dynamical Systems Describing the Behavior of Complex Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/18997733ec258a9fcaf239cc55d53363-Abstract.html",
        "author": "Thomas B. Kepler; L. F. Abbott; Eve Marder",
        "abstract": "We have devised a scheme to reduce the complexity of dynamical  systems belonging to a class that includes most biophysically realistic  neural models. The reduction is based on transformations of variables  and perturbation expansions and it preserves a high level of fidelity to  the original system. The techniques are illustrated by reductions of the  Hodgkin-Huxley system and an augmented Hodgkin-Huxley system.",
        "bibtex": "@inproceedings{NIPS1990_18997733,\n author = {Kepler, Thomas and Abbott, L. and Marder, Eve},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Order Reduction for Dynamical Systems Describing the Behavior of Complex Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/18997733ec258a9fcaf239cc55d53363-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1305293,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12007868458354619851&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "147c4ba767",
        "title": "Oriented Non-Radial Basis Functions for Image Coding and Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/42e7aaa88b48137a16a1acd04ed91125-Abstract.html",
        "author": "Avijit Saha; Jim Christian; Dun-Sung Tang; Wu Chuan-Lin",
        "abstract": "We introduce oriented non-radial basis function networks (ONRBF)  as a generalization of Radial Basis Function networks (RBF)- wherein  the Euclidean distance metric in the exponent of the Gaussian is re(cid:173) placed by a more general polynomial. This permits the definition of  more general regions and in particular- hyper-ellipses with orienta(cid:173) tions. In the case of hyper-surface estimation this scheme requires a  smaller number of hidden units and alleviates the \"curse of dimen(cid:173) sionality\" associated kernel type approximators.In the case of an im(cid:173) age, the hidden units correspond to features in the image and the  parameters associated with each unit correspond to the rotation, scal(cid:173) ing and translation properties of that particular \"feature\". In the con(cid:173) text of the ONBF scheme, this means that an image can be  represented by a small number of features. Since, transformation of an  image by rotation, scaling and translation correspond to identical  transformations of the individual features, the ONBF scheme can be  used to considerable advantage for the purposes of image recognition  and analysis.",
        "bibtex": "@inproceedings{NIPS1990_42e7aaa8,\n author = {Saha, Avijit and Christian, Jim and Tang, Dun-Sung and Chuan-Lin, Wu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Oriented Non-Radial Basis Functions for Image Coding and Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/42e7aaa88b48137a16a1acd04ed91125-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/42e7aaa88b48137a16a1acd04ed91125-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/42e7aaa88b48137a16a1acd04ed91125-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1589116,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1379818468933145471&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3b68c68317",
        "title": "Oscillation Onset in Neural Delayed Feedback",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/04025959b191f8f9de3f924f0940515f-Abstract.html",
        "author": "Andr\u00e9 Longtin",
        "abstract": "This paper studies dynamical aspects of neural systems with delayed  neg(cid:173) ative  feedback  modelled  by  nonlinear  delay-differential  equations.  These  systems  undergo  a  Hopf bifurcation  from  a  stable  fixed  point  to  a  sta(cid:173) ble  limit  cycle  oscillation  as  certain  parameters  are  varied.  It is  shown  that  their  frequency  of oscillation  is  robust  to  parameter  variations  and  noisy  fluctuations,  a  property that  makes  these  systems  good  candidates  for  pacemakers.  The  onset  of oscillation  is  postponed  by  both  additive  and parametric noise in the sense that the state variable spends more time  near the fixed  point than it would  in  the absence of noise.  This is  also the  case  when  noise  affects  the  delayed  variable,  i.e.  when  the  system  has  a  faulty  memory.  Finally,  it  is  shown  that  a  distribution  of delays  (rather  than a  fixed  delay)  also  stabilizes  the fixed  point solution.",
        "bibtex": "@inproceedings{NIPS1990_04025959,\n author = {Longtin, Andr\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Oscillation Onset in Neural Delayed Feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/04025959b191f8f9de3f924f0940515f-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/04025959b191f8f9de3f924f0940515f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/04025959b191f8f9de3f924f0940515f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1523342,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1972372463379218771&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Complex Systems Group and Center for Nonlinear Studies, Theoretical Division B213, Los Alamos National Laboratory, Los Alamos, NM 87545",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Los Alamos National Laboratory",
        "aff_unique_dep": "Complex Systems Group and Center for Nonlinear Studies, Theoretical Division",
        "aff_unique_url": "https://www.lanl.gov",
        "aff_unique_abbr": "LANL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Alamos",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "90d175e071",
        "title": "Phase-coupling in Two-Dimensional Networks of Interacting Oscillators",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/577bcc914f9e55d5e4e4f82f9f00e7d4-Abstract.html",
        "author": "Ernst Niebur; Daniel M. Kammen; Christof Koch; Daniel L. Ruderman; Heinz G. Schuster",
        "abstract": "Coherent oscillatory activity in large networks of biological or artifi(cid:173) cial neural units may be a useful mechanism for coding information  pertaining to a single perceptual object or for  detailing regularities  within  a  data set.  We  consider  the  dynamics  of a  large  array  of  simple  coupled  oscillators  under  a  variety  of connection  schemes.  Of particular  interest  is  the  rapid  and  robust  phase-locking  that  results  from  a  \"sparse\"  scheme  where  each  oscillator  is  strongly  coupled  to a  tiny,  randomly selected, subset of its neighbors.",
        "bibtex": "@inproceedings{NIPS1990_577bcc91,\n author = {Niebur, Ernst and Kammen, Daniel and Koch, Christof and Ruderman, Daniel and Schuster, Heinz},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Phase-coupling in Two-Dimensional Networks of Interacting Oscillators},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1315558,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7768723576747625540&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computation and Neural Systems, Caltech 216-76, Pasadena, CA 91125; Department of Physics, University of California, Berkeley, CA 94720; Computation and Neural Systems, Caltech 216-76, Pasadena, CA 91125; Computation and Neural Systems, Caltech 216-76, Pasadena, CA 91125; Institut fiir Theoretische Physik, Universitat Kiel, 2300 Kiell, Germany",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "California Institute of Technology;University of California, Berkeley;Universitat Kiel",
        "aff_unique_dep": "Computation and Neural Systems;Department of Physics;Institut fiir Theoretische Physik",
        "aff_unique_url": "https://www.caltech.edu;https://www.berkeley.edu;",
        "aff_unique_abbr": "Caltech;UC Berkeley;",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Pasadena;Berkeley;",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "f80cad6e22",
        "title": "Phonetic Classification and Recognition Using the Multi-Layer Perceptron",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/3dd48ab31d016ffcbf3314df2b3cb9ce-Abstract.html",
        "author": "Hong C. Leung; James R. Glass; Michael S. Phillips; Victor W. Zue",
        "abstract": "In this paper, we will describe several extensions to our earlier work, utiliz(cid:173) ing a segment-based approach. We will formulate our segmental framework  and report our study on the use of multi-layer perceptrons for detection  and classification of phonemes. We will also examine the outputs of the  network, and compare the network performance with other classifiers. Our  investigation is performed within a set of experiments that attempts to  recognize 38 vowels and consonants in American English independent of  speaker. When evaluated on the TIMIT database, our system achieves an  accuracy of 56%.",
        "bibtex": "@inproceedings{NIPS1990_3dd48ab3,\n author = {Leung, Hong and Glass, James and Phillips, Michael and Zue, Victor W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Phonetic Classification and Recognition Using the Multi-Layer Perceptron},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1502505,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5776807557832731425&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "744be592dc",
        "title": "Planning with an Adaptive World Model",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/9be40cee5b0eee1462c82c6964087ff9-Abstract.html",
        "author": "Sebastian Thrun; Knut M\u00f6ller; Alexander Linden",
        "abstract": "We present a new connectionist planning method [TML90].  By interaction  with  an  unknown  environment,  a  world  model  is  progressively  construc(cid:173) ted  using  gradient  descent.  For  deriving  optimal actions  with  respect  to  future  reinforcement,  planning is  applied in two steps:  an experience  net(cid:173) work proposes a  plan which is subsequently optimized by gradient descent  with  a  chain of world  models,  so  that  an  optimal reinforcement  may be  obtained  when  it  is  actually  run.  The  appropriateness  of this  method  is  demonstrated by a  robotics  application and a  pole balancing task.",
        "bibtex": "@inproceedings{NIPS1990_9be40cee,\n author = {Thrun, Sebastian and M\\\"{o}ller, Knut and Linden, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Planning with an Adaptive World Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9be40cee5b0eee1462c82c6964087ff9-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/9be40cee5b0eee1462c82c6964087ff9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/9be40cee5b0eee1462c82c6964087ff9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1607564,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14786629825740985259&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e3dc160602",
        "title": "Proximity Effect Corrections in Electron Beam Lithography Using a Neural Network",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/d34ab169b70c9dcd35e62896010cd9ff-Abstract.html",
        "author": "Robert C. Frye; Kevin D. Cummings; Edward A. Rietman",
        "abstract": "We have used a neural network to compute corrections for images written  by electron beams to eliminate the proximity effects caused by electron  Iterative methods are effective. but require prohibitively  scattering.  computation time. We have instead trained a neural network to perform  equivalent corrections. resulting in a significant speed-up. We have  examined hardware  implementations using both analog and digital  electronic networks. Both had an acceptably small error of 0.5% compared  to the iterative results. Additionally. we verified that the neural network  correctly generalized the solution of the problem to include patterns not  contained in its training set. We have experimentally verified this approach  on a Cambridge Instruments EBMF 10.5 exposure system.",
        "bibtex": "@inproceedings{NIPS1990_d34ab169,\n author = {Frye, Robert and Cummings, Kevin and Rietman, Edward},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Proximity Effect Corrections in Electron Beam Lithography Using a Neural Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d34ab169b70c9dcd35e62896010cd9ff-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/d34ab169b70c9dcd35e62896010cd9ff-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/d34ab169b70c9dcd35e62896010cd9ff-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1504391,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2348992156298705527&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "AT&T Bell Laboratories; AT&T Bell Laboratories; AT&T Bell Laboratories + Motorola Inc. Phoenix Corporate Research Laboratories",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "AT&T Bell Laboratories;Motorola Inc.",
        "aff_unique_dep": ";Corporate Research Laboratories",
        "aff_unique_url": "https://www.att.com/labs;https://www.motorola.com",
        "aff_unique_abbr": "AT&T Bell Labs;Motorola",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Phoenix",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3d8a382f4b",
        "title": "Qualitative structure from motion",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/5b8add2a5d98b1a652ea7fd72d942dac-Abstract.html",
        "author": "Daphna Weinshall",
        "abstract": "Exact structure from motion is an ill-posed computation and therefore  very sensitive to noise. In this work I describe how a qualitative shape  representation, based on the sign of the Gaussian curvature, can be com(cid:173) puted directly from motion disparities, without the computation of an  exact depth map or the directions of surface normals. I show that humans  can judge the curvature sense of three points undergoing 3D motion from  two, three and four views with success rate significantly above chance. A  simple RBF net has been trained to perform the same task.",
        "bibtex": "@inproceedings{NIPS1990_5b8add2a,\n author = {Weinshall, Daphna},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Qualitative structure from motion},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5b8add2a5d98b1a652ea7fd72d942dac-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/5b8add2a5d98b1a652ea7fd72d942dac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/5b8add2a5d98b1a652ea7fd72d942dac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1572880,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:CJ2-iJvAY1sJ:scholar.google.com/&scioq=Qualitative+structure+from+motion&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": "Center for Biological Information Processing, MIT, E25-201, Cambridge MA 02139",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Center for Biological Information Processing",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "156f51a08e",
        "title": "Rapidly Adapting Artificial Neural Networks for Autonomous Navigation",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/248e844336797ec98478f85e7626de4a-Abstract.html",
        "author": "Dean Pomerleau",
        "abstract": "The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses  the problem  of training artificial  neural  networks in  real  time to  perform difficult  perception tasks.  ALVINN ,is  a back-propagation network that uses inputs from  a  video camera and an imaging laser rangefinder to drive the CMU Navlab, a modified  Chevy van.  This paper describes training techniques which allow ALVINN to learn  in under 5 minutes to autonomously control the Navlab by watching a human driver's  response  to  new  situations.  Using  these  techniques,  ALVINN  has  been  trained  to  drive  in  a  variety  of circumstances  including  single-lane  paved  and  unpaved  roads,  multilane  lined  and  unlined  roads,  and  obstacle-ridden  on- and  off-road  environments, at speeds of up to 20 miles per hour.",
        "bibtex": "@inproceedings{NIPS1990_248e8443,\n author = {Pomerleau, Dean},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Rapidly Adapting Artificial Neural Networks for Autonomous Navigation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/248e844336797ec98478f85e7626de4a-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/248e844336797ec98478f85e7626de4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/248e844336797ec98478f85e7626de4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1774812,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8068856921543820870&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science, Carnegie Mellon University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a66d9902f7",
        "title": "Real-time autonomous robot navigation using VLSI neural networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/66808e327dc79d135ba18e051673d906-Abstract.html",
        "author": "Lionel Tarassenko; Michael Brownlow; Gillian Marshall; Jan Tombs; Alan Murray",
        "abstract": "We describe a real time robot navigation system based on three VLSI  neural network modules. These are a resistive grid for path planning, a  nearest-neighbour classifier for localization using range data from a time(cid:173) of-flight infra-red sensor and a sensory-motor associative network for dy(cid:173) namic obstacle avoidance .",
        "bibtex": "@inproceedings{NIPS1990_66808e32,\n author = {Tarassenko, Lionel and Brownlow, Michael and Marshall, Gillian and Tombs, Jan and Murray, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Real-time autonomous robot navigation using VLSI neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/66808e327dc79d135ba18e051673d906-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/66808e327dc79d135ba18e051673d906-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/66808e327dc79d135ba18e051673d906-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1585564,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9782724872220302650&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Engineering Science, Oxford University, Oxford, OX1 3PJ, UK + RSRE, Great Malvern, Worcester, WR14 3PS; Department of Engineering Science, Oxford University, Oxford, OX1 3PJ, UK; Department of Engineering Science, Oxford University, Oxford, OX1 3PJ, UK; Department of Electrical Engineering, Edinburgh University, Edinburgh, EH9 3JL, UK; Department of Electrical Engineering, Edinburgh University, Edinburgh, EH9 3JL, UK",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;2;2",
        "aff_unique_norm": "Oxford University;RSRE;Edinburgh University",
        "aff_unique_dep": "Department of Engineering Science;;Department of Electrical Engineering",
        "aff_unique_url": "https://www.ox.ac.uk;;https://www.ed.ac.uk",
        "aff_unique_abbr": "Oxford;;Edinburgh",
        "aff_campus_unique_index": "0+1;0;0;2;2",
        "aff_campus_unique": "Oxford;Great Malvern;Edinburgh",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "cb37bdde86",
        "title": "RecNorm: Simultaneous Normalisation and Classification applied to Speech Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/cd00692c3bfe59267d5ecfac5310286c-Abstract.html",
        "author": "John S. Bridle; Stephen J. Cox",
        "abstract": "A particular form of neural network is described, which has terminals  for acoustic patterns, class labels and speaker parameters. A method of  training this network to \"tune in\" the speaker parameters to a particular  speaker is outlined, based on a trick for converting a supervised network  to an unsupervised mode. We describe experiments using this approach  in isolated word recognition based on whole-word hidden Markov models.  The results indicate an improvement over speaker-independent perfor(cid:173) mance and, for unlabelled data, a performance close to that achieved on  labelled data.",
        "bibtex": "@inproceedings{NIPS1990_cd00692c,\n author = {Bridle, John and Cox, Stephen J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {RecNorm: Simultaneous Normalisation and Classification applied to Speech Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/cd00692c3bfe59267d5ecfac5310286c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/cd00692c3bfe59267d5ecfac5310286c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/cd00692c3bfe59267d5ecfac5310286c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1466832,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15408343383080058561&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Royal Signals and Radar Est. Great Malvern UK WR143PS; British Telecom Research Labs. Ipswich UK IP57RE",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Royal Signals and Radar Establishment;British Telecom",
        "aff_unique_dep": ";Research Labs",
        "aff_unique_url": ";https://www.btplabs.com",
        "aff_unique_abbr": "RSRE;BT",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Great Malvern;Ipswich",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "0a32ad25c5",
        "title": "Reconfigurable Neural Net Chip with 32K Connections",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html",
        "author": "H. P. Graf; R. Janow; D. Henderson; R. Lee",
        "abstract": "We describe a CMOS neural net chip with a reconfigurable network archi(cid:173) tecture. It contains 32,768 binary, programmable connections arranged in  256 'building block' neurons. Several 'building blocks' can be connected to  form long neurons with up to 1024 binary connections or to form neurons  with analog connections. Single- or multi-layer networks can be imple(cid:173) mented with this chip. We have integrated this chip into a board system  together with a digital signal processor and fast memory. This system is  currently in use for image processing applications in which the chip extracts  features such as edges and corners from binary and gray-level images.",
        "bibtex": "@inproceedings{NIPS1990_37bc2f75,\n author = {Graf, H. P. and Janow, R. and Henderson, D. and Lee, R.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Reconfigurable Neural Net Chip with 32K Connections},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/37bc2f75bf1bcfe8450a1a41c200364c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1493545,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18061428949147352173&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "24a2360560",
        "title": "Reinforcement Learning in Markovian and Non-Markovian Environments",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/70c639df5e30bdee440e4cdf599fec2b-Abstract.html",
        "author": "J\u00fcrgen Schmidhuber",
        "abstract": "This work addresses three problems with reinforcement learning and adap(cid:173) tive  neuro-control:  1.  Non-Markovian interfaces  between  learner and en(cid:173) vironment.  2.  On-line learning  based  on  system  realization.  3.  Vector(cid:173) valued adaptive critics.  An algorithm is described which is based on system  realization and on two interacting fully recurrent  continually running net(cid:173) works  which  may  learn  in  parallel.  Problems  with  parallel  learning  are  attacked  by  'adaptive randomness'.  It is  also  described  how  interacting  model/controller  systems  can  be  combined  with  vector-valued  'adaptive  critics'  (previous  critics have been  scalar).",
        "bibtex": "@inproceedings{NIPS1990_70c639df,\n author = {Schmidhuber, J\\\"{u}rgen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Reinforcement Learning in Markovian and Non-Markovian Environments},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/70c639df5e30bdee440e4cdf599fec2b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/70c639df5e30bdee440e4cdf599fec2b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/70c639df5e30bdee440e4cdf599fec2b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1773898,
        "gs_citation": 218,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=36581724787517731&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Institut fiir Informatik, Technische Universitat Miinchen",
        "aff_domain": "tumult.informatik.tu-muenchen.de",
        "email": "tumult.informatik.tu-muenchen.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Technische Universitat Munchen",
        "aff_unique_dep": "Institut fur Informatik",
        "aff_unique_url": "https://www.tum.de",
        "aff_unique_abbr": "TUM",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "d53f509b7e",
        "title": "Relaxation Networks for Large Supervised Learning Problems",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/05049e90fa4f5039a8cadc6acbb4b2cc-Abstract.html",
        "author": "Joshua Alspector; Robert B. Allen; Anthony Jayakumar; Torsten Zeppenfeld; Ronny Meir",
        "abstract": "Feedback  connections  are  required  so  that  the  teacher  signal  on  the  output  neurons  can  modify  weights  during  supervised  learning.  Relaxation  methods  are  needed  for  learning  static  patterns  with  full-time  feedback  connections.  Feedback  network  learning  techniques  have  not  achieved  wide  popularity  because  of the  still greater  computational efficiency  of back-propagation.  We  show by simulation that relaxation networks of the kind we  are implementing in  VLSI  are  capable  of  learning  large  problems  just  like  back-propagation  networks.  A microchip incorporates deterministic mean-field theory learning as  well  as  stochastic  Boltzmann  learning.  A  multiple-chip  electronic  system  implementing  these  networks  will  make  high-speed  parallel  learning  in  them  feasible in the future.",
        "bibtex": "@inproceedings{NIPS1990_05049e90,\n author = {Alspector, Joshua and Allen, Robert and Jayakumar, Anthony and Zeppenfeld, Torsten and Meir, Ronny},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Relaxation Networks for Large Supervised Learning Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/05049e90fa4f5039a8cadc6acbb4b2cc-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/05049e90fa4f5039a8cadc6acbb4b2cc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/05049e90fa4f5039a8cadc6acbb4b2cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1400975,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16957386278410807867&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Bellcore; Bellcore; Bellcore; Bellcore; Bellcore",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Bellcore",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bellcore.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f962ae3340",
        "title": "Remarks on Interpolation and Recognition Using Neural Nets",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/2421fcb1263b9530df88f7f002e78ea5-Abstract.html",
        "author": "Eduardo D. Sontag",
        "abstract": "We consider different  types  of single-hidden-Iayer feedforward  nets:  with  or  without  direct  input  to  output  connections,  and  using  either  thresh(cid:173) old  or  sigmoidal activation functions.  The  main results  show  that  direct  connections in  threshold nets  double  the  recognition  but not  the interpo(cid:173) lation power, while using sigmoids  rather than thresholds allows (at least)  doubling  both.  Various results are also given on VC dimension and  other  measures of recognition capabilities.",
        "bibtex": "@inproceedings{NIPS1990_2421fcb1,\n author = {Sontag, Eduardo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Remarks on Interpolation and Recognition Using Neural Nets},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/2421fcb1263b9530df88f7f002e78ea5-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/2421fcb1263b9530df88f7f002e78ea5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/2421fcb1263b9530df88f7f002e78ea5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1588343,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15899732701236978213&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ac11ef4713",
        "title": "SEXNET: A Neural Network Identifies Sex From Human Faces",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/bbcbff5c1f1ded46c25d28119a85c6c2-Abstract.html",
        "author": "B.A. Golomb; D.T. Lawrence; T.J. Sejnowski",
        "abstract": "Sex identification in animals has biological importance. Humans are good  at making this determination visually, but machines have not matched  this ability. A neural network was trained to discriminate sex in human  faces, and performed as well as humans on a set of 90 exemplars. Images  sampled at 30x30 were compressed using a 900x40x900 fully-connected  back-propagation network; activities of hidden units served as input to a  back-propagation \"SexNet\" trained to produce values of 1 for male and  o for female faces. The network's average error rate of 8.1% compared  favorably to humans, who averaged 11.6%. Some SexNet errors mimicked  those of humans.",
        "bibtex": "@inproceedings{NIPS1990_bbcbff5c,\n author = {Golomb, B.A. and Lawrence, D.T. and Sejnowski, T.J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {SEXNET: A Neural Network Identifies Sex From Human Faces},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bbcbff5c1f1ded46c25d28119a85c6c2-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/bbcbff5c1f1ded46c25d28119a85c6c2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/bbcbff5c1f1ded46c25d28119a85c6c2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 177044,
        "gs_citation": 770,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4955255718794930793&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b0f3b3bd60",
        "title": "Second Order Properties of Error Surfaces: Learning Time and Generalization",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/758874998f5bd0c393da094e1967a72b-Abstract.html",
        "author": "Yann LeCun; Ido Kanter; Sara A. Solla",
        "abstract": "The learning time of a simple neural network model is obtained through an  analytic computation of the eigenvalue spectrum for the Hessian matrix,  which describes the second order properties of the cost function in the  space of coupling coefficients. The form of the eigenvalue distribution  suggests new techniques for accelerating the learning process, and provides  a theoretical justification for the choice of centered versus biased state  variables.",
        "bibtex": "@inproceedings{NIPS1990_75887499,\n author = {LeCun, Yann and Kanter, Ido and Solla, Sara},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Second Order Properties of Error Surfaces: Learning Time and Generalization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/758874998f5bd0c393da094e1967a72b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/758874998f5bd0c393da094e1967a72b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/758874998f5bd0c393da094e1967a72b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1483367,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4019249806778564128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d4e4fe364b",
        "title": "Self-organization of Hebbian Synapses in Hippocampal Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/138bb0696595b338afbab333c555292a-Abstract.html",
        "author": "Thomas H. Brown; Zachary F. Mainen; Anthony M. Zador; Brenda J. Claiborne",
        "abstract": "We are exploring the significance of biological complexity for neuronal  computation.  Here we demonstrate that Hebbian synapses in realistical(cid:173) ly-modeled  hippocampal  pyramidal  cells  may  give  rise  to  two  novel  forms of self -organization in response to structured synaptic input.  First,  on the basis of the electrotonic relationships between synaptic contacts,  a cell may become tuned to a small subset of its input space.  Second, the  same mechanisms may produce  clusters of potentiated synapses across  the space of the dendrites.  The latter type of self-organization may be  functionally  significant in  the presence of nonlinear dendritic  conduc(cid:173) tances.",
        "bibtex": "@inproceedings{NIPS1990_138bb069,\n author = {Brown, Thomas and Mainen, Zachary and Zador, Anthony and Claiborne, Brenda},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Self-organization of Hebbian Synapses in Hippocampal Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/138bb0696595b338afbab333c555292a-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/138bb0696595b338afbab333c555292a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/138bb0696595b338afbab333c555292a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1506039,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18311976520674284582&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b3b330019e",
        "title": "Sequential Adaptation of Radial Basis Function Neural Networks and its Application to Time-series Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/ffd52f3c7e12435a724a8f30fddadd9c-Abstract.html",
        "author": "V. Kadirkamanathan; M. Niranjan; F. Fallside",
        "abstract": "We develop a sequential adaptation algorithm for radial basis function  (RBF) neural networks of Gaussian nodes, based on the method of succes(cid:173) sive F-Projections. This method makes use of each observation efficiently  in that the network mapping function so obtained is consistent with that  information and is also optimal in the least L 2-norm sense. The RBF  network with the F-Projections adaptation algorithm was used for pre(cid:173) dicting a chaotic time-series. We compare its performance to an adapta(cid:173) tion scheme based on the method of stochastic approximation, and show  that the F-Projections algorithm converges to the underlying model much  faster.",
        "bibtex": "@inproceedings{NIPS1990_ffd52f3c,\n author = {Kadirkamanathan, V. and Niranjan, M. and Fallside, F.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Sequential Adaptation of Radial Basis Function Neural Networks and its Application to Time-series Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ffd52f3c7e12435a724a8f30fddadd9c-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/ffd52f3c7e12435a724a8f30fddadd9c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/ffd52f3c7e12435a724a8f30fddadd9c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1431837,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10376512699837496945&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Engineering Department, Cambridge University, Cambridge CB2 IPZ, UK; ; ",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Engineering Department",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "e6e53b8330",
        "title": "Shaping the State Space Landscape in Recurrent Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/4f6ffe13a5d75b2d6a3923922b3922e5-Abstract.html",
        "author": "Patrice Simard; Jean Pierre Raysz; Bernard Victorri",
        "abstract": "Bernard Victorri  ELSAP  Universite  de  Caen  14032 Caen  Cedex  France",
        "bibtex": "@inproceedings{NIPS1990_4f6ffe13,\n author = {Simard, Patrice and Raysz, Jean and Victorri, Bernard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Shaping the State Space Landscape in Recurrent Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/4f6ffe13a5d75b2d6a3923922b3922e5-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/4f6ffe13a5d75b2d6a3923922b3922e5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/4f6ffe13a5d75b2d6a3923922b3922e5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1486567,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1648930697078034619&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "61ca31ecbd",
        "title": "Signal Processing by Multiplexing and Demultiplexing in Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html",
        "author": "David C. Tam",
        "abstract": "to",
        "bibtex": "@inproceedings{NIPS1990_c5ff2543,\n author = {Tam, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Signal Processing by Multiplexing and Demultiplexing in Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/c5ff2543b53f4cc0ad3819a36752467b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1607581,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17967428958125191148&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Division of Neuroscience, Baylor College of Medicine",
        "aff_domain": "next-cns.neusc.bcm.tmc.edu",
        "email": "next-cns.neusc.bcm.tmc.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Baylor College of Medicine",
        "aff_unique_dep": "Division of Neuroscience",
        "aff_unique_url": "https://www.bcm.edu",
        "aff_unique_abbr": "BCM",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7dd1eaccae",
        "title": "Simple Spin Models for the Development of Ocular Dominance Columns and Iso-Orientation Patches",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/9de6d14fff9806d4bcd1ef555be766cd-Abstract.html",
        "author": "J.D. Cowan; A. E. Friedman",
        "abstract": "Simple classical spin models well-known to physicists as the ANNNI  and Heisenberg XY Models. in which long-range interactions occur in  a pattern given by the Mexican Hat operator. can generate many of the  structural properties characteristic of the ocular dominance columns  and iso-orientation patches seen in cat and primate visual cortex.",
        "bibtex": "@inproceedings{NIPS1990_9de6d14f,\n author = {Cowan, J.D. and Friedman, A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Simple Spin Models for the Development of Ocular Dominance Columns and Iso-Orientation Patches},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/9de6d14fff9806d4bcd1ef555be766cd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 922995,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12509629845301407543&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d95426a57d",
        "title": "Simulation of the Neocognitron on a CCD Parallel Processing Architecture",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/17d63b1625c816c22647a73e1482372b-Abstract.html",
        "author": "Michael L. Chuang; Alice M. Chiang",
        "abstract": "The neocognitron  is  a  neural network for  pattern recognition  and feature  extraction.  An  analog  CCD  parallel  processing  architecture  developed  at Lincoln Laboratory is  particularly  well suited to the computational re(cid:173) quirements of shared-weight networks such as the neocognitron, and imple(cid:173) mentation of the neocognitron using the CCD architecture was simulated.  A  modification  to  the  neocognitron  training  procedure,  which  improves  network performance under the limited arithmetic  precision that would be  imposed  by the CCD  architecture,  is  presented.",
        "bibtex": "@inproceedings{NIPS1990_17d63b16,\n author = {Chuang, Michael and Chiang, Alice},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Simulation of the Neocognitron on a CCD Parallel Processing Architecture},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/17d63b1625c816c22647a73e1482372b-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/17d63b1625c816c22647a73e1482372b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/17d63b1625c816c22647a73e1482372b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1412110,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:2yLP82EIzYoJ:scholar.google.com/&scioq=Simulation+of+the+Neocognitron+on+a+CCD+Parallel+Processing+Architecture&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d6041080ed",
        "title": "Speech Recognition Using Connectionist Approaches",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html",
        "author": "Khalid Choukri",
        "abstract": "This  paper  is  a  summary  of SPRINT  project  aims  and  results.  The  project  focus  on the use  of neuro-computing techniques to tackle various problems that  remain  unsolved  in  speech  recognition.  First  results  concern  the  use  of feed(cid:173) forward  nets  for  phonetic  units  classification,  isolated  word  recognition,  and  speaker  adaptation.",
        "bibtex": "@inproceedings{NIPS1990_a01a0380,\n author = {Choukri, Khalid},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Speech Recognition Using Connectionist Approaches},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/a01a0380ca3c61428c26a231f0e49a09-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1882508,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15822858406041314235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "CAP GEMINI INNOVATION",
        "aff_domain": "capsogeti.fr",
        "email": "capsogeti.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Capgemini",
        "aff_unique_dep": "Innovation",
        "aff_unique_url": "https://www.capgemini.com",
        "aff_unique_abbr": "Capgemini",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "id": "c9e0f23285",
        "title": "Speech Recognition Using Demi-Syllable Neural Prediction Model",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html",
        "author": "Ken-ichi Iso; Takao Watanabe",
        "abstract": "The  Neural  Prediction  Model  is  the  speech  recognition  model  based  on  pattern  prediction  by  multilayer  perceptrons.  Its  effectiveness  was  con(cid:173) firmed  by  the  speaker-independent  digit  recognition  experiments.  This  paper  presents  an  improvement  in  the model  and  its  application  to  large  vocabulary speech recognition,  based on subword units.  The improvement  involves an introduction  of \"backward  prediction,\"  which further  improves  the  prediction  accuracy  of the  original  model  with  only  \"forward  predic(cid:173) tion\".  In  application of the model  to  speaker-dependent large vocabulary  speech recognition,  the demi-syllable unit is  used  as  a subword recognition  unit.  Experimental  results  indicated  a  95.2%  recognition  accuracy  for  a  5000  word  test  set  and  the  effectiveness  was  confirmed  for  the  proposed  model  improvement and  the demi-syllable subword units.",
        "bibtex": "@inproceedings{NIPS1990_e7b24b11,\n author = {Iso, Ken-ichi and Watanabe, Takao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Speech Recognition Using Demi-Syllable Neural Prediction Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1408480,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6674807219997954671&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f7ed45dde4",
        "title": "Spherical Units as Dynamic Consequential Regions: Implications for Attention, Competition and Categorization",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/9fd81843ad7f202f26c1a174c7357585-Abstract.html",
        "author": "Stephen Jose Hanson; Mark A. Gluck",
        "abstract": "to construct dynamic",
        "bibtex": "@inproceedings{NIPS1990_9fd81843,\n author = {Hanson, Stephen and Gluck, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Spherical Units as Dynamic Consequential Regions: Implications for Attention, Competition and Categorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9fd81843ad7f202f26c1a174c7357585-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/9fd81843ad7f202f26c1a174c7357585-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/9fd81843ad7f202f26c1a174c7357585-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1764692,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7655788265930518364&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Learning and Knowledge Acquisition Group, Rutgers University, Newark, NJ 07102 + Cognitive Science Laboratory, Princeton University, Princeton, NJ 08544; Center for Molecular & Behavioral Neuroscience, Rutgers University, Newark, NJ 07102 + Siemens Corporate Research, Princeton, NJ 08540",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2",
        "aff_unique_norm": "Rutgers University;Princeton University;Siemens Corporate Research",
        "aff_unique_dep": "Learning and Knowledge Acquisition Group;Cognitive Science Laboratory;",
        "aff_unique_url": "https://www.rutgers.edu;https://www.princeton.edu;https://www.siemens.com/research",
        "aff_unique_abbr": "Rutgers;Princeton;SCR",
        "aff_campus_unique_index": "0+1;0+1",
        "aff_campus_unique": "Newark;Princeton",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e64de98908",
        "title": "Spoken Letter Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/49182f81e6a13cf5eaa496d51fea6406-Abstract.html",
        "author": "Mark Fanty; Ronald Cole",
        "abstract": "Through the use of neural network classifiers and careful feature selection,  we have achieved high-accuracy speaker-independent spoken letter recog(cid:173) nition. For isolated letters, a broad-category segmentation is performed  Location of segment boundaries allows us to measure features at specific  locations in the signal such as vowel onset, where important information  resides. Letter classification is performed with a feed-forward neural net(cid:173) work. Recognition accuracy on a test set of 30 speakers was 96%. Neu(cid:173) ral network classifiers are also used for pitch tracking and broad-category  segmentation of letter strings. Our research has been extended to recog(cid:173) nition of names spelled with pauses between the letters. When searching  a database of 50,000 names, we achieved 95% first choice name retrieval.  Work has begun on a continuous letter classifier which does frame-by-frame  phonetic classification of spoken letters.",
        "bibtex": "@inproceedings{NIPS1990_49182f81,\n author = {Fanty, Mark and Cole, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Spoken Letter Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/49182f81e6a13cf5eaa496d51fea6406-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1702453,
        "gs_citation": 303,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5992516178391196261&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Science and Engineering, Oregon Graduate Institute; Dept. of Computer Science and Engineering, Oregon Graduate Institute",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon Graduate Institute",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.ogi.edu",
        "aff_unique_abbr": "OGI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5fe981e562",
        "title": "Statistical Mechanics of Temporal Association in Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html",
        "author": "Andreas V. M. Herz; Zhaoping Li; J. Leo van Hemmen",
        "abstract": "We study the representation of static patterns and temporal associa(cid:173) tions in  neural networks with  a  broad  distribution of signal  delays.  For a certain class of such systems, a simple intuitive understanding  of the spatia-temporal computation becomes possible with the help  of a  novel  Lyapunov  functional. It allows  a  quantitative  study  of  the  asymptotic  network  behavior  through  a  statistical  mechanical  analysis. We  present  analytic  calculations of both retrieval  quality  and storage capacity and compare  them with  simulation results.",
        "bibtex": "@inproceedings{NIPS1990_ad972f10,\n author = {Herz, Andreas and Li, Zhaoping and van Hemmen, J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Statistical Mechanics of Temporal Association in Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/ad972f10e0800b49d76fed33a21f6698-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1693767,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11857555628666730328&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "df07302afb",
        "title": "Stereopsis by a Neural Network Which Learns the Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/f9b902fc3289af4dd08de5d1de54f68f-Abstract.html",
        "author": "Alireza Khotanzad; Ying-Wung Lee",
        "abstract": "This paper presents a neural network (NN) approach to the problem of  stereopsis. The correspondence problem (finding the correct matches  between the pixels of the epipolar lines of the stereo pair from amongst all  the possible matches) is posed as a non-iterative many-to-one mapping . A  two-layer feed forward NN architecture is developed to learn and code this  nonlinear and complex mapping using the back-propagation learning rule  and a training set. The important aspect of this technique is that none of  the typical constraints such as uniqueness and continuity are explicitly  imposed. All the applicable constraints are learned and internally coded  by the NN enabling it to be more flexible and more accurate than the  existing methods. The approach is successfully tested on several random(cid:173) dot stereograms. It is shown that the net can generalize its learned map(cid:173) ping to cases outside its training set. Advantages over the Marr-Poggio  Algorithm are discussed and it is shown that the NN performance is supe(cid:173) rIOr.",
        "bibtex": "@inproceedings{NIPS1990_f9b902fc,\n author = {Khotanzad, Alireza and Lee, Ying-Wung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Stereopsis by a Neural Network Which Learns the Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/f9b902fc3289af4dd08de5d1de54f68f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1799240,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=942490610319582665&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Image Processing and Analysis Laboratory, Electrical Engineering Department, Southern Methodist University; Image Processing and Analysis Laboratory, Electrical Engineering Department, Southern Methodist University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Southern Methodist University",
        "aff_unique_dep": "Electrical Engineering Department",
        "aff_unique_url": "https://www.smu.edu",
        "aff_unique_abbr": "SMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e6c70ab1af",
        "title": "Stochastic Neurodynamics",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/3c7781a36bcd6cf08c11a970fbe0e2a6-Abstract.html",
        "author": "J.D. Cowan",
        "abstract": "The main point of this paper is that stochastic neural networks have a  mathematical structure that corresponds quite closely with that of  quantum field theory. Neural network Liouvillians and Lagrangians  can be derived, just as can spin Hamiltonians and Lagrangians in QFf.  It remains to show the efficacy of such a description.",
        "bibtex": "@inproceedings{NIPS1990_3c7781a3,\n author = {Cowan, J.D.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Stochastic Neurodynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1094361,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6270848582909185184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Mathematics, Committee on Neurobiology, and Brain Research Institute, The University of Chicago",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Chicago",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.uchicago.edu",
        "aff_unique_abbr": "UChicago",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "88503eb757",
        "title": "The Devil and the Network: What Sparsity Implies to Robustness and Memory",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/eed5af6add95a9a6f1252739b1ad8c24-Abstract.html",
        "author": "Sanjay Biswas; Santosh S. Venkatesh",
        "abstract": "Robustness is a commonly bruited property of neural networks; in particu(cid:173) lar, a folk theorem in neural computation asserts that neural networks-in  contexts with large interconnectivity-continue to function efficiently, al(cid:173) beit with some degradation, in the presence of component damage or loss.  A second folk theorem in such contexts asserts that dense interconnectiv(cid:173) ity between neural elements is a sine qua non for the efficient usage of  resources. These premises are formally examined in this communication  in a setting that invokes the notion of the \"devil\" 1 in the network as an  agent that produces sparsity by snipping connections.",
        "bibtex": "@inproceedings{NIPS1990_eed5af6a,\n author = {Biswas, Sanjay and Venkatesh, Santosh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {The Devil and the Network: What Sparsity Implies to Robustness and Memory},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/eed5af6add95a9a6f1252739b1ad8c24-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/eed5af6add95a9a6f1252739b1ad8c24-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/eed5af6add95a9a6f1252739b1ad8c24-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1397965,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3732911650413021063&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical Engineering, University of Pennsylvania; Department of Electrical Engineering, University of Pennsylvania",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a58e5814e1",
        "title": "The Recurrent Cascade-Correlation Architecture",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/fe73f687e5bc5280214e0486b273a5f9-Abstract.html",
        "author": "Scott E. Fahlman",
        "abstract": "Recurrent  Cascade-Correlation  CRCC)  is  a recurrent  version  of the  Cascade(cid:173) Correlation learning architecture of Fah I man and Lebiere [Fahlman, 1990].  RCC  can learn from examples to map a sequence of inputs into a desired sequence of  outputs.  New hidden units with recurrent connections are added to the network  as needed during training.  In effect, the network builds up a finite-state machine  tailored  specifically  for  the current problem.  RCC  retains  the  advantages  of  Cascade-Correlation:  fast learning, good generalization, automatic construction  of a near-minimal multi-layered network, and incremental training.",
        "bibtex": "@inproceedings{NIPS1990_fe73f687,\n author = {Fahlman, Scott},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {The Recurrent Cascade-Correlation Architecture},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/fe73f687e5bc5280214e0486b273a5f9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1826054,
        "gs_citation": 348,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8906513360858640820&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "baf4b22424",
        "title": "The Tempo 2 Algorithm: Adjusting Time-Delays By Supervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/faa9afea49ef2ff029a833cccc778fd0-Abstract.html",
        "author": "Ulrich Bodenhausen; Alex Waibel",
        "abstract": "In this work we describe a new method that adjusts time-delays and the widths of  time-windows in artificial neural networks automatically.  The input of the units  are weighted by a gaussian input-window over time which allows the learning  rules for the delays and widths to be derived in the same way as it is used for the  weights.  Our results on a phoneme classification task compare well with results  obtained with the TDNN by Waibel et al., which was manually optimized for the  same task.",
        "bibtex": "@inproceedings{NIPS1990_faa9afea,\n author = {Bodenhausen, Ulrich and Waibel, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {The Tempo 2 Algorithm: Adjusting Time-Delays By Supervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/faa9afea49ef2ff029a833cccc778fd0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1530215,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5723524204580178811&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eb4cd9946e",
        "title": "Time Trials on Second-Order and Variable-Learning-Rate Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/f73b76ce8949fe29bf2a537cfa420e8f-Abstract.html",
        "author": "Richard Rohwer",
        "abstract": "The performance of seven minimization algorithms are compared on five  neural network problems. These include a variable-step-size algorithm,  conjugate gradient, and several methods with explicit analytic or numerical  approximations to the Hessian.",
        "bibtex": "@inproceedings{NIPS1990_f73b76ce,\n author = {Rohwer, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Time Trials on Second-Order and Variable-Learning-Rate Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/f73b76ce8949fe29bf2a537cfa420e8f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1329831,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14490553277614182978&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ed074164b4",
        "title": "Training Knowledge-Based Neural Networks to Recognize Genes in DNA Sequences",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/8efb100a295c0c690931222ff4467bb8-Abstract.html",
        "author": "Michiel O. Noordewier; Geoffrey G. Towell; Jude W. Shavlik",
        "abstract": "We describe the application of a hybrid symbolic/connectionist machine  learning algorithm to the task of recognizing important genetic sequences.  The symbolic portion of the KBANN system utilizes inference rules that  provide a roughly-correct method for recognizing a class of DNA sequences  known as eukaryotic splice-junctions. We then map this \"domain theory\"  into a neural network and provide training examples. Using the samples,  the neural network's learning algorithm adjusts the domain theory so that  it properly classifies these DNA sequences. Our procedure constitutes  a general method for incorporating preexisting knowledge into artificial  neural networks. We present an experiment in molecular genetics that  demonstrates the value of doing so.",
        "bibtex": "@inproceedings{NIPS1990_8efb100a,\n author = {Noordewier, Michiel and Towell, Geoffrey and Shavlik, Jude},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Training Knowledge-Based Neural Networks to Recognize Genes in DNA Sequences},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8efb100a295c0c690931222ff4467bb8-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/8efb100a295c0c690931222ff4467bb8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/8efb100a295c0c690931222ff4467bb8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1612947,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1320327826864391277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e0f09b4f60",
        "title": "Transforming Neural-Net Output Levels to Probability Distributions",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/7eacb532570ff6858afd2723755ff790-Abstract.html",
        "author": "John S. Denker; Yann LeCun",
        "abstract": "(1)  The  outputs  of a  typical  multi-output  classification  network  do  not  satisfy the axioms of probability; probabilities should be positive and sum  to one.  This problem  can  be solved  by  treating  the trained  network  as  a  preprocessor that produces  a  feature  vector that can be further  processed,  for instance by classical statistical estimation techniques.  (2) We present a  method for computing the first two moments ofthe probability distribution  indicating the range of outputs that are  consistent  with the input and the  training  data.  It is  particularly  useful  to  combine  these  two  ideas:  we  implement the  ideas  of section  1 using  Parzen  windows,  where  the  shape  and relative size  of each  window  is  computed  using the ideas of section  2.  This  allows  us  to make  contact  between  important  theoretical ideas  (e.g.  the  ensemble  formalism)  and  practical  techniques  (e.g.  back-prop).  Our  results  also  shed  new  light  on  and  generalize  the  well-known  \"soft max\"  scheme.",
        "bibtex": "@inproceedings{NIPS1990_7eacb532,\n author = {Denker, John and LeCun, Yann},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Transforming Neural-Net Output Levels to Probability Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/7eacb532570ff6858afd2723755ff790-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/7eacb532570ff6858afd2723755ff790-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/7eacb532570ff6858afd2723755ff790-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1702827,
        "gs_citation": 437,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10488973702651922776&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9c802b0a95",
        "title": "Translating Locative Prepositions",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html",
        "author": "Paul W. Munro; Mary Tabasko",
        "abstract": "A network was trained by back propagation to map locative expressions  of the form \"noun-preposition-noun\" to a semantic representation, as in  Cosic  and  Munro  (1988).  The  network's  performance  was  analyzed  over  several  simulations  with  training  sets  in  both  English  and  German.  Translation  of prepositions  was  attempted  by  presenting  a  locative expression to a  network trained in one language to generate a  semantic representation; the semantic representation was  then presented  to the network trained in the other language to generate the appropriate  preposition.",
        "bibtex": "@inproceedings{NIPS1990_0c74b7f7,\n author = {Munro, Paul and Tabasko, Mary},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Translating Locative Prepositions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1394583,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7328865754357815251&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Information Science, University of Pittsburgh; Department of Information Science, University of Pittsburgh",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "Department of Information Science",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1eab787487",
        "title": "Using Genetic Algorithms to Improve Pattern Classification Performance",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/00ec53c4682d36f5c4359f4ae7bd7ba1-Abstract.html",
        "author": "Eric I. Chang; Richard P Lippmann",
        "abstract": "Genetic  algorithms  were  used  to select  and  create  features  and  to select  reference  exemplar  patterns for  machine vision  and speech  pattern  classi(cid:173) fication  tasks.  For a  complex speech  recognition  task,  genetic  algorithms  required no more computation time than traditional approaches to feature  selection  but reduced  the number of input features  required  by a factor  of  five  (from 153 to 33 features).  On a difficult artificial machine-vision task,  genetic  algorithms were  able  to create  new features  (polynomial functions  of the original features)  which reduced  classification error rates from  19%  to  almost  0%.  Neural  net  and  k  nearest  neighbor  (KNN)  classifiers  were  unable to provide such low error rates using only the original features.  Ge(cid:173) netic algorithms were also used to reduce the number of reference exemplar  patterns for  a  KNN classifier.  On a  338 training pattern vowel-recognition  problem with  10  classes,  genetic  algorithms reduced  the number of stored  exemplars from 338 to 43 without significantly increasing classification er(cid:173) ror  rate.  In  all  applications,  genetic  algorithms  were  easy  to  apply  and  found  good  solutions  in  many fewer  trials  than would  be  required  by ex(cid:173) haustive search.  Run times were long, but not unreasonable.  These results  suggest  that genetic  algorithms  are  becoming practical for  pattern  classi(cid:173) fication  problems  as faster  serial  and  parallel computers  are  developed.",
        "bibtex": "@inproceedings{NIPS1990_00ec53c4,\n author = {Chang, Eric and Lippmann, Richard P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Using Genetic Algorithms to Improve Pattern Classification Performance},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1596203,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10070630208169667504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "bf38f0e5a6",
        "title": "VLSI Implementation of TInMANN",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html",
        "author": "Matt Melton; Tan Phan; Doug Reeves; Dave Van den Bout",
        "abstract": "A massively parallel, all-digital, stochastic architecture - TlnMAN N - is  described which performs competitive and Kohonen types of learning. A  VLSI design is shown for a TlnMANN neuron which fits within a small,  inexpensive MOSIS TinyChip frame, yet which can be used to build larger  networks of several hundred neurons. The neuron operates at a speed of  15 MHz which allows the network to process 290,000 training examples  per second. Use of level sensitive scan logic provides the chip with 100%  fault coverage, permitting very reliable neural systems to be built.",
        "bibtex": "@inproceedings{NIPS1990_bac9162b,\n author = {Melton, Matt and Phan, Tan and Reeves, Doug and Van den Bout, Dave},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {VLSI Implementation of TInMANN},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/bac9162b47c56fc8a4d2a519803d51b3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1632540,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10478971476336602461&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5db823eeca",
        "title": "VLSI Implementations of Learning and Memory Systems: A Review",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html",
        "author": "Mark A. Holler",
        "abstract": "A large number of VLSI implementations of neural network models  have been  reported. The diversity of these implementations is  noteworthy. This paper attempts to put a group of representative  VLSI implementations in perspective by comparing and contrast(cid:173) ing them. Design trade-offs are discussed and some suggestions forthe  direction of future implementation efforts are made.",
        "bibtex": "@inproceedings{NIPS1990_2f2b2656,\n author = {Holler, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {VLSI Implementations of Learning and Memory Systems: A Review},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/2f2b265625d76a6704b08093c652fd79-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1769205,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16024267510724576610&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Intel Corporation",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Intel",
        "aff_unique_dep": "Intel Corporation",
        "aff_unique_url": "https://www.intel.com",
        "aff_unique_abbr": "Intel",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "872531edc8",
        "title": "e-Entropy and the Complexity of Feedforward Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1990/hash/39461a19e9eddfb385ea76b26521ea48-Abstract.html",
        "author": "Robert C. Williamson",
        "abstract": "We develop a. new feedforward neuralnet.work represent.ation of Lipschitz  functions from [0, p]n into [0,1] ba'3ed on the level sets of the function. We  show that",
        "bibtex": "@inproceedings{NIPS1990_39461a19,\n author = {Williamson, Robert C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {R.P. Lippmann and J. Moody and D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {e-Entropy and the Complexity of Feedforward Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf},\n volume = {3},\n year = {1990}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1990/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1990/file/39461a19e9eddfb385ea76b26521ea48-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1279624,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13713076740173544283&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    }
]