[
    {
        "id": "761096b58a",
        "title": "(RF)^2 -- Random Forest Random Field",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html",
        "author": "Nadia Payet; Sinisa Todorovic",
        "abstract": "We combine random forest (RF) and conditional random field (CRF) into a new computational framework, called random forest random field (RF)^2. Inference of (RF)^2 uses the Swendsen-Wang cut algorithm, characterized by Metropolis-Hastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a non-parametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)^2. (RF)^2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)^2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.",
        "bibtex": "@inproceedings{NIPS2010_289dff07,\n author = {Payet, Nadia and Todorovic, Sinisa},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {(RF)\\^{}2 -- Random Forest Random Field},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/289dff07669d7a23de0ef88d2f7129e7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1437862,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "School of Electrical Engineering and Computer Science; School of Electrical Engineering and Computer Science",
        "aff_domain": "onid.orst.edu;eecs.oregonstate.edu",
        "email": "onid.orst.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Electrical Engineering and Computer Science",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8e136f133d",
        "title": "A Bayesian Approach to Concept Drift",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8edd72158ccd2a879f79cb2538568fdc-Abstract.html",
        "author": "Stephen Bach; Mark Maloof",
        "abstract": "To cope with concept drift, we placed a probability distribution over the location of the most-recent drift point. We used Bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability. We compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection. In our experiments, our approach generally yielded improved accuracy and/or speed over these other methods.",
        "bibtex": "@inproceedings{NIPS2010_8edd7215,\n author = {Bach, Stephen and Maloof, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian Approach to Concept Drift},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8edd72158ccd2a879f79cb2538568fdc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 259315,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4283680352705333836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Georgetown University; Department of Computer Science, Georgetown University",
        "aff_domain": "cs.georgetown.edu;cs.georgetown.edu",
        "email": "cs.georgetown.edu;cs.georgetown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgetown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.georgetown.edu",
        "aff_unique_abbr": "GU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2a1658d6eb",
        "title": "A Bayesian Framework for Figure-Ground Interpretation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html",
        "author": "Vicky Froyen; Jacob Feldman; Manish Singh",
        "abstract": "Figure/ground assignment, in which the visual image is divided into nearer (figural) and farther (ground) surfaces, is an essential step in visual processing, but its underlying computational mechanisms are poorly understood. Figural assignment (often referred to as border ownership) can vary along a contour, suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership. In this paper we model figure/ground estimation in a Bayesian belief network, attempting to capture the propagation of border ownership across the image as local cues (contour curvature and T-junctions) interact with more global cues to yield a figure/ground assignment. Our network includes as a nonlocal factor skeletal (medial axis) structure, under the hypothesis that medial structure ``draws'' border ownership so that borders are owned by their interiors. We also briefly present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue (a T-junction). Both the human subjects and the network show similar patterns of performance, converging rapidly to a similar pattern of spatial variation in border ownership along contours.",
        "bibtex": "@inproceedings{NIPS2010_7b13b220,\n author = {Froyen, Vicky and Feldman, Jacob and Singh, Manish},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian Framework for Figure-Ground Interpretation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7b13b2203029ed80337f27127a9f1d28-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1176806,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7206635886671357563&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Center for Cognitive Science, Rutgers University, Piscataway, NJ 08854 + Laboratory of Experimental Psychology, University of Leuven (K.U. Leuven), Belgium; Center for Cognitive Science, Rutgers University, Piscataway, NJ 08854; Center for Cognitive Science, Rutgers University, Piscataway, NJ 08854",
        "aff_domain": "eden.rutgers.edu;ruccs.rutgers.edu;ruccs.rutgers.edu",
        "email": "eden.rutgers.edu;ruccs.rutgers.edu;ruccs.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Rutgers University;University of Leuven",
        "aff_unique_dep": "Center for Cognitive Science;Laboratory of Experimental Psychology",
        "aff_unique_url": "https://www.rutgers.edu;https://www.kuleuven.be",
        "aff_unique_abbr": "Rutgers;K.U. Leuven",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Piscataway;",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "United States;Belgium"
    },
    {
        "id": "8571b25e30",
        "title": "A Computational Decision Theory for Interactive Assistants",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/07871915a8107172b3b5dc15a6574ad3-Abstract.html",
        "author": "Alan Fern; Prasad Tadepalli",
        "abstract": "We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We first introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in finite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant's action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution.",
        "bibtex": "@inproceedings{NIPS2010_07871915,\n author = {Fern, Alan and Tadepalli, Prasad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Computational Decision Theory for Interactive Assistants},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/07871915a8107172b3b5dc15a6574ad3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 271098,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=318373694129217537&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of EECS, Oregon State University; School of EECS, Oregon State University",
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://eecs.oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2e0e0724ce",
        "title": "A Dirty Model for Multi-task Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/00e26af6ac3b1c1c49d7c3d79c60d000-Abstract.html",
        "author": "Ali Jalali; Sujay Sanghavi; Chao Ruan; Pradeep K. Ravikumar",
        "abstract": "We consider the multiple linear regression problem, in a setting where some of the set of relevant features could be shared across the tasks. A lot of recent research has studied the use of $\\ell_1/\\ell_q$ norm block-regularizations with $q > 1$  for such (possibly) block-structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However, these papers also caution that the performance of such block-regularized methods are very dependent on the {\\em extent} to which the features are shared across tasks. Indeed they show~\\citep{NWJoint} that if the extent of overlap is less than a threshold, or even if parameter {\\em values} in the shared features are highly uneven, then block $\\ell_1/\\ell_q$ regularization could actually perform {\\em worse} than simple separate elementwise $\\ell_1$ regularization. We are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well.  Here, we ask the question: can we leverage support and parameter overlap when it exists, but not pay a penalty when it does not? Indeed, this falls under a more general question of whether we can model such \\emph{dirty data} which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on). Here, we take a first step, focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we decompose  the parameters into two components and {\\em regularize these differently.} We show both theoretically and empirically, our method strictly and noticeably outperforms both $\\ell_1$ and $\\ell_1/\\ell_q$ methods, over the entire range of possible overlaps. We also provide theoretical guarantees that the method performs well under high-dimensional scaling.",
        "bibtex": "@inproceedings{NIPS2010_00e26af6,\n author = {Jalali, Ali and Sanghavi, Sujay and Ruan, Chao and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Dirty Model for Multi-task Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 166047,
        "gs_citation": 497,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3298386876729995680&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "University of Texas at Austin; University of Texas at Asutin; University of Texas at Austin; University of Texas at Austin",
        "aff_domain": "mail.utexas.edu;cs.utexas.edu;mail.utexas.edu;cs.utexas.edu",
        "email": "mail.utexas.edu;cs.utexas.edu;mail.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fbe790ffda",
        "title": "A Discriminative Latent Model of Image Region and Object Tag Correspondence",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e2ef524fbf3d9fe611d5a8e90fefdc9c-Abstract.html",
        "author": "Yang Wang; Greg Mori",
        "abstract": "We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth region-to-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.",
        "bibtex": "@inproceedings{NIPS2010_e2ef524f,\n author = {Wang, Yang and Mori, Greg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Discriminative Latent Model of Image Region and Object Tag Correspondence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2834735,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2262577140204730485&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign; School of Computing Science, Simon Fraser University",
        "aff_domain": "uiuc.edu;cs.sfu.ca",
        "email": "uiuc.edu;cs.sfu.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Simon Fraser University",
        "aff_unique_dep": "Department of Computer Science;School of Computing Science",
        "aff_unique_url": "https://illinois.edu;https://www.sfu.ca",
        "aff_unique_abbr": "UIUC;SFU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Urbana-Champaign;Burnaby",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "e0de7f8c95",
        "title": "A Family of Penalty Functions for Structured Sparsity",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/884d247c6f65a96a7da4d1105d584ddd-Abstract.html",
        "author": "Jean Morales; Charles A. Micchelli; Massimiliano Pontil",
        "abstract": "We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients. This family subsumes the $\\ell_1$ norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.",
        "bibtex": "@inproceedings{NIPS2010_884d247c,\n author = {Morales, Jean and Micchelli, Charles and Pontil, Massimiliano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Family of Penalty Functions for Structured Sparsity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/884d247c6f65a96a7da4d1105d584ddd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 198767,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4240396662055019275&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Mathematics, City University of Hong Kong + Dept. of Mathematics and Statistics, State University of New York, Albany, USA; Department of Computer Science, University College London, England, UK; Department of Computer Science, University College London, England, UK",
        "aff_domain": "hotmail.com;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "email": "hotmail.com;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;2",
        "aff_unique_norm": "City University of Hong Kong;State University of New York;University College London",
        "aff_unique_dep": "Department of Mathematics;Department of Mathematics and Statistics;Department of Computer Science",
        "aff_unique_url": "https://www.cityu.edu.hk;https://www.albany.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "CityU;SUNY;UCL",
        "aff_campus_unique_index": "0+1;2;2",
        "aff_campus_unique": "Hong Kong SAR;Albany;London",
        "aff_country_unique_index": "0+1;2;2",
        "aff_country_unique": "China;United States;United Kingdom"
    },
    {
        "id": "8b13c4d96a",
        "title": "A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html",
        "author": "Yi-da Wu; Shi-jie Lin; Hsin Chen",
        "abstract": "The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuous-time paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the log-domain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time.",
        "bibtex": "@inproceedings{NIPS2010_7bcdf75a,\n author = {Wu, Yi-da and Lin, Shi-jie and Chen, Hsin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 893026,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6228836850323863414&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering, National Tsing Hua University; Department of Electrical Engineering, National Tsing Hua University; Department of Electrical Engineering, National Tsing Hua University",
        "aff_domain": "ee.nthu.edu.tw; ;ee.nthu.edu.tw",
        "email": "ee.nthu.edu.tw; ;ee.nthu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National Tsing Hua University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.nthu.edu.tw",
        "aff_unique_abbr": "NTHU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "f1fb4c13b3",
        "title": "A New Probabilistic Model for Rank Aggregation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html",
        "author": "Tao Qin; Xiubo Geng; Tie-yan Liu",
        "abstract": "This paper is concerned with rank aggregation, which aims to combine multiple input rankings to get a better ranking. A popular approach to rank aggregation is based on probabilistic models on permutations, e.g., the Luce model and the Mallows model. However, these models have their limitations in either poor expressiveness or high computational complexity. To avoid these limitations, in this paper, we propose a new model, which is defined with a coset-permutation distance, and models the generation of a permutation as a stagewise process. We refer to the new model as coset-permutation distance based stagewise (CPS) model. The CPS model has rich expressiveness and can therefore be used in versatile applications, because many different permutation distances can be used to induce the coset-permutation distance. The complexity of the CPS model is low because of the stagewise decomposition of the permutation probability and the efficient computation of most coset-permutation distances. We apply the CPS model to supervised rank aggregation, derive the learning and inference algorithms, and empirically study their effectiveness and efficiency. Experiments on public datasets show that the derived algorithms based on the CPS model can achieve state-of-the-art ranking accuracy, and are much more efficient than previous algorithms.",
        "bibtex": "@inproceedings{NIPS2010_8dd48d6a,\n author = {Qin, Tao and Geng, Xiubo and Liu, Tie-yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A New Probabilistic Model for Rank Aggregation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8dd48d6a2e2cad213179a3992c0be53c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 111829,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11873528110140634558&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Research Asia; Chinese Academy of Sciences; Microsoft Research Asia",
        "aff_domain": "microsoft.com;gmail.com;microsoft.com",
        "email": "microsoft.com;gmail.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Microsoft;Chinese Academy of Sciences",
        "aff_unique_dep": "Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia;https://www.cas.cn",
        "aff_unique_abbr": "MSR Asia;CAS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Asia;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "552039aed1",
        "title": "A Novel Kernel for Learning a Neuron Model from Spike Train Data",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c410003ef13d451727aeff9082c29a5c-Abstract.html",
        "author": "Nicholas Fisher; Arunava Banerjee",
        "abstract": "From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classification based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach.",
        "bibtex": "@inproceedings{NIPS2010_c410003e,\n author = {Fisher, Nicholas and Banerjee, Arunava},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Novel Kernel for Learning a Neuron Model from Spike Train Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c410003ef13d451727aeff9082c29a5c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c410003ef13d451727aeff9082c29a5c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c410003ef13d451727aeff9082c29a5c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 541168,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10686527452400509324&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer and Information Science and Engineering, University of Florida; Department of Computer and Information Science and Engineering, University of Florida",
        "aff_domain": "cise.ufl.edu;cise.ufl.edu",
        "email": "cise.ufl.edu;cise.ufl.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "Department of Computer and Information Science and Engineering",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c4a36cc3e6",
        "title": "A POMDP Extension with Belief-dependent Rewards",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html",
        "author": "Mauricio Araya; Olivier Buffet; Vincent Thomas; Fran\u00e7cois Charpillet",
        "abstract": "Partially Observable Markov Decision Processes (POMDPs) model sequential decision-making problems under uncertainty and partial observability. Unfortunately, some problems cannot be modeled with state-dependent reward functions, e.g., problems whose objective explicitly implies reducing the uncertainty on the state. To that end, we introduce rho-POMDPs, an extension of POMDPs where the reward function rho depends on the belief state. We show that, under the common assumption that rho is convex, the value function is also convex, what makes it possible to (1) approximate rho arbitrarily well with a piecewise linear and convex (PWLC) function, and (2) use state-of-the-art exact or approximate solving algorithms with limited changes.",
        "bibtex": "@inproceedings{NIPS2010_68053af2,\n author = {Araya, Mauricio and Buffet, Olivier and Thomas, Vincent and Charpillet, Fran\\c{c}cois},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A POMDP Extension with Belief-dependent Rewards},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272070,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13580815997311819999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Nancy Universit\u00e9 / INRIA; Nancy Universit\u00e9 / INRIA; Nancy Universit\u00e9 / INRIA; Nancy Universit\u00e9 / INRIA",
        "aff_domain": "loria.fr;loria.fr;loria.fr;loria.fr",
        "email": "loria.fr;loria.fr;loria.fr;loria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nancy Universit\u00e9",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.univ-lorraine.fr",
        "aff_unique_abbr": "UL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "57bddb61a5",
        "title": "A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8c6744c9d42ec2cb9e8885b54ff744d0-Abstract.html",
        "author": "Sofia Mosci; Silvia Villa; Alessandro Verri; Lorenzo Rosasco",
        "abstract": "We deal with the problem of variable selection when  variables must be selected group-wise, with possibly overlapping groups defined a priori. In particular we propose a new optimization procedure  for solving the regularized algorithm presented in Jacob et al. 09, where the group lasso  penalty is generalized to overlapping groups of variables. While in Jacob et al. 09 the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and constrained Newton method in a reduced dual space, corresponding to the active groups.  This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing  to reduce the  dimensionality of the data.  The computational advantages of our scheme with respect to state-of-the-art algorithms  using data duplication are shown empirically with numerical simulations.",
        "bibtex": "@inproceedings{NIPS2010_8c6744c9,\n author = {Mosci, Sofia and Villa, Silvia and Verri, Alessandro and Rosasco, Lorenzo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 179169,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13682802864007760437&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "DISI- Universit `a di Genova; DISI- Universit `a di Genova; DISI- Universit `a di Genova; IIT - MIT",
        "aff_domain": "disi.unige.it;dima.unige.it;disi.unige.it;MIT.EDU",
        "email": "disi.unige.it;dima.unige.it;disi.unige.it;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Universit\u00e0 di Genova;Massachusetts Institute of Technology",
        "aff_unique_dep": "DISI;",
        "aff_unique_url": "https://www.unige.it;https://www.mit.edu",
        "aff_unique_abbr": "UniGe;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Italy;United States"
    },
    {
        "id": "09488d08a5",
        "title": "A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/dc912a253d1e9ba40e2c597ed2376640-Abstract.html",
        "author": "Tamir Hazan; Raquel Urtasun",
        "abstract": "In this paper we propose an approximated learning framework for large scale graphical models and derive message passing algorithms for learning their parameters efficiently.  We first relate CRFs and structured SVMs  and show that in the CRF's primal a variant of the log-partition function, known as soft-max, smoothly approximates the hinge loss function of structured SVMs.  We then propose an intuitive approximation for structured prediction problems using Fenchel duality based on a local entropy approximation that computes the exact gradients of the approximated problem and is guaranteed to converge. Unlike existing approaches, this allow us to learn graphical models with cycles and very large number of parameters efficiently. We demonstrate the effectiveness of our approach  in an image denoising task. This task was previously solved by sharing parameters across cliques. In contrast, our algorithm is able to efficiently learn large number of parameters resulting in orders of magnitude better prediction.",
        "bibtex": "@inproceedings{NIPS2010_dc912a25,\n author = {Hazan, Tamir and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/dc912a253d1e9ba40e2c597ed2376640-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1466872,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15763867844922755116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "TTI Chicago; TTI Chicago",
        "aff_domain": "ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9366787ec3",
        "title": "A Reduction from Apprenticeship Learning to Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/5c572eca050594c7bc3c36e7e8ab9550-Abstract.html",
        "author": "Umar Syed; Robert E. Schapire",
        "abstract": "We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classification algorithm to learn to imitate the expert's behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classifier has error rate $\\eps$, the difference between the value of the apprentice's policy and the expert's policy is $O(\\sqrt{\\eps})$. Further, we prove that this difference is only $O(\\eps)$ when the expert's policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difficult to obtain.",
        "bibtex": "@inproceedings{NIPS2010_5c572eca,\n author = {Syed, Umar and Schapire, Robert E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Reduction from Apprenticeship Learning to Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/5c572eca050594c7bc3c36e7e8ab9550-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 110797,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1343715416198092997&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer and Information Science, University of Pennsylvania+Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "cis.upenn.edu;cs.princeton.edu",
        "email": "cis.upenn.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "University of Pennsylvania;Princeton University",
        "aff_unique_dep": "Department of Computer and Information Science;",
        "aff_unique_url": "https://www.upenn.edu;https://www.princeton.edu",
        "aff_unique_abbr": "UPenn;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4b79add2f4",
        "title": "A Theory of Multiclass Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/46fc943ecd56441056a560ba37d0b9e8-Abstract.html",
        "author": "Indraneel Mukherjee; Robert E. Schapire",
        "abstract": "Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the \"correct\" requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.",
        "bibtex": "@inproceedings{NIPS2010_46fc943e,\n author = {Mukherjee, Indraneel and Schapire, Robert E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Theory of Multiclass Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 922056,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5098515199787987683&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Princeton University, Department of Computer Science, Princeton, NJ 08540; Princeton University, Department of Computer Science, Princeton, NJ 08540",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c0ffc7ac37",
        "title": "A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/25b2822c2f5a3230abfadd476e8b04c9-Abstract.html",
        "author": "Sebastian Millner; Andreas Gr\u00fcbl; Karlheinz Meier; Johannes Schemmel; Marc-olivier Schwartz",
        "abstract": "We describe an accelerated hardware neuron being capable of emulating the adap-tive exponential integrate-and-fire neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulation and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientific experiments, the focus lays on parameterizability and reproduction of the analytical model.",
        "bibtex": "@inproceedings{NIPS2010_25b2822c,\n author = {Millner, Sebastian and Gr\\\"{u}bl, Andreas and Meier, Karlheinz and Schemmel, Johannes and Schwartz, Marc-olivier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/25b2822c2f5a3230abfadd476e8b04c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 852765,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3599907184858565718&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Kirchhoff-Institut f \u00a8ur Physik, Ruprecht-Karls-Universit \u00a8at Heidelberg; Kirchhoff-Institut f \u00a8ur Physik, Ruprecht-Karls-Universit \u00a8at Heidelberg; Kirchhoff-Institut f \u00a8ur Physik, Ruprecht-Karls-Universit \u00a8at Heidelberg; Kirchhoff-Institut f \u00a8ur Physik, Ruprecht-Karls-Universit \u00a8at Heidelberg; Kirchhoff-Institut f \u00a8ur Physik, Ruprecht-Karls-Universit \u00a8at Heidelberg",
        "aff_domain": "kip.uni-heidelberg.de; ; ; ; ",
        "email": "kip.uni-heidelberg.de; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Ruprecht-Karls-Universit\u00e4t Heidelberg",
        "aff_unique_dep": "Kirchhoff-Institut f\u00fcr Physik",
        "aff_unique_url": "https://www.uni-heidelberg.de",
        "aff_unique_abbr": "Uni Heidelberg",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Heidelberg",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "a8f3f3536b",
        "title": "A biologically plausible network for the computation of orientation dominance",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8c235f89a8143a28a1d6067e959dd858-Abstract.html",
        "author": "Kritika Muralidharan; Nuno Vasconcelos",
        "abstract": "The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation $\\theta$, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justification for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classification tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems.",
        "bibtex": "@inproceedings{NIPS2010_8c235f89,\n author = {Muralidharan, Kritika and Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A biologically plausible network for the computation of orientation dominance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8c235f89a8143a28a1d6067e959dd858-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 864626,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2071138085668581015&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Statistical Visual Computing Laboratory, University of California San Diego, La Jolla, CA 92039; Statistical Visual Computing Laboratory, University of California San Diego, La Jolla, CA 92039",
        "aff_domain": "ucsd.edu;ece.ucsd.edu",
        "email": "ucsd.edu;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Statistical Visual Computing Laboratory",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "La Jolla",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fdef4083d3",
        "title": "A novel family of non-parametric cumulative based divergences for point processes",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/912d2b1c7b2826caf99687388d2e8f7c-Abstract.html",
        "author": "Sohan Seth; Park Il; Austin Brockmeier; Mulugeta Semework; John Choi; Joseph Francis; Jose Principe",
        "abstract": "Hypothesis testing on point processes has several applications such as model fitting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean firing rate and time varying rate function. However, these statistics do not fully describe a point process and thus the tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. We extend the traditional Kolmogorov--Smirnov and Cramer--von-Mises tests for point process via stratification. The proposed divergence measures compare the underlying probability structure and, thus, is zero if and only if the point processes are the same. This leads to a more robust test of hypothesis. We prove consistency and show that these measures can be efficiently estimated from data. We demonstrate an application of using the proposed divergence as a cost function to find optimally matched spike trains.",
        "bibtex": "@inproceedings{NIPS2010_912d2b1c,\n author = {Seth, Sohan and Il, Park and Brockmeier, Austin and Semework, Mulugeta and Choi, John and Francis, Joseph and Principe, Jose},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A novel family of non-parametric cumulative based divergences for point processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 159571,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6693685616925169760&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "725b7c4c90",
        "title": "A rational decision making framework for inhibitory control",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/692f93be8c7a41525c0baf2076aecfb4-Abstract.html",
        "author": "Pradeep Shenoy; Angela J. Yu; Rajesh P. Rao",
        "abstract": "Intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands. The requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology. We formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task. Using Bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands. Our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems.",
        "bibtex": "@inproceedings{NIPS2010_692f93be,\n author = {Shenoy, Pradeep and Yu, Angela J and Rao, Rajesh PN},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A rational decision making framework for inhibitory control},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/692f93be8c7a41525c0baf2076aecfb4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1789280,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9917170079347735484&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Cognitive Science, University of California, San Diego; Department of Computer Science, University of Washington; Department of Cognitive Science, University of California, San Diego",
        "aff_domain": "ucsd.edu;cs.washington.edu;ucsd.edu",
        "email": "ucsd.edu;cs.washington.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;University of Washington",
        "aff_unique_dep": "Department of Cognitive Science;Department of Computer Science",
        "aff_unique_url": "https://ucsd.edu;https://www.washington.edu",
        "aff_unique_abbr": "UCSD;UW",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "San Diego;Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a5f4192ec3",
        "title": "A unified model of short-range and long-range motion perception",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a50abba8132a77191791390c3eb19fe7-Abstract.html",
        "author": "Shuang Wu; Xuming He; Hongjing Lu; Alan L. Yuille",
        "abstract": "The human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes. Previous work has assumed that two different mechanisms are involved in processing these two types of motion. In this paper, we propose a hierarchical model as a unified framework for modeling both short-range and long-range motion perception. Our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion field at multiple scales. We tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research. We demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.",
        "bibtex": "@inproceedings{NIPS2010_a50abba8,\n author = {Wu, Shuang and He, Xuming and Lu, Hongjing and Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A unified model of short-range and long-range motion perception},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a50abba8132a77191791390c3eb19fe7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 572643,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16593504947942085522&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Statistics, UCLA; Department of Statistics, UCLA; Department of Psychology, UCLA; Department of Statistics, Psychology, and Computer Science, UCLA",
        "aff_domain": "stat.ucla.edu;stat.ucla.edu;ucla.edu;stat.ucla.edu",
        "email": "stat.ucla.edu;stat.ucla.edu;ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "763d93eae7",
        "title": "Accounting for network effects in neuronal responses using L1 regularized point process models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c7e1249ffc03eb9ded908c236bd1996d-Abstract.html",
        "author": "Ryan Kelly; Matthew Smith; Robert Kass; Tai S. Lee",
        "abstract": "Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive field or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable influence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the firing rate of many individual units recorded simultaneously from V1 with a 96-electrode Utah\" array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron's response, in addition to the neuron's receptive field transfer function. We also found that the same spikes could be accounted for with the local field potentials, a surrogate measure of global network states. This work shows that accounting for network fluctuations can improve estimates of single trial firing rate and stimulus-response transfer functions.\"",
        "bibtex": "@inproceedings{NIPS2010_c7e1249f,\n author = {Kelly, Ryan and Smith, Matthew and Kass, Robert and Lee, Tai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accounting for network effects in neuronal responses using L1 regularized point process models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c7e1249ffc03eb9ded908c236bd1996d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c7e1249ffc03eb9ded908c236bd1996d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c7e1249ffc03eb9ded908c236bd1996d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 345530,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1740303777413582825&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Computer Science Department+Center for the Neural Basis of Cognition, Carnegie Mellon University; University of Pittsburgh+Center for the Neural Basis of Cognition; Department of Statistics+Center for the Neural Basis of Cognition+Machine Learning Department, Carnegie Mellon University; Computer Science Department+Center for the Neural Basis of Cognition, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cnbc.cmu.edu;stat.cmu.edu;cnbc.cmu.edu",
        "email": "cs.cmu.edu;cnbc.cmu.edu;stat.cmu.edu;cnbc.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+3;4+3+1;0+1",
        "aff_unique_norm": "Computer Science Department;Carnegie Mellon University;University of Pittsburgh;Center for the Neural Basis of Cognition;University Affiliation Not Specified",
        "aff_unique_dep": "Computer Science;Center for the Neural Basis of Cognition;;;Department of Statistics",
        "aff_unique_url": ";https://www.cmu.edu;https://www.pitt.edu;;",
        "aff_unique_abbr": ";CMU;Pitt;;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1+1;1+1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "53866a3daf",
        "title": "Active Estimation of F-Measures",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d7a728a67d909e714c0774e22cb806f2-Abstract.html",
        "author": "Christoph Sawade; Niels Landwehr; Tobias Scheffer",
        "abstract": "We address the problem of estimating the F-measure of a given model as accurately as possible on a fixed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reflect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of F-measures are more accurate than estimates based on instances sampled from the test distribution.",
        "bibtex": "@inproceedings{NIPS2010_d7a728a6,\n author = {Sawade, Christoph and Landwehr, Niels and Scheffer, Tobias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Estimation of F-Measures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 289180,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7038991425625943381&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Potsdam, Department of Computer Science; University of Potsdam, Department of Computer Science; University of Potsdam, Department of Computer Science",
        "aff_domain": "cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de",
        "email": "cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Potsdam",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uni-potsdam.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "6f4df1b7bf",
        "title": "Active Instance Sampling via Matrix Partition",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/f29c21d4897f78948b91f03172341b7b-Abstract.html",
        "author": "Yuhong Guo",
        "abstract": "Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural form of mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although the matrix partition is an NP-hard combinatorial optimization problem, we show a good local solution can be obtained by exploiting an effective local optimization technique on the relaxed continuous optimization problem. The proposed active learning approach is independent of employed classification models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods.",
        "bibtex": "@inproceedings{NIPS2010_f29c21d4,\n author = {Guo, Yuhong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Instance Sampling via Matrix Partition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/f29c21d4897f78948b91f03172341b7b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 112519,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10724654080193193790&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "49667503fe",
        "title": "Active Learning Applied to Patient-Adaptive Heartbeat Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d93ed5b6db83be78efb0d05ae420158e-Abstract.html",
        "author": "Jenna Wiens; John V. Guttag",
        "abstract": "While clinicians can accurately identify different types of heartbeats in electrocardiograms (ECGs) from different patients, researchers have had limited success in applying supervised machine learning to the same task. The problem is made challenging by the variety of tasks, inter- and intra-patient differences, an often severe class imbalance, and the high cost of getting cardiologists to label data for individual patients. We address these difficulties using active learning to perform patient-adaptive and task-adaptive heartbeat classification. When tested on a benchmark database of cardiologist annotated ECG recordings, our method had considerably better performance than other recently proposed methods on the two primary classification tasks recommended by the Association for the Advancement of Medical Instrumentation. Additionally, our method required over 90% less patient-specific training data than the methods to which we compared it.",
        "bibtex": "@inproceedings{NIPS2010_d93ed5b6,\n author = {Wiens, Jenna and Guttag, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Learning Applied to Patient-Adaptive Heartbeat Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 330348,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17929621393038399478&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "CSAIL, MIT; CSAIL, MIT",
        "aff_domain": "csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "12062e76d4",
        "title": "Active Learning by Querying Informative and Representative Examples",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/5487315b1286f907165907aa8fc96619-Abstract.html",
        "author": "Sheng-jun Huang; Rong Jin; Zhi-Hua Zhou",
        "abstract": "Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criterions for query selection, they are usually ad hoc in finding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed QUIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed QUIRE approach outperforms several state-of -the-art active learning approaches.",
        "bibtex": "@inproceedings{NIPS2010_5487315b,\n author = {Huang, Sheng-jun and Jin, Rong and Zhou, Zhi-Hua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Learning by Querying Informative and Representative Examples},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5487315b1286f907165907aa8fc96619-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/5487315b1286f907165907aa8fc96619-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/5487315b1286f907165907aa8fc96619-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/5487315b1286f907165907aa8fc96619-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 265819,
        "gs_citation": 888,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3509220004995198814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China",
        "aff_domain": "lamda.nju.edu.cn;cse.msu.edu;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;cse.msu.edu;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Nanjing University;Michigan State University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;Department of Computer Science and Engineering",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.msu.edu",
        "aff_unique_abbr": "Nanjing U;MSU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Nanjing;East Lansing",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "c195dd21b9",
        "title": "Adaptive Multi-Task Lasso: with Application to eQTL Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/3cf166c6b73f030b4f67eeaeba301103-Abstract.html",
        "author": "Seunghak Lee; Jun Zhu; Eric P. Xing",
        "abstract": "To understand the relationship between genomic variations among population and complex diseases, it is essential to detect eQTLs which are associated with phenotypic effects. However, detecting eQTLs remains a challenge due to complex underlying mechanisms and the very large number of genetic loci involved compared to the number of samples. Thus, to address the problem, it is desirable to take advantage of the structure of the data and prior information about genomic locations such as conservation scores and transcription factor binding sites.  In this paper, we propose a novel regularized regression approach for detecting eQTLs which takes into account related traits simultaneously while incorporating many regulatory features. We first present a Bayesian network for a multi-task learning problem that includes priors on SNPs, making it possible to estimate the significance of each covariate adaptively. Then we find the maximum a posteriori (MAP) estimation of regression coefficients and estimate weights of covariates jointly. This optimization procedure is efficient since it can be achieved by using convex optimization and a coordinate descent procedure iteratively. Experimental results on simulated and real yeast datasets confirm that our model outperforms previous methods for finding eQTLs.",
        "bibtex": "@inproceedings{NIPS2010_3cf166c6,\n author = {Lee, Seunghak and Zhu, Jun and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Multi-Task Lasso: with Application to eQTL Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3cf166c6b73f030b4f67eeaeba301103-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/3cf166c6b73f030b4f67eeaeba301103-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/3cf166c6b73f030b4f67eeaeba301103-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 178253,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15945649915937799999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a512d7e2c7",
        "title": "Agnostic Active Learning Without Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html",
        "author": "Alina Beygelzimer; Daniel J. Hsu; John Langford; Tong Zhang",
        "abstract": "We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.",
        "bibtex": "@inproceedings{NIPS2010_00411460,\n author = {Beygelzimer, Alina and Hsu, Daniel J and Langford, John and Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Agnostic Active Learning Without Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 160391,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3290101394640025352&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "IBM Research, Hawthorne, NY; Rutgers University & University of Pennsylvania; Yahoo! Research, New York, NY; Rutgers University, Piscataway, NJ",
        "aff_domain": "us.ibm.com;rci.rutgers.edu;yahoo-inc.com;rci.rutgers.edu",
        "email": "us.ibm.com;rci.rutgers.edu;yahoo-inc.com;rci.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "IBM;Rutgers University;Yahoo! Research",
        "aff_unique_dep": "IBM Research;;",
        "aff_unique_url": "https://www.ibm.com/research;https://www.rutgers.edu;https://research.yahoo.com",
        "aff_unique_abbr": "IBM;Rutgers;Yahoo! Res",
        "aff_campus_unique_index": "0;2;3",
        "aff_campus_unique": "Hawthorne;;New York;Piscataway",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a3f29d4b51",
        "title": "An Alternative to Low-level-Sychrony-Based Methods for Speech Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/68b1fbe7f16e4ae3024973f12f3cb313-Abstract.html",
        "author": "Javier R. Movellan; Paul L. Ruvolo",
        "abstract": "Determining whether someone is talking has applications in many areas such as speech recognition, speaker diarization, social robotics, facial expression recognition, and human computer interaction.  One popular approach to this problem is audio-visual synchrony detection. A candidate speaker is deemed to be talking if the visual signal around that speaker correlates with the auditory signal.  Here we show that with the proper visual features (in this case movements of various facial muscle groups), a very accurate detector of speech can be created that does not use the audio signal at all.  Further we show that this person independent visual-only detector can be used to train very accurate audio-based person dependent voice models.  The voice model has the advantage of being able to identify when a particular person is speaking even when they are not visible to the camera (e.g. in the case of a mobile robot).  Moreover, we show that a simple sensory fusion scheme between the auditory and visual models improves performance on the task of talking detection.  The work here provides dramatic evidence about the efficacy of two very different approaches to multimodal speech detection on a challenging database.",
        "bibtex": "@inproceedings{NIPS2010_68b1fbe7,\n author = {Movellan, Javier and Ruvolo, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Alternative to Low-level-Sychrony-Based Methods for Speech Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68b1fbe7f16e4ae3024973f12f3cb313-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/68b1fbe7f16e4ae3024973f12f3cb313-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/68b1fbe7f16e4ae3024973f12f3cb313-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 964669,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10310310335010076160&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, San Diego Machine Perception Laboratory Atkinson Hall (CALIT2), 6100 9500 Gilman Dr., Mail Code 0440 La Jolla, CA 92093-0440; University of California, San Diego Machine Perception Laboratory Atkinson Hall (CALIT2), 6100 9500 Gilman Dr., Mail Code 0440 La Jolla, CA 92093-0440",
        "aff_domain": "mplab.ucsd.edu;mplab.ucsd.edu",
        "email": "mplab.ucsd.edu;mplab.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Machine Perception Laboratory",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "La Jolla",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f1c14180d1",
        "title": "An Approximate Inference Approach to Temporal Optimization in Optimal Control",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/b5dc4e5d9b495d0196f61d45b26ef33e-Abstract.html",
        "author": "Konrad Rawlik; Marc Toussaint; Sethu Vijayakumar",
        "abstract": "Algorithms based on iterative local approximations present a practical approach to optimal control in robotic systems. However, they generally require the temporal parameters (for e.g. the movement duration or the time point of reaching an intermediate goal) to be specified \\textit{a priori}. Here, we present a methodology that is capable of jointly optimising the temporal parameters in addition to the control command profiles. The presented approach is based on a Bayesian canonical time formulation of the optimal control problem, with the temporal mapping from canonical to real time parametrised by an additional control variable. An approximate EM algorithm is derived that efficiently optimises both the movement duration and control commands offering, for the first time, a practical approach to tackling generic via point problems in a systematic way under the optimal control framework. The proposed approach is evaluated on simulations of a redundant robotic plant.",
        "bibtex": "@inproceedings{NIPS2010_b5dc4e5d,\n author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Approximate Inference Approach to Temporal Optimization in Optimal Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 149231,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4858862958000845232&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "19025d8cd9",
        "title": "An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/fd06b8ea02fe5b1c2496fe1700e9d16c-Abstract.html",
        "author": "Matthias Hein; Thomas B\u00fchler",
        "abstract": "Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to finding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems.",
        "bibtex": "@inproceedings{NIPS2010_fd06b8ea,\n author = {Hein, Matthias and B\\\"{u}hler, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 332367,
        "gs_citation": 261,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7199852390464674573&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Saarland University, Saarbr\u00fccken, Germany; Saarland University, Saarbr\u00fccken, Germany",
        "aff_domain": "cs.uni-saarland.de;cs.uni-saarland.de",
        "email": "cs.uni-saarland.de;cs.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "UdS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Saarbr\u00fccken",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "187ccc805b",
        "title": "An analysis on negative curvature induced by singularity in multi-layer neural-network learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a9b7ba70783b617e9998dc4dd82eb3c5-Abstract.html",
        "author": "Eiji Mizutani; Stuart Dreyfus",
        "abstract": "In the neural-network parameter space,  an attractive field is likely to be induced by singularities. In such a singularity region, first-order gradient learning typically causes a long plateau with very little change  in the objective function value E (hence, a flat region). Therefore, it may be confused with ``attractive'' local minima. Our analysis shows that the Hessian matrix of E tends to be indefinite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers)  that allow us to confirm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau.  Even for those small problems, no efficient methods have been previously developed that avoided plateaus.",
        "bibtex": "@inproceedings{NIPS2010_a9b7ba70,\n author = {Mizutani, Eiji and Dreyfus, Stuart},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An analysis on negative curvature induced by singularity in multi-layer neural-network learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 387318,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4303311220244891001&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Industrial Management, Taiwan Univ. of Science & Technology; Industrial Engineering & Operations Research, University of California, Berkeley",
        "aff_domain": "mail.ntust.edu.tw;ieor.berkeley.edu",
        "email": "mail.ntust.edu.tw;ieor.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Taiwan University of Science and Technology;University of California, Berkeley",
        "aff_unique_dep": "Department of Industrial Management;Industrial Engineering & Operations Research",
        "aff_unique_url": "https://www.tust.edu.tw;https://www.berkeley.edu",
        "aff_unique_abbr": "TUST;UC Berkeley",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Taiwan;Berkeley",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "a8041c5508",
        "title": "Approximate Inference by Compilation to Arithmetic Circuits",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html",
        "author": "Daniel Lowd; Pedro Domingos",
        "abstract": "Arithmetic circuits (ACs) exploit context-specific independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the first ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we find that the methods based on sampling and learning work best: one such method (AC2-F) is faster and usually more accurate than loopy belief propagation, mean field, and Gibbs sampling; another (AC2-G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines.",
        "bibtex": "@inproceedings{NIPS2010_addfa9b7,\n author = {Lowd, Daniel and Domingos, Pedro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximate Inference by Compilation to Arithmetic Circuits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 302365,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4713092475730810662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer and Information Science, University of Oregon; Department of Computer Science and Engineering, University of Washington",
        "aff_domain": "cs.uoregon.edu;cs.washington.edu",
        "email": "cs.uoregon.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Oregon;University of Washington",
        "aff_unique_dep": "Department of Computer and Information Science;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.uoregon.edu;https://www.washington.edu",
        "aff_unique_abbr": "UO;UW",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "72c2df26da",
        "title": "Approximate inference in continuous time Gaussian-Jump processes",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d6723e7cd6735df68d1ce4c704c29a04-Abstract.html",
        "author": "Manfred Opper; Andreas Ruttor; Guido Sanguinetti",
        "abstract": "We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We first consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efficient mean field approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.",
        "bibtex": "@inproceedings{NIPS2010_d6723e7c,\n author = {Opper, Manfred and Ruttor, Andreas and Sanguinetti, Guido},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximate inference in continuous time Gaussian-Jump processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d6723e7cd6735df68d1ce4c704c29a04-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 316454,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4241084103126045252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Fakult\u00e4t Elektrotechnik und Informatik, Technische Universit\u00e4t Berlin, Berlin, Germany; Fakult\u00e4t Elektrotechnik und Informatik, Technische Universit\u00e4t Berlin, Berlin, Germany; School of Informatics, University of Edinburgh",
        "aff_domain": "cs.tu-berlin.de;tu-berlin.de;ed.ac.uk",
        "email": "cs.tu-berlin.de;tu-berlin.de;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;University of Edinburgh",
        "aff_unique_dep": "Fakult\u00e4t Elektrotechnik und Informatik;School of Informatics",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.ed.ac.uk",
        "aff_unique_abbr": "TU Berlin;Edinburgh",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Berlin;Edinburgh",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "aeeadb431b",
        "title": "Attractor Dynamics with Synaptic Depression",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Abstract.html",
        "author": "K. Wong; He Wang; Si Wu; Chi Fung",
        "abstract": "Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We find that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we find that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.",
        "bibtex": "@inproceedings{NIPS2010_7d04bbbe,\n author = {Wong, K. and Wang, He and Wu, Si and Fung, Chi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Attractor Dynamics with Synaptic Depression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 312974,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2899614538070221505&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Tsinghua University, Beijing, China; Institute of Neuroscience, Chinese Academy of Sciences, Shanghai, China",
        "aff_domain": "ust.hk;ust.hk;mails.tsinghua.edu.cn;ion.ac.cn",
        "email": "ust.hk;ust.hk;mails.tsinghua.edu.cn;ion.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Tsinghua University;Chinese Academy of Sciences",
        "aff_unique_dep": ";;Institute of Neuroscience",
        "aff_unique_url": "https://www.ust.hk;https://www.tsinghua.edu.cn;http://www.cas.cn",
        "aff_unique_abbr": "HKUST;THU;CAS",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Hong Kong;Beijing;Shanghai",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "007ab7cd16",
        "title": "Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/242c100dc94f871b6d7215b868a875f8-Abstract.html",
        "author": "Chris Barber; Joseph Bockhorst; Paul Roebber",
        "abstract": "Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efficient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables at geographically distributed sites. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). Because of this high cost, we introduce a simple approximate inference method for AR-HMMs, which we believe has applications to other sequential and temporal problem domains that involve continuous variables. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes significantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes.",
        "bibtex": "@inproceedings{NIPS2010_242c100d,\n author = {Barber, Chris and Bockhorst, Joseph and Roebber, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/242c100dc94f871b6d7215b868a875f8-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/242c100dc94f871b6d7215b868a875f8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/242c100dc94f871b6d7215b868a875f8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 456242,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1501596838577103776&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f27e00dad5",
        "title": "Avoiding False Positive in Multi-Instance Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/ccb0989662211f61edae2e26d58ea92f-Abstract.html",
        "author": "Yanjun Han; Qing Tao; Jue Wang",
        "abstract": "In multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive. Current research mainly focus on avoding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis, we define a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.",
        "bibtex": "@inproceedings{NIPS2010_ccb09896,\n author = {Han, Yanjun and Tao, Qing and Wang, Jue},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Avoiding False Positive in Multi-Instance Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/ccb0989662211f61edae2e26d58ea92f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1303189,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3692024618415486055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "06b5e9cc4e",
        "title": "Basis Construction from Power Series Expansions of Value Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html",
        "author": "Sridhar Mahadevan; Bo Liu",
        "abstract": "This paper explores links between basis construction methods in Markov decision processes and power series expansions of value functions. This perspective provides a useful framework to analyze properties of existing bases, as well as provides insight into constructing more effective bases. Krylov and Bellman error bases are based on the Neumann series expansion. These bases incur very large initial Bellman errors, and can converge rather slowly as the discount factor approaches unity. The Laurent series expansion, which relates discounted and average-reward formulations, provides both an explanation for this slow convergence as well as suggests a way to construct more efficient basis representations. The first two terms in the Laurent series represent the scaled average-reward and the average-adjusted sum of rewards, and subsequent terms expand the discounted value function using powers of a generalized inverse called the Drazin (or group inverse) of a singular matrix derived from the transition matrix. Experiments show that Drazin bases converge considerably more quickly than several other bases, particularly for large values of the discount factor. An incremental variant of Drazin bases called Bellman average-reward bases (BARBs) is described, which provides some of the same benefits at lower computational cost.",
        "bibtex": "@inproceedings{NIPS2010_c2aee861,\n author = {Mahadevan, Sridhar and Liu, Bo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Basis Construction from Power Series Expansions of Value Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c2aee86157b4a40b78132f1e71a9e6f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 548909,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4945378179314141010&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Massachusetts; Department of Computer Science, University of Massachusetts",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "11e99741cd",
        "title": "Batch Bayesian Optimization via Simulation Matching",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e702e51da2c0f5be4dd354bb3e295d37-Abstract.html",
        "author": "Javad Azimi; Alan Fern; Xiaoli Z. Fern",
        "abstract": "Bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate. Typically, these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation. There are a number of effective sequential policies for selecting the individual inputs. In many applications, however, it is desirable to perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once. In this paper, we propose a novel approach to batch Bayesian optimization, providing a policy for selecting batches of inputs with the goal of optimizing the function as efficiently as possible. The key idea is to exploit the availability of high-quality and efficient sequential policies, by using Monte-Carlo simulation to select input batches that closely match their expected behavior. To the best of our knowledge, this is the first batch selection policy for Bayesian optimization. Our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time.",
        "bibtex": "@inproceedings{NIPS2010_e702e51d,\n author = {Azimi, Javad and Fern, Alan and Fern, Xiaoli},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Batch Bayesian Optimization via Simulation Matching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 535739,
        "gs_citation": 169,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1282345766516152370&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of EECS, Oregon State University; School of EECS, Oregon State University; School of EECS, Oregon State University",
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://eecs.oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d657f2257d",
        "title": "Bayesian Action-Graph Games",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/847cc55b7032108eee6dd897f3bca8a5-Abstract.html",
        "author": "Albert X. Jiang; Kevin Leyton-brown",
        "abstract": "Games of incomplete information, or Bayesian games, are an important game-theoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-specific utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time.  Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve significantly on the state of the art.",
        "bibtex": "@inproceedings{NIPS2010_847cc55b,\n author = {Jiang, Albert and Leyton-brown, Kevin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Action-Graph Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 249007,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11720228465291204818&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "ad14036570",
        "title": "Beyond Actions: Discriminative Models for Contextual Group Activities",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2b44928ae11fb9384c4cf38708677c48-Abstract.html",
        "author": "Tian Lan; Yang Wang; Weilong Yang; Greg Mori",
        "abstract": "We propose a discriminative model for recognizing group activities. Our model jointly captures the group activity, the individual person actions, and the interactions among them. Two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. Different from most of the previous latent structured models which assume a predefined structure for the hidden layer, e.g. a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. Our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can significantly improve activity recognition performance.",
        "bibtex": "@inproceedings{NIPS2010_2b44928a,\n author = {Lan, Tian and Wang, Yang and Yang, Weilong and Mori, Greg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond Actions: Discriminative Models for Contextual Group Activities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2b44928ae11fb9384c4cf38708677c48-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2b44928ae11fb9384c4cf38708677c48-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2b44928ae11fb9384c4cf38708677c48-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3767011,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13199045366004261195&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "School of Computing Science, Simon Fraser University; Department of Computer Science, University of Illinois at Urbana-Champaign; School of Computing Science, Simon Fraser University; School of Computing Science, Simon Fraser University",
        "aff_domain": "sfu.ca;uiuc.edu;sfu.ca;cs.sfu.ca",
        "email": "sfu.ca;uiuc.edu;sfu.ca;cs.sfu.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Simon Fraser University;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "School of Computing Science;Department of Computer Science",
        "aff_unique_url": "https://www.sfu.ca;https://illinois.edu",
        "aff_unique_abbr": "SFU;UIUC",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Burnaby;Urbana-Champaign",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "0c71668048",
        "title": "Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html",
        "author": "Vikas Sindhwani; Aurelie C. Lozano",
        "abstract": "We consider multivariate regression problems involving high-dimensional predictor and response spaces. To efficiently address such problems, we propose a variable selection method, Multivariate Group Orthogonal Matching Pursuit, which extends the standard Orthogonal Matching Pursuit technique to account for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables, while also taking advantage of the correlation that may exist between the multiple outputs. We illustrate the utility of this framework for inferring causal relationships over a collection of high-dimensional time series variables. When applied to time-evolving social media content, our models yield a new family of causality-based influence measures that may be seen as an alternative to PageRank. Theoretical guarantees, extensive simulations and empirical studies confirm the generality and value of our framework.",
        "bibtex": "@inproceedings{NIPS2010_bcc0d400,\n author = {Sindhwani, Vikas and Lozano, Aurelie C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1389005,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12825442680061912684&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "IBM T.J. Watson Research Center, 1101 Kitchawan Road, Yorktown Heights NY 10598, USA; IBM T.J. Watson Research Center, 1101 Kitchawan Road, Yorktown Heights NY 10598, USA",
        "aff_domain": "us.ibm.com;us.ibm.com",
        "email": "us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM T.J. Watson Research Center",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM Watson",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Yorktown Heights",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8a5a005835",
        "title": "Boosting Classifier Cascades",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0ff39bbbf981ac0151d340c9aa40e63e-Abstract.html",
        "author": "Nuno Vasconcelos; Mohammad J. Saberian",
        "abstract": "The problem of optimal and automatic design of a detector cascade is considered. A novel mathematical model is introduced for a cascaded detector. This model is analytically tractable, leads to recursive computation, and accounts for both classification and complexity. A boosting algorithm, FCBoost, is proposed for fully automated cascade design. It exploits the new cascade model, minimizes a Lagrangian cost that accounts for both classification risk and complexity. It searches the space of cascade configurations to automatically determine the optimal number of stages and their predictors, and is compatible with bootstrapping of negative examples and cost sensitive learning. Experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems.",
        "bibtex": "@inproceedings{NIPS2010_0ff39bbb,\n author = {Vasconcelos, Nuno and Saberian, Mohammad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Boosting Classifier Cascades},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0ff39bbbf981ac0151d340c9aa40e63e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 124507,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4326488264445712976&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Statistical Visual Computing Laboratory, University of California, San Diego; Statistical Visual Computing Laboratory, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Statistical Visual Computing Laboratory",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "033e248d54",
        "title": "Bootstrapping Apprenticeship Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/47a658229eb2368a99f1d032c8848542-Abstract.html",
        "author": "Abdeslam Boularias; Brahim Chaib-draa",
        "abstract": "We consider the problem of apprenticeship learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is maximizing a utility function that is a linear combination of state-action features. Most IRL algorithms use a simple Monte Carlo estimation to approximate the expected feature counts under the expert's policy. In this paper, we show that the quality of the learned policies is highly sensitive to the error in estimating the feature counts. To reduce this error, we introduce a novel approach for bootstrapping the demonstration by assuming that: (i), the expert is (near-)optimal, and (ii), the dynamics of the system is known. Empirical results on gridworlds and car racing problems show that our approach is able to learn good policies from a small number of demonstrations.",
        "bibtex": "@inproceedings{NIPS2010_47a65822,\n author = {Boularias, Abdeslam and Chaib-draa, Brahim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bootstrapping Apprenticeship Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/47a658229eb2368a99f1d032c8848542-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/47a658229eb2368a99f1d032c8848542-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/47a658229eb2368a99f1d032c8848542-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 204993,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2582002119022807252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Empirical Inference, Max-Planck Institute for Biological Cybernetics; Department of Computer Science, Laval University",
        "aff_domain": "tuebingen.mpg.de;damas.ift.ulaval.ca",
        "email": "tuebingen.mpg.de;damas.ift.ulaval.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Max-Planck Institute for Biological Cybernetics;Laval University",
        "aff_unique_dep": "Department of Empirical Inference;Department of Computer Science",
        "aff_unique_url": "https://www.biological-cybernetics.de;https://www.laval.ca",
        "aff_unique_abbr": "MPIBC;Laval",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "id": "b3fdf5d5b6",
        "title": "Brain covariance selection: better individual functional connectivity models using population prior",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/db576a7d2453575f29eab4bac787b919-Abstract.html",
        "author": "Gael Varoquaux; Alexandre Gramfort; Jean-baptiste Poline; Bertrand Thirion",
        "abstract": "Spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies. An important view of modern neuroscience is that such large-scale structure of coherent activity reflects modularity properties of brain connectivity graphs. However, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data.  Learning such models entails two main challenges:  i) modeling full brain connectivity is a difficult estimation problem that faces the curse of dimensionality and  ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging.  We describe subject-level brain functional connectivity structure as a multivariate Gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population. We show that individual models learned from functional Magnetic Resonance Imaging (fMRI) data using this population prior generalize better to unseen data than models based on alternative regularization schemes. To our knowledge, this is the first report of a cross-validated model of spontaneous brain activity. Finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the first time that known cognitive networks appear as the integrated communities of functional connectivity graph.",
        "bibtex": "@inproceedings{NIPS2010_db576a7d,\n author = {Varoquaux, Gael and Gramfort, Alexandre and Poline, Jean-baptiste and Thirion, Bertrand},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Brain covariance selection: better individual functional connectivity models using population prior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db576a7d2453575f29eab4bac787b919-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/db576a7d2453575f29eab4bac787b919-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/db576a7d2453575f29eab4bac787b919-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/db576a7d2453575f29eab4bac787b919-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1325363,
        "gs_citation": 360,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11512438538064060753&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Parietal, INRIA+NeuroSpin, CEA, France; Parietal, INRIA+NeuroSpin, CEA, France; LNAO, I2BM, DSV+NeuroSpin, CEA, France; Parietal, INRIA+NeuroSpin, CEA, France",
        "aff_domain": "normalesup.org;inria.fr;cea.fr;inria.fr",
        "email": "normalesup.org;inria.fr;cea.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2+1;0+1",
        "aff_unique_norm": "INRIA;Commissariat \u00e0 l'\u00c9nergie Atomique et aux \u00c9nergies Alternatives (CEA);LNAO",
        "aff_unique_dep": "Parietal;NeuroSpin;",
        "aff_unique_url": "https://www.inria.fr;https://www.cea.fr;",
        "aff_unique_abbr": "INRIA;CEA;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "France;"
    },
    {
        "id": "66e14a2882",
        "title": "CUR from a Sparse Optimization Viewpoint",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/b20bb95ab626d93fd976af958fbc61ba-Abstract.html",
        "author": "Jacob Bien; Ya Xu; Michael W. Mahoney",
        "abstract": "The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. In particular, we show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.",
        "bibtex": "@inproceedings{NIPS2010_b20bb95a,\n author = {Bien, Jacob and Xu, Ya and Mahoney, Michael W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {CUR from a Sparse Optimization Viewpoint},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/b20bb95ab626d93fd976af958fbc61ba-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 124392,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9160649517277626507&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistics, Stanford University; Department of Statistics, Stanford University; Department of Mathematics, Stanford University",
        "aff_domain": "stanford.edu;gmail.com;cs.stanford.edu",
        "email": "stanford.edu;gmail.com;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eccf7a022a",
        "title": "Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/3dc4876f3f08201c7c76cb71fa1da439-Abstract.html",
        "author": "Danial Lashkari; Ramesh Sridharan; Polina Golland",
        "abstract": "We present a model that describes the structure in the responses of different brain areas to a set of stimuli in terms of stimulus categories\" (clusters of stimuli) and \"functional units\" (clusters of voxels). We assume that voxels within a unit respond similarly to all stimuli from the same category, and design a nonparametric hierarchical model to capture inter-subject variability among the units. The model explicitly captures the relationship between brain activations and fMRI time courses. A variational inference algorithm derived based on the model can learn categories, units, and a set of unit-category activation probabilities from data. When applied to data from an fMRI study of object recognition, the method finds meaningful and consistent clusterings of stimuli into categories and voxels into units.\"",
        "bibtex": "@inproceedings{NIPS2010_3dc4876f,\n author = {Lashkari, Danial and Sridharan, Ramesh and Golland, Polina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1266305,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14627718655276408010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9898352bb5",
        "title": "Causal discovery in multiple models from different experiments",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/5807a685d1a9ab3b599035bc566ce2b9-Abstract.html",
        "author": "Tom Claassen; Tom Heskes",
        "abstract": "A long-standing open research problem is how to use information from different experiments, including background knowledge, to infer causal relations. Recent developments have shown ways to use multiple data sets, provided they originate from identical experiments. We present the MCI-algorithm as the first method that can infer provably valid causal relations in the large sample limit from different experiments. It is fast, reliable and produces very clear and easily interpretable output. It is based on a result that shows that constraint-based causal discovery is decomposable into a candidate pair identification and subsequent elimination step that can be applied separately from different models. We test the algorithm on a variety of synthetic input model sets to assess its behavior and the quality of the output. The method shows promising signs that it can be adapted to suit causal discovery in real-world application areas as well, including large databases.",
        "bibtex": "@inproceedings{NIPS2010_5807a685,\n author = {Claassen, Tom and Heskes, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Causal discovery in multiple models from different experiments},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/5807a685d1a9ab3b599035bc566ce2b9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 304022,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15561680928374706508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Radboud University Nijmegen; Radboud University Nijmegen",
        "aff_domain": "cs.ru.nl;cs.ru.nl",
        "email": "cs.ru.nl;cs.ru.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Radboud University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ru.nl/",
        "aff_unique_abbr": "RU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nijmegen",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "5dcc43dd04",
        "title": "Co-regularization Based Semi-supervised Domain Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4a213d37242bdcad8e7300e202e7caa4-Abstract.html",
        "author": "Abhishek Kumar; Avishek Saha; Hal Daume",
        "abstract": "This paper presents a co-regularization based approach to semi-supervised domain adaptation. Our proposed approach (EA++) builds on the notion of augmented space (introduced in EASYADAPT (EA) [1]) and harnesses unlabeled data in target domain to further enable the transfer of information from source to target. This semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner. Our theoretical analysis (in terms of Rademacher complexity) of EA and EA++ show that the hypothesis class of EA++ has lower complexity (compared to EA) and hence results in tighter generalization bounds. Experimental results on sentiment analysis tasks reinforce our theoretical findings and demonstrate the efficacy of the proposed method when compared to EA as well as a few other baseline approaches.",
        "bibtex": "@inproceedings{NIPS2010_4a213d37,\n author = {Kumar, Abhishek and Saha, Avishek and Daume, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Co-regularization Based Semi-supervised Domain Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 148028,
        "gs_citation": 223,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17661791956460803278&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Maryland, CP, MD, USA; Department of Computer Science, University of Maryland, CP, MD, USA; School Of Computing, University of Utah, UT, USA",
        "aff_domain": "umiacs.umd.edu;umiacs.umd.edu;cs.utah.edu",
        "email": "umiacs.umd.edu;umiacs.umd.edu;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Maryland;University of Utah",
        "aff_unique_dep": "Department of Computer Science;School of Computing",
        "aff_unique_url": "https://www/umd.edu;https://www.utah.edu",
        "aff_unique_abbr": "UMD;U of U",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "College Park;Salt Lake City",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f3cb4966fe",
        "title": "Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/67d96d458abdef21792e6d8e590244e7-Abstract.html",
        "author": "Nathan Srebro; Ruslan Salakhutdinov",
        "abstract": "We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields significant gains on the highly non-uniformly sampled Netflix dataset.",
        "bibtex": "@inproceedings{NIPS2010_67d96d45,\n author = {Srebro, Nathan and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/67d96d458abdef21792e6d8e590244e7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 149882,
        "gs_citation": 278,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3887227198619712945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "Brain and Cognitive Sciences and CSAIL, MIT; Toyota Technological Institute at Chicago",
        "aff_domain": "mit.edu;ttic.edu",
        "email": "mit.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Brain and Cognitive Sciences and Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.mit.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "MIT;TTI Chicago",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "00dabb778d",
        "title": "Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html",
        "author": "Matthias Broecheler; Lise Getoor",
        "abstract": "Continuous Markov random fields are a general formalism to model joint probability distributions over events with continuous outcomes. We prove that marginal computation for constrained continuous MRFs is #P-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random field. Moreover, we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efficiency. Continuous MRFs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning. On the problem of collective classification, we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of confidence.",
        "bibtex": "@inproceedings{NIPS2010_459a4ddc,\n author = {Broecheler, Matthias and Getoor, Lise},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/459a4ddcb586f24efd9395aa7662bc7c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 396840,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7556059607830442617&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "208e8c55df",
        "title": "Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/27ed0fb950b856b06e1273989422e7d3-Abstract.html",
        "author": "George Konidaris; Scott Kuindersma; Roderic Grupen; Andrew G. Barto",
        "abstract": "We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further refined through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction.",
        "bibtex": "@inproceedings{NIPS2010_27ed0fb9,\n author = {Konidaris, George and Kuindersma, Scott and Grupen, Roderic and Barto, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 807200,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2641259541208601151&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Autonomous Learning Laboratory\u2020; Laboratory for Perceptual Robotics\u2021; Autonomous Learning Laboratory\u2020; Laboratory for Perceptual Robotics\u2021",
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Autonomous Learning Laboratory;Laboratory for Perceptual Robotics",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ffc64cb742",
        "title": "Construction of Dependent Dirichlet Processes based on Poisson Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/9f60ab2b55468f104055b16df8f69e81-Abstract.html",
        "author": "Dahua Lin; Eric Grimson; John W. Fisher",
        "abstract": "We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson pro- cesses in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, re- moval, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Addition- ally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effec- tive in estimating dynamically varying mixture models.",
        "bibtex": "@inproceedings{NIPS2010_9f60ab2b,\n author = {Lin, Dahua and Grimson, Eric and Fisher, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Construction of Dependent Dirichlet Processes based on Poisson Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1753718,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14602253772153836591&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CSAIL, MIT; CSAIL, MIT; CSAIL, MIT",
        "aff_domain": "mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "mit.edu;csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "93459bb48f",
        "title": "Convex Multiple-Instance Learning by Estimating Likelihood Ratio",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0e9fa1f3e9e66792401a6972d477dcc3-Abstract.html",
        "author": "Fuxin Li; Cristian Sminchisescu",
        "abstract": "Multiple-Instance learning has been long known as a hard non-convex problem.\n In this work, we propose an approach that recasts it as a convex likelihood ratio\n estimation problem. Firstly, the constraint in multiple-instance learning is reformulated\n into a convex constraint on the likelihood ratio. Then we show that a joint\n estimation of a likelihood ratio function and the likelihood on training instances\n can be learned convexly. Theoretically, we prove a quantitative relationship between\n the risk estimated under the 0-1 classification loss, and under a loss function\n for likelihood ratio estimation. It is shown that our likelihood ratio estimation is\n generally a good surrogate for the 0-1 loss, and separates positive and negative\n instances well. However with the joint estimation it tends to underestimate the\n likelihood of an example to be positive. We propose to use these likelihood ratio\n estimates as features, and learn a linear combination on them to classify the bags.\n Experiments on synthetic and real datasets show the superiority of the approach.",
        "bibtex": "@inproceedings{NIPS2010_0e9fa1f3,\n author = {Li, Fuxin and Sminchisescu, Cristian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convex Multiple-Instance Learning by Estimating Likelihood Ratio},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 650445,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13076724308275062937&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Institute for Numerical Simulation, University of Bonn; Institute for Numerical Simulation, University of Bonn",
        "aff_domain": "ins.uni-bonn.de;ins.uni-bonn.de",
        "email": "ins.uni-bonn.de;ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Bonn",
        "aff_unique_dep": "Institute for Numerical Simulation",
        "aff_unique_url": "https://www.uni-bonn.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "3d45dc0592",
        "title": "Copula Bayesian Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract.html",
        "author": "Gal Elidan",
        "abstract": "We present the Copula Bayesian Network model for representing multivariate continuous distributions. Our approach builds on a novel copula-based parameterization of a conditional density that, joined with a graph that encodes independencies, offers great flexibility in modeling high-dimensional densities, while maintaining control over the form of the univariate marginals. We demonstrate the advantage of our framework for generalization over standard Bayesian networks as well as tree structured copula models for varied real-life domains that are of substantially higher dimension than those typically considered in the copula literature.",
        "bibtex": "@inproceedings{NIPS2010_2a79ea27,\n author = {Elidan, Gal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Copula Bayesian Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 999027,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1684612710506890430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, Hebrew University, Jerusalem, 91905, Israel",
        "aff_domain": "huji.ac.il",
        "email": "huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Hebrew University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "6dfe2eac3d",
        "title": "Copula Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/fc8001f834f6a5f0561080d134d53d29-Abstract.html",
        "author": "Andrew G Wilson; Zoubin Ghahramani",
        "abstract": "We define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We find our model can outperform GARCH on simulated and financial data. And unlike GARCH, GCPV can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.",
        "bibtex": "@inproceedings{NIPS2010_fc8001f8,\n author = {Wilson, Andrew G and Ghahramani, Zoubin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Copula Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 324318,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12022680847689227026&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Engineering, University of Cambridge + Machine Learning Department, Carnegie Mellon University; Department of Engineering, University of Cambridge",
        "aff_domain": "cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "http://mlg.eng.cam.ac.uk/andrew",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "University of Cambridge;Carnegie Mellon University",
        "aff_unique_dep": "Department of Engineering;Machine Learning Department",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.cmu.edu",
        "aff_unique_abbr": "Cambridge;CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "eca71829c6",
        "title": "Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/94f6d7e04a4d452035300f18b984988c-Abstract.html",
        "author": "Ziv Bar-joseph; Hai-son P. Le",
        "abstract": "Recent studies compare gene expression data across species to identify core and species specific genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes.",
        "bibtex": "@inproceedings{NIPS2010_94f6d7e0,\n author = {Bar-joseph, Ziv and Le, Hai-son},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 195748,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12716980522548183359&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7dbcae0d8b",
        "title": "Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a0160709701140704575d499c997b6ca-Abstract.html",
        "author": "Guy Isely; Christopher Hillar; Fritz Sommer",
        "abstract": "A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efficient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through fiber bottlenecks are able to form coherent response properties.",
        "bibtex": "@inproceedings{NIPS2010_a0160709,\n author = {Isely, Guy and Hillar, Christopher and Sommer, Fritz},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a0160709701140704575d499c997b6ca-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a0160709701140704575d499c997b6ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a0160709701140704575d499c997b6ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 370384,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1590534181701663031&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Redwood Center for Theoretical Neuroscience, University of California, Berkeley; Mathematical Sciences Research Institute; University of California, Berkeley",
        "aff_domain": "berkeley.edu;msri.org;berkeley.edu",
        "email": "berkeley.edu;msri.org;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Mathematical Sciences Research Institute",
        "aff_unique_dep": "Redwood Center for Theoretical Neuroscience;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.msri.org",
        "aff_unique_abbr": "UC Berkeley;MSRI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fa828d5b26",
        "title": "Decoding Ipsilateral Finger Movements from ECoG Signals in Humans",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2b8a61594b1f4c4db0902a8a395ced93-Abstract.html",
        "author": "Yuzong Liu; Mohit Sharma; Charles Gaona; Jonathan Breshears; Jarod Roland; Zachary Freudenburg; Eric Leuthardt; Kilian Q. Weinberger",
        "abstract": "Several motor related Brain Computer Interfaces (BCIs) have been developed over the years that use activity decoded from the contralateral hemisphere to operate devices. Many recent studies have also talked about the importance of ipsilateral activity in planning of motor movements. For successful upper limb BCIs, it is important to decode finger movements from brain activity. This study uses ipsilateral cortical signals from humans (using ECoG) to decode finger movements. We demonstrate, for the first time, successful finger movement detection using machine learning algorithms. Our results show high decoding accuracies in all cases which are always above chance. We also show that significant accuracies can be achieved with the use of only a fraction of all the features recorded and that these core features also make sense physiologically. The results of this study have a great potential in the emerging world of motor neuroprosthetics and other BCIs.",
        "bibtex": "@inproceedings{NIPS2010_2b8a6159,\n author = {Liu, Yuzong and Sharma, Mohit and Gaona, Charles and Breshears, Jonathan and Roland, Jarod and Freudenburg, Zachary and Leuthardt, Eric and Weinberger, Kilian Q},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decoding Ipsilateral Finger Movements from ECoG Signals in Humans},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2b8a61594b1f4c4db0902a8a395ced93-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 618598,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10803741671504274985&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7622581362",
        "title": "Decomposing Isotonic Regression for Efficiently Solving Large Problems",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/03c6b06952c750899bb03d998e631860-Abstract.html",
        "author": "Ronny Luss; Saharon Rosset; Moni Shahar",
        "abstract": "A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efficient methods for each partitioning subproblem through an equivalent representation as a network flow problem, and prove that this sequence of partitions converges to the global solution. These network flow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm's favorable computational properties are demonstrated through simulated examples as large as 2x10^5 variables and 10^7 constraints.",
        "bibtex": "@inproceedings{NIPS2010_03c6b069,\n author = {Luss, Ronny and Rosset, Saharon and Shahar, Moni},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Decomposing Isotonic Regression for Efficiently Solving Large Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/03c6b06952c750899bb03d998e631860-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/03c6b06952c750899bb03d998e631860-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/03c6b06952c750899bb03d998e631860-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 559798,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1387241630038592531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Statistics and OR, Tel Aviv University; Dept. of Statistics and OR, Tel Aviv University; Dept. of Electrical Eng., Tel Aviv University",
        "aff_domain": "gmail.com;post.tau.ac.il;eng.tau.ac.il",
        "email": "gmail.com;post.tau.ac.il;eng.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "Dept. of Statistics and OR",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tel Aviv;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "c40d80061d",
        "title": "Deep Coding Network",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html",
        "author": "Yuanqing Lin; Tong Zhang; Shenghuo Zhu; Kai Yu",
        "abstract": "This paper proposes a principled extension of the traditional single-layer flat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.",
        "bibtex": "@inproceedings{NIPS2010_062ddb6c,\n author = {Lin, Yuanqing and Zhang, Tong and Zhu, Shenghuo and Yu, Kai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deep Coding Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/062ddb6c727310e76b6200b7c71f63b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 112076,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3352221647434308135&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cfe181d3f9",
        "title": "Deterministic Single-Pass Algorithm for LDA",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7a53928fa4dd31e82c6ef826f341daec-Abstract.html",
        "author": "Issei Sato; Kenichi Kurihara; Hiroshi Nakagawa",
        "abstract": "We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.",
        "bibtex": "@inproceedings{NIPS2010_7a53928f,\n author = {Sato, Issei and Kurihara, Kenichi and Nakagawa, Hiroshi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Deterministic Single-Pass Algorithm for LDA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 350969,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5279930460920099472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Tokyo, Japan; Google; University of Tokyo, Japan",
        "aff_domain": "r.dl.itc.u-tokyo.ac.jp;gmail.com;dl.itc.u-tokyo.ac.jp",
        "email": "r.dl.itc.u-tokyo.ac.jp;gmail.com;dl.itc.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Tokyo;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.google.com",
        "aff_unique_abbr": "UTokyo;Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "cf7115021e",
        "title": "Direct Loss Minimization for Structured Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/ca8155f4d27f205953f9d3d7974bdd70-Abstract.html",
        "author": "Tamir Hazan; Joseph Keshet; David A. McAllester",
        "abstract": "In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance, or loss. In binary classification one typically tries to minimizes the error rate. But in structured prediction each task often has its own measure of performance such as the BLEU score in machine translation or the intersection-over-union score in PASCAL segmentation. The most common approaches to structured prediction, structural SVMs and CRFs, do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss. The main contribution of this paper is a theorem stating that a certain perceptron-like learning rule, involving features vectors derived from loss-adjusted inference, directly corresponds to the gradient of task loss. We give empirical results on phonetic alignment of a standard test set from the TIMIT corpus, which surpasses all previously reported results on this problem.",
        "bibtex": "@inproceedings{NIPS2010_ca8155f4,\n author = {Hazan, Tamir and Keshet, Joseph and McAllester, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Direct Loss Minimization for Structured Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/ca8155f4d27f205953f9d3d7974bdd70-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 601194,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=194923650781315092&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "TTI-Chicago; TTI-Chicago; TTI-Chicago",
        "aff_domain": "ttic.edu;ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e118655319",
        "title": "Discriminative Clustering by Regularized Information Maximization",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/42998cf32d552343bc8e460416382dca-Abstract.html",
        "author": "Andreas Krause; Pietro Perona; Ryan G. Gomes",
        "abstract": "Is there a principled way to learn a probabilistic discriminative classifier from an unlabeled data set? We present a framework that simultaneously clusters the data and trains a discriminative classifier. We call it Regularized Information Maximization (RIM). RIM optimizes an intuitive information-theoretic objective function which balances class separation, class balance and classifier complexity. The approach can flexibly incorporate different likelihood functions, express prior assumptions about the relative size of different classes and incorporate partial labels for semi-supervised learning. In particular, we instantiate the framework to unsupervised, multi-class kernelized logistic regression. Our empirical evaluation indicates that RIM outperforms existing methods on several real data sets, and demonstrates that RIM is an effective model selection method.",
        "bibtex": "@inproceedings{NIPS2010_42998cf3,\n author = {Krause, Andreas and Perona, Pietro and Gomes, Ryan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative Clustering by Regularized Information Maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/42998cf32d552343bc8e460416382dca-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/42998cf32d552343bc8e460416382dca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/42998cf32d552343bc8e460416382dca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 982210,
        "gs_citation": 459,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3179509508554002983&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "California Institute of Technology; California Institute of Technology; California Institute of Technology",
        "aff_domain": "vision.caltech.edu;caltech.edu;vision.caltech.edu",
        "email": "vision.caltech.edu;caltech.edu;vision.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2b46d7149e",
        "title": "Distributed Dual Averaging In Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/faa9afea49ef2ff029a833cccc778fd0-Abstract.html",
        "author": "Alekh Agarwal; Martin J. Wainwright; John C. Duchi",
        "abstract": "The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and we provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks.",
        "bibtex": "@inproceedings{NIPS2010_faa9afea,\n author = {Agarwal, Alekh and Wainwright, Martin J and Duchi, John C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Dual Averaging In Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 704786,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7685303596565994233&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering and Computer Science1 + Department of Statistics2; Department of Electrical Engineering and Computer Science1 + Department of Statistics2; Department of Electrical Engineering and Computer Science1 + Department of Statistics2",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "University of Michigan;University of California, Los Angeles",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Department of Statistics",
        "aff_unique_url": "https://www.eecs.umich.edu;https://www.ucla.edu",
        "aff_unique_abbr": "UM EECS;UCLA",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e7a37936ab",
        "title": "Distributionally Robust Markov Decision Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/19f3cd308f1455b3fa09a282e0d496f4-Abstract.html",
        "author": "Huan Xu; Shie Mannor",
        "abstract": "We consider Markov decision processes where the values of the parameters are uncertain. This uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different confidence level so that a set of admissible probability distributions of the unknown parameters is specified. This formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a-priori information of the distribution of parameters, and arises naturally in practice where methods to estimate the confidence region of parameters abound. We propose a decision criterion based on",
        "bibtex": "@inproceedings{NIPS2010_19f3cd30,\n author = {Xu, Huan and Mannor, Shie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributionally Robust Markov Decision Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/19f3cd308f1455b3fa09a282e0d496f4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 133336,
        "gs_citation": 318,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12829790638222669349&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "ECE, University of Texas at Austin; Department of Electrical Engineering, Technion, Israel",
        "aff_domain": "mail.utexas.edu;ee.technion.ac.il",
        "email": "mail.utexas.edu;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Texas at Austin;Technion",
        "aff_unique_dep": "Electrical and Computer Engineering;Department of Electrical Engineering",
        "aff_unique_url": "https://www.utexas.edu;https://www.technion.ac.il",
        "aff_unique_abbr": "UT Austin;Technion",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "4b96587027",
        "title": "Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/f033ab37c30201f73f142449d037028d-Abstract.html",
        "author": "Siwei Lyu",
        "abstract": "Divisive normalization (DN) has been advocated as an effective nonlinear {\\em efficient coding} transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate {\\em t} model to capture some important statistical properties of natural sensory signals. The multivariate {\\em t} model justifies DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate {\\em t} model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations confirm DN as an effective efficient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small.",
        "bibtex": "@inproceedings{NIPS2010_f033ab37,\n author = {Lyu, Siwei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f033ab37c30201f73f142449d037028d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/f033ab37c30201f73f142449d037028d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/f033ab37c30201f73f142449d037028d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 340276,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4163921981242313920&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, University at Albany, State University of New York",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University at Albany, State University of New York",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.albany.edu",
        "aff_unique_abbr": "UAlbany",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Albany",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "46128ca89c",
        "title": "Double Q-learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html",
        "author": "Hado V. Hasselt",
        "abstract": "In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.",
        "bibtex": "@inproceedings{NIPS2010_091d584f,\n author = {Hasselt, Hado},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Double Q-learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 195211,
        "gs_citation": 2469,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12123279254640438894&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1c2bf8de7a",
        "title": "Dynamic Infinite Relational Model for Time-varying Relational Data Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html",
        "author": "Katsuhiko Ishiguro; Tomoharu Iwata; Naonori Ueda; Joshua B. Tenenbaum",
        "abstract": "We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed time-varying object-object relationships into relationships between object clusters. We extend the infinite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.",
        "bibtex": "@inproceedings{NIPS2010_432aca3a,\n author = {Ishiguro, Katsuhiko and Iwata, Tomoharu and Ueda, Naonori and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic Infinite Relational Model for Time-varying Relational Data Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/432aca3a1e345e339f35a30c8f65edce-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 249830,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17837724687352385107&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "NTT Communication Science Laboratories; NTT Communication Science Laboratories; NTT Communication Science Laboratories; MIT",
        "aff_domain": "cslab.kecl.ntt.co.jp;cslab.kecl.ntt.co.jp;cslab.kecl.ntt.co.jp;mit.edu",
        "email": "cslab.kecl.ntt.co.jp;cslab.kecl.ntt.co.jp;cslab.kecl.ntt.co.jp;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "NTT Communication Science Laboratories;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntt-csl.com;https://web.mit.edu",
        "aff_unique_abbr": "NTT CSL;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "938e8f776b",
        "title": "Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/11b921ef080f7736089c757404650e40-Abstract.html",
        "author": "Kentaro Katahira; Kazuo Okanoya; Masato Okada",
        "abstract": "When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modifications in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein\u2019s matching law. Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufficiently strong so that the fluctuations in input from individual sensory neurons influence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.",
        "bibtex": "@inproceedings{NIPS2010_11b921ef,\n author = {Katahira, Kentaro and Okanoya, Kazuo and Okada, Masato},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/11b921ef080f7736089c757404650e40-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/11b921ef080f7736089c757404650e40-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/11b921ef080f7736089c757404650e40-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/11b921ef080f7736089c757404650e40-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 295964,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14293798817134272297&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "ERATO Okanoya Emotional Information Project, Japan Science Technology Agency + Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Chiba 277-8561, Japan + RIKEN Brain Science Institute, Wako, Saitama 351-0198, Japan; ERATO Okanoya Emotional Information Project, Japan Science Technology Agency + RIKEN Brain Science Institute, Wako, Saitama 351-0198, Japan; ERATO Okanoya Emotional Information Project, Japan Science Technology Agency + Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Chiba 277-8561, Japan + RIKEN Brain Science Institute, Wako, Saitama 351-0198, Japan",
        "aff_domain": "mns.k.u-tokyo.ac.jp;brain.riken.jp;k.u-tokyo.ac.jp",
        "email": "mns.k.u-tokyo.ac.jp;brain.riken.jp;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+2;0+1+2",
        "aff_unique_norm": "Japan Science Technology Agency;University of Tokyo;RIKEN Brain Science Institute",
        "aff_unique_dep": "ERATO Okanoya Emotional Information Project;Graduate School of Frontier Sciences;Brain Science Institute",
        "aff_unique_url": "https://www.jst.go.jp;https://www.u-tokyo.ac.jp;https://briken.org",
        "aff_unique_abbr": "JST;UTokyo;RIKEN BSI",
        "aff_campus_unique_index": "1+2;2;1+2",
        "aff_campus_unique": ";Kashiwa;Wako",
        "aff_country_unique_index": "0+0+0;0+0;0+0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "85fd6668c3",
        "title": "Efficient Minimization of Decomposable Submodular Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6ea2ef7311b482724a9b7b0bc0dd85c6-Abstract.html",
        "author": "Peter Stobbe; Andreas Krause",
        "abstract": "Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for practical problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to linear functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.",
        "bibtex": "@inproceedings{NIPS2010_6ea2ef73,\n author = {Stobbe, Peter and Krause, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Minimization of Decomposable Submodular Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 347451,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=536823729597085768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "California Institute of Technology; California Institute of Technology",
        "aff_domain": "caltech.edu;caltech.edu",
        "email": "caltech.edu;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5a9cd02e97",
        "title": "Efficient Optimization for Discriminative Latent Class Models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4da04049a062f5adfe81b67dd755cecc-Abstract.html",
        "author": "Armand Joulin; Jean Ponce; Francis R. Bach",
        "abstract": "Dimensionality reduction is commonly used in the setting of multi-label supervised classification to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression; we show that this model provides a probabilistic interpretation of discriminative clustering methods with added benefits in terms of number of hyperparameters and optimization. While expectation-maximization (EM) algorithm is commonly used to learn these models, its optimization usually leads to local minimum because it relies on a non-convex cost function with many such local minima. To avoid this problem, we introduce a local approximation of this cost function, which leads to a quadratic non-convex optimization problem over a product of simplices. In order to minimize such functions, we propose an efficient algorithm based on convex relaxation and low-rank representation of our data, which allows to deal with large instances. Experiments on text document classification show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods.",
        "bibtex": "@inproceedings{NIPS2010_4da04049,\n author = {Joulin, Armand and Ponce, Jean and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Optimization for Discriminative Latent Class Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 224278,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11435140208567803475&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "INRIA; INRIA; Ecole Normale Sup\u00e9rieure",
        "aff_domain": "inria.fr;inria.fr;ens.fr",
        "email": "inria.fr;inria.fr;ens.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "INRIA;Ecole Normale Sup\u00e9rieure",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.inria.fr;https://www.ens.fr",
        "aff_unique_abbr": "INRIA;ENS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "6ad595fdac",
        "title": "Efficient Relational Learning with Hidden Variable Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html",
        "author": "Ni Lao; Jun Zhu; Liu Liu; Yandong Liu; William W. Cohen",
        "abstract": "Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this flexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efficient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efficiency; and on the other hand, the CVI algorithm efficiently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efficient yet does not sacrifice its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is significantly more efficient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.",
        "bibtex": "@inproceedings{NIPS2010_bf62768c,\n author = {Lao, Ni and Zhu, Jun and Liu, Liu and Liu, Yandong and Cohen, William W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Relational Learning with Hidden Variable Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/bf62768ca46b6c3b5bea9515d1a1fc45-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 144529,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10554440395938234368&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "98d23e5edb",
        "title": "Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/df7f28ac89ca37bf1abd2f6c184fe1cf-Abstract.html",
        "author": "Achintya Kundu; Vikram Tankasali; Chiranjib Bhattacharyya; Aharon Ben-tal",
        "abstract": "In this paper we consider the problem of learning an n x n Kernel matrix from m similarity matrices under general convex loss. Past research have extensively studied the m =1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in O(m^2 log n) iterations; in each iteration one solves an MKL involving m kernels and m eigen-decomposition of n x n matrices. By suitably defining a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efficacy of the proposed algorithms.",
        "bibtex": "@inproceedings{NIPS2010_df7f28ac,\n author = {Kundu, Achintya and Tankasali, Vikram and Bhattacharyya, Chiranjib and Ben-tal, Aharon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 139476,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15429145612812017832&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Science & Automation, Indian Institute of Science, Bangalore; Dept. of Computer Science & Automation, Indian Institute of Science, Bangalore; Dept. of Computer Science & Automation, Indian Institute of Science, Bangalore; Faculty of Industrial Engg. & Management, Technion-Israel Institute of Technology, Haifa + Visiting Professor, CWI, Amsterdam",
        "aff_domain": "csa.iisc.ernet.in;csa.iisc.ernet.in;csa.iisc.ernet.in;ie.technion.ac.il",
        "email": "csa.iisc.ernet.in;csa.iisc.ernet.in;csa.iisc.ernet.in;ie.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1+2",
        "aff_unique_norm": "Indian Institute of Science;Technion-Israel Institute of Technology;Centrum Wiskunde & Informatica",
        "aff_unique_dep": "Dept. of Computer Science & Automation;Faculty of Industrial Engineering & Management;",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.technion.ac.il;https://www.cwi.nl",
        "aff_unique_abbr": "IISc;Technion;CWI",
        "aff_campus_unique_index": "0;0;0;1+2",
        "aff_campus_unique": "Bangalore;Haifa;Amsterdam",
        "aff_country_unique_index": "0;0;0;1+2",
        "aff_country_unique": "India;Israel;Netherlands"
    },
    {
        "id": "6dc44eba38",
        "title": "Efficient and Robust Feature Selection via Joint \u21132,1-Norms Minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html",
        "author": "Feiping Nie; Heng Huang; Xiao Cai; Chris H. Ding",
        "abstract": "Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint \u21132,1-norm minimization on both loss function and regularization. The \u21132,1-norm based loss function is robust to outliers in data points and the \u21132,1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies were performed on six data sets to demonstrate the effectiveness of our feature selection method.",
        "bibtex": "@inproceedings{NIPS2010_09c6c378,\n author = {Nie, Feiping and Huang, Heng and Cai, Xiao and Ding, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient and Robust Feature Selection via Joint \\mathscr{l}2,1-Norms Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/09c6c3783b4a70054da74f2538ed47c6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 270068,
        "gs_citation": 2605,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15797254902507496570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science and Engineering, University of Texas at Arlington; Computer Science and Engineering, University of Texas at Arlington; Computer Science and Engineering, University of Texas at Arlington; Computer Science and Engineering, University of Texas at Arlington",
        "aff_domain": "gmail.com;uta.edu;mavs.uta.edu;uta.edu",
        "email": "gmail.com;uta.edu;mavs.uta.edu;uta.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Texas at Arlington",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.uta.edu",
        "aff_unique_abbr": "UTA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Arlington",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4c573b004a",
        "title": "Empirical Bernstein Inequalities for U-Statistics",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d6ef5f7fa914c19931a55bb262ec879c-Abstract.html",
        "author": "Thomas Peel; Sandrine Anthoine; Liva Ralaivola",
        "abstract": "We present original empirical Bernstein inequalities for U-statistics with bounded symmetric kernels q. They are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the Bernstein-type inequality for U-statistics derived by Arcones [2]. Our result subsumes other existing empirical Bernstein inequalities, as it reduces to them when U-statistics of order 1 are considered. In addition, it is based on a rather direct argument using two applications of the same (non-empirical) Bernstein inequality for U-statistics. We discuss potential applications of our new inequalities, especially in the realm of learning ranking/scoring functions. In the process, we exhibit an efficient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument. We also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions.",
        "bibtex": "@inproceedings{NIPS2010_d6ef5f7f,\n author = {Peel, Thomas and Anthoine, Sandrine and Ralaivola, Liva},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Empirical Bernstein Inequalities for U-Statistics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d6ef5f7fa914c19931a55bb262ec879c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 371420,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9282631869994734957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "LIF, Aix-Marseille Universit \u00b4e; LATP, Aix-Marseille Universit \u00b4e, CNRS; LIF, Aix-Marseille Universit \u00b4e",
        "aff_domain": "lif.univ-mrs.fr;cmi.univ-mrs.fr;lif.univ-mrs.fr",
        "email": "lif.univ-mrs.fr;cmi.univ-mrs.fr;lif.univ-mrs.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Aix-Marseille University;Aix-Marseille Universit\u00e9",
        "aff_unique_dep": "Laboratoire d'Informatique Fondamentale;Laboratoire d'Analyse Topologie et Probabilit\u00e9s",
        "aff_unique_url": "https://www.univ-amu.fr;https://www.univ-amu.fr",
        "aff_unique_abbr": "AMU;AMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Aix-en-Provence",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "cbbdf0c77d",
        "title": "Empirical Risk Minimization with Approximations of Probabilistic Grammars",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html",
        "author": "Noah A. Smith; Shay B. Cohen",
        "abstract": "Probabilistic grammars are generative statistical models that are  useful for compositional and sequential structures. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a fixed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting.",
        "bibtex": "@inproceedings{NIPS2010_1aa48fc4,\n author = {Smith, Noah A and Cohen, Shay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Empirical Risk Minimization with Approximations of Probabilistic Grammars},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 260250,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8421260903875808315&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 25,
        "aff": "Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute, School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b0eee03038",
        "title": "Energy Disaggregation via Discriminative Sparse Coding",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7810ccd41bf26faaa2c4e1f20db70a71-Abstract.html",
        "author": "J. Z. Kolter; Siddharth Batra; Andrew Y. Ng",
        "abstract": "Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem. In particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance. We show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage.",
        "bibtex": "@inproceedings{NIPS2010_7810ccd4,\n author = {Kolter, J. and Batra, Siddharth and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Energy Disaggregation via Discriminative Sparse Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7810ccd41bf26faaa2c4e1f20db70a71-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 274894,
        "gs_citation": 513,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6123169740248235689&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "csail.mit.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "csail.mit.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Stanford University",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;Computer Science Department",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.stanford.edu",
        "aff_unique_abbr": "MIT;Stanford",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Cambridge;Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "602d4c3765",
        "title": "Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/37a749d808e46495a8da1e5352d03cae-Abstract.html",
        "author": "Kamiya Motwani; Nagesh Adluru; Chris Hinrichs; Andrew Alexander; Vikas Singh",
        "abstract": "We study the problem of segmenting specific white matter structures of interest from Diffusion Tensor (DT-MR) images of the human brain. This is an important requirement in many Neuroimaging studies: for instance, to evaluate whether a brain structure exhibits group level differences as a function of disease in a set of images. Typically, interactive expert guided segmentation has been the method of choice for such applications, but this is tedious for large datasets common today. To address this problem, we endow an image segmentation algorithm with 'advice' encoding some global characteristics of the region(s) we want to extract. This is accomplished by constructing (using expert-segmented images) an epitome of a specific region - as a histogram over a bag of 'words' (e.g.,suitable feature descriptors). Now, given such a representation, the problem reduces to segmenting new brain image with additional constraints that enforce consistency between the segmented foreground and the pre-specified histogram over features. We present combinatorial approximation algorithms to incorporate such domain specific constraints for Markov Random Field (MRF) segmentation. Making use of recent results on image co-segmentation, we derive effective solution strategies for our problem. We provide an analysis of solution quality, and present promising experimental evidence showing that many structures of interest in Neuroscience can be extracted reliably from 3-D brain image volumes using our algorithm.",
        "bibtex": "@inproceedings{NIPS2010_37a749d8,\n author = {Motwani, Kamiya and Adluru, Nagesh and Hinrichs, Chris and Alexander, Andrew and Singh, Vikas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/37a749d808e46495a8da1e5352d03cae-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5592828,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7295545412731172783&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 20,
        "aff": "Computer Sciences; Biostatistics & Medical Informatics; Computer Sciences; Medical Physics; Computer Sciences + Biostatistics & Medical Informatics",
        "aff_domain": "cs.wisc.edu;wisc.edu;cs.wisc.edu;wisc.edu;cs.wisc.edu",
        "email": "cs.wisc.edu;wisc.edu;cs.wisc.edu;wisc.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;0+1",
        "aff_unique_norm": "Computer Sciences;University of Wisconsin-Madison;Medical Physics",
        "aff_unique_dep": ";Department of Biostatistics & Medical Informatics;",
        "aff_unique_url": ";https://www.wisc.edu;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "33bc99a697",
        "title": "Error Propagation for Approximate Policy and Value Iteration",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/65cc2c8205a05d7379fa3a6386f710e1-Abstract.html",
        "author": "Amir-massoud Farahmand; Csaba Szepesv\u00e1ri; R\u00e9mi Munos",
        "abstract": "We address the question of how the approximation error/Bellman residual at each iteration of the Approximate Policy/Value Iteration algorithms influences the quality of the resulted policy. We quantify the performance loss as the Lp norm of the approximation error/Bellman residual at each iteration. Moreover, we show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution rather than its supremum -- as opposed to what has been suggested by the previous results.  Also our results indicate that the contribution of the approximation/Bellman error to the performance loss is more prominent in the later iterations of API/AVI, and the effect of an error term in the earlier iterations decays exponentially fast.",
        "bibtex": "@inproceedings{NIPS2010_65cc2c82,\n author = {Farahmand, Amir-massoud and Szepesv\\'{a}ri, Csaba and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Error Propagation for Approximate Policy and Value Iteration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 220774,
        "gs_citation": 300,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13519723184441285465&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Computing Science, University of Alberta; Sequel Project, INRIA Lille; Department of Computing Science, University of Alberta + MTA SZTAKI",
        "aff_domain": "ualberta.ca;inria.fr;ualberta.ca",
        "email": "ualberta.ca;inria.fr;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "University of Alberta;INRIA;Hungarian Academy of Sciences",
        "aff_unique_dep": "Department of Computing Science;Sequel Project;Computer and Automation Research Institute",
        "aff_unique_url": "https://www.ualberta.ca;https://www.inria.fr;https://www.sztaki.hu",
        "aff_unique_abbr": "UAlberta;INRIA;SZTAKI",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;1;0+2",
        "aff_country_unique": "Canada;France;Hungary"
    },
    {
        "id": "93814c3706",
        "title": "Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/076a0c97d09cf1a0ec3e19c7f2529f2b-Abstract.html",
        "author": "Abhinav Gupta; Martial Hebert; Takeo Kanade; David M. Blei",
        "abstract": "There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning significantly improves the performance of the state-of-the-art.",
        "bibtex": "@inproceedings{NIPS2010_076a0c97,\n author = {Gupta, Abhinav and Hebert, Martial and Kanade, Takeo and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 9750658,
        "gs_citation": 385,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6746780867798347562&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "58070d3bcc",
        "title": "Estimation of R\u00e9nyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/577ef1154f3240ad5b9b413aa7346a1e-Abstract.html",
        "author": "D\u00e1vid P\u00e1l; Barnab\u00e1s P\u00f3czos; Csaba Szepesv\u00e1ri",
        "abstract": "We present simple and computationally efficient nonparametric estimators of R\\'enyi entropy and mutual information based on an i.i.d. sample drawn from an unknown, absolutely continuous distribution over $\\R^d$. The estimators are calculated as the sum of $p$-th powers of the Euclidean lengths of the edges of the `generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively. For the first time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis.",
        "bibtex": "@inproceedings{NIPS2010_577ef115,\n author = {P\\'{a}l, D\\'{a}vid and P\\'{o}czos, Barnab\\'{a}s and Szepesv\\'{a}ri, Csaba},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimation of R\\'{e}nyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 293685,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14862263714288321155&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computing Science, University of Alberta; School of Computer Science, Carnegie Mellon University + Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "cs.ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "University of Alberta;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computing Science;School of Computer Science",
        "aff_unique_url": "https://www.ualberta.ca;https://www.cmu.edu",
        "aff_unique_abbr": "UAlberta;CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "8def8e9847",
        "title": "Evaluating neuronal codes for inference using Fisher information",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/08b255a5d42b89b0585260b6f2360bdd-Abstract.html",
        "author": "Haefner Ralf; Matthias Bethge",
        "abstract": "Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and find it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We find that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions.",
        "bibtex": "@inproceedings{NIPS2010_08b255a5,\n author = {Ralf, Haefner and Bethge, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Evaluating neuronal codes for inference using Fisher information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/08b255a5d42b89b0585260b6f2360bdd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 756634,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1554330316717510684&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Centre for Integrative Neuroscience, University of T\u00fcbingen + Bernstein Center for Computational Neuroscience, T\u00fcbingen + Max Planck Institute for Biological Cybernetics; Centre for Integrative Neuroscience, University of T\u00fcbingen + Bernstein Center for Computational Neuroscience, T\u00fcbingen + Max Planck Institute for Biological Cybernetics",
        "aff_domain": "gmail.com; ",
        "email": "gmail.com; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+1+2",
        "aff_unique_norm": "University of T\u00fcbingen;Bernstein Center for Computational Neuroscience;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Centre for Integrative Neuroscience;Computational Neuroscience;Biological Cybernetics",
        "aff_unique_url": "https://www.uni-tuebingen.de;;https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": ";;MPIBC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "0+0+0;0+0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "bb2fe0a6c9",
        "title": "Evaluation of Rarity of Fingerprints in Forensics",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/287e03db1d99e0ec2edb90d079e142f3-Abstract.html",
        "author": "Chang Su; Sargur Srihari",
        "abstract": "A method for computing the rarity of latent fingerprints represented by minutiae is given. It allows determining the probability of finding a match for an evidence print in a database of n known prints. The probability of random correspondence between evidence and database is determined in three procedural steps. In the registration step the latent print is aligned by finding its core point; which is done using a procedure based on a machine learning approach based on Gaussian processes. In the evidence probability evaluation step a generative model based on Bayesian networks is used to determine the probability of the evidence; it takes into account both the dependency of each minutia on nearby minutiae and the confidence of their presence in the evidence. In the specific probability of random correspondence step the evidence probability is used to determine the probability of match among n for a given tolerance; the last evaluation is similar to the birthday correspondence probability for a specific birthday. The generative model is validated using a goodness-of-fit test evaluated with a standard database of fingerprints. The probability of random correspondence for several latent fingerprints are evaluated for varying numbers of minutiae.",
        "bibtex": "@inproceedings{NIPS2010_287e03db,\n author = {Su, Chang and Srihari, Sargur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Evaluation of Rarity of Fingerprints in Forensics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 468935,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9738833772238663953&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Engineering, University at Buffalo; Department of Computer Science and Engineering, University at Buffalo",
        "aff_domain": "buffalo.edu;buffalo.edu",
        "email": "buffalo.edu;buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University at Buffalo",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.buffalo.edu",
        "aff_unique_abbr": "UB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Buffalo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "33f5156eae",
        "title": "Evidence-Specific Structures for Rich Tractable CRFs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2ac2406e835bd49c70469acae337d292-Abstract.html",
        "author": "Anton Chechetka; Carlos Guestrin",
        "abstract": "We present a simple and effective approach to learning tractable conditional random fields with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efficient exact inference and exact parameter learning. At the same time, our algorithm does not suffer a large expressive power penalty inherent to fixed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup",
        "bibtex": "@inproceedings{NIPS2010_2ac2406e,\n author = {Chechetka, Anton and Guestrin, Carlos},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Evidence-Specific Structures for Rich Tractable CRFs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2ac2406e835bd49c70469acae337d292-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2ac2406e835bd49c70469acae337d292-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2ac2406e835bd49c70469acae337d292-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 403126,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13952036562544935894&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9979b179ad",
        "title": "Exact inference and learning for cumulative distribution functions on loopy graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/705f2172834666788607efbfca35afb3-Abstract.html",
        "author": "Nebojsa Jojic; Chris Meek; Jim C. Huang",
        "abstract": "Probabilistic graphical models use local factors to represent dependence among sets of variables. For many problem domains, for instance climatology and epidemiology, in addition to local dependencies, we may also wish to model heavy-tailed statistics, where extreme deviations should not be treated as outliers. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Currently, algorithms for inference and learning, which correspond to computing mixed derivatives, are exact only for tree-structured graphs. For graphs of arbitrary topology, an efficient algorithm is needed that takes advantage of the sparse structure of the model, unlike symbolic differentiation programs such as Mathematica and D* that do not. We present an algorithm for recursively decomposing the computation of derivatives for CDNs of arbitrary topology, where the decomposition is naturally described using junction trees. We compare the performance of the resulting algorithm to Mathematica and D*, and we apply our method to learning models for rainfall and H1N1 data, where we show that CDNs with cycles are able to provide a significantly better fits to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.",
        "bibtex": "@inproceedings{NIPS2010_705f2172,\n author = {Jojic, Nebojsa and Meek, Chris and Huang, Jim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exact inference and learning for cumulative distribution functions on loopy graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/705f2172834666788607efbfca35afb3-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/705f2172834666788607efbfca35afb3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/705f2172834666788607efbfca35afb3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/705f2172834666788607efbfca35afb3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 988068,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7638825773852881993&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a21321fe80",
        "title": "Exact learning curves for Gaussian process regression on large random graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/43baa6762fa81bb43b39c62553b2970d-Abstract.html",
        "author": "Matthew Urry; Peter Sollich",
        "abstract": "We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difficult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a finite number of others. The method is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations to the learning curve fail.",
        "bibtex": "@inproceedings{NIPS2010_43baa676,\n author = {Urry, Matthew and Sollich, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exact learning curves for Gaussian process regression on large random graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/43baa6762fa81bb43b39c62553b2970d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 337043,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2728820148881683151&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Mathematics, King\u2019s College London, London, WC2R 2LS, U.K.; Department of Mathematics, King\u2019s College London, London, WC2R 2LS, U.K.",
        "aff_domain": "kcl.ac.uk;kcl.ac.uk",
        "email": "kcl.ac.uk;kcl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "King\u2019s College London",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.kcl.ac.uk",
        "aff_unique_abbr": "KCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2bf7001292",
        "title": "Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/98dce83da57b0395e163467c9dae521b-Abstract.html",
        "author": "Alessandro Bergamo; Lorenzo Torresani",
        "abstract": "Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. The time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes. In order to address this shortcoming, in recent years several authors have proposed to learn object classifiers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. While this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to Web data.  In this paper we investigate and compare methods that learn image classifiers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. We cast this as a domain adaptation problem: given a few strongly-labeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classifiers yielding small generalization error on the target domain. Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces significant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classifiers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.",
        "bibtex": "@inproceedings{NIPS2010_98dce83d,\n author = {Bergamo, Alessandro and Torresani, Lorenzo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/98dce83da57b0395e163467c9dae521b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/98dce83da57b0395e163467c9dae521b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2419475,
        "gs_citation": 328,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12474932413621567594&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6007eb1f39",
        "title": "Extended Bayesian Information Criteria for Gaussian Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html",
        "author": "Rina Foygel; Mathias Drton",
        "abstract": "Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the asymptotic consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjuction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n.",
        "bibtex": "@inproceedings{NIPS2010_072b030b,\n author = {Foygel, Rina and Drton, Mathias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extended Bayesian Information Criteria for Gaussian Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1727354,
        "gs_citation": 1297,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15302928183615343982&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Chicago; University of Chicago",
        "aff_domain": "uchicago.edu;uchicago.edu",
        "email": "uchicago.edu;uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uchicago.edu",
        "aff_unique_abbr": "UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e4c2ab9dff",
        "title": "Extensions of Generalized Binary Search to Group Identification and Exponential Costs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html",
        "author": "Gowtham Bellala; Suresh Bhavnani; Clayton Scott",
        "abstract": "Generalized Binary Search (GBS) is a well known greedy algorithm for identifying an unknown object while minimizing the number of yes\" or \"no\" questions posed about that object, and arises in problems such as active learning and active diagnosis. Here, we provide a coding-theoretic interpretation for GBS and show that GBS can be viewed as a top-down algorithm that greedily minimizes the expected number of queries required to identify an object. This interpretation is then used to extend GBS in two ways. First, we consider the case where the objects are partitioned into groups, and the objective is to identify only the group to which the object belongs. Then, we consider the case where the cost of identifying an object grows exponentially in the number of queries. In each case, we present an exact formula for the objective function involving Shannon or Renyi entropy, and develop a greedy algorithm for minimizing it.\"",
        "bibtex": "@inproceedings{NIPS2010_5dd9db5e,\n author = {Bellala, Gowtham and Bhavnani, Suresh and Scott, Clayton},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extensions of Generalized Binary Search to Group Identification and Exponential Costs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 230802,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6880158478297238068&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of EECS, University of Michigan, Ann Arbor, MI 48109; Institute for Translational Sciences+Dept. of Preventative Medicine and Community Health, University of Texas Medical Branch, Galveston, TX 77555+School of Biomedical Informatics, University of Texas, Houston, TX 77030; Department of EECS, University of Michigan, Ann Arbor, MI 48109",
        "aff_domain": "umich.edu;gmail.com;umich.edu",
        "email": "umich.edu;gmail.com;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2+3;0",
        "aff_unique_norm": "University of Michigan;Institute for Translational Sciences;University of Texas Medical Branch;University of Texas Health Science Center at Houston",
        "aff_unique_dep": "Department of EECS;;Dept. of Preventative Medicine and Community Health;School of Biomedical Informatics",
        "aff_unique_url": "https://www.umich.edu;;https://utmb.edu;https://www.uth.edu/",
        "aff_unique_abbr": "UM;;UTMB;UTHealth",
        "aff_campus_unique_index": "0;2+3;0",
        "aff_campus_unique": "Ann Arbor;;Galveston;Houston",
        "aff_country_unique_index": "0;0+0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1d0d24216b",
        "title": "Factorized Latent Spaces with Structured Sparsity",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html",
        "author": "Yangqing Jia; Mathieu Salzmann; Trevor Darrell",
        "abstract": "Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multi-view learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow :having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation.",
        "bibtex": "@inproceedings{NIPS2010_a49e9411,\n author = {Jia, Yangqing and Salzmann, Mathieu and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Factorized Latent Spaces with Structured Sparsity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a49e9411d64ff53eccfdd09ad10a15b3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1544219,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18187615851447155207&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "UC Berkeley EECS and ICSI+TTI-Chicago; UC Berkeley EECS and ICSI+TTI-Chicago; UC Berkeley EECS and ICSI",
        "aff_domain": "eecs.berkeley.edu;ttic.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;ttic.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0",
        "aff_unique_norm": "University of California, Berkeley;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UC Berkeley;TTI",
        "aff_campus_unique_index": "0+1;0+1;0",
        "aff_campus_unique": "Berkeley;Chicago",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "89a7656101",
        "title": "Fast Large-scale Mixture Modeling with Component-specific Data Partitions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6cdd60ea0045eb7a6ec44c54d29ed402-Abstract.html",
        "author": "Bo Thiesson; Chong Wang",
        "abstract": "Remarkably easy implementation and guaranteed convergence has made the EM algorithm one of the most used algorithms for mixture modeling. On the downside, the E-step is linear in both the sample size and the number of mixture components, making it impractical for large-scale data. Based on the variational EM framework, we propose a fast alternative that uses component-specific data partitions to obtain a sub-linear E-step in sample size, while the algorithm still maintains provable convergence. Our approach builds on previous work, but is significantly faster and scales much better in the number of mixture components. We demonstrate this speedup by experiments on large-scale synthetic and real data.",
        "bibtex": "@inproceedings{NIPS2010_6cdd60ea,\n author = {Thiesson, Bo and Wang, Chong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Large-scale Mixture Modeling with Component-specific Data Partitions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 353929,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16570588982533552127&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Microsoft Research; Princeton University + Microsoft Research",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "Microsoft;Princeton University",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.princeton.edu",
        "aff_unique_abbr": "MSR;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b757bf5790",
        "title": "Fast detection of multiple change-points shared by many signals using group LARS",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/fe709c654eac84d5239d1a12a4f71877-Abstract.html",
        "author": "Jean-philippe Vert; Kevin Bleakley",
        "abstract": "We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results.",
        "bibtex": "@inproceedings{NIPS2010_fe709c65,\n author = {Vert, Jean-philippe and Bleakley, Kevin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast detection of multiple change-points shared by many signals using group LARS},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe709c654eac84d5239d1a12a4f71877-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/fe709c654eac84d5239d1a12a4f71877-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/fe709c654eac84d5239d1a12a4f71877-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 273473,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1871943941429767343&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Mines ParisTech CBIO + Institut Curie + INSERM U900; Mines ParisTech CBIO + Institut Curie + INSERM U900",
        "aff_domain": "mines-paristech.fr;mines-paristech.fr",
        "email": "mines-paristech.fr;mines-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+1+2",
        "aff_unique_norm": "MINES ParisTech;Institut Curie;Institut National de la Sant\u00e9 et de la Recherche M\u00e9dicale",
        "aff_unique_dep": "CBIO;;Unit\u00e9 900",
        "aff_unique_url": "https://www.minesparistech.fr;https://www.institut-curie.org;https://www.inserm.fr",
        "aff_unique_abbr": ";Institut Curie;INSERM",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "b278d2e7fb",
        "title": "Fast global convergence rates of gradient methods for high-dimensional statistical recovery",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7cce53cf90577442771720a370c3c723-Abstract.html",
        "author": "Alekh Agarwal; Sahand Negahban; Martin J. Wainwright",
        "abstract": "Many statistical $M$-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer.  We analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension $d$ to grow with (and possibly exceed) the sample size $n$.  This high-dimensional structure precludes the usual global assumptions---namely, strong convexity and smoothness conditions---that underlie classical optimization analysis.  We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models.  Under these conditions, our theory guarantees that Nesterov's first-order method~\\cite{Nesterov07} has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter $\\theta^*$ and the optimal solution $\\widehat{\\theta}$.  This globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates.  Our analysis applies to a wide range of $M$-estimators and statistical models, including sparse linear regression using Lasso ($\\ell_1$-regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization.  Overall, this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation.",
        "bibtex": "@inproceedings{NIPS2010_7cce53cf,\n author = {Agarwal, Alekh and Negahban, Sahand and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast global convergence rates of gradient methods for high-dimensional statistical recovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7cce53cf90577442771720a370c3c723-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7cce53cf90577442771720a370c3c723-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7cce53cf90577442771720a370c3c723-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 210355,
        "gs_citation": 451,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17055538718143268984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Electrical Engineering and Computer Science1 + Department of Statistics2; Department of Electrical Engineering and Computer Science1 + Department of Statistics2; Department of Electrical Engineering and Computer Science1 + Department of Statistics2",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "University of Michigan;University of California, Los Angeles",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Department of Statistics",
        "aff_unique_url": "https://www.eecs.umich.edu;https://www.ucla.edu",
        "aff_unique_abbr": "UM EECS;UCLA",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c7833bed31",
        "title": "Feature Construction for Inverse Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a8f15eda80c50adb0e71943adc8015cf-Abstract.html",
        "author": "Sergey Levine; Zoran Popovic; Vladlen Koltun",
        "abstract": "The goal of inverse reinforcement learning is to find a reward function for a Markov decision process, given example traces from its optimal policy. Current IRL techniques generally rely on user-supplied features that form a concise basis for the reward. We present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy. Given example traces, the algorithm returns a reward function as well as the constructed features. The reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well defined.",
        "bibtex": "@inproceedings{NIPS2010_a8f15eda,\n author = {Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feature Construction for Inverse Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a8f15eda80c50adb0e71943adc8015cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 922940,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12360810391514903480&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Stanford University; University of Washington; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.washington.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.washington.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.washington.edu",
        "aff_unique_abbr": "Stanford;UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d0f5d3370b",
        "title": "Feature Set Embedding for Incomplete Data",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/5f0f5e5f33945135b874349cfbed4fb9-Abstract.html",
        "author": "David Grangier; Iain Melvin",
        "abstract": "We present a new learning strategy for classification problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-specific subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classification strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then non-linearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the final classification objective. This simple strategy allows great flexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets.",
        "bibtex": "@inproceedings{NIPS2010_5f0f5e5f,\n author = {Grangier, David and Melvin, Iain},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feature Set Embedding for Incomplete Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/5f0f5e5f33945135b874349cfbed4fb9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 170070,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1858194033921004884&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "NEC Labs America, Princeton, NJ; NEC Labs America, Princeton, NJ",
        "aff_domain": "nec-labs.com;nec-labs.com",
        "email": "nec-labs.com;nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NEC Labs America",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nec-labs.com",
        "aff_unique_abbr": "NEC LA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "da4fcb3d7f",
        "title": "Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/20f07591c6fcb220ffe637cda29bb3f6-Abstract.html",
        "author": "Stella X. Yu",
        "abstract": "Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent flickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and find out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation.",
        "bibtex": "@inproceedings{NIPS2010_20f07591,\n author = {Yu, Stella},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/20f07591c6fcb220ffe637cda29bb3f6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/20f07591c6fcb220ffe637cda29bb3f6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/20f07591c6fcb220ffe637cda29bb3f6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 531383,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12801537566580179895&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, Boston College",
        "aff_domain": "bc.edu",
        "email": "bc.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Boston College",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.bostoncollege.edu",
        "aff_unique_abbr": "BC",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "162adc5ccf",
        "title": "Fractionally Predictive Spiking Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2dace78f80bc92e6d7493423d729448e-Abstract.html",
        "author": "Jaldert Rombouts; Sander M. Bohte",
        "abstract": "Recent experimental work has suggested that the neural firing rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron suffices to carry out such an approximation, given a suitable refractory response. Empirically, we find that the online approximation of signals with a sum of power-law kernels is beneficial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spike-trains by a receiving neuron allows for natural and transparent temporal signal filtering by tuning the weights of the decoding kernel.",
        "bibtex": "@inproceedings{NIPS2010_2dace78f,\n author = {Rombouts, Jaldert and Bohte, Sander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fractionally Predictive Spiking Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2dace78f80bc92e6d7493423d729448e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2087006,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15719549203861786126&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CWI, Life Sciences, Amsterdam, The Netherlands; CWI, Life Sciences, Amsterdam, The Netherlands",
        "aff_domain": "cwi.nl;cwi.nl",
        "email": "cwi.nl;cwi.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Centrum Wiskunde & Informatica",
        "aff_unique_dep": "Life Sciences",
        "aff_unique_url": "https://www.cwi.nl",
        "aff_unique_abbr": "CWI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amsterdam",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "23e564898d",
        "title": "Functional Geometry Alignment and Localization of Brain Areas",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/35f4a8d465e6e1edc05f3d8ab658c551-Abstract.html",
        "author": "Georg Langs; Yanmei Tie; Laura Rigolo; Alexandra Golby; Polina Golland",
        "abstract": "Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difficult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We first embed each brain into a functional map that reflects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions.",
        "bibtex": "@inproceedings{NIPS2010_35f4a8d4,\n author = {Langs, Georg and Tie, Yanmei and Rigolo, Laura and Golby, Alexandra and Golland, Polina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Functional Geometry Alignment and Localization of Brain Areas},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35f4a8d465e6e1edc05f3d8ab658c551-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/35f4a8d465e6e1edc05f3d8ab658c551-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/35f4a8d465e6e1edc05f3d8ab658c551-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6374225,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17620953569475488996&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology; Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology; Department of Neurosurgery, Brigham and Women\u2019s Hospital, Harvard Medical School; Department of Neurosurgery, Brigham and Women\u2019s Hospital, Harvard Medical School; Department of Neurosurgery, Brigham and Women\u2019s Hospital, Harvard Medical School",
        "aff_domain": "csail.mit.edu;csail.mit.edu;bwh.harvard.edu;bwh.harvard.edu;bwh.harvard.edu",
        "email": "csail.mit.edu;csail.mit.edu;bwh.harvard.edu;bwh.harvard.edu;bwh.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Harvard Medical School",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Lab;Department of Neurosurgery",
        "aff_unique_url": "https://www.csail.mit.edu;https://hms.harvard.edu",
        "aff_unique_abbr": "MIT;HMS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d007fc0136",
        "title": "Functional form of motion priors in human motion perception",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/f4552671f8909587cf485ea990207f3b-Abstract.html",
        "author": "Hongjing Lu; Tungyou Lin; Alan Lee; Luminita Vese; Alan L. Yuille",
        "abstract": "It has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal, or rational, manner. The basic goal of our work is to discover experimentally which prior distribution is used. More specifically, we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks. We restricted ourselves to priors which combine three terms for motion slowness, first-order smoothness, and second-order smoothness. We focused on two functional forms for prior distributions: L2-norm and L1-norm regularization corresponding to the Gaussian and Laplace distributions respectively. In our first experimental session we estimate the weights of the three terms for each functional form to maximize the fit to human performance. We then measured human performance for motion tasks and found that we obtained better fit for the L1-norm (Laplace) than for the L2-norm (Gaussian). We note that the L1-norm is also a better fit to the statistics of motion in natural environments. In addition, we found large weights for the second-order smoothness term, indicating the importance of high-order smoothness compared to slowness and lower-order smoothness. To validate our results further, we used the best fit models using the L1-norm to predict human performance in a second session with different experimental setups. Our results showed excellent agreement between human performance and model prediction -- ranging from 3\\% to 8\\% for five human subjects over ten experimental conditions -- and give further support that the human visual system uses an L1-norm (Laplace) prior.",
        "bibtex": "@inproceedings{NIPS2010_f4552671,\n author = {Lu, Hongjing and Lin, Tungyou and Lee, Alan and Vese, Luminita and Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Functional form of motion priors in human motion perception},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f4552671f8909587cf485ea990207f3b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/f4552671f8909587cf485ea990207f3b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/f4552671f8909587cf485ea990207f3b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 371516,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3412427365536098519&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Psychology; Department of Mathematics; Department of Psychology; Department of Mathematics; Department of Psychology",
        "aff_domain": "ucla.edu;math.ucla.edu;ucla.edu;math.ucla.edu;stat.ucla.edu",
        "email": "ucla.edu;math.ucla.edu;ucla.edu;math.ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "University Affiliation Not Specified;Mathematics Department",
        "aff_unique_dep": "Department of Psychology;Department of Mathematics",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "311995190c",
        "title": "Gated Softmax Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html",
        "author": "Roland Memisevic; Christopher Zach; Marc Pollefeys; Geoffrey E. Hinton",
        "abstract": "We describe a log-bilinear\" model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables. Even though the latent variables can take on exponentially many possible combinations of values, we can efficiently compute the exact probability of each class by marginalizing over the latent variables. This makes it possible to get the exact gradient of the log likelihood. The bilinear score-functions are defined using a three-dimensional weight tensor, and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions. Experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) SVMs, backpropagation, and deep belief nets.\"",
        "bibtex": "@inproceedings{NIPS2010_5737c6ec,\n author = {Memisevic, Roland and Zach, Christopher and Pollefeys, Marc and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gated Softmax Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 424924,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7290199501038970759&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, University of Toronto, Canada; Department of Computer Science, ETH Zurich, Switzerland",
        "aff_domain": "gmail.com;inf.ethz.ch;cs.toronto.edu;inf.ethz.ch",
        "email": "gmail.com;inf.ethz.ch;cs.toronto.edu;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "ETH Zurich;University of Toronto",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch;https://www.utoronto.ca",
        "aff_unique_abbr": "ETHZ;U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Switzerland;Canada"
    },
    {
        "id": "9dbb996ac0",
        "title": "Gaussian Process Preference Elicitation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/46922a0880a8f11f8f69cbb52b1396be-Abstract.html",
        "author": "Shengbo Guo; Scott Sanner; Edwin V. Bonilla",
        "abstract": "Bayesian approaches to preference elicitation (PE) are particularly attractive due to their ability to explicitly model uncertainty in users' latent utility functions. However, previous approaches to Bayesian PE have ignored the important problem of generalizing from previous users to an unseen user in order to reduce the elicitation burden on new users. In this paper, we address this deficiency by introducing a Gaussian Process (GP) prior over users' latent utility functions on the joint space of user and item features. We learn the hyper-parameters of this GP on a set of preferences of previous users and use it to aid in the elicitation process for a new user. This approach provides a flexible model of a multi-user utility function, facilitates an efficient value of information (VOI) heuristic query selection strategy, and provides a principled way to incorporate the elicitations of multiple users back into the model. We show the effectiveness of our method in comparison to previous work on a real dataset of user preferences over sushi types.",
        "bibtex": "@inproceedings{NIPS2010_46922a08,\n author = {Guo, Shengbo and Sanner, Scott and Bonilla, Edwin V},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian Process Preference Elicitation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/46922a0880a8f11f8f69cbb52b1396be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 259729,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12981624102657387091&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "NICTA & ANU, Locked Bag 8001, Canberra ACT 2601, Australia; NICTA & ANU, Locked Bag 8001, Canberra ACT 2601, Australia; NICTA & ANU, Locked Bag 8001, Canberra ACT 2601, Australia",
        "aff_domain": "nicta.com.au;nicta.com.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "7cd3d0ccfa",
        "title": "Gaussian sampling by local perturbations",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d09bf41544a3365a46c9077ebb5e35c3-Abstract.html",
        "author": "George Papandreou; Alan L. Yuille",
        "abstract": "We present a technique for exact simulation of Gaussian Markov random fields (GMRFs), which can be interpreted as locally injecting noise to each Gaussian factor independently, followed by computing the mean/mode of the perturbed GMRF. Coupled with standard iterative techniques for the solution of symmetric positive definite systems, this yields a very efficient sampling algorithm with essentially linear complexity in terms of speed and memory requirements, well suited to extremely large scale probabilistic models. Apart from synthesizing data under a Gaussian model, the proposed technique directly leads to an efficient unbiased estimator of marginal variances. Beyond Gaussian models, the proposed algorithm is also very useful for handling highly non-Gaussian continuously-valued MRFs such as those arising in statistical image modeling or in the first layer of deep belief networks describing real-valued data, where the non-quadratic potentials coupling different sites can be represented as finite or infinite mixtures of Gaussians with the help of local or distributed latent mixture assignment variables. The Bayesian treatment of such models most naturally involves a block Gibbs sampler which alternately draws samples of the conditionally independent latent mixture assignments and the conditionally multivariate Gaussian continuous vector and we show that it can directly benefit from the proposed methods.",
        "bibtex": "@inproceedings{NIPS2010_d09bf415,\n author = {Papandreou, George and Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian sampling by local perturbations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d09bf41544a3365a46c9077ebb5e35c3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 813431,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8517363946126665003&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, University of California, Los Angeles; Depts. of Statistics, Computer Science & Psychology, University of California, Los Angeles",
        "aff_domain": "stat.ucla.edu;stat.ucla.edu",
        "email": "stat.ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7abd4a6d6a",
        "title": "Generalized roof duality and bisubmodular functions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html",
        "author": "Vladimir Kolmogorov",
        "abstract": "Consider a convex relaxation $\\hat f$ of a pseudo-boolean function $f$. We say that the relaxation is {\\em totally half-integral} if $\\hat f(\\bx)$ is a polyhedral function with half-integral extreme points $\\bx$, and this property is preserved after adding an arbitrary combination of constraints of the form $x_i=x_j$, $x_i=1-x_j$, and $x_i=\\gamma$ where $\\gamma\\in\\{0,1,\\frac{1}{2}\\}$ is a constant. A well-known example is the {\\em roof duality} relaxation for quadratic pseudo-boolean functions $f$. We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions.  Our contributions are as follows. First, we provide a complete characterization of totally half-integral relaxations $\\hat f$ by establishing a one-to-one correspondence with {\\em bisubmodular functions}. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality.",
        "bibtex": "@inproceedings{NIPS2010_093f65e0,\n author = {Kolmogorov, Vladimir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalized roof duality and bisubmodular functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/093f65e080a295f8076b1c5722a46aa2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 305244,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=837773378432428924&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University College London, UK",
        "aff_domain": "cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "fa90628182",
        "title": "Generating more realistic images using gated MRF's",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6e7d2da6d3953058db75714ac400b584-Abstract.html",
        "author": "Marc'aurelio Ranzato; Volodymyr Mnih; Geoffrey E. Hinton",
        "abstract": "Probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks, such as denoising and inpainting. A more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images. This method is seldom used with high-resolution images, because current models produce samples that are very different from natural images, as assessed by even simple visual inspection. We investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables, one set modelling pixel intensities and the other set modelling image-specific pixel covariances, we are able to generate high-resolution images that look much more realistic than before. The overall model can be interpreted as a gated MRF where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables. Finally, we confirm that if we disallow weight-sharing between receptive fields that overlap each other, the gated MRF learns more efficient internal representations, as demonstrated in several recognition tasks.",
        "bibtex": "@inproceedings{NIPS2010_6e7d2da6,\n author = {Ranzato, Marc\\textquotesingle aurelio and Mnih, Volodymyr and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generating more realistic images using gated MRF\\textquotesingle s},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6e7d2da6d3953058db75714ac400b584-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1114428,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1600711958642974704&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science; Department of Computer Science; Department of Computer Science",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Unknown Institution",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fa6f4dc511",
        "title": "Generative Local Metric Learning for Nearest Neighbor Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html",
        "author": "Yung-kyun Noh; Byoung-tak Zhang; Daniel D. Lee",
        "abstract": "We consider the problem of learning a local metric to enhance the performance of nearest neighbor classification. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from finite sampling effects, and find an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct, the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction, which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models.",
        "bibtex": "@inproceedings{NIPS2010_01386bd6,\n author = {Noh, Yung-kyun and Zhang, Byoung-tak and Lee, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generative Local Metric Learning for Nearest Neighbor Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/01386bd6d8e091c2ab4c7c7de644d37b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 535849,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9408460730402121045&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "GRASP Lab, University of Pennsylvania, Philadelphia, PA 19104, USA+Biointelligence Lab, Seoul National University, Seoul 151-742, Korea; Biointelligence Lab, Seoul National University, Seoul 151-742, Korea; GRASP Lab, University of Pennsylvania, Philadelphia, PA 19104, USA",
        "aff_domain": "seas.upenn.edu;snu.ac.kr;seas.upenn.edu",
        "email": "seas.upenn.edu;snu.ac.kr;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "University of Pennsylvania;Seoul National University",
        "aff_unique_dep": "GRASP Lab;Biointelligence Lab",
        "aff_unique_url": "https://www.upenn.edu;https://www.snu.ac.kr",
        "aff_unique_abbr": "UPenn;SNU",
        "aff_campus_unique_index": "0+1;1;0",
        "aff_campus_unique": "Philadelphia;Seoul",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "22afb3c061",
        "title": "Getting lost in space: Large sample analysis of the resistance distance",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0d0871f0806eae32d30983b62252da50-Abstract.html",
        "author": "Ulrike V. Luxburg; Agnes Radl; Matthias Hein",
        "abstract": "The commute distance between two vertices in a graph is the expected   time it takes a random walk to travel from the first to the second   vertex and back. We study the behavior of the commute distance as the size of the underlying graph   increases. We prove that the commute distance converges to an   expression that does not take into account the structure of the   graph at all and that is completely meaningless as a distance   function on the graph. Consequently, the use of the raw commute   distance for machine learning purposes is strongly discouraged for   large graphs and in high dimensions. As an alternative we introduce   the amplified commute distance that corrects for   the undesired large sample effects.",
        "bibtex": "@inproceedings{NIPS2010_0d0871f0,\n author = {Luxburg, Ulrike and Radl, Agnes and Hein, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Getting lost in space: Large sample analysis of the resistance distance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0871f0806eae32d30983b62252da50-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0d0871f0806eae32d30983b62252da50-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/0d0871f0806eae32d30983b62252da50-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0d0871f0806eae32d30983b62252da50-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 810492,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12688336859732702395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany; Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany; Saarland University, Saarbr\u00fccken, Germany",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;cs.uni-sb.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;cs.uni-sb.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Saarland University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;https://www.uni-saarland.de",
        "aff_unique_abbr": "MPIBC;UdS",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "T\u00fcbingen;Saarbr\u00fccken",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "755d55f751",
        "title": "Global Analytic Solution for Variational Bayesian Matrix Factorization",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/818f4654ed39a1c147d1e51a00ffb4cb-Abstract.html",
        "author": "Shinichi Nakajima; Masashi Sugiyama; Ryota Tomioka",
        "abstract": "Bayesian methods of matrix factorization (MF) have been actively explored recently as promising alternatives to classical singular value decomposition. In this paper, we show that, despite the fact that the optimization problem is non-convex, the global optimal solution of variational Bayesian (VB) MF can be computed analytically by solving a quartic equation. This is highly advantageous over a popular VBMF algorithm based on iterated conditional modes since it can only find a local optimal solution after iterations. We further show that the global optimal solution of empirical VBMF (hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments.",
        "bibtex": "@inproceedings{NIPS2010_818f4654,\n author = {Nakajima, Shinichi and Sugiyama, Masashi and Tomioka, Ryota},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Global Analytic Solution for Variational Bayesian Matrix Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/818f4654ed39a1c147d1e51a00ffb4cb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 239951,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4345234161299236481&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Nikon Corporation, Tokyo, 140-8601, Japan; Tokyo Institute of Technology, Tokyo 152-8552, Japan; The University of Tokyo, Tokyo 113-8685, Japan",
        "aff_domain": "nikon.co.jp;cs.titech.ac.jp;mist.i.u-tokyo.ac.jp",
        "email": "nikon.co.jp;cs.titech.ac.jp;mist.i.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Nikon Corporation;Tokyo Institute of Technology;University of Tokyo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nikon.com;https://www.titech.ac.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "Nikon;Titech;UTokyo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "7ace0c2498",
        "title": "Global seismic monitoring as probabilistic inference",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html",
        "author": "Nimar Arora; Stuart Russell; Paul Kidwell; Erik B. Sudderth",
        "abstract": "The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the first stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits significantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output.",
        "bibtex": "@inproceedings{NIPS2010_cfbce4c1,\n author = {Arora, Nimar and Russell, Stuart J and Kidwell, Paul and Sudderth, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Global seismic monitoring as probabilistic inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/cfbce4c1d7c425baf21d6b6f2babe6be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 849223,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5570401271532769062&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "Department of Computer Science, University of California, Berkeley; Department of Computer Science, University of California, Berkeley; Lawrence Livermore National Lab; Department of Computer Science, Brown University",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;llnl.gov;cs.brown.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;llnl.gov;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of California, Berkeley;Lawrence Livermore National Laboratory;Brown University",
        "aff_unique_dep": "Department of Computer Science;;Department of Computer Science",
        "aff_unique_url": "https://www.berkeley.edu;https://www.llnl.gov;https://www.brown.edu",
        "aff_unique_abbr": "UC Berkeley;LLNL;Brown",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9a0028ea8c",
        "title": "Graph-Valued Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/821fa74b50ba3f7cba1e6c53e8fa6845-Abstract.html",
        "author": "Han Liu; Xi Chen; Larry Wasserman; John D. Lafferty",
        "abstract": "Undirected graphical models encode in a graph $G$ the dependency structure of a random vector $Y$. In many applications, it is of interest to model $Y$ given another random vector $X$ as input. We refer to the problem of estimating the graph $G(x)$ of $Y$ conditioned on $X=x$ as ``graph-valued regression''. In this paper, we propose a semiparametric method for estimating $G(x)$ that builds a tree on the $X$ space just as in CART (classification and regression trees), but at each leaf of the tree estimates a graph. We call the method ``Graph-optimized CART'', or Go-CART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data.",
        "bibtex": "@inproceedings{NIPS2010_821fa74b,\n author = {Liu, Han and Chen, Xi and Wasserman, Larry and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Graph-Valued Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 840347,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=806309096326589277&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7c9217bc29",
        "title": "Group Sparse Coding with a Laplacian Scale Mixture Prior",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2d6cc4b2d139a53512fb8cbb3086ae2e-Abstract.html",
        "author": "Pierre Garrigues; Bruno A. Olshausen",
        "abstract": "We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefficients. Each coefficient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efficient inference procedures for both the coefficients and the scale parameter. When the scale parameters of a group of coefficients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude fluctuations among coefficients, which have been shown to constitute a large fraction of the redundancy in natural images. We show that, as a consequence of this group sparse coding, the resulting inference of the coefficients follows a divisive normalization rule, and that this may be efficiently implemented a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model.",
        "bibtex": "@inproceedings{NIPS2010_2d6cc4b2,\n author = {Garrigues, Pierre and Olshausen, Bruno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Group Sparse Coding with a Laplacian Scale Mixture Prior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 766333,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3973966106250556069&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IQ Engines, Inc., Berkeley, CA 94704; Helen Wills Neuroscience Institute, School of Optometry, University of California, Berkeley, Berkeley, CA 94720",
        "aff_domain": "gmail.com;berkeley.edu",
        "email": "gmail.com;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IQ Engines, Inc.;University of California, Berkeley",
        "aff_unique_dep": ";Helen Wills Neuroscience Institute, School of Optometry",
        "aff_unique_url": ";https://www.berkeley.edu",
        "aff_unique_abbr": ";UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5129d974fe",
        "title": "Guaranteed Rank Minimization via Singular Value Projection",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/08d98638c6fcd194a4b1e6992063e944-Abstract.html",
        "author": "Prateek Jain; Raghu Meka; Inderjit S. Dhillon",
        "abstract": "Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under affine constraints ARMP and show that SVP recovers the minimum rank solution for affine constraints that satisfy a Restricted Isometry Property} (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of low-rank matrix completion, for which the defining affine constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank Incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of \\cite{CaiCS2008,LeeB2009b, KeshavanOM2009}, for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is significantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem.",
        "bibtex": "@inproceedings{NIPS2010_08d98638,\n author = {Jain, Prateek and Meka, Raghu and Dhillon, Inderjit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Guaranteed Rank Minimization via Singular Value Projection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 154090,
        "gs_citation": 599,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15866575613276957986&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Microsoft Research Bangalore, Bangalore, India; UT Austin Dept. of Computer Sciences, Austin, TX, USA; UT Austin Dept. of Computer Sciences, Austin, TX, USA",
        "aff_domain": "microsoft.com;cs.utexas.edu;cs.utexas.edu",
        "email": "microsoft.com;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Microsoft;University of Texas at Austin",
        "aff_unique_dep": "Microsoft Research;Department of Computer Sciences",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india;https://www.utexas.edu",
        "aff_unique_abbr": "MSR India;UT Austin",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Bangalore;Austin",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "f3283e7002",
        "title": "Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d2ed45a52bc0edfa11c2064e9edee8bf-Abstract.html",
        "author": "Peggy Series; David P. Reichert; Amos J. Storkey",
        "abstract": "The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative findings in CBS. We also introduce a modification to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception.",
        "bibtex": "@inproceedings{NIPS2010_d2ed45a5,\n author = {Series, Peggy and Reichert, David and Storkey, Amos J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d2ed45a52bc0edfa11c2064e9edee8bf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 470925,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8085514418690247013&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;inf.ed.ac.uk;ed.ac.uk",
        "email": "sms.ed.ac.uk;inf.ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "1503998442",
        "title": "Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/470e7a4f017a5476afb7eeb3f8b96f9b-Abstract.html",
        "author": "Prateek Jain; Sudheendra Vijayanarasimhan; Kristen Grauman",
        "abstract": "We consider the problem of retrieving the database points nearest to a given {\\em hyperplane} query without exhaustively scanning the database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method's preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods' tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points.",
        "bibtex": "@inproceedings{NIPS2010_470e7a4f,\n author = {Jain, Prateek and Vijayanarasimhan, Sudheendra and Grauman, Kristen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 436353,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16510374277732042930&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Microsoft Research, Bangalore, India; University of Texas at Austin; University of Texas at Austin",
        "aff_domain": "microsoft.com;cs.utexas.edu;cs.utexas.edu",
        "email": "microsoft.com;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Microsoft;University of Texas at Austin",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india;https://www.utexas.edu",
        "aff_unique_abbr": "MSR;UT Austin",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Bangalore;Austin",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "82fdc62274",
        "title": "Heavy-Tailed Process Priors for Selective Shrinkage",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8d317bdcf4aafcfc22149d77babee96d-Abstract.html",
        "author": "Fabian L. Wauthier; Michael I. Jordan",
        "abstract": "Heavy-tailed distributions are often used to enhance the robustness of regression and classification methods to outliers in output space.  Often, however, we are confronted with ``outliers'' in input space, which are isolated observations in sparsely populated regions. We show that heavy-tailed process priors (which we construct from Gaussian processes via a copula), can be used to improve robustness of regression and classification estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs provided the marginals of the heavy-tailed process have sufficiently heavy tails. The analysis is complemented by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions.",
        "bibtex": "@inproceedings{NIPS2010_8d317bdc,\n author = {Wauthier, Fabian L and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Heavy-Tailed Process Priors for Selective Shrinkage},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8d317bdcf4aafcfc22149d77babee96d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 367688,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18079786019956866619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "39a93707a5",
        "title": "Humans Learn Using Manifolds, Reluctantly",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/996009f2374006606f4c0b0fda878af1-Abstract.html",
        "author": "Tim Rogers; Chuck Kalish; Joseph Harrison; Xiaojin Zhu; Bryan R. Gibson",
        "abstract": "When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classification in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human's ability to use a manifold in a semi-supervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary.",
        "bibtex": "@inproceedings{NIPS2010_996009f2,\n author = {Rogers, Tim and Kalish, Chuck and Harrison, Joseph and Zhu, Jerry and Gibson, Bryan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Humans Learn Using Manifolds, Reluctantly},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/996009f2374006606f4c0b0fda878af1-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/996009f2374006606f4c0b0fda878af1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/996009f2374006606f4c0b0fda878af1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 364504,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4817962668489609786&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Sciences; Department of Computer Sciences; Psychology + Educational Psychology; Educational Psychology; Psychology",
        "aff_domain": "cs.wisc.edu;cs.wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "email": "cs.wisc.edu;cs.wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2;2;1",
        "aff_unique_norm": "University of Wisconsin-Madison;Psychology Department;Educational Psychology",
        "aff_unique_dep": "Department of Computer Sciences;Department of Psychology;Department of Educational Psychology",
        "aff_unique_url": "https://www.cs.wisc.edu;;",
        "aff_unique_abbr": "UW-Madison;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;",
        "aff_country_unique": "United States;"
    },
    {
        "id": "e06a7a1f4c",
        "title": "Identifying Dendritic Processing",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/35051070e572e47d2c26c241ab88307f-Abstract.html",
        "author": "Aurel A. Lazar; Yevgeniy Slutskiy",
        "abstract": "In system identification both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing filter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identification of the dendritic processing filter and reconstruct its kernel with arbitrary precision.",
        "bibtex": "@inproceedings{NIPS2010_35051070,\n author = {Lazar, Aurel A and Slutskiy, Yevgeniy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identifying Dendritic Processing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35051070e572e47d2c26c241ab88307f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/35051070e572e47d2c26c241ab88307f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/35051070e572e47d2c26c241ab88307f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 983863,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4818882485090385652&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical Engineering, Columbia University; Department of Electrical Engineering, Columbia University",
        "aff_domain": "ee.columbia.edu;columbia.edu",
        "email": "ee.columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b2b3a41ffc",
        "title": "Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6f3ef77ac0e3619e98159e9b6febf557-Abstract.html",
        "author": "Zeeshan Syed; John V. Guttag",
        "abstract": "Cardiovascular disease is the leading cause of death globally, resulting in 17 million deaths each year. Despite the availability of various treatment options, existing techniques based upon conventional medical knowledge often fail to identify patients who might have benefited from more aggressive therapy. In this paper, we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratification. The key idea of our approach is to avoid specialized medical knowledge, and assess patient risk using symbolic mismatch, a new metric to assess similarity in long-term time-series activity. We hypothesize that high risk patients can be identified using symbolic mismatch, as individuals in a population with unusual long-term physiological activity. We describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks. We first describe how to compute the symbolic mismatch between pairs of long term electrocardiographic (ECG) signals. This algorithm maps the original signals into a symbolic domain, and provides a quantitative assessment of the difference between these symbolic representations of the original signals. We then show how this measure can be used with each of a one-class SVM, a nearest neighbor classifier, and hierarchical clustering to improve risk stratification. We evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data. In a univariate analysis, all of the methods provided a statistically significant association with the occurrence of a major adverse cardiac event in the next 90 days. In a multivariate analysis that incorporated the most widely used clinical risk variables, the nearest neighbor and hierarchical clustering approaches were able to statistically significantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days.",
        "bibtex": "@inproceedings{NIPS2010_6f3ef77a,\n author = {Syed, Zeeshan and Guttag, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6f3ef77ac0e3619e98159e9b6febf557-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 86576,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3654968695269480877&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Michigan; Massachusetts Institute of Technology",
        "aff_domain": "eecs.umich.edu;csail.mit.edu",
        "email": "eecs.umich.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Michigan;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://web.mit.edu",
        "aff_unique_abbr": "UM;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "80f5646634",
        "title": "Identifying graph-structured activation patterns in networks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/1cc3633c579a90cfdd895e64021e2163-Abstract.html",
        "author": "James Sharpnack; Aarti Singh",
        "abstract": "We consider the problem of identifying an activation pattern in a complex, large-scale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of high-dimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size.",
        "bibtex": "@inproceedings{NIPS2010_1cc3633c,\n author = {Sharpnack, James and Singh, Aarti},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identifying graph-structured activation patterns in networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1cc3633c579a90cfdd895e64021e2163-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/1cc3633c579a90cfdd895e64021e2163-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/1cc3633c579a90cfdd895e64021e2163-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 532867,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10320287735731912201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Machine Learning Department, Statistics Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cmu.edu",
        "email": "andrew.cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "23cb2b284a",
        "title": "Implicit Differentiation by Perturbation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6ecbdd6ec859d284dc13885a37ce8d81-Abstract.html",
        "author": "Justin Domke",
        "abstract": "This paper proposes a simple and efficient finite difference method for implicit differentiation of marginal inference results in discrete graphical models. Given an arbitrary loss function, defined on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters. This method can be used with approximate inference, with a loss function over approximate marginals. Convenient choices of loss functions make it practical to fit graphical models with hidden variables, high treewidth and/or model misspecification.",
        "bibtex": "@inproceedings{NIPS2010_6ecbdd6e,\n author = {Domke, Justin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Implicit Differentiation by Perturbation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6ecbdd6ec859d284dc13885a37ce8d81-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 484501,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14743792585637358550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Rochester Institute of Technology",
        "aff_domain": "rit.edu",
        "email": "rit.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Rochester Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rit.edu",
        "aff_unique_abbr": "RIT",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c33ec62202",
        "title": "Implicit encoding of prior probabilities in optimal neural populations",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/f9a40a4780f5e1306c46f1c8daecee3b-Abstract.html",
        "author": "Deep Ganguli; Eero P. Simoncelli",
        "abstract": "Optimal coding provides a guiding principle for understanding the representation of sensory variables in neural populations. Here we consider the influence of a prior probability distribution over sensory variables on the optimal allocation of cells and spikes in a neural population. We model the spikes of each cell as samples from an independent Poisson process with rate governed by an associated tuning curve. For this response model, we approximate the Fisher information in terms of the density and amplitude of the tuning curves, under the assumption that tuning width varies inversely with cell density. We consider a family of objective functions based on the expected value, over the sensory prior, of a functional of the Fisher information. This family includes lower bounds on mutual information and perceptual discriminability as special cases. In all cases, we find a closed form expression for the optimum, in which the density and gain of the cells in the population are power law functions of the stimulus prior. This also implies a power law relationship between the prior and perceptual discriminability. We show preliminary evidence that the theory successfully predicts the relationship between empirically measured stimulus priors, physiologically measured neural response properties (cell density, tuning widths, and firing rates), and psychophysically measured discrimination thresholds.",
        "bibtex": "@inproceedings{NIPS2010_f9a40a47,\n author = {Ganguli, Deep and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Implicit encoding of prior probabilities in optimal neural populations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/f9a40a4780f5e1306c46f1c8daecee3b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 175825,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4600477450236317335&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Howard Hughes Medical Institute + Center for Neural Science, New York University; Howard Hughes Medical Institute + Center for Neural Science, New York University",
        "aff_domain": "cns.nyu.edu;cns.nyu.edu",
        "email": "cns.nyu.edu;cns.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Howard Hughes Medical Institute;New York University",
        "aff_unique_dep": ";Center for Neural Science",
        "aff_unique_url": "https://www.hhmi.org;https://www.nyu.edu",
        "aff_unique_abbr": "HHMI;NYU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d9b75e252",
        "title": "Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/115f89503138416a242f40fb7d7f338e-Abstract.html",
        "author": "Mathieu Salzmann; Raquel Urtasun",
        "abstract": "Estimating 3D pose from monocular images is a highly ambiguous problem. Physical constraints can be exploited to restrict the space of feasible configurations. In this paper we propose an approach to constraining the prediction of a discriminative predictor. We first show that the mean prediction of a Gaussian process implicitly satisfies linear constraints if those constraints are satisfied by the training examples. We then show how, by performing a change of variables, a GP can be forced to satisfy quadratic constraints. As evidenced by the experiments, our method outperforms state-of-the-art approaches on the tasks of rigid and non-rigid pose estimation.",
        "bibtex": "@inproceedings{NIPS2010_115f8950,\n author = {Salzmann, Mathieu and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/115f89503138416a242f40fb7d7f338e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/115f89503138416a242f40fb7d7f338e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/115f89503138416a242f40fb7d7f338e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3384292,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10202696049915510272&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "ICSI & EECS, UC Berkeley + TTI Chicago; TTI Chicago",
        "aff_domain": "ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "University of California, Berkeley;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UC Berkeley;TTI",
        "aff_campus_unique_index": "0+1;1",
        "aff_campus_unique": "Berkeley;Chicago",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f1abcff44e",
        "title": "Improvements to the Sequence Memoizer",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html",
        "author": "Jan Gasthaus; Yee W. Teh",
        "abstract": "The sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efficient representation, and inference algorithms operating on the new representation. Our derivations are based on precise definitions of the various processes that will also allow us to provide an elementary proof of the mysterious\" coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements.\"",
        "bibtex": "@inproceedings{NIPS2010_6cfe0e61,\n author = {Gasthaus, Jan and Teh, Yee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improvements to the Sequence Memoizer},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 226499,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18407286650868329585&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "66e61ac025",
        "title": "Improving Human Judgments by Decontaminating Sequential Dependencies",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/621461af90cadfdaf0e8d4cc25129f91-Abstract.html",
        "author": "Michael Mozer; Harold Pashler; Matthew Wilder; Robert Lindsey; Matt Jones; Michael N. Jones",
        "abstract": "For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are influenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random field with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments.",
        "bibtex": "@inproceedings{NIPS2010_621461af,\n author = {Mozer, Michael C and Pashler, Harold and Wilder, Matthew and Lindsey, Robert V and Jones, Matt and Jones, Michael N},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improving Human Judgments by Decontaminating Sequential Dependencies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/621461af90cadfdaf0e8d4cc25129f91-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 167238,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7234535363055084057&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Dept. of Computer Science, University of Colorado; Dept. of Psychology, UCSD; Dept. of Computer Science, University of Colorado; Dept. of Computer Science, University of Colorado; Dept. of Psychology, University of Colorado; Dept. of Psychological and Brain Sciences, Indiana University",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0;2",
        "aff_unique_norm": "University of Colorado;University of California, San Diego;Indiana University",
        "aff_unique_dep": "Dept. of Computer Science;Department of Psychology;Dept. of Psychological and Brain Sciences",
        "aff_unique_url": "https://www.colorado.edu;https://www.ucsd.edu;https://www.indiana.edu",
        "aff_unique_abbr": "CU;UCSD;IU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2b3263595d",
        "title": "Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/819f46e52c25763a55cc642422644317-Abstract.html",
        "author": "Yi Sun; J\u00fcrgen Schmidhuber; Faustino J. Gomez",
        "abstract": "We present a new way of converting a reversible finite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result confirms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC.",
        "bibtex": "@inproceedings{NIPS2010_819f46e5,\n author = {Sun, Yi and Schmidhuber, J\\\"{u}rgen and Gomez, Faustino},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/819f46e52c25763a55cc642422644317-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/819f46e52c25763a55cc642422644317-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/819f46e52c25763a55cc642422644317-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 764124,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12140866992335504585&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "IDSIA; IDSIA; IDSIA",
        "aff_domain": "idsia.ch;idsia.ch;idsia.ch",
        "email": "idsia.ch;idsia.ch;idsia.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute of Digital Technologies",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idsia.ch",
        "aff_unique_abbr": "IDSIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "49753e1bde",
        "title": "Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/ccb1d45fb76f7c5a0bf619f979c6cf36-Abstract.html",
        "author": "Kaiming Li; Lei Guo; Carlos Faraco; Dajiang Zhu; Fan Deng; Tuo Zhang; Xi Jiang; Degang Zhang; Hanbo Chen; Xintao Hu; Steve Miller; Tianming Liu",
        "abstract": "Functional segregation and integration are fundamental characteristics of the human brain. Studying the connectivity among segregated regions and the dynamics of integrated brain networks has drawn increasing interest. A very controversial, yet fundamental issue in these studies is how to determine the best functional brain regions or ROIs (regions of interests) for individuals. Essentially, the computed connectivity patterns and dynamics of brain networks are very sensitive to the locations, sizes, and shapes of the ROIs. This paper presents a novel methodology to optimize the locations of an individual's ROIs in the working memory system. Our strategy is to formulate the individual ROI optimization as a group variance minimization problem, in which group-wise functional and structural connectivity patterns, and anatomic profiles are defined as optimization constraints. The optimization problem is solved via the simulated annealing approach. Our experimental results show that the optimized ROIs have significantly improved consistency in structural and functional profiles across subjects, and have more reasonable localizations and more consistent morphological and anatomic profiles.",
        "bibtex": "@inproceedings{NIPS2010_ccb1d45f,\n author = {Li, Kaiming and Guo, Lei and Faraco, Carlos and Zhu, Dajiang and Deng, Fan and Zhang, Tuo and Jiang, Xi and Zhang, Degang and Chen, Hanbo and Hu, Xintao and Miller, Steve and Liu, Tianming},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1282828,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1238603657020823509&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Automation, Northwestern Polytechnical University, China; Department of Computer Science, the University of Georgia, USA; Department of Psychology, the University of Georgia, USA; Department of Computer Science, the University of Georgia, USA; Department of Computer Science, the University of Georgia, USA; School of Automation, Northwestern Polytechnical University, China; School of Automation, Northwestern Polytechnical University, China; School of Automation, Northwestern Polytechnical University, China; School of Automation, Northwestern Polytechnical University, China; School of Automation, Northwestern Polytechnical University, China; Department of Psychology, the University of Georgia, USA; Department of Computer Science, the University of Georgia, USA",
        "aff_domain": "gmail.com; ; ; ; ; ; ; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 12,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;0;0;0;0;0;1;1",
        "aff_unique_norm": "Northwestern Polytechnical University;University of Georgia",
        "aff_unique_dep": "School of Automation;Department of Computer Science",
        "aff_unique_url": "https://www.nwpu.edu.cn;https://www.uga.edu",
        "aff_unique_abbr": "NPU;UGA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;0;0;0;0;0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "c631dd52d3",
        "title": "Inductive Regularized Learning of Kernel Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html",
        "author": "Prateek Jain; Brian Kulis; Inderjit S. Dhillon",
        "abstract": "In this paper we consider the fundamental problem of semi-supervised kernel function learning. We propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. We introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the $k$-NN classification accuracy significantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies.",
        "bibtex": "@inproceedings{NIPS2010_d86ea612,\n author = {Jain, Prateek and Kulis, Brian and Dhillon, Inderjit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inductive Regularized Learning of Kernel Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 347843,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3886582026996685917&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research Bangalore; UC Berkeley EECS and ICSI; UT Austin Dept. of Computer Sciences",
        "aff_domain": "microsoft.com;eecs.berkeley.edu;cs.utexas.edu",
        "email": "microsoft.com;eecs.berkeley.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft;University of California, Berkeley;University of Texas at Austin",
        "aff_unique_dep": "Microsoft Research;Electrical Engineering and Computer Sciences;Department of Computer Sciences",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india;https://www.berkeley.edu;https://www.utexas.edu",
        "aff_unique_abbr": "MSR India;UC Berkeley;UT Austin",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Bangalore;Berkeley;Austin",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "ea1c020ea0",
        "title": "Inference and communication in the game of Password",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/766d856ef1a6b02f93d894415e6bfa0e-Abstract.html",
        "author": "Yang Xu; Charles Kemp",
        "abstract": "Communication between a speaker and hearer will be most efficient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efficient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other\u2019s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We find evidence in support of all three predictions, and demonstrate in addition that efficient communication tends to break down when speakers and hearers are placed under time pressure.",
        "bibtex": "@inproceedings{NIPS2010_766d856e,\n author = {Xu, Yang and Kemp, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inference and communication in the game of Password},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/766d856ef1a6b02f93d894415e6bfa0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 112417,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4056946958491814398&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Machine Learning Department; School of Computer Science+Department of Psychology",
        "aff_domain": "cs.cmu.edu;cmu.edu",
        "email": "cs.cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Carnegie Mellon University;School of Computer Science;University Affiliation Not Specified",
        "aff_unique_dep": "Machine Learning Department;Computer Science;Department of Psychology",
        "aff_unique_url": "https://www.cs.cmu.edu/ml;;",
        "aff_unique_abbr": "CMU ML;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;",
        "aff_country_unique": "United States;"
    },
    {
        "id": "18ad495117",
        "title": "Inference with Multivariate Heavy-Tails in Linear Models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html",
        "author": "Danny Bickson; Carlos Guestrin",
        "abstract": "Heavy-tailed distributions naturally occur in many real life problems. Unfortunately, it is typically not possible to compute inference in closed-form in graphical models which involve such heavy tailed distributions.   In this work, we propose a novel simple linear graphical model for  independent latent random variables, called linear characteristic model (LCM), defined in the characteristic function domain. Using stable distributions, a heavy-tailed family of distributions which is a generalization of Cauchy, L\\'evy and Gaussian distributions, we show for the first time, how to compute both exact and approximate inference in such a linear multivariate graphical model. LCMs are not limited to only stable distributions, in fact LCMs are always defined for any random variables (discrete, continuous or a mixture of both).   We provide a realistic problem from the field of computer networks to demonstrate the applicability of our construction. Other potential application is iterative decoding of linear channels with non-Gaussian noise.",
        "bibtex": "@inproceedings{NIPS2010_e995f98d,\n author = {Bickson, Danny and Guestrin, Carlos},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inference with Multivariate Heavy-Tails in Linear Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 229604,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14427186264205521458&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "88a41e7c9d",
        "title": "Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/19b650660b253761af189682e03501dd-Abstract.html",
        "author": "Kanaka Rajan; L Abbott; Haim Sompolinsky",
        "abstract": "How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of fluctuations in the input-generated response of a neural network, by comparing the distribution of evoked and intrinsically generated activity across the different units. We develop a complementary approach to principal component analysis in which separate high-variance directions are typically derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses.",
        "bibtex": "@inproceedings{NIPS2010_19b65066,\n author = {Rajan, Kanaka and Abbott, L and Sompolinsky, Haim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19b650660b253761af189682e03501dd-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/19b650660b253761af189682e03501dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/19b650660b253761af189682e03501dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2012717,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17756480420903602661&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Lewis-Sigler Institute for Integrative Genomics, Carl Icahn Laboratories # 262, Princeton University, Princeton NJ 08544 USA; Department of Neuroscience, Department of Physiology and Cellular Biophysics, Columbia University College of Physicians and Surgeons, New York, NY 10032-2695 USA; Racah Institute of Physics, Interdisciplinary Center for Neural Computation, Hebrew University, Jerusalem, Israel + Center for Brain Science, Harvard University, Cambridge, MA 02138 USA",
        "aff_domain": "princeton.edu;columbia.edu;fiz.huji.ac.il",
        "email": "princeton.edu;columbia.edu;fiz.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "Princeton University;Columbia University College of Physicians and Surgeons;Hebrew University;Harvard University",
        "aff_unique_dep": "Lewis-Sigler Institute for Integrative Genomics;Department of Neuroscience;Racah Institute of Physics;Center for Brain Science",
        "aff_unique_url": "https://www.princeton.edu;https://www.cumc.columbia.edu;http://www.huji.ac.il;https://www.harvard.edu",
        "aff_unique_abbr": "Princeton;CUMC;HUJI;Harvard",
        "aff_campus_unique_index": "0;1;2+3",
        "aff_campus_unique": "Princeton;New York;Jerusalem;Cambridge",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "71b5b83bf3",
        "title": "Infinite Relational Modeling of Functional Connectivity in Resting State fMRI",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/68a83eeb494a308fe5295da69428a507-Abstract.html",
        "author": "Morten M\u00f8rup; Kristoffer Madsen; Anne-marie Dogonowski; Hartwig Siebner; Lars K. Hansen",
        "abstract": "Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the definition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the infinite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects.",
        "bibtex": "@inproceedings{NIPS2010_68a83eeb,\n author = {M\\o rup, Morten and Madsen, Kristoffer and Dogonowski, Anne-marie and Siebner, Hartwig and Hansen, Lars K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Infinite Relational Modeling of Functional Connectivity in Resting State fMRI},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68a83eeb494a308fe5295da69428a507-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/68a83eeb494a308fe5295da69428a507-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/68a83eeb494a308fe5295da69428a507-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/68a83eeb494a308fe5295da69428a507-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5738346,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1281087367518149650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Section for Cognitive Systems, DTU Informatics, Technical University of Denmark; Danish Research Centre for Magnetic Resonance, Copenhagen University Hospital Hvidovre; Danish Research Centre for Magnetic Resonance, Copenhagen University Hospital Hvidovre; Danish Research Centre for Magnetic Resonance, Copenhagen University Hospital Hvidovre; Section for Cognitive Systems, DTU Informatics, Technical University of Denmark",
        "aff_domain": "imm.dtu.dk;drcmr.dk;drcmr.dk;drcmr.dk;imm.dtu.dk",
        "email": "imm.dtu.dk;drcmr.dk;drcmr.dk;drcmr.dk;imm.dtu.dk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Technical University of Denmark;Copenhagen University Hospital Hvidovre",
        "aff_unique_dep": "Section for Cognitive Systems, DTU Informatics;Danish Research Centre for Magnetic Resonance",
        "aff_unique_url": "https://www.dtu.dk;https://www.hvidovrehospital.dk",
        "aff_unique_abbr": "DTU;",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Hvidovre",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "276e6cc18a",
        "title": "Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/81e74d678581a3bb7a720b019f4f1a93-Abstract.html",
        "author": "Dirk Husmeier; Frank Dondelinger; Sophie Lebre",
        "abstract": "Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have therefore been proposed in the last few years. The present paper aims to improve the flexibility of two recent versions of non-homogeneous DBNs, which either (i) suffer from the need for data discretization, or (ii) assume a time-invariant network structure. Allowing the network structure to be fully flexible leads to the risk of overfitting and inflated inference uncertainty though, especially in the highly topical field of systems biology, where independent measurements tend to be sparse. In the present paper we investigate three conceptually different regularization schemes based on inter-segment information sharing. We assess the performance in a comparative evaluation study based on simulated data. We compare the predicted segmentation of gene expression time series obtained during embryogenesis in Drosophila melanogaster with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known regulatory network of five genes in Saccharomyces cerevisiae.",
        "bibtex": "@inproceedings{NIPS2010_81e74d67,\n author = {Husmeier, Dirk and Dondelinger, Frank and Lebre, Sophie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/81e74d678581a3bb7a720b019f4f1a93-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 208385,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16296329873391620969&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Biomathematics & Statistics Scotland (BioSS); Biomathematics & Statistics Scotland (BioSS); Universit\\u00e9 de Strasbourg, LSIIT-UMR 7005",
        "aff_domain": "bioss.ac.uk;bioss.ac.uk;lsiit-cnrs.unistra.fr",
        "email": "bioss.ac.uk;bioss.ac.uk;lsiit-cnrs.unistra.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Biomathematics & Statistics Scotland;Universit\u00e9 de Strasbourg",
        "aff_unique_dep": "Biomathematics & Statistics;LSIIT-UMR 7005",
        "aff_unique_url": "https://www.bioss.ac.uk;https://www.unistra.fr",
        "aff_unique_abbr": "BioSS;Unistra",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "id": "d5f4a12c75",
        "title": "Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/13f3cf8c531952d72e5847c4183e6910-Abstract.html",
        "author": "Martha White; Adam White",
        "abstract": "The reinforcement learning community has explored many approaches to obtain- ing value estimates and models to guide decision making; these approaches, how- ever, do not usually provide a measure of confidence in the estimate. Accurate estimates of an agent\u2019s confidence are useful for many applications, such as bi- asing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing confidence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agent- environment interaction rarely satisfies traditional assumptions. Samples of value- estimates are dependent, likely non-normally distributed and often limited, partic- ularly in early learning when confidence estimates are pivotal. In this work, we investigate how to compute robust confidences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute confidence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applica- bility of our confidence estimation algorithms with experiments on exploration, parameter estimation and tracking.",
        "bibtex": "@inproceedings{NIPS2010_13f3cf8c,\n author = {White, Martha and White, Adam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 284276,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10899585632174644994&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "e8acf86d3f",
        "title": "Joint Analysis of Time-Evolving Binary Matrices and Associated Documents",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/41ae36ecb9b3eee609d05b90c14222fb-Abstract.html",
        "author": "Eric Wang; Dehong Liu; Jorge Silva; Lawrence Carin; David B. Dunson",
        "abstract": "We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here defined by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efficient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives.",
        "bibtex": "@inproceedings{NIPS2010_41ae36ec,\n author = {Wang, Eric and Liu, Dehong and Silva, Jorge and Carin, Lawrence and Dunson, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Joint Analysis of Time-Evolving Binary Matrices and Associated Documents},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/41ae36ecb9b3eee609d05b90c14222fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2411599,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14714188039555816487&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Electrical and Computer Engineering Department, Duke University; Electrical and Computer Engineering Department, Duke University; Electrical and Computer Engineering Department, Duke University; Statistics Department, Duke University; Electrical and Computer Engineering Department, Duke University",
        "aff_domain": "duke.edu;duke.edu;duke.edu;stat.duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;stat.duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Electrical and Computer Engineering Department",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4e876845dd",
        "title": "Joint Cascade Optimization Using A Product Of Boosted Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/aa169b49b583a2b5af89203c2b78c67c-Abstract.html",
        "author": "Leonidas Lefakis; Francois Fleuret",
        "abstract": "The standard strategy for efficient object detection consists of building a cascade composed of several binary classifiers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classifiers, and concentrates the computation on difficult parts of the image which can not be trivially rejected.  We introduce a novel algorithm to construct jointly the classifiers of such a cascade. We interpret the response of a classifier as a probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set.  Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efficiency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines.",
        "bibtex": "@inproceedings{NIPS2010_aa169b49,\n author = {Lefakis, Leonidas and Fleuret, Francois},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Joint Cascade Optimization Using A Product Of Boosted Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/aa169b49b583a2b5af89203c2b78c67c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 136686,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6402800327331307592&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland",
        "aff_domain": "idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Idiap Research Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idiap.ch",
        "aff_unique_abbr": "Idiap",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Martigny",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "774d858eca",
        "title": "Kernel Descriptors for Visual Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4558dbb6f6f8bb2e16d03b85bde76e2c-Abstract.html",
        "author": "Liefeng Bo; Xiaofeng Ren; Dieter Fox",
        "abstract": "The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT~\\cite{Lowe2004Distinctive} and HOG~\\cite{Dalal2005Histograms}, are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a unified and principled framework to turn pixel attributes (gradient, color, local binary pattern, \\etc) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA)~\\cite{Scholkopf1998Nonlinear}. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classification benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.",
        "bibtex": "@inproceedings{NIPS2010_4558dbb6,\n author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Descriptors for Visual Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 342676,
        "gs_citation": 465,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17529357756697140548&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "University of Washington; Intel Labs Seattle; University of Washington + Intel Labs Seattle",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "University of Washington;Intel",
        "aff_unique_dep": ";Intel Labs",
        "aff_unique_url": "https://www.washington.edu;https://www.intel.com",
        "aff_unique_abbr": "UW;Intel",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "041210053e",
        "title": "LSTD with Random Projections",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html",
        "author": "Mohammad Ghavamzadeh; Alessandro Lazaric; Odalric Maillard; R\u00e9mi Munos",
        "abstract": "We consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples. In particular, we study the least-squares temporal difference (LSTD) learning algorithm when a space of low dimension is generated with a random projection from a high-dimensional space. We provide a thorough theoretical analysis of the LSTD with random projections and derive performance bounds for the resulting algorithm. We also show how the error of LSTD with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (LSPI) algorithm.",
        "bibtex": "@inproceedings{NIPS2010_a8e864d0,\n author = {Ghavamzadeh, Mohammad and Lazaric, Alessandro and Maillard, Odalric and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {LSTD with Random Projections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a8e864d04c95572d1aece099af852d0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 335571,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16259082171550041277&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "073c8bfed0",
        "title": "Label Embedding Trees for Large Multi-Class Tasks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html",
        "author": "Samy Bengio; Jason Weston; David Grangier",
        "abstract": "Multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a tree-structure of classifiers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster.",
        "bibtex": "@inproceedings{NIPS2010_06138bc5,\n author = {Bengio, Samy and Weston, Jason and Grangier, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Label Embedding Trees for Large Multi-Class Tasks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/06138bc5af6023646ede0e1f7c1eac75-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/06138bc5af6023646ede0e1f7c1eac75-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/06138bc5af6023646ede0e1f7c1eac75-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 165993,
        "gs_citation": 492,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9379663896135003975&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Google Research, New York, NY; Google Research, New York, NY; NEC Labs America, Princeton, NJ",
        "aff_domain": "google.com;google.com;nec-labs.com",
        "email": "google.com;google.com;nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Google;NEC Labs America",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.nec-labs.com",
        "aff_unique_abbr": "Google Research;NEC LA",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "New York;Princeton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dc43e82c9b",
        "title": "Large Margin Learning of Upstream Scene Understanding Models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/39461a19e9eddfb385ea76b26521ea48-Abstract.html",
        "author": "Jun Zhu; Li-jia Li; Li Fei-fei; Eric P. Xing",
        "abstract": "Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classification. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efficiently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.",
        "bibtex": "@inproceedings{NIPS2010_39461a19,\n author = {Zhu, Jun and Li, Li-jia and Fei-fei, Li and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large Margin Learning of Upstream Scene Understanding Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2475257,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7791333703438063514&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213; Department of Computer Science, Stanford University, Stanford, CA 94305; Department of Computer Science, Stanford University, Stanford, CA 94305; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "cs.cmu.edu;cs.stanford.edu;cs.stanford.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.stanford.edu;cs.stanford.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Stanford University",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.cmu.edu;https://www.stanford.edu",
        "aff_unique_abbr": "CMU;Stanford",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Pittsburgh;Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "10044b2b21",
        "title": "Large Margin Multi-Task Metric Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/087408522c31eeb1f982bc0eaf81d35f-Abstract.html",
        "author": "Shibin Parameswaran; Kilian Q. Weinberger",
        "abstract": "Multi-task learning (MTL) improves the prediction performance on multiple, different but related, learning problems through shared parameters or representations. One of the most prominent multi-task learning algorithms is an extension to svms by Evgeniou et al. Although very elegant, multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which, in a multi-task setting, requires the different learning tasks to share the same set of classes. This paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor (lmnn) algorithm to the MTL paradigm. Instead of relying on separating hyperplanes, its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multitask learning. We evaluate the resulting multi-task lmnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task kNN under several metrics and state-of-the-art MTL classifiers.",
        "bibtex": "@inproceedings{NIPS2010_08740852,\n author = {Parameswaran, Shibin and Weinberger, Kilian Q},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large Margin Multi-Task Metric Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 509251,
        "gs_citation": 356,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16119977459060586613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego; Department of Computer Science and Engineering, Washington University in St. Louis",
        "aff_domain": "ucsd.edu;wustl.edu",
        "email": "ucsd.edu;wustl.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, San Diego;Washington University in St. Louis",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu;https://wustl.edu",
        "aff_unique_abbr": "UCSD;WashU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "San Diego;St. Louis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e562889de0",
        "title": "Large-Scale Matrix Factorization with Missing Data under Additional Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/da4fb5c6e93e74d3df8527599fa62642-Abstract.html",
        "author": "Kaushik Mitra; Sameer Sheorey; Rama Chellappa",
        "abstract": "Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM), non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semidefinite program (LRSDP) with the advantage that: $1)$ an efficient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems, and $2)$ additional constraints such as ortho-normality, required in orthographic SfM, can be directly incorporated in the new formulation. Our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm finds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the affine SfM problem, non-rigid SfM and photometric stereo problems.",
        "bibtex": "@inproceedings{NIPS2010_da4fb5c6,\n author = {Mitra, Kaushik and Sheorey, Sameer and Chellappa, Rama},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large-Scale Matrix Factorization with Missing Data under Additional Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 290195,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6121551150016386182&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical and Computer Engineering and UMIACS, University of Maryland, College Park, MD 20742; Toyota Technological Institute, Chicago; Department of Electrical and Computer Engineering and UMIACS, University of Maryland, College Park, MD 20742",
        "aff_domain": "umiacs.umd.edu;ttic.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;ttic.edu;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Maryland, College Park;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.umd.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UMD;TTI-Chicago",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "College Park;Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ad5de4d9dd",
        "title": "Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/cc1aa436277138f61cda703991069eaf-Abstract.html",
        "author": "Diane Hu; Laurens Maaten; Youngmin Cho; Sorin Lerner; Lawrence K. Saul",
        "abstract": "When software developers modify one or more files in a large code base, they must also identify and update other related files. Many file dependencies can be detected by mining the development history of the code base: in essence, groups of related files are revealed by the logs of previous workflows. From data of this form, we show how to detect dependent files by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we find that LVMs improve the performance of related file prediction over current leading methods.",
        "bibtex": "@inproceedings{NIPS2010_cc1aa436,\n author = {Hu, Diane and Maaten, Laurens and Cho, Youngmin and Lerner, Sorin and Saul, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cc1aa436277138f61cda703991069eaf-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/cc1aa436277138f61cda703991069eaf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/cc1aa436277138f61cda703991069eaf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 317275,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8666369544139828338&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Computer Science & Engineering, University of California, San Diego; Dept. of Computer Science & Engineering, University of California, San Diego + Pattern Recognition & Bioinformatics Lab, Delft University of Technology; Dept. of Computer Science & Engineering, University of California, San Diego; Dept. of Computer Science & Engineering, University of California, San Diego; Dept. of Computer Science & Engineering, University of California, San Diego",
        "aff_domain": "cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "University of California, San Diego;Delft University of Technology",
        "aff_unique_dep": "Department of Computer Science & Engineering;Pattern Recognition & Bioinformatics Lab",
        "aff_unique_url": "https://www.ucsd.edu;https://www.tudelft.nl",
        "aff_unique_abbr": "UCSD;TUDelft",
        "aff_campus_unique_index": "0;0+1;0;0;0",
        "aff_campus_unique": "San Diego;Delft",
        "aff_country_unique_index": "0;0+1;0;0;0",
        "aff_country_unique": "United States;Netherlands"
    },
    {
        "id": "dcc35d59e6",
        "title": "Layer-wise analysis of deep networks with Gaussian kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7eabe3a1649ffa2b3ff8c02ebfd5659f-Abstract.html",
        "author": "Gr\u00e9goire Montavon; Klaus-Robert M\u00fcller; Mikio L. Braun",
        "abstract": "Deep networks can potentially express a learning problem more efficiently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers.",
        "bibtex": "@inproceedings{NIPS2010_7eabe3a1,\n author = {Montavon, Gr\\'{e}goire and M\\\"{u}ller, Klaus-Robert and Braun, Mikio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Layer-wise analysis of deep networks with Gaussian kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 270614,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18055583554225607989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning Group, TU Berlin; Machine Learning Group, TU Berlin; Machine Learning Group, TU Berlin",
        "aff_domain": "cs.tu-berlin.de;cs.tu-berlin.de;cs.tu-berlin.de",
        "email": "cs.tu-berlin.de;cs.tu-berlin.de;cs.tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin",
        "aff_unique_dep": "Machine Learning Group",
        "aff_unique_url": "https://www.tu-berlin.de",
        "aff_unique_abbr": "TU Berlin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "9dabec2d68",
        "title": "Layered image motion with explicit occlusions, temporal consistency, and depth ordering",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html",
        "author": "Deqing Sun; Erik B. Sudderth; Michael J. Black",
        "abstract": "Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.",
        "bibtex": "@inproceedings{NIPS2010_f7664060,\n author = {Sun, Deqing and Sudderth, Erik and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Layered image motion with explicit occlusions, temporal consistency, and depth ordering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1660313,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18338961038216206734&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "DepartmentofComputerScience,BrownUniversity; DepartmentofComputerScience,BrownUniversity; DepartmentofComputerScience,BrownUniversity",
        "aff_domain": "cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "email": "cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "81359e80ec",
        "title": "Learning Bounds for Importance Weighting",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/59c33016884a62116be975a9bb8257e3-Abstract.html",
        "author": "Corinna Cortes; Yishay Mansour; Mehryar Mohri",
        "abstract": "This paper presents an analysis of importance weighting for learning from finite samples and gives a series of theoretical and algorithmic results. We point out simple cases where importance weighting can fail, which suggests the need for an analysis of the properties of this technique. We then give both upper and lower bounds for generalization with bounded importance weights and, more significantly, give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded, a condition related to the Renyi divergence of the training and test distributions. These results are based on a series of novel and general bounds we derive for unbounded loss functions, which are of independent interest. We use these bounds to guide the definition of an alternative reweighting algorithm and report the results of experiments demonstrating its benefits. Finally, we analyze the properties of normalized importance weights which are also commonly used.",
        "bibtex": "@inproceedings{NIPS2010_59c33016,\n author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Bounds for Importance Weighting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/59c33016884a62116be975a9bb8257e3-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/59c33016884a62116be975a9bb8257e3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/59c33016884a62116be975a9bb8257e3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/59c33016884a62116be975a9bb8257e3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 283137,
        "gs_citation": 455,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14439478643447812710&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Google Research, New York, NY 10011; Tel-Aviv University, Tel-Aviv 69978, Israel; Courant Institute and Google, New York, NY 10012",
        "aff_domain": "google.com;tau.ac.il;cims.nyu.edu",
        "email": "google.com;tau.ac.il;cims.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;Tel Aviv University;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Google Research;;Mathematical Sciences",
        "aff_unique_url": "https://research.google;https://www.tau.ac.il;https://courant.nyu.edu",
        "aff_unique_abbr": "Google;TAU;Courant",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "New York;Tel-Aviv",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "19fbf93d8e",
        "title": "Learning Convolutional Feature Hierarchies for Visual Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a01610228fe998f515a72dd730294d87-Abstract.html",
        "author": "Koray Kavukcuoglu; Pierre Sermanet; Y-lan Boureau; Karol Gregor; Michael Mathieu; Yann L. Cun",
        "abstract": "We propose an unsupervised method for learning multi-stage   hierarchies of sparse convolutional features. While sparse coding   has become an increasingly popular method for learning visual   features, it is most often trained at the patch level. Applying the   resulting filters convolutionally results in highly redundant codes   because overlapping patches are encoded in isolation.  By training   convolutionally over large image windows, our method reduces the   redudancy between feature vectors at neighboring locations and   improves the efficiency of the overall representation. In addition   to a linear decoder that reconstructs the image from sparse   features, our method trains an efficient feed-forward encoder that   predicts quasi-sparse features from the input.  While patch-based   training rarely produces anything but oriented edge detectors, we   show that convolutional training produces highly diverse filters,   including center-surround filters, corner detectors, cross   detectors, and oriented grating detectors.  We show that using these   filters in multi-stage convolutional network architecture improves   performance on a number of visual recognition and detection tasks.",
        "bibtex": "@inproceedings{NIPS2010_a0161022,\n author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-lan and Gregor, Karol and Mathieu, Michael and Cun, Yann},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Convolutional Feature Hierarchies for Visual Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a01610228fe998f515a72dd730294d87-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a01610228fe998f515a72dd730294d87-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a01610228fe998f515a72dd730294d87-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 397459,
        "gs_citation": 767,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6313357809286128980&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Courant Institute of Mathematical Sciences, New York University; Courant Institute of Mathematical Sciences, New York University; INRIA - Willow project-team + Courant Institute of Mathematical Sciences, New York University; Courant Institute of Mathematical Sciences, New York University; Courant Institute of Mathematical Sciences, New York University; Courant Institute of Mathematical Sciences, New York University",
        "aff_domain": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;clipper.ens.fr;cs.nyu.edu",
        "email": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;clipper.ens.fr;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0;0;0;0",
        "aff_unique_norm": "New York University;INRIA",
        "aff_unique_dep": "Courant Institute of Mathematical Sciences;Willow project-team",
        "aff_unique_url": "https://www.courant.nyu.edu;https://www.inria.fr",
        "aff_unique_abbr": "NYU;INRIA",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0;1+0;0;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "a911a46195",
        "title": "Learning Efficient Markov Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e5e63da79fcd2bebbd7cb8bf1c1d0274-Abstract.html",
        "author": "Vibhav Gogate; William Webb; Pedro Domingos",
        "abstract": "We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context specific independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature or its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efficient.Experiments on a variety of domains show that our approach compares favorably with thin junction trees and other Markov network structure learners.",
        "bibtex": "@inproceedings{NIPS2010_e5e63da7,\n author = {Gogate, Vibhav and Webb, William and Domingos, Pedro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Efficient Markov Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 174592,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12151117407357344576&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science & Engineering, University of Washington; Department of Computer Science & Engineering, University of Washington; Department of Computer Science & Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science & Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a0c909f40b",
        "title": "Learning Kernels with Radiuses of Minimum Enclosing Balls",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/05049e90fa4f5039a8cadc6acbb4b2cc-Abstract.html",
        "author": "Kun Gai; Guangyun Chen; Chang-shui Zhang",
        "abstract": "In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classifier. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2) of norm constraints on combination coefficients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method significantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches.",
        "bibtex": "@inproceedings{NIPS2010_05049e90,\n author = {Gai, Kun and Chen, Guangyun and Zhang, Chang-shui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Kernels with Radiuses of Minimum Enclosing Balls},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 189633,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9878795340205779622&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e979a4d03a",
        "title": "Learning Multiple Tasks using Manifold Regularization",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2cbca44843a864533ec05b321ae1f9d1-Abstract.html",
        "author": "Arvind Agarwal; Samuel Gerber; Hal Daume",
        "abstract": "We present a novel method for multitask learning (MTL) based on {\\it manifold regularization}: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common {\\it linear} subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is fixed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efficient and easy to implement. We show the efficacy of our method on several datasets.",
        "bibtex": "@inproceedings{NIPS2010_2cbca448,\n author = {Agarwal, Arvind and Gerber, Samuel and Daume, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Multiple Tasks using Manifold Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2cbca44843a864533ec05b321ae1f9d1-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2cbca44843a864533ec05b321ae1f9d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2cbca44843a864533ec05b321ae1f9d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 263863,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4995951812996391396&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland + School of Computing, University of Utah; Scientific Computing and Imaging Institute, University of Utah",
        "aff_domain": "cs.umd.edu;umiacs.umd.edu;cs.utah.edu",
        "email": "cs.umd.edu;umiacs.umd.edu;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;1",
        "aff_unique_norm": "University of Maryland;University of Utah",
        "aff_unique_dep": "Department of Computer Science;School of Computing",
        "aff_unique_url": "https://www/umd.edu;https://www.utah.edu",
        "aff_unique_abbr": "UMD;U of U",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Utah;Salt Lake City",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c356c89dad",
        "title": "Learning Multiple Tasks with a Sparse Matrix-Normal Penalty",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/51d92be1c60d1db1d2e5e7a07da55b26-Abstract.html",
        "author": "Yi Zhang; Jeff G. Schneider",
        "abstract": "In this paper, we propose a matrix-variate normal penalty with sparse inverse covariances to couple multiple tasks. Learning multiple (parametric) models can be viewed as estimating a matrix of parameters, where rows and columns of the matrix correspond to tasks and features, respectively. Following the matrix-variate normal density, we design a penalty that decomposes the full covariance of matrix elements into the Kronecker product of row covariance and column covariance, which characterizes both task relatedness and feature representation. Several recently proposed methods are variants of the special cases of this formulation. To address the overfitting issue and select meaningful task and feature structures, we include sparse covariance selection into our matrix-normal regularization via L-1 penalties on task and feature inverse covariances. We empirically study the proposed method and compare with related models in two real-world problems: detecting landmines in multiple fields and recognizing faces between different subjects. Experimental results show that the proposed framework provides an effective and flexible way to model various different structures of multiple tasks.",
        "bibtex": "@inproceedings{NIPS2010_51d92be1,\n author = {Zhang, Yi and Schneider, Jeff},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Multiple Tasks with a Sparse Matrix-Normal Penalty},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/51d92be1c60d1db1d2e5e7a07da55b26-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 97785,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16058854491049025382&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Machine Learning Department, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5e1f3a5679",
        "title": "Learning Networks of Stochastic Differential Equations",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8df707a948fac1b4a0f97aa554886ec8-Abstract.html",
        "author": "Jos\u00e9 Pereira; Morteza Ibrahimi; Andrea Montanari",
        "abstract": "We consider linear models for stochastic dynamics. Any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics. We tackle the problem of learning such a network from observation of the system trajectory over a time interval T. We analyse the l1-regularized least squares algorithm and, in the setting in which the underlying network is sparse, we prove performance guarantees that are uniform in the sampling rate as long as this is sufficiently high. This result substantiates the notion of a well defined \u2018time complexity\u2019 for the network inference problem.",
        "bibtex": "@inproceedings{NIPS2010_8df707a9,\n author = {Pereira, Jos\\'{e} and Ibrahimi, Morteza and Montanari, Andrea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Networks of Stochastic Differential Equations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 147546,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7104762687070996306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical Engineering, Stanford University; Department of Electrical Engineering, Stanford University; Department of Electrical Engineering and Statistics, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "80ac909676",
        "title": "Learning To Count Objects in Images",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/fe73f687e5bc5280214e0486b273a5f9-Abstract.html",
        "author": "Victor Lempitsky; Andrew Zisserman",
        "abstract": "We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object).   Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efficiently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization.  The proposed framework is very flexible as it can accept any domain-specific visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data.",
        "bibtex": "@inproceedings{NIPS2010_fe73f687,\n author = {Lempitsky, Victor and Zisserman, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning To Count Objects in Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/fe73f687e5bc5280214e0486b273a5f9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 513796,
        "gs_citation": 1601,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12325140929890847634&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Visual Geometry Group, University of Oxford; Visual Geometry Group, University of Oxford",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Visual Geometry Group",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "a4dfe6d784",
        "title": "Learning concept graphs from text with stick-breaking priors",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6c8dba7d0df1c4a79dd07646be9a26c8-Abstract.html",
        "author": "America Chambers; Padhraic Smyth; Mark Steyvers",
        "abstract": "We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents-a task that is difficult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (such as hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs.",
        "bibtex": "@inproceedings{NIPS2010_6c8dba7d,\n author = {Chambers, America and Smyth, Padhraic and Steyvers, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning concept graphs from text with stick-breaking priors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 324699,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2307244549524378462&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Cognitive Science, University of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fba4753658",
        "title": "Learning from Candidate Labeling Sets",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c9e1074f5b3f9fc8ea15d152add07294-Abstract.html",
        "author": "Jie Luo; Francesco Orabona",
        "abstract": "In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classifiers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difficult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efficient algorithm to solve it. Experiments conducted on artificial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to SVM trained with the ground-truth labels, and outperforms other baselines.",
        "bibtex": "@inproceedings{NIPS2010_c9e1074f,\n author = {Luo, Jie and Orabona, Francesco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning from Candidate Labeling Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c9e1074f5b3f9fc8ea15d152add07294-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 210507,
        "gs_citation": 221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9333415551936974853&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 18,
        "aff": "Idiap Research Institute and EPF Lausanne; DSI, Universit`a degli Studi di Milano",
        "aff_domain": "idiap.ch;dsi.unimi.it",
        "email": "idiap.ch;dsi.unimi.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Idiap Research Institute;Universit\u00e0 degli Studi di Milano",
        "aff_unique_dep": ";DSI",
        "aff_unique_url": "https://www.idiap.ch;https://www.unimi.it",
        "aff_unique_abbr": "Idiap;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Switzerland;Italy"
    },
    {
        "id": "8ccd4473b5",
        "title": "Learning from Logged Implicit Exploration Data",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c0f168ce8900fa56e57789e2a2f2c9d0-Abstract.html",
        "author": "Alex Strehl; John Langford; Lihong Li; Sham M. Kakade",
        "abstract": "We provide a sound and consistent foundation for the use of \\emph{nonrandom} exploration data in",
        "bibtex": "@inproceedings{NIPS2010_c0f168ce,\n author = {Strehl, Alex and Langford, John and Li, Lihong and Kakade, Sham M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning from Logged Implicit Exploration Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c0f168ce8900fa56e57789e2a2f2c9d0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 125097,
        "gs_citation": 289,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16578099578589574428&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Facebook Inc. + Yahoo! Research; Yahoo! Research; Yahoo! Research; Department of Statistics, University of Pennsylvania",
        "aff_domain": "facebook.com;yahoo-inc.com;yahoo-inc.com;wharton.upenn.edu",
        "email": "facebook.com;yahoo-inc.com;yahoo-inc.com;wharton.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;2",
        "aff_unique_norm": "Meta;Yahoo!;University of Pennsylvania",
        "aff_unique_dep": "Facebook;Yahoo! Research;Department of Statistics",
        "aff_unique_url": "https://www.facebook.com;https://research.yahoo.com;https://www.upenn.edu",
        "aff_unique_abbr": "FB;Yahoo!;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ce5ab22343",
        "title": "Learning invariant features using the Transformed Indian Buffet Process",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/fccb60fb512d13df5083790d64c4d5dd-Abstract.html",
        "author": "Joseph L. Austerweil; Thomas L. Griffiths",
        "abstract": "Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to define a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features.",
        "bibtex": "@inproceedings{NIPS2010_fccb60fb,\n author = {Austerweil, Joseph and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning invariant features using the Transformed Indian Buffet Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fccb60fb512d13df5083790d64c4d5dd-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/fccb60fb512d13df5083790d64c4d5dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/fccb60fb512d13df5083790d64c4d5dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126770,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1696335205332179152&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Psychology, University of California, Berkeley; Department of Psychology, University of California, Berkeley",
        "aff_domain": "gmail.com;berkeley.edu",
        "email": "gmail.com;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dd959d74f3",
        "title": "Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/ffeabd223de0d4eacb9a3e6e53e5448d-Abstract.html",
        "author": "Alessandro Chiuso; Gianluigi Pillonetto",
        "abstract": "We introduce a new Bayesian nonparametric approach to identification of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as defined by the recently introduced \u201cStable Spline kernel\u201d. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a definite advantage over a group LAR algorithm and state-of-the-art parametric identification techniques based on prediction error minimization.",
        "bibtex": "@inproceedings{NIPS2010_ffeabd22,\n author = {Chiuso, Alessandro and Pillonetto, Gianluigi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/ffeabd223de0d4eacb9a3e6e53e5448d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 419091,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8134017872106387499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Management and Engineering, University of Padova, Vicenza, Italy; Department of Information Engineering, University of Padova, Padova, Italy",
        "aff_domain": "unipd.it;dei.unipd.it",
        "email": "unipd.it;dei.unipd.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Padova",
        "aff_unique_dep": "Department of Management and Engineering",
        "aff_unique_url": "https://www.unipd.it",
        "aff_unique_abbr": "UNIPD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Padova",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "3bfdd53d49",
        "title": "Learning the context of a category",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/ebd9629fc3ae5e9f6611e2ee05a31cef-Abstract.html",
        "author": "Dan Navarro",
        "abstract": "This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is fit to multiple data sets, and provides a parsimonious method for describing how humans learn context specific conceptual representations.",
        "bibtex": "@inproceedings{NIPS2010_ebd9629f,\n author = {Navarro, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning the context of a category},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 169875,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "School of Psychology, University of Adelaide, Adelaide, SA 5005, Australia",
        "aff_domain": "adelaide.edu.au",
        "email": "adelaide.edu.au",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "School of Psychology",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Adelaide",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "980f379830",
        "title": "Learning to combine foveal glimpses with a third-order Boltzmann machine",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/677e09724f0e2df9b6c000b75b5da10d-Abstract.html",
        "author": "Hugo Larochelle; Geoffrey E. Hinton",
        "abstract": "We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the glimpse\" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images.\"",
        "bibtex": "@inproceedings{NIPS2010_677e0972,\n author = {Larochelle, Hugo and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to combine foveal glimpses with a third-order Boltzmann machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 935861,
        "gs_citation": 682,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14734927794625745724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "841803f1e7",
        "title": "Learning to localise sounds with spiking neural networks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/42a0e188f5033bc65bf8d78622277c4e-Abstract.html",
        "author": "Dan Goodman; Romain Brette",
        "abstract": "To localise the source of a sound, we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism's lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from filtered sounds. We present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difficult task of distinguishing sounds coming from the front and back.",
        "bibtex": "@inproceedings{NIPS2010_42a0e188,\n author = {Goodman, Dan and Brette, Romain},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to localise sounds with spiking neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/42a0e188f5033bc65bf8d78622277c4e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1407110,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7141229931527830690&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "D\u00b4epartment d\u2019Etudes Cognitive, Ecole Normale Sup \u00b4erieure, Paris 75005, France; D\u00b4epartment d\u2019Etudes Cognitive, Ecole Normale Sup \u00b4erieure, Paris 75005, France",
        "aff_domain": "ens.fr;ens.fr",
        "email": "ens.fr;ens.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ecole Normale Sup\u00e9rieure",
        "aff_unique_dep": "Department d'Etudes Cognitive",
        "aff_unique_url": "https://www.ens.fr",
        "aff_unique_abbr": "ENS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "4b1edadf61",
        "title": "Learning via Gaussian Herding",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html",
        "author": "Koby Crammer; Daniel D. Lee",
        "abstract": "We introduce a new family of online learning algorithms based upon   constraining the velocity flow over a distribution of weight   vectors.  In particular, we show how to effectively  herd a   Gaussian weight vector distribution by trading off velocity   constraints with a loss function.  By uniformly bounding this loss   function, we demonstrate how to solve the resulting optimization   analytically.  We compare the resulting algorithms on a variety of    real world datasets, and demonstrate how these algorithms achieve   state-of-the-art robust performance, especially with high label   noise in the training data.",
        "bibtex": "@inproceedings{NIPS2010_eddea82a,\n author = {Crammer, Koby and Lee, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning via Gaussian Herding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/eddea82ad2755b24c4e168c5fc2ebd40-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 401663,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14945970574563579908&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Enginering, The Technion, Haifa, 32000 Israel; Dept. of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA 19104",
        "aff_domain": "ee.technion.ac.il;seas.upenn.edu",
        "email": "ee.technion.ac.il;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technion;University of Pennsylvania",
        "aff_unique_dep": "Department of Electrical Engineering;Dept. of Electrical and Systems Engineering",
        "aff_unique_url": "http://www.technion.ac.il;https://www.upenn.edu",
        "aff_unique_abbr": "Technion;UPenn",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Haifa;Philadelphia",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "6bd67d7cf0",
        "title": "Lifted Inference Seen from the Other Side : The Tractable Features",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html",
        "author": "Abhay Jha; Vibhav Gogate; Alexandra Meliou; Dan Suciu",
        "abstract": "Lifted inference algorithms for representations that combine first-order logic and probabilistic graphical models have been the focus of much recent research. All lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.) and improve its efficiency by exploiting repeated structure in the first-order model. In this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference. In particular, we define a set of rules that look only at the logical representation to identify models for which exact efficient inference is possible. We show that our rules yield several new tractable classes that cannot be solved efficiently by any of the existing techniques.",
        "bibtex": "@inproceedings{NIPS2010_33e8075e,\n author = {Jha, Abhay and Gogate, Vibhav and Meliou, Alexandra and Suciu, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Lifted Inference Seen from the Other Side : The Tractable Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/33e8075e9970de0cfea955afd4644bb2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 227217,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=135311383990690920&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Computer Science & Engineering, University of Washington; Computer Science & Engineering, University of Washington; Computer Science & Engineering, University of Washington; Computer Science & Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Computer Science & Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c859bd1b70",
        "title": "Linear Complementarity for Regularized Policy Evaluation and Improvement",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/81dc9bdb52d04dc20036dbd8313ed055-Abstract.html",
        "author": "Jeffrey Johns; Christopher Painter-wakefield; Ronald Parr",
        "abstract": "Recent work in reinforcement learning has emphasized the power of L1 regularization to perform feature selection and prevent overfitting. We propose formulating the L1 regularized linear fixed point problem as a linear complementarity problem (LCP). This formulation offers several advantages over the LARS-inspired formulation, LARS-TD. The LCP formulation allows the use of efficient off-the-shelf solvers, leads to a new uniqueness result, and can be initialized with starting points from similar problems (warm starts). We demonstrate that warm starts, as well as the efficiency of LCP solvers, can speed up policy iteration. Moreover, warm starts permit a form of modified policy iteration that can be used to approximate a greedy\" homotopy path, a generalization of the LARS-TD homotopy path that combines policy evaluation and optimization.\"",
        "bibtex": "@inproceedings{NIPS2010_81dc9bdb,\n author = {Johns, Jeffrey and Painter-wakefield, Christopher and Parr, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linear Complementarity for Regularized Policy Evaluation and Improvement},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 584751,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10463133972743361515&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e23c1aab5d",
        "title": "Linear readout from a neural population with partial correlation data",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0188e8b8b014829e2fa0f430f0a95961-Abstract.html",
        "author": "Adrien Wohrer; Ranulfo Romo; Christian K. Machens",
        "abstract": "How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reflect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to fill the gaps\" in noise correlations matrices using an iterative application of the Wishart distribution over positive definitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternative-forced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations.\"",
        "bibtex": "@inproceedings{NIPS2010_0188e8b8,\n author = {Wohrer, Adrien and Romo, Ranulfo and Machens, Christian K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linear readout from a neural population with partial correlation data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0188e8b8b014829e2fa0f430f0a95961-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0188e8b8b014829e2fa0f430f0a95961-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0188e8b8b014829e2fa0f430f0a95961-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 338398,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11967425541536521759&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Group for Neural Theory, Laboratoire de Neurosciences Cognitives, \u00c9cole Normale Sup\u00e9rieure, 75005 Paris, France; Instituto de Fisiolog\u00eda Celular, Universidad Nacional Aut\u00f3noma de M\u00e9xico, Mexico City, Mexico; Group for Neural Theory, Laboratoire de Neurosciences Cognitives, \u00c9cole Normale Sup\u00e9rieure, 75005 Paris, France",
        "aff_domain": "ens.fr;ens.fr;ifc.unam.mx",
        "email": "ens.fr;ens.fr;ifc.unam.mx",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure;Universidad Nacional Aut\u00f3noma de M\u00e9xico",
        "aff_unique_dep": "Laboratoire de Neurosciences Cognitives;Instituto de Fisiolog\u00eda Celular",
        "aff_unique_url": "https://www.ens.fr;https://www.unam.mx",
        "aff_unique_abbr": "ENS;UNAM",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Paris;Mexico City",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;Mexico"
    },
    {
        "id": "8aeeacc35c",
        "title": "Link Discovery using Graph Feature Tracking",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/24896ee4c6526356cc127852413ea3b4-Abstract.html",
        "author": "Emile Richard; Nicolas Baskiotis; Theodoros Evgeniou; Nicolas Vayatis",
        "abstract": "We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices.  We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We  show experiments with both simulated and real data which reveal the interest of our methodology.",
        "bibtex": "@inproceedings{NIPS2010_24896ee4,\n author = {Richard, Emile and Baskiotis, Nicolas and Evgeniou, Theodoros and Vayatis, Nicolas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Link Discovery using Graph Feature Tracking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/24896ee4c6526356cc127852413ea3b4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 233643,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10580152360227770962&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "ENS Cachan - CMLA &MilleMercis, France; ENS Cachan - CMLA; Technology Management and Decision Sciences, INSEAD; ENS Cachan &UniverSud - CMLA UMR CNRS 8536, France",
        "aff_domain": "gmail.com;lip6.com;insead.edu;cmla.ens-cachan.fr",
        "email": "gmail.com;lip6.com;insead.edu;cmla.ens-cachan.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Ecole Normale Superieure de Cachan;INSEAD;ENS Cachan",
        "aff_unique_dep": "CMLA;Technology Management and Decision Sciences;CMLA UMR CNRS 8536",
        "aff_unique_url": "https://www.ens-cachan.fr;https://www.insead.edu;",
        "aff_unique_abbr": "ENS Cachan;INSEAD;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "c49d7e4a6e",
        "title": "Lower Bounds on Rate of Convergence of Cutting Plane Methods",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html",
        "author": "Xinhua Zhang; Ankan Saha; S.v.n. Vishwanathan",
        "abstract": "In a recent paper Joachims (2006) presented SVM-Perf, a cutting plane method (CPM) for training linear Support Vector Machines (SVMs) which converges to an $\\epsilon$ accurate solution in $O(1/\\epsilon^{2})$ iterations. By tightening the analysis, Teo et al. (2010) showed that $O(1/\\epsilon)$ iterations suffice. Given the impressive convergence speed of CPM on a number of practical problems, it was conjectured that these rates could be further improved. In this paper we disprove this conjecture. We present counter examples which are not only applicable for training linear SVMs with hinge loss, but also hold for support vector methods which optimize a \\emph{multivariate} performance score. However, surprisingly, these problems are not inherently hard. By exploiting the structure of the objective function we can devise an algorithm that converges in $O(1/\\sqrt{\\epsilon})$ iterations.",
        "bibtex": "@inproceedings{NIPS2010_e53a0a29,\n author = {Zhang, Xinhua and Saha, Ankan and Vishwanathan, S.v.n.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Lower Bounds on Rate of Convergence of Cutting Plane Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 369275,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18252218105059825421&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Dept. of Computing Science, University of Alberta; Dept. of Computer Science, University of Chicago; Dept. of Statistics + Dept. of Computer Science, Purdue University",
        "aff_domain": "ualberta.ca;cs.uchicago.edu;stat.purdue.edu",
        "email": "ualberta.ca;cs.uchicago.edu;stat.purdue.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "University of Alberta;University of Chicago;University Affiliation Not Specified;Purdue University",
        "aff_unique_dep": "Dept. of Computing Science;Dept. of Computer Science;Department of Statistics;Dept. of Computer Science",
        "aff_unique_url": "https://www.ualberta.ca;https://www.uchicago.edu;;https://www.purdue.edu",
        "aff_unique_abbr": "UAlberta;UChicago;;Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;United States;"
    },
    {
        "id": "c0bfa82228",
        "title": "MAP Estimation for Graphical Models by Likelihood Maximization",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/1c1d4df596d01da60385f0bb17a4a9e0-Abstract.html",
        "author": "Akshat Kumar; Shlomo Zilberstein",
        "abstract": "Computing a {\\em maximum a posteriori} (MAP) assignment in graphical models is a crucial inference problem for many practical applications. Several provably convergent approaches have been successfully developed using linear programming (LP) relaxation of the MAP problem. We present an alternative approach, which transforms the MAP problem into that of inference in a finite mixture of simple Bayes nets. We then derive the Expectation Maximization (EM) algorithm for this mixture that also monotonically increases a lower bound on the MAP assignment until convergence. The update equations for the EM algorithm are remarkably simple, both conceptually and computationally, and can be implemented using a graph-based message passing paradigm similar to max-product computation. We experiment on the real-world protein design dataset and show that EM's convergence rate is significantly higher than the previous LP relaxation based approach MPLP. EM achieves a solution quality within $95$\\% of optimal for most instances and is often an order-of-magnitude faster than MPLP.",
        "bibtex": "@inproceedings{NIPS2010_1c1d4df5,\n author = {Kumar, Akshat and Zilberstein, Shlomo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {MAP Estimation for Graphical Models by Likelihood Maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/1c1d4df596d01da60385f0bb17a4a9e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 389402,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13486922267053782662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Massachusetts; Department of Computer Science, University of Massachusetts",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4e1010b0dd",
        "title": "MAP estimation in Binary MRFs via Bipartite Multi-cuts",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/adc8ca1b15e20915c3ea6008fc2f52ed-Abstract.html",
        "author": "Sashank J. Reddi; Sunita Sarawagi; Sundar Vishwanathan",
        "abstract": "We propose a new LP relaxation for obtaining the MAP assignment of a binary MRF with pairwise potentials. Our relaxation is derived from reducing the MAP assignment problem to an instance of a recently proposed Bipartite Multi-cut problem where the LP relaxation is guaranteed to provide an O(log k) approximation where k is the number of vertices adjacent to non-submodular edges in the MRF. We then propose a combinatorial algorithm to efficiently solve the LP and also provide a lower bound by concurrently solving its dual to within an approximation. The algorithm is up to an order of magnitude faster and provides better MAP scores and bounds than the state of the art message passing algorithm of [1] that tightens the local marginal polytope with third-order marginal constraints.",
        "bibtex": "@inproceedings{NIPS2010_adc8ca1b,\n author = {J. Reddi, Sashank and Sarawagi, Sunita and Vishwanathan, Sundar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {MAP estimation in Binary MRFs via Bipartite Multi-cuts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 728838,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11732451689330270211&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "IIT Bombay + Google Inc.; IIT Bombay; IIT Bombay",
        "aff_domain": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.google.com",
        "aff_unique_abbr": "IITB;Google",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Mumbai;Mountain View",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "b4ee6f0705",
        "title": "Minimum Average Cost Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/9c01802ddb981e6bcfbec0f0516b8e35-Abstract.html",
        "author": "Kiyohito Nagano; Yoshinobu Kawahara; Satoru Iwata",
        "abstract": "A number of objective functions in clustering problems can be described with submodular functions. In this paper, we introduce the minimum average cost criterion, and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions. The proposed algorithm does not require the number of clusters in advance, and it will be determined by the property of a given set of data points. The minimum average cost clustering problem is parameterized with a real variable, and surprisingly, we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total. Additionally, we evaluate the performance of the proposed algorithm through computational experiments.",
        "bibtex": "@inproceedings{NIPS2010_9c01802d,\n author = {Nagano, Kiyohito and Kawahara, Yoshinobu and Iwata, Satoru},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimum Average Cost Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/9c01802ddb981e6bcfbec0f0516b8e35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 204332,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8507784001934685464&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Institute of Industrial Science, University of Tokyo, Japan; The Institute of Scientific and Industrial Research, Osaka University, Japan; Research Institute for Mathematical Sciences, Kyoto University, Japan",
        "aff_domain": "sat.t.u-tokyo.ac.jp;ar.sanken.osaka-u.ac.jp;kurims.kyoto-u.ac.jp",
        "email": "sat.t.u-tokyo.ac.jp;ar.sanken.osaka-u.ac.jp;kurims.kyoto-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Tokyo;Osaka University;Kyoto University",
        "aff_unique_dep": "Institute of Industrial Science;The Institute of Scientific and Industrial Research;Research Institute for Mathematical Sciences",
        "aff_unique_url": "https://www.iis.u-tokyo.ac.jp/en/;https://www.osaka-u.ac.jp;https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "UTokyo;OSU;Kyoto U",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Tokyo;Osaka;Kyoto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "67bad8059a",
        "title": "Mixture of time-warped trajectory models for movement decoding",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e2230b853516e7b05d79744fbd4c9c13-Abstract.html",
        "author": "Elaine Corbett; Eric Perreault; Konrad Koerding",
        "abstract": "Applications of Brain-Machine-Interfaces typically estimate user intent based on biological signals that are under voluntary control. For example, we might want to estimate how a patient with a paralyzed arm wants to move based on residual muscle activity. To solve such problems it is necessary to integrate obtained information over time. To do so, state of the art approaches typically use a probabilistic model of how the state, e.g. position and velocity of the arm, evolves over time \u2013 a so-called trajectory model. We wanted to further develop this approach using two intuitive insights: (1) At any given point of time there may be a small set of likely movement targets, potentially identified by the location of objects in the workspace or by gaze information from the user. (2) The user may want to produce movements at varying speeds. We thus use a generative model with a trajectory model incorporating these insights. Approximate inference on that generative model is implemented using a mixture of extended Kalman filters. We find that the resulting algorithm allows us to decode arm movements dramatically better than when we use a trajectory model with linear dynamics.",
        "bibtex": "@inproceedings{NIPS2010_e2230b85,\n author = {Corbett, Elaine and Perreault, Eric and Koerding, Konrad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mixture of time-warped trajectory models for movement decoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e2230b853516e7b05d79744fbd4c9c13-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e2230b853516e7b05d79744fbd4c9c13-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e2230b853516e7b05d79744fbd4c9c13-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 533032,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14788896822078331766&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Northwestern University; Northwestern University; Northwestern University",
        "aff_domain": "u.northwestern.edu; ; ",
        "email": "u.northwestern.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northwestern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northwestern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e1d2ae8e0e",
        "title": "Monte-Carlo Planning in Large POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/edfbe1afcf9246bb0d40eb4d8027d90f-Abstract.html",
        "author": "David Silver; Joel Veness",
        "abstract": "This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, Rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10x10 Battleship and Partially Observable PacMan, with approximately 10^18 and 10^56 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.",
        "bibtex": "@inproceedings{NIPS2010_edfbe1af,\n author = {Silver, David and Veness, Joel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Monte-Carlo Planning in Large POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/edfbe1afcf9246bb0d40eb4d8027d90f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 450446,
        "gs_citation": 1607,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3076708823025822549&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "MIT, Cambridge, MA 02139; UNSW, Sydney, Australia",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of New South Wales",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.unsw.edu.au",
        "aff_unique_abbr": "MIT;UNSW",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Sydney",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Australia"
    },
    {
        "id": "a1c2c73897",
        "title": "More data means less inference: A pseudo-max approach to structured learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/32b30a250abd6331e03a2a1f16466346-Abstract.html",
        "author": "David Sontag; Ofer Meshi; Amir Globerson; Tommi S. Jaakkola",
        "abstract": "The problem of learning to predict structured labels is of key importance in many applications. However, for general graph structure both learning and inference in this setting are intractable. Here we show that it is possible to circumvent this difficulty when the input distribution is rich enough via a method similar in spirit to pseudo-likelihood. We show how our new method achieves consistency, and illustrate empirically that it indeed performs as well as exact methods when sufficiently large training sets are used.",
        "bibtex": "@inproceedings{NIPS2010_32b30a25,\n author = {Sontag, David and Meshi, Ofer and Globerson, Amir and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {More data means less inference: A pseudo-max approach to structured learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 385715,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18240088591213282742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f4f89edac8",
        "title": "Moreau-Yosida Regularization for Grouped Tree Structure Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d490d7b4576290fa60eb31b5fc917ad1-Abstract.html",
        "author": "Jun Liu; Jieping Ye",
        "abstract": "We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-defined tree structure is based on a group-Lasso penalty, where one group is defined for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efficient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efficient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efficiency and effectiveness of the proposed algorithm.",
        "bibtex": "@inproceedings{NIPS2010_d490d7b4,\n author = {Liu, Jun and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Moreau-Yosida Regularization for Grouped Tree Structure Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d490d7b4576290fa60eb31b5fc917ad1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 294306,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8435869118563933839&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science and Engineering, Arizona State University; Computer Science and Engineering, Arizona State University",
        "aff_domain": "asu.edu;asu.edu",
        "email": "asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "530c8a4e1b",
        "title": "Movement extraction by detecting dynamics switches and repetitions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/704afe073992cbe4813cae2f7715336f-Abstract.html",
        "author": "Silvia Chiappa; Jan R. Peters",
        "abstract": "Many time-series such as human movement data consist of a sequence of basic actions, e.g., forehands and backhands in tennis. Automatically extracting and characterizing such actions is an important problem for a variety of different applications. In this paper, we present a probabilistic segmentation approach in which an observed time-series is modeled as a concatenation of segments corresponding to different basic actions. Each segment is generated through a noisy transformation of one of a few hidden trajectories representing different types of movement, with possible time re-scaling. We analyze three different approximation methods for dealing with model intractability, and demonstrate how the proposed approach can successfully segment table tennis movements recorded using a robot arm as haptic input device.",
        "bibtex": "@inproceedings{NIPS2010_704afe07,\n author = {Chiappa, Silvia and Peters, Jan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Movement extraction by detecting dynamics switches and repetitions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/704afe073992cbe4813cae2f7715336f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/704afe073992cbe4813cae2f7715336f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/704afe073992cbe4813cae2f7715336f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 605885,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7324516909619372486&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Statistical Laboratory, Wilberforce Road, Cambridge, UK; Max Planck Institute for Biological Cybernetics, Spemannstrasse 38, Tuebingen, Germany",
        "aff_domain": "statslab.cam.ac.uk;tuebingen.mpg.de",
        "email": "statslab.cam.ac.uk;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Statistical Laboratory;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "Cambridge;MPIBC",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Tuebingen",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "e8870b1dbf",
        "title": "Multi-Stage Dantzig Selector",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e5f6ad6ce374177eef023bf5d0c018b6-Abstract.html",
        "author": "Ji Liu; Peter Wonka; Jieping Ye",
        "abstract": "We consider the following sparse signal recovery (or feature selection) problem: given a design matrix $X\\in \\mathbb{R}^{n\\times m}$ $(m\\gg n)$ and a noisy observation vector $y\\in \\mathbb{R}^{n}$ satisfying $y=X\\beta^*+\\epsilon$ where $\\epsilon$ is the noise vector following a Gaussian distribution $N(0,\\sigma^2I)$, how to recover the signal (or parameter vector) $\\beta^*$ when the signal is sparse?  The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal $\\beta^*$. We show that if $X$ obeys a certain condition, then with a large probability the difference between the solution $\\hat\\beta$ estimated by the proposed method and the true solution $\\beta^*$ measured in terms of the $l_p$ norm ($p\\geq 1$) is bounded as \\begin{equation*} \\|\\hat\\beta-\\beta^*\\|_p\\leq \\left(C(s-N)^{1/p}\\sqrt{\\log m}+\\Delta\\right)\\sigma, \\end{equation*} $C$ is a constant, $s$ is the number of nonzero entries in $\\beta^*$, $\\Delta$ is independent of $m$ and is much smaller than the first term, and $N$ is the number of entries of $\\beta^*$ larger than a certain value in the order of $\\mathcal{O}(\\sigma\\sqrt{\\log m})$. The proposed method improves the estimation bound of the standard Dantzig selector approximately from $Cs^{1/p}\\sqrt{\\log m}\\sigma$ to $C(s-N)^{1/p}\\sqrt{\\log m}\\sigma$ where the value $N$ depends on the number of large entries in $\\beta^*$. When $N=s$, the proposed algorithm achieves the oracle solution with a high probability. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector.",
        "bibtex": "@inproceedings{NIPS2010_e5f6ad6c,\n author = {Liu, Ji and Wonka, Peter and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Stage Dantzig Selector},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 223502,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9e3742c491",
        "title": "Multi-View Active Learning in the Non-Realizable Case",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html",
        "author": "Wei Wang; Zhi-Hua Zhou",
        "abstract": "The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be $\\widetilde{O}(\\log \\frac{1}{\\epsilon})$, contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\\widetilde{O}(\\frac{1}{\\epsilon})$, where the order of $1/\\epsilon$ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of $1/\\epsilon$ is related to the parameter in Tsybakov noise.",
        "bibtex": "@inproceedings{NIPS2010_3621f145,\n author = {Wang, Wei and Zhou, Zhi-Hua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-View Active Learning in the Non-Realizable Case},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 171690,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12260768331138790038&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China",
        "aff_domain": "lamda.nju.edu.cn;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "cb0660e51d",
        "title": "Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/feab05aa91085b7a8012516bc3533958-Abstract.html",
        "author": "Serhat Bucak; Rong Jin; Anil K. Jain",
        "abstract": "Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity  of kernel learning in computer vision problems. In this work, we develop an efficient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under  consideration  share the same combination of kernel functions, and the objective is to find the optimal kernel combination that benefits all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge  frequently  encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis shows that the complexity of our algorithm is $O(m^{1/3}\\sqrt{ln m})$, where $m$ is the number of classes. Empirical studies with object recognition show that while achieving similar classification accuracy, the proposed method is significantly more efficient than the state-of-the-art algorithms for ML-MKL.",
        "bibtex": "@inproceedings{NIPS2010_feab05aa,\n author = {Bucak, Serhat and Jin, Rong and Jain, Anil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/feab05aa91085b7a8012516bc3533958-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/feab05aa91085b7a8012516bc3533958-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/feab05aa91085b7a8012516bc3533958-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146352,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11762421758598150364&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Comp. Sci. & Eng.*; Dept. of Comp. Sci. & Eng.*; Dept. of Comp. Sci. & Eng.* + Dept. of Brain & Cognitive Eng.\u2020",
        "aff_domain": "cse.msu.edu;cse.msu.edu;cse.msu.edu",
        "email": "cse.msu.edu;cse.msu.edu;cse.msu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University Affiliation;Dept. of Brain & Cognitive Eng.",
        "aff_unique_dep": "Department of Computer Science and Engineering;Brain & Cognitive Engineering",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9b3697271d",
        "title": "Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0d0fd7c6e093f7b804fa0150b875b868-Abstract.html",
        "author": "Manas Pathak; Shantanu Rane; Bhiksha Raj",
        "abstract": "As increasing amounts of sensitive personal information finds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model  provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classifier  using classifiers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classifier. We also present a detailed theoretical analysis containing a proof of differential privacy  of the perturbed aggregate classifier and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset.",
        "bibtex": "@inproceedings{NIPS2010_0d0fd7c6,\n author = {Pathak, Manas and Rane, Shantanu and Raj, Bhiksha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 271821,
        "gs_citation": 247,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14982208684419261745&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Carnegie Mellon University; Mitsubishi Electric Research Labs; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;merl.com;cs.cmu.edu",
        "email": "cs.cmu.edu;merl.com;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Mitsubishi Electric Research Laboratories",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.merl.com",
        "aff_unique_abbr": "CMU;MERL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f5015b912",
        "title": "Multiple Kernel Learning and the SMO Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html",
        "author": "Zhaonan Sun; Nawanol Ampornpunt; Manik Varma; S.v.n. Vishwanathan",
        "abstract": "Our objective is to train $p$-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efficiently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the $p$-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efficiency and is significantly faster than the state-of-the-art specialised $p$-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on fifty thousand points in less than half an hour on a single core.",
        "bibtex": "@inproceedings{NIPS2010_a01a0380,\n author = {Sun, Zhaonan and Ampornpunt, Nawanol and Varma, Manik and Vishwanathan, S.v.n.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiple Kernel Learning and the SMO Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a01a0380ca3c61428c26a231f0e49a09-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 153939,
        "gs_citation": 235,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11969256924677929330&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Purdue University; Purdue University; Purdue University; Microsoft Research India",
        "aff_domain": "stat.purdue.edu;stat.purdue.edu;cs.purdue.edu;microsoft.com",
        "email": "stat.purdue.edu;stat.purdue.edu;cs.purdue.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Purdue University;Microsoft",
        "aff_unique_dep": ";Microsoft Research India",
        "aff_unique_url": "https://www.purdue.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "Purdue;MSR India",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "4cd594d8c6",
        "title": "Multitask Learning without Label Correspondences",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/aff1621254f7c1be92f64550478c56e6-Abstract.html",
        "author": "Novi Quadrianto; James Petterson; Tib\u00e9rio S. Caetano; Alex J. Smola; S.v.n. Vishwanathan",
        "abstract": "We propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available. This is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle. Our method directly maximizes the mutual information among the labels, and we show that the resulting objective function can be efficiently optimized using existing algorithms. Our proposed approach has a direct application for data integration with different label spaces for the purpose of classification, such as integrating Yahoo! and DMOZ web directories.",
        "bibtex": "@inproceedings{NIPS2010_aff16212,\n author = {Quadrianto, Novi and Petterson, James and Caetano, Tib\\'{e}rio and Smola, Alex and Vishwanathan, S.v.n.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multitask Learning without Label Correspondences},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 190490,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16553467254521032827&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "SML-NICTA & RSISE-ANU, Canberra, ACT, Australia; Yahoo! Research, Santa Clara, CA, USA; SML-NICTA & RSISE-ANU, Canberra, ACT, Australia; Purdue University, West Lafayette, IN, USA; SML-NICTA & RSISE-ANU, Canberra, ACT, Australia",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "National Information and Communication Technology Australia (NICTA);Yahoo! Research;Purdue University",
        "aff_unique_dep": "SML-NICTA;;",
        "aff_unique_url": "https://www.nicta.com.au;https://research.yahoo.com;https://www.purdue.edu",
        "aff_unique_abbr": "NICTA;Yahoo! Res;Purdue",
        "aff_campus_unique_index": "0;1;0;2;0",
        "aff_campus_unique": "Canberra;Santa Clara;West Lafayette",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "590e6ae3e4",
        "title": "Multivariate Dyadic Regression Trees for Sparse Learning Problems",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8f121ce07d74717e0b1f21d122e04521-Abstract.html",
        "author": "Han Liu; Xi Chen",
        "abstract": "We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs).  Unlike traditional dyadic decision trees (DDTs) or classification and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty.  Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of $(\\alpha, C)$-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable  for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets.",
        "bibtex": "@inproceedings{NIPS2010_8f121ce0,\n author = {Liu, Han and Chen, Xi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multivariate Dyadic Regression Trees for Sparse Learning Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 119418,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=410032432539712083&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c794dc2826",
        "title": "Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/44c4c17332cace2124a1a836d9fc4b6f-Abstract.html",
        "author": "Atsushi Miyamae; Yuichi Nagata; Isao Ono; Shigenobu Kobayashi",
        "abstract": "In this paper, we propose an efficient algorithm for estimating the natural policy gradient with parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods.",
        "bibtex": "@inproceedings{NIPS2010_44c4c173,\n author = {Miyamae, Atsushi and Nagata, Yuichi and Ono, Isao and Kobayashi, Shigenobu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/44c4c17332cace2124a1a836d9fc4b6f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/44c4c17332cace2124a1a836d9fc4b6f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/44c4c17332cace2124a1a836d9fc4b6f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 235201,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3583978459372365670&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology, Kanagawa, Japan + Research Fellow of the Japan Society for the Promotion of Science; Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology, Kanagawa, Japan; Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology, Kanagawa, Japan; Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology, Kanagawa, Japan",
        "aff_domain": "fe.dis.titech.ac.jp;fe.dis.titech.ac.jp;dis.titech.ac.jp;dis.titech.ac.jp",
        "email": "fe.dis.titech.ac.jp;fe.dis.titech.ac.jp;dis.titech.ac.jp;dis.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology;Japan Society for the Promotion of Science",
        "aff_unique_dep": "Department of Computational Intelligence and Systems Science;Research Fellow",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.jsps.go.jp",
        "aff_unique_abbr": "Titech;JSPS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Kanagawa;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "eb9a84de4c",
        "title": "Near-Optimal Bayesian Active Learning with Noisy Observations",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html",
        "author": "Daniel Golovin; Andreas Krause; Debajyoti Ray",
        "abstract": "We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non\u2013uniform cost and their noise is correlated. We also propose EffECXtive, a particularly fast approximation of EC2, and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty.",
        "bibtex": "@inproceedings{NIPS2010_1e6e0a04,\n author = {Golovin, Daniel and Krause, Andreas and Ray, Debajyoti},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-Optimal Bayesian Active Learning with Noisy Observations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/1e6e0a04d20f50967c64dac2d639a577-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 432992,
        "gs_citation": 251,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13527612973106848668&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "641ad90e9e",
        "title": "Network Flow Algorithms for Structured Sparsity",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/537d9b6c927223c796cac288cced29df-Abstract.html",
        "author": "Julien Mairal; Rodolphe Jenatton; Francis R. Bach; Guillaume R. Obozinski",
        "abstract": "We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of $\\ell_\\infty$-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of groups and variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems.",
        "bibtex": "@inproceedings{NIPS2010_537d9b6c,\n author = {Mairal, Julien and Jenatton, Rodolphe and Bach, Francis and Obozinski, Guillaume R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Network Flow Algorithms for Structured Sparsity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/537d9b6c927223c796cac288cced29df-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/537d9b6c927223c796cac288cced29df-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/537d9b6c927223c796cac288cced29df-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 323183,
        "gs_citation": 206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4695040356662615936&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "INRIA - Willow Project-Team\u2020; INRIA - Willow Project-Team\u2020; INRIA - Willow Project-Team\u2020; INRIA - Willow Project-Team\u2020",
        "aff_domain": "inria.fr;inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "Willow Project-Team",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "b4fe67bb91",
        "title": "New Adaptive Algorithms for Online Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2bcab9d935d219641434683dd9d18a03-Abstract.html",
        "author": "Francesco Orabona; Koby Crammer",
        "abstract": "We propose a general framework to online learning for   classification problems with time-varying potential functions in the   adversarial setting. This framework allows to design and prove   relative mistake bounds for any generic loss function. The mistake   bounds can be specialized for the hinge loss, allowing to recover   and improve the bounds of known online classification   algorithms. By optimizing the general bound we derive a new online   classification algorithm, called NAROW, that hybridly uses adaptive- and fixed- second order   information. We analyze the properties of the algorithm and   illustrate its performance using synthetic dataset.",
        "bibtex": "@inproceedings{NIPS2010_2bcab9d9,\n author = {Orabona, Francesco and Crammer, Koby},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {New Adaptive Algorithms for Online Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2bcab9d935d219641434683dd9d18a03-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1802841,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18070284357687776862&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "DSI, Universit `a degli Studi di Milano, Milano, 20135 Italy; Department of Electrical Enginering, The Technion, Haifa, 32000 Israel",
        "aff_domain": "dsi.unimi.it;ee.technion.ac.il",
        "email": "dsi.unimi.it;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universit\u00e0 degli Studi di Milano;Technion",
        "aff_unique_dep": "DSI;Department of Electrical Engineering",
        "aff_unique_url": "https://www.unimi.it;http://www.technion.ac.il",
        "aff_unique_abbr": ";Technion",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Milano;Haifa",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Italy;Israel"
    },
    {
        "id": "f02e8e033b",
        "title": "Non-Stochastic Bandit Slate Problems",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/390e982518a50e280d8e2b535462ec1f-Abstract.html",
        "author": "Satyen Kale; Lev Reyzin; Robert E. Schapire",
        "abstract": "We consider bandit problems, motivated by applications in online advertising and news story selection, in which the learner must repeatedly select a slate, that is, a subset of size s from K possible actions, and then receives rewards for just the selected actions. The goal is to minimize the regret with respect to total reward of the best slate computed in hindsight. We consider unordered and ordered versions of the problem, and give efficient algorithms which have regret O(sqrt(T)), where the constant depends on the specific nature of the problem. We also consider versions of the problem where we have access to a number of policies which make recommendations for slates in every round, and give algorithms with O(sqrt(T)) regret for competing with the best such policy as well. We make use of the technique of relative entropy projections combined with the usual multiplicative weight update algorithm to obtain our algorithms.",
        "bibtex": "@inproceedings{NIPS2010_390e9825,\n author = {Kale, Satyen and Reyzin, Lev and Schapire, Robert E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-Stochastic Bandit Slate Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/390e982518a50e280d8e2b535462ec1f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 207250,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11564463574501979563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Yahoo! Research, Santa Clara, CA; Georgia Inst. of Technology, Atlanta, GA + Yahoo! Research, New York; Princeton University, Princeton, NJ + Yahoo! Research, New York",
        "aff_domain": "yahoo-inc.com;cc.gatech.edu;cs.princeton.edu",
        "email": "yahoo-inc.com;cc.gatech.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;2+0",
        "aff_unique_norm": "Yahoo! Research;Georgia Institute of Technology;Princeton University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://research.yahoo.com;https://www.gatech.edu;https://www.princeton.edu",
        "aff_unique_abbr": "Yahoo! Res;Georgia Tech;Princeton",
        "aff_campus_unique_index": "0;1+2;3+2",
        "aff_campus_unique": "Santa Clara;Atlanta;New York;Princeton",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7e42b3d8a7",
        "title": "Nonparametric Bayesian Policy Priors for Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c44e503833b64e9f27197a484f4257c0-Abstract.html",
        "author": "Finale Doshi-velez; David Wingate; Nicholas Roy; Joshua B. Tenenbaum",
        "abstract": "We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning.",
        "bibtex": "@inproceedings{NIPS2010_c44e5038,\n author = {Doshi-velez, Finale and Wingate, David and Roy, Nicholas and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Bayesian Policy Priors for Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c44e503833b64e9f27197a484f4257c0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 180235,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1163119569248222189&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 18,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dbf6ec9f5f",
        "title": "Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html",
        "author": "Lauren Hannah; Warren Powell; David M. Blei",
        "abstract": "We study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation for the joint distribution of state-outcome pairs to create weights for previous observations. The weights effectively group similar states. Those similar to the current state are used to create a convex, deterministic approximation of the objective function. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We offer two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show Dirichlet process weights can offer substantial benefits over kernel based weights and, more generally, that nonparametric estimation methods provide good solutions to otherwise intractable problems.",
        "bibtex": "@inproceedings{NIPS2010_e1e32e23,\n author = {Hannah, Lauren and Powell, Warren and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 329211,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4667591109613221835&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Duke University; Princeton University; Princeton University",
        "aff_domain": "duke.edu;princeton.edu;cs.princeton.edu",
        "email": "duke.edu;princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Duke University;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.duke.edu;https://www.princeton.edu",
        "aff_unique_abbr": "Duke;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b8c5536460",
        "title": "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html",
        "author": "Li-jia Li; Hao Su; Li Fei-fei; Eric P. Xing",
        "abstract": "Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efficient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.",
        "bibtex": "@inproceedings{NIPS2010_140f6969,\n author = {Li, Li-jia and Su, Hao and Fei-fei, Li and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Object Bank: A High-Level Image Representation for Scene Classification \\&amp; Semantic Feature Sparsification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/140f6969d5213fd0ece03148e62e461e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2755708,
        "gs_citation": 1261,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7023849659259118668&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dcd56e7c23",
        "title": "Occlusion Detection and Motion Estimation with Convex Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/9778d5d219c5080b9a6a17bef029331c-Abstract.html",
        "author": "Alper Ayvaci; Michalis Raptis; Stefano Soatto",
        "abstract": "We tackle the problem of simultaneously detecting occlusions and estimating optical flow. We show that, under standard assumptions of Lambertian reflection and static illumination, the task can be posed as a convex minimization problem. Therefore, the solution, computed using efficient algorithms, is guaranteed to be globally optimal, for any number of independently moving objects, and any number of occlusion layers. We test the proposed algorithm on benchmark datasets, expanded to enable evaluation of occlusion detection performance.",
        "bibtex": "@inproceedings{NIPS2010_9778d5d2,\n author = {Ayvaci, Alper and Raptis, Michalis and Soatto, Stefano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Occlusion Detection and Motion Estimation with Convex Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/9778d5d219c5080b9a6a17bef029331c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 809802,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7546403520857209856&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f427005b77",
        "title": "On Herding and the Perceptron Cycling Theorem",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html",
        "author": "Andrew Gelfand; Yutian Chen; Laurens Maaten; Max Welling",
        "abstract": "The paper develops a connection between traditional perceptron algorithms and recently introduced herding algorithms. It is shown that both algorithms can be viewed as an application of the perceptron cycling theorem. This connection strengthens some herding results and suggests new (supervised) herding algorithms that, like CRFs or discriminative RBMs, make predictions by conditioning on the input attributes. We develop and investigate variants of conditional herding, and show that conditional herding leads to practical algorithms that perform better than or on par with related classifiers such as the voted perceptron and the discriminative RBM.",
        "bibtex": "@inproceedings{NIPS2010_019d385e,\n author = {Gelfand, Andrew and Chen, Yutian and Maaten, Laurens and Welling, Max},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Herding and the Perceptron Cycling Theorem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/019d385eb67632a7e958e23f24bd07d7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 561814,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1836115535472925742&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of CSE, UC San Diego + PRB Lab, Delft University of Tech.",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu;gmail.com",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1+2",
        "aff_unique_norm": "University of California, Irvine;University of California, San Diego;Delft University of Technology",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science and Engineering;PRB Lab",
        "aff_unique_url": "https://www.uci.edu;https://www.ucsd.edu;https://www.tudelft.nl",
        "aff_unique_abbr": "UCI;UCSD;TUDelft",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Irvine;San Diego;",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "United States;Netherlands"
    },
    {
        "id": "8bbd0511e8",
        "title": "On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/35cf8659cfcb13224cbd47863a34fc58-Abstract.html",
        "author": "Tang Jie; Pieter Abbeel",
        "abstract": "Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (a) using the past experience to estimate {\\em only} the gradient of the expected return $U(\\theta)$ at the current policy parameterization $\\theta$, rather than to obtain a more complete estimate of $U(\\theta)$, and (b) using past experience under the current policy {\\em only} rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines---a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds.",
        "bibtex": "@inproceedings{NIPS2010_35cf8659,\n author = {Jie, Tang and Abbeel, Pieter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35cf8659cfcb13224cbd47863a34fc58-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/35cf8659cfcb13224cbd47863a34fc58-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/35cf8659cfcb13224cbd47863a34fc58-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 512926,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=849973878221575938&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7b35efa77d",
        "title": "On the Convexity of Latent Social Network Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/cd758e8f59dfdf06a852adad277986ca-Abstract.html",
        "author": "Seth Myers; Jure Leskovec",
        "abstract": "In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1-like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks on thousands of nodes in a matter of minutes.",
        "bibtex": "@inproceedings{NIPS2010_cd758e8f,\n author = {Myers, Seth and Leskovec, Jure},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Convexity of Latent Social Network Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cd758e8f59dfdf06a852adad277986ca-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/cd758e8f59dfdf06a852adad277986ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/cd758e8f59dfdf06a852adad277986ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 141751,
        "gs_citation": 394,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9490688862818587641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Institute for Computational and Mathematical Engineering, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "stanford.edu;cs.stanford.edu",
        "email": "stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Institute for Computational and Mathematical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "124f8f1da4",
        "title": "On the Theory of Learnining with Privileged Information",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c73dfe6c630edb4c1692db67c510f65c-Abstract.html",
        "author": "Dmitry Pechyony; Vladimir Vapnik",
        "abstract": "In Learning Using Privileged Information (LUPI) paradigm, along with the standard training data in the decision space, a teacher supplies a learner with the privileged information in the correcting space. The goal of the learner is to find a classifier with a low generalization error in the decision space. We consider a new version of  empirical risk minimization algorithm, called Privileged ERM, that takes into account the privileged information in order to find a good function in the decision space. We outline the conditions on the correcting space that, if satisfied, allow Privileged ERM to have much faster learning rate in the decision space than the one of the regular empirical risk minimization.",
        "bibtex": "@inproceedings{NIPS2010_c73dfe6c,\n author = {Pechyony, Dmitry and Vapnik, Vladimir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Theory of Learnining with Privileged Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 219196,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8042224633239671129&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "NEC Laboratories; NEC Laboratories",
        "aff_domain": "nec-labs.com;nec-labs.com",
        "email": "nec-labs.com;nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NEC Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nec-labs.com",
        "aff_unique_abbr": "NEC Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4da7d20c69",
        "title": "Online Classification with Specificity Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/9cfdf10e8fc047a44b08ed031e1f0ed1-Abstract.html",
        "author": "Andrey Bernstein; Shie Mannor; Nahum Shimkin",
        "abstract": "We consider the online binary classification problem, where we are given m classifiers. At each stage, the classifiers map the input to the probability that the input belongs to the positive class. An online classification meta-algorithm is an algorithm that combines the outputs of the classifiers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classifiers. In this paper, we use sensitivity and  specificity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm which satisfies the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold, and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classifiers that satisfies fp-rate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classifiers, we show that this algorithm takes a very simple form. To our best knowledge, this is the first algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting.",
        "bibtex": "@inproceedings{NIPS2010_9cfdf10e,\n author = {Bernstein, Andrey and Mannor, Shie and Shimkin, Nahum},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Classification with Specificity Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146894,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12093492270169839285&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, 32000, Israel; Department of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, 32000, Israel; Department of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, 32000, Israel",
        "aff_domain": "tx.technion.ac.il;ee.technion.ac.il;ee.technion.ac.il",
        "email": "tx.technion.ac.il;ee.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "49c464ba11",
        "title": "Online Learning for Latent Dirichlet Allocation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html",
        "author": "Matthew Hoffman; Francis R. Bach; David M. Blei",
        "abstract": "We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.",
        "bibtex": "@inproceedings{NIPS2010_71f6278d,\n author = {Hoffman, Matthew and Bach, Francis and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning for Latent Dirichlet Allocation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 730851,
        "gs_citation": 2378,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18273883294143239997&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Computer Science, Princeton University, Princeton, NJ; Department of Computer Science, Princeton University, Princeton, NJ; INRIA\u2014Ecole Normale Sup\u00e9rieure, Paris, France",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu;ens.fr",
        "email": "cs.princeton.edu;cs.princeton.edu;ens.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Princeton University;INRIA",
        "aff_unique_dep": "Department of Computer Science;Ecole Normale Sup\u00e9rieure",
        "aff_unique_url": "https://www.princeton.edu;https://www.inria.fr",
        "aff_unique_abbr": "Princeton;INRIA",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Princeton;Paris",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "8166129dc6",
        "title": "Online Learning in The Manifold of Low-Rank Matrices",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/298923c8190045e91288b430794814c4-Abstract.html",
        "author": "Uri Shalit; Daphna Weinshall; Gal Chechik",
        "abstract": "When learning models that are represented in matrix forms, enforcing   a low-rank constraint can dramatically improve the memory and run   time complexity, while providing a natural regularization of the   model.  However, naive approaches for minimizing functions over the   set of low-rank matrices are either prohibitively time   consuming (repeated singular value decomposition of the matrix) or   numerically unstable (optimizing a factored representation of the   low rank matrix). We build on recent advances in optimization over   manifolds, and describe an iterative online learning procedure, consisting of a   gradient step, followed by a second-order retraction back to the   manifold. While the ideal retraction is hard to compute, and so is   the projection operator that approximates it, we describe another   second-order retraction that can be computed efficiently, with run   time and memory complexity of O((n+m)k) for a rank-k   matrix of dimension m x n, given rank one gradients.  We use   this algorithm, LORETA, to learn a matrix-form similarity measure over pairs of   documents represented as high dimensional vectors. LORETA   improves the mean average precision over a passive-   aggressive approach in a factorized model, and also improves over   a full model trained over pre-selected features using the same   memory requirements. LORETA also showed consistent improvement over   standard methods in a large (1600 classes) multi-label image classification task.",
        "bibtex": "@inproceedings{NIPS2010_298923c8,\n author = {Shalit, Uri and Weinshall, Daphna and Chechik, Gal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning in The Manifold of Low-Rank Matrices},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/298923c8190045e91288b430794814c4-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/298923c8190045e91288b430794814c4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/298923c8190045e91288b430794814c4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/298923c8190045e91288b430794814c4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 198765,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1109789772377195169&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Computer Science Dept. and ICNC, The Hebrew University of Jerusalem + Gonda Brain Research Center, Bar Ilan University; Computer Science Dept. and ICNC, The Hebrew University of Jerusalem; Google Research + Gonda Brain Research Center, Bar Ilan University",
        "aff_domain": "mail.huji.ac.il;cs.huji.ac.il;google.com",
        "email": "mail.huji.ac.il;cs.huji.ac.il;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;2+1",
        "aff_unique_norm": "Hebrew University of Jerusalem;Bar-Ilan University;Google",
        "aff_unique_dep": "Computer Science Dept.;Gonda Brain Research Center;Google Research",
        "aff_unique_url": "http://www.huji.ac.il;https://www.biu.ac.il;https://research.google",
        "aff_unique_abbr": "HUJI;BIU;Google Research",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Jerusalem;;Mountain View",
        "aff_country_unique_index": "0+0;0;1+0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "728debdac6",
        "title": "Online Learning: Random Averages, Combinatorial Parameters, and Learnability",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e00406144c1e7e35240afed70f34166a-Abstract.html",
        "author": "Alexander Rakhlin; Karthik Sridharan; Ambuj Tewari",
        "abstract": "We develop a theory of online learning by defining several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fat-shattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting.",
        "bibtex": "@inproceedings{NIPS2010_e0040614,\n author = {Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning: Random Averages, Combinatorial Parameters, and Learnability},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e00406144c1e7e35240afed70f34166a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/e00406144c1e7e35240afed70f34166a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e00406144c1e7e35240afed70f34166a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 240028,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3520534837130788142&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b9a1d8e047",
        "title": "Online Markov Decision Processes under Bandit Feedback",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7bb060764a818184ebb1cc0d43d382aa-Abstract.html",
        "author": "Gergely Neu; Andras Antos; Andr\u00e1s Gy\u00f6rgy; Csaba Szepesv\u00e1ri",
        "abstract": "We consider online learning in finite stochastic Markovian environments where in each time step a new reward function is chosen by an oblivious adversary. The goal of the learning agent is to compete with the best stationary policy in terms of the total reward received. In each time step the agent observes the current state and the reward associated with the last transition, however, the agent does not observe the rewards associated with other state-action pairs. The agent is assumed to know the transition probabilities. The state of the art result for this setting is a no-regret algorithm. In this paper we propose a new learning algorithm and assuming that stationary policies mix uniformly fast, we show that after T time steps, the expected regret of the new algorithm is O(T^{2/3} (ln T)^{1/3}), giving the first rigorously proved convergence rate result for the problem.",
        "bibtex": "@inproceedings{NIPS2010_7bb06076,\n author = {Neu, Gergely and Antos, Andras and Gy\\\"{o}rgy, Andr\\'{a}s and Szepesv\\'{a}ri, Csaba},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Markov Decision Processes under Bandit Feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 261876,
        "gs_citation": 229,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14819018663945018592&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 25,
        "aff": "Department of Computer Science and Information Theory, Budapest University of Technology and Economics, Hungary+Machine Learning Research Group, MTA SZTAKI Institute for Computer Science and Control, Hungary; Machine Learning Research Group, MTA SZTAKI Institute for Computer Science and Control, Hungary; Department of Computing Science, University of Alberta, Canada; Machine Learning Research Group, MTA SZTAKI Institute for Computer Science and Control, Hungary",
        "aff_domain": "gmail.com;szit.bme.hu;ualberta.ca;szit.bme.hu",
        "email": "gmail.com;szit.bme.hu;ualberta.ca;szit.bme.hu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;2;1",
        "aff_unique_norm": "Budapest University of Technology and Economics;MTA SZTAKI Institute for Computer Science and Control;University of Alberta",
        "aff_unique_dep": "Department of Computer Science and Information Theory;Machine Learning Research Group;Department of Computing Science",
        "aff_unique_url": "https://www.bme.hu;;https://www.ualberta.ca",
        "aff_unique_abbr": "BME;;UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;1;0",
        "aff_country_unique": "Hungary;Canada"
    },
    {
        "id": "3c9221c472",
        "title": "Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/550a141f12de6341fba65b0ad0433500-Abstract.html",
        "author": "Paolo Viappiani; Craig Boutilier",
        "abstract": "Bayesian approaches to utility elicitation typically adopt (myopic) expected value of information (EVOI) as a natural criterion for selecting queries.  However, EVOI-optimization is  usually computationally prohibitive.  In this paper, we examine EVOI optimization using \\emph{choice queries}, queries in which a user  is ask to select her most preferred product from a set. We show that,  under very general assumptions, the optimal choice query w.r.t.\\ EVOI coincides with \\emph{optimal recommendation set}, that is, a set maximizing expected utility of the user selection. Since recommendation set optimization is a simpler, submodular problem, this can greatly reduce the complexity of both exact and approximate (greedy) computation of optimal choice queries.  We  also examine the case where user responses to choice queries are error-prone (using both constant and follow mixed multinomial logit  noise models) and provide worst-case guarantees.   Finally we present a local search technique that works well with large outcome spaces.",
        "bibtex": "@inproceedings{NIPS2010_550a141f,\n author = {Viappiani, Paolo and Boutilier, Craig},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/550a141f12de6341fba65b0ad0433500-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/550a141f12de6341fba65b0ad0433500-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/550a141f12de6341fba65b0ad0433500-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/550a141f12de6341fba65b0ad0433500-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 193925,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3881058369042595249&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Toronto + University of Regina + Aalborg University; Department of Computer Science, University of Toronto",
        "aff_domain": "gmail.com;cs.toronto.edu",
        "email": "gmail.com;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0",
        "aff_unique_norm": "University of Toronto;University of Regina;Aalborg University",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.uregina.ca;https://www.aau.dk",
        "aff_unique_abbr": "U of T;U of R;AAU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto;",
        "aff_country_unique_index": "0+0+1;0",
        "aff_country_unique": "Canada;Denmark"
    },
    {
        "id": "a86228cbd1",
        "title": "Optimal Web-Scale Tiering as a Flow Problem",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4c27cea8526af8cfee3be5e183ac9605-Abstract.html",
        "author": "Gilbert Leung; Novi Quadrianto; Kostas Tsioutsiouliklis; Alex J. Smola",
        "abstract": "We present a fast online solver for large scale maximum-flow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics. Our algorithm solves an integer linear program in an online fashion. It exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to solve the problem as a convex online game. The algorithm generates approximate solutions of max-flow problems by performing stochastic gradient descent on a set of flows. We apply the algorithm to optimize tier arrangement of over 80 Million web pages on a layered set of caches to serve an incoming query stream optimally. We provide an empirical demonstration of the effectiveness of our method on real query-pages data.",
        "bibtex": "@inproceedings{NIPS2010_4c27cea8,\n author = {Leung, Gilbert and Quadrianto, Novi and Tsioutsiouliklis, Kostas and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Web-Scale Tiering as a Flow Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 336037,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=660019312600000621&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "eBay, Inc., San Jose, CA, USA; SML-NICTA & RSISE-ANU, Canberra, ACT, Australia; Yahoo! Research, Santa Clara, CA, USA; Yahoo! Labs, Sunnyvale, CA, USA",
        "aff_domain": "alum.mit.edu;gmail.com;smola.org;yahoo-inc.com",
        "email": "alum.mit.edu;gmail.com;smola.org;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "eBay, Inc.;National Information and Communication Technology Australia (NICTA);Yahoo! Research;Yahoo!",
        "aff_unique_dep": ";SML-NICTA;;Yahoo! Labs",
        "aff_unique_url": "https://www.ebayinc.com;https://www.nicta.com.au;https://research.yahoo.com;https://yahoo.com",
        "aff_unique_abbr": "eBay;NICTA;Yahoo! Res;Yahoo!",
        "aff_campus_unique_index": "0;1;2;3",
        "aff_campus_unique": "San Jose;Canberra;Santa Clara;Sunnyvale",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Australia"
    },
    {
        "id": "c83ee50dda",
        "title": "Optimal learning rates for Kernel Conjugate Gradient regression",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/b2f627fff19fda463cb386442eac2b3d-Abstract.html",
        "author": "Gilles Blanchard; Nicole Kr\u00e4mer",
        "abstract": "We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overfitting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: first, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if  the true regression function belongs to the reproducing kernel Hilbert space. If the latter assumption is not fulfilled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates in these two situations match state-of-the-art results that were recently obtained for the least squares support vector machine and for linear regularization operators.",
        "bibtex": "@inproceedings{NIPS2010_b2f627ff,\n author = {Blanchard, Gilles and Kr\\\"{a}mer, Nicole},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal learning rates for Kernel Conjugate Gradient regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 195348,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=59163754919817628&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Mathematics Institute, University of Potsdam; Weierstrass Institute",
        "aff_domain": "math.uni-potsdam.de;wias-berlin.de",
        "email": "math.uni-potsdam.de;wias-berlin.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Potsdam;Weierstrass Institute for Applied Analysis and Stochastics",
        "aff_unique_dep": "Mathematics Institute;",
        "aff_unique_url": "https://www.uni-potsdam.de;https://www.wias-berlin.de/",
        "aff_unique_abbr": ";WIAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "9bddd1566d",
        "title": "Over-complete representations on recurrent neural networks can support persistent percepts",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/39059724f73a9969845dfe4146c5660e-Abstract.html",
        "author": "Shaul Druckmann; Dmitri B. Chklovskii",
        "abstract": "A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive field of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit.",
        "bibtex": "@inproceedings{NIPS2010_39059724,\n author = {Druckmann, Shaul and Chklovskii, Dmitri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Over-complete representations on recurrent neural networks can support persistent percepts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39059724f73a9969845dfe4146c5660e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/39059724f73a9969845dfe4146c5660e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/39059724f73a9969845dfe4146c5660e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1231249,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8673481523048303703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Janelia Farm Research Campus, Howard Hughes Medical Institute, Ashburn, V A 20147; Janelia Farm Research Campus, Howard Hughes Medical Institute, Ashburn, V A 20147",
        "aff_domain": "janelia.hhmi.org;janelia.hhmi.org",
        "email": "janelia.hhmi.org;janelia.hhmi.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Howard Hughes Medical Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hhmi.org",
        "aff_unique_abbr": "HHMI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Janelia Farm",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "29fa8226fa",
        "title": "PAC-Bayesian Model Selection for Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html",
        "author": "Mahdi M. Fard; Joelle Pineau",
        "abstract": "This paper introduces the first set of PAC-Bayesian bounds for the batch reinforcement learning problem in finite state spaces. These bounds hold regardless of the correctness of the prior distribution. We demonstrate how such bounds can be used for model-selection in control problems where prior information is available either on the dynamics of the environment, or on the value of actions. Our empirical results confirm that PAC-Bayesian model-selection is able to leverage prior distributions when they are informative and, unlike standard Bayesian RL approaches, ignores them when they are misleading.",
        "bibtex": "@inproceedings{NIPS2010_66368270,\n author = {Fard, M. and Pineau, Joelle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PAC-Bayesian Model Selection for Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/66368270ffd51418ec58bd793f2d9b1b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272702,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17258129099723943327&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2772844923",
        "title": "Parallelized Stochastic Gradient Descent",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/abea47ba24142ed16b7d8fbf2c740e0d-Abstract.html",
        "author": "Martin Zinkevich; Markus Weimer; Lihong Li; Alex J. Smola",
        "abstract": "With the increase in available data parallel machine learning has   become an increasingly pressing problem. In this paper we present   the first parallel stochastic gradient descent algorithm including a   detailed analysis and experimental evidence. Unlike prior work on   parallel optimization algorithms our   variant comes with parallel acceleration guarantees and it poses no   overly tight latency constraints, which might only be available in   the multicore setting. Our analysis introduces a novel proof   technique --- contractive mappings to quantify the   speed of convergence of parameter distributions to their asymptotic   limits. As a side effect this answers the question of how quickly   stochastic gradient descent algorithms reach the asymptotically   normal regime.",
        "bibtex": "@inproceedings{NIPS2010_abea47ba,\n author = {Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallelized Stochastic Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 554889,
        "gs_citation": 1890,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14210428190493224944&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Yahoog )abs; Yahoog )abs; Yahoog )abs; Yahoog )abs",
        "aff_domain": "yahoo-inc.com;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com",
        "email": "yahoo-inc.com;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Yahoo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yahoo.com",
        "aff_unique_abbr": "Yahoo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a3f994e08b",
        "title": "Parametric Bandits: The Generalized Linear Case",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c2626d850c80ea07e7511bbae4c76f4b-Abstract.html",
        "author": "Sarah Filippi; Olivier Cappe; Aur\u00e9lien Garivier; Csaba Szepesv\u00e1ri",
        "abstract": "We consider structured multi-armed bandit tasks in which the agent is guided by prior structural knowledge that can be exploited to efficiently select the optimal arm(s) in situations where the number of arms is large, or even infinite. We pro- pose a new optimistic, UCB-like, algorithm for non-linearly parameterized bandit problems using the Generalized Linear Model (GLM) framework. We analyze the regret of the proposed algorithm, termed GLM-UCB, obtaining results similar to those recently proved in the literature for the linear regression case. The analysis also highlights a key difficulty of the non-linear case which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual efficiency of current parameterized bandit algorithms is often deceiving in practice, we provide an asymptotic argument leading to significantly faster convergence. Simulation studies on real data sets illustrate the performance and the robustness of the proposed GLM-UCB approach.",
        "bibtex": "@inproceedings{NIPS2010_c2626d85,\n author = {Filippi, Sarah and Cappe, Olivier and Garivier, Aur\\'{e}lien and Szepesv\\'{a}ri, Csaba},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parametric Bandits: The Generalized Linear Case},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 150749,
        "gs_citation": 608,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17423387631840636989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "LTCI TelecomParisTechet CNRS Paris,France; LTCI TelecomParisTechet CNRS Paris,France; LTCI TelecomParisTechet CNRS Paris,France; RLAI Laboratory UniversityofAlberta Edmonton,Canada",
        "aff_domain": "telecom-paristech.fr;telecom-paristech.fr;telecom-paristech.fr;ualberta.ca",
        "email": "telecom-paristech.fr;telecom-paristech.fr;telecom-paristech.fr;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Telecom ParisTech;University of Alberta",
        "aff_unique_dep": "LTCI;RLAI Laboratory",
        "aff_unique_url": "https://www.telecom-paris.fr;https://www.ualberta.ca",
        "aff_unique_abbr": "Telecom ParisTech;",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Paris;Edmonton",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "France;Canada"
    },
    {
        "id": "d181ba2345",
        "title": "Penalized Principal Component Regression on Graphs for Analysis of Subnetworks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html",
        "author": "Ali Shojaie; George Michailidis",
        "abstract": "Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efficient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a flexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology.",
        "bibtex": "@inproceedings{NIPS2010_846c260d,\n author = {Shojaie, Ali and Michailidis, George},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Penalized Principal Component Regression on Graphs for Analysis of Subnetworks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/846c260d715e5b854ffad5f70a516c88-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3337987,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3563920720510765710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, University of Michigan; Department of Statistics and EECS, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ann Arbor;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "02db015dbf",
        "title": "Permutation Complexity Bound on Out-Sample Error",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7cbbc409ec990f19c78c75bd1e06f215-Abstract.html",
        "author": "Malik Magdon-Ismail",
        "abstract": "We define a data dependent permutation complexity for a hypothesis set \\math{\\hset}, which is similar to a Rademacher complexity or maximum discrepancy. The permutation complexity is based like the maximum discrepancy on (dependent) sampling. We prove a uniform bound on the generalization error, as well as a concentration result which means that the permutation estimate can be efficiently estimated.",
        "bibtex": "@inproceedings{NIPS2010_7cbbc409,\n author = {Magdon-Ismail, Malik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Permutation Complexity Bound on Out-Sample Error},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7cbbc409ec990f19c78c75bd1e06f215-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 129834,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15661903408240478003&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, Rensselaer Polytechnic Institute",
        "aff_domain": "cs.rpi.edu",
        "email": "cs.rpi.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.rpi.edu",
        "aff_unique_abbr": "RPI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2b3a3210db",
        "title": "Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/b73ce398c39f506af761d2277d853a92-Abstract.html",
        "author": "George Dahl; Marc'aurelio Ranzato; Abdel-rahman Mohamed; Geoffrey E. Hinton",
        "abstract": "Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5\\%, which is superior to all published results on speaker-independent TIMIT to date.",
        "bibtex": "@inproceedings{NIPS2010_b73ce398,\n author = {Dahl, George and Ranzato, Marc\\textquotesingle aurelio and Mohamed, Abdel-rahman and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b73ce398c39f506af761d2277d853a92-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/b73ce398c39f506af761d2277d853a92-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/b73ce398c39f506af761d2277d853a92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 195320,
        "gs_citation": 452,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8615607536818380937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "aa797ee8c8",
        "title": "Phoneme Recognition with Large Hierarchical Reservoirs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html",
        "author": "Fabian Triefenbach; Azarakhsh Jalalvand; Benjamin Schrauwen; Jean-pierre Martens",
        "abstract": "Automatic speech recognition has gradually improved over the years, but the reliable recognition of unconstrained speech is still not within reach. In order to achieve a breakthrough, many research groups are now investigating new methodologies that have potential to outperform the Hidden Markov Model technology that is at the core of all present commercial systems. In this paper, it is shown that the recently introduced concept of Reservoir Computing might form the basis of such a methodology. In a limited amount of time, a reservoir system that can recognize the elementary sounds of continuous speech has been built. The system already achieves a state-of-the-art performance, and there is evidence that the margin for further improvements is still significant.",
        "bibtex": "@inproceedings{NIPS2010_2ca65f58,\n author = {Triefenbach, Fabian and Jalalvand, Azarakhsh and Schrauwen, Benjamin and Martens, Jean-pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Phoneme Recognition with Large Hierarchical Reservoirs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 805053,
        "gs_citation": 233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17934256879012066802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electronics and Information Systems, Ghent University; Department of Electronics and Information Systems, Ghent University; Department of Electronics and Information Systems, Ghent University; Department of Electronics and Information Systems, Ghent University",
        "aff_domain": "elis.ugent.be; ; ; ",
        "email": "elis.ugent.be; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Ghent University",
        "aff_unique_dep": "Department of Electronics and Information Systems",
        "aff_unique_url": "https://www.ugent.be",
        "aff_unique_abbr": "UGent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "4d48c2c238",
        "title": "Policy gradients in linearly-solvable MDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/69421f032498c97020180038fddb8e24-Abstract.html",
        "author": "Emanuel Todorov",
        "abstract": "We present policy gradient results within the framework of linearly-solvable MDPs. For the first time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the first compatible function approximators and natural policy gradients for continuous-time stochastic systems.",
        "bibtex": "@inproceedings{NIPS2010_69421f03,\n author = {Todorov, Emanuel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Policy gradients in linearly-solvable MDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/69421f032498c97020180038fddb8e24-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/69421f032498c97020180038fddb8e24-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/69421f032498c97020180038fddb8e24-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/69421f032498c97020180038fddb8e24-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 339837,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5556582480989000765&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Applied Mathematics and Computer Science & Engineering, University of Washington",
        "aff_domain": "cs.washington.edu",
        "email": "cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Applied Mathematics and Computer Science & Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "255bb150c7",
        "title": "Pose-Sensitive Embedding by Nonlinear NCA Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html",
        "author": "Graham W. Taylor; Rob Fergus; George Williams; Ian Spiro; Christoph Bregler",
        "abstract": "This paper tackles the complex problem of visually matching people in similar pose but with different clothes, background, and other appearance changes. We achieve this with a novel method for learning a nonlinear embedding based on several extensions to the Neighborhood Component Analysis (NCA) framework. Our method is convolutional, enabling it to scale to realistically-sized images. By cheaply labeling the head and hands in large video databases through Amazon Mechanical Turk (a crowd-sourcing service), we can use the task of localizing the head and hands as a proxy for determining body pose. We apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose. We evaluate our method quantitatively against other embedding methods. We also demonstrate that real-world performance can be improved through the use of synthetic data.",
        "bibtex": "@inproceedings{NIPS2010_d56b9fc4,\n author = {Taylor, Graham W and Fergus, Rob and Williams, George and Spiro, Ian and Bregler, Christoph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Pose-Sensitive Embedding by Nonlinear NCA Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1689186,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4182336259302886938&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Courant Institute of Mathematics, New York University; Courant Institute of Mathematics, New York University; Courant Institute of Mathematics, New York University; Courant Institute of Mathematics, New York University; Courant Institute of Mathematics, New York University",
        "aff_domain": "cs.nyu.edu;cs.nyu.edu; ;cs.nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;cs.nyu.edu; ;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Courant Institute of Mathematics",
        "aff_unique_url": "https://www.courant.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c54a5f1a31",
        "title": "Practical Large-Scale Optimization for Max-norm Regularization",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/9fe8593a8a330607d76796b35c64c600-Abstract.html",
        "author": "Jason Lee; Ben Recht; Nathan Srebro; Joel Tropp; Ruslan Salakhutdinov",
        "abstract": "The max-norm was proposed as a convex matrix regularizer by Srebro et al (2004) and was shown to be empirically superior to the trace-norm for collaborative filtering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro (2003) to devise scalable first-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas.",
        "bibtex": "@inproceedings{NIPS2010_9fe8593a,\n author = {Lee, Jason D and Recht, Ben and Srebro, Nathan and Tropp, Joel and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Practical Large-Scale Optimization for Max-norm Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9fe8593a8a330607d76796b35c64c600-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/9fe8593a8a330607d76796b35c64c600-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/9fe8593a8a330607d76796b35c64c600-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 291542,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11159980314303503245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 28,
        "aff": "Institute of Computational and Mathematical Engineering, Stanford University; Department of Computer Sciences, University of Wisconsin-Madison; Brain and Cognitive Sciences and CSAIL, Massachusetts Institute of Technology; Toyota Technological Institute at Chicago; Computing and Mathematical Sciences, California Institute of Technology",
        "aff_domain": "yahoo.com;cs.wisc.edu;mit.edu;ttic.edu;acm.caltech.edu",
        "email": "yahoo.com;cs.wisc.edu;mit.edu;ttic.edu;acm.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "Stanford University;University of Wisconsin-Madison;Massachusetts Institute of Technology;Toyota Technological Institute at Chicago;California Institute of Technology",
        "aff_unique_dep": "Institute of Computational and Mathematical Engineering;Department of Computer Sciences;Brain and Cognitive Sciences and Computer Science and Artificial Intelligence Laboratory;;Computing and Mathematical Sciences",
        "aff_unique_url": "https://www.stanford.edu;https://www.wisc.edu;https://web.mit.edu;https://www.tti-chicago.org;https://www.caltech.edu",
        "aff_unique_abbr": "Stanford;UW-Madison;MIT;TTI Chicago;Caltech",
        "aff_campus_unique_index": "0;1;2;3;4",
        "aff_campus_unique": "Stanford;Madison;Cambridge;Chicago;Pasadena",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "727f247259",
        "title": "Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/995665640dc319973d3173a74a03860c-Abstract.html",
        "author": "Ling Huang; Jinzhu Jia; Bin Yu; Byung-gon Chun; Petros Maniatis; Mayur Naik",
        "abstract": "Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program\u2019s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.",
        "bibtex": "@inproceedings{NIPS2010_99566564,\n author = {Huang, Ling and Jia, Jinzhu and Yu, Bin and Chun, Byung-gon and Maniatis, Petros and Naik, Mayur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/995665640dc319973d3173a74a03860c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/995665640dc319973d3173a74a03860c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/995665640dc319973d3173a74a03860c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/995665640dc319973d3173a74a03860c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 127836,
        "gs_citation": 201,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16460791782926927883&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Intel Labs Berkeley; UC Berkeley; UC Berkeley; Intel Labs Berkeley; Intel Labs Berkeley; Intel Labs Berkeley",
        "aff_domain": "intel.com;stat.berkeley.edu;stat.berkeley.edu;intel.com;intel.com;intel.com",
        "email": "intel.com;stat.berkeley.edu;stat.berkeley.edu;intel.com;intel.com;intel.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;0;0",
        "aff_unique_norm": "Intel;University of California, Berkeley",
        "aff_unique_dep": "Intel Labs;",
        "aff_unique_url": "https://www.intel.com/research/labs;https://www.berkeley.edu",
        "aff_unique_abbr": "Intel Labs;UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6116d5a62d",
        "title": "Predictive State Temporal Difference Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/3473decccb0509fb264818a7512a8b9b-Abstract.html",
        "author": "Byron Boots; Geoffrey J. Gordon",
        "abstract": "We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.",
        "bibtex": "@inproceedings{NIPS2010_3473decc,\n author = {Boots, Byron and Gordon, Geoffrey J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predictive State Temporal Difference Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3473decccb0509fb264818a7512a8b9b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/3473decccb0509fb264818a7512a8b9b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/3473decccb0509fb264818a7512a8b9b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 535446,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7495508267658497175&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dda417a1e2",
        "title": "Predictive Subspace Learning for Multi-view Data: a Large Margin Approach",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8e6b42f1644ecb1327dc03ab345e618b-Abstract.html",
        "author": "Ning Chen; Jun Zhu; Eric P. Xing",
        "abstract": "Learning from multi-view data is important in many applications, such as image classification and annotation. In this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulfills a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efficient inference and parameter estimation methods for the latent subspace model. Finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classification, annotation and retrieval.",
        "bibtex": "@inproceedings{NIPS2010_8e6b42f1,\n author = {Chen, Ning and Zhu, Jun and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predictive Subspace Learning for Multi-view Data: a Large Margin Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1095898,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18197769626023604288&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "47921da1bb",
        "title": "Probabilistic Belief Revision with Structural Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html",
        "author": "Peter Jones; Venkatesh Saligrama; Sanjoy Mitter",
        "abstract": "Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence.",
        "bibtex": "@inproceedings{NIPS2010_1f4477ba,\n author = {Jones, Peter and Saligrama, Venkatesh and Mitter, Sanjoy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Belief Revision with Structural Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/1f4477bad7af3616c1f933a02bfabe4e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 284049,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6420958453722886746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "MIT Lincoln Laboratory; Dept. of ECE, Boston University; Dept. of EECS, MIT",
        "aff_domain": "ll.mit.edu;bu.edu;mit.edu",
        "email": "ll.mit.edu;bu.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Massachusetts Institute of Technology Lincoln Laboratory;Boston University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Lincoln Laboratory;Dept. of Electrical and Computer Engineering;Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.ll.mit.edu;https://www.bu.edu;https://web.mit.edu",
        "aff_unique_abbr": "MIT LL;BU;MIT",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Lexington;;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "72a735e8d6",
        "title": "Probabilistic Deterministic Infinite Automata",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html",
        "author": "David Pfau; Nicholas Bartlett; Frank Wood",
        "abstract": "We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA). We define and develop and sampler for a PDFA with an infinite number of states which we call the probabilistic deterministic infinite automata (PDIA). Posterior predictive inference in this model, given a finite training sequence, can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models.",
        "bibtex": "@inproceedings{NIPS2010_dabd8d2c,\n author = {Pfau, David and Bartlett, Nicholas and Wood, Frank},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Deterministic Infinite Automata},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/dabd8d2ce74e782c65a973ef76fd540b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 930203,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1009328557127238925&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Columbia University, New York, NY 10027, USA; Columbia University, New York, NY 10027, USA; Columbia University, New York, NY 10027, USA",
        "aff_domain": "neurotheory.columbia.edu;stat.columbia.edu;stat.columbia.edu",
        "email": "neurotheory.columbia.edu;stat.columbia.edu;stat.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3fd7a8d642",
        "title": "Probabilistic Inference and Differential Privacy",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html",
        "author": "Oliver Williams; Frank Mcsherry",
        "abstract": "We identify and investigate a strong connection between probabilistic inference and differential privacy, the latter being a recent privacy definition that permits only indirect observation of data through noisy measurement. Previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own. We consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof. We find that probabilistic inference can improve accuracy, integrate multiple observations, measure uncertainty, and even provide posterior distributions over quantities that were not directly measured.",
        "bibtex": "@inproceedings{NIPS2010_fb60d411,\n author = {Williams, Oliver and Mcsherry, Frank},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Inference and Differential Privacy},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1313574,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5504785819475555680&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft Research, Mountain View, CA 94043; Microsoft Research, Mountain View, CA 94043",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e699d104ab",
        "title": "Probabilistic Multi-Task Feature Selection",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/839ab46820b524afda05122893c2fe8e-Abstract.html",
        "author": "Yu Zhang; Dit-Yan Yeung; Qian Xu",
        "abstract": "Recently, some variants of the $l_1$ norm, particularly matrix norms such as the $l_{1,2}$ and $l_{1,\\infty}$ norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the $l_{1,2}$ and $l_{1,\\infty}$ norms by considering a family of $l_{1,q}$ norms for $1 < q\\le\\infty$ and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the $l_{1,q}$ norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization~(EM) algorithms to learn all model parameters, including $q$, automatically. Experiments have been conducted on two cancer classification applications using microarray gene expression data.",
        "bibtex": "@inproceedings{NIPS2010_839ab468,\n author = {Zhang, Yu and Yeung, Dit-Yan and Xu, Qian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Multi-Task Feature Selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/839ab46820b524afda05122893c2fe8e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/839ab46820b524afda05122893c2fe8e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/839ab46820b524afda05122893c2fe8e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 360879,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17381117783549255235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering; Department of Computer Science and Engineering; Bioengineering Program",
        "aff_domain": "cse.ust.hk;cse.ust.hk;ust.hk",
        "email": "cse.ust.hk;cse.ust.hk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, San Diego;Bioengineering Program",
        "aff_unique_dep": "Department of Computer Science and Engineering;Bioengineering",
        "aff_unique_url": "https://cse.ucsd.edu;",
        "aff_unique_abbr": "UCSD CSE;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "1b823e9c12",
        "title": "Probabilistic latent variable models for distinguishing between cause and effect",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html",
        "author": "Oliver Stegle; Dominik Janzing; Kun Zhang; Joris M. Mooij; Bernhard Sch\u00f6lkopf",
        "abstract": "We propose a novel method for inferring whether X causes Y or vice versa from joint observations of X and Y. The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results.",
        "bibtex": "@inproceedings{NIPS2010_c850371f,\n author = {Stegle, Oliver and Janzing, Dominik and Zhang, Kun and Mooij, Joris M and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic latent variable models for distinguishing between cause and effect},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 314515,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6634150696169644891&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "MPI for Biological Cybernetics, T\u00a8ubingen, Germany; MPI for Biological Cybernetics, T\u00a8ubingen, Germany; MPI for Biological Cybernetics, T\u00a8ubingen, Germany; MPI for Biological Cybernetics, T\u00a8ubingen, Germany; MPI for Biological Cybernetics, T\u00a8ubingen, Germany",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Biological Cybernetics",
        "aff_unique_url": "https://www.biological-cybernetics.de",
        "aff_unique_abbr": "MPIBC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "95aaa9f8ad",
        "title": "Random Conic Pursuit for Semidefinite Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7940ab47468396569a906f75ff3f20ef-Abstract.html",
        "author": "Ariel Kleiner; Ali Rahimi; Michael I. Jordan",
        "abstract": "We present a novel algorithm, Random Conic Pursuit, that solves semidefinite programs (SDPs) via repeated optimization over randomly selected two-dimensional subcones of the PSD cone. This scheme is simple, easily implemented, applicable to very general SDPs, scalable, and theoretically interesting. Its advantages are realized at the expense of an ability to readily compute highly exact solutions, though useful approximate solutions are easily obtained. This property renders Random Conic Pursuit of particular interest for machine learning applications, in which the relevant SDPs are generally based upon random data and so exact minima are often not a priority. Indeed, we present empirical results to this effect for various SDPs encountered in machine learning; these experiments demonstrate the potential practical usefulness of Random Conic Pursuit. We also provide a preliminary analysis that yields insight into the theoretical properties and convergence of the algorithm.",
        "bibtex": "@inproceedings{NIPS2010_7940ab47,\n author = {Kleiner, Ariel and Rahimi, Ali and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Conic Pursuit for Semidefinite Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 372689,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15854904924361306253&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Division, University of California, Berkeley, CA 94720; Intel Research Berkeley, Berkeley, CA 94720; Computer Science Division, University of California, Berkeley, CA 94720",
        "aff_domain": "cs.berkeley.edu;intel.com;cs.berkeley.edu",
        "email": "cs.berkeley.edu;intel.com;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Intel",
        "aff_unique_dep": "Computer Science Division;Intel Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.intel.com",
        "aff_unique_abbr": "UC Berkeley;Intel",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "543246ae6d",
        "title": "Random Projection Trees Revisited",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/3def184ad8f4755ff269862ea77393dd-Abstract.html",
        "author": "Aman Dhesi; Purushottam Kar",
        "abstract": "The Random Projection Tree (RPTree) structures proposed in [Dasgupta-Freund-STOC-08] are space partitioning data structures that automatically adapt to various notions of intrinsic dimensionality of data. We prove new results for both the RPTree-Max and the RPTree-Mean data structures. Our result for RPTree-Max gives a near-optimal bound on the number of levels required by this data structure to reduce the size of its cells by a factor s >= 2. We also prove a packing lemma for this data structure. Our final result shows that low-dimensional manifolds possess bounded Local Covariance Dimension. As a consequence we show that RPTree-Mean adapts to manifold dimension as well.",
        "bibtex": "@inproceedings{NIPS2010_3def184a,\n author = {Dhesi, Aman and Kar, Purushottam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Projection Trees Revisited},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120098,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10103415343348362318&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Princeton University, Princeton, New Jersey, USA; Department of Computer Science and Engineering, Indian Institute of Technology, Kanpur, Uttar Pradesh, INDIA",
        "aff_domain": "princeton.edu;cse.iitk.ac.in",
        "email": "princeton.edu;cse.iitk.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Princeton University;Indian Institute of Technology Kanpur",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.princeton.edu;https://www.iitk.ac.in",
        "aff_unique_abbr": "Princeton;IIT Kanpur",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Princeton;Kanpur",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "d71d2a9b36",
        "title": "Random Projections for $k$-means Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/73278a4a86960eeb576a8fd4c9ec6997-Abstract.html",
        "author": "Christos Boutsidis; Anastasios Zouzias; Petros Drineas",
        "abstract": "This paper discusses the topic of dimensionality reduction for $k$-means clustering. We prove that any set of $n$ points in $d$ dimensions (rows in a matrix $A \\in \\RR^{n \\times d}$) can be projected into $t = \\Omega(k / \\eps^2)$ dimensions, for any $\\eps \\in (0,1/3)$, in $O(n d \\lceil \\eps^{-2} k/ \\log(d) \\rceil )$ time, such that with  constant probability the optimal $k$-partition of the point set is preserved within a factor of $2+\\eps$. The projection is done by post-multiplying $A$ with a $d \\times t$ random matrix $R$ having entries $+1/\\sqrt{t}$ or $-1/\\sqrt{t}$ with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.",
        "bibtex": "@inproceedings{NIPS2010_73278a4a,\n author = {Boutsidis, Christos and Zouzias, Anastasios and Drineas, Petros},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Projections for k-means Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/73278a4a86960eeb576a8fd4c9ec6997-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 159347,
        "gs_citation": 222,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9841819099302225418&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, RPI; Department of Computer Science, University of Toronto; Department of Computer Science, RPI",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;University of Toronto",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.rpi.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "RPI;U of T",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "e83e27e174",
        "title": "Random Walk Approach to Regret Minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/aeb3135b436aa55373822c010763dd54-Abstract.html",
        "author": "Hariharan Narayanan; Alexander Rakhlin",
        "abstract": "We propose a computationally efficient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efficient method for implementing mixture forecasting strategies.",
        "bibtex": "@inproceedings{NIPS2010_aeb3135b,\n author = {Narayanan, Hariharan and Rakhlin, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Walk Approach to Regret Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aeb3135b436aa55373822c010763dd54-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/aeb3135b436aa55373822c010763dd54-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/aeb3135b436aa55373822c010763dd54-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 409160,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7365468925116836081&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "M]T; University of Pennsylvania",
        "aff_domain": "mit.edu;wharton.upenn.edu",
        "email": "mit.edu;wharton.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "1",
        "aff_unique_norm": ";University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.upenn.edu",
        "aff_unique_abbr": ";UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "6562658b31",
        "title": "Rates of convergence for the cluster tree",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/b534ba68236ba543ae44b22bd110a1d6-Abstract.html",
        "author": "Kamalika Chaudhuri; Sanjoy Dasgupta",
        "abstract": "For a density f on R^d, a high-density cluster is any connected component of {x: f(x) >= c}, for some c > 0. The set of all high-density clusters form a hierarchy called the cluster tree of f. We present a procedure for estimating the cluster tree given samples from f. We give finite-sample convergence rates for our algorithm, as well as lower bounds on the sample complexity of this estimation problem.",
        "bibtex": "@inproceedings{NIPS2010_b534ba68,\n author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rates of convergence for the cluster tree},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126463,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13920972282273438710&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "UCSan Diego; UCSan Diego",
        "aff_domain": "ucsd.edu;cs.ucsd.edu",
        "email": "ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "faae4e46fd",
        "title": "Regularized estimation of image statistics by Score Matching",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html",
        "author": "Diederik P. Kingma; Yann L. Cun",
        "abstract": "Score Matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable. It has been applied to learning natural image statistics but has so-far been limited to simple models due to the difficulty of differentiating the loss with respect to the model parameters. We show how this differentiation can be automated with an extended version of the double-backpropagation algorithm. In addition, we introduce a regularization term for the Score Matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with finite training sample sizes and quantized input values. Results are reported for image denoising and super-resolution.",
        "bibtex": "@inproceedings{NIPS2010_6f3e29a3,\n author = {Kingma, Durk P and Cun, Yann},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regularized estimation of image statistics by Score Matching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272936,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2525889271107101541&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Information and Computing Sciences, Universiteit Utrecht; Courant Institute of Mathematical Sciences, New York University",
        "aff_domain": "students.uu.nl;cs.nyu.edu",
        "email": "students.uu.nl;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universiteit Utrecht;New York University",
        "aff_unique_dep": "Department of Information and Computing Sciences;Courant Institute of Mathematical Sciences",
        "aff_unique_url": "https://www.uu.nl;https://www.courant.nyu.edu",
        "aff_unique_abbr": "UU;NYU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Netherlands;United States"
    },
    {
        "id": "df15152ea0",
        "title": "Relaxed Clipping: A Global Training Method for Robust Regression and Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/01882513d5fa7c329e940dda99b12147-Abstract.html",
        "author": "Min Yang; Linli Xu; Martha White; Dale Schuurmans; Yao-liang Yu",
        "abstract": "Robust regression and classification are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of ``loss clipping'' can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classification problems.",
        "bibtex": "@inproceedings{NIPS2010_01882513,\n author = {Yang, Min and Xu, Linli and White, Martha and Schuurmans, Dale and Yu, Yao-liang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Relaxed Clipping: A Global Training Method for Robust Regression and Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01882513d5fa7c329e940dda99b12147-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/01882513d5fa7c329e940dda99b12147-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/01882513d5fa7c329e940dda99b12147-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/01882513d5fa7c329e940dda99b12147-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 152960,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3187534203174856325&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "University of Alberta, Dept. Computing Science, Edmonton A BT6G 2E8, Canada; University of Alberta, Dept. Computing Science, Edmonton A BT6G 2E8, Canada; University of Alberta, Dept. Computing Science, Edmonton A BT6G 2E8, Canada; University of Alberta, Dept. Computing Science, Edmonton A BT6G 2E8, Canada; University of Alberta, Dept. Computing Science, Edmonton A BT6G 2E8, Canada",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Dept. Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "994814c5f8",
        "title": "Repeated Games against Budgeted Adversaries",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0a0a0c8aaa00ade50f74a3f0ca981ed7-Abstract.html",
        "author": "Jacob D. Abernethy; Manfred K. Warmuth",
        "abstract": "We study repeated zero-sum games against an adversary on a budget. Given that an adversary has some constraint on the sequence of actions that he plays, we consider what ought to be the player's best mixed strategy with knowledge of this budget. We show that, for a general class of normal-form games, the minimax strategy is indeed efficiently computable and relies on a random playout\" technique. We give three diverse applications of this algorithmic template: a cost-sensitive \"Hedge\" setting, a particular problem in Metrical Task Systems, and the design of combinatorial prediction markets.\"",
        "bibtex": "@inproceedings{NIPS2010_0a0a0c8a,\n author = {Abernethy, Jacob D and Warmuth, Manfred K. K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Repeated Games against Budgeted Adversaries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 185959,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9666753864437072088&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Division of Computer Science, UC Berkeley; Department of Computer Science, UC Santa Cruz",
        "aff_domain": "cs.berkeley.edu;cse.ucsc.edu",
        "email": "cs.berkeley.edu;cse.ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;University of California, Santa Cruz",
        "aff_unique_dep": "Division of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ucsc.edu",
        "aff_unique_abbr": "UC Berkeley;UCSC",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Berkeley;Santa Cruz",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b4413a6b0d",
        "title": "Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/f2201f5191c4e92cc5af043eebfd0946-Abstract.html",
        "author": "Felipe Gerhard; Wulfram Gerstner",
        "abstract": "Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-fit using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural firing rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-fit tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models.",
        "bibtex": "@inproceedings{NIPS2010_f2201f51,\n author = {Gerhard, Felipe and Gerstner, Wulfram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/f2201f5191c4e92cc5af043eebfd0946-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 385412,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15418873228183054182&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Brain Mind Institute, Ecole Polytechnique F\u00e9d\u00e9r\u00e1le de Lausanne, 1015 Lausanne EPFL, Switzerland; Brain Mind Institute, Ecole Polytechnique F\u00e9d\u00e9r\u00e1le de Lausanne, 1015 Lausanne EPFL, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ecole Polytechnique F\u00e9d\u00e9r\u00e1le de Lausanne",
        "aff_unique_dep": "Brain Mind Institute",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lausanne",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "06d9c7366f",
        "title": "Reverse Multi-Label Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/be3159ad04564bfb90db9e32851ebf9c-Abstract.html",
        "author": "James Petterson; Tib\u00e9rio S. Caetano",
        "abstract": "Multi-label classification is the task of predicting potentially multiple labels for a given instance. This is common in several applications such as image annotation, document classification and gene function prediction. In this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels. By viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classification admit relaxations that can be efficiently optimised. We optimise these relaxations with standard algorithms and compare our results with several state-of-the-art methods, showing excellent performance.",
        "bibtex": "@inproceedings{NIPS2010_be3159ad,\n author = {Petterson, James and Caetano, Tib\\'{e}rio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reverse Multi-Label Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 418882,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10493250297036033062&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Australian National University; Australian National University",
        "aff_domain": "nicta.com.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "034a85433e",
        "title": "Reward Design via Online Gradient Ascent",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/168908dd3227b8358eababa07fcaf091-Abstract.html",
        "author": "Jonathan Sorg; Richard L. Lewis; Satinder P. Singh",
        "abstract": "Recent work has demonstrated that when artificial agents are limited in their ability to achieve their goals, the agent designer can benefit by making the agent's goals different from the designer's. This gives rise to the optimization problem of designing the artificial agent's goals---in the RL framework, designing the agent's reward function. Existing attempts at solving this optimal reward problem do not leverage experience gained online during the agent's lifetime nor do they take advantage of knowledge about the agent's structure. In this work, we develop a gradient ascent approach with formal convergence guarantees for approximately solving the optimal reward problem online during an agent's lifetime. We show that our method generalizes a standard policy gradient approach, and we demonstrate its ability to improve reward functions in agents with various forms of limitations.",
        "bibtex": "@inproceedings{NIPS2010_168908dd,\n author = {Sorg, Jonathan and Lewis, Richard L and Singh, Satinder},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reward Design via Online Gradient Ascent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/168908dd3227b8358eababa07fcaf091-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/168908dd3227b8358eababa07fcaf091-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/168908dd3227b8358eababa07fcaf091-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1384855,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14978961918718626813&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science and Eng., University of Michigan; Computer Science and Eng., University of Michigan; Department of Psychology, University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1cdd6ab07b",
        "title": "Robust Clustering as Ensembles of Affinity Relations",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/b2eb7349035754953b57a32e2841bda5-Abstract.html",
        "author": "Hairong Liu; Longin J. Latecki; Shuicheng Yan",
        "abstract": "In this paper, we regard clustering as ensembles of k-ary affinity relations and clusters correspond to subsets of objects with maximal average affinity relations. The average affinity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efficient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a unified solution to clustering from k-ary affinity relations with k \u2265 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.",
        "bibtex": "@inproceedings{NIPS2010_b2eb7349,\n author = {Liu, Hairong and Latecki, Longin and Yan, Shuicheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Clustering as Ensembles of Affinity Relations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2eb7349035754953b57a32e2841bda5-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/b2eb7349035754953b57a32e2841bda5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/b2eb7349035754953b57a32e2841bda5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 248544,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7384696314256255399&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; Department of Electrical and Computer Engineering, National University of Singapore, Singapore",
        "aff_domain": "gmail.com;temple.edu;nus.edu.sg",
        "email": "gmail.com;temple.edu;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "National University of Singapore;Temple University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Computer and Information Sciences",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.temple.edu",
        "aff_unique_abbr": "NUS;Temple",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Philadelphia",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "fde97388ec",
        "title": "Robust PCA via Outlier Pursuit",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/fe8c15fed5f808006ce95eddb7366e35-Abstract.html",
        "author": "Huan Xu; Constantine Caramanis; Sujay Sanghavi",
        "abstract": "Singular Value Decomposition (and Principal Component Analysis)  is one of the most widely used techniques for dimensionality reduction: successful and efficiently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative filtering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted.  We present an efficient convex optimization-based  algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems) recovers the",
        "bibtex": "@inproceedings{NIPS2010_fe8c15fe,\n author = {Xu, Huan and Caramanis, Constantine and Sanghavi, Sujay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust PCA via Outlier Pursuit},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/fe8c15fed5f808006ce95eddb7366e35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 184863,
        "gs_citation": 913,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13625166629528227466&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "ElectricalandComputerEngineering,UniversityofTexasat Austin; ElectricalandComputerEngineering,UniversityofTexasat Austin; ElectricalandComputerEngineering,UniversityofTexasat Austin",
        "aff_domain": "mail.utexas.edu;ece.utexas.edu;mail.utexas.edu",
        "email": "mail.utexas.edu;ece.utexas.edu;mail.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a7e66484b4",
        "title": "Sample Complexity of Testing the Manifold Hypothesis",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html",
        "author": "Hariharan Narayanan; Sanjoy Mitter",
        "abstract": "The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error.   Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is {\\it independent} of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume and curvature is unavoidable. Whether the known lower bound of $O(\\frac{k}{\\eps^2} + \\frac{\\log \\frac{1}{\\de}}{\\eps^2})$ for the sample complexity of Empirical Risk minimization on $k-$means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 \\cite{bart2}. Here $\\eps$ is the desired bound on the error and $\\de$ is a bound on the probability of failure. We improve the best currently known upper bound \\cite{pontil} of $O(\\frac{k^2}{\\eps^2} + \\frac{\\log \\frac{1}{\\de}}{\\eps^2})$ to $O\\left(\\frac{k}{\\eps^2}\\left(\\min\\left(k, \\frac{\\log^4 \\frac{k}{\\eps}}{\\eps^2}\\right)\\right) + \\frac{\\log \\frac{1}{\\de}}{\\eps^2}\\right)$. Based on these results, we devise a simple algorithm for $k-$means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to  high dimensional data, where the sample complexity is independent of the ambient dimension.",
        "bibtex": "@inproceedings{NIPS2010_8a1e808b,\n author = {Narayanan, Hariharan and Mitter, Sanjoy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sample Complexity of Testing the Manifold Hypothesis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8a1e808b55fde9455cb3d8857ed88389-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 334630,
        "gs_citation": 289,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9424043040251892170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Laboratory for Information and Decision Systems, EECS, MIT; Laboratory for Information and Decision Systems, EECS, MIT",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c53ab9c3d3",
        "title": "Scrambled Objects for Least-Squares Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/632cee946db83e7a52ce5e8d6f0fed35-Abstract.html",
        "author": "Odalric Maillard; R\u00e9mi Munos",
        "abstract": "We consider least-squares regression using a randomly generated subspace G",
        "bibtex": "@inproceedings{NIPS2010_632cee94,\n author = {Maillard, Odalric and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scrambled Objects for Least-Squares Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/632cee946db83e7a52ce5e8d6f0fed35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 354505,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11591482569108489900&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "SequeL Project, INRIA Lille - Nord Europe, France; SequeL Project, INRIA Lille - Nord Europe, France",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA Lille - Nord Europe",
        "aff_unique_dep": "SequeL Project",
        "aff_unique_url": "https://www.inria.fr/en/centre/lille-nord-europe",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2d5b5aa69c",
        "title": "Segmentation as Maximum-Weight Independent Set",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a0a080f42e6f13b3a2df133f073095dd-Abstract.html",
        "author": "William Brendel; Sinisa Todorovic",
        "abstract": "Given an ensemble of distinct, low-level segmentations of an image, our goal is to identify visually meaningful\" segments in the ensemble. Knowledge about any specific objects and surfaces present in the image is not available. The selection of image regions occupied by objects is formalized as the maximum-weight independent set (MWIS) problem. MWIS is the heaviest subset of mutually non-adjacent nodes of an attributed graph. We construct such a graph from all segments in the ensemble. Then, MWIS selects maximally distinctive segments that together partition the image. A new MWIS algorithm is presented. The algorithm seeks a solution directly in the discrete domain, instead of relaxing MWIS to a continuous problem, as common in previous work. It iteratively finds a candidate discrete solution of the Taylor series expansion of the original MWIS objective function around the previous solution. The algorithm is shown to converge to a maximum. Our empirical evaluation on the benchmark Berkeley segmentation dataset shows that the new algorithm eliminates the need for hand-picking optimal input parameters of the state-of-the-art segmenters, and outperforms their best, manually optimized results.\"",
        "bibtex": "@inproceedings{NIPS2010_a0a080f4,\n author = {Brendel, William and Todorovic, Sinisa},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Segmentation as Maximum-Weight Independent Set},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a0a080f42e6f13b3a2df133f073095dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 665559,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7587694919626881271&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5a0166f6ff",
        "title": "Self-Paced Learning for Latent Variable Models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e57c6b956a6521b28495f2886ca0977a-Abstract.html",
        "author": "M. P. Kumar; Benjamin Packer; Daphne Koller",
        "abstract": "Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by  proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.",
        "bibtex": "@inproceedings{NIPS2010_e57c6b95,\n author = {Kumar, M. and Packer, Benjamin and Koller, Daphne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Self-Paced Learning for Latent Variable Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 624737,
        "gs_citation": 1798,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10046450202929763892&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "35d2a7235a",
        "title": "Semi-Supervised Learning with Adversarially Missing Label Information",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/556f391937dfd4398cbac35e050a2177-Abstract.html",
        "author": "Umar Syed; Ben Taskar",
        "abstract": "We address the problem of semi-supervised learning in an adversarial setting. Instead of assuming that labels are missing at random, we analyze a less favorable scenario where the label information can be missing partially and arbitrarily, which is motivated by several practical examples. We present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information. Motivated by the analysis, we formulate a convex optimization problem for parameter estimation, derive an efficient algorithm, and analyze its convergence. We provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information, outperforming several strong baselines.",
        "bibtex": "@inproceedings{NIPS2010_556f3919,\n author = {Syed, Umar and Taskar, Ben},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-Supervised Learning with Adversarially Missing Label Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/556f391937dfd4398cbac35e050a2177-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/556f391937dfd4398cbac35e050a2177-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/556f391937dfd4398cbac35e050a2177-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/556f391937dfd4398cbac35e050a2177-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 134745,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5333418565062621066&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer and Information Science, University of Pennsylvania; Department of Computer and Information Science, University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d00a59beb9",
        "title": "Shadow Dirichlet for Restricted Probability Modeling",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/acf4b89d3d503d8252c9c4ba75ddbf6d-Abstract.html",
        "author": "Bela Frigyik; Maya Gupta; Yihua Chen",
        "abstract": "Although the Dirichlet distribution is widely used, the independence structure of its components limits its accuracy as a model. The proposed shadow Dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems, such as regularized pmfs, monotonic pmfs, and pmfs with bounded variation. We describe some properties of this new class of distributions, provide maximum entropy constructions, give an expectation-maximization method for estimating the mean parameter, and illustrate with real data.",
        "bibtex": "@inproceedings{NIPS2010_acf4b89d,\n author = {Frigyik, Bela and Gupta, Maya and Chen, Yihua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Shadow Dirichlet for Restricted Probability Modeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 421303,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10967684300397489177&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical Engineering, University of Washington; Department of Electrical Engineering, University of Washington; Department of Electrical Engineering, University of Washington",
        "aff_domain": "gmail.com;ee.washington.edu;gmail.com",
        "email": "gmail.com;ee.washington.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0759055820",
        "title": "Short-term memory in neuronal networks through dynamical compressed sensing",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0f2c9a93eea6f38fabb3acb1c31488c6-Abstract.html",
        "author": "Surya Ganguli; Haim Sompolinsky",
        "abstract": "Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that {\\it exceeds} the number of neurons. This enhanced capacity is achieved by a class of ``orthogonal recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance.\"",
        "bibtex": "@inproceedings{NIPS2010_0f2c9a93,\n author = {Ganguli, Surya and Sompolinsky, Haim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Short-term memory in neuronal networks through dynamical compressed sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 545034,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16223204575223481667&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Sloan-Swartz Center for Theoretical Neurobiology, UCSF, San Francisco, CA 94143; Interdisciplinary Center for Neural Computation, Hebrew University, Jerusalem 91904, Israel+Center for Brain Science, Harvard University, Cambridge, Massachusetts 02138, USA",
        "aff_domain": "phy.ucsf.edu;fiz.huji.ac.il",
        "email": "phy.ucsf.edu;fiz.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "University of California, San Francisco;Hebrew University;Harvard University",
        "aff_unique_dep": "Sloan-Swartz Center for Theoretical Neurobiology;Interdisciplinary Center for Neural Computation;Center for Brain Science",
        "aff_unique_url": "https://www.ucsf.edu;http://www.huji.ac.il;https://www.harvard.edu",
        "aff_unique_abbr": "UCSF;HUJI;Harvard",
        "aff_campus_unique_index": "0;1+2",
        "aff_campus_unique": "San Francisco;Jerusalem;Cambridge",
        "aff_country_unique_index": "0;1+0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "063fa91c6c",
        "title": "Sidestepping Intractable Inference with Structured Ensemble Cascades",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/93fb9d4b16aa750c7475b6d601c35c2c-Abstract.html",
        "author": "David Weiss; Benjamin Sapp; Ben Taskar",
        "abstract": "For many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees. In this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade. We focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks. Unlike other variational methods, our ensembles do not enforce agreement between sub-models, but filter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model. Our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model. We provide a generalization bound on the filtering loss of the ensemble as a theoretical justification of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos. We find that our approach significantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem.",
        "bibtex": "@inproceedings{NIPS2010_93fb9d4b,\n author = {Weiss, David and Sapp, Benjamin and Taskar, Ben},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sidestepping Intractable Inference with Structured Ensemble Cascades},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3957539,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=933670168271947776&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b205e0c86e",
        "title": "Simultaneous Object Detection and Ranking with Weak Supervision",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html",
        "author": "Matthew Blaschko; Andrea Vedaldi; Andrew Zisserman",
        "abstract": "A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs and the PASCAL VOC dataset, and it is shown that for a significant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results.",
        "bibtex": "@inproceedings{NIPS2010_6da37dd3,\n author = {Blaschko, Matthew and Vedaldi, Andrea and Zisserman, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simultaneous Object Detection and Ranking with Weak Supervision},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 200711,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8390982525319014160&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c83131fcba",
        "title": "Size Matters: Metric Visual Search Constraints from Monocular Metadata",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/1d72310edc006dadf2190caad5802983-Abstract.html",
        "author": "Mario Fritz; Kate Saenko; Trevor Darrell",
        "abstract": "Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information\u2013object and feature absolute size\u2013can be added to models learned from commonly available online imagery, without use of any 3-D sensing or re- construction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a \u201c2.1D\u201d local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata fields specifying camera intrinstics. We develop an efficient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata.",
        "bibtex": "@inproceedings{NIPS2010_1d72310e,\n author = {Fritz, Mario and Saenko, Kate and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Size Matters: Metric Visual Search Constraints from Monocular Metadata},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1d72310edc006dadf2190caad5802983-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/1d72310edc006dadf2190caad5802983-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/1d72310edc006dadf2190caad5802983-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3905013,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5401796102608728371&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "eecbcf7996",
        "title": "Slice sampling covariance hyperparameters of latent Gaussian models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4d5b995358e7798bc7e9d9db83c612a5-Abstract.html",
        "author": "Iain Murray; Ryan P. Adams",
        "abstract": "The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.",
        "bibtex": "@inproceedings{NIPS2010_4d5b9953,\n author = {Murray, Iain and Adams, Ryan P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Slice sampling covariance hyperparameters of latent Gaussian models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 345819,
        "gs_citation": 288,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7007458693428865270&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "School of Informatics, University of Edinburgh; Dept. Computer Science, University of Toronto",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Edinburgh;University of Toronto",
        "aff_unique_dep": "School of Informatics;Department of Computer Science",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.utoronto.ca",
        "aff_unique_abbr": "Edinburgh;U of T",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Edinburgh;Toronto",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Canada"
    },
    {
        "id": "f50106ce6c",
        "title": "Smoothness, Low Noise and Fast Rates",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/76cf99d3614e23eabab16fb27e944bf9-Abstract.html",
        "author": "Nathan Srebro; Karthik Sridharan; Ambuj Tewari",
        "abstract": "We establish an excess risk bound of O(H R",
        "bibtex": "@inproceedings{NIPS2010_76cf99d3,\n author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Smoothness, Low Noise and Fast Rates},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 341133,
        "gs_citation": 334,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12888672753658569008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Computer Science Dept., University of Texas at Austin",
        "aff_domain": "ttic.edu;ttic.edu;cs.utexas.edu",
        "email": "ttic.edu;ttic.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Texas at Austin",
        "aff_unique_dep": ";Computer Science Dept.",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.utexas.edu",
        "aff_unique_abbr": "TTI Chicago;UT Austin",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Chicago;Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "13149c2ce4",
        "title": "Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html",
        "author": "Anand Singh; Renaud Jolivet; Pierre Magistretti; Bruno Weber",
        "abstract": "Sodium entry during an action potential determines the energy efficiency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefficient in that regard with about 4 times more charges flowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efficiency and that the dynamics of their voltage-gated channels is significantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of Hodgkin-Huxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models.",
        "bibtex": "@inproceedings{NIPS2010_8065d07d,\n author = {Singh, Anand and Jolivet, Renaud and Magistretti, Pierre and Weber, Bruno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8065d07da4a77621450aa84fee5656d9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 894871,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9548280178590906839&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Institute of Pharmacology and Toxicology, University of Z\u00fcrich, Z\u00fcrich, Switzerland; Institute of Pharmacology and Toxicology, University of Z\u00fcrich, Z\u00fcrich, Switzerland; Brain Mind Institute, EPFL, Lausanne, Switzerland+Center for Psychiatric Neuroscience, University of Lausanne, Lausanne, Switzerland; Institute of Pharmacology and Toxicology, University of Z\u00fcrich, Z\u00fcrich, Switzerland",
        "aff_domain": "pharma.uzh.ch;a3.epfl.ch;epfl.ch;pharma.uzh.ch",
        "email": "pharma.uzh.ch;a3.epfl.ch;epfl.ch;pharma.uzh.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2;0",
        "aff_unique_norm": "University of Z\u00fcrich;Ecole Polytechnique Federale de Lausanne (EPFL);University of Lausanne",
        "aff_unique_dep": "Institute of Pharmacology and Toxicology;Brain Mind Institute;Center for Psychiatric Neuroscience",
        "aff_unique_url": "https://www.uzh.ch;https://www.epfl.ch;https://www.unil.ch",
        "aff_unique_abbr": "UZH;EPFL;",
        "aff_campus_unique_index": "0;0;1+1;0",
        "aff_campus_unique": "Z\u00fcrich;Lausanne",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "72e27bc75f",
        "title": "Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7f5d04d189dfb634e6a85bb9d9adf21e-Abstract.html",
        "author": "Stefan Harmeling; Hirsch Michael; Bernhard Sch\u00f6lkopf",
        "abstract": "Modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake, but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera. In order to overcome such limitations we contribute threefold: (i) we introduce a taxonomy of camera shakes, (ii) we show how to combine a recently introduced framework for space-variant filtering based on overlap-add from Hirsch et al.~and a fast algorithm for single image blind deconvolution for space-invariant filters from Cho and Lee to introduce a method for blind deconvolution for space-variant blur. And (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time record the space-variant point spread function corresponding to that blur. Finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake.",
        "bibtex": "@inproceedings{NIPS2010_7f5d04d1,\n author = {Harmeling, Stefan and Michael, Hirsch and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5985185,
        "gs_citation": 170,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14122073423788348878&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany; Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany; Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "MPIBC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "845e6a6a3c",
        "title": "Sparse Coding for Learning Interpretable Spatio-Temporal Primitives",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/bbf94b34eb32268ada57a3be5062fe7d-Abstract.html",
        "author": "Taehwan Kim; Gregory Shakhnarovich; Raquel Urtasun",
        "abstract": "Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend sparse coding to learn interpretable spatio-temporal primitives of human motion.  We cast the problem of learning spatio-temporal primitives as a tensor factorization problem  and introduce constraints to learn interpretable primitives. In particular, we use group norms over those tensors, diagonal constraints on the activations as well as smoothness constraints that are inherent to human motion.   We demonstrate the effectiveness of our approach to learn interpretable representations  of human motion from motion capture data, and show that our approach outperforms  recently developed matching pursuit and  sparse coding algorithms.",
        "bibtex": "@inproceedings{NIPS2010_bbf94b34,\n author = {Kim, Taehwan and Shakhnarovich, Gregory and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Coding for Learning Interpretable Spatio-Temporal Primitives},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 953840,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11087510072660417&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "TTI Chicago; TTI Chicago; TTI Chicago",
        "aff_domain": "ttic.edu;ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "631547b24f",
        "title": "Sparse Instrumental Variables (SPIV) for Genome-Wide Studies",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4476b929e30dd0c4e8bdbcc82c6ba23a-Abstract.html",
        "author": "Paul Mckeigue; Jon Krohn; Amos J. Storkey; Felix V. Agakov",
        "abstract": "This paper describes a probabilistic framework for studying associations between multiple genotypes, biomarkers, and phenotypic traits in the presence of noise and unobserved confounders for large genetic studies. The framework builds on sparse linear methods developed for regression and modified here for inferring causal structures of richer networks with latent variables. The method is motivated by the use of genotypes as ``instruments'' to infer causal associations between phenotypic biomarkers and outcomes, without making the common restrictive assumptions of instrumental variable methods. The method may be used for an effective screening of potentially interesting genotype phenotype and biomarker-phenotype associations in genome-wide studies, which may have important implications for validating biomarkers as possible proxy endpoints for early stage clinical trials. Where the biomarkers are gene transcripts, the method can be used for fine mapping of quantitative trait loci (QTLs) detected in genetic linkage studies. The method is applied for examining effects of gene transcript levels in the liver on plasma HDL cholesterol levels for a sample of sequenced mice from a heterogeneous stock, with $\\sim 10^5$ genetic instruments and $\\sim 47 \\times 10^3$ gene transcripts.",
        "bibtex": "@inproceedings{NIPS2010_4476b929,\n author = {Mckeigue, Paul and Krohn, Jon and Storkey, Amos J and Agakov, Felix},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Instrumental Variables (SPIV) for Genome-Wide Studies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 205168,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7145653064441933435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Public Health Sciences, University of Edinburgh; Public Health Sciences, University of Edinburgh; WTCHG, Oxford; School of Informatics, University of Edinburgh",
        "aff_domain": "aivalley.com;ed.ac.uk;magd.ox.ac.uk;ed.ac.uk",
        "email": "aivalley.com;ed.ac.uk;magd.ox.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Edinburgh;University of Oxford",
        "aff_unique_dep": "Public Health Sciences;Weatherall Institute of Molecular Medicine",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.ox.ac.uk",
        "aff_unique_abbr": "Edinburgh;Oxford",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Oxford;Edinburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2b5499a21e",
        "title": "Sparse Inverse Covariance Selection via Alternating Linearization Methods",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2723d092b63885e0d7c260cc007e8b9d-Abstract.html",
        "author": "Katya Scheinberg; Shiqian Ma; Donald Goldfarb",
        "abstract": "Gaussian graphical models are of great interest in statistical learning.  Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an $\\ell_1$-regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem's special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an $\\epsilon$-optimal solution in $O(1/\\epsilon)$ iterations. Numerical experiments on both synthetic  and real data from gene association networks show that a practical version of  this algorithm outperforms other competitive algorithms.",
        "bibtex": "@inproceedings{NIPS2010_2723d092,\n author = {Scheinberg, Katya and Ma, Shiqian and Goldfarb, Donald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Inverse Covariance Selection via Alternating Linearization Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 118682,
        "gs_citation": 257,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10123553736251557216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of ISE, Lehigh University; Department of IEOR, Columbia University; Department of IEOR, Columbia University",
        "aff_domain": "lehigh.edu;columbia.edu;columbia.edu",
        "email": "lehigh.edu;columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Lehigh University;Columbia University",
        "aff_unique_dep": "Department of Industrial and Systems Engineering;Department of Industrial Engineering and Operations Research",
        "aff_unique_url": "https://www.lehigh.edu;https://www.columbia.edu",
        "aff_unique_abbr": "Lehigh;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "15817582ac",
        "title": "Spatial and anatomical regularization of SVM for brain image analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html",
        "author": "Remi Cuingnet; Marie Chupin; Habib Benali; Olivier Colliot",
        "abstract": "Support vector machines (SVM) are increasingly used in brain image analyses since they allow capturing complex multivariate relationships in the data. Moreover, when the kernel is linear, SVMs can be used to localize spatial patterns of discrimination between two groups of subjects. However, the features' spatial distribution is not taken into account. As a consequence, the optimal margin hyperplane is often scattered and lacks spatial coherence, making its anatomical interpretation difficult. This paper introduces a framework to spatially regularize SVM for brain image analysis. We show that Laplacian regularization provides a flexible framework to integrate various types of constraints and can be applied to both cortical surfaces and 3D brain images. The proposed framework is applied to the classification of MR images based on gray matter concentration maps and cortical thickness measures from 30 patients with Alzheimer's disease and 30 elderly controls. The results demonstrate that the proposed method enables natural spatial and anatomical regularization of the classifier.",
        "bibtex": "@inproceedings{NIPS2010_eecca5b6,\n author = {Cuingnet, Remi and Chupin, Marie and Benali, Habib and Colliot, Olivier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spatial and anatomical regularization of SVM for brain image analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/eecca5b6365d9607ee5a9d336962c534-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 746250,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3212265331529584886&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 27,
        "aff": "CRICM (UPMC/Inserm/CNRS), Paris, France+Inserm - LIF (UMR S 678), Paris, France; CRICM, Paris, France; Inserm - LIF, Paris, France; CRICM, Paris, France",
        "aff_domain": "imed.jussieu.fr;upmc.fr;imed.jussieu.fr;upmc.fr",
        "email": "imed.jussieu.fr;upmc.fr;imed.jussieu.fr;upmc.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1;0",
        "aff_unique_norm": "CRICM;Institut National de la Sant\u00e9 et de la Recherche M\u00e9dicale",
        "aff_unique_dep": ";Laboratoire d'Ing\u00e9nierie des Fonctions Motrices",
        "aff_unique_url": ";https://www.inserm.fr",
        "aff_unique_abbr": "CRICM;Inserm",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "155c9e4a75",
        "title": "Spectral Regularization for Support Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7143d7fbadfa4693b9eec507d9d37443-Abstract.html",
        "author": "Ernesto D. Vito; Lorenzo Rosasco; Alessandro Toigo",
        "abstract": "In this paper we consider the problem of learning from data  the  support of a probability distribution when  the distribution {\\em does not} have a density (with respect to some reference measure). We  propose a  new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space,  which we call {\\em   ``completely regular''}. Completely regular kernels   allow to capture the relevant  geometric and topological properties  of an arbitrary probability space.  In particular, they are the key ingredient to prove the  universal consistency  of the spectral  estimators and in this respect they are the analogue of  universal kernels for supervised problems. Numerical  experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for  density support  estimation.",
        "bibtex": "@inproceedings{NIPS2010_7143d7fb,\n author = {Vito, Ernesto and Rosasco, Lorenzo and Toigo, Alessandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Regularization for Support Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 130087,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7987594581389276048&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "DSA, Univ. di Genova, and INFN, Sezione di Genova, Italy; CBCL - MIT, USA, and IIT, Italy; Politec. di Milano, Dept. of Math., and INFN, Sezione di Milano, Italy",
        "aff_domain": "dima.ungie.it;mit.edu;ge.infn.it",
        "email": "dima.ungie.it;mit.edu;ge.infn.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Genoa;Massachusetts Institute of Technology;Politecnico di Milano",
        "aff_unique_dep": "DSA;Computer Science and Artificial Intelligence Laboratory;Department of Mathematics",
        "aff_unique_url": "https://www.unige.it;https://www.csail.mit.edu;https://www.polimi.it/",
        "aff_unique_abbr": "Univ. di Genova;MIT;Polimi",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Italy;United States"
    },
    {
        "id": "ecbfa597d3",
        "title": "Sphere Embedding: An Application to Part-of-Speech Induction",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/9adeb82fffb5444e81fa0ce8ad8afe7a-Abstract.html",
        "author": "Yariv Maron; Michael Lamar; Elie Bienenstock",
        "abstract": "Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a high-dimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks.",
        "bibtex": "@inproceedings{NIPS2010_9adeb82f,\n author = {Maron, Yariv and Lamar, Michael and Bienenstock, Elie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sphere Embedding: An Application to Part-of-Speech Induction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 346176,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3798947279957079993&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Gonda Brain Research Center, Bar-Ilan University; Department of Mathematics and Computer Science, Saint Louis University; Division of Applied Mathematics and Department of Neuroscience, Brown University",
        "aff_domain": "yahoo.com;slu.edu;brown.edu",
        "email": "yahoo.com;slu.edu;brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Bar-Ilan University;Saint Louis University;Brown University",
        "aff_unique_dep": "Gonda Brain Research Center;Department of Mathematics and Computer Science;Division of Applied Mathematics",
        "aff_unique_url": "https://www.biu.ac.il;https://www.slu.edu;https://www.brown.edu",
        "aff_unique_abbr": "BIU;SLU;Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "f48f9255e3",
        "title": "Spike timing-dependent plasticity as dynamic filter",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/892c91e0a653ba19df81a90f89d99bcd-Abstract.html",
        "author": "Joscha Schmiedt; Christian Albers; Klaus Pawelzik",
        "abstract": "When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with attenuated and enhanced pre- and postsynaptic contributions to long-term synaptic modifications. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We find that our model reproduces a wide range of experimental results with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear filter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic firing rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. For low baseline rates modifications are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also find emphasis of low baseline spike rates and suppression for high baseline rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models.",
        "bibtex": "@inproceedings{NIPS2010_892c91e0,\n author = {Schmiedt, Joscha and Albers, Christian and Pawelzik, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spike timing-dependent plasticity as dynamic filter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1988382,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=859352185958366002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Institute for Theoretical Physics, University of Bremen, Bremen, Germany; Institute for Theoretical Physics, University of Bremen, Bremen, Germany; Institute for Theoretical Physics, University of Bremen, Bremen, Germany",
        "aff_domain": "uni-bremen.de;neuro.uni-bremen.de;neuro.uni-bremen.de",
        "email": "uni-bremen.de;neuro.uni-bremen.de;neuro.uni-bremen.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Bremen",
        "aff_unique_dep": "Institute for Theoretical Physics",
        "aff_unique_url": "https://www.uni-bremen.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bremen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "9bfab2b781",
        "title": "SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html",
        "author": "Sylvain Chevallier; H\u00e9l\\`ene Paugam-moisy; Michele Sebag",
        "abstract": "Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Specifically, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities - similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed.",
        "bibtex": "@inproceedings{NIPS2010_7c9d0b1f,\n author = {Chevallier, Sylvain and Paugam-moisy, H\\'{e}l\\textbackslash \\textasciigrave ene and Sebag, Michele},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 346655,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2873176435381994100&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "TAO, INRIA-Saclay, Univ. Paris-Sud, F-91405 Orsay, France; LIRIS, CNRS, Univ. Lyon 2, F-69676 Bron, France; TAO, LRI\u2212CNRS, Univ. Paris-Sud, F-91405 Orsay, France",
        "aff_domain": "lri.fr;liris.cnrs.fr;lri.fr",
        "email": "lri.fr;liris.cnrs.fr;lri.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "INRIA-Saclay;CNRS;University Paris-Sud",
        "aff_unique_dep": "TAO;LIRIS;TAO, LRI\u2212CNRS",
        "aff_unique_url": "https://www.inria.fr/en;https://www.cnrs.fr;https://www.universite-paris-sud.fr",
        "aff_unique_abbr": "INRIA;CNRS;Paris-Sud",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Saclay;;Orsay",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "7f35dd024d",
        "title": "Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/301ad0e3bd5cb1627a2044908a42fdc2-Abstract.html",
        "author": "Han Liu; Kathryn Roeder; Larry Wasserman",
        "abstract": "A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include $K$-fold cross-validation ($K$-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size asymptotically increases with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all competing procedures.",
        "bibtex": "@inproceedings{NIPS2010_301ad0e3,\n author = {Liu, Han and Roeder, Kathryn and Wasserman, Larry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/301ad0e3bd5cb1627a2044908a42fdc2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 408489,
        "gs_citation": 633,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10295275232786504335&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c06d9dd731",
        "title": "Static Analysis of Binary Executables Using Structural SVMs",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a1d33d0dfec820b41b54430b50e96b5c-Abstract.html",
        "author": "Nikos Karampatziakis",
        "abstract": "We cast the problem of identifying basic blocks of code in a binary executable as learning a mapping from a byte sequence to a segmentation of the sequence. In general, inference in segmentation models, such as semi-CRFs, can be cubic in the length of the sequence. By taking advantage of the structure of our problem, we derive a linear-time inference algorithm which makes our approach practical, given that even small programs are tens or hundreds of thousands bytes long. Furthermore, we introduce two loss functions which are appropriate for our problem and show how to use structural SVMs to optimize the learned mapping for these losses. Finally, we present experimental results that demonstrate the advantages of our method against a strong baseline.",
        "bibtex": "@inproceedings{NIPS2010_a1d33d0d,\n author = {Karampatziakis, Nikos},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Static Analysis of Binary Executables Using Structural SVMs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a1d33d0dfec820b41b54430b50e96b5c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a1d33d0dfec820b41b54430b50e96b5c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a1d33d0dfec820b41b54430b50e96b5c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 187582,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6669979305442123542&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Cornell University",
        "aff_domain": "cs.cornell.edu",
        "email": "cs.cornell.edu",
        "github": "",
        "project": "http://www.cs.cornell.edu/~nk",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f1366036b3",
        "title": "Structural epitome: a way to summarize one\u2019s visual experience",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a51fb975227d6640e4fe47854476d133-Abstract.html",
        "author": "Nebojsa Jojic; Alessandro Perina; Vittorio Murino",
        "abstract": "In order to study the properties of total visual input in humans, a single subject wore a camera for two weeks capturing, on average, an image every 20 seconds (www.research.microsoft.com/~jojic/aihs). The resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects. Our first analysis goal is to create a visual summary of the subject\u2019s two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions. Direct application of existing algorithms, such as panoramic stitching (e.g.  Photosynth) or appearance-based clustering models (e.g. the epitome), is impractical due to either the large dataset size or the dramatic variation in the lighting conditions.    As a remedy to these problems, we introduce a novel image representation, the \u201cstel epitome,\u201d and an associated efficient learning algorithm. In our model, each image or image patch is characterized by a hidden mapping T, which, as in previous epitome models, defines a mapping between the image-coordinates and the coordinates in the large all-I-have-seen\" epitome matrix. The limited epitome real-estate forces the mappings of different images to overlap, with this overlap indicating image similarity. However, in our model the image similarity does not depend on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models, but on spatial configuration of scene or object parts, as the model is based on the palette-invariant stel models. As a result, stel epitomes capture structure that is invariant to non-structural changes, such as illumination, that tend to uniformly affect pixels belonging to a single scene or object part.\"",
        "bibtex": "@inproceedings{NIPS2010_a51fb975,\n author = {Jojic, Nebojsa and Perina, Alessandro and Murino, Vittorio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structural epitome: a way to summarize one\u2019s visual experience},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a51fb975227d6640e4fe47854476d133-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a51fb975227d6640e4fe47854476d133-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/a51fb975227d6640e4fe47854476d133-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a51fb975227d6640e4fe47854476d133-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1753439,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8005161432123077297&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "fde46c0e09",
        "title": "Structured Determinantal Point Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/1f50893f80d6830d62765ffad7721742-Abstract.html",
        "author": "Alex Kulesza; Ben Taskar",
        "abstract": "We present a novel probabilistic model for distributions over sets of structures -- for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of structured probabilistic models, like Markov random fields and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions. We extend the determinantal point process model to handle an exponentially-sized set of particles (structures) via a natural factorization of the model into parts. We show how this factorization leads to tractable algorithms for exact inference, including computing marginals, computing conditional probabilities, and sampling. Our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes, and use message passing over a special semiring to compute relevant quantities. We illustrate the advantages of the model on tracking and articulated pose estimation problems.",
        "bibtex": "@inproceedings{NIPS2010_1f50893f,\n author = {Kulesza, Alex and Taskar, Ben},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured Determinantal Point Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f50893f80d6830d62765ffad7721742-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/1f50893f80d6830d62765ffad7721742-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/1f50893f80d6830d62765ffad7721742-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/1f50893f80d6830d62765ffad7721742-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 492498,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13584436157776604234&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer and Information Science, University of Pennsylvania; Department of Computer and Information Science, University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1a49d0216d",
        "title": "Structured sparsity-inducing norms through submodular functions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4b0a59ddf11c58e7446c9df0da541a84-Abstract.html",
        "author": "Francis R. Bach",
        "abstract": "Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lovasz extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.",
        "bibtex": "@inproceedings{NIPS2010_4b0a59dd,\n author = {Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured sparsity-inducing norms through submodular functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4b0a59ddf11c58e7446c9df0da541a84-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4b0a59ddf11c58e7446c9df0da541a84-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4b0a59ddf11c58e7446c9df0da541a84-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 463606,
        "gs_citation": 232,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16384504733898901737&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "634b08b1ad",
        "title": "Subgraph Detection Using Eigenvector L1 Norms",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6395ebd0f4b478145ecfbaf939454fa4-Abstract.html",
        "author": "Benjamin Miller; Nadya Bliss; Patrick J. Wolfe",
        "abstract": "When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a detection theory\" for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph\u2019s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets confirms the efficacy of this approach.\"",
        "bibtex": "@inproceedings{NIPS2010_6395ebd0,\n author = {Miller, Benjamin and Bliss, Nadya and Wolfe, Patrick},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Subgraph Detection Using Eigenvector L1 Norms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6395ebd0f4b478145ecfbaf939454fa4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 252832,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7837861836974722924&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Lincoln Laboratory, Massachusetts Institute of Technology; Lincoln Laboratory, Massachusetts Institute of Technology; Statistics and Information Sciences Laboratory, Harvard University",
        "aff_domain": "ll.mit.edu;ll.mit.edu;stat.harvard.edu",
        "email": "ll.mit.edu;ll.mit.edu;stat.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Harvard University",
        "aff_unique_dep": "Lincoln Laboratory;Statistics and Information Sciences Laboratory",
        "aff_unique_url": "https://web.mit.edu;https://www.harvard.edu",
        "aff_unique_abbr": "MIT;Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8e237e8285",
        "title": "Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6c524f9d5d7027454a783c841250ba71-Abstract.html",
        "author": "Hongbo Zhou; Qiang Cheng",
        "abstract": "Regularization technique has become a principle tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further gave mathematically exact definition for a novel representation called sparse grouping representation (SGR), and proved sufficient conditions for generating such group level sparsity. Under these sufficient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also gave out some generalization bounds in a classification setting.",
        "bibtex": "@inproceedings{NIPS2010_6c524f9d,\n author = {Zhou, Hongbo and Cheng, Qiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6c524f9d5d7027454a783c841250ba71-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 144978,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2003408833611543136&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science department, Southern Illinois University Carbondale, IL, 62901; Computer Science department, Southern Illinois University Carbondale, IL, 62901",
        "aff_domain": "siu.edu;cs.siu.edu",
        "email": "siu.edu;cs.siu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Southern Illinois University",
        "aff_unique_dep": "Computer Science department",
        "aff_unique_url": "https://www.siu.edu",
        "aff_unique_abbr": "SIU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Carbondale",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "269b16c3b5",
        "title": "Supervised Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/18997733ec258a9fcaf239cc55d53363-Abstract.html",
        "author": "Pranjal Awasthi; Reza B. Zadeh",
        "abstract": "Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority  of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class  in that model. Our algorithm is query-efficient in the sense that it involves only a small amount  of interaction with the teacher. We also present and study two natural generalizations of the model.  The model assumes that the teacher response to the algorithm is perfect. We eliminate  this limitation by proposing a noisy model and give an algorithm for  clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees  a random subset of the points. Finally, for datasets  satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class  of clustering functions containing Single-Linkage will find the target clustering under the strongest  property.",
        "bibtex": "@inproceedings{NIPS2010_18997733,\n author = {Awasthi, Pranjal and Zadeh, Reza},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Supervised Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/18997733ec258a9fcaf239cc55d53363-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 96891,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7426994754424239808&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Carnegie Mellon University; Stanford University",
        "aff_domain": "cs.cmu.edu;stanford.edu",
        "email": "cs.cmu.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.stanford.edu",
        "aff_unique_abbr": "CMU;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cf7c76f643",
        "title": "Switched Latent Force Models for Movement Segmentation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/3a029f04d76d32e79367c4b3255dda4d-Abstract.html",
        "author": "Mauricio Alvarez; Jan R. Peters; Neil D. Lawrence; Bernhard Sch\u00f6lkopf",
        "abstract": "Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology.",
        "bibtex": "@inproceedings{NIPS2010_3a029f04,\n author = {Alvarez, Mauricio and Peters, Jan and Lawrence, Neil and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Switched Latent Force Models for Movement Segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 890744,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5808626008200367507&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4bfc16e8cf",
        "title": "Switching state space model for simultaneously estimating state transitions and nonstationary firing rates",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0aa1883c6411f7873cb83dacb17b0afc-Abstract.html",
        "author": "Ken Takiyama; Masato Okada",
        "abstract": "We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary firing rates using a switching state space model (SSSM).  This model enables us to detect state transitions based not only on the discontinuous changes of mean firing rates but also on discontinuous changes in temporal profiles of firing rates, e.g., temporal correlation.  We derive a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events.  Synthetic data analysis reveals the high performance of our algorithm in estimating state transitions, the number of neural states, and nonstationary firing rates compared to previous methods.  We also analyze neural data recorded from the medial temporal area.  The statistically detected neural states probably coincide with transient and sustained states, which have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition based on discontinuous change in the temporal correlation of firing rates, which transitions previous methods cannot detect.  This result suggests the advantage of our algorithm in real-data analysis.",
        "bibtex": "@inproceedings{NIPS2010_0aa1883c,\n author = {Takiyama, Ken and Okada, Masato},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Switching state space model for simultaneously estimating state transitions and nonstationary firing rates},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0aa1883c6411f7873cb83dacb17b0afc-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0aa1883c6411f7873cb83dacb17b0afc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0aa1883c6411f7873cb83dacb17b0afc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 206594,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3017320685598268651&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "12decfecb7",
        "title": "Synergies in learning words and their referents",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/d64a340bcb633f536d56e51874281454-Abstract.html",
        "author": "Mark Johnson; Katherine Demuth; Bevan Jones; Michael J. Black",
        "abstract": "This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these.",
        "bibtex": "@inproceedings{NIPS2010_d64a340b,\n author = {Johnson, Mark and Demuth, Katherine and Jones, Bevan and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Synergies in learning words and their referents},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d64a340bcb633f536d56e51874281454-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/d64a340bcb633f536d56e51874281454-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/d64a340bcb633f536d56e51874281454-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 199666,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6085862946095719726&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computing, Macquarie University, Sydney, NSW 2109; Department of Linguistics, Macquarie University, Sydney, NSW 2109; Department of Psychology, Stanford University, Palo Alto, CA 94305; School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK",
        "aff_domain": "mq.edu.au;mq.edu.au;mit.edu;sms.ed.ac.uk",
        "email": "mq.edu.au;mq.edu.au;mit.edu;sms.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Macquarie University;Stanford University;University of Edinburgh",
        "aff_unique_dep": "Department of Computing;Department of Psychology;School of Informatics",
        "aff_unique_url": "https://www.mq.edu.au;https://www.stanford.edu;https://www.ed.ac.uk",
        "aff_unique_abbr": "MQ;Stanford;Edinburgh",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Sydney;Palo Alto;Edinburgh",
        "aff_country_unique_index": "0;0;1;2",
        "aff_country_unique": "Australia;United States;United Kingdom"
    },
    {
        "id": "6a74e9f83f",
        "title": "The LASSO risk: asymptotic results and real world examples",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/f47330643ae134ca204bf6b2481fec47-Abstract.html",
        "author": "Mohsen Bayati; Jos\u00e9 Pereira; Andrea Montanari",
        "abstract": "We consider the problem of learning a coefficient vector x0 from noisy linear observation y=Ax0+w. In many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator. In this case, a popular approach consists in solving an l1-penalized least squares problem known as the LASSO or BPDN.  For sequences of matrices A of increasing dimensions, with iid gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the first rigorous derivation of an explicit formula for the asymptotic risk of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efficient algorithm, that is inspired from graphical models ideas.   Through simulations on real data matrices (gene expression data and hospital medical records) we observe that these results can be relevant in a broad array of practical applications.",
        "bibtex": "@inproceedings{NIPS2010_f4733064,\n author = {Bayati, Mohsen and Pereira, Jos\\'{e} and Montanari, Andrea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The LASSO risk: asymptotic results and real world examples},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/f47330643ae134ca204bf6b2481fec47-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 148332,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8340456876473602&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8d7f6fc488",
        "title": "The Maximal Causes of Natural Scenes are Edge Filters",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/6f2268bd1d3d3ebaabb04d6b5d099425-Abstract.html",
        "author": "Jose Puertas; Joerg Bornschein; J\u00f6rg L\u00fccke",
        "abstract": "We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we find that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the first large-scale application of such an approach.",
        "bibtex": "@inproceedings{NIPS2010_6f2268bd,\n author = {Puertas, Jose and Bornschein, Joerg and L\\\"{u}cke, J\\\"{o}rg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Maximal Causes of Natural Scenes are Edge Filters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4745828,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8840909204142064597&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Frankfurt Institute for Advanced Studies + Goethe-University Frankfurt, Germany; Frankfurt Institute for Advanced Studies + Goethe-University Frankfurt, Germany; Frankfurt Institute for Advanced Studies + Goethe-University Frankfurt, Germany",
        "aff_domain": "fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de",
        "email": "fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Frankfurt Institute for Advanced Studies;Goethe University Frankfurt",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.fias.uni-frankfurt.de/;https://www.uni-frankfurt.de",
        "aff_unique_abbr": "FIAS;GU Frankfurt",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Frankfurt",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "058976b99a",
        "title": "The Multidimensional Wisdom of Crowds",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0f9cafd014db7a619ddb4276af0d692c-Abstract.html",
        "author": "Peter Welinder; Steve Branson; Pietro Perona; Serge J. Belongie",
        "abstract": "Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different \"schools of thought\" amongst the annotators, and can group together images belonging to separate categories.",
        "bibtex": "@inproceedings{NIPS2010_0f9cafd0,\n author = {Welinder, Peter and Branson, Steve and Perona, Pietro and Belongie, Serge},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Multidimensional Wisdom of Crowds},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f9cafd014db7a619ddb4276af0d692c-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0f9cafd014db7a619ddb4276af0d692c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0f9cafd014db7a619ddb4276af0d692c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2544253,
        "gs_citation": 1238,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11269155237596441685&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "California Institute of Technology; University of California, San Diego; University of California, San Diego; California Institute of Technology",
        "aff_domain": "caltech.edu;caltech.edu;cs.ucsd.edu;cs.ucsd.edu",
        "email": "caltech.edu;caltech.edu;cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "California Institute of Technology;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.caltech.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "Caltech;UCSD",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Pasadena;San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e7a0a308d2",
        "title": "The Neural Costs of Optimal Control",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/8a0e1141fd37fa5b98d5bb769ba1a7cc-Abstract.html",
        "author": "Samuel Gershman; Robert Wilson",
        "abstract": "Optimal control entails combining probabilities and utilities. However, for most practical problems probability densities can be represented only approximately. Choosing an approximation requires balancing the benefits of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive fields.",
        "bibtex": "@inproceedings{NIPS2010_8a0e1141,\n author = {Gershman, Samuel and Wilson, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Neural Costs of Optimal Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 255017,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9334387962804858010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8768853e1f",
        "title": "Throttling Poisson Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/b56a18e0eacdf51aa2a5306b0f533204-Abstract.html",
        "author": "Uwe Dick; Peter Haider; Thomas Vanck; Michael Br\u00fcckner; Tobias Scheffer",
        "abstract": "We study a setting in which Poisson processes generate sequences of decision-making events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efficiently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service.",
        "bibtex": "@inproceedings{NIPS2010_b56a18e0,\n author = {Dick, Uwe and Haider, Peter and Vanck, Thomas and Br\\\"{u}ckner, Michael and Scheffer, Tobias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Throttling Poisson Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/b56a18e0eacdf51aa2a5306b0f533204-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 122144,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17046129001787599879&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Potsdam; University of Potsdam; University of Potsdam; University of Potsdam; University of Potsdam",
        "aff_domain": "cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de",
        "email": "cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de;cs.uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Potsdam",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-potsdam.de",
        "aff_unique_abbr": "UP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "e37b5d24c7",
        "title": "Tight Sample Complexity of Large-Margin Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/9b04d152845ec0a378394003c96da594-Abstract.html",
        "author": "Sivan Sabato; Nathan Srebro; Naftali Tishby",
        "abstract": "We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L2 regularization: We introduce the gamma-adapted-dimension, which is a simple function of the spectrum of a distribution's covariance matrix, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the gamma-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. The bounds hold for a rich family of sub-Gaussian distributions.",
        "bibtex": "@inproceedings{NIPS2010_9b04d152,\n author = {Sabato, Sivan and Srebro, Nathan and Tishby, Naftali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tight Sample Complexity of Large-Margin Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9b04d152845ec0a378394003c96da594-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/9b04d152845ec0a378394003c96da594-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/9b04d152845ec0a378394003c96da594-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/9b04d152845ec0a378394003c96da594-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 144454,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14818903216830672226&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel; Toyota Technological Institute at Chicago, Chicago, IL 60637, USA; School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_domain": "cs.huji.ac.il;ttic.edu;cs.huji.ac.il",
        "email": "cs.huji.ac.il;ttic.edu;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Hebrew University;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "School of Computer Science & Engineering;",
        "aff_unique_url": "http://www.huji.ac.il;https://www.tti-chicago.org",
        "aff_unique_abbr": "HUJI;TTI Chicago",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Jerusalem;Chicago",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "eb6b9599af",
        "title": "Tiled convolutional neural networks",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/01f78be6f7cad02658508fe4616098a9-Abstract.html",
        "author": "Jiquan Ngiam; Zhenghao Chen; Daniel Chia; Pang W. Koh; Quoc V. Le; Andrew Y. Ng",
        "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signi\ufb01cantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular \u201ctiled\u201d pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs\u2019 advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.",
        "bibtex": "@inproceedings{NIPS2010_01f78be6,\n author = {Ngiam, Jiquan and Chen, Zhenghao and Chia, Daniel and Koh, Pang and Le, Quoc and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tiled convolutional neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01f78be6f7cad02658508fe4616098a9-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/01f78be6f7cad02658508fe4616098a9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/01f78be6f7cad02658508fe4616098a9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 925774,
        "gs_citation": 476,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14282515616261813077&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5de0caa656",
        "title": "Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/eb160de1de89d9058fcb0b968dbbbd68-Abstract.html",
        "author": "Congcong Li; Adarsh Kowdle; Ashutosh Saxena; Tsuhan Chen",
        "abstract": "In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classifiers already exist for many sub-tasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classifier.  We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a \u2018black-box\u2019 interface to the original classifier for each sub-task. We use a two-layer cascade of classifiers, which are repeated instantiations of the original ones, with the output of the first layer fed into the second layer as input. Our training method involves a feedback step that allows later classifiers to provide earlier classifiers information about what error modes to focus on. We show that our method significantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classification.",
        "bibtex": "@inproceedings{NIPS2010_eb160de1,\n author = {Li, Congcong and Kowdle, Adarsh and Saxena, Ashutosh and Chen, Tsuhan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4687048,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5115054035442885357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Cornell University; Cornell University; Cornell University; Cornell University",
        "aff_domain": "cornell.edu;cornell.edu;cs.cornell.edu;ece.cornell.edu",
        "email": "cornell.edu;cornell.edu;cs.cornell.edu;ece.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cc66c6d770",
        "title": "Towards Property-Based Classification of Clustering Paradigms",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/f93882cbd8fc7fb794c1011d63be6fb6-Abstract.html",
        "author": "Margareta Ackerman; Shai Ben-David; David Loker",
        "abstract": "Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill defined problem - given a data set, it is not clear what a \u201ccorrect\u201d clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some first steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms.  In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinbergs famous impossibility result, while providing a simpler proof.",
        "bibtex": "@inproceedings{NIPS2010_f93882cb,\n author = {Ackerman, Margareta and Ben-David, Shai and Loker, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Towards Property-Based Classification of Clustering Paradigms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 246124,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16369405513565455350&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "D.R.C. School of Computer Science, University of Waterloo, Canada; D.R.C. School of Computer Science, University of Waterloo, Canada; D.R.C. School of Computer Science, University of Waterloo, Canada",
        "aff_domain": "cs.uwaterloo.ca;cs.uwaterloo.ca;cs.uwaterloo.ca",
        "email": "cs.uwaterloo.ca;cs.uwaterloo.ca;cs.uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "ec52920aae",
        "title": "Trading off Mistakes and Don't-Know Predictions",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/286674e3082feb7e5afb92777e48821f-Abstract.html",
        "author": "Amin Sayedi; Morteza Zadimoghaddam; Avrim Blum",
        "abstract": "We discuss an online learning framework in which the agent is allowed to say",
        "bibtex": "@inproceedings{NIPS2010_286674e3,\n author = {Sayedi, Amin and Zadimoghaddam, Morteza and Blum, Avrim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Trading off Mistakes and Don\\textquotesingle t-Know Predictions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/286674e3082feb7e5afb92777e48821f-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/286674e3082feb7e5afb92777e48821f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/286674e3082feb7e5afb92777e48821f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 113041,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11751955963756776444&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Tepper School of Business, CMU, Pittsburgh, PA 15213 + Microsoft Research New England, MA; CSAIL, MIT, Cambridge, MA 02139 + Microsoft Research Cambridge, UK; Department of Computer Science, CMU, Pittsburgh, PA 15213",
        "aff_domain": "cmu.edu;mit.edu;cs.cmu.edu",
        "email": "cmu.edu;mit.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;0",
        "aff_unique_norm": "Carnegie Mellon University;Microsoft;Massachusetts Institute of Technology",
        "aff_unique_dep": "Tepper School of Business;Microsoft Research;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.cmu.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-new-england;https://www.csail.mit.edu",
        "aff_unique_abbr": "CMU;MSR NE;MIT",
        "aff_campus_unique_index": "0+1;2+2;0",
        "aff_campus_unique": "Pittsburgh;New England;Cambridge",
        "aff_country_unique_index": "0+0;0+1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "6f9db90e1b",
        "title": "Transduction with Matrix Completion: Three Birds with One Stone",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/06409663226af2f3114485aa4e0a23b4-Abstract.html",
        "author": "Andrew Goldberg; Ben Recht; Junming Xu; Robert Nowak; Xiaojin Zhu",
        "abstract": "We pose transductive classification as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspecified, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modified fixed-point continuation method that is guaranteed to find the global optimum.",
        "bibtex": "@inproceedings{NIPS2010_06409663,\n author = {Goldberg, Andrew and Recht, Ben and Xu, Junming and Nowak, Robert and Zhu, Jerry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Transduction with Matrix Completion: Three Birds with One Stone},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/06409663226af2f3114485aa4e0a23b4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 151254,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3643620334272350719&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Sciences; Department of Computer Sciences; Department of Computer Sciences; Department of Computer Sciences; Department of Electrical and Computer Engineering",
        "aff_domain": "cs.wisc.edu;cs.wisc.edu;cs.wisc.edu;cs.wisc.edu;ece.wisc.edu",
        "email": "cs.wisc.edu;cs.wisc.edu;cs.wisc.edu;cs.wisc.edu;ece.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "University of Wisconsin-Madison;Unknown Institution",
        "aff_unique_dep": "Department of Computer Sciences;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.cs.wisc.edu;",
        "aff_unique_abbr": "UW-Madison;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "a50ddf8908",
        "title": "Tree-Structured Stick Breaking for Hierarchical Data",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html",
        "author": "Zoubin Ghahramani; Michael I. Jordan; Ryan P. Adams",
        "abstract": "Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a flexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data.",
        "bibtex": "@inproceedings{NIPS2010_a5e00132,\n author = {Ghahramani, Zoubin and Jordan, Michael and Adams, Ryan P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Tree-Structured Stick Breaking for Hierarchical Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1244687,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13060292342108984574&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Dept. of Computer Science, University of Toronto; Dept. of Engineering, University of Cambridge; Depts. of EECS and Statistics, University of California, Berkeley",
        "aff_domain": "cs.toronto.edu;eng.cam.ac.uk;eecs.berkeley.edu",
        "email": "cs.toronto.edu;eng.cam.ac.uk;eecs.berkeley.edu",
        "github": "",
        "project": "http://www.cs.toronto.edu/~rpa/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Toronto;University of Cambridge;University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science;Dept. of Engineering;Department of Electrical Engineering and Computer Sciences, Department of Statistics",
        "aff_unique_url": "https://www.utoronto.ca;https://www.cam.ac.uk;https://www.berkeley.edu",
        "aff_unique_abbr": "U of T;Cambridge;UC Berkeley",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Toronto;Cambridge;Berkeley",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Canada;United Kingdom;United States"
    },
    {
        "id": "44727c10b0",
        "title": "Two-Layer Generalization Analysis for Ranking Using Rademacher Average",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/c8ffe9a587b126f152ed3d89a146b445-Abstract.html",
        "author": "Wei Chen; Tie-yan Liu; Zhi-ming Ma",
        "abstract": "This paper is concerned with the generalization analysis on learning to rank for information retrieval (IR). In IR, data are hierarchically organized, i.e., consisting of queries and documents per query. Previous generalization analysis for ranking, however, has not fully considered this structure, and cannot explain how the simultaneous change of query number and document number in the training data will affect the performance of algorithms. In this paper, we propose performing generalization analysis under the assumption of two-layer sampling, i.e., the i.i.d. sampling of queries and the conditional i.i.d sampling of documents per query. Such a sampling can better describe the generation mechanism of real data, and the corresponding generalization analysis can better explain the real behaviors of learning to rank algorithms. However, it is challenging to perform such analysis, because the documents associated with different queries are not identically distributed, and the documents associated with the same query become no longer independent if represented by features extracted from the matching between document and query. To tackle the challenge, we decompose the generalization error according to the two layers, and make use of the new concept of two-layer Rademacher average. The generalization bounds we obtained are quite intuitive and are in accordance with previous empirical studies on the performance of ranking algorithms.",
        "bibtex": "@inproceedings{NIPS2010_c8ffe9a5,\n author = {Chen, Wei and Liu, Tie-yan and Ma, Zhi-ming},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Two-Layer Generalization Analysis for Ranking Using Rademacher Average},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 147776,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12415929297066405351&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Chinese Academy of Sciences; Microsoft Research Asia; Chinese Academy of Sciences",
        "aff_domain": "amss.ac.cn;micorsoft.com;amt.ac.cn",
        "email": "amss.ac.cn;micorsoft.com;amt.ac.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Chinese Academy of Sciences;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.cas.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "CAS;MSR Asia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "33b1ac6838",
        "title": "Universal Consistency of Multi-Class Support Vector Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/182be0c5cdcd5072bb1864cdee4d3d6e-Abstract.html",
        "author": "Tobias Glasmachers",
        "abstract": "Steinwart was the \ufb01rst to prove universal consistency of support vector machine  classi\ufb01cation. His proof analyzed the \u2018standard\u2019 support vector machine classi\ufb01er,  which is restricted to binary classi\ufb01cation problems. In contrast, recent analysis  has resulted in the common belief that several extensions of SVM classi\ufb01cation to  more than two classes are inconsistent.  Countering this belief, we proof the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart\u2019s techniques to the multi-class case.",
        "bibtex": "@inproceedings{NIPS2010_182be0c5,\n author = {Glasmachers, Tobias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Universal Consistency of Multi-Class Support Vector Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146548,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16552647118275585472&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dalle Molle Institute for Arti\ufb01cial Intelligence (IDSIA), 6928 Manno-Lugano, Switzerland",
        "aff_domain": "idsia.ch",
        "email": "idsia.ch",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Dalle Molle Institute for Arti\ufb01cial Intelligence",
        "aff_unique_dep": "Artificial Intelligence",
        "aff_unique_url": "http://www.idsia.ch/",
        "aff_unique_abbr": "IDSIA",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2a5c74467a",
        "title": "Universal Kernels on Non-Standard Input Spaces",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4e0cb6fb5fb446d1c92ede2ed8780188-Abstract.html",
        "author": "Andreas Christmann; Ingo Steinwart",
        "abstract": "During the last years support vector machines (SVMs) have been successfully applied even in situations where the input space $X$ is not necessarily a subset of $R^d$. Examples include SVMs using probability measures to analyse e.g. histograms or coloured images, SVMs for text classification and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) $H\\subset L_p(P_X)$ is dense, or if the SVM is based on a universal kernel $k$.  So far, however, there are no RKHSs of practical interest known that satisfy these assumptions on $\\cH$ or $k$ if $X \\not\\subset R^d$.  We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of $R^d$. We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing.",
        "bibtex": "@inproceedings{NIPS2010_4e0cb6fb,\n author = {Christmann, Andreas and Steinwart, Ingo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Universal Kernels on Non-Standard Input Spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4e0cb6fb5fb446d1c92ede2ed8780188-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272063,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15870012509473701041&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Bayreuth, Department of Mathematics; University of Stuttgart, Department of Mathematics",
        "aff_domain": "uni-bayreuth.de;mathematik.uni-stuttgart.de",
        "email": "uni-bayreuth.de;mathematik.uni-stuttgart.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Bayreuth;University of Stuttgart",
        "aff_unique_dep": "Department of Mathematics;Department of Mathematics",
        "aff_unique_url": "https://www.uni-bayreuth.de;https://www.uni-stuttgart.de",
        "aff_unique_abbr": ";USTuttgart",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2c04769aa5",
        "title": "Unsupervised Kernel Dimension Reduction",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/afda332245e2af431fb7b672a68b659d-Abstract.html",
        "author": "Meihong Wang; Fei Sha; Michael I. Jordan",
        "abstract": "We apply the framework of kernel dimension reduction, originally designed for supervised problems, to unsupervised dimensionality reduction. In this framework, kernel-based measures of independence are used to derive low-dimensional representations that maximally capture information in covariates in order to predict responses. We extend this idea and develop similarly motivated measures for unsupervised problems where covariates and responses are the same. Our empirical studies show that the resulting compact representation yields meaningful and appealing visualization and clustering of data. Furthermore, when used in conjunction with supervised learners for classification, our methods lead to lower classification errors than state-of-the-art methods, especially when embedding data in spaces of very few dimensions.",
        "bibtex": "@inproceedings{NIPS2010_afda3322,\n author = {Wang, Meihong and Sha, Fei and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Kernel Dimension Reduction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/afda332245e2af431fb7b672a68b659d-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/afda332245e2af431fb7b672a68b659d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/afda332245e2af431fb7b672a68b659d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/afda332245e2af431fb7b672a68b659d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 889630,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Dept. of Computer Science, U. of Southern California, Los Angeles, CA 90089; Dept. of Computer Science, U. of Southern California, Los Angeles, CA 90089; Dept. of Statistics, U. of California, Berkeley, CA",
        "aff_domain": "usc.edu;usc.edu;cs.berkeley.edu",
        "email": "usc.edu;usc.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Southern California;University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science;Department of Statistics",
        "aff_unique_url": "https://www.usc.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "USC;UC Berkeley",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Los Angeles;Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d734496ba5",
        "title": "Using body-anchored priors for identifying actions in single images",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/15d4e891d784977cacbfcbb00c48f133-Abstract.html",
        "author": "Leonid Karlinsky; Michael Dinerstein; Shimon Ullman",
        "abstract": "This paper presents an approach to the visual recognition of human actions using only single images as input. The task is easy for humans but difficult for current approaches to object recognition, because action instances may be similar in terms of body pose, and often require detailed examination of relations between participating objects and body parts in order to be recognized. The proposed approach applies a two-stage interpretation procedure to each training and test image. The first stage produces accurate detection of the relevant body parts of the actor, forming a prior for the local evidence needed to be considered for identifying the action. The second stage extracts features that are \u2018anchored\u2019 to the detected body parts, and uses these features and their feature-to-part relations in order to recognize the action. The body anchored priors we propose apply to a large range of human actions. These priors allow focusing on the relevant regions and relations, thereby significantly simplifying the learning process and increasing recognition performance.",
        "bibtex": "@inproceedings{NIPS2010_15d4e891,\n author = {Karlinsky, Leonid and Dinerstein, Michael and Ullman, Shimon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Using body-anchored priors for identifying actions in single images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6382604,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16533510032129169816&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Weizmann Institute of Science; Department of Computer Science, Weizmann Institute of Science; Department of Computer Science, Weizmann Institute of Science",
        "aff_domain": "weizmann.ac.il;weizmann.ac.il;weizmann.ac.il",
        "email": "weizmann.ac.il;weizmann.ac.il;weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Weizmann Institute of Science",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.weizmann.ac.il",
        "aff_unique_abbr": "Weizmann",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "3a3adb3aef",
        "title": "Variable margin losses for classifier design",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html",
        "author": "Hamed Masnadi-shirazi; Nuno Vasconcelos",
        "abstract": "The problem of controlling the margin of a classifier is studied. A detailed analytical study is presented on how properties of the classification risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classification margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the fixed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter.",
        "bibtex": "@inproceedings{NIPS2010_e0c64119,\n author = {Masnadi-shirazi, Hamed and Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variable margin losses for classifier design},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 165071,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5377512430274790492&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Statistical Visual Computing Laboratory, University of California, San Diego; Statistical Visual Computing Laboratory, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Statistical Visual Computing Laboratory",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cd72797619",
        "title": "Variational Inference over Combinatorial Spaces",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html",
        "author": "Alexandre Bouchard-c\u00f4t\u00e9; Michael I. Jordan",
        "abstract": "Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems (Karzanov et al., 1991; Jerrum et al., 2001; Wilson, 2004), theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods.  Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning.  Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference (Siepel et al., 2004).   Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models (Wainwright et al., 2008); unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments.   We propose a new framework that extends variational inference to a wide range of combinatorial spaces.  Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm.   We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset (Thompson et al., 1999).",
        "bibtex": "@inproceedings{NIPS2010_cb70ab37,\n author = {Bouchard-c\\^{o}t\\'{e}, Alexandre and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Inference over Combinatorial Spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1299884,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13490994431786666929&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Division; Computer Science Division+Department of Statistics",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of California, Berkeley;University Affiliation Not Specified",
        "aff_unique_dep": "Computer Science Division;Department of Statistics",
        "aff_unique_url": "https://www.cs.berkeley.edu;",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4132b3685f",
        "title": "Variational bounds for mixed-data factor analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/2a084e55c87b1ebcdaad1f62fdbbac8e-Abstract.html",
        "author": "Mohammad Emtiyaz Khan; Guillaume Bouchard; Kevin P. Murphy; Benjamin M. Marlin",
        "abstract": "We propose a new variational EM algorithm for fitting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function.  In the special case of fully observed binary data, the bound we propose is significantly faster than previous variational methods. We show that EM is significantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods.  A further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method.",
        "bibtex": "@inproceedings{NIPS2010_2a084e55,\n author = {Khan, Mohammad Emtiyaz and Bouchard, Guillaume and Murphy, Kevin P and Marlin, Benjamin M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational bounds for mixed-data factor analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 257751,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9810836024865943204&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of British Columbia; Xerox Research Center Europe; University of British Columbia; University of British Columbia",
        "aff_domain": "cs.ubc.ca;xerox.com;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;xerox.com;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of British Columbia;Xerox Research Center Europe",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ubc.ca;https://www.xerox.com/research/europe.html",
        "aff_unique_abbr": "UBC;XRC Europe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Canada;Unknown"
    },
    {
        "id": "2701b465ae",
        "title": "Why are some word orders more common than others? A uniform information density account",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html",
        "author": "Luke Maurits; Dan Navarro; Amy Perfors",
        "abstract": "Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an information-theoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-first languages are particularly disfavored because they are highly non-optimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work.",
        "bibtex": "@inproceedings{NIPS2010_0c74b7f7,\n author = {Maurits, Luke and Navarro, Dan and Perfors, Amy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Why are some word orders more common than others? A uniform information density account},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 223763,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5407522376933623375&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "42de004569",
        "title": "Word Features for Latent Dirichlet Allocation",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/db85e2590b6109813dafa101ceb2faeb-Abstract.html",
        "author": "James Petterson; Wray Buntine; Shravan M. Narayanamurthy; Tib\u00e9rio S. Caetano; Alex J. Smola",
        "abstract": "We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model.",
        "bibtex": "@inproceedings{NIPS2010_db85e259,\n author = {Petterson, James and Buntine, Wray and Narayanamurthy, Shravan and Caetano, Tib\\'{e}rio and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Word Features for Latent Dirichlet Allocation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1166547,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9287515757091452239&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "69343d024f",
        "title": "Worst-Case Linear Discriminant Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/4e4b5fbbbb602b6d35bea8460aa8f8e5-Abstract.html",
        "author": "Yu Zhang; Dit-Yan Yeung",
        "abstract": "Dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved. In this paper, we first analyze the scatter measures used in the conventional linear discriminant analysis~(LDA) model and note that the formulation is based on the average-case view. Based on this analysis, we then propose a new dimensionality reduction method called worst-case linear discriminant analysis~(WLDA) by defining new between-class and within-class scatter measures. This new model adopts the worst-case view which arguably is more suitable for applications such as classification. When the number of training data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise, we take a greedy approach by finding one direction of the transformation at a time. Moreover, we also analyze a special case of WLDA to show its relationship with conventional LDA. Experiments conducted on several benchmark datasets demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods.",
        "bibtex": "@inproceedings{NIPS2010_4e4b5fbb,\n author = {Zhang, Yu and Yeung, Dit-Yan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Worst-Case Linear Discriminant Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 364797,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12287941848895647763&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Engineering; Department of Computer Science and Engineering",
        "aff_domain": "cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://cse.ucsd.edu",
        "aff_unique_abbr": "UCSD CSE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ea2650b00f",
        "title": "Worst-case bounds on the quality of max-product fixed-points",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html",
        "author": "Meritxell Vinyals; Jes\\'us Cerquides; Alessandro Farinelli; Juan A. Rodr\u00edguez-aguilar",
        "abstract": "We study worst-case bounds on the quality of any fixed point assignment of the max-product algorithm for Markov Random Fields (MRF). We start proving a bound   independent of the MRF structure and parameters. Afterwards, we show how this bound can be improved for MRFs with particular structures such as bipartite graphs or grids.  Our results provide interesting insight into the behavior of max-product. For example, we prove that max-product provides very good results (at least 90% of the optimal) on MRFs  with large variable-disjoint cycles (MRFs in which all cycles are variable-disjoint, namely that they do not share any edge and in which each cycle contains at least 20 variables).",
        "bibtex": "@inproceedings{NIPS2010_67f7fb87,\n author = {Vinyals, Meritxell and Cerquides, Jes\\textbackslash \\textquotesingle us and Farinelli, Alessandro and Rodr\\'{\\i}guez-aguilar, Juan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Worst-case bounds on the quality of max-product fixed-points},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 361698,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16325060587030765821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Artificial Intelligence Research Institute (IIIA) + Spanish Scientific Research Council (CSIC); Artificial Intelligence Research Institute (IIIA) + Spanish Scientific Research Council (CSIC); Department of Computer Science, University of Verona; Artificial Intelligence Research Institute (IIIA) + Spanish Scientific Research Council (CSIC)",
        "aff_domain": "iiia.csic.es;iiia.csic.es;univr.it;iiia.csic.es",
        "email": "iiia.csic.es;iiia.csic.es;univr.it;iiia.csic.es",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2;0+1",
        "aff_unique_norm": "Artificial Intelligence Research Institute;Spanish Scientific Research Council;University of Verona",
        "aff_unique_dep": "AI Research;;Department of Computer Science",
        "aff_unique_url": "https://www.iiia.csic.es/;https://www.csic.es;https://www.univr.it",
        "aff_unique_abbr": "IIIA;CSIC;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;1;0+0",
        "aff_country_unique": "Spain;Italy"
    },
    {
        "id": "19edfb1651",
        "title": "b-Bit Minwise Hashing for Estimating Three-Way Similarities",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/208e43f0e45c4c78cafadb83d2888cb6-Abstract.html",
        "author": "Ping Li; Arnd Konig; Wenhao Gui",
        "abstract": "Computing two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits (where b>= 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that $b$-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.",
        "bibtex": "@inproceedings{NIPS2010_208e43f0,\n author = {Li, Ping and Konig, Arnd and Gui, Wenhao},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {b-Bit Minwise Hashing for Estimating Three-Way Similarities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/208e43f0e45c4c78cafadb83d2888cb6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 303243,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4428636066960549155&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Statistical Science, Cornell University; Microsoft Research, Microsoft Corporation; Dept. of Statistical Science, Cornell University",
        "aff_domain": "cornell.edu;microsoft.com;cornell.edu",
        "email": "cornell.edu;microsoft.com;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Cornell University;Microsoft",
        "aff_unique_dep": "Dept. of Statistical Science;Microsoft Research",
        "aff_unique_url": "https://www.cornell.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Cornell;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "142f410f05",
        "title": "t-logistic regression",
        "site": "https://papers.nips.cc/paper_files/paper/2010/hash/96da2f590cd7246bbde0051047b0d6f7-Abstract.html",
        "author": "Nan Ding; S.v.n. Vishwanathan",
        "abstract": "We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss  function. An efficient block coordinate descent optimization  scheme can be derived for estimating the parameters. Because of the  nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex   loss functions, our algorithm is fairly robust to the choice of  initial values. We verify both these observations empirically on a  number of synthetic and real datasets.",
        "bibtex": "@inproceedings{NIPS2010_96da2f59,\n author = {Ding, Nan and Vishwanathan, S.v.n.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {t-logistic regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf},\n volume = {23},\n year = {2010}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 470689,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13280930428585270369&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Departments of Statistics; Departments of Computer Science",
        "aff_domain": "purdue.edu;stat.purdue.edu",
        "email": "purdue.edu;stat.purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University Affiliation Not Specified",
        "aff_unique_dep": "Departments of Statistics",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    }
]